{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing yake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from config import GenericArgs\n",
    "from utils.misc import compute_ber, riskset, stop\n",
    "from utils.contextls_utils import synchronicity_test, substitutability_test, tokenizer, riskset, stop\n",
    "from utils.logging import getLogger\n",
    "from utils.dataset_utils import preprocess_txt, preprocess2sentence, get_result_txt, get_dataset\n",
    "from utils.metric import Metric\n",
    "\n",
    "import json\n",
    "\n",
    "random.seed(1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(string):\n",
    "    output = []\n",
    "    string = string.replace(\",\", \"\")\n",
    "    str_list = list(string.replace(\" \", \"\"))\n",
    "    for i in str_list:\n",
    "        output.append(int(i))\n",
    "    return output\n",
    "\n",
    "def main(cover_text, f, extracting=False):\n",
    "    \"\"\"\n",
    "    cover_text: å¾…å¤„ç†çš„å•ä¸ªå¥å­\n",
    "    \"\"\"\n",
    "    encoded_text = tokenizer(cover_text, add_special_tokens=False, truncation=True,\n",
    "                             max_length=tokenizer.model_max_length // 2 - 2)    # ç¼–ç \n",
    "    word_ids = encoded_text._encodings[0].word_ids   # è·å–å¥å­ä¸­æ¯ä¸ªå•è¯å¯¹åº”çš„id\n",
    "    watermarking_wordset = [None] * len(encoded_text['input_ids'])   # æ°´å°è¯é›†\n",
    "    substituted_idset = []   # è¢«æ›¿æ¢çš„è¯çš„idé›†åˆ\n",
    "    substituted_indices = []   # è¢«æ›¿æ¢çš„ç´¢å¼•\n",
    "    watermarked_text = []   # æ°´å°æ–‡æœ¬\n",
    "    message = []   # åµŒå…¥çš„ä¿¡æ¯\n",
    "\n",
    "    latest_embed_index = -1\n",
    "    index = 1\n",
    "    while index < len(encoded_text['input_ids']) - f:   # ä»ç¬¬äºŒä¸ªè¯å¼€å§‹å¤„ç†ï¼Œç›´åˆ°åˆ’å®šçš„fä¸ºæ­¢\n",
    "        text = tokenizer.decode(encoded_text['input_ids'][index])  # è¿˜åŸå½“å‰å¤„ç†çš„è¯\n",
    "        watermarked_text.append(text)   # \n",
    "\n",
    "        if text in riskset:   # è‹¥è¯¥è¯åœ¨é£é™©é›†ä¸­ï¼Œå°±ä¸å¤„ç†è¯¥è¯\n",
    "            watermarking_wordset[index] = 'riskset'\n",
    "            index = index + 1\n",
    "            continue\n",
    "\n",
    "        valid_indx = [t == index for t in word_ids]\n",
    "        if sum(valid_indx) >= 2:  # skip this word; subword\n",
    "            watermarking_wordset[index] = 'subword'\n",
    "            index = index + 1\n",
    "            continue\n",
    "\n",
    "        local_context = encoded_text['input_ids'][:index + 2]  # è·å–å½“å‰è¯çš„å±€éƒ¨ä¸Šä¸‹æ–‡\n",
    "        is_synch, words = synchronicity_test(index, local_context)   # åšåŒæ­¥æ€§æµ‹è¯•\n",
    "\n",
    "        if not is_synch:  # ä¸å…·å¤‡åŒæ­¥æ€§ï¼Œè·³è¿‡è¯¥è¯\n",
    "            watermarking_wordset[index] = 'syn'\n",
    "            index = index + 1\n",
    "            continue\n",
    "\n",
    "        if index - latest_embed_index != f + 1:   # å½“å‰è¯ä¸ä¸Šä¸€ä¸ªè¯ä¹‹é—´çš„è·ç¦»ä¸è¶…è¿‡fï¼Œåšå¯æ›¿æ¢æ€§æµ‹è¯•\n",
    "            if not substitutability_test(local_context[:-1], index,\n",
    "                                         words):  # skip this word if substitutability test fails\n",
    "                watermarking_wordset[index] = 'sub'    # æ²¡é€šè¿‡å¯æ›¿æ¢æ€§æµ‹è¯•ï¼Œè·³è¿‡è¯¥è¯\n",
    "                index = index + 1\n",
    "                continue\n",
    "\n",
    "        watermarking_wordset[index] = tokenizer.decode(words)\n",
    "        words_decoded = tokenizer.decode(words).split(\" \")\n",
    "        # sort token_id alphabetically\n",
    "        words = [w for w, wd in sorted(zip(words, words_decoded), key=lambda pair: pair[1])]\n",
    "        words_decoded.sort()\n",
    "\n",
    "        # extraction\n",
    "        if extracting:\n",
    "            extracted_msg = words_decoded.index(text)\n",
    "            message.append(extracted_msg)\n",
    "\n",
    "        # embedding\n",
    "        else:\n",
    "            random_msg = random.choice([0,1])\n",
    "            word_chosen_by_msg = words[random_msg]\n",
    "            encoded_text['input_ids'][index] = word_chosen_by_msg\n",
    "            message.append(random_msg)\n",
    "\n",
    "\n",
    "        substituted_idset.append(words)\n",
    "        substituted_indices.append(index)\n",
    "        latest_embed_index = index\n",
    "        index = index + f + 1\n",
    "\n",
    "    return substituted_idset, substituted_indices, watermarking_wordset, encoded_text['input_ids'], message\n",
    "\n",
    "def extract(dataset, name, f=1):\n",
    "    print(f'======{name}æå–æ°´å°======')\n",
    "    z_score_result = []\n",
    "\n",
    "    corrupted_watermarked = []\n",
    "\n",
    "\n",
    "    num_corrupted_sen = 0\n",
    "    sample_level_bit = {'gt':[], 'extracted':[]}\n",
    "    bit_error_agg = {}\n",
    "    midx_match_cnt = 0\n",
    "    mword_match_cnt = 0\n",
    "    infill_match_cnt = 0\n",
    "    prev_c_idx = 0\n",
    "        \n",
    "    for index in tqdm(range(len(dataset))):\n",
    "        text_index = index\n",
    "        sentence_length = len(dataset[index])\n",
    "        bit_error_agg = {\"sentence_err_cnt\": 0, \"sentence_cnt\": 0}\n",
    "        for idx in range(len(dataset[index])):  # å¤„ç†æ¯ä¸ªå¥å­\n",
    "            data = dataset[index][idx]\n",
    "            c_idx = data['c_idx']\n",
    "            sen_idx = data['sen_idx']\n",
    "            sub_idset = data['s_idset_str']\n",
    "            sub_idx = data['s_indices_str']\n",
    "            clean_wm_sen = data['clean_wm_text']\n",
    "            key = data['keys_str']\n",
    "            msg = data['message_str']\n",
    "\n",
    "            if prev_c_idx != c_idx:\n",
    "                error_cnt, cnt = compute_ber(sample_level_bit['extracted'], sample_level_bit['gt'])\n",
    "                bit_error_agg['sample_err_cnt'] = bit_error_agg.get('sample_err_cnt', 0) + error_cnt\n",
    "                bit_error_agg['sample_cnt'] = bit_error_agg.get('sample_cnt', 0) + cnt\n",
    "                sample_level_bit = {'gt': [], 'extracted': []}\n",
    "                prev_c_idx = c_idx\n",
    "\n",
    "            if len(corrupted_watermarked) > 0 and len(corrupted_watermarked) <= idx:\n",
    "                break\n",
    "\n",
    "            clean_encoded = tokenizer(clean_wm_sen.strip(), add_special_tokens=False, truncation=True,\n",
    "                                      max_length=tokenizer.model_max_length// 2 - 2)['input_ids']\n",
    "            \n",
    "            wm_texts = [clean_wm_sen.strip()]\n",
    "            for wm_text in wm_texts:\n",
    "                wm_text = wm_text.strip()\n",
    "\n",
    "                num_corrupted_sen += 1\n",
    "                extracted_idset, extracted_indices, watermarking_wordset, encoded_text, extracted_msg = \\\n",
    "                    main(wm_text, f, extracting=True)\n",
    "                extracted_key = [tokenizer.decode(s_id) for s_id in extracted_idset]\n",
    "\n",
    "                midx_match_flag = set(extracted_indices) == set(sub_idx)\n",
    "                if midx_match_flag:\n",
    "                    midx_match_cnt += 1\n",
    "\n",
    "                mword_match_flag = set([encoded_text[idx] for idx in extracted_indices]) == \\\n",
    "                                   set([clean_encoded[idx] for idx in sub_idx])\n",
    "                if mword_match_flag:\n",
    "                    mword_match_cnt += 1\n",
    "\n",
    "                infill_match_list = []\n",
    "                if len(sub_idset) == len(extracted_idset):\n",
    "                    for a, b in zip(sub_idset, extracted_idset):\n",
    "                        infill_match_flag = a==b\n",
    "                        infill_match_list.append(infill_match_flag)\n",
    "                else:\n",
    "                    infill_match_list.append(False)\n",
    "                if all(infill_match_list):\n",
    "                    infill_match_cnt += 1\n",
    "\n",
    "                sample_level_bit['extracted'].extend(extracted_msg)\n",
    "                sample_level_bit['gt'].extend(msg)\n",
    "                error_cnt, cnt = compute_ber(msg, extracted_msg)\n",
    "                bit_error_agg['sentence_err_cnt'] = bit_error_agg.get('sentence_err_cnt', 0) + error_cnt\n",
    "                bit_error_agg['sentence_cnt'] = bit_error_agg.get('sentence_cnt', 0) + cnt\n",
    "\n",
    "                match_flag = error_cnt == 0\n",
    "\n",
    "        # åšz-test\n",
    "        ones = bit_error_agg['sentence_cnt'] - bit_error_agg['sentence_err_cnt']\n",
    "        n = bit_error_agg['sentence_cnt']\n",
    "        p = 0.5  # Null hypothesis: perfect extraction (BER = 0)\n",
    "        if n == 0:\n",
    "            z_score = 0 \n",
    "        else:\n",
    "            z_score = (ones - p * n) / (n * p * (1 - p)) ** 0.5\n",
    "        z_score_result.append({\n",
    "            'text-index': text_index,\n",
    "            'sentence-length': sentence_length,\n",
    "            'z-score': z_score\n",
    "        })\n",
    "    print(f'=========={name}å®Œæˆ==========')\n",
    "    return z_score_result\n",
    "\n",
    "def embed(file_name, original_file, watermark_file):\n",
    "    f = 1\n",
    "    start_sample_idx = 0\n",
    "\n",
    "    # corpus: æ•°æ®é›†\n",
    "    dataset = json.load(open(file_name))\n",
    "    # print(len(dataset2))\n",
    "    # print(f'==========\\n{dataset2[0]}\\n==========')\n",
    "    # cover_texts: å¾…å¤„ç†çš„æ•°æ®é›†\n",
    "    cover_texts = preprocess2sentence(preprocess_txt(dataset), corpus_name=\"custom\", start_sample_idx=0, cutoff_q=(0.0, 1.0), use_cache=False)\n",
    "\n",
    "    # num_sentence = 0\n",
    "    # for c_idx, sentences in enumerate(cover_texts):\n",
    "    #     num_sentence += len(sentences)\n",
    "    #     if num_sentence >= args.num_sample:\n",
    "    #         break\n",
    "    # cover_texts = cover_texts[:c_idx + 1]\n",
    "\n",
    "    bit_count = 0\n",
    "    word_count = 0\n",
    "\n",
    "    print(f'=========={len(cover_texts)}==========')\n",
    "    # print(cover_texts)\n",
    "\n",
    "    # åµŒå…¥æ°´å°\n",
    "    original_output = []\n",
    "    watermark_output = []\n",
    "    for c_idx, sentences in enumerate(tqdm(cover_texts)):  # setences: æ¯ä¸€æ®µæ–‡æœ¬\n",
    "        original_result = []\n",
    "        watermark_result = []\n",
    "        for sen_idx, sen in enumerate(sentences):    # æ¯ä¸€ä¸ªå¥å­\n",
    "            # print(f'======sen:{sen_idx}======')\n",
    "            sen_text = sen.text    \n",
    "            substituted_idset, substituted_indices, watermarking_wordset, encoded_text, message = main(sen_text, f)\n",
    "            punct_removed = sen_text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "            word_count += len([i for i in punct_removed.split(\" \") if i not in stop])\n",
    "            num_cases = 1\n",
    "            for sid in substituted_idset:\n",
    "                num_cases *= len(sid)\n",
    "            bit_count += math.log2(num_cases)\n",
    "            s_idset_str = \"\"\n",
    "            for s_id in substituted_idset:\n",
    "                s_idset_str += \" \".join(str(x) for x in s_id) + \",\"\n",
    "            s_indices_str = \" \".join(str(x) for x in substituted_indices)\n",
    "            message_str = [str(m) for m in message]\n",
    "            message_str = \" \".join(message_str) if len(message_str) else \"\"\n",
    "            watermarked_text = tokenizer.decode(encoded_text)\n",
    "            keys = [tokenizer.decode(s_id) for s_id in substituted_idset]\n",
    "            keys_str = \", \".join(keys)\n",
    "            # wr.write(f\"{c_idx}\\t{sen_idx}\\t{s_idset_str}\\t{s_indices_str}\\t\"\n",
    "            #              f\"{watermarked_text}\\t{keys_str}\\t{message_str}\\n\")\n",
    "            message_str = get_list(message_str)\n",
    "            s_idset_str = get_list(s_idset_str)\n",
    "            s_indices_str = get_list(s_indices_str)\n",
    "            watermark_result.append({\n",
    "                'c_idx': c_idx,\n",
    "                'sen_idx': sen_idx,\n",
    "                's_idset_str': s_idset_str,\n",
    "                's_indices_str': s_indices_str,\n",
    "                'clean_wm_text': watermarked_text,\n",
    "                'keys_str': keys_str,\n",
    "                'message_str': message_str\n",
    "            })\n",
    "            original_result.append({\n",
    "                'c_idx': c_idx,\n",
    "                'sen_idx': sen_idx,\n",
    "                's_idset_str': s_idset_str,\n",
    "                's_indices_str': s_indices_str,\n",
    "                'clean_wm_text': sen.text,\n",
    "                'keys_str': keys_str,\n",
    "                'message_str': message_str\n",
    "            })\n",
    "        original_output.append(original_result)\n",
    "        watermark_output.append(watermark_result)\n",
    "\n",
    "    # print(original_output[0])\n",
    "\n",
    "    os.makedirs(os.path.dirname(original_file), exist_ok=True)\n",
    "    with open(original_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(original_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    os.makedirs(os.path.dirname(watermark_file), exist_ok=True)\n",
    "    with open(watermark_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(watermark_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return original_output, watermark_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bloomz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========800==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 517/800 [8:07:22<5:58:00, 75.90s/it]  "
     ]
    }
   ],
   "source": [
    "file_name_bloomz = f'data/M4/arxiv/abstract/dataset-M4-random-bloomz-800.json'\n",
    "original_file_bloomz = f'result/ContextLS/M4/original/bloomz-random-800.json'\n",
    "watermark_file_bloomz = f'result/ContextLS/M4/watermark/bloomz-random-800.json'\n",
    "z_score_file_bloomz = f'result/ContextLS/M4/z_score/bloomz-random-800.json'\n",
    "\n",
    "original_output_bloomz, watermark_output_bloomz = embed(file_name_bloomz, original_file_bloomz, watermark_file_bloomz)\n",
    "\n",
    "ori_fast_z_score_bloomz = extract(original_output_bloomz, 'åŸå§‹æ–‡æœ¬')\n",
    "water_fast_z_score_bloomz = extract(watermark_output_bloomz, 'æ°´å°æ–‡æœ¬')\n",
    "\n",
    "z_score_output_bloomz = []\n",
    "\n",
    "for index in range(len(ori_fast_z_score_bloomz)):\n",
    "    z_score_output_bloomz.append({\n",
    "        'text-index': ori_fast_z_score_bloomz[index]['text-index'],\n",
    "        'sentence-length': ori_fast_z_score_bloomz[index]['sentence-length'],\n",
    "        'ori-fast-z-score': ori_fast_z_score_bloomz[index]['z-score'],\n",
    "        'water-fast-z-score': water_fast_z_score_bloomz[index]['z-score']\n",
    "\n",
    "    })\n",
    "\n",
    "os.makedirs(os.path.dirname(z_score_file_bloomz), exist_ok=True)\n",
    "with open(z_score_file_bloomz, 'w', encoding='utf-8') as f:\n",
    "    json.dump(z_score_output_bloomz, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_chatGPT = f'data/M4/arxiv/abstract/dataset-M4-random-chatGPT-800.json'\n",
    "original_file_chatGPT = f'result/ContextLS/M4/original/chatGPT-random-800.json'\n",
    "watermark_file_chatGPT = f'result/ContextLS/M4/watermark/chatGPT-random-800.json'\n",
    "z_score_file_chatGPT = f'result/ContextLS/M4/z_score/chatGPT-random-800.json'\n",
    "\n",
    "original_output_chatGPT, watermark_output_chatGPT = embed(file_name_chatGPT, original_file_chatGPT, watermark_file_chatGPT)\n",
    "\n",
    "ori_fast_z_score_chatGPT = extract(original_output_chatGPT, 'åŸå§‹æ–‡æœ¬')\n",
    "water_fast_z_score_chatGPT = extract(watermark_output_chatGPT, 'æ°´å°æ–‡æœ¬')\n",
    "\n",
    "z_score_output_chatGPT = []\n",
    "\n",
    "for index in range(len(ori_fast_z_score_chatGPT)):\n",
    "    z_score_output_chatGPT.append({\n",
    "        'text-index': ori_fast_z_score_chatGPT[index]['text-index'],\n",
    "        'sentence-length': ori_fast_z_score_chatGPT[index]['sentence-length'],\n",
    "        'ori-fast-z-score': ori_fast_z_score_chatGPT[index]['z-score'],\n",
    "        'water-fast-z-score': water_fast_z_score_chatGPT[index]['z-score']\n",
    "\n",
    "    })\n",
    "\n",
    "os.makedirs(os.path.dirname(z_score_file_chatGPT), exist_ok=True)\n",
    "with open(z_score_file_chatGPT, 'w', encoding='utf-8') as f:\n",
    "    json.dump(z_score_output_chatGPT, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = ['bloomz', 'chatGPT', 'cohere', 'davinci', 'dolly', 'flant5']\n",
    "\n",
    "file_name_cohere = f'data/M4/arxiv/abstract/dataset-M4-random-cohere-800.json'\n",
    "original_file_cohere = f'result/ContextLS/M4/original/cohere-random-800.json'\n",
    "watermark_file_cohere = f'result/ContextLS/M4/watermark/cohere-random-800.json'\n",
    "z_score_file_cohere = f'result/ContextLS/M4/z_score/cohere-random-800.json'\n",
    "\n",
    "original_output_cohere, watermark_output_cohere = embed(file_name_cohere, original_file_cohere, watermark_file_cohere)\n",
    "\n",
    "ori_fast_z_score_cohere = extract(original_output_cohere, 'åŸå§‹æ–‡æœ¬')\n",
    "water_fast_z_score_cohere = extract(watermark_output_cohere, 'æ°´å°æ–‡æœ¬')\n",
    "\n",
    "z_score_output_cohere = []\n",
    "\n",
    "for index in range(len(ori_fast_z_score_cohere)):\n",
    "    z_score_output_cohere.append({\n",
    "        'text-index': ori_fast_z_score_cohere[index]['text-index'],\n",
    "        'sentence-length': ori_fast_z_score_cohere[index]['sentence-length'],\n",
    "        'ori-fast-z-score': ori_fast_z_score_cohere[index]['z-score'],\n",
    "        'water-fast-z-score': water_fast_z_score_cohere[index]['z-score']\n",
    "\n",
    "    })\n",
    "\n",
    "os.makedirs(os.path.dirname(z_score_file_cohere), exist_ok=True)\n",
    "with open(z_score_file_cohere, 'w', encoding='utf-8') as f:\n",
    "    json.dump(z_score_output_cohere, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
