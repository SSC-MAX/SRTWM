{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/SRTWM/nlp-watermarking-main/models/watermark.py:35: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  if torch.has_mps:\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/bert-large-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-01-08 23:22:33 - MainThread - INFO - Using component: [grammar]\n",
      "2025-01-08 23:22:33 - MainThread - INFO - Using component: [grammar]\n",
      "2025-01-08 23:22:33 - MainThread - INFO - Using component: [grammar]\n",
      "2025-01-08 23:22:33 - MainThread - INFO - and ordering by [dep]\n",
      "2025-01-08 23:22:33 - MainThread - INFO - and ordering by [dep]\n",
      "2025-01-08 23:22:33 - MainThread - INFO - and ordering by [dep]\n",
      "2025-01-08 23:22:33 - MainThread - INFO - Namespace(debug_mode=False, eval_only=False, do_watermark=True, train_infill=False, model_ckpt=None, model_name='/root/autodl-tmp/bert-large-cased', exp_name='', num_epochs=10, dtype=None, spacy_model='en_core_web_sm', keyword_ratio=0.05, topk=2, mask_select_method='grammar', mask_order_by='dep', keyword_mask='adjacent', exclude_cc=True, custom_keywords=['watermarking', 'watermark'])\n",
      "2025-01-08 23:22:33 - MainThread - INFO - Namespace(debug_mode=False, eval_only=False, do_watermark=True, train_infill=False, model_ckpt=None, model_name='/root/autodl-tmp/bert-large-cased', exp_name='', num_epochs=10, dtype=None, spacy_model='en_core_web_sm', keyword_ratio=0.05, topk=2, mask_select_method='grammar', mask_order_by='dep', keyword_mask='adjacent', exclude_cc=True, custom_keywords=['watermarking', 'watermark'])\n",
      "2025-01-08 23:22:33 - MainThread - INFO - Namespace(debug_mode=False, eval_only=False, do_watermark=True, train_infill=False, model_ckpt=None, model_name='/root/autodl-tmp/bert-large-cased', exp_name='', num_epochs=10, dtype=None, spacy_model='en_core_web_sm', keyword_ratio=0.05, topk=2, mask_select_method='grammar', mask_order_by='dep', keyword_mask='adjacent', exclude_cc=True, custom_keywords=['watermarking', 'watermark'])\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from itertools import product\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from config import WatermarkArgs, GenericArgs, stop\n",
    "from models.watermark import InfillModel\n",
    "from utils.dataset_utils import get_result_txt\n",
    "from utils.logging import getLogger\n",
    "from utils.metric import Metric\n",
    "from utils.misc import compute_ber\n",
    "from utils.dataset_utils import preprocess2sentence, preprocess_txt\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "random.seed(1230)\n",
    "\n",
    "infill_parser = WatermarkArgs()\n",
    "generic_parser = GenericArgs()\n",
    "infill_args, _ = infill_parser.parse_known_args()\n",
    "generic_args, _ = generic_parser.parse_known_args()\n",
    "infill_args.mask_select_method = \"grammar\"\n",
    "infill_args.mask_order_by = \"dep\"\n",
    "infill_args.exclude_cc = True\n",
    "infill_args.topk = 2\n",
    "infill_args.dtype = None\n",
    "infill_args.model_name = '/root/autodl-tmp/bert-large-cased'\n",
    "infill_args.custom_keywords = [\"watermarking\", \"watermark\"]\n",
    "DEBUG_MODE = generic_args.debug_mode   # => False\n",
    "dtype = generic_args.dtype   # => imdb\n",
    "\n",
    "dirname = f\"./results/ours/{dtype}/{generic_args.exp_name}\"   # => .results/ours/imdb/tmp\n",
    "start_sample_idx = 0\n",
    "num_sample = generic_args.num_sample   # => 100\n",
    "\n",
    "spacy_tokenizer = spacy.load(generic_args.spacy_model)\n",
    "if \"trf\" in generic_args.spacy_model:\n",
    "    spacy.require_gpu()\n",
    "model = InfillModel(infill_args, dirname=dirname)\n",
    "\n",
    "bit_count = 0\n",
    "word_count = 0\n",
    "upper_bound = 0\n",
    "candidate_kwd_cnt = 0\n",
    "kwd_match_cnt = 0\n",
    "mask_match_cnt = 0\n",
    "sample_cnt = 0\n",
    "one_cnt = 0\n",
    "zero_cnt = 0\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "metric = Metric(device, **vars(generic_args))\n",
    "\n",
    "if not os.path.exists(dirname):\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "def get_list(string):\n",
    "    output = []\n",
    "    str_list = list(string.replace(\" \", \"\"))\n",
    "    for i in str_list:\n",
    "        output.append(int(i))\n",
    "    return output\n",
    "\n",
    "def extract(dataset, prompt):\n",
    "    #提取水印模式\n",
    "    print(f'==={prompt}提取水印===')\n",
    "    z_score_result = []\n",
    "    start_sample_idx = 0\n",
    "\n",
    "    result_dir = os.path.join(dirname, \"watermarked.txt\")\n",
    "\n",
    "    num_corrupted_sen = 0\n",
    "    \n",
    "    infill_match_cnt = 0\n",
    "\n",
    "    num_mask_match_cnt = 0\n",
    "    zero_cnt = 0\n",
    "    one_cnt = 0\n",
    "    kwd_match_cnt = 0\n",
    "    midx_match_cnt = 0\n",
    "    mword_match_cnt = 0\n",
    "\n",
    "    for index in tqdm(range(len(dataset))):  # 一段文本\n",
    "        text_index = index  # 文本的索引\n",
    "        sentence_length = len(dataset[index]) # 文本中句子的数目\n",
    "        bit_error_agg = {\"sentence_err_cnt\": 0, \"sentence_cnt\": 0}\n",
    "        # 处理每个句子\n",
    "        for data in dataset[index]:\n",
    "            c_idx = data['c_idx']\n",
    "            sen_idx = data['sen_idx']\n",
    "            sub_idset = data['sub_idset']\n",
    "            sub_idx = data['sub_idx']\n",
    "            clean_wm_text = data['clean_wm_text']\n",
    "            key = data['key']\n",
    "            msg = data['msg']\n",
    "            # print(f'==========\\nc_idx:{c_idx}\\nsen_idx:{sen_idx}\\nsub_idset:{sub_idset}\\nsub_idx:{sub_idx}\\nclean_wm_text:{clean_wm_text}\\ntext_type:{type(clean_wm_text)}\\nkey:{key}\\nmsg:{msg}\\n==========')\n",
    "            wm_texts = [clean_wm_text.strip()]\n",
    "            for wm_text in wm_texts:\n",
    "                num_corrupted_sen += 1\n",
    "                sen = spacy_tokenizer(wm_text.strip())\n",
    "                all_keywords, entity_keywords = model.keyword_module.extract_keyword([sen])\n",
    "                # for sanity check, we use the uncorrupted watermarked texts\n",
    "                sen_ = spacy_tokenizer(clean_wm_text.strip())\n",
    "                all_keywords_, entity_keywords_ = model.keyword_module.extract_keyword([sen_])\n",
    "\n",
    "                # extracting states for corrupted\n",
    "                keyword = all_keywords[0]\n",
    "                ent_keyword = entity_keywords[0]\n",
    "                agg_cwi, agg_probs, tokenized_pt, (mask_idx_pt, mask_idx, mask_word) = model.run_iter(sen, keyword, ent_keyword,\n",
    "                                                                                                  train_flag=False, embed_flag=True)\n",
    "                wm_keys = model.tokenizer(\" \".join([t.text for t in mask_word]), add_special_tokens=False)['input_ids']\n",
    "\n",
    "                # extracting states for uncorrupted\n",
    "                keyword_ = all_keywords_[0]\n",
    "                ent_keyword_ = entity_keywords_[0]\n",
    "                agg_cwi_, agg_probs_, tokenized_pt_, (mask_idx_pt_, mask_idx_, mask_word_) = model.run_iter(sen_, keyword_, ent_keyword_,\n",
    "                                                                                                        train_flag=False, embed_flag=True)\n",
    "                kwd_match_flag = set([x.text for x in keyword]) == set([x.text for x in keyword_])\n",
    "                if kwd_match_flag:\n",
    "                    kwd_match_cnt += 1\n",
    "\n",
    "                midx_match_flag = set(mask_idx) == set(mask_idx_)\n",
    "                if midx_match_flag:\n",
    "                    midx_match_cnt += 1\n",
    "\n",
    "                mword_match_flag = set([m.text for m in mask_word]) == set([m.text for m in mask_word_])\n",
    "                if mword_match_flag:\n",
    "                    mword_match_cnt += 1\n",
    "\n",
    "                num_mask_match_flag = len(mask_idx) == len(mask_idx_)\n",
    "                if num_mask_match_flag:\n",
    "                    num_mask_match_cnt += 1\n",
    "\n",
    "                infill_match_list = []\n",
    "                if len(agg_cwi) == len(agg_cwi_):\n",
    "                    for a, b in zip(agg_cwi, agg_cwi_):\n",
    "                        if len(a) == len(b):\n",
    "                            infill_match_flag = (a == b).all()\n",
    "                        else:\n",
    "                            infill_match_flag = False\n",
    "                        infill_match_list.append(infill_match_flag)\n",
    "                else:\n",
    "                    infill_match_list.append(False)\n",
    "                if all(infill_match_list):\n",
    "                    infill_match_cnt += 1\n",
    "\n",
    "                valid_watermarks = []\n",
    "                valid_keys = []\n",
    "                tokenized_text = [token.text_with_ws for token in sen]\n",
    "\n",
    "                if len(agg_cwi) > 0:\n",
    "                    for cwi in product(*agg_cwi):\n",
    "                        wm_text = tokenized_text.copy()\n",
    "                        for m_idx, c_id in zip(mask_idx, cwi):\n",
    "                            wm_text[m_idx] = re.sub(r\"\\S+\", model.tokenizer.decode(c_id), wm_text[m_idx])\n",
    "\n",
    "                        wm_tokenized = spacy_tokenizer(\"\".join(wm_text).strip())\n",
    "                        # extract keyword of watermark\n",
    "                        wm_keywords, wm_ent_keywords = model.keyword_module.extract_keyword([wm_tokenized])\n",
    "                        wm_kwd = wm_keywords[0]\n",
    "                        wm_ent_kwd = wm_ent_keywords[0]\n",
    "                        wm_mask_idx, wm_mask = model.mask_selector.return_mask(wm_tokenized, wm_kwd, wm_ent_kwd)\n",
    "\n",
    "                        # checking whether the watermark can be embedded\n",
    "                        mask_match_flag = len(wm_mask) > 0 and set(wm_mask_idx) == set(mask_idx)\n",
    "                        if mask_match_flag:\n",
    "                            valid_watermarks.append(wm_tokenized.text)\n",
    "                            valid_keys.append(torch.stack(cwi).tolist())\n",
    "\n",
    "                extracted_msg = []\n",
    "                if len(valid_keys) > 1:\n",
    "                    try:\n",
    "                        extracted_msg_decimal = valid_keys.index(wm_keys)\n",
    "                    except:\n",
    "                        similarity = [len(set(wm_keys).intersection(x)) for x in valid_keys]\n",
    "                        similar_key = max(zip(valid_keys, similarity), key=lambda x: x[1])[0]\n",
    "                        extracted_msg_decimal = valid_keys.index(similar_key)\n",
    "\n",
    "                    num_digit = math.ceil(math.log2(len(valid_keys)))\n",
    "                    extracted_msg = format(extracted_msg_decimal, f\"0{num_digit}b\")\n",
    "                    extracted_msg = list(map(int, extracted_msg))\n",
    "\n",
    "                one_cnt += sum(msg)\n",
    "                zero_cnt += len(msg) - sum(msg)\n",
    "\n",
    "                error_cnt, cnt = compute_ber(msg, extracted_msg)\n",
    "                bit_error_agg['sentence_err_cnt'] = bit_error_agg.get('sentence_err_cnt', 0) + error_cnt\n",
    "                bit_error_agg['sentence_cnt'] = bit_error_agg.get('sentence_cnt', 0) + cnt\n",
    "\n",
    "        # 做z-test\n",
    "        # ber = bit_error_agg['sentence_err_cnt'] / bit_error_agg['sentence_cnt']\n",
    "        ones = bit_error_agg['sentence_cnt'] - bit_error_agg['sentence_err_cnt']\n",
    "        n = bit_error_agg['sentence_cnt']\n",
    "        p = 0.5  # Null hypothesis: perfect extraction (BER = 0)\n",
    "        # se = math.sqrt((ber * (1 - ber)) / n)\n",
    "        if n == 0:\n",
    "            z_score = 0 \n",
    "        else:\n",
    "            z_score = (ones - p * n) / (n * p * (1 - p)) ** 0.5\n",
    "        # print(f\"==========\\nSentence BER: {bit_error_agg['sentence_err_cnt']}/{bit_error_agg['sentence_cnt']}=\"f\"{ber}\\n==========\")\n",
    "        z_score_result.append({\n",
    "            'text-index': text_index,\n",
    "            'sentence-length': sentence_length,\n",
    "            'z-score': z_score\n",
    "        })\n",
    "        # print(f'======\\n{z_score_result}\\n======')\n",
    "    return z_score_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bloomz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===嵌入水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50db6f0ec53b49de96738a806e3c3335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name_bloomz = 'M4/arxiv/abstract/dataset-M4-random-bloomz-800.json'\n",
    "watermark_result_bloomz = 'result/nlp/M4/watermark/dataset-M4-random-bloomz-800.json'\n",
    "original_result_bloomz = 'result/nlp/M4/original/dataset-M4-random-bloomz-800.json'\n",
    "result_file_bloomz = 'result/nlp/M4/z_score/dataset-M4-random-bloomz-800.json'\n",
    "try:   # 嵌入水印模式\n",
    "    \n",
    "\n",
    "    dataset = json.load(open(file_name_bloomz))\n",
    "\n",
    "    cover_texts = preprocess2sentence(preprocess_txt(dataset), corpus_name=\"custom\", start_sample_idx=0, cutoff_q=(0.0, 1.0), use_cache=False)\n",
    "\n",
    "    print('===嵌入水印===')\n",
    "\n",
    "    watermark_text_output = []\n",
    "    original_text_output = []\n",
    "    \n",
    "\n",
    "    # 创建结果文件\n",
    "    result_dir = os.path.join(dirname, \"watermarked.txt\")\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(os.path.dirname(result_dir), exist_ok=True)\n",
    "\n",
    "    progress_bar = tqdm(range(len(cover_texts)))\n",
    "    wr = open(result_dir, \"w\")\n",
    "    for c_idx, sentences in enumerate(cover_texts):\n",
    "        text_result = []    # 每段文本的结果，存储每段文本中每个句子添加水印后的版本与嵌入的比特信息\n",
    "        original_text_result = []\n",
    "        for s_idx, sen in enumerate(sentences):\n",
    "            sen = spacy_tokenizer(sen.text.strip())     # 将句子划分成单词\n",
    "            # all_keywords: 全体关键词； entity_keyword: 实体关键词\n",
    "            all_keywords, entity_keywords = model.keyword_module.extract_keyword([sen])   # 提取关键词\n",
    "            keyword = all_keywords[0]\n",
    "            ent_keyword = entity_keywords[0]\n",
    "            agg_cwi, agg_probs, tokenized_pt, (mask_idx_pt, mask_idx, mask_word) = model.run_iter(sen, keyword, ent_keyword,\n",
    "                                                                                                  train_flag=False, embed_flag=True)\n",
    "            # check if keyword & mask_indices matches\n",
    "            valid_watermarks = []\n",
    "            candidate_kwd_cnt = 0\n",
    "            tokenized_text = [token.text_with_ws for token in sen]\n",
    "\n",
    "            if len(agg_cwi) > 0:\n",
    "                for cwi in product(*agg_cwi):\n",
    "                    wm_text = tokenized_text.copy()\n",
    "                    for m_idx, c_id in zip(mask_idx, cwi):\n",
    "                        wm_text[m_idx] = re.sub(r\"\\S+\", model.tokenizer.decode(c_id), wm_text[m_idx])\n",
    "\n",
    "                    wm_tokenized = spacy_tokenizer(\"\".join(wm_text).strip())\n",
    "\n",
    "                    # extract keyword of watermark\n",
    "                    wm_keywords, wm_ent_keywords = model.keyword_module.extract_keyword([wm_tokenized])\n",
    "                    wm_kwd = wm_keywords[0]\n",
    "                    wm_ent_kwd = wm_ent_keywords[0]\n",
    "                    wm_mask_idx, wm_mask = model.mask_selector.return_mask(wm_tokenized, wm_kwd, wm_ent_kwd)\n",
    "\n",
    "                    kwd_match_flag = set([x.text for x in wm_kwd]) == set([x.text for x in keyword])\n",
    "                    if kwd_match_flag:\n",
    "                        kwd_match_cnt += 1\n",
    "\n",
    "                    # checking whether the watermark can be embedded without the assumption of corruption\n",
    "                    mask_match_flag = len(wm_mask) and set(wm_mask_idx) == set(mask_idx)\n",
    "                    if mask_match_flag:\n",
    "                        valid_watermarks.append(wm_tokenized.text)\n",
    "                        mask_match_cnt += 1\n",
    "\n",
    "                    sample_cnt += 1\n",
    "                    candidate_kwd_cnt += 1\n",
    "    \n",
    "            punct_removed = sen.text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "            word_count += len([i for i in punct_removed.split(\" \") if i not in stop])\n",
    "            if len(valid_watermarks) > 1:\n",
    "                bit_count += math.log2(len(valid_watermarks))\n",
    "                random_msg_decimal = random.choice(range(len(valid_watermarks)))\n",
    "                num_digit = math.ceil(math.log2(len(valid_watermarks)))\n",
    "                random_msg_binary = format(random_msg_decimal, f\"0{num_digit}b\")\n",
    "\n",
    "                wm_text = valid_watermarks[random_msg_decimal]\n",
    "                watermarked_text = \"\".join(wm_text) if len(mask_idx) > 0 else \"\"\n",
    "                message_str = list(random_msg_binary)\n",
    "                one_cnt += len([i for i in message_str if i ==\"1\"])\n",
    "                zero_cnt += len([i for i in message_str if i ==\"0\"])\n",
    "\n",
    "                keys = []\n",
    "                wm_tokenized = spacy_tokenizer(wm_text)\n",
    "                for m_idx in mask_idx:\n",
    "                    keys.append(wm_tokenized[m_idx].text)\n",
    "                keys_str = \", \".join(keys)\n",
    "                message_str = ' '.join(message_str) if len(message_str) else \"\"\n",
    "                # ''.join(wm_text): 水印文本\n",
    "                # keys_str: 替换的词\n",
    "                # message_str: 嵌入的比特信息\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{''.join(wm_text)}\\t{keys_str}\\t{message_str}\\n\")  \n",
    "                watermarksss_text = ''.join(wm_text)\n",
    "                keys = []\n",
    "                keys.append(keys_str)\n",
    "                msg = get_list(message_str)\n",
    "                # msg.append(int(message_str))\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':watermarksss_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "            else:\n",
    "                # 无可添加水印，直接将原始文本作为水印\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{original_text}\\t \\t \\n\")\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text': original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "\n",
    "            if candidate_kwd_cnt > 0:\n",
    "                upper_bound += math.log2(candidate_kwd_cnt)\n",
    "        progress_bar.update(1)\n",
    "        watermark_text_output.append(text_result)\n",
    "        original_text_output.append(original_text_result)\n",
    "\n",
    "    # wr.close()\n",
    "\n",
    "    # ss_scores = []\n",
    "    # for model_name in [\"roberta\", \"all-MiniLM-L6-v2\"]:\n",
    "    #     ss_score, ss_dist = metric.compute_ss(result_dir, model_name, cover_texts)\n",
    "    #     ss_scores.append(sum(ss_score) / len(ss_score))\n",
    "    # nli_score, _, _ = metric.compute_nli(result_dir, cover_texts)\n",
    "\n",
    "    # result_dir = os.path.join(dirname, \"embed-metrics.txt\")\n",
    "    # with open(result_dir, \"a\") as wr:\n",
    "    #     wr.write(str(vars(infill_args))+\"\\n\")\n",
    "    #     wr.write(f\"num.sample={num_sample}\\t bpw={bit_count / word_count}\\t \"\n",
    "    #              f\"ss={ss_scores[0]}\\t ss={ss_scores[1]}\\t\"\n",
    "    #              f\"nli={sum(nli_score) / len(nli_score)}\\n\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(watermark_result_bloomz), exist_ok=True)\n",
    "    with open(watermark_result_bloomz, 'w', encoding='utf-8') as f:\n",
    "        json.dump(watermark_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    os.makedirs(os.path.dirname(original_result_bloomz), exist_ok=True)\n",
    "    with open(original_result_bloomz, 'w', encoding='utf-8') as f:\n",
    "        json.dump(original_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===原始文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab504e4b78e4c87abbc032268c1ccf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===水印文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89185494e1d44efdb3dbf92ab72ef131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===bloomz完成===\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    original_text_output = json.load(open(original_result_bloomz))\n",
    "    watermark_text_output = json.load(open(watermark_result_bloomz))\n",
    "\n",
    "    ori_fast_z_score = extract(original_text_output, '原始文本')\n",
    "    water_fast_z_score = extract(watermark_text_output, '水印文本')\n",
    "\n",
    "    output_result = []\n",
    "\n",
    "    for index in range(len(ori_fast_z_score)):\n",
    "        output_result.append({\n",
    "            'text-index': ori_fast_z_score[index]['text-index'],\n",
    "            'sentence-length': ori_fast_z_score[index]['sentence-length'],\n",
    "            'ori-fast-z-score': ori_fast_z_score[index]['z-score'],\n",
    "            'water-fast-z-score': water_fast_z_score[index]['z-score']\n",
    "\n",
    "        })\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_file_bloomz), exist_ok=True)\n",
    "    with open(result_file_bloomz, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_result, f, indent=4, ensure_ascii=False)\n",
    "    print('===bloomz完成===')\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===嵌入水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd95613b6ff4587a6f0ed012b9a01c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name_chatGPT = 'M4/arxiv/abstract/dataset-M4-random-chatGPT-800.json'\n",
    "watermark_result_chatGPT = 'result/nlp/M4/watermark/dataset-M4-random-chatGPT-800.json'\n",
    "original_result_chatGPT = 'result/nlp/M4/original/dataset-M4-random-chatGPT-800.json'\n",
    "result_file_chatGPT = 'result/nlp/M4/z_score/dataset-M4-random-chatGPT-800.json'\n",
    "\n",
    "try:   # 嵌入水印模式\n",
    "    dataset = json.load(open(file_name_chatGPT))\n",
    "\n",
    "    cover_texts = preprocess2sentence(preprocess_txt(dataset), corpus_name=\"custom\", start_sample_idx=0, cutoff_q=(0.0, 1.0), use_cache=False)\n",
    "\n",
    "    print('===嵌入水印===')\n",
    "\n",
    "    watermark_text_output = []\n",
    "    original_text_output = []\n",
    "    \n",
    "\n",
    "    # 创建结果文件\n",
    "    result_dir = os.path.join(dirname, \"watermarked.txt\")\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(os.path.dirname(result_dir), exist_ok=True)\n",
    "\n",
    "    progress_bar = tqdm(range(len(cover_texts)))\n",
    "    wr = open(result_dir, \"w\")\n",
    "    for c_idx, sentences in enumerate(cover_texts):\n",
    "        text_result = []    # 每段文本的结果，存储每段文本中每个句子添加水印后的版本与嵌入的比特信息\n",
    "        original_text_result = []\n",
    "        for s_idx, sen in enumerate(sentences):\n",
    "            sen = spacy_tokenizer(sen.text.strip())     # 将句子划分成单词\n",
    "            # all_keywords: 全体关键词； entity_keyword: 实体关键词\n",
    "            all_keywords, entity_keywords = model.keyword_module.extract_keyword([sen])   # 提取关键词\n",
    "            keyword = all_keywords[0]\n",
    "            ent_keyword = entity_keywords[0]\n",
    "            agg_cwi, agg_probs, tokenized_pt, (mask_idx_pt, mask_idx, mask_word) = model.run_iter(sen, keyword, ent_keyword,\n",
    "                                                                                                  train_flag=False, embed_flag=True)\n",
    "            # check if keyword & mask_indices matches\n",
    "            valid_watermarks = []\n",
    "            candidate_kwd_cnt = 0\n",
    "            tokenized_text = [token.text_with_ws for token in sen]\n",
    "\n",
    "            if len(agg_cwi) > 0:\n",
    "                for cwi in product(*agg_cwi):\n",
    "                    wm_text = tokenized_text.copy()\n",
    "                    for m_idx, c_id in zip(mask_idx, cwi):\n",
    "                        wm_text[m_idx] = re.sub(r\"\\S+\", model.tokenizer.decode(c_id), wm_text[m_idx])\n",
    "\n",
    "                    wm_tokenized = spacy_tokenizer(\"\".join(wm_text).strip())\n",
    "\n",
    "                    # extract keyword of watermark\n",
    "                    wm_keywords, wm_ent_keywords = model.keyword_module.extract_keyword([wm_tokenized])\n",
    "                    wm_kwd = wm_keywords[0]\n",
    "                    wm_ent_kwd = wm_ent_keywords[0]\n",
    "                    wm_mask_idx, wm_mask = model.mask_selector.return_mask(wm_tokenized, wm_kwd, wm_ent_kwd)\n",
    "\n",
    "                    kwd_match_flag = set([x.text for x in wm_kwd]) == set([x.text for x in keyword])\n",
    "                    if kwd_match_flag:\n",
    "                        kwd_match_cnt += 1\n",
    "\n",
    "                    # checking whether the watermark can be embedded without the assumption of corruption\n",
    "                    mask_match_flag = len(wm_mask) and set(wm_mask_idx) == set(mask_idx)\n",
    "                    if mask_match_flag:\n",
    "                        valid_watermarks.append(wm_tokenized.text)\n",
    "                        mask_match_cnt += 1\n",
    "\n",
    "                    sample_cnt += 1\n",
    "                    candidate_kwd_cnt += 1\n",
    "    \n",
    "            punct_removed = sen.text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "            word_count += len([i for i in punct_removed.split(\" \") if i not in stop])\n",
    "            if len(valid_watermarks) > 1:\n",
    "                bit_count += math.log2(len(valid_watermarks))\n",
    "                random_msg_decimal = random.choice(range(len(valid_watermarks)))\n",
    "                num_digit = math.ceil(math.log2(len(valid_watermarks)))\n",
    "                random_msg_binary = format(random_msg_decimal, f\"0{num_digit}b\")\n",
    "\n",
    "                wm_text = valid_watermarks[random_msg_decimal]\n",
    "                watermarked_text = \"\".join(wm_text) if len(mask_idx) > 0 else \"\"\n",
    "                message_str = list(random_msg_binary)\n",
    "                one_cnt += len([i for i in message_str if i ==\"1\"])\n",
    "                zero_cnt += len([i for i in message_str if i ==\"0\"])\n",
    "\n",
    "                keys = []\n",
    "                wm_tokenized = spacy_tokenizer(wm_text)\n",
    "                for m_idx in mask_idx:\n",
    "                    keys.append(wm_tokenized[m_idx].text)\n",
    "                keys_str = \", \".join(keys)\n",
    "                message_str = ' '.join(message_str) if len(message_str) else \"\"\n",
    "                # ''.join(wm_text): 水印文本\n",
    "                # keys_str: 替换的词\n",
    "                # message_str: 嵌入的比特信息\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{''.join(wm_text)}\\t{keys_str}\\t{message_str}\\n\")  \n",
    "                watermarksss_text = ''.join(wm_text)\n",
    "                keys = []\n",
    "                keys.append(keys_str)\n",
    "                msg = get_list(message_str)\n",
    "                # msg.append(int(message_str))\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':watermarksss_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "            else:\n",
    "                # 无可添加水印，直接将原始文本作为水印\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{original_text}\\t \\t \\n\")\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text': original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "\n",
    "            if candidate_kwd_cnt > 0:\n",
    "                upper_bound += math.log2(candidate_kwd_cnt)\n",
    "        progress_bar.update(1)\n",
    "        watermark_text_output.append(text_result)\n",
    "        original_text_output.append(original_text_result)\n",
    "\n",
    "    # wr.close()\n",
    "\n",
    "    # ss_scores = []\n",
    "    # for model_name in [\"roberta\", \"all-MiniLM-L6-v2\"]:\n",
    "    #     ss_score, ss_dist = metric.compute_ss(result_dir, model_name, cover_texts)\n",
    "    #     ss_scores.append(sum(ss_score) / len(ss_score))\n",
    "    # nli_score, _, _ = metric.compute_nli(result_dir, cover_texts)\n",
    "\n",
    "    # result_dir = os.path.join(dirname, \"embed-metrics.txt\")\n",
    "    # with open(result_dir, \"a\") as wr:\n",
    "    #     wr.write(str(vars(infill_args))+\"\\n\")\n",
    "    #     wr.write(f\"num.sample={num_sample}\\t bpw={bit_count / word_count}\\t \"\n",
    "    #              f\"ss={ss_scores[0]}\\t ss={ss_scores[1]}\\t\"\n",
    "    #              f\"nli={sum(nli_score) / len(nli_score)}\\n\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(watermark_result_chatGPT), exist_ok=True)\n",
    "    with open(watermark_result_chatGPT, 'w', encoding='utf-8') as f:\n",
    "        json.dump(watermark_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    os.makedirs(os.path.dirname(original_result_chatGPT), exist_ok=True)\n",
    "    with open(original_result_chatGPT, 'w', encoding='utf-8') as f:\n",
    "        json.dump(original_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===原始文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c1a942fabd4f439ed609e8b280dccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===水印文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ef5468d74f47c9aac8b49e8125f539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===chatGPT完成===\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    original_text_output = json.load(open(original_result_chatGPT))\n",
    "    watermark_text_output = json.load(open(watermark_result_chatGPT))\n",
    "\n",
    "    ori_fast_z_score = extract(original_text_output, '原始文本')\n",
    "    water_fast_z_score = extract(watermark_text_output, '水印文本')\n",
    "\n",
    "    output_result = []\n",
    "\n",
    "    for index in range(len(ori_fast_z_score)):\n",
    "        output_result.append({\n",
    "            'text-index': ori_fast_z_score[index]['text-index'],\n",
    "            'sentence-length': ori_fast_z_score[index]['sentence-length'],\n",
    "            'ori-fast-z-score': ori_fast_z_score[index]['z-score'],\n",
    "            'water-fast-z-score': water_fast_z_score[index]['z-score']\n",
    "\n",
    "        })\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_file_chatGPT), exist_ok=True)\n",
    "    with open(result_file_chatGPT, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_result, f, indent=4, ensure_ascii=False)\n",
    "    print('===chatGPT完成===')\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===嵌入水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12e032a1811417094a370b8e0b876b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name_cohere = 'M4/arxiv/abstract/dataset-M4-random-cohere-800.json'\n",
    "watermark_result_cohere = 'result/nlp/M4/watermark/dataset-M4-random-cohere-800.json'\n",
    "original_result_cohere = 'result/nlp/M4/original/dataset-M4-random-cohere-800.json'\n",
    "result_file_cohere = 'result/nlp/M4/z_score/dataset-M4-random-cohere-800.json'\n",
    "\n",
    "try:   # 嵌入水印模式\n",
    "    dataset = json.load(open(file_name_cohere))\n",
    "\n",
    "    cover_texts = preprocess2sentence(preprocess_txt(dataset), corpus_name=\"custom\", start_sample_idx=0, cutoff_q=(0.0, 1.0), use_cache=False)\n",
    "\n",
    "    print('===嵌入水印===')\n",
    "\n",
    "    watermark_text_output = []\n",
    "    original_text_output = []\n",
    "    \n",
    "\n",
    "    # 创建结果文件\n",
    "    result_dir = os.path.join(dirname, \"watermarked.txt\")\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(os.path.dirname(result_dir), exist_ok=True)\n",
    "\n",
    "    progress_bar = tqdm(range(len(cover_texts)))\n",
    "    wr = open(result_dir, \"w\")\n",
    "    for c_idx, sentences in enumerate(cover_texts):\n",
    "        text_result = []    # 每段文本的结果，存储每段文本中每个句子添加水印后的版本与嵌入的比特信息\n",
    "        original_text_result = []\n",
    "        for s_idx, sen in enumerate(sentences):\n",
    "            sen = spacy_tokenizer(sen.text.strip())     # 将句子划分成单词\n",
    "            # all_keywords: 全体关键词； entity_keyword: 实体关键词\n",
    "            all_keywords, entity_keywords = model.keyword_module.extract_keyword([sen])   # 提取关键词\n",
    "            keyword = all_keywords[0]\n",
    "            ent_keyword = entity_keywords[0]\n",
    "            agg_cwi, agg_probs, tokenized_pt, (mask_idx_pt, mask_idx, mask_word) = model.run_iter(sen, keyword, ent_keyword,\n",
    "                                                                                                  train_flag=False, embed_flag=True)\n",
    "            # check if keyword & mask_indices matches\n",
    "            valid_watermarks = []\n",
    "            candidate_kwd_cnt = 0\n",
    "            tokenized_text = [token.text_with_ws for token in sen]\n",
    "\n",
    "            if len(agg_cwi) > 0:\n",
    "                for cwi in product(*agg_cwi):\n",
    "                    wm_text = tokenized_text.copy()\n",
    "                    for m_idx, c_id in zip(mask_idx, cwi):\n",
    "                        wm_text[m_idx] = re.sub(r\"\\S+\", model.tokenizer.decode(c_id), wm_text[m_idx])\n",
    "\n",
    "                    wm_tokenized = spacy_tokenizer(\"\".join(wm_text).strip())\n",
    "\n",
    "                    # extract keyword of watermark\n",
    "                    wm_keywords, wm_ent_keywords = model.keyword_module.extract_keyword([wm_tokenized])\n",
    "                    wm_kwd = wm_keywords[0]\n",
    "                    wm_ent_kwd = wm_ent_keywords[0]\n",
    "                    wm_mask_idx, wm_mask = model.mask_selector.return_mask(wm_tokenized, wm_kwd, wm_ent_kwd)\n",
    "\n",
    "                    kwd_match_flag = set([x.text for x in wm_kwd]) == set([x.text for x in keyword])\n",
    "                    if kwd_match_flag:\n",
    "                        kwd_match_cnt += 1\n",
    "\n",
    "                    # checking whether the watermark can be embedded without the assumption of corruption\n",
    "                    mask_match_flag = len(wm_mask) and set(wm_mask_idx) == set(mask_idx)\n",
    "                    if mask_match_flag:\n",
    "                        valid_watermarks.append(wm_tokenized.text)\n",
    "                        mask_match_cnt += 1\n",
    "\n",
    "                    sample_cnt += 1\n",
    "                    candidate_kwd_cnt += 1\n",
    "    \n",
    "            punct_removed = sen.text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "            word_count += len([i for i in punct_removed.split(\" \") if i not in stop])\n",
    "            if len(valid_watermarks) > 1:\n",
    "                bit_count += math.log2(len(valid_watermarks))\n",
    "                random_msg_decimal = random.choice(range(len(valid_watermarks)))\n",
    "                num_digit = math.ceil(math.log2(len(valid_watermarks)))\n",
    "                random_msg_binary = format(random_msg_decimal, f\"0{num_digit}b\")\n",
    "\n",
    "                wm_text = valid_watermarks[random_msg_decimal]\n",
    "                watermarked_text = \"\".join(wm_text) if len(mask_idx) > 0 else \"\"\n",
    "                message_str = list(random_msg_binary)\n",
    "                one_cnt += len([i for i in message_str if i ==\"1\"])\n",
    "                zero_cnt += len([i for i in message_str if i ==\"0\"])\n",
    "\n",
    "                keys = []\n",
    "                wm_tokenized = spacy_tokenizer(wm_text)\n",
    "                for m_idx in mask_idx:\n",
    "                    keys.append(wm_tokenized[m_idx].text)\n",
    "                keys_str = \", \".join(keys)\n",
    "                message_str = ' '.join(message_str) if len(message_str) else \"\"\n",
    "                # ''.join(wm_text): 水印文本\n",
    "                # keys_str: 替换的词\n",
    "                # message_str: 嵌入的比特信息\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{''.join(wm_text)}\\t{keys_str}\\t{message_str}\\n\")  \n",
    "                watermarksss_text = ''.join(wm_text)\n",
    "                keys = []\n",
    "                keys.append(keys_str)\n",
    "                msg = get_list(message_str)\n",
    "                # msg.append(int(message_str))\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':watermarksss_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "            else:\n",
    "                # 无可添加水印，直接将原始文本作为水印\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{original_text}\\t \\t \\n\")\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text': original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "\n",
    "            if candidate_kwd_cnt > 0:\n",
    "                upper_bound += math.log2(candidate_kwd_cnt)\n",
    "        progress_bar.update(1)\n",
    "        watermark_text_output.append(text_result)\n",
    "        original_text_output.append(original_text_result)\n",
    "\n",
    "    # wr.close()\n",
    "\n",
    "    # ss_scores = []\n",
    "    # for model_name in [\"roberta\", \"all-MiniLM-L6-v2\"]:\n",
    "    #     ss_score, ss_dist = metric.compute_ss(result_dir, model_name, cover_texts)\n",
    "    #     ss_scores.append(sum(ss_score) / len(ss_score))\n",
    "    # nli_score, _, _ = metric.compute_nli(result_dir, cover_texts)\n",
    "\n",
    "    # result_dir = os.path.join(dirname, \"embed-metrics.txt\")\n",
    "    # with open(result_dir, \"a\") as wr:\n",
    "    #     wr.write(str(vars(infill_args))+\"\\n\")\n",
    "    #     wr.write(f\"num.sample={num_sample}\\t bpw={bit_count / word_count}\\t \"\n",
    "    #              f\"ss={ss_scores[0]}\\t ss={ss_scores[1]}\\t\"\n",
    "    #              f\"nli={sum(nli_score) / len(nli_score)}\\n\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(watermark_result_cohere), exist_ok=True)\n",
    "    with open(watermark_result_cohere, 'w', encoding='utf-8') as f:\n",
    "        json.dump(watermark_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    os.makedirs(os.path.dirname(original_result_cohere), exist_ok=True)\n",
    "    with open(original_result_cohere, 'w', encoding='utf-8') as f:\n",
    "        json.dump(original_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===原始文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88badb5cfe54c4484d817330f70c5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===水印文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dae3cdf149d4898805e8fdc32cf577e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===cohere完成===\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    original_text_output = json.load(open(original_result_cohere))\n",
    "    watermark_text_output = json.load(open(watermark_result_cohere))\n",
    "\n",
    "    ori_fast_z_score = extract(original_text_output, '原始文本')\n",
    "    water_fast_z_score = extract(watermark_text_output, '水印文本')\n",
    "\n",
    "    output_result = []\n",
    "\n",
    "    for index in range(len(ori_fast_z_score)):\n",
    "        output_result.append({\n",
    "            'text-index': ori_fast_z_score[index]['text-index'],\n",
    "            'sentence-length': ori_fast_z_score[index]['sentence-length'],\n",
    "            'ori-fast-z-score': ori_fast_z_score[index]['z-score'],\n",
    "            'water-fast-z-score': water_fast_z_score[index]['z-score']\n",
    "\n",
    "        })\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_file_cohere), exist_ok=True)\n",
    "    with open(result_file_cohere, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_result, f, indent=4, ensure_ascii=False)\n",
    "    print('===cohere完成===')\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# davinci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===嵌入水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08878640827542aca448b2116c593864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name_davinci = 'M4/arxiv/abstract/dataset-M4-random-davinci-800.json'\n",
    "watermark_result_davinci = 'result/nlp/M4/watermark/dataset-M4-random-davinci-800.json'\n",
    "original_result_davinci = 'result/nlp/M4/original/dataset-M4-random-davinci-800.json'\n",
    "result_file_davinci = 'result/nlp/M4/z_score/dataset-M4-random-davinci-800.json'\n",
    "\n",
    "try:   # 嵌入水印模式\n",
    "    dataset = json.load(open(file_name_davinci))\n",
    "\n",
    "    cover_texts = preprocess2sentence(preprocess_txt(dataset), corpus_name=\"custom\", start_sample_idx=0, cutoff_q=(0.0, 1.0), use_cache=False)\n",
    "\n",
    "    print('===嵌入水印===')\n",
    "\n",
    "    watermark_text_output = []\n",
    "    original_text_output = []\n",
    "    \n",
    "\n",
    "    # 创建结果文件\n",
    "    result_dir = os.path.join(dirname, \"watermarked.txt\")\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(os.path.dirname(result_dir), exist_ok=True)\n",
    "\n",
    "    progress_bar = tqdm(range(len(cover_texts)))\n",
    "    wr = open(result_dir, \"w\")\n",
    "    for c_idx, sentences in enumerate(cover_texts):\n",
    "        text_result = []    # 每段文本的结果，存储每段文本中每个句子添加水印后的版本与嵌入的比特信息\n",
    "        original_text_result = []\n",
    "        for s_idx, sen in enumerate(sentences):\n",
    "            sen = spacy_tokenizer(sen.text.strip())     # 将句子划分成单词\n",
    "            # all_keywords: 全体关键词； entity_keyword: 实体关键词\n",
    "            all_keywords, entity_keywords = model.keyword_module.extract_keyword([sen])   # 提取关键词\n",
    "            keyword = all_keywords[0]\n",
    "            ent_keyword = entity_keywords[0]\n",
    "            agg_cwi, agg_probs, tokenized_pt, (mask_idx_pt, mask_idx, mask_word) = model.run_iter(sen, keyword, ent_keyword,\n",
    "                                                                                                  train_flag=False, embed_flag=True)\n",
    "            # check if keyword & mask_indices matches\n",
    "            valid_watermarks = []\n",
    "            candidate_kwd_cnt = 0\n",
    "            tokenized_text = [token.text_with_ws for token in sen]\n",
    "\n",
    "            if len(agg_cwi) > 0:\n",
    "                for cwi in product(*agg_cwi):\n",
    "                    wm_text = tokenized_text.copy()\n",
    "                    for m_idx, c_id in zip(mask_idx, cwi):\n",
    "                        wm_text[m_idx] = re.sub(r\"\\S+\", model.tokenizer.decode(c_id), wm_text[m_idx])\n",
    "\n",
    "                    wm_tokenized = spacy_tokenizer(\"\".join(wm_text).strip())\n",
    "\n",
    "                    # extract keyword of watermark\n",
    "                    wm_keywords, wm_ent_keywords = model.keyword_module.extract_keyword([wm_tokenized])\n",
    "                    wm_kwd = wm_keywords[0]\n",
    "                    wm_ent_kwd = wm_ent_keywords[0]\n",
    "                    wm_mask_idx, wm_mask = model.mask_selector.return_mask(wm_tokenized, wm_kwd, wm_ent_kwd)\n",
    "\n",
    "                    kwd_match_flag = set([x.text for x in wm_kwd]) == set([x.text for x in keyword])\n",
    "                    if kwd_match_flag:\n",
    "                        kwd_match_cnt += 1\n",
    "\n",
    "                    # checking whether the watermark can be embedded without the assumption of corruption\n",
    "                    mask_match_flag = len(wm_mask) and set(wm_mask_idx) == set(mask_idx)\n",
    "                    if mask_match_flag:\n",
    "                        valid_watermarks.append(wm_tokenized.text)\n",
    "                        mask_match_cnt += 1\n",
    "\n",
    "                    sample_cnt += 1\n",
    "                    candidate_kwd_cnt += 1\n",
    "    \n",
    "            punct_removed = sen.text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "            word_count += len([i for i in punct_removed.split(\" \") if i not in stop])\n",
    "            if len(valid_watermarks) > 1:\n",
    "                bit_count += math.log2(len(valid_watermarks))\n",
    "                random_msg_decimal = random.choice(range(len(valid_watermarks)))\n",
    "                num_digit = math.ceil(math.log2(len(valid_watermarks)))\n",
    "                random_msg_binary = format(random_msg_decimal, f\"0{num_digit}b\")\n",
    "\n",
    "                wm_text = valid_watermarks[random_msg_decimal]\n",
    "                watermarked_text = \"\".join(wm_text) if len(mask_idx) > 0 else \"\"\n",
    "                message_str = list(random_msg_binary)\n",
    "                one_cnt += len([i for i in message_str if i ==\"1\"])\n",
    "                zero_cnt += len([i for i in message_str if i ==\"0\"])\n",
    "\n",
    "                keys = []\n",
    "                wm_tokenized = spacy_tokenizer(wm_text)\n",
    "                for m_idx in mask_idx:\n",
    "                    keys.append(wm_tokenized[m_idx].text)\n",
    "                keys_str = \", \".join(keys)\n",
    "                message_str = ' '.join(message_str) if len(message_str) else \"\"\n",
    "                # ''.join(wm_text): 水印文本\n",
    "                # keys_str: 替换的词\n",
    "                # message_str: 嵌入的比特信息\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{''.join(wm_text)}\\t{keys_str}\\t{message_str}\\n\")  \n",
    "                watermarksss_text = ''.join(wm_text)\n",
    "                keys = []\n",
    "                keys.append(keys_str)\n",
    "                msg = get_list(message_str)\n",
    "                # msg.append(int(message_str))\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':watermarksss_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "            else:\n",
    "                # 无可添加水印，直接将原始文本作为水印\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{original_text}\\t \\t \\n\")\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text': original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "\n",
    "            if candidate_kwd_cnt > 0:\n",
    "                upper_bound += math.log2(candidate_kwd_cnt)\n",
    "        progress_bar.update(1)\n",
    "        watermark_text_output.append(text_result)\n",
    "        original_text_output.append(original_text_result)\n",
    "\n",
    "    # wr.close()\n",
    "\n",
    "    # ss_scores = []\n",
    "    # for model_name in [\"roberta\", \"all-MiniLM-L6-v2\"]:\n",
    "    #     ss_score, ss_dist = metric.compute_ss(result_dir, model_name, cover_texts)\n",
    "    #     ss_scores.append(sum(ss_score) / len(ss_score))\n",
    "    # nli_score, _, _ = metric.compute_nli(result_dir, cover_texts)\n",
    "\n",
    "    # result_dir = os.path.join(dirname, \"embed-metrics.txt\")\n",
    "    # with open(result_dir, \"a\") as wr:\n",
    "    #     wr.write(str(vars(infill_args))+\"\\n\")\n",
    "    #     wr.write(f\"num.sample={num_sample}\\t bpw={bit_count / word_count}\\t \"\n",
    "    #              f\"ss={ss_scores[0]}\\t ss={ss_scores[1]}\\t\"\n",
    "    #              f\"nli={sum(nli_score) / len(nli_score)}\\n\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(watermark_result_davinci), exist_ok=True)\n",
    "    with open(watermark_result_davinci, 'w', encoding='utf-8') as f:\n",
    "        json.dump(watermark_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    os.makedirs(os.path.dirname(original_result_davinci), exist_ok=True)\n",
    "    with open(original_result_davinci, 'w', encoding='utf-8') as f:\n",
    "        json.dump(original_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===原始文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7ec6483df643ed84ccbc6ca3271299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===水印文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad02bb537e54216b5a7a1f976771067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===davinci完成===\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    original_text_output = json.load(open(original_result_davinci))\n",
    "    watermark_text_output = json.load(open(watermark_result_davinci))\n",
    "\n",
    "    ori_fast_z_score = extract(original_text_output, '原始文本')\n",
    "    water_fast_z_score = extract(watermark_text_output, '水印文本')\n",
    "\n",
    "    output_result = []\n",
    "\n",
    "    for index in range(len(ori_fast_z_score)):\n",
    "        output_result.append({\n",
    "            'text-index': ori_fast_z_score[index]['text-index'],\n",
    "            'sentence-length': ori_fast_z_score[index]['sentence-length'],\n",
    "            'ori-fast-z-score': ori_fast_z_score[index]['z-score'],\n",
    "            'water-fast-z-score': water_fast_z_score[index]['z-score']\n",
    "\n",
    "        })\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_file_davinci), exist_ok=True)\n",
    "    with open(result_file_davinci, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_result, f, indent=4, ensure_ascii=False)\n",
    "    print('===davinci完成===')\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===嵌入水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176aa64c959845efabb78d2728ef9898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name_dolly = 'M4/arxiv/abstract/dataset-M4-random-dolly-800.json'\n",
    "watermark_result_dolly = 'result/nlp/M4/watermark/dataset-M4-random-dolly-800.json'\n",
    "original_result_dolly = 'result/nlp/M4/original/dataset-M4-random-dolly-800.json'\n",
    "result_file_dolly = 'result/nlp/M4/z_score/dataset-M4-random-dolly-800.json'\n",
    "try:   # 嵌入水印模式\n",
    "    \n",
    "    dataset = json.load(open(file_name_dolly))\n",
    "\n",
    "    cover_texts = preprocess2sentence(preprocess_txt(dataset), corpus_name=\"custom\", start_sample_idx=0, cutoff_q=(0.0, 1.0), use_cache=False)\n",
    "\n",
    "    print('===嵌入水印===')\n",
    "\n",
    "    watermark_text_output = []\n",
    "    original_text_output = []\n",
    "    \n",
    "\n",
    "    # 创建结果文件\n",
    "    result_dir = os.path.join(dirname, \"watermarked.txt\")\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(os.path.dirname(result_dir), exist_ok=True)\n",
    "\n",
    "    progress_bar = tqdm(range(len(cover_texts)))\n",
    "    wr = open(result_dir, \"w\")\n",
    "    for c_idx, sentences in enumerate(cover_texts):\n",
    "        text_result = []    # 每段文本的结果，存储每段文本中每个句子添加水印后的版本与嵌入的比特信息\n",
    "        original_text_result = []\n",
    "        for s_idx, sen in enumerate(sentences):\n",
    "            sen = spacy_tokenizer(sen.text.strip())     # 将句子划分成单词\n",
    "            # all_keywords: 全体关键词； entity_keyword: 实体关键词\n",
    "            all_keywords, entity_keywords = model.keyword_module.extract_keyword([sen])   # 提取关键词\n",
    "            keyword = all_keywords[0]\n",
    "            ent_keyword = entity_keywords[0]\n",
    "            agg_cwi, agg_probs, tokenized_pt, (mask_idx_pt, mask_idx, mask_word) = model.run_iter(sen, keyword, ent_keyword,\n",
    "                                                                                                  train_flag=False, embed_flag=True)\n",
    "            # check if keyword & mask_indices matches\n",
    "            valid_watermarks = []\n",
    "            candidate_kwd_cnt = 0\n",
    "            tokenized_text = [token.text_with_ws for token in sen]\n",
    "\n",
    "            if len(agg_cwi) > 0:\n",
    "                for cwi in product(*agg_cwi):\n",
    "                    wm_text = tokenized_text.copy()\n",
    "                    for m_idx, c_id in zip(mask_idx, cwi):\n",
    "                        wm_text[m_idx] = re.sub(r\"\\S+\", model.tokenizer.decode(c_id), wm_text[m_idx])\n",
    "\n",
    "                    wm_tokenized = spacy_tokenizer(\"\".join(wm_text).strip())\n",
    "\n",
    "                    # extract keyword of watermark\n",
    "                    wm_keywords, wm_ent_keywords = model.keyword_module.extract_keyword([wm_tokenized])\n",
    "                    wm_kwd = wm_keywords[0]\n",
    "                    wm_ent_kwd = wm_ent_keywords[0]\n",
    "                    wm_mask_idx, wm_mask = model.mask_selector.return_mask(wm_tokenized, wm_kwd, wm_ent_kwd)\n",
    "\n",
    "                    kwd_match_flag = set([x.text for x in wm_kwd]) == set([x.text for x in keyword])\n",
    "                    if kwd_match_flag:\n",
    "                        kwd_match_cnt += 1\n",
    "\n",
    "                    # checking whether the watermark can be embedded without the assumption of corruption\n",
    "                    mask_match_flag = len(wm_mask) and set(wm_mask_idx) == set(mask_idx)\n",
    "                    if mask_match_flag:\n",
    "                        valid_watermarks.append(wm_tokenized.text)\n",
    "                        mask_match_cnt += 1\n",
    "\n",
    "                    sample_cnt += 1\n",
    "                    candidate_kwd_cnt += 1\n",
    "    \n",
    "            punct_removed = sen.text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "            word_count += len([i for i in punct_removed.split(\" \") if i not in stop])\n",
    "            if len(valid_watermarks) > 1:\n",
    "                bit_count += math.log2(len(valid_watermarks))\n",
    "                random_msg_decimal = random.choice(range(len(valid_watermarks)))\n",
    "                num_digit = math.ceil(math.log2(len(valid_watermarks)))\n",
    "                random_msg_binary = format(random_msg_decimal, f\"0{num_digit}b\")\n",
    "\n",
    "                wm_text = valid_watermarks[random_msg_decimal]\n",
    "                watermarked_text = \"\".join(wm_text) if len(mask_idx) > 0 else \"\"\n",
    "                message_str = list(random_msg_binary)\n",
    "                one_cnt += len([i for i in message_str if i ==\"1\"])\n",
    "                zero_cnt += len([i for i in message_str if i ==\"0\"])\n",
    "\n",
    "                keys = []\n",
    "                wm_tokenized = spacy_tokenizer(wm_text)\n",
    "                for m_idx in mask_idx:\n",
    "                    keys.append(wm_tokenized[m_idx].text)\n",
    "                keys_str = \", \".join(keys)\n",
    "                message_str = ' '.join(message_str) if len(message_str) else \"\"\n",
    "                # ''.join(wm_text): 水印文本\n",
    "                # keys_str: 替换的词\n",
    "                # message_str: 嵌入的比特信息\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{''.join(wm_text)}\\t{keys_str}\\t{message_str}\\n\")  \n",
    "                watermarksss_text = ''.join(wm_text)\n",
    "                keys = []\n",
    "                keys.append(keys_str)\n",
    "                msg = get_list(message_str)\n",
    "                # msg.append(int(message_str))\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':watermarksss_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "            else:\n",
    "                # 无可添加水印，直接将原始文本作为水印\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{original_text}\\t \\t \\n\")\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text': original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "\n",
    "            if candidate_kwd_cnt > 0:\n",
    "                upper_bound += math.log2(candidate_kwd_cnt)\n",
    "        progress_bar.update(1)\n",
    "        watermark_text_output.append(text_result)\n",
    "        original_text_output.append(original_text_result)\n",
    "\n",
    "    # wr.close()\n",
    "\n",
    "    # ss_scores = []\n",
    "    # for model_name in [\"roberta\", \"all-MiniLM-L6-v2\"]:\n",
    "    #     ss_score, ss_dist = metric.compute_ss(result_dir, model_name, cover_texts)\n",
    "    #     ss_scores.append(sum(ss_score) / len(ss_score))\n",
    "    # nli_score, _, _ = metric.compute_nli(result_dir, cover_texts)\n",
    "\n",
    "    # result_dir = os.path.join(dirname, \"embed-metrics.txt\")\n",
    "    # with open(result_dir, \"a\") as wr:\n",
    "    #     wr.write(str(vars(infill_args))+\"\\n\")\n",
    "    #     wr.write(f\"num.sample={num_sample}\\t bpw={bit_count / word_count}\\t \"\n",
    "    #              f\"ss={ss_scores[0]}\\t ss={ss_scores[1]}\\t\"\n",
    "    #              f\"nli={sum(nli_score) / len(nli_score)}\\n\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(watermark_result_dolly), exist_ok=True)\n",
    "    with open(watermark_result_dolly, 'w', encoding='utf-8') as f:\n",
    "        json.dump(watermark_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    os.makedirs(os.path.dirname(original_result_dolly), exist_ok=True)\n",
    "    with open(original_result_dolly, 'w', encoding='utf-8') as f:\n",
    "        json.dump(original_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===原始文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe1352627254c8fa9cd17fa9794b6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===水印文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93822c03d64b4265942c39294966047f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===dolly完成===\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    original_text_output = json.load(open(original_result_dolly))\n",
    "    watermark_text_output = json.load(open(watermark_result_dolly))\n",
    "\n",
    "    ori_fast_z_score = extract(original_text_output, '原始文本')\n",
    "    water_fast_z_score = extract(watermark_text_output, '水印文本')\n",
    "\n",
    "    output_result = []\n",
    "\n",
    "    for index in range(len(ori_fast_z_score)):\n",
    "        output_result.append({\n",
    "            'text-index': ori_fast_z_score[index]['text-index'],\n",
    "            'sentence-length': ori_fast_z_score[index]['sentence-length'],\n",
    "            'ori-fast-z-score': ori_fast_z_score[index]['z-score'],\n",
    "            'water-fast-z-score': water_fast_z_score[index]['z-score']\n",
    "\n",
    "        })\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_file_dolly), exist_ok=True)\n",
    "    with open(result_file_dolly, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_result, f, indent=4, ensure_ascii=False)\n",
    "    print('===dolly完成===')\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flant5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===嵌入水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb205a1a35714fe18e46a8ebb1701c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name_flant5 = 'M4/arxiv/abstract/dataset-M4-random-flant5-800.json'\n",
    "watermark_result_flant5 = 'result/nlp/M4/watermark/dataset-M4-random-flant5-800.json'\n",
    "original_result_flant5 = 'result/nlp/M4/original/dataset-M4-random-flant5-800.json'\n",
    "result_file_flant5 = 'result/nlp/M4/z_score/dataset-M4-random-flant5-800.json'\n",
    "try:   # 嵌入水印模式\n",
    "    \n",
    "    dataset = json.load(open(file_name_flant5))\n",
    "\n",
    "    cover_texts = preprocess2sentence(preprocess_txt(dataset), corpus_name=\"custom\", start_sample_idx=0, cutoff_q=(0.0, 1.0), use_cache=False)\n",
    "\n",
    "    print('===嵌入水印===')\n",
    "\n",
    "    watermark_text_output = []\n",
    "    original_text_output = []\n",
    "    \n",
    "\n",
    "    # 创建结果文件\n",
    "    result_dir = os.path.join(dirname, \"watermarked.txt\")\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(os.path.dirname(result_dir), exist_ok=True)\n",
    "\n",
    "    progress_bar = tqdm(range(len(cover_texts)))\n",
    "    wr = open(result_dir, \"w\")\n",
    "    for c_idx, sentences in enumerate(cover_texts):\n",
    "        text_result = []    # 每段文本的结果，存储每段文本中每个句子添加水印后的版本与嵌入的比特信息\n",
    "        original_text_result = []\n",
    "        for s_idx, sen in enumerate(sentences):\n",
    "            sen = spacy_tokenizer(sen.text.strip())     # 将句子划分成单词\n",
    "            # all_keywords: 全体关键词； entity_keyword: 实体关键词\n",
    "            all_keywords, entity_keywords = model.keyword_module.extract_keyword([sen])   # 提取关键词\n",
    "            keyword = all_keywords[0]\n",
    "            ent_keyword = entity_keywords[0]\n",
    "            agg_cwi, agg_probs, tokenized_pt, (mask_idx_pt, mask_idx, mask_word) = model.run_iter(sen, keyword, ent_keyword,\n",
    "                                                                                                  train_flag=False, embed_flag=True)\n",
    "            # check if keyword & mask_indices matches\n",
    "            valid_watermarks = []\n",
    "            candidate_kwd_cnt = 0\n",
    "            tokenized_text = [token.text_with_ws for token in sen]\n",
    "\n",
    "            if len(agg_cwi) > 0:\n",
    "                for cwi in product(*agg_cwi):\n",
    "                    wm_text = tokenized_text.copy()\n",
    "                    for m_idx, c_id in zip(mask_idx, cwi):\n",
    "                        wm_text[m_idx] = re.sub(r\"\\S+\", model.tokenizer.decode(c_id), wm_text[m_idx])\n",
    "\n",
    "                    wm_tokenized = spacy_tokenizer(\"\".join(wm_text).strip())\n",
    "\n",
    "                    # extract keyword of watermark\n",
    "                    wm_keywords, wm_ent_keywords = model.keyword_module.extract_keyword([wm_tokenized])\n",
    "                    wm_kwd = wm_keywords[0]\n",
    "                    wm_ent_kwd = wm_ent_keywords[0]\n",
    "                    wm_mask_idx, wm_mask = model.mask_selector.return_mask(wm_tokenized, wm_kwd, wm_ent_kwd)\n",
    "\n",
    "                    kwd_match_flag = set([x.text for x in wm_kwd]) == set([x.text for x in keyword])\n",
    "                    if kwd_match_flag:\n",
    "                        kwd_match_cnt += 1\n",
    "\n",
    "                    # checking whether the watermark can be embedded without the assumption of corruption\n",
    "                    mask_match_flag = len(wm_mask) and set(wm_mask_idx) == set(mask_idx)\n",
    "                    if mask_match_flag:\n",
    "                        valid_watermarks.append(wm_tokenized.text)\n",
    "                        mask_match_cnt += 1\n",
    "\n",
    "                    sample_cnt += 1\n",
    "                    candidate_kwd_cnt += 1\n",
    "    \n",
    "            punct_removed = sen.text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "            word_count += len([i for i in punct_removed.split(\" \") if i not in stop])\n",
    "            if len(valid_watermarks) > 1:\n",
    "                bit_count += math.log2(len(valid_watermarks))\n",
    "                random_msg_decimal = random.choice(range(len(valid_watermarks)))\n",
    "                num_digit = math.ceil(math.log2(len(valid_watermarks)))\n",
    "                random_msg_binary = format(random_msg_decimal, f\"0{num_digit}b\")\n",
    "\n",
    "                wm_text = valid_watermarks[random_msg_decimal]\n",
    "                watermarked_text = \"\".join(wm_text) if len(mask_idx) > 0 else \"\"\n",
    "                message_str = list(random_msg_binary)\n",
    "                one_cnt += len([i for i in message_str if i ==\"1\"])\n",
    "                zero_cnt += len([i for i in message_str if i ==\"0\"])\n",
    "\n",
    "                keys = []\n",
    "                wm_tokenized = spacy_tokenizer(wm_text)\n",
    "                for m_idx in mask_idx:\n",
    "                    keys.append(wm_tokenized[m_idx].text)\n",
    "                keys_str = \", \".join(keys)\n",
    "                message_str = ' '.join(message_str) if len(message_str) else \"\"\n",
    "                # ''.join(wm_text): 水印文本\n",
    "                # keys_str: 替换的词\n",
    "                # message_str: 嵌入的比特信息\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{''.join(wm_text)}\\t{keys_str}\\t{message_str}\\n\")  \n",
    "                watermarksss_text = ''.join(wm_text)\n",
    "                keys = []\n",
    "                keys.append(keys_str)\n",
    "                msg = get_list(message_str)\n",
    "                # msg.append(int(message_str))\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':watermarksss_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': keys_str,\n",
    "                    'msg': msg\n",
    "                })\n",
    "            else:\n",
    "                # 无可添加水印，直接将原始文本作为水印\n",
    "                original_text = ''.join(tokenized_text)\n",
    "                wr.write(f\"{c_idx}\\t{s_idx}\\t \\t \\t\"\n",
    "                         f\"{original_text}\\t \\t \\n\")\n",
    "                text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text': original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "                original_text_result.append({\n",
    "                    'c_idx': c_idx,\n",
    "                    'sen_idx': s_idx,\n",
    "                    'sub_idset':[],\n",
    "                    'sub_idx':[],\n",
    "                    'clean_wm_text':original_text,\n",
    "                    'key': '',\n",
    "                    'msg': []\n",
    "                })\n",
    "\n",
    "            if candidate_kwd_cnt > 0:\n",
    "                upper_bound += math.log2(candidate_kwd_cnt)\n",
    "        progress_bar.update(1)\n",
    "        watermark_text_output.append(text_result)\n",
    "        original_text_output.append(original_text_result)\n",
    "\n",
    "    # wr.close()\n",
    "\n",
    "    # ss_scores = []\n",
    "    # for model_name in [\"roberta\", \"all-MiniLM-L6-v2\"]:\n",
    "    #     ss_score, ss_dist = metric.compute_ss(result_dir, model_name, cover_texts)\n",
    "    #     ss_scores.append(sum(ss_score) / len(ss_score))\n",
    "    # nli_score, _, _ = metric.compute_nli(result_dir, cover_texts)\n",
    "\n",
    "    # result_dir = os.path.join(dirname, \"embed-metrics.txt\")\n",
    "    # with open(result_dir, \"a\") as wr:\n",
    "    #     wr.write(str(vars(infill_args))+\"\\n\")\n",
    "    #     wr.write(f\"num.sample={num_sample}\\t bpw={bit_count / word_count}\\t \"\n",
    "    #              f\"ss={ss_scores[0]}\\t ss={ss_scores[1]}\\t\"\n",
    "    #              f\"nli={sum(nli_score) / len(nli_score)}\\n\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(watermark_result_flant5), exist_ok=True)\n",
    "    with open(watermark_result_flant5, 'w', encoding='utf-8') as f:\n",
    "        json.dump(watermark_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    os.makedirs(os.path.dirname(original_result_flant5), exist_ok=True)\n",
    "    with open(original_result_flant5, 'w', encoding='utf-8') as f:\n",
    "        json.dump(original_text_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===原始文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09191be1e4240769b3b155dd4d32f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===水印文本提取水印===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a0b6822de6485e9f58ec8bd56ecf0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===flant5完成===\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    original_text_output = json.load(open(original_result_flant5))\n",
    "    watermark_text_output = json.load(open(watermark_result_flant5))\n",
    "\n",
    "    ori_fast_z_score = extract(original_text_output, '原始文本')\n",
    "    water_fast_z_score = extract(watermark_text_output, '水印文本')\n",
    "\n",
    "    output_result = []\n",
    "\n",
    "    for index in range(len(ori_fast_z_score)):\n",
    "        output_result.append({\n",
    "            'text-index': ori_fast_z_score[index]['text-index'],\n",
    "            'sentence-length': ori_fast_z_score[index]['sentence-length'],\n",
    "            'ori-fast-z-score': ori_fast_z_score[index]['z-score'],\n",
    "            'water-fast-z-score': water_fast_z_score[index]['z-score']\n",
    "\n",
    "        })\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_file_flant5), exist_ok=True)\n",
    "    with open(result_file_flant5, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_result, f, indent=4, ensure_ascii=False)\n",
    "        print('===flant5完成===')\n",
    "except BaseException as e:\n",
    "    print(f'出现错误:{e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
