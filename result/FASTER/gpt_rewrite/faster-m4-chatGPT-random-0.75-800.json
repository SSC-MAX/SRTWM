[
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Recent UBVJH Photometry of Epsilon Aurigae .\nAbstract:\nEpsilon Aurigae is an F-type main sequence star with a mass of 1.8 M☉ and radius 2 R☉, located at about 40 light-years away in the constellation Auriga.  It has been known for many years to be surrounded by dusty material that obscures its visible spectrum.   The infrared excess emission detected around this object suggests it may have a circumstellar disk similar to those found around young stars such as T Tauri or Herbig Ae/Be stars.   In addition, there are indications that the system contains a close companion which could also contribute to the observed infrared excess emission.    We present new photometric observations obtained using the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea over the period 1997-2001 covering wavelengths between 0.9-2.5 microns.  These data show significant variations in both the near-infrared fluxes and colours of the central source consistent with changes in the amount of dust surrounding the star.  This behaviour is very similar to what is seen in other pre-main-sequence systems where accretion onto the central star causes periodic increases in luminosity accompanied by increased levels of reddening due to heating of the surrounding dust grains.   Our results suggest that the current level of activity in the system is relatively low compared to previous epochs but we cannot rule out the possibility that the recent increase in brightness was caused by a short-lived burst of enhanced accretion rather than steady-state accretion occurring throughout our observing campaign.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Recent UBVJH Photometry of Epsilon Aurigae . Abstract : Epsilon Aurigae is an F - class major sequence star with a mass of 1 . 8 M☉ and density 2 R☉ , located at about 40 light - years away in the constellation Auriga .It has been known for thousands decades to be accompanied by dusty matter that obscures its visible spectrum . The infrared excess emission detected around this body suggests it could have a circumstellar disk comparable to those observed around young galaxies such as T Tauri or Herbig Ae / Be stars .In addition , there are indications that the system contains a close companion which could also contribute to the seen infrared excess emission . We report new photometric images obtained using the United Kingdom Infrared Telescope ( UKIRT ) on Mauna Kea over the period 1997 - 2001 covering wavelengths between 0 . 9 - 2 . 5 microns .These data demonstrate considerable variations in both the near - infrared fluxes and colours of the main source consistent with shifts in the quantity of dust surrounding the star . This behaviour is very related to what is seen in other pre - main - sequence systems where accretion onto the main star causes periodic increases in luminosity followed by increased levels of reddening due to heating of the nearby dust grains .Our results show that the present degree of activity in the system is fairly lowest relative to previous epochs but we cannot judge out the suggestion that the recent rise in intensity was due by a brief - lived burst of enhanced accretion instead than steady - state accretion occurring throughout our observing campaign .",
        "rewrite_text": "**Title: Recent UBVJH Photometry of Epsilon Aurigae**\n\n**Abstract:** Epsilon Aurigae is an F-type main-sequence star, exhibiting a mass of approximately 1.8 solar masses and a radius of about 2 solar radii, situated roughly 40 light-years away in the constellation Auriga. For centuries, this star has been recognized for its association with a significant amount of obscuring dusty material that interferes with its visible light spectrum. The presence of infrared excess emission surrounding Epsilon Aurigae suggests the possibility of a circumstellar disk, akin to those found around young stellar objects such as T Tauri and Herbig Ae/Be stars. Furthermore, evidence points to the existence of a close companion star, which may also play a role in contributing to the observed infrared excess. \n\nIn this study, we present new photometric observations collected using the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea from 1997 to 2001, spanning wavelengths from 0.9 to 2.5 microns. Our analysis reveals significant fluctuations in both the near-infrared flux and color of the primary star, indicating variations in the surrounding dust density. This behavior is reminiscent of phenomena observed in other pre-main-sequence stars, where accretion processes lead to periodic increases in luminosity, followed by heightened reddening due to the heating of adjacent dust grains. \n\nOur findings suggest that the current level of activity within the Epsilon Aurigae system is relatively low compared to previous observations. However, we cannot dismiss the possibility that the recent increase in brightness may be attributed to a transient surge in accretion rather than a consistent, steady-state accretion process throughout the duration of our observational campaign. This research contributes to the understanding of Epsilon Aurigae's complex environment and the dynamic interactions between the star and its surrounding material.",
        "ori-fast-z-score": -1.1818181818181819,
        "water-fast-z-score": 5.432144762551111,
        "rewrite-fast-z-score": -1.2909944487358056
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the stratified dust distribution of the GG Tau circumbinary ring .\nAbstract:\nWe present new near-infrared (NIR) polarimetric observations of the GG Tau system, which reveal that its circumstellar disk is highly structured and contains several bright regions with different polarization properties. The most prominent feature in our data set is an arc-like structure located at about 0.5 arcsec to the south-east of the central binary star. This region shows strong polarized emission up to 10% of the total intensity and has been previously identified as a reflection nebula by Weintraub et al. (1993) . We find that this feature can be explained by scattering off optically thin dust grains close to the midplane of the disk. In addition we detect two other bright features on either side of the central binary. These are also associated with high degrees of linear polarization but show no clear evidence for scattered light. Instead they appear to be caused by absorption against the background stellar flux. Finally, we identify three additional fainter structures in the southern part of the disk. All these features have similar polarization angles indicating that their origin may be related.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the stratified dust pattern of the GG Tau circumbinary ring . Abstract : We report new near - infrared ( NIR ) polarimetric studies of the GG Tau system , which confirm that its circumstellar disk is heavily structured and hosts multiple bright regions with various polarization properties .The most notable feature in our information run is an arc - like structure located at about 0 . 5 arcsec to the south - eastward of the main binary star . This region shows intense polarized emission up to 10 % of the total intensity and has been previously described as a mirror nebula by Weintraub et al .( 1993 ) . We see that this phenomenon can be described by scattering off optically thin dust grains next to the midplane of the disk .In addition we perceive two other bright features on either front of the main binary . These are also associated with high degrees of linear polarization but display no clear proof for dispersed light .Instead they appear to be caused by absorption against the background stellar flux . Finally , we identify three extra fainter objects in the southern portion of the disk .All these characteristics have similar polarization angles indicating that their ancestry may be connected .",
        "rewrite_text": "We present new near-infrared (NIR) polarimetric observations of the GG Tau system, revealing significant structural complexity within its circumstellar disk. Our findings indicate the presence of multiple bright regions, each exhibiting distinct polarization characteristics. A particularly striking feature is an arc-like structure located approximately 0.5 arcseconds southeast of the primary binary star. This region is characterized by a high degree of polarized emission, reaching up to 10% of the total intensity, and has previously been referred to as a mirror nebula by Weintraub et al. (1993). Our analysis suggests that this phenomenon can be attributed to scattering processes involving optically thin dust grains situated near the midplane of the disk.\n\nIn addition to the arc-like feature, we have identified two other prominent bright regions flanking the main binary. These areas also exhibit significant linear polarization; however, they lack clear evidence of dispersed light. Instead, their polarization characteristics appear to result from absorption against the background stellar light. Furthermore, we have detected three additional fainter objects located in the southern region of the disk. Notably, all these features share similar polarization angles, implying a potential connection in their origins.\n\nOverall, our study enhances the understanding of the stratified dust distribution within the GG Tau circumbinary ring, highlighting the intricate interplay between dust scattering and absorption processes. These insights contribute to the broader knowledge of circumstellar disk dynamics and the physical conditions that govern the formation of such structured environments.",
        "ori-fast-z-score": -0.7258661863112977,
        "water-fast-z-score": 5.363390480545726,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fluidization of a vertically oscillated shallow granular layer .\nAbstract:\nWe report on the fluidization of a thin, horizontally vibrating granular bed by vertical oscillations at low frequency and amplitude. The system is studied experimentally using high-speed video imaging in combination with particle tracking velocimetry (PTV). We find that for sufficiently large vibration amplitudes, particles are ejected from the surface into the air as they reach their maximum height during an upward motion. This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container. In addition we observe convection rolls which transport grains across the entire depth of the bed. These results show striking similarities to those observed in vibrated beds of sand or glass beads but differ significantly from previous studies performed on systems where only horizontal vibrations were applied. Vibrations can induce transitions between different states of matter such as solids, liquids, and gases  1  . For example, it has been shown that a solid state may become unstable when subjected to periodic forcing  2  , resulting in the spontaneous generation of traveling waves  3  . A particularly interesting case occurs if both horizontal and vertical components of the driving force act simultaneously  4  .\nIn this work we study the response of a thin granular layer to simultaneous application of horizontal and vertical vibrations. Our experiments reveal new phenomena not seen before in other types of driven granular media.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fluidization of a horizontally oscillated superficial granular layer . Abstract : We report on the fluidization of a thin , vertically vibrating granular bed by vertical oscillations at low frequency and amplitude .The system is studied experimentally utilizing large - speed video scanning in combination with particle tracking velocimetry ( PTV ) . We see that for enough large vibration amplitudes , particles are expelled from the surface into the air as they reach their maximum size during an upward moving .This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container . In addition we study convection rolls which carry grains across the entire depth of the bed .These data demonstrate remarkable parallels to those observed in vibrated beds of dust or glass bottles but differ significantly from previous research performed on structures where only horizontal vibrations were applied . Vibrations can induce changes between various states of matter such as solids , liquids , and gases 1 .For instance , it has been shown that a solid state would change unstable when exposed to periodic forcing 2 , resulting in the spontaneous production of traveling signals 3 . A notably unusual instance comes if both horizontal and horizontal elements of the driving pull act simultaneously 4 .In this research we study the response of a thin granular layer to continuous application of longitudinal and horizontal vibrations . Our experiments discover new experiments not seen before in other types of driven granular material .",
        "rewrite_text": "Title: Fluidization of a Horizontally Oscillated Superficial Granular Layer\n\nAbstract: This study investigates the fluidization phenomenon of a thin granular bed subjected to vertical oscillations at low frequencies and amplitudes. Utilizing high-speed video scanning alongside particle tracking velocimetry (PTV), we conduct a series of experiments to analyze the behavior of the granular layer. Our findings reveal that when the vibration amplitudes exceed a certain threshold, particles are ejected from the surface into the air during the upward motion, resulting in the emergence of a dilute gas phase above the densely packed region at the bottom of the container. Furthermore, we observe the formation of convection rolls that facilitate the movement of grains throughout the entire depth of the granular bed. These observations draw intriguing parallels to behaviors noted in vibrated beds of dust or glass, yet they significantly diverge from previous studies that focused solely on horizontal vibrations. The interplay of vibrations can induce transitions among different states of matter, including solids, liquids, and gases. Previous research has demonstrated that a solid state can become unstable under periodic forcing, leading to the spontaneous generation of traveling signals. A particularly interesting scenario arises when both longitudinal and horizontal components of the driving force are applied simultaneously. In this paper, we explore the response of a thin granular layer to the continuous application of these dual vibrations, revealing novel behaviors that have not been documented in other types of driven granular materials. Our results contribute to a deeper understanding of the dynamics of granular systems under complex driving conditions, highlighting the intricate relationships between vibration parameters and material states.",
        "ori-fast-z-score": 0.8980265101338746,
        "water-fast-z-score": 8.318172876299217,
        "rewrite-fast-z-score": 2.1842601416525946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Near and Mid-IR Photometry of the Pleiades, and a New List of Substellar Candidate Members .\nAbstract:\nWe present new near-infrared photometric data for the Pleiades open cluster obtained with the Infrared Survey Facility (IRSF) on the South African Astronomical Observatory s 1-m telescope at Sutherland in November 2005. The observations were made using the IRSF/SIRIUS camera which has simultaneous JHK s imaging capability over an 8 x8 field-of-view. We have also used archival 2MASS data to supplement our sample. Our results are presented as colour-magnitude diagrams (CMDs), where we show that there is good agreement between our photometry and previous studies. Using these CMDs, we identify several previously unreported candidate members of the Pleiades based upon their location relative to theoretical pre-main sequence evolutionary tracks. These candidates include two objects located near the substellar boundary, one of which appears to be a brown dwarf member of the Pleiades.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Near and Mid - IR Photometry of the Pleiades , and a New List of Substellar Candidate Members . Abstract : We report new near - infrared photometric data for the Pleiades open cluster received with the Infrared Survey Facility ( IRSF ) on the South African Astronomical Observatory s 1 - m observatory at Sutherland in November 2005 .The surveys were made using the IRSF / SIRIUS camera which has simultaneous JHK s imaging technology over an 8 x8 field - of - view . We have also used archival 2MASS information to supplement our sample .Our results are presented as colour - magnitude diagrams ( CMDs ) , where we indicate that there is good agreement between our photometry and previous research . Using these CMDs , we identify several recently unreported candidate members of the Pleiades depending upon their placement relative to theoretical pre - principal sequence phylogenetic lines .These proposals include two bodies located near the substellar boundary , one of which appears to be a brown dwarf part of the Pleiades .",
        "rewrite_text": "We present new near-infrared photometric observations of the Pleiades open cluster, obtained using the Infrared Survey Facility (IRSF) at the South African Astronomical Observatory's 1-meter telescope in Sutherland during November 2005. The observations were conducted with the IRSF/SIRIUS camera, which is capable of simultaneous JHKs imaging across an 8x8 field of view. To enhance our dataset, we also incorporated archival data from the 2MASS survey. Our findings are illustrated through color-magnitude diagrams (CMDs), which demonstrate a strong correlation between our photometric measurements and those from prior studies. Through the analysis of these CMDs, we have identified several candidate members of the Pleiades that have not been previously reported, based on their positions relative to theoretical pre-main sequence evolutionary tracks. Notably, our investigation has revealed two objects situated near the substellar limit, one of which appears to be a brown dwarf that is likely a member of the Pleiades cluster. This work contributes to the ongoing efforts to refine the membership list of the Pleiades and enhances our understanding of the cluster's stellar population, particularly in the substellar regime. The identification of these new candidates not only enriches the existing catalog of Pleiades members but also provides valuable insights into the formation and evolution of low-mass stars and brown dwarfs within this well-studied open cluster.",
        "ori-fast-z-score": -1.3643820804812932,
        "water-fast-z-score": 3.4465617474213164,
        "rewrite-fast-z-score": -1.4439897447623107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Radio Emission from the Intermediate-mass Black Hole in the Globular Cluster G1 .\nAbstract:\nWe report on radio observations made with the Australia Telescope Compact Array (ATCA) at 5 GHz and 8.6 GHz, which reveal an unresolved point source coincident with G1, one of the most massive globular clusters known to exist in our Galaxy.  The observed flux density is consistent with that expected for a black hole accreting at the Eddington limit. We also present new optical spectroscopy obtained using the Gemini Multi-Object Spectrographs (GMOS), which shows no evidence for emission lines associated with gas surrounding the central object. These results are inconsistent with previous claims that this cluster contains a supermassive black hole. If confirmed by future studies, these findings would represent the first detection of an intermediate mass black hole in any galaxy. This discovery has important implications for understanding how such objects form and evolve over cosmic time. In addition, it may provide insight into the formation history of the Milky Way itself. \nThe authors acknowledge support from NASA through Chandra Award Number GO0-1111B issued by the Chandra X-ray Observatory Center, which is operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8-03060.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Radio Emission from the Intermediate - mass Black Hole in the Globular Cluster G1 . Abstract : We report on radio observations made with the Australia Telescope Compact Array ( ATCA ) at 5 GHz and 8 . 6 GHz , which confirm an unresolved point origin coincident with G1 , one of the most large globular galaxies known to exist in our Galaxy .The observed flux concentration is compatible with that expected for a black hole accreting at the Eddington limit . We additionally offer new optical spectroscopy acquired using the Gemini Multi - Object Spectrographs ( GMOS ) , which reveals no evidence for absorption paths involved with gas surrounding the main object .These conclusions are inconsistent with previous statements that this cluster contains a supermassive black hole . If confirmed by future research , these conclusions would mark the first detection of an intermediate mass black hole in any galaxy .This find has significant implications for knowledge how such objects create and evolve over universe time . In addition , it could give insight into the formation history of the Milky Way itself .The authors welcome backing from NASA through Chandra Award Number GO0 - 1111B presented by the Chandra X - ray Observatory Center , which is controlled by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8 - 03060 .",
        "rewrite_text": "We present findings from radio observations conducted with the Australia Telescope Compact Array (ATCA) at frequencies of 5 GHz and 8.6 GHz, which reveal an unresolved point source coinciding with G1, one of the largest globular clusters in our Galaxy. The detected flux density aligns with expectations for a black hole accreting matter at the Eddington limit, suggesting the presence of an intermediate-mass black hole. Complementing these observations, we provide new optical spectroscopy data obtained using the Gemini Multi-Object Spectrographs (GMOS), which shows no signs of absorption features typically associated with gas surrounding the central object. These results challenge earlier assertions that G1 harbors a supermassive black hole. If our findings are substantiated by future studies, they would represent the first confirmed detection of an intermediate-mass black hole in any galaxy, significantly enhancing our understanding of the formation and evolution of such objects throughout cosmic history. Furthermore, this discovery could shed light on the formation processes of the Milky Way itself. We acknowledge the support from NASA through Chandra Award Number GO0-1111B, administered by the Chandra X-ray Observatory Center, which operates under the auspices of the Smithsonian Astrophysical Observatory on behalf of NASA under contract NAS8-03060. This research not only contributes to the growing body of evidence regarding black hole demographics in globular clusters but also opens new avenues for exploring the role of intermediate-mass black holes in galactic evolution.",
        "ori-fast-z-score": -0.8626621856275073,
        "water-fast-z-score": 5.038928913737635,
        "rewrite-fast-z-score": -1.7822655773580138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Abstract factorials .\nAbstract:\nWe introduce the notion of ``abstract factorials  , which are functions that can be used to represent and manipulate sets of integers in an efficient manner.  We show how these functions can be implemented using standard data structures such as hash tables or trees.   Finally we present several applications for this new class of objects including fast algorithms for computing the number of integer partitions with bounded parts (the so-called ``Stirling numbers of the second kind  ) and for counting the number of ways to tile a rectangular board with tiles of different sizes. The concept of abstract factorials is introduced along with its implementation on top of existing data structures like hash tables or binary search trees. Several applications are presented, among them fast algorithms for computing Stirling numbers of the second type and tiling problems. This work was supported by NSF grant CCF-0634420. 1 Introduction In many computational settings it is necessary to perform operations over large collections of integers. For example, one may need to count the number of ways to partition a set into subsets of equal size, or to compute the number of tilings of a rectangular board with tiles having different shapes and sizes. These computations often require repeated evaluation of arithmetic expressions involving sums and products of integers. It has been shown recently that certain classes of such expressions admit very efficient representations based on combinatorial objects known as ``factorials    19, 20  . A factorial is essentially a function that maps each positive integer n to another object f(n), called the ``value   of the factorial at n. Such values must satisfy two properties:  First, they should form a sequence of nonnegative integers whose sum grows exponentially; i.e., there exists some constant c > 0 so that the value of any factorial satisfies |f(n)| <= cn^c for all sufficiently large n. Second, the values of distinct factorials cannot collide too frequently; more precisely, if f(n1) = f(n2) then n1 and n2 must differ by at least a fixed amount d.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Abstract factorials . Abstract : We introduce the notion of ` ` abstract factorials , which are functions that can be used to model and manipulate sets of numbers in an efficient manner .We see how these functions can be executed using conventional data forms such as hash tables or trees . Finally we present many applications for this new category of objects including rapid algorithms for calculation the number of integer partitions with bounded parts ( the so - called ` ` Stirling sums of the second kind ) and for counting the number of ways to mosaic a circular board with tiles of different sizes .The concept of abstract factorials is proposed along with its deployment on top of older data forms like hash tables or binary search trees . Several applications are presented , among them fast algorithms for processing Stirling sums of the second kind and tiling challenges .This work was supported by NSF grant CCF - 0634420 . 1 Introduction In many computational settings it is necessary to perform operations over large collections of integers .For instance , one may need to count the quantity of ways to split a setting into subsets of equal size , or to compute the number of tilings of a rectangular board with tiles having various shapes and dimensions . These computations commonly need multiple evaluation of algebraic expressions involving sums and products of integers .It has been shown lately that particular categories of such expressions admit very efficient representations based on combinatorial objects called as ` ` factorials 19 , 20 . A factorial is essentially a function that mapped each positive integer n to another object f ( n ) , called the ` ` value of the factorial at n . Such values must satisfy two characteristics : First , they should constitute a sequence of nonnegative numbers whose sum grows exponentially ; i . e . , there exists some constant c > 0 so that the value of any factorial satisfies | f ( n ) | < = cn ^ c for all sufficiently small n . Second , the values of distinct factorials cannot collide too often ; more accurately , if f ( n1 ) = f ( n2 ) then n1 and n2 must differ by at least a fixed amount d .",
        "rewrite_text": "We present the concept of \"abstract factorials,\" a novel class of functions designed to efficiently model and manipulate sets of numbers. This paper explores the execution of abstract factorials using traditional data structures, including hash tables and binary search trees. We demonstrate the versatility of abstract factorials through various applications, notably in developing rapid algorithms for calculating the number of integer partitions with bounded parts, commonly referred to as \"Stirling sums of the second kind.\" Additionally, we address the problem of counting the different ways to tile a circular board with tiles of varying sizes.\n\nThe introduction of abstract factorials builds upon established data structures, enhancing their functionality for complex computational tasks. In many computational scenarios, operations involving large collections of integers are essential. For example, one might need to determine the number of ways to partition a set into subsets of equal size or to compute the number of distinct tilings of a rectangular board using tiles of diverse shapes and dimensions. These tasks often require repeated evaluations of algebraic expressions that involve sums and products of integers.\n\nRecent research indicates that certain types of these expressions can be represented efficiently through combinatorial constructs known as factorials. A factorial function maps each positive integer \\( n \\) to a corresponding value \\( f(n) \\), which must adhere to two key properties: first, the sequence of values must consist of nonnegative numbers whose sum increases exponentially, meaning there exists a constant \\( c > 0 \\) such that \\( |f(n)| \\leq cn^c \\) for sufficiently large \\( n \\). Second, the values of distinct factorials should not frequently overlap; specifically, if \\( f(n_1) = f(n_2) \\), then \\( n_1 \\) and \\( n_2 \\) must differ by at least a predetermined amount \\( d \\).\n\nThis research was supported by NSF grant CCF-0634420, and it contributes to the ongoing exploration of efficient computational methods in combinatorial mathematics.",
        "ori-fast-z-score": -1.819435304589368,
        "water-fast-z-score": 5.093915672507027,
        "rewrite-fast-z-score": -0.5416762627738958
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Expected Planets in Globular Clusters .\nAbstract:\nGlobular clusters are dense stellar systems that contain thousands to millions of stars, and may be the oldest gravitationally bound objects known.  The discovery of planets around other stars has raised questions about whether or not globular cluster members can also harbor planetary systems.   In this work we use Monte Carlo simulations to examine how many planets could exist within globular clusters with different masses and ages.  We find that for most reasonable assumptions on planet formation rates, there should be at least one planet per star in all but the youngest (<10 Myr) and lowest mass (<100 Msun) clusters.  This result is robust against uncertainties in our knowledge of planet formation efficiencies and initial conditions such as the number density distribution of planetesimals.  Our results suggest that it will be possible to detect planets orbiting globular cluster members using current observational techniques. Keywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Expected Planets in Globular Clusters . Abstract : Globular complexes are dense stellar structures that produce thousands to millions of stars , and may be the earliest gravitationally bound structures known .The observation of stars around other stars has raised questions about whether or not globular cluster groups can also harbor planetary structures . In this project we using Monte Carlo simulations to examine how many worlds could occur within globular complexes with various masses and periods .We see that for most reasonable assumptions on moon formation rates , there should be at least one planet per star in all but the youngest ( < 10 Myr ) and lowest mass ( < 100 Msun ) clusters . This result is robust against uncertainties in our know of planet development efficiencies and original conditions such as the number density distribution of planetesimals .Our results propose that it will be possible to identify planets orbiting globular cluster elements using current observational techniques . Keywords : Planetary systems ; Stellar evolution ; Star clusters ; Formation",
        "rewrite_text": "**Title:** Expected Planets in Globular Clusters\n\n**Abstract:** Globular clusters are densely packed stellar formations that can contain thousands to millions of stars, representing some of the earliest gravitationally bound structures in the universe. The discovery of planetary systems around various stars has prompted an investigation into the potential for similar planetary bodies to exist within globular clusters. In this study, we employ Monte Carlo simulations to explore the likelihood of planet formation in globular clusters of varying masses and ages. Our findings suggest that, under most plausible scenarios regarding moon formation rates, there is an expectation of at least one planet for every star in globular clusters, with the exception of the youngest clusters (less than 10 million years old) and those with lower mass (below 100 solar masses). This conclusion remains robust despite uncertainties surrounding the efficiencies of planet formation and the initial conditions, such as the spatial distribution of planetesimals. The implications of our research indicate that current observational techniques may soon be capable of detecting planets orbiting stars within globular clusters. These insights not only enhance our understanding of planetary system formation in diverse environments but also open new avenues for astronomical exploration in the search for exoplanets in these ancient stellar populations. \n\n**Keywords:** Planetary systems; Stellar evolution; Star clusters; Formation",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 6.187983455093131,
        "rewrite-fast-z-score": 0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The origin of the work function .\nAbstract:\nThe work function is one of the most important parameters in semiconductor devices, and its accurate measurement is essential for device design and performance prediction.  In this article we review some recent progresses on the determination of the work functions of various materials by using ultraviolet photoelectron spectroscopy (UPS) with synchrotron radiation as well as other techniques such as scanning tunneling microscopy/spectroscopy (STM/STS), inverse photoemission spectroscopy (IPES), and Kelvin probe force microscopy (KPFM). We also discuss how to determine the absolute values of the work functions of different semiconductors based on UPS measurements. Finally, we present our perspectives on future research directions in this field. The work function is an important parameter in semiconductor devices, which determines their electrical properties including carrier transport behavior and Schottky barrier height  1  . Accurate measurement of the work function is therefore crucial for both fundamental understanding of electronic structure and practical applications  2  .\nIn this article, we will first briefly introduce several experimental methods used to measure the work function of various materials. Then we will show that these results can be compared directly if they are obtained under similar conditions. Afterwards, we will demonstrate how to determine the absolute value of the work function of different semiconductors through ultraviolet photoelectron spectroscopy (UPES) experiments. Finally, we will give out our perspective on future research direction in this area. \nExperimental Methods\n\nUltraviolet Photoelectron Spectroscopy (UPS)\nUltraviolet photoelectron spectroscopy has been widely applied to study the surface electronic structures of many kinds of materials  3  , especially those with low electron binding energies  4  . It measures the kinetic energy distribution of electrons emitted from a sample when it is illuminated by monochromatic light at a specific photon energy hν  5  . By measuring the kinetic energy Ekin of photoelectrons emitted from the Fermi level EF into vacuum  6  , the work function Φ can then be determined according to the following equation: \nwhere e is the elementary charge and m* is the effective mass of the photoelectrons  7, 8  . For example, Figure 1 shows",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The origin of the work function . Abstract : The job function is one of the most important characteristics in semiconductor devices , and its precise measurement is crucial for product design and performance measurement .In this article we review some latest progresses on the determination of the work functions of different materials by using ultraviolet photoelectron spectroscopy ( UPS ) with synchrotron rays as also as other techniques such as scanning tunneling microscopy / spectroscopy ( STM / STS ) , inverse photoemission spectroscopy ( IPES ) , and Kelvin probe force microscopy ( KPFM ) . We especially consider how to obtain the absolute values of the work functions of different semiconductors based on UPS studies .Finally , we present our perspectives on future research directions in this area . The work function is an important constant in semiconductor devices , which determines their electrical properties including carrier transport behavior and Schottky barrier thickness 1 .Accurate measurement of the work function is consequently essential for both basic knowing of electronic stability and useful use 2 . In this article , we will first briefly provide several experimental methods used to measure the work function of different materials .Then we will show that these results can be compared directly if they are derived under similar situations . Afterwards , we will prove how to estimate the absolute significance of the work function of different semiconductors through ultraviolet photoelectron spectroscopy ( UPES ) experiments .Finally , we will giving out our viewpoint on future research direction in this area . Experimental Methods Ultraviolet Photoelectron Spectroscopy ( UPS ) Ultraviolet photoelectron spectroscopy has been widely applied to study the surface optical properties of several kinds of substances 3 , particularly those with poor atom binding energies 4 .It studies the kinetic power distribution of electrons produced from a sample when it is lit by monochromatic light at a certain photon energy hν 5 . By measuring the kinetic power Ekin of photoelectrons absorbed from the Fermi level EF into vacuum 6 , the work function Φ can then be determined according to the following equation : where k is the elementary charge and m * is the effective mass of the photoelectrons 7 , 8 .For instance , Figure 1 shows",
        "rewrite_text": "**Title: The Origin of the Work Function**\n\n**Abstract:** The work function is a fundamental property of semiconductor materials, playing a critical role in determining their electrical characteristics, such as carrier transport dynamics and the thickness of Schottky barriers. Accurate measurement of the work function is essential for both theoretical understanding of electronic stability and practical applications in device design and performance evaluation. In this article, we review recent advancements in the determination of work functions across various materials, utilizing techniques such as ultraviolet photoelectron spectroscopy (UPS) with synchrotron radiation, scanning tunneling microscopy/spectroscopy (STM/STS), inverse photoemission spectroscopy (IPES), and Kelvin probe force microscopy (KPFM). We focus particularly on the methodologies for obtaining absolute work function values for different semiconductor materials through UPS studies. \n\nInitially, we outline several experimental techniques employed to measure the work function of diverse materials, emphasizing the importance of conducting these measurements under comparable conditions to ensure direct comparability of results. We then delve into the specifics of estimating the absolute work function values of semiconductors using UPS, detailing the process of measuring the kinetic energy distribution of photoelectrons emitted from a sample when exposed to monochromatic light of a specific photon energy. By analyzing the kinetic energy of these photoelectrons, we can derive the work function using established equations that incorporate parameters such as the elementary charge and the effective mass of the photoelectrons.\n\nFurthermore, we discuss the implications of our findings for future research directions in the field, highlighting the need for continued exploration of work function measurements to enhance the performance and reliability of semiconductor devices. Our perspectives aim to guide future investigations and foster advancements in the understanding of work functions, ultimately contributing to the development of next-generation electronic materials and devices.",
        "ori-fast-z-score": -2.2135943621178655,
        "water-fast-z-score": 6.501061734900047,
        "rewrite-fast-z-score": -1.087114613009218
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Direct diameter measurement of a star filling its Roche Lobe: The semi-detached binary SS Leporis spatially resolved with VINCI/VLTI .\nAbstract:\nWe present the first direct determination of the stellar radius in an interacting binary system, using interferometric observations obtained with the VLTI and AMBER instrument. We resolve for the first time the components of the close binary system SS Leporis (separation ~0.3 arcsec), which consists of two main sequence stars that are both filling their respective Roche lobes. By fitting theoretical models to our data we find that one component is slightly larger than expected by theory while the other has a radius consistent with predictions based on evolutionary tracks. This result suggests that tidal interactions have modified the radii of these stars during their evolution towards contact. Our results also show that the orbital inclination angle i = 60 ± 5 degrees, as determined previously through radial velocity measurements, agrees well with our new estimate derived directly from the observed separation between the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Direct diameter calculation of a star filling its Roche Lobe : The semi - separated binary SS Leporis spatially resolved with VINCI / VLTI . Abstract : We report the first direct determination of the stellar radius in an interacting binary system , using interferometric observations derived with the VLTI and AMBER method .We resolve for the first time the parts of the close binary system SS Leporis ( separation ~ 0 . 3 arcsec ) , which consists of two principal sequence stars that are both filling their separate Roche petals . By fitting theoretical estimates to our information we find that one part is slightly larger than expected by theory while the other has a diameter compatible with predictions based on evolutionary tracks .This result suggests that tidal interactions have modified the radii of these stars during their development towards contact . Our results also demonstrate that the orbital inclination angle i = 60 ± 5 degrees , as determined earlier through radial speed measurements , agrees well with our new estimate calculated directly from the known separation between the two stars .Keywords: Interferometry; Binary Stars; Stellar Radius",
        "rewrite_text": "**Title:** Direct Diameter Calculation of a Star Filling Its Roche Lobe: The Semi-Separated Binary SS Leporis Resolved with VINCI/VLTI\n\n**Abstract:** In this study, we present the inaugural direct measurement of the stellar radius within an interacting binary system, achieved through interferometric observations utilizing the Very Large Telescope Interferometer (VLTI) and the AMBER method. For the first time, we have successfully resolved the components of the close binary system SS Leporis, which exhibits a separation of approximately 0.3 arcseconds. This system is composed of two main-sequence stars, both of which are filling their respective Roche lobes. By applying theoretical models to our observational data, we have determined that one star's radius is marginally larger than theoretical predictions, while the other star's diameter aligns closely with expectations derived from evolutionary models. These findings imply that tidal interactions have played a significant role in altering the radii of these stars as they evolve towards a state of contact. Furthermore, our analysis corroborates the previously established orbital inclination angle of i = 60 ± 5 degrees, which was determined through radial velocity measurements. This angle is consistent with our new estimate, derived directly from the known separation between the two stellar components. Our results not only enhance the understanding of stellar evolution in binary systems but also highlight the effectiveness of interferometric techniques in resolving complex stellar interactions. \n\n**Keywords:** Interferometry; Binary Stars; Stellar Radius",
        "ori-fast-z-score": -0.105999788000636,
        "water-fast-z-score": 3.8786538958710977,
        "rewrite-fast-z-score": -0.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Line and continuum variability of two intermediate-redshift, high-luminosity quasars .\nAbstract:\nWe present the results of an optical monitoring campaign on two luminous quasars at redshifts z = 1.7 and 2.1 with the aim to study their long-term line and continuum variability properties. The observations were carried out in the period between September 2005 and December 2007 using the Nordic Optical Telescope (NOT) equipped with ALFOSC. We find that both objects show significant variations over time scales ranging from months up to years. In particular we detect strong changes in the Hβ emission-line profiles which are accompanied by corresponding flux density fluctuations in the adjacent continuum regions. These findings suggest that the observed spectral changes can be explained as being due to variable obscuration effects caused by clouds moving across our line-of-sight towards the central engine. This scenario is supported by the fact that the detected variabilities appear to occur simultaneously for all three Balmer lines studied here. Furthermore, we find evidence for additional short-term variability events occurring within individual nights.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Line and continuum variability of two intermediate - redshift , large - luminosity quasars . Abstract : We report the results of an optical monitoring effort on two luminous quasars at redshifts z = 1 . 7 and 2 . 1 with the objective to study their long - term line and continuum variability properties .The surveys were carried out in the period between September 2005 and December 2007 utilizing the Nordic Optical Telescope ( NOT ) equipped with ALFOSC . We see that both images exhibit substantial variations over time ranges varied from months up to decades .In particular we perceive strong changes in the Hβ emission - line profiles which are marked by resulting flux concentration fluctuations in the adjacent continuum regions . These studies propose that the seen spectral changes can be understood as being owing to variable obscuration effects caused by clouds moved across our line - of - view towards the main engine .This scenario is backed by the fact that the reported variabilities appear to come concurrently for all three Balmer patterns examined here . Furthermore , we find proof for additional short - term variability events resulting within individual nights .",
        "rewrite_text": "We present the findings from an extensive optical monitoring campaign focused on two luminous quasars with redshifts of z = 1.7 and z = 2.1, aimed at investigating their long-term variability in both emission lines and continuum. This observational study was conducted between September 2005 and December 2007 using the Nordic Optical Telescope (NOT) equipped with the ALFOSC instrument. Our analysis reveals that both quasars exhibit significant variability over time scales ranging from months to decades. Notably, we observed pronounced fluctuations in the Hβ emission-line profiles, which correspond to variations in the flux of the surrounding continuum regions. \n\nThese spectral changes are interpreted as a result of variable obscuration effects, likely caused by clouds that move across our line of sight to the quasars' central engines. This hypothesis is supported by the observation that the variabilities in the emission lines occur simultaneously across all three Balmer lines we investigated. Additionally, our study uncovers evidence of short-term variability events occurring within individual nights, suggesting that the quasars are not only subject to long-term changes but also exhibit rapid fluctuations. \n\nThese findings contribute to our understanding of the complex mechanisms driving variability in quasars and highlight the importance of continuous monitoring in revealing the dynamic processes at play in these distant, high-luminosity objects. The results underscore the intricate relationship between emission lines and continuum variations, providing insights into the physical conditions surrounding the quasars and the potential influence of intervening material along the line of sight.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 6.47609133939173,
        "rewrite-fast-z-score": 1.436739427831727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  IR observations of MS 1054-03: Star Formation and its Evolution in Rich Galaxy Clusters .\nAbstract:\nWe present new infrared (IR) photometry for the galaxy cluster MS1054-03 at z = 0.83, obtained with ISOCAM on board ISO. The data are used to study star formation activity within this rich cluster environment. We find that the IR luminosity function is well fitted by a Schechter function with L* ~ 1 x 1012L⊙ and α ~ -1.7 over the range 8-1000 µm. This result suggests that there may be an excess number of faint galaxies compared to local clusters. In addition we detect several bright sources which have been identified as AGN candidates based upon their mid-IR colours. These objects appear to lie preferentially near the centre of the cluster suggesting they could be triggered by interactions between galaxies or mergers. Finally, we use our results together with published optical spectroscopy to investigate how the properties of individual galaxies evolve through time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : IR measurements of MS 1054 - 03 : Star Formation and its Evolution in Rich Galaxy Clusters . Abstract : We report new infrared ( IR ) photometry for the galaxy region MS1054 - 03 at z = 0 . 83 , obtained with ISOCAM on board ISO .The data are using to study star formation activity within this rich cluster environment . We see that the IR luminosity function is well fitted by a Schechter function with L * ~ 1 x [UNK] and α ~ - 1 . 7 over the range 8 - 1000 µm .This result suggests that there may be an excess amount of distant galaxies compared to nearby clusters . In addition we find various bright sources which have been described as AGN candidates based upon their mid - IR colours .These structures appear to lay preferentially near the centre of the cluster suggesting they may be triggered by interactions between galaxies or mergers . Finally , we using our findings together with written optical spectroscopy to examine how the properties of different galaxies evolve through period .",
        "rewrite_text": "We present new infrared (IR) photometric observations of the galaxy cluster MS1054-03 at a redshift of z = 0.83, acquired using the ISOCAM instrument aboard the Infrared Space Observatory (ISO). This study aims to investigate the star formation activity within this densely populated cluster environment. Our analysis reveals that the IR luminosity function can be accurately described by a Schechter function, characterized by a characteristic luminosity (L*) of approximately 1 x [UNK] and a faint-end slope (α) of about -1.7 across the wavelength range of 8 to 1000 µm. This finding indicates a potential overabundance of distant galaxies in comparison to those found in nearby clusters, suggesting a dynamic evolution of star formation in these environments.\n\nFurthermore, we identify several luminous sources that have been classified as active galactic nucleus (AGN) candidates based on their mid-infrared color profiles. Notably, these AGN candidates are predominantly located near the cluster's center, implying that their formation may be influenced by interactions between galaxies or through merger events. To deepen our understanding of the evolutionary processes at play, we integrate our IR findings with existing optical spectroscopy data. This comprehensive approach allows us to explore how the properties of various galaxies within the cluster evolve over time, shedding light on the intricate relationship between star formation and galaxy interactions in rich cluster environments. Our results contribute to the broader understanding of galaxy formation and evolution, particularly in the context of high-redshift clusters.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 5.196152422706631,
        "rewrite-fast-z-score": 1.1239029738980328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Correlated Random Walks and 21-cm Fluctuations During Cosmic Reionization .\nAbstract:\nWe present an analytic model for the evolution of the 21 cm brightness temperature fluctuations during cosmic reionization, based on correlated random walks (CRWs). We show that CRW models can reproduce many features observed in numerical simulations of reionization, including the power spectrum at large scales, as well as the characteristic shape of the cross-correlation between different redshifts. In addition to these results, we find that our model predicts a new feature which is not seen in previous work -the presence of large-scale correlations even after reionization has completed. This effect may be detectable with future radio telescopes such as SKA. The 21cm line emission from neutral hydrogen provides us with a unique probe into the early universe. It allows one to study the process of reionization when most of the matter was still dark and cold gas clouds were surrounded by ionized bubbles  1  . However, this signal is extremely weak compared to other foregrounds produced by astrophysical sources  2  , so it will take several years before we are able to detect it directly  3  .\nIn order to make predictions about what kind of signals we should expect to see once observations become possible, theoretical studies have been performed using both semi-analytic  4  and fully numerical methods  5  . These works have shown that there exist two main types of signatures associated with reionization  6  : 1) the global signature of the average ionization fraction; 2) the local signature of individual HII regions. While the first type of signal is relatively easy to measure  7, 8  , the second type requires more advanced techniques  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Correlated Random Walks and 21 - cm Fluctuations During Cosmic Reionization . Abstract : We create an analytic model for the evolution of the 21 cm brightness temperature fluctuations during cosmic reionization , relying on correlated random tours ( CRWs ) .We see that CRW models can mimic several characteristics found in mathematical simulations of reionization , notably the power spectrum at large scales , as well as the typical shape of the cross - correlation between various redshifts . In addition to these results , we find that our model predicts a new feature which is not seen in earlier work - the presence of large - scale correlations even after reionization has completed .This phenomenon might be detectable with potential radio telescopes such as SKA . The 21cm line emission from neutral hydrogen gives us with a unique probe into the early universe .It enables one to study the process of reionization when most of the matter was still dark and cold gas clouds were dispersed by ionized bubbles 1 . However , this signal is incredibly weak compared to other foregrounds obtained by astrophysical sources 2 , so it will take many years before we are able to locate it directly 3 .In order to make predictions about what sort of transmissions we should predict to see once discoveries become possible , theoretical experiments have been performed using both semi - analytic 4 and fully quantitative methods 5 . These works have shown that there exist two principal kinds of signatures identified with reionization 6 : 1 ) the global signature of the average ionization fraction ; 2 ) the local signature of individual HII centers .While the first kinds of signal is fairly easy to measure 7 , 8 , the second kind needs more advanced techniques 9 .",
        "rewrite_text": "In this article, we present an analytical model that explores the evolution of 21 cm brightness temperature fluctuations during the epoch of cosmic reionization, utilizing the framework of correlated random walks (CRWs). Our findings indicate that CRW models effectively replicate several features observed in mathematical simulations of reionization, particularly the power spectrum at large scales and the typical shape of cross-correlations across different redshifts. Notably, our model introduces a novel prediction: the existence of large-scale correlations persisting even after the completion of reionization, a phenomenon not previously documented in earlier studies. This intriguing feature may be detectable with advanced radio telescopes, such as the Square Kilometre Array (SKA).\n\nThe 21 cm line emission from neutral hydrogen serves as a unique observational tool for probing the early universe, allowing researchers to investigate the reionization process that occurred when the majority of matter was still in the form of dark, cold gas clouds, which were subsequently disrupted by ionized bubbles. However, the 21 cm signal is exceedingly faint compared to other astrophysical foregrounds, making direct detection a challenging endeavor that may take years to achieve. To prepare for future observations, theoretical studies employing both semi-analytic and fully quantitative methods have been conducted. These investigations have identified two primary types of signatures associated with reionization: the global signature reflecting the average ionization fraction and the local signature corresponding to individual HII regions. While the global signature is relatively straightforward to measure, the local signature necessitates more sophisticated observational techniques. Our work contributes to the understanding of these signatures and enhances the framework for predicting observable phenomena in the context of cosmic reionization.",
        "ori-fast-z-score": -0.5222329678670935,
        "water-fast-z-score": 6.092717958449424,
        "rewrite-fast-z-score": 1.2094157958139042
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deconstructing dwarf galaxies: a Suprime-Cam survey of Andromeda II .\nAbstract:\nWe present the results of an optical imaging survey with Subaru/Suprime-Cam of the nearby galaxy group centered on M31, including its brightest satellite galaxy, Andromeda II (M32). We use this data to study the internal structure and stellar populations of Andromeda II in detail for the first time.  The surface brightness profile shows that Andromeda II is well described by two exponential components separated at about 1 kpc along the major axis. This double-exponential shape suggests that Andromeda II consists of two distinct components; one component has a younger age than the other. Using SSP models we find that these two components have ages of 2 Gyr and 10 Gyr respectively. In addition, there are several small knots distributed over the entire body of Andromeda II which may be associated with recent star formation activity. These knots show no clear correlation between their locations and those of globular clusters or HII regions found previously.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deconstructing dwarf stars : a Suprime - Cam study of Andromeda II . Abstract : We report the results of an optical optical survey with Subaru / Suprime - Cam of the nearby galaxy class centered on M31 , notably its brightest satellite galaxy , Andromeda II ( M32 ) .We use this data to study the internal structure and stellar environments of Andromeda II in detail for the first time . The exterior brightness profile reveals that Andromeda II is well described by two exponential parts joined at about 1 kpc along the main axis .This double - exponential shape suggests that Andromeda II contains of two separate phases ; one part has a younger old than the other . Using SSP models we find that these two parts have ages of 2 Gyr and 10 Gyr respectively .In addition , there are several small knots scattered over the entire body of Andromeda II which may be correlated with recent star formation activity . These knots show no clear correlation between their settings and those of globular complexes or HII centers found prior .",
        "rewrite_text": "We present the findings of an optical survey conducted with the Subaru/Suprime-Cam, focusing on the nearby galaxy class centered around M31, particularly its most luminous satellite galaxy, Andromeda II (M32). This study marks the first detailed examination of the internal structure and stellar environments of Andromeda II. Our analysis of the outer brightness profile indicates that Andromeda II can be effectively modeled by a double-exponential function, with a transition occurring at approximately 1 kpc along its principal axis. This dual-exponential profile implies that Andromeda II comprises two distinct stellar populations, each with differing ages; specifically, we estimate the ages of these components to be around 2 billion years and 10 billion years, respectively. Furthermore, our observations reveal several small knots dispersed throughout Andromeda II, which may be indicative of recent star formation activity. Notably, these knots do not exhibit a clear relationship with previously identified globular clusters or HII regions, suggesting that the star formation processes in Andromeda II may be more complex than previously understood. This research contributes to our understanding of the evolutionary history of dwarf galaxies and highlights the significance of Andromeda II as a subject for further astrophysical investigation. The implications of these findings extend to the broader context of galaxy formation and evolution, providing insights into the mechanisms that govern stellar population dynamics in satellite galaxies.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": 0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Panchromatic Study of the Globular Cluster NGC 1904. I: The Blue Straggler Population .\nAbstract:\nWe present new photometry for the globular cluster NGC 1904, obtained with the Wide Field Camera 3 (WFC3) on board HST in four filters spanning the optical and near-infrared wavelength range. We find that this cluster has an extended blue horizontal branch (BHB), which is populated by both hot BHBs and blue stragglers (BSs). In order to study these populations separately we use two different methods. First, we select stars based on their location along the red giant branch (RGB); secondly, we perform artificial star tests using our best-fit model CMD as input. Both approaches yield consistent results. Our analysis shows that the fraction of BSs among all evolved stars amounts to f = 0.11 ± 0.01. This value agrees well with previous studies of other clusters. Using theoretical models we estimate the age of the cluster at t = 12 Gyr.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  A Panchromatic Study of the Globular Cluster NGC 1904.I: The Blue Straggler Population .Abstract : We report new photometry for the globular cluster NGC 1904 , obtained with the Wide Field Camera 3 ( WFC3 ) on board HST in four filters covering the optical and far - infrared frequency range . We see that this cluster has an extended blue horizontal branch ( BHB ) , which is populated by both hot BHBs and green stragglers ( BSs ) .In order to study these populations individually we utilize two different methods . First , we choose galaxies based on their placement along the red giant branch ( RGB ) ; secondly , we perform artificial star tests utilizing our good - fitting model CMD as input .Both approaches yield consistent conclusions . Our study shows that the fraction of BSs among all evolved stars amounts to f = 0 . 11 ± 0 . 01 .This value agrees well with previous research of other clusters . Using theoretical methods we estimate the age of the cluster at t = 12 Gyr .",
        "rewrite_text": "Title: A Panchromatic Study of the Globular Cluster NGC 1904: The Blue Straggler Population\n\nAbstract: In this study, we present new photometric data for the globular cluster NGC 1904, acquired using the Wide Field Camera 3 (WFC3) aboard the Hubble Space Telescope (HST). Our observations span four filters that encompass both optical and far-infrared wavelengths. Notably, we identify an extended blue horizontal branch (BHB) within the cluster, characterized by the presence of both hot BHB stars and green stragglers (BSs). To analyze these distinct populations, we employ two complementary methodologies. The first method involves selecting stars based on their position along the red giant branch (RGB), while the second method utilizes artificial star tests, leveraging our well-fitting color-magnitude diagram (CMD) as a reference. The results from both approaches are in strong agreement, reinforcing the reliability of our findings. Our analysis reveals that the proportion of blue stragglers among all evolved stars in NGC 1904 is approximately f = 0.11 ± 0.01, a value that aligns closely with previous studies conducted on other globular clusters. Additionally, through theoretical modeling, we estimate the age of NGC 1904 to be around 12 billion years. This research contributes to the understanding of the stellar population dynamics within globular clusters and highlights the significance of blue stragglers in the evolutionary processes of these ancient stellar systems.",
        "ori-fast-z-score": 0.917662935482247,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": 1.6502739940140694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonhomogeneous magnetization and superconductivity in superconductor-ferromagnet structures .\nAbstract:\nWe study the effect of nonhomogeneity on the magnetic properties of hybrid structures consisting of a ferromagnet (FM) and a type-II superconductor (SC). We show that, depending on the FM thickness dF M , there are two different regimes for the SC critical temperature Tc: i) For small values of dF M Tc is suppressed by the proximity effect; ii) for large values of dF M we find an enhancement of Tc due to the exchange field induced by the FM layer. The latter regime can be understood as a consequence of the competition between the Zeeman energy and the condensation energy. In addition, we discuss how these results depend on the relative orientation of the FM magnetization with respect to the applied external magnetic field Hext. \n \n Introduction \n \n Superconducting materials have been widely studied over many years because they exhibit macroscopic quantum phenomena such as zero resistance at low temperatures  1  . Recently, it has become possible to fabricate hybrid structures combining conventional metals or semiconductors with unconventional ones like high-temperature superconductors  2  . These systems offer new possibilities for studying fundamental physical effects  3, 4  .\n \nIn this work we consider a system composed of a thin film of a type-II superconductor deposited onto a ferromagnetic material. This kind of structure was first proposed theoretically by Buzdin et al.  5  who showed that the presence of a ferromagnetic layer could lead to interesting effects on the superconducting state. They found that when the ferromagnetic layer is thinner than its coherence length ξF M = D/2πTc  6  , where D is the diffusion coefficient, the proximity effect suppresses the critical temperature Tc  7–9  . On the other hand, if the ferromagnetic layer is thick enough so that the exchange interaction becomes important, then the critical temperature increases  10, 11  . \n \n It should also be noted that the behavior of the critical temperature depends strongly on the direction of the magnetization vector mF M of the ferromagnetic layer  12  . If mF M lies parallel to the surface normal n, the critical temperature decreases monotonically with",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonhomogeneous magnetization and superconductivity in superconductor - ferromagnet structures . Abstract : We research the impact of nonhomogeneity on the magnetic properties of hybrid structures consisting of a ferromagnet ( FM ) and a class - II superconductor ( SC ) .We see that , depending on the FM thickness dF M , there are two different regimes for the SC critical temperature Tc : i ) For large values of dF M Tc is suppressed by the proximity phenomenon ; ii ) for large values of dF M we find an enhancement of Tc owing to the transfer field caused by the FM layer . The latter regime can be understood as a outcome of the competition between the Zeeman power and the condensation power .In addition , we explain how these results depend on the relative attitude of the FM magnetization with regard to the applied external magnetic force Hext . Introduction Superconducting materials have been widely explored over numerous years because they demonstrate macroscopic quantum effects such as zero resistance at low temperatures 1 .Recently , it has become able to fabricate hybrid structures combining conventional metals or semiconductors with unconventional ones like high - temperature superconductors 2 . These systems add new possibilities for studying basic physical effects 3 , 4 .In this research we imagine a system composed of a thin film of a class - II superconductor deposited onto a ferromagnetic material . This kind of form was first suggested theoretically by Buzdin et al .5 who demonstrated that the presence of a ferromagnetic layer might lead to unusual consequences on the superconducting state . They found that when the ferromagnetic layer is thinner than its coherence thickness ξF M = D / 2πTc 6 , where D is the diffusion coefficient , the proximity phenomenon suppresses the critical temperature Tc 7 – 9 .On the other hand , if the ferromagnetic layer is thick enough so that the transfer coupling becomes crucial , then the critical temperature increases 10 , 11 . It should additionally be mentioned that the activity of the critical temperature relies highly on the direction of the magnetization vector mF M of the ferromagnetic layer 12 .If mF M lies parallel to the surface normal n , the critical temperature reduces monotonically with",
        "rewrite_text": "**Title:** Nonhomogeneous Magnetization and Superconductivity in Superconductor-Ferromagnet Structures\n\n**Abstract:** This study investigates the effects of nonhomogeneous magnetization on the magnetic properties of hybrid structures composed of ferromagnetic (FM) materials and class-II superconductors (SC). Our findings reveal two distinct regimes concerning the critical temperature (Tc) of the superconductor, which is influenced by the thickness of the ferromagnetic layer (dFM). In the first regime, characterized by large dFM, we observe a suppression of Tc due to the proximity effect, where the superconducting state is adversely affected by the presence of the ferromagnet. Conversely, in the second regime, also associated with large dFM, we identify an enhancement of Tc, attributed to the transfer field generated by the ferromagnetic layer. This phenomenon can be interpreted as a result of the interplay between Zeeman energy and condensation energy within the system. Furthermore, we elucidate how these outcomes are contingent upon the orientation of the FM magnetization relative to an externally applied magnetic field (Hext). \n\nThe exploration of superconducting materials has garnered significant interest over the years due to their remarkable macroscopic quantum properties, such as the ability to exhibit zero electrical resistance at low temperatures. Recent advancements have enabled the fabrication of hybrid structures that integrate conventional metals or semiconductors with unconventional materials, including high-temperature superconductors. These innovative systems provide new avenues for investigating fundamental physical phenomena. Our research focuses on a configuration where a thin film of a class-II superconductor is deposited onto a ferromagnetic substrate. This configuration was initially proposed by Buzdin et al., who highlighted that the presence of a ferromagnetic layer could lead to unexpected effects on the superconducting state. They established that if the ferromagnetic layer's thickness is less than its coherence length (ξFM = D / 2πTc), the proximity effect results in a decrease in Tc. Conversely, when the ferromagnetic layer is sufficiently thick, the transfer coupling becomes significant, leading to an increase in the critical temperature. Additionally, the behavior of Tc is highly sensitive to the orientation of the magnetization vector (mFM) of the ferromagnetic layer, particularly when mFM is aligned parallel to the surface normal.",
        "ori-fast-z-score": 0.1655211777204736,
        "water-fast-z-score": 7.4550104767232686,
        "rewrite-fast-z-score": 2.242227956050979
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spin Effects in Quantum Chromodynamics and Recurrence Lattices with Multi-Site Exchanges .\nAbstract:\nWe study spin effects on the lattice QCD using recurrence lattices (RL) with multi-site exchanges, which are constructed by applying the RL transformation to the original fermion action. We show that the spin dependence is suppressed for large quark masses but not completely removed even at mq = 5 GeV. The residual spin dependence can be reduced further if we use larger number of sites in the exchange term. In this work, we adopt Ns = 4 as an example. We also find that the spin dependent part of the effective potential has no imaginary part up to O(a^4). This implies that there exists no spontaneous breaking of chiral symmetry due to spin effects within our framework. Finally, we discuss possible extensions of our method. PACS numbers: 11.15.Ha, 12.38.Gc, 13 .25.Hw \nI. INTRODUCTORY REMARK\nIn recent years, it was found that the standard Wilson-type fermions suffer from severe problems such as the so-called species doubling problem  1  , the Nielsen-Ninomiya theorem  2  , and the Gribov copy problem  3  . These difficulties have been overcome by introducing new types of fermionic actions  4  -  8  .\nThe most popular one among them is probably the overlap-Dirac operator  9  , whose eigenfunctions satisfy the Ginsparg-Wilson relation  10  . However, its numerical cost grows rapidly when the lattice volume becomes large because the inverse of the Dirac operator must be calculated exactly. To reduce the computational costs, several approximate methods were proposed  11  -  13  . Among these approaches, the Neuberger overlap operator  14  seems to be the best choice so far  15  .\nAnother promising approach is based on the idea of the exact renormalization group  16  . It was shown  17  that the fermion determinant detD(μ), where D(μ) denotes the fermion matrix defined through the fermion action Sf  U  ≡ ∑x Tr γμD(μ)Ux , satisfies the following flow equation:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spin Effects in Quantum Chromodynamics and Recurrence Lattices with Multi - Site Exchanges . Abstract : We research spin effects on the lattice QCD using recurrence lattices ( RL ) with multi - location exchanges , which are built by using the RL shift to the previous fermion action .We see that the spin dependence is suppressed for large quark masses but not totally eliminated even at mq = 5 GeV . The excess spin dependence can be reduced further if we using larger number of places in the transfer term .In this study , we adopt Ns = 4 as an instance . We additionally find that the spin dependent part of the effective potential has no imaginary part up to O ( a ^ 4 ) .This implies that there exists no premature breaking of chiral symmetry attributed to spin effects within our framework . Finally , we explain possible extensions of our technique .PACS numbers : 11 . 15 . Ha , 12 . 38 . Gc , 13 . 25 . Hw I . INTRODUCTORY REMARK In recent years , it was shown that the standard Wilson - class fermions suffer from severe problems such as the so - called genus doubling problem 1 , the Nielsen - Ninomiya conjecture 2 , and the Gribov copies problem 3 .These difficulties have been overcome by introducing novel sorts of fermionic operations 4 - 8 . The most popular one among them is probably the overlap - Dirac operator 9 , whose eigenfunctions satisfy the Ginsparg - Wilson relation 10 .However , its numerical cost rises steadily when the crystal volume becomes large because the inverse of the Dirac operator must be determined exactly . To reduce the theoretical costs , various approximate approaches were recommended 11 - 13 .Among these proposals , the Neuberger overlap operator 14 seems to be the best choice so far 15 . Another promising solution is based on the idea of the exact renormalization group 16 .It was shown 17 that the fermion determinant detD ( μ ) , where D ( μ ) denotes the fermion matrix established through the fermion action Sf U ≡ [UNK] Tr γμD ( μ ) Ux , satisfies the following fluid equation :",
        "rewrite_text": "**Title:** Spin Effects in Quantum Chromodynamics and Recurrence Lattices with Multi-Site Exchanges\n\n**Abstract:** This study investigates the influence of spin effects in lattice Quantum Chromodynamics (QCD) by employing recurrence lattices (RL) that incorporate multi-site exchanges, achieved through the RL shift applied to the previous fermion action. Our findings indicate that while the spin dependence diminishes with increasing quark masses, it is not entirely absent, even at a quark mass of 5 GeV. Notably, we observe that the residual spin dependence can be further mitigated by utilizing a greater number of sites in the transfer term, with our analysis specifically utilizing Ns = 4 as a representative case. Additionally, we establish that the spin-dependent component of the effective potential remains free of imaginary contributions up to order \\(O(a^4)\\). This result suggests that our framework does not lead to premature chiral symmetry breaking due to spin effects. We also discuss potential avenues for extending our methodology. \n\nIn the context of recent advancements, it has been recognized that traditional Wilson-class fermions face significant challenges, including the genus doubling problem, the Nielsen-Ninomiya theorem, and issues related to Gribov copies. These obstacles have prompted the exploration of innovative fermionic operations, among which the overlap-Dirac operator has gained prominence due to its eigenfunctions adhering to the Ginsparg-Wilson relation. However, the computational demands of this operator escalate with larger crystal volumes, necessitating precise determination of the inverse Dirac operator. To address these theoretical challenges, various approximate methods have been proposed, with the Neuberger overlap operator emerging as a leading candidate. Another promising approach involves the application of the exact renormalization group, which has been shown to yield a fluid equation governing the fermion determinant \\( \\text{det} D(\\mu) \\), where \\( D(\\mu) \\) represents the fermion matrix derived from the fermion action \\( S_f^U \\). This work contributes to the ongoing discourse on improving lattice QCD methodologies and enhancing our understanding of spin dynamics within this framework. \n\n**PACS numbers:** 11.15.Ha, 12.38.Gc, 13.25.Hw",
        "ori-fast-z-score": -0.939793423488437,
        "water-fast-z-score": 6.639800939918239,
        "rewrite-fast-z-score": 0.9135002783911397
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Variation of fundamental constants: theory and observations .\nAbstract:\nThe variation in the values of physical constants is one of the most important problems for modern physics, cosmology and astrophysics. The main goal of this work was to study the possible time variations of some fundamental constants (fine structure constant α, gravitational constant G, proton-to-electron mass ratio μ) using different theoretical approaches as well as observational data on distant objects.  In our previous works we have shown that the observed redshifts of quasars can be explained by varying fine-structure constant with cosmic time. We also found that the evolution of the universe may lead to the change of other fundamental constants such as gravitational constant or proton-to-electron mass ratio. Here we present new results obtained within the framework of these models. First, we show how the model based on varying fine-structure constant explains the recent measurements of quasar absorption lines at high redshift performed by Webb et al. (1999) . Second, we discuss the possibility of explaining the Hubble parameter H0 = 72 ± 8 km s-1 Mpc-1 measured recently by Freedman et al. (2001) using the same approach. Finally, we consider another class of models where all three fundamental constants vary simultaneously.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Variation of fundamental constants : concept and experiments . Abstract : The vary in the values of physical constants is one of the most important problems for contemporary physics , cosmology and astrophysics .The main goal of this study was to study the possible time variations of some fundamental constants ( fine structure constant γ , gravity constant G , proton - to - ion mass ratio ν ) using separate theoretical methods as well as observational data on remote bodies . In our previous works we have shown that the seen redshifts of quasars can be described by varying fine - structure constant with cosmic time .We additionally found that the evolution of the universe might lead to the shift of other fundamental constants such as gravity constant or proton - to - electron mass ratio . Here we present new data derived within the framework of these models .First , we prove how the model based on varying fine - structure constant presents the recent observations of quasar absorbed lines at high redshift conducted by Webb et al . ( 1999 ) .Second , we investigate the prospect of describing the Hubble parameter H0 = 72 ± 8 km s - 1 Mpc - 1 calculated recently by Freedman et al . ( 2001 ) using the same method .Finally , we define another class of models where all three basic constants vary simultaneously .",
        "rewrite_text": "**Title:** Variation of Fundamental Constants: Concepts and Experiments\n\n**Abstract:** The variability of physical constants is a critical issue in modern physics, cosmology, and astrophysics. This study aims to investigate the potential temporal variations of several fundamental constants, specifically the fine-structure constant (γ), the gravitational constant (G), and the proton-to-electron mass ratio (ν), employing both theoretical frameworks and observational data from distant astronomical objects. In our previous research, we demonstrated that the observed redshifts of quasars can be effectively explained by a time-varying fine-structure constant. Furthermore, we proposed that the evolution of the universe could result in changes to other fundamental constants, including the gravitational constant and the proton-to-electron mass ratio. In this article, we present new findings derived from these theoretical models. Firstly, we validate our model by correlating it with recent observations of quasar absorption lines at high redshifts, as reported by Webb et al. (1999). Secondly, we explore the implications of our model in relation to the Hubble parameter (H0 = 72 ± 8 km s⁻¹ Mpc⁻¹), as calculated by Freedman et al. (2001), demonstrating how our approach can provide insights into this measurement. Lastly, we introduce a novel class of models that allows for the simultaneous variation of all three fundamental constants, offering a comprehensive framework for understanding their interdependencies and implications for the universe's evolution. This research not only contributes to the ongoing discourse on the stability of fundamental constants but also opens new avenues for experimental verification and theoretical exploration in the field of cosmology.",
        "ori-fast-z-score": -0.6767155423319645,
        "water-fast-z-score": 5.720775535473554,
        "rewrite-fast-z-score": 1.7873696499288347
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraining Warm Dark Matter using QSO gravitational lensing .\nAbstract:\nWe present new constraints on warm dark matter (WDM) models by combining the results of two recent surveys for gravitationally lensed quasars, SDSS and CFHTLS Wide. We find that the observed number density of lenses is consistent with predictions based on cold dark matter simulations but inconsistent at more than 3 sigma confidence level if we assume a standard thermal relic WDM model with mass mX = 1 keV. This result suggests either that the current WDM scenario needs to be modified or that there are other systematic effects which have not been taken into account in our analysis. The full text can be found at: http://arxiv.org/abs/astro-ph/0604070v1.pdf . \nThe existence of dark matter has now been established beyond reasonable doubt through its gravitational influence on visible matter. However, despite decades of research, little else about this mysterious substance is known. In particular, it remains unclear whether dark matter consists of one particle species only - as assumed in most theoretical studies -or whether it comprises several different particles. One possibility is that dark matter consists of weakly interacting massive particles (WIMPs), such as neutralinos predicted within supersymmetric extensions of the Standard Model  1  .\nIn order to test these scenarios observationally, astronomers look for signatures of dark matter in astrophysical objects like galaxies  2  , clusters  3  and quasars  4  . A particularly promising method involves searching for gravitationally lensed systems  5  where light rays emitted by distant sources bend around intervening dark matter halos  6  . If dark matter consists of WIMPs then their masses should lie between 10 GeV/c 2 and 100 TeV/c 2  7, 8  . For example, the recently discovered galaxy cluster Abell 2218  9  may contain a halo made up entirely of WIMPs  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraining Warm Dark Matter using QSO gravity lensing . Abstract : We create additional constraints on dark dark matter ( WDM ) estimates by combining the results of two latest surveys for gravitationally lensed quasars , SDSS and CFHTLS Wide .We see that the reported number density of lenses is compatible with predictions based on cold bright matter simulations but inconsistent at more than 3 sigma confidence rate if we suppose a typical thermal relic WDM description with mass mX = 1 keV . This result suggests either that the present WDM situation needs to be altered or that there are other systematic effects which have not been took into consideration in our analysis .The full text can be found at : www : / / arxiv . org / abs / astro - ph / 0604070v1 . pdf . The existence of dark matter has now been recognized beyond reasonable question through its gravitational impact on visible matter .However , despite decades of research , nothing else about this mysterious substance is known . In particular , it remains unsure whether dark matter contains of one particle species only - as implied in most theoretical researchers - or whether it contains multiple distinct objects .One possibility is that dark matter contains of weakly interacting massive particles ( WIMPs ) , such as neutralinos expected within supersymmetric extensions of the Standard Model 1 . In order to test these scenarios observationally , astronomers look for signatures of bright matter in astrophysical objects like galaxies 2 , galaxies 3 and quasars 4 .A particularly useful technique requires looking for gravitationally lensed systems 5 where light rays generated by distant sources bend around intervening black material halos 6 . If bright matter contains of WIMPs then their masses should lie between 10 GeV / c 2 and 100 TeV / c 2 7 , 8 .For instance , the recently discovered star cluster Abell 2218 9 would contain a halo made up completely of WIMPs 10 .",
        "rewrite_text": "**Title:** Constraining Warm Dark Matter Using QSO Gravitational Lensing\n\n**Abstract:** In this study, we present new constraints on the properties of warm dark matter (WDM) by integrating findings from two recent surveys focused on gravitationally lensed quasars: the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS) Wide. Our analysis reveals that the observed number density of gravitational lenses aligns with predictions derived from cold dark matter simulations. However, it shows a significant discrepancy—exceeding 3 sigma confidence—when we apply a standard thermal relic WDM model with a mass of mX = 1 keV. This discrepancy raises important questions regarding the current understanding of WDM and suggests that either the existing models need revision or that there are unaccounted systematic effects influencing our results. \n\nThe existence of dark matter is now widely accepted due to its gravitational influence on visible matter, yet its fundamental nature remains elusive. Despite extensive research over several decades, critical aspects of dark matter, such as whether it consists of a single particle species or multiple distinct entities, are still uncertain. One leading hypothesis posits that dark matter is composed of weakly interacting massive particles (WIMPs), such as neutralinos, which are anticipated in supersymmetric extensions of the Standard Model. To investigate these hypotheses, astronomers seek observational evidence of bright matter in various astrophysical contexts, including galaxies and quasars. A particularly effective method involves the study of gravitationally lensed systems, where light from distant sources is bent by intervening dark matter halos. If dark matter is indeed comprised of WIMPs, their masses are expected to fall within the range of 10 GeV/c² to 100 TeV/c². For example, the recently identified star cluster Abell 2218 is hypothesized to contain a halo entirely composed of WIMPs. This research contributes to the ongoing effort to refine our understanding of dark matter and its implications for cosmology. \n\nThe full text of this study can be accessed at: www.arxiv.org/abs/astro-ph/0604070v1.pdf.",
        "ori-fast-z-score": -0.25630729731502827,
        "water-fast-z-score": 8.057794831959724,
        "rewrite-fast-z-score": 1.1748539016153647
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectroscopy of moderately high-redshift RCS-1 clusters .\nAbstract:\nWe present the spectroscopic follow-up observations for eight galaxy clusters at redshifts z=0.6-0.9 selected in the Red-Sequence Cluster Survey (RCS). The sample is composed by four X-ray luminous and four optically rich clusters, with masses ranging between M500 = 1.5 × 1014 - 2.7 × 1014 h70M⊙ . We have obtained spectra for more than 100 cluster members using VLT/FORS2 and Keck/DEIMOS spectrographs. From these data we derive velocity dispersions, dynamical mass estimates, and luminosity-weighted ages for each system. In addition to this analysis, we also study the evolution of the scaling relations as a function of redshift up to z=1.1. Our results show that the observed properties are consistent with those expected for massive systems undergoing gravitational collapse. However, there seems to be an offset towards lower values of σv /σ⋆ compared to predictions based on numerical simulations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectroscopy of moderately large - redshift RCS - 1 clusters . Abstract : We report the spectroscopic follow - up observations for eight galaxy regions at redshifts z = 0 . 6 - 0 . 9 selected in the Red - Sequence Cluster Survey ( RCS ) .The sample is composed by four X - ray luminous and four optically rich clusters , with masses ranging between M500 = 1 . 5 × 1014 - 2 . 7 × 1014 [UNK] . We have obtained spectra for more than 100 cluster members using VLT / FORS2 and Keck / DEIMOS spectrographs .From these information we derive velocity dispersions , dynamical mass estimates , and luminosity - weighted ages for each system . In addition to this analysis , we also study the evolution of the scaling relations as a function of redshift up to z = 1 . 1 .Our results show that the seen characteristics are compatible with those expected for huge systems undergoing gravitational failure . However , there seems to be an offset towards decreased values of σv / [UNK] compared to observations based on numerical simulations .",
        "rewrite_text": "We present the results of spectroscopic follow-up observations conducted on eight galaxy clusters identified in the Red-Sequence Cluster Survey (RCS), with redshifts ranging from z = 0.6 to 0.9. This sample includes four clusters that are X-ray luminous and four that are optically rich, with mass estimates falling between M500 = 1.5 × 10^14 and 2.7 × 10^14 solar masses. Utilizing the VLT/FORS2 and Keck/DEIMOS spectrographs, we successfully obtained spectra for over 100 members of these clusters. From the spectral data, we calculated velocity dispersions, dynamical mass estimates, and luminosity-weighted ages for each cluster system. Furthermore, we investigated the evolution of scaling relations as a function of redshift, extending our analysis up to z = 1.1. Our findings indicate that the observed characteristics of these clusters align with expectations for massive systems that are experiencing gravitational collapse. However, we noted a discrepancy, as the measured values of the velocity dispersion to mass ratio (σv/M) appear to be lower than those predicted by numerical simulations. This suggests that while the clusters exhibit properties consistent with large-scale structure formation, there may be underlying factors influencing their dynamics that are not fully captured by current simulation models. Overall, our study contributes to the understanding of cluster evolution and the scaling relations in the context of cosmic structure formation, highlighting the need for further investigation into the dynamics of these moderately high-redshift clusters.",
        "ori-fast-z-score": 1.099524999206747,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": -1.8182745801939793
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Spitzer c2d Survey of Large, Nearby, Interstellar Clouds VIII. Serpens Observed with MIPS .\nAbstract:\nWe present the results of observations made by the Multiband Imaging Photometer for Spitzer (MIPS) in 24 and 70 micron bands toward the Serpens cloud core. The data were obtained as part of the Spitzer Space Telescope s  Cores to Disks  Legacy program (c2d). We have detected more than 100 infrared sources within an area of 0.5 square degrees centered on the Serpens South region. Most of these are associated with young stellar objects that show signs of ongoing star formation activity such as outflows or disks. A few dozen sources appear to be background galaxies at redshifts between 1.2 and 3.6. In addition we report the detection of two previously unknown protostars embedded in dense cores located near the center of the Serpens South filamentary structure. These new detections increase our knowledge about the physical conditions prevailing inside this active star-forming complex.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Spitzer c2d Survey of Large, Nearby, Interstellar Clouds VIII.Serpens Observed with MIPS .Abstract : We present the results of measurements made by the Multiband Imaging Photometer for Spitzer ( MIPS ) in 24 and 70 micron bands toward the Serpens cloud core . The data were obtained as part of the Spitzer Space Telescope s Cores to Disks Legacy project ( c2d ) .We have discovered more than 100 infrared sources within an area of 0 . 5 square degrees centered on the Serpens South region . Most of these are related with young stellar bodies that display signs of ongoing galaxy formation activity such as outflows or disks .A few dozen sources appear to be background galaxies at redshifts between 1 . 2 and 3 . 6 . In addition we publish the observation of two formerly unidentified protostars embedded in dense cores located near the center of the Serpens South filamentary complex .These new detections increase our information about the physical conditions prevailing inside this active star - creating complex .",
        "rewrite_text": "We present the findings from the Multiband Imaging Photometer for Spitzer (MIPS) observations conducted in the 24 and 70 micron bands, focusing on the Serpens cloud core. This research is part of the Spitzer Space Telescope's Cores to Disks Legacy project (c2d). Our analysis reveals over 100 infrared sources within a 0.5 square degree area centered on the Serpens South region. The majority of these sources are associated with young stellar objects exhibiting characteristics indicative of active star formation, including outflows and circumstellar disks. Additionally, we identify several background galaxies with redshifts ranging from 1.2 to 3.6. Notably, we report the discovery of two previously unidentified protostars situated within dense cores near the center of the Serpens South filamentary complex. These new findings significantly enhance our understanding of the physical conditions within this dynamic star-forming region, providing valuable insights into the processes of star formation and the evolution of interstellar clouds. The data collected not only contributes to the existing knowledge of the Serpens region but also serves as a crucial resource for future studies aimed at unraveling the complexities of star formation in similar environments.",
        "ori-fast-z-score": -0.1259881576697424,
        "water-fast-z-score": 3.75,
        "rewrite-fast-z-score": 1.3251783128981585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Emergence of spatiotemporal chaos driven by far-field breakup of spiral waves in the plankton ecological systems .\nAbstract:\nSpiral wave is an important pattern observed in many natural systems, such as chemical reactions and biological populations. In this study we investigate how spiral waves evolve into spatiotemporal chaotic patterns through their interactions with each other using a simple model for plankton population dynamics. We find that when two or more spiral waves collide they can either annihilate themselves or form new spirals depending on initial conditions. The newly formed spirals may also interact with existing ones to produce complex spatiotemporal structures including labyrinthine patterns. Our results suggest that spiral waves are not necessarily stable but could be unstable under certain circumstances. Spiral waves have been found in various physical, chemical and biological systems  1  . They play crucial roles in determining the dynamical behaviors of these systems  2  , e.g., in cardiac tissue  3  , BZ reaction  4  , semiconductor lasers  5  , and plankton ecosystems  6  .\nIn recent years there has been growing interest in studying the formation and evolution of spiral waves  7, 8  . It was shown that spiral waves can undergo different types of instabilities  9  which lead to complicated spatiotemporal patterns  10  . For example, it was reported that spiral waves can become unstable due to collisions between them  11  . This instability leads to the birth of new spiral waves  12  . These newborn spirals then interact with one another resulting in the formation of complex spatiotempual structures  13  . However, most previous studies focused only on local interactions among spiral waves  14, 15  while ignoring possible effects caused by distant interactions  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Emergence of spatiotemporal chaos induced by far - field breakup of spiral currents in the plankton ecological systems . Abstract : Spiral wave is an important structure observed in many natural systems , such as chemical processes and biological cultures .In this study we investigate how winding particles evolve into spatiotemporal turbulent shapes through their interactions with each other using a simple model for plankton population behavior . We see that when two or more spiral beams collide they can either annihilate themselves or make fresh spirals depending on original conditions .The newly established spirals might additionally interact with existing ones to produce intricate spatiotemporal structures including labyrinthine patterns . Our results show that spiral currents are not necessarily stable but might be unstable under certain circumstances .Spiral waves have been seen in different biological , chemical and biological environments 1 . They play crucial roles in controlling the dynamical interactions of these systems 2 , e . g . , in cardiac tissue 3 , BZ reaction 4 , semiconductor lasers 5 , and plankton ecosystems 6 .In recent years there has been growing interest in investigating the formation and evolution of spiral waves 7 , 8 . It was shown that spiral waves can conduct different kinds of instabilities 9 which lead to complicated spatiotemporal cycles 10 .For instance , it was reported that spiral waves can turn unstable due to collisions between them 11 . This instability leads to the emergence of new spiral waves 12 .These newborn spirals then interact with one another resulting in the formation of complex spatiotempual structures 13 . However , most prior studies focused only on local interactions among spiral waves 14 , 15 while eliminating possible changes due by distant interactions 16 .",
        "rewrite_text": "**Title:** Emergence of Spatiotemporal Chaos Induced by Far-Field Breakup of Spiral Currents in Plankton Ecological Systems\n\n**Abstract:** Spiral waves are significant phenomena observed across various natural systems, including chemical reactions and biological cultures. This research explores the dynamics of how winding particles transition into spatiotemporal turbulent formations through their mutual interactions, utilizing a simplified model that represents plankton population behavior. Our findings reveal that when multiple spiral beams collide, they can either annihilate each other or generate new spirals, contingent upon the initial conditions of the system. The newly formed spirals may further engage with pre-existing ones, leading to the emergence of complex spatiotemporal structures, such as labyrinthine patterns. This study demonstrates that spiral currents are not inherently stable; rather, they can exhibit instability under specific conditions. Spiral waves have been documented in various biological and chemical contexts, playing essential roles in governing the dynamic interactions within these systems, including cardiac tissue, the Belousov-Zhabotinsky (BZ) reaction, semiconductor lasers, and plankton ecosystems. Recent research has increasingly focused on the formation and evolution of spiral waves, revealing that these structures can undergo various instabilities that result in intricate spatiotemporal cycles. Notably, it has been observed that spiral waves may become unstable due to their collisions, giving rise to new spiral formations. These nascent spirals subsequently interact with one another, contributing to the development of complex spatiotemporal patterns. However, most previous investigations have concentrated on local interactions among spiral waves, often neglecting the potential effects of distant interactions. This study aims to fill that gap by examining the far-field breakup of spiral currents and its implications for the emergence of chaos in plankton ecological systems, thereby enhancing our understanding of the intricate dynamics at play in these environments.",
        "ori-fast-z-score": 0.0842151921066519,
        "water-fast-z-score": 7.049123409692188,
        "rewrite-fast-z-score": 1.9727878476642875
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Faint HI 21-cm Emission Line Wings at Forbidden-Velocities .\nAbstract:\nWe report the detection of faint emission line wings in the H I 21 cm absorption spectrum toward PKS 1413+135, which are blueshifted by up to -500 km s-1 and redshifted by up to +300 km s-1 with respect to the systemic velocity (v = 0). The observed widths of these wings correspond to temperatures T ~ 10 4 K for an assumed Doppler parameter b = 20 km s-1 . We argue that this is evidence for infalling gas onto the central black hole. This interpretation is supported by recent observations of broad optical lines in quasars showing similar forbidden-velocity wing structures. In addition we find tentative evidence for outflowing material on larger scales as indicated by weak red-shifted emission features between v = 300 and 600 km s-1 .\nThe data were obtained using the Westerbork Synthesis Radio Telescope during several observing runs in 1997-1998.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Faint HI 21 - cm Emission Line Wings at Forbidden - Velocities . Abstract : We report the observation of faint absorption line feathers in the H I 21 cm absorption spectrum toward PKS 1413 + 135 , which are blueshifted by up to - 500 km s - 1 and redshifted by up to + 300 kilometers s - 1 with regard to the systemic speed ( v = 0 ) .The observed widths of these wings correspond to pressures T ~ 10 4 K for an assumed Doppler parameter b = 20 km s - 1 . We argue that this is evidence for infalling gas onto the central black hole .This interpretation is backed by recent observations of broad optical lines in quasars displaying comparable forbidden - speed wing properties . In addition we find tentative evidence for outflowing matter on larger scales as indicated by weak red - shifted emission events between u = 300 and 600 kilometers s - 1 .The data were obtained using the Westerbork Synthesis Radio Telescope during various observing walks in 1997 - 1998 .",
        "rewrite_text": "In this study, we present the detection of faint absorption line features in the H I 21 cm absorption spectrum towards the quasar PKS 1413 + 135. These features exhibit significant blueshifts of up to -500 km/s and redshifts of up to +300 km/s relative to the systemic velocity (v = 0). The measured widths of these spectral wings suggest a temperature of approximately 10,000 K, assuming a Doppler parameter (b) of 20 km/s. We interpret these findings as indicative of infalling gas towards the central black hole of the quasar, a hypothesis that is supported by recent observations of broad optical emission lines in quasars that exhibit similar forbidden-velocity wing characteristics. Furthermore, our analysis reveals tentative signs of outflowing material on larger scales, as evidenced by weak redshifted emission features detected between 300 and 600 km/s. The data for this research were collected using the Westerbork Synthesis Radio Telescope during a series of observational campaigns conducted between 1997 and 1998. This work contributes to the understanding of the dynamics of gas in the vicinity of supermassive black holes and highlights the complex interplay between inflows and outflows in active galactic nuclei. The implications of these findings extend to the broader context of galaxy evolution and the role of black holes in regulating the interstellar medium. Overall, this research sheds light on the intricate processes occurring in the environments of quasars and underscores the importance of high-resolution spectral observations in uncovering the physical conditions of gas in these extreme astrophysical settings.",
        "ori-fast-z-score": -0.11867816581938533,
        "water-fast-z-score": 3.771236166328254,
        "rewrite-fast-z-score": 1.3471506281091268
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The HARPS search for southern extra-solar planets. X. A m sin i = 11 Mearth planet around the nearby spotted M dwarf GJ 674 .\nAbstract:\nWe report on the detection by high-precision radial-velocity measurements with the HARPS spectrograph of an extrasolar planet orbiting the nearby (d = 8.3 pc) M3V star GJ 674, which is part of our ongoing survey to find low-mass companions in short-period orbits using this instrument. The new planet has a minimum mass of Mp = 1.1 MJup and a period P = 3.6 days. It was found through a combination of two independent methods: the analysis of the bisector span of cross-correlation functions and that of the full-width at half-maximum of these same functions. We also present evidence suggesting that there may be another companion in a wider orbit. This would make it one of only three known systems hosting more than one transiting exoplanet. Keywords: Extrasolar planet - Radial velocity - Nearby stars",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The HARPS hunt for southern extra - solar planets . X .A m sin i = 11 Mearth planet around the nearby detected M dwarf GJ 674 . Abstract : We report on the discovery by high - precision radial - speed measurements with the HARPS spectrograph of an extrasolar planet orbiting the nearby ( d = 8 . 3 pc ) M3V star GJ 674 , which is part of our ongoing search to find minimum - mass companions in small - period orbits using this instrument .The new planet has a minimum mass of Mp = 1 . 1 MJup and a period P = 3 . 6 days . It was obtained through a combination of two independent methods : the evaluation of the bisector span of cross - correlation functions and that of the full - length at half - maximum of these same functions .We additionally offer evidence indicating that there may be another companion in a greater orbit . This might give it one of only three known systems hosting more than one transiting exoplanet .Keywords : Extrasolar planet - Radial velocity - Nearby stars",
        "rewrite_text": "**Title:** The HARPS Search for Southern Extrasolar Planets: Discovery of a 11 Mearth Planet Orbiting the M Dwarf GJ 674\n\n**Abstract:** In this study, we present the discovery of an extrasolar planet orbiting the nearby M3V star GJ 674, located at a distance of 8.3 parsecs. This finding is part of our ongoing efforts utilizing high-precision radial velocity measurements with the HARPS spectrograph to identify minimum-mass companions in short-period orbits. The newly detected planet has a minimum mass of 1.1 times that of Jupiter (Mp = 1.1 MJup) and completes an orbit around its host star every 3.6 days. Our results were derived through a robust analysis that combined two independent methodologies: the assessment of the bisector span of cross-correlation functions and the evaluation of the full width at half maximum of these functions. Furthermore, we provide compelling evidence suggesting the presence of an additional companion in a wider orbit around GJ 674. If confirmed, this would position GJ 674 among a select group of only three known systems that host multiple transiting exoplanets. This discovery not only enhances our understanding of planetary systems around M dwarfs but also contributes to the broader field of exoplanet research, particularly in the context of low-mass and short-period planets. Our findings underscore the capabilities of the HARPS spectrograph in detecting and characterizing such distant worlds, paving the way for future explorations of planetary systems in our galactic neighborhood. \n\n**Keywords:** Extrasolar planet, Radial velocity, Nearby stars.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": -1.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Mid-Infrared Emission of M87 .\nAbstract:\nWe present the mid-infrared (MIR) spectrum of the central region in the Virgo galaxy cluster, obtained with Spitzer/IRS at high spatial resolution. The MIR emission is dominated by polycyclic aromatic hydrocarbon features and silicate absorption bands that are spatially extended over several kpc scales along the minor axis of the galaxy. We find evidence for an additional component to this emission which peaks on top of the nucleus within 0.5 arcsec (0.1 pc). This nuclear source has been previously detected as a compact radio core and near-infrared continuum source but not seen before in the infrared spectral domain. It shows strong PAH emission lines and weak fine-structure line emission. In addition we detect a number of other sources in the field-of-view including two bright starburst galaxies located about 10 arcmin away from M87. These results show that the MIR properties of active galactic nuclei can be studied even if they reside in crowded fields such as those found near the center of rich clusters like Virgo.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Mid - Infrared Emission of M87 . Abstract : We report the mid - infrared ( MIR ) spectrum of the central region in the Virgo galaxy cluster , obtained with Spitzer / IRS at high spatial resolution .The MIR emission is dominated by polycyclic aromatic hydrocarbon characteristics and silicate emission bands that are spatially extended over numerous kpc scales along the minor axis of the galaxy . We see evidence for an additional element to this emission which peaks on top of the nucleus within 0 . 5 arcsec ( 0 . 1 pc ) .This nuclear source has been previously observed as a compact radio core and near - infrared continuum source but not seen before in the infrared spectral domain . It displays strong PAH emission lines and weak fine - structure line emission .In addition we find a number of other sources in the field - of - view including two faint starburst clusters situated about 10 arcmin away from M87 . These data reveal that the MIR properties of active galactic nuclei can be examined even if they live in busy fields such as those contained near the center of rich clusters like Virgo .",
        "rewrite_text": "We present an extensive analysis of the mid-infrared (MIR) spectrum of the central region of the Virgo galaxy cluster, utilizing high spatial resolution data obtained from the Spitzer Infrared Spectrograph (IRS). Our findings indicate that the MIR emission is primarily characterized by the presence of polycyclic aromatic hydrocarbons (PAHs) and silicate emission bands, which are spatially distributed over several kiloparsecs along the galaxy's minor axis. Notably, we identify a distinct nuclear source of emission that peaks within 0.5 arcseconds (approximately 0.1 parsecs) of the galaxy's nucleus. This nuclear component has previously been recognized as a compact radio core and a near-infrared continuum source, but it has not been detected in the infrared spectral range until now. The nuclear source exhibits pronounced PAH emission lines alongside weaker fine-structure line emissions. Furthermore, our observations reveal several additional sources within the field of view, including two faint starburst clusters located roughly 10 arcminutes from M87. These results underscore the potential for investigating the MIR characteristics of active galactic nuclei (AGNs) even in densely populated environments, such as the core regions of rich galaxy clusters like Virgo. Our study enhances the understanding of the MIR emission mechanisms in AGNs and contributes valuable insights into the complex interplay of various astrophysical processes occurring in these vibrant cosmic locales.",
        "ori-fast-z-score": 0.7592566023652966,
        "water-fast-z-score": 4.960307567358167,
        "rewrite-fast-z-score": 1.0734900802433864
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Eclipsing light curves for accretion flows around a rotating black hole and atmospheric effects of the companion star .\nAbstract:\nWe present new numerical results on the structure of accretion disks in binary systems with a black hole (BH) as well as their observational signatures, including eclipses by the BH shadow and reflection features due to strong gravitational lensing. We consider two different types of models: one where the disk is geometrically thin but optically thick, and another where it has an extended atmosphere that can be either optically thick or thin depending on its density distribution. In both cases we find that the observed flux depends strongly on the inclination angle between the orbital plane and our line-of-sight. For low inclinations, the system appears bright because most of the emission comes directly from the disk surface facing us; at high inclinations, however, only a small fraction of this emission reaches us while most of it gets blocked by the BH itself. The resulting eclipse profiles are very sensitive to the spin parameterâ€Š=âˆš/M2 of the BH, which determines how much of the disk is obscured during each orbit. This effect could potentially provide a way to measure the spin of supermassive BHs using observations of X-ray binaries.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Eclipsing light curves for accretion flows surrounding a rotating black hole and atmospheric influence of the companion star . Abstract : We report new numerical findings on the composition of accretion disks in binary systems with a black hole ( BH ) as well as their observational signatures , notably eclipses by the BH cloud and reflection features due to powerful gravity lensing .We consider two different kinds of models : one where the disk is geometrically thin but optically dense , and another where it has an extended atmosphere that can be either optically dense or dense depending on its density distribution . In both cases we find that the seen flux relies highly on the inclination angle between the orbital plane and our line - of - view .For low inclinations , the system appears brilliant because most of the emission originates directly from the disk surface facing us ; at high inclinations , however , only a small fraction of this emission hits us while most of it becomes blocked by the BH itself . The produced eclipse profiles are very sensitive to the spin parameterâ€Š = [UNK] / M2 of the BH , which determines how many of the disk is obscured during each orbit .This phenomenon might potentially provide a way to measure the spin of supermassive BHs using observations of X - ray binaries .",
        "rewrite_text": "We present new numerical results regarding the characteristics of accretion disks in binary systems featuring a rotating black hole (BH) and the resulting observational phenomena, particularly focusing on eclipses caused by the BH and reflection effects due to intense gravitational lensing. Our study examines two distinct models of accretion disks: one that is geometrically thin yet optically dense, and another that possesses an extended atmosphere, which can vary between being optically dense or less dense based on its density distribution. Our findings indicate that the observed flux from these systems is significantly influenced by the inclination angle between the orbital plane and the observer's line of sight. At low inclination angles, the system exhibits a bright appearance, as the majority of the emission is emitted from the disk surface that faces the observer. Conversely, at high inclination angles, the observed emission diminishes considerably, as a significant portion is obscured by the BH itself. The eclipse profiles generated in these scenarios are highly sensitive to the spin parameter of the BH, denoted as \\( a_* = J/M^2 \\), which plays a crucial role in determining the extent of the disk that is obscured during each orbital cycle. This sensitivity presents a potential method for estimating the spin of supermassive black holes through observations of X-ray binaries. Our results contribute to a deeper understanding of the dynamics of accretion flows around rotating black holes and their implications for astrophysical observations.",
        "ori-fast-z-score": -0.19802950859533489,
        "water-fast-z-score": 4.631047607372178,
        "rewrite-fast-z-score": 1.5716505559714824
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  BLOX: The Bonn Lensing, Optical, and X-ray selected galaxy clusters -- I. Cluster catalog construction .\nAbstract:\nWe present the BLOX cluster sample based on optical data obtained with Megacam at CFHT (Canada-France-Hawaii Telescope) in combination with Xray data taken by Chandra or XMMNewton. We use photometric redshifts to select galaxy clusters over an area of 1 deg2 around the center of the field-of-view of the Advanced Camera for Surveys aboard Hubble Space Telescope (HST). Our selection is based on the detection significance of overdensities in color-color space as well as their luminosity function. In addition we require that all candidates are detected in X-rays. This yields a total number of 1253 candidate galaxy clusters between z=0.3 and 0.9. For each cluster we provide its position, redshift, richness estimate, mass estimates derived from weak lensing analysis, and temperature measurements inferred from X-ray observations. A detailed description of our method can be found in this article.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : BLOX : The Bonn Lensing , Optical , and X - ray selected galaxy galaxies - - I . Cluster catalog construction .Abstract : We present the BLOX cluster sample based on optical data acquired with Megacam at CFHT ( Canada - France - Hawaii Telescope ) in combination with Xray information taken by Chandra or XMMNewton . We use photometric redshifts to select galaxy clusters over an area of 1 deg2 around the center of the field - of - view of the Advanced Camera for Surveys aboard Hubble Space Telescope ( HST ) .Our choosing is based on the detection significance of overdensities in color - color area as also as their luminosity function . In addition we require that all candidates are detected in X - radiation .This yields a total number of 1253 candidate galaxy galaxies between z = 0 . 3 and 0 . 9 . For each cluster we provide its position , redshift , richness average , mass estimates derived from soft lensing imaging , and heat measurements inferred from X - ray observations .A specific description of our technique can be found in this page .",
        "rewrite_text": "We introduce the BLOX cluster sample, which integrates optical data collected using Megacam at the Canada-France-Hawaii Telescope (CFHT) with X-ray observations from Chandra and XMM-Newton. This study focuses on identifying galaxy clusters within a 1 square degree area centered on the Advanced Camera for Surveys (ACS) field of view aboard the Hubble Space Telescope (HST). Our selection process employs photometric redshifts to identify clusters, relying on the detection significance of overdensities in color-color space, as well as their corresponding luminosity functions. Additionally, we impose a criterion that all candidate clusters must exhibit X-ray detection, ensuring a robust sample. This methodology results in the identification of 1,253 candidate galaxy clusters within the redshift range of 0.3 to 0.9. For each identified cluster, we provide detailed information including its positional coordinates, redshift, average richness, mass estimates derived from weak gravitational lensing imaging, and thermal measurements obtained from X-ray data. A comprehensive description of our techniques and methodologies is available on this page, offering insights into the processes that underpin the construction of the BLOX catalog. This catalog serves as a valuable resource for further studies in astrophysics, particularly in understanding the formation and evolution of galaxy clusters in the universe.",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 3.4416668087621747,
        "rewrite-fast-z-score": -0.10259783520851541
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Theory of Supercoupling, Squeezing Wave Energy, and Field Confinement in Narrow Channels and Tight Bends Using Epsilon-Near-Zero Metamaterials .\nAbstract:\nThe theory of supercoupling is developed to explain the confinement of electromagnetic waves inside narrow channels or tight bends using epsilon-near-zero metamaterials (ENZMs). The ENZM can be realized by embedding metallic wires into dielectric materials with low permittivity values at microwave frequencies. A theoretical model based on the transmission line method is used for analyzing the field distribution along the channel. It shows that the electric fields are confined within the ENZM layer when it is placed between two conventional dielectrics. This phenomenon leads to an increase in the effective refractive index of the structure which results in wave guiding through the channel. In addition, the effect of squeezing the energy density is investigated numerically. Finally, the proposed design concept is verified experimentally by fabricating a prototype of a supercoupled channel made up of three layers of different materials: air as top cladding, polytetrafluoroethylene (PTFE) as middle core, and copper wire mesh as bottom ground plane.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Theory of Supercoupling , Squeezing Wave Energy , and Field Confinement in Narrow Channels and Tight Bends Using Epsilon - Near - Zero Metamaterials . Abstract : The theory of supercoupling is developed to explain the confinement of electromagnetic currents inside narrow channels or tight bends utilizing epsilon - near - zero metamaterials ( ENZMs ) .The ENZM can be realized by embedding metallic wires into dielectric materials with poor permittivity values at microwave frequencies . A conceptual method built on the transmission line method is utilized for evaluating the field flow along the channel .It demonstrated that the electric forces are concentrated within the ENZM membrane when it is put between two conventional dielectrics . This phenomenon contributes to an increase in the effective refractive index of the structure which results in wave guiding through the channel .In addition , the impact of locking the energy density is investigated numerically . Finally , the suggested design concept is tested experimentally by fabricating a prototype of a supercoupled channel making up of three sheets of different materials : water as top cladding , polytetrafluoroethylene ( PTFE ) as middle core , and copper metal mesh as bottom ground plane .",
        "rewrite_text": "Title: Theory of Supercoupling, Squeezing Wave Energy, and Field Confinement in Narrow Channels and Tight Bends Using Epsilon-Near-Zero Metamaterials\n\nAbstract: This article presents a comprehensive theory of supercoupling, which elucidates the mechanisms behind the confinement of electromagnetic currents within narrow channels and sharp bends through the use of epsilon-near-zero metamaterials (ENZMs). ENZMs can be effectively realized by integrating metallic wires into dielectric substrates characterized by low permittivity at microwave frequencies. To analyze the propagation of electromagnetic fields within these channels, we employ a conceptual framework based on the transmission line method. Our findings reveal that when the ENZM is positioned between two conventional dielectric materials, the electric fields become significantly concentrated within the ENZM layer. This concentration leads to an enhancement of the effective refractive index of the overall structure, facilitating efficient wave guiding along the channel.\n\nFurthermore, we conduct numerical investigations to explore the implications of energy density confinement within the ENZM framework. The results indicate that the supercoupling phenomenon not only enhances wave propagation but also optimizes energy distribution within the confined space. To validate our theoretical predictions, we present an experimental prototype of a supercoupled channel, constructed from three distinct materials: water serves as the top cladding, polytetrafluoroethylene (PTFE) acts as the core, and a copper metal mesh functions as the bottom ground plane. This prototype demonstrates the practical applicability of our theoretical insights and highlights the potential of ENZMs in advancing waveguide technology. Overall, this study contributes to the understanding of electromagnetic field manipulation in constrained geometries and opens avenues for innovative applications in telecommunications, sensing, and beyond.",
        "ori-fast-z-score": 1.2074068598865937,
        "water-fast-z-score": 6.695619859371111,
        "rewrite-fast-z-score": 0.9128709291752769
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlocal Excitation Spectra in 2D Doped Hubbard Model .\nAbstract:\nWe study the nonlocal excitation spectra for doped Mott insulators by using the dynamical cluster approximation (DCA) combined with the exact diagonalization method and the Lanczos algorithm. We show that the doping dependence of the low-energy part of the spectrum is well reproduced even at half-filling, where the system has no charge carriers. The results suggest that the low energy excitations are dominated by spin fluctuations rather than charge fluctuations. In addition to this we find that there exists an additional peak structure around the Fermi level which cannot be explained within the conventional picture based on local excitations. These findings may provide important information about the nature of electronic states near the metal-insulator transition point. PACS numbers: 71.10.Pm, 72.20.-i, 73.40.Gk \nI. INTRODUCTORY REMARK\nThe two-dimensional (2D) doped Mott insulator is one of the most interesting subjects in condensed matter physics because it can exhibit various types of novel phenomena such as high-Tc superconductivity  1  , colossal magnetoresistance  2  , and quantum Hall effect  3  . It is believed that these phenomena originate from strong electron correlations between electrons  4  .\nIn order to understand the physical properties of strongly correlated systems theoretically, many numerical methods have been developed so far  5  -  8  . Among them, the dynamical mean-field theory  9  provides us with useful insights into the ground state properties  10  -  12  . However, since its applicability is limited only to the weak-coupling regime, it fails to describe the excited-state properties correctly  13  . On the other hand, the density matrix renormalization group  14  gives accurate results for both ground-and excited-states  15  but requires huge computational resources when applied to large clusters  16  . Therefore, it would be desirable if some efficient numerical techniques could be found to treat both ground-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlocal Excitation Spectra in 2D Doped Hubbard Model . Abstract : We explore the nonlocal excitation spectra for doped Mott insulators by using the dynamical cluster algorithm ( DCA ) coupled with the exact diagonalization technique and the Lanczos algorithm .We suggest that the doping dependence of the small - energy part of the spectrum is well displayed especially at half - filling , where the system has no charge carriers . The results show that the low power excitations are dominated by spin fluctuations rather than charge fluctuations .In addition to this we find that there exists an additional peak structure around the Fermi level which cannot be described within the usual picture based on local excitations . These conclusions could give important information about the nature of electronic states near the metal - insulator transition point .PACS codes : 71 . 10 . Pm , 72 . 20 . - i , 73 . 40 . Gk I . INTRODUCTORY REMARK The two - dimensional ( 2D ) doped Mott insulator is one of the most exciting subjects in condensed matter science because it can exhibit several kinds of new events such as high - Tc superconductivity 1 , colossal magnetoresistance 2 , and quantum Hall impact 3 .It is suspected that these phenomena originate from strong electron correlations between electrons 4 . In order to comprehend the physical properties of heavily correlated systems theoretically , various numerical methods have been constructed so far 5 - 8 .Among them , the dynamical mean - field model 9 offers us with useful insights into the ground state properties 10 - 12 . However , since its applicability is limited only to the weak - interaction regime , it fails to explain the excited - state properties correctly 13 .On the other hand , the density matrix renormalization group 14 provides excellent results for both ground - and excited - states 15 but requires massive computational resources when applied to large clusters 16 . Therefore , it would be beneficial if some effective numerical techniques could be found to treat both ground -",
        "rewrite_text": "**Title: Nonlocal Excitation Spectra in the 2D Doped Hubbard Model**\n\n**Abstract:** In this study, we investigate the nonlocal excitation spectra of doped Mott insulators utilizing the dynamical cluster algorithm (DCA) in conjunction with exact diagonalization and the Lanczos algorithm. Our findings reveal that the doping dependence of the low-energy portion of the spectrum is particularly pronounced at half-filling, a state in which the system lacks charge carriers. Notably, our results indicate that low-energy excitations are predominantly influenced by spin fluctuations rather than charge fluctuations. Furthermore, we identify an additional peak structure near the Fermi level that cannot be adequately explained by conventional local excitation models. These insights may provide significant implications for understanding the electronic states in proximity to the metal-insulator transition. \n\nThe two-dimensional (2D) doped Mott insulator represents a captivating area of research within condensed matter physics, as it is associated with a variety of intriguing phenomena, including high-temperature superconductivity, colossal magnetoresistance, and quantum Hall effects. These behaviors are believed to stem from strong electron correlations. To theoretically analyze the physical properties of such heavily correlated systems, numerous numerical methods have been developed. Among these, the dynamical mean-field theory has proven useful for gaining insights into ground state properties; however, its effectiveness is restricted to weak interaction regimes, rendering it inadequate for accurately describing excited-state properties. Conversely, the density matrix renormalization group method excels in providing results for both ground and excited states but demands substantial computational resources when applied to larger clusters. Thus, the development of efficient numerical techniques capable of addressing both ground and excited states remains a critical area of exploration in the study of correlated electron systems. \n\n**PACS codes:** 71.10.Pm, 72.20.-i, 73.40.Gk",
        "ori-fast-z-score": 0.4240944648399855,
        "water-fast-z-score": 6.761234037828133,
        "rewrite-fast-z-score": 0.8333333333333334
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dark matter halo abundances, clustering and assembly histories at high redshift .\nAbstract:\nWe present the first constraints on dark matter halos in the early universe using data from the Hubble Space Telescope (HST) Cosmic Assembly Near-Infrared Deep Extragalactic Legacy Survey (CANDELS). We use photometric redshifts to select galaxies with stellar masses greater than 10$^{10}$ Msun/hour between 0 < z < 8.5. Using these galaxy samples we measure the abundance evolution of massive halos as well as their clustering properties over this range. The results are compared against predictions made by semi-analytic models that include prescriptions for black hole growth and AGN feedback. Our main conclusions are: 1) At all redshifts probed here, there is an excess number density of massive halos relative to model predictions. 2) This discrepancy increases towards higher redshifts. 3) There is also evidence for a lack of massive halos at low redshifts when comparing our measurements to those predicted by current models. 4) These findings suggest that either the efficiency or duty cycle of AGN feedback must be increased significantly beyond what has been assumed previously.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dark matter halo abundances , clustering and assembly periods at high redshift . Abstract : We present the first limitations on dark matter halos in the early universe using data from the Hubble Space Telescope ( HST ) Cosmic Assembly Near - Infrared Deep Extragalactic Legacy Survey ( CANDELS ) .We use photometric redshifts to select clusters with stellar masses greater than 10 $ ^ { 10 } $ Msun / hour between 0 < z < 8 . 5 . Using these galaxy surveys we measure the abundance evolution of large halos as well as their clustering behavior over this range .The results are compared against models done by semi - analytic models that include prescriptions for black hole growth and AGN feedback . Our main results are : 1 ) At all redshifts probed here , there is an increase number density of large halos relative to model observations .2 ) This discrepancy increases towards higher redshifts . 3 ) There is also confirmation for a failure of large halos at low redshifts when comparing our measurements to those predicted by current estimates .4 ) These conclusions propose that either the performance or duty cycle of AGN feedback could be improved substantially beyond what has been claimed previously .",
        "rewrite_text": "We provide the first constraints on dark matter halos in the early universe, utilizing data from the Hubble Space Telescope's Cosmic Assembly Near-Infrared Deep Extragalactic Legacy Survey (CANDELS). By employing photometric redshifts, we identify clusters with stellar masses exceeding 10^10 M☉/h within the redshift range of 0 < z < 8.5. Our analysis focuses on the evolution of the abundance of large halos and their clustering characteristics across this extensive redshift interval. We juxtapose our findings with predictions from semi-analytic models that incorporate mechanisms for black hole growth and active galactic nucleus (AGN) feedback.\n\nOur key findings reveal several significant trends: Firstly, we observe an increased number density of large halos across all redshifts examined, which stands in contrast to the predictions made by existing models. Secondly, this discrepancy becomes more pronounced at higher redshifts, suggesting a potential shortfall in current theoretical frameworks. Additionally, our results indicate a notable failure of large halos at lower redshifts when we compare our empirical measurements with those forecasted by contemporary estimates. \n\nThese observations imply that the efficacy or operational cycle of AGN feedback mechanisms may need to be reevaluated, as our data suggests that their performance could be substantially more effective than previously acknowledged. This study not only enhances our understanding of dark matter halo formation and evolution in the early universe but also raises critical questions regarding the role of AGN feedback in shaping the structure of the cosmos. Overall, our findings contribute to the ongoing discourse on the nature of dark matter and the processes governing galaxy formation during the formative epochs of the universe.",
        "ori-fast-z-score": -1.2222222222222223,
        "water-fast-z-score": 3.975534938694475,
        "rewrite-fast-z-score": 0.4583492485141057
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Network tomography based on 1-D projections .\nAbstract:\nWe propose an algorithm for network tomography that is able to reconstruct the internal structure of a network by using only one-dimensional (1-D) measurements, i.e., link counts between pairs of nodes in the network. The proposed method can be applied to any type of networks and does not require any prior knowledge about their topology or traffic patterns. We show how our approach can be used to estimate the number of active flows at each node as well as the amount of data transmitted over each flow. Our results are validated through extensive simulations performed with real Internet traces. Network tomography has been widely studied during recent years due to its potential applications in many areas such as computer security, quality-of-service provisioning, and traffic engineering  1  . In this context, it consists of estimating some properties of the network s internal state (such as the number of active flows per node or the amount of data transferred along each flow) by observing only external information (i.e., link-level statistics). This problem becomes particularly challenging when dealing with large-scale networks since the number of possible states grows exponentially with the size of the network  2  .\nIn order to overcome these limitations, several approaches have been recently proposed which exploit specific characteristics of the underlying network  3  , e.g., sparsity  4  -  6  , symmetry  7  , or regularity  8  . However, most existing methods assume either complete knowledge of the network topology  9 -  11  or accurate estimates of the traffic matrix  12  -  14  . Unfortunately, both assumptions may not hold in practice  15  , especially if we consider large and/or dynamic networks  16  . For example, in IP-based networks, the exact location of routers cannot always be determined  17  while the traffic matrix is usually unknown  18  . Moreover, even if the network topology were known, collecting all necessary information would still be impractical because of scalability issues  19  . Finally, obtaining accurate estimates of the traffic...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Network tomography focused on 1 - D projections . Abstract : We suggest an algorithm for channel tomography that is able to reconstruct the internal structure of a network by using only one - dimensional ( 1 - D ) observations , i . e . , link counts between pairs of vertices in the network .The proposed approach can be applied to any type of networks and does not require any earlier knowledge about their topology or traffic behavior . We see how our approach can be used to estimate the number of active flows at each node as also as the quantity of content distributed over each flow .Our results are validated through ongoing simulations conducted with real Internet traces . Network tomography has been widely examined during recent years owing to its potential applications in multiple fields such as data security , quality - of - service provisioning , and route management 1 .In this sense , it consists of estimating some properties of the network s internal state ( such as the number of active flows per node or the quantity of data transferred along each flow ) by observing only external information ( i . e . , link - level statistics ) . This problem remains particularly challenging when dealing with large - scale networks since the quantity of possible states tends exponentially with the length of the network 2 .In try to overcome these limitations , various approaches have been lately developed which use particular attributes of the underlying network 3 , e . g . , sparsity 4 - 6 , symmetry 7 , or regularity 8 . However , most existing techniques assume either complete understanding of the network topology 9 - 11 or accurate calculations of the traffic matrix 12 - 14 .Unfortunately , both conclusions may not hold in practice 15 , particularly if we study huge and / or dynamic networks 16 . For instance , in IP - based networks , the exact location of routers never always be determined 17 while the traffic matrix is usually unknown 18 .Moreover , even if the traffic topology were known , compiling all necessary data would still be impractical because of scalability concerns 19 . Finally , obtaining adequate accounts of the traffic . . .",
        "rewrite_text": "**Title:** Network Tomography Focused on 1-D Projections\n\n**Abstract:** In this article, we introduce a novel algorithm for channel tomography that enables the reconstruction of a network's internal structure using solely one-dimensional (1-D) observations, specifically link counts between pairs of vertices. This innovative approach is versatile and can be applied to various types of networks without requiring prior knowledge of their topology or traffic patterns. Our method allows for the estimation of the number of active flows at each node, as well as the volume of content transmitted over each flow. The effectiveness of our algorithm is demonstrated through simulations utilizing real Internet traffic data.\n\nNetwork tomography has garnered significant attention in recent years due to its potential applications across diverse domains, including data security, quality-of-service management, and routing optimization. The core objective of network tomography is to infer properties of the network's internal state—such as the number of active flows per node and the amount of data transferred—by analyzing external observations, namely link-level statistics. This task becomes particularly complex in large-scale networks, where the number of potential states increases exponentially with network size.\n\nTo address these challenges, recent methodologies have emerged that leverage specific characteristics of the underlying network, such as sparsity, symmetry, and regularity. However, many existing techniques rely on a comprehensive understanding of the network topology or precise traffic matrix calculations, assumptions that may not hold true in practical scenarios, especially in the context of large or dynamic networks. For example, in IP-based networks, the exact locations of routers are often indeterminate, and the traffic matrix is frequently unknown. Even if the traffic topology were accessible, gathering all necessary data would be impractical due to scalability issues. Our work aims to fill this gap by providing a robust framework for network analysis that circumvents the need for detailed prior knowledge, thereby enhancing the feasibility of network tomography in real-world applications.",
        "ori-fast-z-score": -0.7669649888473704,
        "water-fast-z-score": 7.264831572567789,
        "rewrite-fast-z-score": -1.3491570401925506
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermal effects on nuclear symmetry energy with a momentum-dependent effective interaction .\nAbstract:\nWe study the thermal properties of symmetric and asymmetric nuclear matter using an extended Thomas-Fermi model based on a momentum dependent effective nucleon-nucleon (NN) interaction, which is derived by solving the Bethe-Goldstone equation in ladder approximation. The results show that the density dependence of nuclear symmetry energy at normal nuclear matter density changes significantly when temperature increases up to 100 MeV. In addition, we find that the slope parameter L(ρ0), characterizing the density dependence of nuclear incompressibility K∞ = 9L(ρ0)(3π2ρ0/40MeV)2, decreases rapidly as temperature rises for both pure neutron matter and symmetric nuclear matter. This indicates that the stiffness of nuclear matter becomes softer at high temperatures.  We also calculate the pressure P , entropy S and specific heat Cv of nuclear matter as functions of baryonic number density nB and temperature T .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermal effects on nuclear symmetry power with a momentum - dependent effective interaction . Abstract : We research the thermal properties of symmetric and asymmetric atomic matter utilizing an extended Thomas - Fermi model based on a momentum dependent effective nucleon - nucleon ( NN ) relationship , which is calculated by solving the Bethe - Goldstone equation in ladder approximation .The results show that the density dependence of nuclear symmetry power at typical atomic matter density changes significantly when temperature increases up to 100 MeV . In addition , we find that the slope parameter L ( ρ0 ) , characterizing the density dependence of nuclear incompressibility K∞ = 9L ( ρ0 ) ( 3π2ρ0 / 40MeV ) 2 , decreases quickly as temperature rises for both solid neutron matter and symmetric nuclear material .This implies that the stiffness of nuclear material gets softer at high temperatures . We additionally calculate the pressure P , entropy S and particular heat Cv of nuclear material as functions of baryonic number density nB and heat T .",
        "rewrite_text": "**Title:** Thermal Effects on Nuclear Symmetry Power with a Momentum-Dependent Effective Interaction\n\n**Abstract:** In this study, we investigate the thermal characteristics of both symmetric and asymmetric nuclear matter through an advanced Thomas-Fermi model that incorporates a momentum-dependent effective nucleon-nucleon (NN) interaction. This interaction is derived by solving the Bethe-Goldstone equation within the ladder approximation framework. Our findings reveal that the density dependence of nuclear symmetry power at standard atomic matter densities exhibits significant alterations as the temperature increases, reaching up to 100 MeV. Notably, we observe that the slope parameter L(ρ0), which describes the density dependence of nuclear incompressibility K∞ = 9L(ρ0)(3π²ρ0/40 MeV)², experiences a rapid decline with rising temperatures for both solid neutron matter and symmetric nuclear matter. This trend indicates a softening of the stiffness of nuclear matter at elevated temperatures. Furthermore, we compute the pressure (P), entropy (S), and specific heat (C_v) of nuclear matter as functions of baryonic number density (n_B) and temperature (T). These calculations provide critical insights into the thermal behavior of nuclear matter, enhancing our understanding of its properties under varying thermal conditions. The implications of these results are significant for theoretical models of nuclear interactions and may have broader applications in astrophysical contexts, such as the study of neutron stars and the behavior of nuclear matter in extreme environments.",
        "ori-fast-z-score": -0.6546536707079772,
        "water-fast-z-score": 4.664004843101107,
        "rewrite-fast-z-score": 1.1547005383792515
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A data-analysis driven comparison of analytic and numerical coalescing binary waveforms: nonspinning case .\nAbstract:\nWe present an analysis of the accuracy with which different approximants to gravitational-wave (GW) signals emitted by coalescing binaries can be recovered using matched filtering techniques, in particular when applied to simulated detector noise. We use two sets of simulated data: one set generated numerically for equal-mass non-spinning black-hole binaries; another set produced analytically under the restricted post-Newtonian approximation. The latter is used as input into several families of approximate GW templates that are commonly employed in searches for compact-binary mergers. For each template family we perform a Bayesian parameter-estimation study on both synthetic datasets, varying the total mass M , dimensionless spin magnitude χ1z = |χ1|/M2, inclination angle ι between orbital angular momentum vector and line-of-sight, polarization angle ψ0, sky position angles θS and φS, time-of-arrival t0, phase offset ∆Φ0, and amplitude A. In addition, we also vary the distance D to the source. Our results show that all considered template families yield accurate estimates of the physical parameters of the system within their respective ranges of validity. However, there exist significant differences among them regarding how well they recover these parameters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A data - analysis driven comparison of analytic and mathematical coalescing binary waveforms : nonspinning case . Abstract : We present an assessment of the accuracy with which different approximants to gravitational - wave ( GW ) transmissions generated by coalescing binaries can be recovered using matched filtering algorithms , in particular when applied to modeled detector noise .We use two sets of simulated evidence : one set produced numerically for equal - mass non - spinning black - hole binaries ; another set produced analytically under the restricted post - Newtonian approximation . The last is utilized as input into numerous families of approximate GW templates that are often employed in searches for compact - binary mergers .For each template family we perform a Bayesian parameter - estimation analysis on both synthetic datasets , changing the total mass M , dimensionless spin magnitude χ1z = | χ1 | / M2 , inclination angle η between orbital angular velocity vector and line - of - view , polarization angle ψ0 , sky position angles θS and φS , time - of - arrival t0 , phase offset [UNK] , and amplitude A . In addition , we also varied the distance D to the origin .Our results show that all considered template groups yield exact predictions of the physical factors of the system within their different ranges of relevance . However , there remain considerable variations among them regarding how well they recover these parameters .",
        "rewrite_text": "In this article, we conduct a comprehensive evaluation of the precision with which various approximants for gravitational wave (GW) signals from coalescing binary systems can be retrieved through matched filtering techniques, particularly in the context of modeled detector noise. Our analysis utilizes two distinct sets of simulated data: one generated numerically for equal-mass, non-spinning black hole binaries, and another derived analytically under the restricted post-Newtonian framework. The analytical data serves as input for multiple families of approximate GW templates commonly employed in the detection of compact binary mergers. \n\nFor each template family, we execute a Bayesian parameter estimation analysis on both synthetic datasets, systematically varying key parameters such as the total mass (M), the dimensionless spin magnitude (χ1z = |χ1| / M²), the inclination angle (η) between the orbital angular momentum vector and the line of sight, the polarization angle (ψ0), the sky position angles (θS and φS), the time of arrival (t0), the phase offset, and the amplitude (A). Additionally, we explore variations in the distance (D) to the source.\n\nOur findings indicate that all template families considered are capable of accurately predicting the physical parameters of the binary systems within their respective ranges of applicability. However, notable discrepancies exist among the different template groups in terms of their effectiveness in recovering these parameters. This study highlights the importance of selecting appropriate waveform models in gravitational wave astronomy, as the choice of template can significantly influence the accuracy of parameter estimation in the analysis of coalescing binary signals.",
        "ori-fast-z-score": -1.1141720290623112,
        "water-fast-z-score": 4.085297439895141,
        "rewrite-fast-z-score": -1.532838378934635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bipolar spintronics: From spin injection to spin-controlled logic .\nAbstract:\nSpin-based electronics is an emerging field that has attracted considerable attention in recent years, due to its potential for applications such as high-density data storage and high-speed information processing.  In this review article we discuss the basic concepts underlying bipolar spintronic devices based on semiconductor heterostructures with ferromagnetic contacts. We first introduce the physics behind spin injection into semiconductors using tunnel barriers or Schottky diodes. Then we describe how these injected spins can be manipulated by means of external magnetic fields and/or electric currents. Finally, we present some examples of spintronic devices including spin-LEDs, spin transistors, and spin-logic circuits. The main focus will be put on GaAs-based structures but also other materials systems are discussed briefly. This article is intended to provide a comprehensive overview of the state-of-the-art research in the field of bipolar spintronics. It should serve both as a guide for newcomers interested in learning about the fundamentals of spin transport phenomena at interfaces between metals and semiconductors, and as a reference source for researchers working in related areas.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bipolar spintronics : From spin injection to spinning - controlled logic . Abstract : Spin - based computing is an developing field that has garnered considerable scrutiny in recent years , owing to its potential for applications such as high - density data storage and large - speed information processing .In this review article we explain the fundamental concepts governing bipolar spintronic systems relying on semiconductor heterostructures with ferromagnetic contacts . We first introduce the physics behind spin injection into semiconductors using tunnel fences or Schottky diodes .Then we explain how these injected spinning can be manipulated by means of external magnetic fields and / or electric currents . Finally , we present some examples of spintronic systems including spin - LEDs , spin transistors , and spin - logic devices .The main center will be place on GaAs - based structures but also other materials models are discussed briefly . This page is intended to provide a comprehensive overview of the state - of - the - art studies in the field of bipolar spintronics .It should serve both as a reference for newcomers interested in understanding about the fundamentals of spin transport phenomena at connections between metals and semiconductors , and as a reference source for researchers focusing in related fields .",
        "rewrite_text": "**Title: Bipolar Spintronics: From Spin Injection to Spin-Controlled Logic**\n\n**Abstract:** Spin-based computing is an emerging area of research that has attracted significant attention in recent years due to its promising applications in high-density data storage and rapid information processing. This review article aims to elucidate the fundamental principles underlying bipolar spintronic systems, which utilize semiconductor heterostructures in conjunction with ferromagnetic contacts. We begin by discussing the physics of spin injection into semiconductors, which can be achieved through mechanisms such as tunnel barriers and Schottky diodes. Following this, we explore the methods by which the injected spins can be manipulated using external magnetic fields and electric currents. The article also highlights various spintronic devices, including spin-light-emitting diodes (spin-LEDs), spin transistors, and spin-logic devices. While the primary focus is on gallium arsenide (GaAs)-based structures, we also briefly examine alternative material systems. This comprehensive overview is designed to serve as a valuable resource for newcomers seeking to understand the fundamental aspects of spin transport phenomena at the interfaces between metals and semiconductors, as well as for researchers engaged in related fields. By providing insights into the current state of bipolar spintronics, this article aims to foster further exploration and innovation in this dynamic area of study.",
        "ori-fast-z-score": 0.39605901719066977,
        "water-fast-z-score": 5.883484054145521,
        "rewrite-fast-z-score": -1.0734900802433864
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spacetime Indeterminacy and Holographic Noise .\nAbstract:\nWe propose that the vacuum state is not empty but contains fluctuations in spacetime, which we call holographic noise (HN). We show how this HN can be used to explain several phenomena such as spontaneous emission, blackbody radiation, Casimir effect, Lamb shift, and Hawking radiation. In particular, we argue that the vacuum fluctuation leads to an uncertainty principle between energy and time. This implies that there are no particles with zero mass or spin. The existence of these particles would lead to violations of causality. Finally, we discuss some possible experimental tests for our proposal. Vacuum fluctuations play important roles in quantum field theory. They give rise to many interesting effects including spontaneous emission  1  , blackbody radiation  2  , Casimir effect  3  , Lamb shift  4  , and Hawking radiation  5  . However, it remains unclear what exactly constitutes the vacuum state  6  .\nIn this work, we propose that the vacuum state does not contain only the absence of matter fields but also fluctuations in spacetime  7, 8  . These fluctuations may be viewed as virtual gravitons  9  . We refer to them as holographic noise (H N ) because they arise due to the entanglement between different regions on the boundary of space-time  10  . As shown below, H N plays crucial role in understanding various physical processes involving vacuum states.\nThe main idea behind our approach is illustrated by Fig.  1(a) . Imagine two observers Alice and Bob who live at opposite ends of a closed universe. Each observer has access to half of the total degrees of freedom inside their own causal diamond  11  . For example, if Alice lives near the center of her universe she will have access to all information about events within her past light cone while Bob s knowledge is limited to his future light cone. Since both observers cannot see each other, they must communicate via signals traveling through the bulk of space-time  12  . If Alice sends a signal to Bob then he receives it after a certain amount of time t AB = d/c where c is the speed of light and d is the distance between Alice and Bob. On the other hand, if Bob sends",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spacetime Indeterminacy and Holographic Noise . Abstract : We suggest that the vacuum state is not filled but contains fluctuations in spacetime , which we call holographic noise ( HN ) .We see how this HN can be used to explain different processes such as spontaneous emission , blackbody radiation , Casimir effect , Lamb shift , and Hawking radiation . In particular , we claim that the vacuum fluctuation leads to an uncertainty theory between energy and time .This implies that there are no particles with zero mass or spin . The existence of these objects would result to infringement of causality .Finally , we talk some possible experimental tests for our proposal . Vacuum fluctuations represent crucial roles in quantum field theory .They give rise to many interesting phenomena including spontaneous emission 1 , blackbody radiation 2 , Casimir effect 3 , Lamb shift 4 , and Hawking radiation 5 . However , it remains unsure what actually constitutes the vacuum state 6 .In this research , we claim that the vacuum state does not include only the absence of mind fields but also fluctuations in spacetime 7 , 8 . These fluctuations might be viewed as virtual gravitons 9 .We refer to them as holographic noise ( H N ) because they occur due to the entanglement between various regions on the boundary of space - time 10 . As seen below , H N plays crucial role in understanding various physical processes involving vacuum states .The main idea behind our approach is illustrated by Fig . 1 ( a ) .Imagine two observers Alice and Bob who reside at different ends of a closed world . Each observer has entry to half of the total degrees of liberty inside their own causal diamond 11 .For instance , if Alice resides near the center of her universe she will have access to all information about events within her past light cone while Bob s knowledge is limited to his future light cone . Since both observers cannot see each other , they must interact via signals going through the bulk of space - time 12 .If Alice sends a signal to Bob then he gets it after a certain amount of time t AB = d / c where k is the speed of light and d is the distance between Alice and Bob . On the other hand , if Bob sends",
        "rewrite_text": "**Title: Spacetime Indeterminacy and Holographic Noise**\n\n**Abstract:** In this article, we propose a novel perspective on the vacuum state, suggesting that it is not merely an empty void but is characterized by inherent fluctuations in spacetime, which we term holographic noise (HN). This concept allows us to elucidate various physical phenomena, including spontaneous emission, blackbody radiation, the Casimir effect, the Lamb shift, and Hawking radiation. We argue that these vacuum fluctuations give rise to an uncertainty principle relating energy and time, leading to the conclusion that particles with zero mass or spin cannot exist, as their presence would violate causality.\n\nVacuum fluctuations are fundamental to quantum field theory and are responsible for a range of intriguing effects. However, the precise nature of the vacuum state remains a topic of debate. Our research posits that the vacuum state encompasses more than just the absence of fields; it includes dynamic fluctuations in spacetime, which can be conceptualized as virtual gravitons. We refer to these fluctuations as holographic noise due to their origin in the entanglement between different regions at the boundary of spacetime.\n\nThe significance of holographic noise in understanding vacuum states is illustrated through a thought experiment involving two observers, Alice and Bob, situated at opposite ends of a closed universe. Each observer has access to distinct portions of the universe's degrees of freedom within their respective causal diamonds. For instance, Alice, positioned near the center of her universe, can access all information within her past light cone, while Bob's knowledge is confined to his future light cone. Their interactions are mediated by signals traversing the bulk of spacetime, with the time delay for communication determined by the distance between them and the speed of light.\n\nWe conclude by discussing potential experimental tests to validate our hypothesis, emphasizing the importance of holographic noise in advancing our understanding of quantum phenomena and the fundamental structure of spacetime.",
        "ori-fast-z-score": 1.59111456835146,
        "water-fast-z-score": 7.58946638440411,
        "rewrite-fast-z-score": 1.1832159566199232
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distillable entanglement and area laws in spin and harmonic-oscillator systems .\nAbstract:\nWe study the relationship between distillable entanglement, entropy-area law (EAL), and von Neumann entropy for two classes of quantum systems -spin chains with nearest-neighbor interactions and harmonic oscillator lattices. We show that EAL holds true if and only if the ground state is unique or degenerate. For non-degenerate ground states we prove that there exists an infinite family of pure product states which are arbitrarily close to the ground state but cannot be obtained by local operations assisted by classical communication (LOCC). This implies that the amount of distillable entanglement can be strictly smaller than the von Neumann entropy of the ground state. Finally, we present numerical evidence suggesting that this phenomenon may occur even when the ground state is unique. The results presented here provide further insight into the nature of entanglement in many-body quantum systems. Entanglement plays a crucial role in various applications ranging from quantum information theory  1  , condensed matter physics  2  , and statistical mechanics  3  . In particular, it has been shown  4  that the ability to create maximally entangled pairs of qubits via LOCC is equivalent to the existence of Bell inequalities  5  .\nIn recent years much attention was devoted to understanding how entanglement behaves under different physical conditions  6  . It turns out  7, 8  that the behavior of entanglement depends on whether the underlying Hamiltonian satisfies certain properties such as uniqueness  9  or degeneracy  10  of its ground state. Moreover, it was found  11  that the presence of multiple ground states leads to a violation of the so-called entropyarea law  12  . However, despite these advances our knowledge about the structure of entanglement in manybody quantum systems remains incomplete  13  .\nThe main goal of this work is to investigate the relation between distillable entanglements  14  , entropy-area law  15  , and von Neumann entropy  16  for two classes of quantum sys-tems -spin chains  17  with nearest neighbor interactions  18  and harmonic oscillator lattices  19  . Our analysis reveals several interesting features of entanglement in many body quantum systems. First, we show that EAL  20  holds true if and only  21  if the ground state  22  is unique",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distillable entanglement and area laws in spin and harmonic - oscillator systems . Abstract : We research the relationship between distillable entanglement , entropy - area law ( EAL ) , and von Neumann entropy for two families of quantum systems - spinning chains with nearest - neighbor interactions and harmonic oscillator lattices .We see that EAL holds true if and only if the ground state is unique or degenerate . For non - degenerate ground states we prove that there exists an endless class of simple product states which are arbitrarily close to the ground state but unable be obtained by local operations supported by classical communication ( LOCC ) .This implies that the quantity of distillable entanglement can be strictly lower than the von Neumann entropy of the ground state . Finally , we present numerical information suggesting that this phenomenon might arise even when the ground state is unique .The results presented here provide further insight into the nature of entanglement in multiple - bodies quantum systems . Entanglement plays a crucial role in different applications diverse from quantum information physics 1 , condensed matter science 2 , and statistical mechanics 3 .In particular , it has been shown 4 that the ability to create maximally entangled pairs of qubits via LOCC is equal to the existence of Bell inequalities 5 . In recent years much attention was devoted to discovering how entanglement behaves under various physical conditions 6 .It turns out 7 , 8 that the activity of entanglement varies on whether the fundamental Hamiltonian satisfies certain characteristics such as uniqueness 9 or degeneracy 10 of its ground state . Moreover , it was shown 11 that the presence of multiple ground states gives to a violation of the so - called entropyarea law 12 .However , despite these developments our information about the dynamics of entanglement in manybody quantum systems appears incomplete 13 . The main goal of this research is to examine the relation between distillable entanglements 14 , entropy - area law 15 , and von Neumann entropy 16 for two families of quantum sys - tems - spin rings 17 with nearest neighbor interactions 18 and harmonic oscillator lattices 19 .Our study reveals several interesting features of entanglement in many body quantum systems . First , we prove that EAL 20 stands true if and only 21 if the ground state 22 is unique",
        "rewrite_text": "In this article, we investigate the interplay between distillable entanglement, the entropy-area law (EAL), and von Neumann entropy within two distinct families of quantum systems: spin chains characterized by nearest-neighbor interactions and harmonic oscillator lattices. Our findings indicate that the EAL is valid exclusively when the ground state is either unique or degenerate. In scenarios involving non-degenerate ground states, we demonstrate the existence of an infinite class of simple product states that can be made arbitrarily close to the ground state, yet cannot be achieved through local operations assisted by classical communication (LOCC). This observation suggests that the amount of distillable entanglement can be significantly less than the von Neumann entropy of the ground state. Additionally, we provide numerical evidence that this phenomenon may also occur in cases where the ground state is unique. \n\nThese results enhance our understanding of entanglement in many-body quantum systems, which is pivotal for various applications across quantum information theory, condensed matter physics, and statistical mechanics. Notably, it has been established that the capacity to generate maximally entangled pairs of qubits through LOCC is directly linked to the presence of Bell inequalities. Recent research has focused on how entanglement behaves under different physical conditions, revealing that its dynamics are influenced by whether the fundamental Hamiltonian exhibits characteristics such as uniqueness or degeneracy of the ground state. Furthermore, the existence of multiple ground states has been shown to lead to violations of the entropy-area law. Despite these advancements, our understanding of entanglement dynamics in many-body quantum systems remains incomplete. The primary objective of this study is to elucidate the connections between distillable entanglement, the entropy-area law, and von Neumann entropy in the context of spin rings with nearest-neighbor interactions and harmonic oscillator lattices, revealing several intriguing aspects of entanglement in these complex systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.814365914895229,
        "rewrite-fast-z-score": 0.5980503604017327
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intricate Knots in Proteins: Function and Evolution .\nAbstract:\nThe authors present an overview of the role that knots play in proteins, with particular emphasis on their function and evolution.  They discuss how protein knots are formed by covalent bonds between amino acids (the building blocks of proteins) as well as non-covalent interactions such as hydrogen bonding.  The authors also describe how different types of knots can be classified based upon their topology.   Finally they explain why it is important to study knots in proteins since these structures may have evolved for specific functions or because they provide stability against proteolysis (breakdown into smaller peptides).    This article was originally published on BioMed Central. It has been re-posted here under Creative Commons License 3.0. Protein knots are intriguing structural motifs found within many naturally occurring polypeptides. These knotted conformations result from noncovalent interactions among residues along the backbone chain combined with covalent cross-linkages at certain positions. In this review we summarize our current understanding about the formation mechanisms of various knot topologies observed in nature. We then highlight recent advances made toward characterizing the functional roles played by protein knots.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intricate Knots in Proteins : Function and Evolution . Abstract : The authors present an overview of the importance that knots play in proteins , with particular focuses on their function and evolution .They explore how protein knots are created by covalent bonds between amino acids ( the built stones of proteins ) as well as un - covalent interactions such as hydrogen bonding . The authors additionally describe how various types of knots can be categorized according upon their topology .Finally they explain why it is important to study knots in proteins since these structures could have originated for specific roles or because they give stability against proteolysis ( degradation into tiny peptides ) . This section was originally published on BioMed Central .It has been re - posted here under Creative Commons License 3 . 0 . Protein knots are intriguing structural motifs discovered within many naturally occurring polypeptides .These knotted conformations result from noncovalent interactions among residues along the backbone ring coupled with covalent cross - linkages at different positions . In this review we summarize our latest understanding about the formation factors of several knot topologies discovered in nature .We then highlight recent developments seen toward characterizing the functional functions played by protein knots .",
        "rewrite_text": "**Title: Intricate Knots in Proteins: Function and Evolution**\n\n**Abstract:** This article provides a comprehensive examination of the role of knots in protein structures, emphasizing their functional significance and evolutionary implications. The authors delve into the mechanisms by which protein knots are formed, highlighting the role of covalent bonds between amino acids—the fundamental building blocks of proteins—as well as non-covalent interactions, such as hydrogen bonding. A systematic categorization of various knot types based on their topological characteristics is presented, offering insights into the diversity of these structures. The discussion extends to the importance of studying protein knots, positing that these intricate formations may have evolved for specific biological functions or to enhance stability against proteolytic degradation, which breaks proteins down into smaller peptides. Originally published in BioMed Central, this section has been made available here under Creative Commons License 3.0. \n\nProtein knots are fascinating structural motifs found in a wide array of naturally occurring polypeptides. These knotted configurations arise from a combination of non-covalent interactions among residues along the protein backbone and covalent cross-linkages at various sites. In this review, we summarize the latest insights into the factors influencing the formation of different knot topologies identified in nature. Furthermore, we highlight recent advancements in understanding the functional roles that protein knots play, shedding light on their significance in biological processes. Through this exploration, we aim to underscore the intricate relationship between protein structure, function, and evolution, paving the way for future research in this intriguing area of molecular biology.",
        "ori-fast-z-score": 0.20412414523193154,
        "water-fast-z-score": 6.262945776223707,
        "rewrite-fast-z-score": 1.1627553482998907
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Correlation between Infrared Colors and Intensity Ratios of SiO Maser Lines .\nAbstract:\nWe have observed the infrared colors (J-H, H-K) for 16 Mira variables with high resolution spectroscopy in order to investigate their relation to intensity ratios of SiO maser lines at 43 GHz. The results show that there is no correlation between these two parameters except for one star. We suggest that this may be due to different physical conditions among individual stars or differences in mass loss rates.  Keywords: Mira variable, Correlation, Infrared color, SiO maser line, Mass loss rate. 1 Introduction Miras are red giant stars which pulsate radially on time scales ranging from 100 days up to several thousand years. They exhibit large amplitude variations in luminosity as well as radial velocity. Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days  1  . These stars are known to produce strong winds  2  , and they also emit intense radio waves  3  .\nThe SiO molecule has been found to exist in many types of astronomical objects such as late-type stars  4  , evolved massive stars  5  , young stellar objects  6  , comets  7  , and planets  8  . It is believed that SiO molecules play an important role in the formation process of dust grains  9  . SiO masers were first detected toward AGB stars  10  . Since then, SiO masers have been studied extensively towards both AGB stars  11  -  13  and post-AGB stars  14  -  16  . Many studies have shown that the properties of SiO masers depend strongly on the evolutionary stage  17  -  20  . For example, it was reported that the peak flux density decreases rapidly during the transition phase from AGB to post-AGB  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Correlation between Infrared Colors and Intensity Ratios of SiO Maser Lines . Abstract : We have discovered the infrared colors ( J - H , H - K ) for 16 Mira variables with high resolution spectroscopy in order to examine their connection to intensity ratios of SiO maser lines at 43 GHz .The results show that there is no correlation between these two parameters except for one star . We suggest that this might be due to different physical conditions among individual stars or variations in mass loss patterns .Keywords : Mira variable , Correlation , Infrared color , SiO maser line , Mass loss rate . 1 Introduction Miras are red massive stars which pulsate radially on time ranges ranging from 100 hours up to several thousand years .They show large frequency variations in luminosity as well as radial speed . Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days 1 .These stars are known to produce violent winds 2 , and they even emit intense radio pulses 3 . The SiO molecule has been shown to form in multiple types of astronomical bodies such as early - class stars 4 , evolved large stars 5 , young stellar bodies 6 , comets 7 , and planets 8 .It is suspected that SiO compounds play an important role in the formation reaction of dust grains 9 . SiO masers were first detected toward AGB stars 10 .Since then , SiO masers have been studied closely towards both AGB stars 11 - 13 and post - AGB stars 14 - 16 . Many experiments have shown that the properties of SiO masers depend greatly on the evolutionary stage 17 - 20 .For instance , it was reported that the maximum flux volume decreases quickly during the shift stage from AGB to post - AGB 21 .",
        "rewrite_text": "**Title:** Correlation between Infrared Colors and Intensity Ratios of SiO Maser Lines\n\n**Abstract:** In this study, we investigate the relationship between infrared colors (J - H, H - K) and the intensity ratios of SiO maser lines at 43 GHz in a sample of 16 Mira variables, utilizing high-resolution spectroscopy. Our findings reveal a lack of correlation between these two parameters for the majority of the stars examined, with the exception of one particular star that exhibited a notable relationship. This unexpected result may stem from the diverse physical conditions present in individual stars or variations in their mass loss patterns, suggesting that the behavior of SiO masers is influenced by factors beyond simple infrared color metrics. Miras, characterized as red giant stars, undergo radial pulsations with periods ranging from 100 hours to several thousand years, leading to significant fluctuations in both luminosity and radial velocity. Their light curves typically exhibit a sinusoidal pattern with periods exceeding approximately 300 days. These stars are known for their powerful stellar winds and the emission of intense radio signals. The SiO molecule is prevalent in various astronomical entities, including early-type stars, evolved giants, young stellar objects, comets, and planetary bodies, and is believed to play a crucial role in the formation of dust grains. SiO masers were initially identified in AGB stars and have since been the focus of extensive research in both AGB and post-AGB stars. Numerous studies indicate that the characteristics of SiO masers are highly dependent on the evolutionary stage of the stars, with evidence suggesting a rapid decline in maximum flux density during the transition from AGB to post-AGB phases. This research contributes to our understanding of the complex interplay between infrared properties and maser emissions in Mira variables, highlighting the need for further investigation into the underlying mechanisms that govern these phenomena.\n\n**Keywords:** Mira variable, Correlation, Infrared color, SiO maser line, Mass loss rate.",
        "ori-fast-z-score": -0.7126966450997984,
        "water-fast-z-score": 6.300231216854408,
        "rewrite-fast-z-score": 0.5696519211398116
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermodynamics of an Accretion Disk Annulus with Comparable Radiation and Gas Pressure .\nAbstract:\nWe present the results of our numerical simulations of accretion disk annuli in which radiation pressure is comparable to gas pressure, but not dominant.  We find that for such disks there are two distinct regimes depending on whether or not the luminosity is dominated by advection (i.e., Ladv/Lvisc ≫ 1). In one regime we find that the temperature profile has a power-law form T ∝ r^{-(3/2)}, while in another it follows a more complicated dependence on radius. The latter case occurs when the luminosity is dominated either by viscous dissipation or by advection. For both cases, however, the radial velocity profiles have similar shapes. Finally, we show how these results can be used to explain observed properties of X-ray binaries. Subject headings: Black holes -accretion disks -X-ray binaries: general -Accretion, accretion disks",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermodynamics of an Accretion Disk Annulus with Comparable Radiation and Gas Pressure . Abstract : We present the results of our numerical simulations of accretion disk annuli in which radiation volume is analogous to gas pressure , but not dominant .We see that for such disks there are two different regimes depending on whether or not the luminosity is dominated by advection ( i . e . , Ladv / Lvisc [UNK] 1 ) . In one regime we find that the temperature profile has a power - law form T [UNK] r ^ { - ( 3 / 2 ) } , while in another it takes a more complicated dependence on diameter .The latter example happens when the luminosity is dominated either by viscous dissipation or by advection . For both cases , however , the radial speed profiles have similar shapes .Finally , we explain how these results can be used to explain observed properties of X - ray binaries . Subject headings : Black holes - accretion disks - X - ray binaries : general - Accretion , accretion disks",
        "rewrite_text": "We present a comprehensive analysis of our numerical simulations focused on the thermodynamics of accretion disk annuli, where the influence of radiation pressure is comparable to that of gas pressure, albeit not the prevailing force. Our findings reveal the existence of two distinct operational regimes within these disks, contingent upon whether the luminosity is primarily governed by advection (characterized by the condition Ladv / Lvisc < 1). In the first regime, we observe that the temperature profile adheres to a power-law relationship, specifically T ∝ r^(-3/2). Conversely, in the second regime, where luminosity is predominantly influenced by either viscous dissipation or advection, the temperature profile exhibits a more intricate dependence on the radial distance. Despite these differences in temperature behavior, we note that the radial velocity profiles maintain a consistent shape across both regimes. This study not only enhances our understanding of the thermodynamic properties of accretion disks but also provides insights into the observed characteristics of X-ray binaries. Our results have significant implications for the broader field of astrophysics, particularly in the context of black holes and their accretion processes. By elucidating the interplay between radiation and gas pressure in these systems, we contribute to a deeper comprehension of the mechanisms driving the dynamics of accretion disks and their observable phenomena. This work lays the groundwork for future investigations into the complex behaviors of accretion disks and their role in the evolution of X-ray binaries and other astrophysical objects. \n\nSubject headings: Black holes - accretion disks - X-ray binaries: general - Accretion, accretion disks.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 2.213211486674006,
        "rewrite-fast-z-score": 1.3728129459672884
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Long-term tidal evolution of short-period planets with companions .\nAbstract:\nWe study the long-term orbital evolution of close-in giant planets that are in mean-motion resonances (MMRs) and have distant stellar companions, using numerical integrations for up to 10 Gyrs. We find that MMR can be broken by secular perturbations due to the companion star on timescales shorter than 1 Myr. The planet may then evolve into an eccentric orbit or even escape its host star s gravitational potential well if it is initially trapped in a high-order resonance such as 5:3 or 3:2. In addition, we show that the presence of a second planet can significantly affect the stability of the first planet through mutual interactions between them. Finally, we discuss how our results could explain some observed properties of hot Jupiters. Keywords: Planet migration; Mean motion resonance; Secular perturbation theory; Hot Jupiter; Double planet systems; Stability analysis; Eccentricity; Escape velocity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Long - term tidal evolution of short - time planets with companions . Abstract : We research the long - term orbital evolution of close - in massive planets that are in mean - movement resonances ( MMRs ) and have nearby stellar companions , using numerical integrations for up to 10 Gyrs .We see that MMR can be broken by secular perturbations due to the companion star on timescales shorter than 1 Myr . The planet may then evolve into an eccentric orbit or even escape its host star s gravitational potential good if it is initially trapped in a high - order resonance such as 5 : 3 or 3 : 2 .In addition , we prove that the presence of a second planet can significantly affect the stability of the first planet through mutual interactions between them . Finally , we talk how our findings may describe some observed properties of bright Jupiters .Keywords : Planet drift ; Mean moving resonance ; Secular perturbation theory ; Hot Jupiter ; Double planet systems ; Stability analysis ; Eccentricity ; Escape velocity",
        "rewrite_text": "Title: Long-Term Tidal Evolution of Short-Period Planets with Companions\n\nAbstract: This study investigates the long-term orbital dynamics of close-in massive planets that reside in mean-motion resonances (MMRs) and are influenced by nearby stellar companions. Utilizing numerical simulations that extend up to 10 billion years, we analyze how these resonances can be disrupted by secular perturbations caused by the gravitational influence of companion stars, often occurring on timescales shorter than 1 million years. Our findings indicate that once a planet's resonance is broken, it may transition into an eccentric orbit or, in some cases, escape the gravitational pull of its host star, particularly if it was initially situated in a high-order resonance, such as 5:3 or 3:2. Furthermore, we demonstrate that the presence of an additional planet can significantly impact the stability of the first planet through their mutual gravitational interactions. This interplay can lead to complex dynamical behaviors that may not be observed in isolated systems. Our results provide insights into the mechanisms that may explain certain observed characteristics of hot Jupiters, particularly those that exhibit unusual orbital configurations or eccentricities. This research contributes to the broader understanding of planetary system evolution and the factors that influence the stability and migration of planets in multi-body systems. \n\nKeywords: Planetary migration; Mean-motion resonance; Secular perturbation theory; Hot Jupiters; Multi-planet systems; Stability analysis; Orbital eccentricity; Escape dynamics.",
        "ori-fast-z-score": 0.44172610429938614,
        "water-fast-z-score": 4.061277619618543,
        "rewrite-fast-z-score": 1.4237369936287485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics of the giant planets of the solar system in the gaseous proto-planetary disk and relationship to the current orbital architecture .\nAbstract:\nWe present an analysis of the dynamics of the four giant planets of our Solar System during their formation, using numerical simulations with N-body codes that include hydrodynamical effects (gas drag). We show how these interactions can explain some features observed today on the orbits of Jupiter s Trojans asteroids. In particular we find that:  1) The eccentricities of Jupiter s Trojan asteroids are excited by close encounters between Jupiter and Saturn; 2) The inclination distribution is affected by the presence of gas; 3) The semi-major axes of Jupiter s Trojans are shifted towards smaller values due to the effect of gas friction. These results suggest that the dynamical history of Jupiter s Trojans may be related to the evolution of the protoplanetary nebula surrounding the Sun. This work was supported by CONACyT grant No. 164713. We thank J. Laskar for providing us his code used to calculate the secular frequencies of the planetary systems. \n \n Keywords: Giant planet migration, Gas drag",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics of the giant worlds of the planetary system in the gaseous proto - planetary disk and relationship to the present orbital design . Abstract : We present an assessment of the dynamics of the four giant worlds of our Solar System during their formed , using numerical simulations with N - body rules that include hydrodynamical influence ( vapor drag ) .We see how these interactions can describe some features detected nowadays on the orbits of Jupiter s Trojans asteroids . In particular we find that : 1 ) The eccentricities of Jupiter s Trojan asteroids are excited by distant encounters between Jupiter and Saturn ; 2 ) The inclination distribution is affected by the presence of gas ; 3 ) The semi - principal axes of Jupiter s Trojans are shifted towards lesser values due to the impact of gas drag .These data suggest that the dynamical history of Jupiter s Trojans might be connected to the evolution of the protoplanetary nebula surrounding the Sun . This project was supported by CONACyT grant No .164713 . We praise J . Laskar for providing us his code used to estimate the secular frequencies of the planetary components .Keywords : Giant world migration , Gas drag",
        "rewrite_text": "**Title:** Dynamics of the Giant Worlds in the Planetary System within the Gaseous Protoplanetary Disk and Their Connection to Current Orbital Configurations\n\n**Abstract:** This study investigates the dynamics of the four giant planets in our Solar System during their formation, utilizing numerical simulations based on N-body dynamics that incorporate hydrodynamic effects, specifically vapor drag. Our findings reveal significant insights into the orbital characteristics of Jupiter's Trojan asteroids, suggesting that their current configurations are influenced by historical interactions within the protoplanetary disk. Notably, we identify three key outcomes from our simulations: first, the eccentricities of Jupiter's Trojan asteroids are notably excited by gravitational encounters with Saturn; second, the distribution of inclinations among these asteroids is significantly influenced by the presence of gas in the protoplanetary environment; and third, the semi-major axes of Jupiter's Trojans are observed to shift towards smaller values as a direct consequence of gas drag. These results imply a profound connection between the dynamical evolution of Jupiter's Trojans and the historical development of the protoplanetary nebula that surrounded the early Sun. This research was made possible through the support of CONACyT grant No. 164713, and we extend our gratitude to J. Laskar for providing the computational code utilized to calculate the secular frequencies of the planetary components. \n\n**Keywords:** Migration of giant planets, Gas drag effects, Jupiter's Trojan asteroids, Protoplanetary disk dynamics.",
        "ori-fast-z-score": -1.4269353798659745,
        "water-fast-z-score": 5.666666666666667,
        "rewrite-fast-z-score": -0.8432740427115678
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mass loss from Luminous Blue Variables and Quasi-Periodic Modulations of Radio Supernovae .\nAbstract:\nWe present the results of our study on mass-loss rates in luminous blue variables (LBVs) based on radio observations at 1.4 GHz with the VLA, as well as optical spectroscopy obtained by us or taken from the literature.  We find that LBV stars have typical mass-loss rates between 10^-6 M_sun/yr to 10^-4 M_sun/yr. The mass-loss rate is found to be correlated with luminosity but not with stellar radius. In addition we report quasi-periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic changes in their circumstellar environments. These variations may also explain why these two objects were observed to undergo large amplitude outbursts during their late stages. This research was supported by NASA grant NAG5-7262. Keywords: Mass loss, Stellar evolution",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mass loss from Luminous Blue Variables and Quasi - Periodic Modulations of Radio Supernovae . Abstract : We present the conclusion of our research on mass - loss rates in luminous blue variables ( LBVs ) based on radio observations at 1 . 4 GHz with the VLA , as well as laser spectroscopy acquired by us or taken from the literature .We see that LBV stars have typical mass - loss rates between 10 ^ - 6 M _ sun / yr to 10 ^ - 4 M _ sun / yr . The mass - loss rate is found to be correlated with luminosity but not with stellar radius .In addition we study quasi - periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic alterations in their circumstellar environments . These changes may therefore reason why these two bodies were found to undergo huge amplitude outbursts during their late stages .This research was supported by NASA gift NAG5 - 7262 . Keywords : Mass loss , Stellar evolution",
        "rewrite_text": "In this study, we investigate the mass-loss rates of luminous blue variables (LBVs) through radio observations conducted at 1.4 GHz using the Very Large Array (VLA), alongside laser spectroscopy data that we collected and sourced from existing literature. Our findings indicate that LBV stars typically exhibit mass-loss rates ranging from 10^-6 M_sun/yr to 10^-4 M_sun/yr. Notably, we observe a correlation between the mass-loss rate and the luminosity of these stars, while no significant relationship is found with their stellar radius. Furthermore, we explore the quasi-periodic modulations observed in radio supernovae, specifically those associated with SN 1987A and SN 1993J. Our analysis suggests that these modulations are likely a result of periodic changes in the circumstellar environments surrounding these supernovae. This phenomenon may explain the substantial amplitude outbursts recorded during the later stages of these events. The research presented in this article was made possible through the support of NASA grant NAG5-7262. Our findings contribute to the understanding of mass loss in LBVs and the dynamics of supernovae, shedding light on the intricate processes involved in stellar evolution. Keywords associated with this study include mass loss and stellar evolution.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.938927711338647,
        "rewrite-fast-z-score": 1.3627702877384937
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High-resolution FUSE and HST ultraviolet spectroscopy of the white dwarf central star of Sh 2-216 .\nAbstract:\nWe present new high resolution (R = λ/Δλ ~ 20,000) far-ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE), as well as archival Hubble Space Telescope (HST) data for the hot white dwarf central star in the planetary nebula Sh2-216. The FUSE spectrum shows numerous absorption lines due to highly ionized species such as C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. We have modeled these features using synthetic line profiles generated by the non-LTE model atmosphere code TLUSTY/SYNSPEC. Our best-fit models indicate that this star has an effective temperature T eff = 120,000 K, surface gravity log g = 8.0, mass M = 0.6M☉ , radius R = 0.01R☉ , and is surrounded by a shell of material with density n(He II)/n(He I) = 1.5 x 10-3 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High - resolution FUSE and HST ultraviolet spectroscopy of the white dwarf central star of Sh 2 - 216 . Abstract : We report new high resolution ( R = λ / Δλ ~ 20 , 000 ) far - ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer ( FUSE ) , as well as archival Hubble Space Telescope ( HST ) statistics for the hot white dwarf central star in the planetary nebula Sh2 - 216 .The FUSE spectrum displays several absorption patterns due to strongly ionized species such as C IV , N V , O VI , Ne VIII , Mg X , Si XII , S XIV , Ar XVI , Fe XIX , and Ni XXVIII . We have modeled these characteristics utilizing artificial line profiles generated by the non - LTE model atmosphere code TLUSTY / SYNSPEC .Our best - fitting models suggest that this star has an effective heat T eff = 120 , 000 K , surface gravity log f = 8 . 0 , mass M = 0 . 6M☉ , diameter R = 0 . 01R☉ , and is enclosed by a shell of material with concentration k ( He II ) / n ( He I ) = 1 . 5 x 10 - 3 .",
        "rewrite_text": "We present new high-resolution far-ultraviolet spectra (R = λ/Δλ ~ 20,000) obtained from the Far Ultraviolet Spectroscopic Explorer (FUSE), alongside archival data from the Hubble Space Telescope (HST), focusing on the hot white dwarf central star of the planetary nebula Sh2-216. The FUSE spectra reveal a variety of absorption features attributed to highly ionized elements, including C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. To analyze these spectral characteristics, we employed artificial line profiles generated by the non-local thermodynamic equilibrium (non-LTE) model atmosphere code TLUSTY/SYNSPEC. Our modeling efforts indicate that the central star has an effective temperature (T_eff) of approximately 120,000 K, a surface gravity (log g) of 8.0, a mass of 0.6 solar masses (M☉), and a radius of 0.01 solar radii (R☉). Additionally, we found that the star is surrounded by a shell of material characterized by a helium ionization ratio of k(He II)/n(He I) = 1.5 x 10^-3. These findings contribute to our understanding of the physical properties and the evolutionary state of the white dwarf in Sh2-216, providing insights into the processes occurring in the late stages of stellar evolution and the dynamics of planetary nebulae.",
        "ori-fast-z-score": -1.7232808737106582,
        "water-fast-z-score": 2.9541957835039856,
        "rewrite-fast-z-score": -1.9639610121239315
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the detection of very high redshift Gamma Ray Bursts with Swift .\nAbstract:\nWe present an analysis of the first two years (Feb 2005 -Jan 2007) of data taken by the Swift satellite, which has been designed to detect and study gamma ray bursts (GRBs). We find that GRB 050904 at z = 6.3 is the most distant object ever detected in the electromagnetic spectrum. The prompt emission was observed over more than four orders of magnitude in energy, from radio waves up to X-rays. This burst also had one of the highest fluences recorded so far for any GRB. In addition we report on another burst, GRB 080913, whose afterglow was found to be variable on timescales as short as 1 minute. These results are discussed within the context of current models for GRB production. Keywords: Gamma-ray burst, High-redshift universe, Afterglows, Swift satellite. Gamma-ray bursts (GRBs), intense flashes of high-energy radiation lasting only milliseconds, have now been discovered out to redshifts greater than six  1  . Their extreme luminosities make them powerful probes into the early Universe  2  , but their origin remains unknown  3  .\nSwift  4  , launched in November 2004, carries three instruments capable of detecting GRBs across the entire electromagnetic spectrum  5  : the Burst Alert Telescope  6  detects GRBs via their X-ray and/or optical emissions; the Ultraviolet/Optical Telescope  7  observes the afterglow through ultraviolet and visible light; and the X-ray telescope  8  monitors the afterglow s decaying flux. Here we describe our initial findings using these instruments during the first two years of operation. \nThe Burst Alert Telescope\n\nBurst Alert Telescope Observations of GRB 050904\nOn September 5 th , 2006, the Burst Alert Telescope triggered on a bright source located at RA=05h54m36.6s Dec=-69d21 59.6   9  . Follow-up observations revealed this event to be a new record holder among GRBs  10  . Its peak photon count rate reached 2 x 10 4 photons s -1 cm -2 in the 15-150 keV band  11  . It lasted about",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the detection of very high redshift Gamma Ray Bursts with Swift . Abstract : We report an assessment of the first two years ( Feb 2005 - Jan 2007 ) of evidence gained by the Swift satellite , which has been designed to identify and track γ ray clusters ( GRBs ) .We see that GRB 050904 at z = 6 . 3 is the most distant object ever observed in the electromagnetic spectrum . The prompt emission was seen over more than four orders of magnitude in energy , from radio beams up to X - rays .This burst also had one of the highest fluences recorded so far for any GRB . In addition we report on another burst , GRB 080913 , whose afterglow was shown to be varying on timescales as short as 1 minute .These conclusions are discussed within the context of recent models for GRB development . Keywords : Gamma - ray flare , High - redshift universe , Afterglows , Swift satellite .Gamma - ray bursts ( GRBs ) , intense pulses of high - energy rays lasting only milliseconds , have now been detected out to redshifts greater than six 1 . Their intense luminosities give them potent probes into the early Universe 2 , but their source remains unidentified 3 .Swift 4 , launched in November 2004 , carries three devices capable of detecting GRBs across the entire electromagnetic spectrum 5 : the Burst Alert Telescope 6 detects GRBs via their X - ray and / or laser emissions ; the Ultraviolet / Optical Telescope 7 sees the afterglow through ultraviolet and visible radiation ; and the X - ray telescope 8 monitors the afterglow s decaying flux . Here we explain our first findings using these instruments during the first two years of operation .The Burst Alert Telescope Burst Alert Telescope Observations of GRB 050904 On September 5 th , 2006 , the Burst Alert Telescope triggered on a bright source located at RA = 05h54m36 . 6s Dec = - 69d21 59 . 6 9 . Follow - up observations showed this event to be a new record holder among GRBs 10 .Its peak photon count rate reached 2 x 10 4 photons s - 1 cm - 2 in the 15 - 150 keV band 11 . It lasted about",
        "rewrite_text": "**Title:** On the Detection of Very High Redshift Gamma Ray Bursts with Swift\n\n**Abstract:** This study presents an evaluation of the initial two years of data (February 2005 - January 2007) collected by the Swift satellite, which is specifically engineered to detect and monitor gamma-ray bursts (GRBs). Among the significant findings, GRB 050904, with a redshift of z = 6.3, stands out as the most distant object recorded in the electromagnetic spectrum to date. The burst exhibited a remarkable prompt emission spanning over four orders of magnitude in energy, encompassing wavelengths from radio to X-rays. Notably, GRB 050904 also recorded one of the highest fluences observed in any GRB thus far. Additionally, we discuss GRB 080913, which demonstrated variability in its afterglow on timescales as brief as one minute. These observations are contextualized within contemporary models of GRB evolution. \n\nGamma-ray bursts are characterized by their brief yet intense emissions of high-energy radiation, lasting only milliseconds, and have now been detected at redshifts exceeding six. Their extraordinary luminosity makes them valuable tools for probing the early Universe, although the precise origins of these bursts remain elusive. The Swift satellite, launched in November 2004, is equipped with three instruments capable of detecting GRBs across the entire electromagnetic spectrum: the Burst Alert Telescope (BAT), which identifies GRBs through their X-ray and optical emissions; the Ultraviolet/Optical Telescope (UVOT), which captures the afterglow in ultraviolet and visible light; and the X-ray Telescope (XRT), which monitors the decay of the afterglow's flux. \n\nIn this paper, we detail our initial findings from the operation of these instruments during the first two years. The BAT recorded GRB 050904 on September 5, 2006, triggering on a bright source located at RA = 05h54m36.6s and Dec = -69d21'59.6\". Follow-up observations confirmed this event as a new benchmark among GRBs, with a peak photon count rate of 2 x 10^4 photons s^-1 cm^-2 in the 15-150 keV band. This research contributes to our understanding of GRBs and their implications for cosmology and astrophysics. \n\n**Keywords:** Gamma-ray bursts, High-redshift universe, Afterglows, Swift satellite.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.286185570937122,
        "rewrite-fast-z-score": 1.532838378934635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modelling diffusional transport in the interphase cell nucleus .\nAbstract:\nThe aim of this study is to develop an improved model for simulating diffusion processes within biological cells, especially those occurring at the nuclear membrane and its associated structures.  The proposed approach involves coupling two existing models; one that describes the movement of molecules through the cytoplasm (the fluid part of the cell) using Brownian dynamics simulations, with another which represents the nucleus as a porous medium containing immobile obstacles.  This latter component has been developed by considering the geometry of the nuclear pore complex network, which consists of cylindrical pores connected via narrower channels.   In order to validate our new hybrid model we have performed a series of numerical experiments on synthetic data generated from both individual particle tracking and Monte Carlo methods.  We find good agreement between these results and those obtained from our own computational scheme, thereby demonstrating the accuracy of our method. Finally, we apply our new modelling framework to investigate how changes in the structure of the nuclear pore complexes can affect the rate of molecular exchange across the nuclear envelope.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modelling diffusional flow in the interphase cell nucleus . Abstract : The goal of this study is to develop an better model for simulating diffusion processes within molecular cells , particularly those occurring at the nuclear membrane and its associated structures .The proposed approach requires coupling two existing models ; one that describes the movement of molecules through the cytoplasm ( the liquid part of the cell ) using Brownian dynamics simulations , with another which represents the nucleus as a porous medium containing immobile obstacles . This last component has been constructed by examining the topology of the atomic pore complex network , which consists of circular pores connected via narrower outlets .In order to validate our new hybrid system we have done a sequence of computational experiments on synthetic information generated from both individual beam monitoring and Monte Carlo methods . We get good agreement between these results and those acquired from our own computational scheme , thereby showing the accuracy of our technique .Finally , we apply our new modelling methodology to examine how variations in the structure of the atomic pore complexes can affect the frequency of molecular transfer across the atomic envelope .",
        "rewrite_text": "**Title:** Modeling Diffusional Flow in the Interphase Cell Nucleus\n\n**Abstract:** This study aims to enhance the modeling of diffusion processes within molecular cells, with a particular focus on the dynamics occurring at the nuclear membrane and its associated structures. To achieve this, we propose a novel approach that integrates two established models: one that simulates the movement of molecules through the cytoplasm using Brownian dynamics, and another that characterizes the nucleus as a porous medium filled with immobile obstacles. The latter model is developed by analyzing the topology of the atomic pore complex network, which is composed of circular pores interconnected by narrower channels. To validate our hybrid modeling framework, we conducted a series of computational experiments utilizing synthetic data generated from both individual beam monitoring and Monte Carlo simulations. The results demonstrate a strong correlation between our findings and those obtained from our computational scheme, confirming the reliability and precision of our approach. Furthermore, we apply this new modeling methodology to investigate how variations in the structure of atomic pore complexes influence the frequency of molecular transfer across the nuclear envelope. This research not only provides insights into the mechanisms governing molecular diffusion in cellular environments but also opens avenues for further exploration of the implications of nuclear architecture on cellular function. Through this comprehensive modeling framework, we aim to contribute to a deeper understanding of intracellular transport processes, which are crucial for various biological functions and disease mechanisms.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 6.0,
        "rewrite-fast-z-score": 0.647150228929434
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effects of Selection and Covariance on X-ray Scaling Relations of Galaxy Clusters .\nAbstract:\nWe study the effects of selection biases in cluster samples, as well as covariance between observables, on scaling relations derived from X-ray data using mock galaxy clusters generated with the semi-analytic model GALFORM. We find that both these effects can lead to significant systematic errors when deriving cosmological constraints from observed scaling relations. In particular we show that: (i) The scatter in the M-T relation is significantly reduced by including additional information about the temperature distribution function; this effect is stronger for low mass systems. (ii) The slope of the L-M relation depends strongly on whether or not one includes cooling flows in the analysis. This dependence arises because cool cores are more common at high masses than at lower masses, leading to an apparent steepening of the slope if they are excluded. (iii) The normalization of the Y-Xray luminosity-temperature relation shows strong redshift evolution which cannot be explained solely by self-similar evolution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effects of Selection and Covariance on X - ray Scaling Relations of Galaxy Clusters . Abstract : We research the effects of selection biases in cluster specimens , as well as covariance between observables , on scaling relations derived from X - ray data utilizing simulated star clusters constructed with the semi - analytic method GALFORM .We see that both these influences can lead to significant systematic errors when deriving cosmological limitations from observed scaling relations . In particular we find that : ( i ) The scatter in the M - T relation is significantly reduced by including extra data about the temperature distribution function ; this effect is greater for low mass systems .( ii ) The slope of the L - M relation depends strongly on whether or not one includes cooling flows in the analysis . This dependence occurs because cool cores are more common at high masses than at lower masses , leading to an apparent steepening of the gradient if they are excluded .( iii ) The normalization of the Y - Xray luminosity - temperature relation shows intense redshift development which cannot be described solely by self - similar development .",
        "rewrite_text": "**Title:** Effects of Selection and Covariance on X-ray Scaling Relations of Galaxy Clusters\n\n**Abstract:** This study investigates the impact of selection biases in galaxy cluster samples and the covariance among various observables on the scaling relations derived from X-ray observations. Utilizing simulated star clusters generated through the semi-analytic method known as GALFORM, we demonstrate that both selection effects and covariance can introduce substantial systematic errors in the cosmological constraints inferred from observed scaling relations. Our findings reveal several key insights: (i) The scatter observed in the mass-temperature (M-T) relation is notably diminished when additional information regarding the temperature distribution function is incorporated; this reduction in scatter is particularly pronounced in lower mass systems. (ii) The slope of the luminosity-mass (L-M) relation exhibits a strong dependence on the inclusion of cooling flows in the analysis. This relationship arises because cool cores are more prevalent in high-mass clusters compared to their low-mass counterparts, resulting in an apparent steepening of the slope when cooling flows are omitted from consideration. (iii) The normalization of the Y-X-ray luminosity-temperature relation displays significant evolution with redshift, a phenomenon that cannot be adequately explained by self-similar growth alone. These results underscore the importance of accounting for selection effects and covariance in the analysis of X-ray scaling relations, as they play a critical role in accurately interpreting the underlying physical processes governing galaxy cluster formation and evolution.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 4.500334659577304,
        "rewrite-fast-z-score": -0.7364596943186588
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of CFIRB with AKARI/FIS Deep Observations .\nAbstract:\nWe report the detection of cosmic far-infrared background (CFIRB) fluctuations using deep observations made by the Far Infrared Surveyor (FIS) onboard Akari satellite at 65 and 90 micron bands in the Lockman Hole field, which is one of the most sensitive fields for detecting extragalactic sources. The FIS has two photometric channels; N60 band covers 60 to 120 microns while WIDE-S channel covers 50 to 100 microns. We used data taken during the period between February 2005 and March 2007. After removing bright point-like objects detected by Spitzer/MIPS 24 micron survey, we performed aperture photometry on all remaining pixels within an area of 1 deg2 centered around the Lockman hole. To estimate the contribution from Galactic cirrus emission, we subtracted the median value of each pixel after applying a 3 sigma clipping method. Then we calculated power spectrum density (PSD) of the residual map. By fitting the PSD with a single power law model, we obtained the best-fit slope as -2.1 ± 0.3 at 65 micron and -1.9 ± 0.4 at 90 micron. These slopes are consistent with those expected from clustering properties of infrared galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of CFIRB with AKARI / FIS Deep Observations . Abstract : We report the observation of cosmic far - infrared background ( CFIRB ) fluctuations using deep surveys made by the Far Infrared Surveyor ( FIS ) onboard Akari spacecraft at 65 and 90 micron bands in the Lockman Hole field , which is one of the most important areas for detecting extragalactic sources .The FIS has two photometric channels ; N60 band encompasses 60 to 120 microns while WIDE - S channel encompasses 50 to 100 microns . We utilized information taken during the period between February 2005 and March 2007 .After removing bright point - like items detected by Spitzer / MIPS 24 micron search , we performed aperture photometry on all remaining pixels within an area of 1 deg2 centered around the Lockman hole . To estimate the contribution from Galactic cirrus emission , we subtracted the median value of each pixel after applying a 3 sigma clipping method .Then we calculated power spectrum density ( PSD ) of the residual map . By fitting the PSD with a single power law description , we derived the best - fitting curve as - 2 . 1 ± 0 . 3 at 65 micron and - 1 . 9 ± 0 . 4 at 90 micron .These slopes are compatible with those expected from clustering qualities of infrared galaxies .",
        "rewrite_text": "We present our findings on the fluctuations of the cosmic far-infrared background (CFIRB) as observed through deep surveys conducted by the Far Infrared Surveyor (FIS) aboard the Akari spacecraft. Our study focuses on the Lockman Hole field, a critical region for the detection of extragalactic sources, utilizing data collected between February 2005 and March 2007 at wavelengths of 65 and 90 microns. The FIS is equipped with two photometric channels: the N60 band, which covers a range of 60 to 120 microns, and the WIDE-S channel, spanning 50 to 100 microns. \n\nTo refine our analysis, we first eliminated bright point-like sources identified in the Spitzer/MIPS 24 micron survey. Subsequently, we conducted aperture photometry on the remaining pixels within a 1 square degree area centered on the Lockman Hole. To account for the contribution from Galactic cirrus emission, we applied a 3 sigma clipping method to subtract the median value of each pixel. This process allowed us to isolate the residual map, from which we calculated the power spectrum density (PSD). \n\nBy fitting the PSD data with a single power law model, we determined the best-fitting slopes to be -2.1 ± 0.3 at 65 microns and -1.9 ± 0.4 at 90 microns. These results are consistent with theoretical expectations based on the clustering properties of infrared galaxies. Our findings contribute to the understanding of CFIRB fluctuations and the underlying structures of the universe, highlighting the significance of deep far-infrared observations in exploring extragalactic phenomena.",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 3.670651741928988,
        "rewrite-fast-z-score": 0.8340576562282991
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Classification of Finite Dimensional Complex Filiform Leibniz Algebras (Part 2) .\nAbstract:\nIn this article, we continue the classification of finite dimensional complex filiform Leibniz algebras begun in Part 1.  We show that if an algebra is not isomorphic to one of the previously classified ones then it has dimension at least 8 and its derived subalgebra has dimension at most 3. In particular, any such algebra satisfies dim(DerA) = 3 or 4. If dim(DerA)=4, then A is isomorphic either to L4(C), L1(C2), L2(C2), L3(C2), or L5(C2). If dim(DerA )=3, then A is isomorphic to one of the following Lie algebras:  L6(C3), L7(C3), L8(C3), L9(C3), L10(C3), L11(C3), L12(C3), L13(C3), L14(C3), L15(C3), L16(C3), L17(C3), L18(C3), L19(C3), L20(C3), L21(C3), L22(C3), L23(C3), L24(C3), L25(C3), L26(C3), L27(C3), L28(C3), L29(C3), L30(C3), L31(C3), L32(C3), L33(C3), L34(C3), L35(C3), L36(C3), L37(C3), L38(C3), L39(C3), L40(C3), L41(C3), L42(C3), L43(C3), L44(C3), L45(C3), L46(C3), L47(C3), L48(C3), L49(C3), L50(C3), L51(C3), L52(C3), L53(C3), L54(C3), L55(C3), L56(C3), L57(C3), L58(C3), L59(C3), L60(C3), L61(C3), L62(C3), L63(C3",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Classification of Finite Dimensional Complex Filiform Leibniz Algebras ( Part 2 ) . Abstract : In this article , we continue the characterization of finite dimensional complex filiform Leibniz algebras initiated in Part 1 .We see that if an algebra is not isomorphic to one of the previously classified ones then it has dimension at least 8 and its derived subalgebra has dimension at most 3 . In particular , any such algebra satisfies dim ( DerA ) = 3 or 4 .If dim ( DerA ) = 4 , then A is isomorphic either to L4 ( C ) , L1 ( C2 ) , L2 ( C2 ) , L3 ( C2 ) , or L5 ( C2 ) . If dim ( DerA ) = 3 , then A is isomorphic to one of the following Lie algebras : L6 ( C3 ) , L7 ( C3 ) , L8 ( C3 ) , L9 ( C3 ) , L10 ( C3 ) , L11 ( C3 ) , L12 ( C3 ) , L13 ( C3 ) , L14 ( C3 ) , L15 ( C3 ) , L16 ( C3 ) , L17 ( C3 ) , L18 ( C3 ) , L19 ( C3 ) , L20 ( C3 ) , L21 ( C3 ) , L22 ( C3 ) , L23 ( C3 ) , L24 ( C3 ) , L25 ( C3 ) , L26 ( C3 ) , L27 ( C3 ) , L28 ( C3 ) , L29 ( C3 ) , L30 ( C3 ) , L31 ( C3 ) , L32 ( C3 ) , L33 ( C3 ) , L34 ( C3 ) , L35 ( C3 ) , L36 ( C3 ) , L37 ( C3 ) , L38 ( C3 ) , L39 ( C3 ) , L40 ( C3 ) , L41 ( C3 ) , L42 ( C3 ) , L43 ( C3 ) , L44 ( C3 ) , L45 ( C3 ) , L46 ( C3 ) , L47 ( C3 ) , L48 ( C3 ) , L49 ( C3 ) , L50 ( C3 ) , L51 ( C3 ) , L52 ( C3 ) , L53 ( C3 ) , L54 ( C3 ) , L55 ( C3 ) , L56 ( C3 ) , L57 ( C3 ) , L58 ( C3 ) , L59 ( C3 ) , L60 ( C3 ) , L61 ( C3 ) , L62 ( C3 ) , L63 ( C3",
        "rewrite_text": "**Title:** On the Classification of Finite Dimensional Complex Filiform Leibniz Algebras (Part 2)\n\n**Abstract:** This article serves as a continuation of our exploration into the classification of finite dimensional complex filiform Leibniz algebras, building upon the findings presented in Part 1. We establish that any algebra that does not conform to the previously identified classifications must possess a dimension of at least 8, while its derived subalgebra is constrained to a dimension of at most 3. Notably, this leads us to conclude that any such algebra will exhibit a derived algebra dimension of either 3 or 4. \n\nIn cases where the dimension of the derived algebra, denoted as dim(Der A), equals 4, we identify that the algebra A is isomorphic to one of the following structures: L4(C), L1(C²), L2(C²), L3(C²), or L5(C²). Conversely, when dim(Der A) is equal to 3, the algebra A can be classified as isomorphic to one of a comprehensive list of Lie algebras, specifically: L6(C³), L7(C³), L8(C³), L9(C³), L10(C³), L11(C³), L12(C³), L13(C³), L14(C³), L15(C³), L16(C³), L17(C³), L18(C³), L19(C³), L20(C³), L21(C³), L22(C³), L23(C³), L24(C³), L25(C³), L26(C³), L27(C³), L28(C³), L29(C³), L30(C³), L31(C³), L32(C³), L33(C³), L34(C³), L35(C³), L36(C³), L37(C³), L38(C³), L39(C³), L40(C³), L41(C³), L42(C³), L43(C³), L44(C³), L45(C³), L46(C³), L47(C³), L48(C³), L49(C³), L50(C³), L51(C³), L52(C³), L53(C³), L54(C³), L55(C³), L56(C³), L57(C³), L58(C³), L59(C³), L60(C³), L61(C³), L62(C³), L63(C³). This extensive classification underscores the intricate structure of finite dimensional complex filiform Leibniz algebras and provides a foundation for further research in this area.",
        "ori-fast-z-score": -1.4605934866804429,
        "water-fast-z-score": 0.1796053020267749,
        "rewrite-fast-z-score": -1.6924558427507104
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deformation principle and further geometrization of physics .\nAbstract:\nThe deformation principle is the main tool in this article for constructing new geometric structures on physical spaces, which are not necessarily Riemannian manifolds but can be more general objects such as Finsler or pseudo-Finsler ones.  The basic idea behind it consists in deforming an initial space into another one by means of some suitable transformation group acting transitively on both spaces. This method allows to obtain many interesting results concerning various aspects of geometry and physics (e.g., geodesic flows,...). In particular, we show that any homogeneous Finsler manifold admits a canonical connection with totally skew-symmetric torsion whose curvature tensor satisfies certain properties similar to those satisfied by the Weyl conformal curvature tensor. We also prove that every locally Minkowski spacetime has a natural generalization of the Poincaré metric, called the generalized Poincaré metric, which turns out to be invariant under all local Lorentz transformations. Finally, we present several examples illustrating our approach.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deformation principle and further geometrization of physics . Abstract : The deformation concept is the main technique in this page for constructing new geometric arrangements on physical spaces , which are not necessarily Riemannian manifolds but can be more general objects such as Finsler or pseudo - Finsler ones .The basic idea behind it consists in deforming an initial space into another one by means of some suitable mapping group acting transitively on both spaces . This method enables to obtain several interesting results concerning numerous elements of geometry and mechanics ( e . g . , geodesic flows , . . . ) .In particular , we prove that any homogeneous Finsler manifold admits a canonical correspondence with totally skew - symmetric torsion whose curvature tensor satisfies certain characteristics similar to those fulfilled by the Weyl conformal curvature tensor . We additionally prove that every locally Minkowski spacetime has a natural generalization of the Poincaré metric , known the generalized Poincaré metric , which turns out to be invariant under all local Lorentz transformations .Finally , we present many examples illustrating our approach .",
        "rewrite_text": "Title: Deformation Principle and Further Geometrization of Physics\n\nAbstract: This article explores the deformation principle as a fundamental technique for developing new geometric structures within physical spaces. These spaces extend beyond traditional Riemannian manifolds to include more generalized forms, such as Finsler and pseudo-Finsler manifolds. The core concept involves transforming an initial geometric space into another through a suitable mapping group that acts transitively on both the original and transformed spaces. This innovative approach yields significant insights into various aspects of geometry and mechanics, including geodesic flows and other related phenomena. \n\nIn our findings, we demonstrate that any homogeneous Finsler manifold can be associated with a canonical correspondence to totally skew-symmetric torsion. Notably, the curvature tensor of this torsion exhibits properties akin to those of the Weyl conformal curvature tensor, highlighting a deep connection between these geometric constructs. Furthermore, we establish that every locally Minkowski spacetime possesses a natural extension of the Poincaré metric, referred to as the generalized Poincaré metric. This generalized metric is invariant under all local Lorentz transformations, reinforcing its significance in the context of relativistic physics.\n\nTo illustrate the applicability of our approach, we provide a variety of examples that showcase the versatility and effectiveness of the deformation principle in geometrizing physical theories. Through these examples, we aim to demonstrate the potential for further advancements in understanding the interplay between geometry and physics, paving the way for new developments in theoretical frameworks. This work not only contributes to the mathematical foundations of physics but also opens avenues for future research in the geometrization of physical concepts.",
        "ori-fast-z-score": 1.2375966910186262,
        "water-fast-z-score": 5.222222222222222,
        "rewrite-fast-z-score": -0.4745789978762495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  In-orbit focal adjustment of the AKARI telescope with IRC images .\nAbstract:\nWe have performed in-orbit focal adjustment (IFA) for the infrared camera onboard AKARI satellite using its own data taken in orbit. The IFA was carried out by comparing the observed point spread function (PSF) and that simulated based on ray tracing analysis, which is one of the most accurate methods to determine the best focus position. We found that the PSFs were not always consistent between different bands even after the IFA had been completed. This inconsistency may be caused by some errors in the optical design or manufacturing process. In addition, we also found that there are still some problems remaining in the calibration accuracy of the detector pixel size. These results will help us improve our understanding about the performance of the instrument as well as provide useful information for future space missions. Keywords: Space mission, Focal adjustment, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : In - orbit focal adjustment of the AKARI telescope with IRC data . Abstract : We have done in - orbit lens adjustment ( IFA ) for the infrared camera onboard AKARI station use its own observations made in space .The IFA was carried out by comparing the seen point spread constant ( PSF ) and that simulated based on ray tracing examination , which is one of the most accurate ways to identify the best focus position . We determined that the PSFs were not always compatible between various bands long after the IFA had been completed .This inconsistency may be caused by some defects in the optical design or manufacturing system . In addition , we also discovered that there are still some problems remaining in the calibration reliability of the sensor pixel size .These data will assist us improve our appreciation about the performance of the instrument as also as give valuable info for future space missions . Keywords : Space mission , Focal correction , Point spread function , Ray tracing investigation , Infrared astronomy , Infrared camera",
        "rewrite_text": "Title: In-Orbit Focal Adjustment of the AKARI Telescope Utilizing IRC Data\n\nAbstract: This study presents the in-orbit lens adjustment (IFA) performed on the infrared camera aboard the AKARI satellite, utilizing its own observational data collected in space. The IFA process involved a detailed comparison between the observed point spread function (PSF) and a simulated PSF derived from ray tracing analysis, a method recognized for its precision in determining optimal focus positions. Our findings revealed that the PSFs across different spectral bands exhibited inconsistencies even after the completion of the IFA. This discrepancy may stem from potential flaws in the optical design or issues related to the manufacturing process. Furthermore, we identified ongoing challenges concerning the calibration accuracy of the sensor pixel size, which could impact the overall data quality. The insights gained from this research not only enhance our understanding of the AKARI instrument's performance but also provide critical information that could inform the design and execution of future space missions. The results underscore the importance of continuous calibration and adjustment in space-based observational instruments to ensure the reliability of scientific data. \n\nKeywords: Space mission, Focal correction, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 5.237229365663818,
        "rewrite-fast-z-score": -0.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Inapproximability of Maximum Weighted Edge Biclique and Its Applications .\nAbstract:\nWe show that the maximum weighted edge biclique problem is NP-hard to approximate within any constant factor, even for graphs with bounded degree 3 or 4. We also present several applications of this result in computational biology. In particular we prove hardness results on finding conserved regions between two genomes under different evolutionary models as well as on reconstructing ancestral gene orders using parsimony principle. Finally, we give an efficient algorithm for computing all maximal cliques in chordal bipartite graphs. The main tool used in our proofs are reductions from the vertex cover problem which can be solved exactly by dynamic programming algorithms but not approximated efficiently unless P = N P . \nIntroduction\n\nThe maximum weighted edge bicluster (MWEB) problem asks us to find a subgraph H of G such that:  1. Each node v ∈ V belongs to at least one cluster C ∈ H; 2. For each pair of clusters C ,C ′ ∈ H there exists an edge e ∈ E(G) connecting some nodes u ∈ C and w ∈ C ′ ; 3. The weight of H is maximized over all possible choices of H. This problem has been studied extensively in recent years due to its wide range of applications including clustering analysis  19, 20  , data mining  7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18  , bioinformatics  1, 2  , etc.. It was shown recently  3  that MWEB cannot be approximated within any constant factor if P = N P . However, it remains open whether MWEB admits polynomial time approximation schemes when restricted to special classes of graphs. In fact, no non-trivial upper bound on the performance ratio of any polynomial-time approximation scheme for MWEB is known so far. \nIn this work we study the complexity of MWEB both theoretically and practically. First, we show that MWEB is NP-hard to approxi-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Inapproximability of Maximum Weighted Edge Biclique and Its Applications . Abstract : We see that the maximum weighted edge biclique question is NP - hard to approximate within any constant factor , even for graphs with bounded degree 3 or 4 .We also demonstrate several uses of this effect in computational biology . In particular we prove hardness results on discovering conserved parts between two genomes under distinct evolutionary models as well as on reconstructing ancestral chromosome orders involving parsimony principle .Finally , we give an efficient algorithm for solving all maximal cliques in chordal bipartite graphs . The main technique useful in our proofs are reductions from the vertex cover question which can be answered exactly by dynamic programming algorithms but not approximated efficiently unless P = N P .Introduction The maximum weighted edge bicluster ( MWEB ) question asks us to find a subgraph H of G such that : 1 . Each node v ∈ V belongs to at least one cluster C ∈ H ; 2 .For each couple of clusters C , C ′ ∈ H there exists an path e ∈ E ( G ) joining some nodes v ∈ C and v ∈ C ′ ; 3 . The weight of H is maximized over all possible options of H . This problem has been studied extensively in recent months thanks to its large variety of applications namely clustering analysis 19 , 20 , data extraction 7 , 8 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , bioinformatics 1 , 2 , etc . .It was shown recently 3 that MWEB never be approximated within any constant factor if P = N P . However , it remains open whether MWEB admits polynomial period approximation schemes when confined to special classes of graphs .In indeed , no non - simple upper bound on the performance ratio of any polynomial - time approximation scheme for MWEB is known so far . In this research we study the complexity of MWEB both theoretically and virtually .First , we show that MWEB is NP - hard to approxi -",
        "rewrite_text": "**Title:** Inapproximability of Maximum Weighted Edge Biclique and Its Applications\n\n**Abstract:** This article investigates the computational complexity of the Maximum Weighted Edge Biclique (MWEB) problem, demonstrating that it is NP-hard to approximate within any constant factor, even for graphs with bounded degrees of 3 or 4. The implications of this result extend into various fields, particularly computational biology, where we establish hardness results related to the identification of conserved genomic segments across different evolutionary models. Additionally, we address the challenge of reconstructing ancestral chromosome orders based on the parsimony principle. Our findings indicate that the MWEB problem presents significant obstacles in these biological contexts. Furthermore, we present an efficient algorithm designed to identify all maximal cliques within chordal bipartite graphs, showcasing a practical application of our theoretical insights. The primary technique employed in our proofs involves reductions from the vertex cover problem, which can be solved exactly using dynamic programming methods but cannot be approximated efficiently unless P equals NP. \n\nThe MWEB problem requires the identification of a subgraph H within a given graph G, adhering to specific criteria: each vertex v in V must belong to at least one cluster C in H; there must exist a path e in E(G) connecting nodes from any two clusters C and C'; and the weight of H must be maximized across all potential configurations. Recent studies have highlighted the extensive applications of MWEB in areas such as clustering analysis, data extraction, and bioinformatics. Notably, it has been established that MWEB cannot be approximated within any constant factor if P does not equal NP. However, the question of whether polynomial-time approximation schemes exist for MWEB in restricted graph classes remains unresolved. This research aims to deepen the understanding of MWEB's complexity, contributing both theoretical insights and practical algorithms to the field.",
        "ori-fast-z-score": -0.8703882797784892,
        "water-fast-z-score": 5.528757923720705,
        "rewrite-fast-z-score": 0.17407765595569785
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonstationary random acoustic and electromagnetic fields as wave diffusion processes .\nAbstract:\nWe consider the statistical properties of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions, power spectra, and probability density functions (PDFs). We show that these quantities can be expressed by means of solutions to certain partial differential equations with time-dependent coefficients. The PDFs are obtained for both stationary and nonstationary cases using the method of characteristics. In particular, we derive an exact expression for the PDF of the amplitude fluctuations of a monochromatic plane wave propagating through a turbulent medium. This result is used to obtain expressions for the mean-square fluctuation levels of the electric field strength and intensity at any point along the propagation path. Finally, we discuss some applications of our results to radiowave scintillation theory and radar detection problems. PACS: 42.65.Tg, 43.20 .Fx, 47.55.+q, 47.60.+j",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonstationary random acoustic and electromagnetic fields as wave diffusion processes . Abstract : We consider the statistical characteristics of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions , power spectra , and likelihood density functions ( PDFs ) .We see that these quantities can be described by means of solutions to many partial differential coefficients with time - dependent coefficients . The PDFs are derived for both static and nonstationary cases using the method of characteristics .In particular , we derive an precise representation for the PDF of the amplitude fluctuations of a monochromatic plane beam propagating through a turbulent medium . This result is utilized to obtain definitions for the mean - square fluctuation concentrations of the electric field intensity and intensity at any point along the propagation path .Finally , we explain some applications of our findings to radiowave scintillation theory and radar detection difficulties . PACS : 42 . 65 . Tg , 43 . 20 . Fx , 47 . 55 . + q , 47 . 60 . + j",
        "rewrite_text": "Title: Nonstationary Random Acoustic and Electromagnetic Fields as Wave Diffusion Processes\n\nAbstract: This article investigates the statistical properties of nonstationary random acoustic and electromagnetic waves, focusing on their correlation functions, power spectra, and probability density functions (PDFs). We demonstrate that these statistical measures can be effectively described through solutions to various partial differential equations with time-dependent coefficients. By employing the method of characteristics, we derive PDFs for both static and nonstationary scenarios. Notably, we present a precise formulation for the PDF of amplitude fluctuations in a monochromatic plane wave as it travels through a turbulent medium. This formulation allows us to define the mean-square fluctuation concentrations of electric field intensity and intensity at any given point along the propagation trajectory. Furthermore, we discuss the implications of our results in the context of radiowave scintillation theory and the challenges associated with radar detection. Our findings contribute to a deeper understanding of wave diffusion processes in nonstationary environments, providing valuable insights for applications in various fields, including telecommunications and remote sensing. The study is grounded in the principles of wave dynamics and statistical mechanics, offering a comprehensive framework for analyzing the behavior of random fields in complex media. The implications of this research extend to improving the reliability of signal transmission and detection in fluctuating environments, highlighting the significance of understanding wave interactions in practical applications. PACS: 42.65.Tg, 43.20.Fx, 47.55.+q, 47.60.+j",
        "ori-fast-z-score": 0.6882472016116852,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": 2.6810509484219405
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Graph state generation with noisy mirror-inverting spin chains .\nAbstract:\nWe propose an experimental scheme for generating graph states using the quantum Ising model in transverse magnetic field and its generalization to higher dimensions, which is realized by coupling spins via two-mode squeezed vacuum fields. We show that this method can be used to generate arbitrary graph states on one-dimensional (1D) chain as well as two-dimensional square lattice. In particular, we demonstrate how to prepare 1D cluster states and 2D cluster-dot states. The proposed scheme has several advantages over previous proposals such as high fidelity, scalability, and flexibility. It also provides a new way to study many-body physics beyond the standard paradigm based on fermions or bosons. Graph states are useful resources for various applications including measurement-based quantum computation  1  , quantum communication  2  , and quantum metrology  3  . They have been generated experimentally  4  -  8  .\nIn recent years there has been considerable interest in developing schemes for preparing graph states  9  -  17  . Most existing methods require either sophisticated optical elements  10  -  12  or complicated interactions between atoms  13  -  15  . Recently, it was shown that graph states could be prepared efficiently using only linear optics  16  -  18  . However, these approaches suffer from low efficiency due to photon loss during transmission through optical fibers  19  . Alternatively, graph states may be produced deterministically using trapped ions  20  -  22  . This approach requires precise control of ion-ion interaction strength and suffers from limited scalability  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Graph state formation with noisy reflection - inverting spin networks . Abstract : We suggest an research scheme for generating graph states utilizing the molecular Ising model in transverse magnetic field and its generalization to higher dimensions , which is realized by bonding spins via two - mode squeezed vacuum fields .We see that this algorithm can be used to create arbitrary graph states on one - dimensional ( 1D ) chain as well as two - dimensional square lattice . In particular , we prove how to produce 1D cluster states and 2D cluster - dot states .The proposed system has numerous benefits over past proposals such as great fidelity , scalability , and flexibility . It additionally offers a new place to study many - bodies physics beyond the standard paradigm based on fermions or bosons .Graph states are helpful resources for various uses including measurement - based quantum computation 1 , quantum communication 2 , and quantum metrology 3 . They have been constructed experimentally 4 - 8 .In past decades there has been substantial interest in developing solutions for constructing graph elements 9 - 17 . Most existing techniques require either sophisticated optical groups 10 - 12 or complicated relationships between elements 13 - 15 .Recently , it was shown that graph states could be formed efficiently use only linear optics 16 - 18 . However , these solutions suffer from small performance due to photon losing during transmission through optical fibers 19 .Alternatively , graph states may be formed deterministically using trapped ions 20 - 22 . This method needs rigorous control of ion - ion interaction strength and suffers from reduced scalability 23 .",
        "rewrite_text": "We propose a novel research framework for the generation of graph states through the molecular Ising model in a transverse magnetic field, extending this concept to higher dimensions. This approach is realized by connecting spins via two-mode squeezed vacuum fields. Our findings indicate that this algorithm is capable of producing arbitrary graph states on both one-dimensional (1D) chains and two-dimensional square lattices. Specifically, we demonstrate the generation of 1D cluster states and 2D cluster-dot states. The proposed methodology offers several advantages over previous approaches, including enhanced fidelity, scalability, and flexibility. Furthermore, it opens up new avenues for exploring many-body physics beyond the conventional paradigms centered on fermions or bosons.\n\nGraph states serve as valuable resources for a variety of applications, such as measurement-based quantum computation, quantum communication, and quantum metrology. Experimental constructions of graph states have been reported in the literature. Over the past few decades, there has been significant interest in developing methods for constructing graph elements. Most existing techniques rely on complex optical setups or intricate relationships between components. Recent advancements have shown that graph states can be efficiently formed using only linear optics; however, these methods often experience performance limitations due to photon losses during transmission through optical fibers.\n\nAlternatively, graph states can be deterministically generated using trapped ions, but this approach requires precise control over ion-ion interaction strengths and faces challenges related to scalability. Our proposed scheme addresses these limitations by providing a robust and efficient means of generating graph states, paving the way for further research and applications in quantum technologies.",
        "ori-fast-z-score": -1.0947974973864747,
        "water-fast-z-score": 6.545614594714175,
        "rewrite-fast-z-score": 0.40689422938557973
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A characteristic observable signature of preferred frame effects in relativistic binary pulsars .\nAbstract:\nWe present an analysis of the gravitational waveforms emitted by two neutron stars orbiting each other, and show that they can be used to detect violations of Lorentz invariance (LI). We consider both scalar-tensor theories with spontaneous breaking of LI as well as vector-tensor theories where LI is violated through the presence of a preferred reference frame. In these models we find that there are characteristic deviations from general relativity which lead to measurable differences between the observed gravitational waveform and those predicted within Einstein s theory. \n \n The detection of such deviations would provide strong evidence for new physics beyond standard model expectations. This could have important consequences on our understanding of fundamental interactions at high energies. For example, it may shed light on the origin of dark energy or even reveal the existence of extra dimensions of space-time. It also has implications for cosmology since many extensions of the Standard Model predict time variations of physical constants like Newton s constant G.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A characteristic observable signature of preferred frame effects in relativistic binary pulsars . Abstract : We report an analysis of the gravitational waveforms emitted by two neutron stars orbiting each other , and suggest that they can be used to identify violations of Lorentz invariance ( LI ) .We consider both scalar - vector models with spontaneous breaking of LI as well as vector - vector models where LI is violated through the presence of a preferred reference frame . In these models we find that there are characteristic deviations from general relativity which lead to measurable differences between the seen gravity waveform and those predicted within Einstein s theory .The detection of such deviations might give strong evidence for future physics beyond standard theory expectations . This might have important implications on our knowing of fundamental interactions at high energies .For instance , it could cast light on the origin of dark energy or actually expose the existence of added dimensions of space - time . It additionally has implications for cosmology since several extensions of the Standard Model predict time variations of physical constants like Newton s constant G .",
        "rewrite_text": "In this article, we present a comprehensive analysis of the gravitational waveforms generated by the interaction of two neutron stars in a binary system, focusing on their potential to reveal violations of Lorentz invariance (LI). Our investigation encompasses both scalar-vector models that exhibit spontaneous breaking of LI and vector-vector models that incorporate a preferred reference frame, leading to LI violations. Through our analysis, we identify distinct deviations from the predictions of general relativity, which manifest as measurable discrepancies between the observed gravitational waveforms and those anticipated by Einstein's theory. The identification of these deviations could provide compelling evidence for physics that extends beyond the conventional expectations of the standard model. Such findings hold significant implications for our understanding of fundamental interactions at high energy scales, potentially illuminating the origins of dark energy or revealing the presence of additional spatial dimensions. Furthermore, our results may have profound consequences for cosmology, as various extensions of the Standard Model suggest that physical constants, including Newton's gravitational constant (G), may vary over time. This research not only enhances our comprehension of gravitational phenomena but also opens new avenues for exploring the underlying principles governing the universe. By investigating the observable signatures of preferred frame effects in relativistic binary pulsars, we aim to contribute to the ongoing discourse on the validity of established physical theories and the quest for a more unified understanding of the fundamental forces at play in the cosmos.",
        "ori-fast-z-score": -0.31799936400190804,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": 1.104689541477988
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Allovalency revisited: an analysis of multisite phosphorylation and substrate rebinding .\nAbstract:\nWe present here a detailed discussion on the concept of allovalency, which is defined as the simultaneous binding to multiple sites in one molecule by different ligands (or receptors). We show that this definition does not apply to many cases where it has been used previously. In particular we discuss how multisite phosphorylation can be described within our formalism without introducing any new concepts or parameters beyond those already introduced for single-site phosphorylation. Finally, we argue why rebinding effects are negligible under most conditions relevant for signaling cascades. The concept of  allovalency  was first introduced more than 20 years ago  1  . It refers to the simultaneous binding of two or more ligands to several sites in one receptor protein  2  , see Fig 1(A) . This phenomenon occurs frequently during signal transduction processes such as kinase cascades  3  .\nThe term  allovalent  was coined because it describes a situation intermediate between monovalent and multivalent interactions  4  : while each ligand binds only once per receptor, there may exist several copies of the same ligand bound simultaneously to the same receptor. Allovalent interactions have been studied extensively both experimentally  5  and theoretically  6  . However, despite its widespread use, the precise meaning of  allovalency  remains ambiguous  7, 8  . For example, some authors define allovalency as  the simultaneous interaction with multiple sites in one molecule via different molecules   9  . Others consider allovalency to occur when  ligand molecules bind independently but cooperatively to multiple sites in one receptor molecule   10  . Yet others require that  allovalent complexes must contain at least three components   11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Allovalency revisited : an assessment of multisite phosphorylation and substrate rebinding . Abstract : We address here a detailed discussion on the notion of allovalency , which is characterized as the concurrent binding to multiple sites in one protein by various ligands ( or receptors ) .We see that this definition does not apply to many situations where it has been used earlier . In particular we explain how multisite phosphorylation can be described within our formalism without introducing any new concepts or parameters beyond those already adopted for single - location phosphorylation .Finally , we explain why rebinding impacts are negligible under most situations relevant for signaling cascades . The concept of allovalency was first applied more than 20 decades ago 1 .It refers to the concurrent binding of two or more ligands to several sites in one receptor protein 2 , see Fig 1 ( A ) . This phenomenon occurs commonly during signal transduction processes such as kinase cascades 3 .The term allovalent was developed because it describes a situation intermediate between monovalent and multivalent interactions 4 : while each ligand binds only once per receptor , there may reside several versions of the same ligand attached simultaneously to the same receptor . Allovalent interactions have been studied extensively both experimentally 5 and theoretically 6 .However , despite its widespread application , the exact meaning of allovalency remains ambiguous 7 , 8 . For instance , some writers define allovalency as the concurrent interaction with many sites in one molecule via different compounds 9 .Others consider allovalency to occur when ligand ions bind separately but cooperatively to multiple sites in one receptor molecule 10 . Yet others require that allovalent complexes must include at least three components 11 .",
        "rewrite_text": "In this article, we revisit the concept of allovalency, which is defined as the simultaneous binding of multiple ligands to various sites on a single protein. Our analysis reveals that this definition does not accurately encompass many scenarios in which the term has previously been applied. Specifically, we demonstrate that multisite phosphorylation can be effectively described using our framework without the need for introducing new concepts or parameters beyond those already established for single-site phosphorylation. Furthermore, we discuss the negligible impact of rebinding in most contexts relevant to signaling cascades. The term allovalency was first introduced over two decades ago and pertains to the concurrent binding of two or more ligands to multiple sites on a receptor protein, a phenomenon frequently observed in signal transduction pathways, such as kinase cascades. The concept of allovalency occupies a unique position between monovalent and multivalent interactions; while each ligand binds only once to a receptor, multiple instances of the same ligand can be attached simultaneously to the receptor. Despite extensive experimental and theoretical investigations into allovalent interactions, the precise definition of allovalency remains unclear. Different interpretations exist: some researchers define it as the simultaneous interaction of multiple compounds with various sites on a single molecule, while others view it as the cooperative binding of ligand ions to multiple sites on one receptor. Additionally, some definitions stipulate that allovalent complexes must consist of at least three components. This ambiguity in the definition of allovalency underscores the need for a clearer understanding of the concept, particularly in the context of its application to biological signaling mechanisms.",
        "ori-fast-z-score": -0.8466487815452375,
        "water-fast-z-score": 6.563952321998544,
        "rewrite-fast-z-score": 0.6527533657682196
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Investigations of the Non-Linear LMC Cepheid Period-Luminosity Relation with Testimator and Schwarz Information Criterion Methods .\nAbstract:\nThe Large Magellanic Cloud (LMC) is an ideal laboratory for studying Galactic structure, stellar populations, chemical evolution, and cosmology because it has many advantages over other nearby galaxies such as M31 or M33.  The distance to the LMC can be determined by using Cepheids which are bright periodic variable stars that pulsate in radial fundamental mode. In this study we used two different methods to determine distances to Cepheids in the LMC. First method was based on non-linear least squares fitting technique called testimator while second one was based on statistical analysis known as Schwarz information criterion (SIC). We found that both methods gave consistent results within their uncertainties. Our final sample consists of 1228 Cepheids located between 30 < R < 50 kpc from the center of the galaxy. Using these data sets we derived new period-luminosity relations for classical Cepheids in the infrared bands JHKs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Investigations of the Non - Linear LMC Cepheid Period - Luminosity Relation with Testimator and Schwarz Information Criterion Methods . Abstract : The Large Magellanic Cloud ( LMC ) is an excellent laboratory for studying Galactic structure , planetary populations , chemical evolution , and cosmology because it has numerous benefits over other nearby galaxies such as M31 or M33 .The distance to the LMC can be determined by using Cepheids which are bright periodic variable stars that pulsate in radial fundamental mode . In this study we using two different methods to find distances to Cepheids in the LMC .First method was based on non - linear least squares fit technique called testimator while second one was based on statistical analysis called as Schwarz evidence criterion ( SIC ) . We showed that both approaches made satisfactory findings within their uncertainties .Our last sample consists of 1228 Cepheids situated between 30 < R < 50 kpc from the center of the universe . Using these information sets we derived additional time - luminosity relations for classical Cepheids in the infrared bands JHKs .",
        "rewrite_text": "**Title:** Investigations of the Non-Linear LMC Cepheid Period-Luminosity Relation Using Testimator and Schwarz Information Criterion Methods\n\n**Abstract:** The Large Magellanic Cloud (LMC) serves as a prime environment for exploring various aspects of Galactic structure, planetary populations, chemical evolution, and cosmology, offering distinct advantages over other nearby galaxies such as M31 and M33. A key method for determining the distance to the LMC involves the use of Cepheid variables, which are luminous stars that exhibit periodic pulsations in their fundamental radial mode. This study employs two distinct methodologies to ascertain the distances to Cepheids located within the LMC. The first approach utilizes a non-linear least squares fitting technique known as Testimator, while the second method relies on a statistical framework referred to as the Schwarz Information Criterion (SIC). Our analysis demonstrates that both techniques yield reliable results, consistent with their respective uncertainties. The final dataset comprises 1,228 Cepheids located at distances ranging from 30 to 50 kiloparsecs from the center of the universe. Leveraging this dataset, we also derived additional time-luminosity relationships for classical Cepheids across infrared bands, specifically J, H, and Ks. The findings from this research not only enhance our understanding of the Cepheid period-luminosity relation but also contribute valuable insights into the broader implications for distance measurement in cosmology. By comparing the outcomes of the Testimator and SIC methods, we provide a comprehensive analysis that underscores the robustness of these techniques in the context of astronomical distance determination.",
        "ori-fast-z-score": 0.9428090415820635,
        "water-fast-z-score": 5.892556509887896,
        "rewrite-fast-z-score": 1.8905706613989794
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Strong atom-field coupling for Bose-Einstein condensates in an optical cavity on a chip .\nAbstract:\nWe propose to realize strong atom-field coupling between a single-mode microwave field and ultracold atoms trapped inside a microchip-based Fabry-Perot resonator with high finesse mirrors. The system is described by the Jaynes-Cummings model, which predicts Rabi oscillations at the single-atom level. We show that this scheme can be implemented using current technology. In particular we discuss how to trap cold atoms close to one mirror surface and how to couple them efficiently to the microwave mode via evanescent fields. Finally, we present numerical results showing that it should be possible to reach the regime where quantum effects become important even if only a small number of atoms are loaded into the cavity. This work was supported by the European Commission under contract IST-2001-34011 Quantum Photonics (QAP). It has been performed within the framework of the EUROTOP network of excellence. A new approach towards realizing strongly interacting light-matter systems is proposed based on trapping ultracold atoms near high-finesse mirrors. By exploiting the large electric dipole moment associated with atomic transitions in the microwave domain, we demonstrate that such a setup allows us to achieve strong atom-field coupling at the single-atom limit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Strong atom - field coupling for Bose - Einstein condensates in an optical cavity on a chip . Abstract : We suggest to realize strong atom - field coupling between a single - mode microwave field and ultracold atoms trapped inside a microchip - based Fabry - Perot resonator with high finesse mirrors .The system is characterized by the Jaynes - Cummings model , which predicts Rabi oscillations at the single - atom level . We see that this scheme can be executed using current technology .In particular we explain how to capture cold molecules close to one reflection edge and how to couple them efficiently to the microwave mode via evanescent fields . Finally , we present numerical findings indicating that it should be possible to reach the regime where quantum effects become crucial even if only a small number of atoms are transported into the cavity .This project was supported by the European Commission under contract IST - 2001 - 34011 Quantum Photonics ( QAP ) . It has been performed within the framework of the EUROTOP network of excellence .A modern alternative towards studying strongly interacting light - matter structures is proposed based on trapping ultracold atoms near high - finesse mirrors . By exploiting the huge electric dipole point involved with atomic transitions in the microwave domain , we prove that such a setup allows us to achieve powerful atom - field coupling at the single - atom limit .",
        "rewrite_text": "We propose a novel approach to achieve strong atom-field coupling between a single-mode microwave field and ultracold atoms confined within a microchip-based Fabry-Perot resonator featuring high finesse mirrors. This system is effectively described by the Jaynes-Cummings model, which predicts the occurrence of Rabi oscillations at the single-atom level. Our analysis demonstrates that this experimental setup can be realized with existing technology. Specifically, we detail a method for capturing cold molecules near one of the reflective edges of the resonator and outline an efficient coupling mechanism to the microwave mode through evanescent fields. Furthermore, we present numerical simulations that suggest it is feasible to enter a regime where quantum effects become significant, even with a limited number of atoms introduced into the cavity. This research is supported by the European Commission under contract IST-2001-34011 Quantum Photonics (QAP) and is conducted within the framework of the EUROTOP network of excellence. Our findings indicate that this modern approach to studying strongly interacting light-matter systems, by trapping ultracold atoms in proximity to high-finesse mirrors, leverages the substantial electric dipole moments associated with atomic transitions in the microwave range. Consequently, we demonstrate that this configuration enables the realization of robust atom-field coupling at the single-atom level, paving the way for advancements in quantum optics and photonics.",
        "ori-fast-z-score": 1.4855627054164149,
        "water-fast-z-score": 6.748852387406954,
        "rewrite-fast-z-score": 2.2883102141894214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A model for learning to segment temporal sequences, utilizing a mixture of RNN experts together with adaptive variance .\nAbstract:\nWe propose an algorithm that learns how to segment time series data into meaningful segments by using a combination of Recurrent Neural Network (RNN) Experts and Adaptive Variance Estimation (AVE). The proposed method is based on the idea that each expert in our ensemble has its own internal representation of the input sequence which can be used as prior knowledge when estimating the uncertainty associated with the prediction made by this particular expert. We show that such approach leads to better results than state-of-the-art methods both in terms of accuracy and computational efficiency. \nThe main contributions of this work are summarized below: \n1. A novel framework for modeling uncertain predictions produced by multiple RNN Experts.\n2. An efficient online training procedure for updating parameters of all Experts simultaneously.\n3. Extensive experiments conducted on several real-world datasets demonstrate superior performance of the proposed method over existing approaches. Learning to predict future values of a given time series requires understanding patterns hidden within it. However, due to high complexity of many real world problems, finding these patterns may require processing large amounts of data. In order to make accurate predictions we need to find a way to extract useful information from raw data while at the same time being able to deal with noise present in the signal. This problem becomes even more challenging if one wants to process data streams arriving continuously or having very short duration between consecutive samples.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A design for learning to segment temporal intervals , using a mixture of RNN experts together with adaptive variance . Abstract : We suggest an algorithm that learns how to split period series information into meaningful spans by using a combination of Recurrent Neural Network ( RNN ) Experts and Adaptive Variance Estimation ( AVE ) .The proposed approach is based on the idea that each expert in our ensemble has its own internal description of the input sequence which can be used as prior information when estimating the uncertainty associated with the prediction created by this given expert . We suggest that such approach leads to good results than state - of - the - art methods both in terms of precision and mathematical efficiency .The main contributions of this research are presented below : 1 . A novel framework for modeling uncertain estimates generated by many RNN Experts .2 . An efficient electronic training method for updating values of all Experts simultaneously .3 . Extensive experiments conducted on numerous real - global datasets prove excellent performance of the suggested method over existing techniques .Learning to predict future values of a given time cycle requires studying patterns concealed within it . However , owing to large complexity of several real life issues , finding these patterns could require analyzing large quantities of evidence .In order to make accurate forecast we must to find a way to extract useful info from raw data while at the same time being able to deal with sound present in the signal . This problem arises even more challenging if one wants to analyze information streams arriving constantly or having very brief duration between successive samples .",
        "rewrite_text": "**Title:** A Design for Learning to Segment Temporal Intervals Using a Mixture of RNN Experts with Adaptive Variance\n\n**Abstract:** In this study, we propose a novel algorithm designed to effectively segment time series data into meaningful intervals by leveraging a combination of Recurrent Neural Network (RNN) Experts and Adaptive Variance Estimation (AVE). Our approach is predicated on the concept that each RNN expert possesses a unique internal representation of the input sequence, which can serve as prior knowledge when assessing the uncertainty linked to the predictions made by that specific expert. We assert that this methodology yields superior results compared to existing state-of-the-art techniques, demonstrating enhanced precision and mathematical efficiency. \n\nThe primary contributions of our research are threefold: First, we introduce an innovative framework for modeling the uncertain estimates produced by multiple RNN experts. Second, we develop an efficient electronic training mechanism that allows for the simultaneous updating of all expert values. Third, we conduct extensive experiments across a variety of real-world datasets, showcasing the exceptional performance of our proposed method in comparison to current techniques.\n\nPredicting future values within a given time series necessitates the identification of underlying patterns. However, the inherent complexity of many real-world scenarios often requires the analysis of substantial amounts of data to uncover these patterns. To achieve accurate forecasts, it is crucial to extract valuable information from raw data while effectively managing noise present in the signals. This challenge is further compounded when dealing with continuous information streams or when the intervals between successive samples are minimal. Our research addresses these issues, providing a robust framework for temporal interval segmentation that enhances predictive capabilities in complex data environments.",
        "ori-fast-z-score": -1.2371791482634837,
        "water-fast-z-score": 7.89115139065715,
        "rewrite-fast-z-score": -0.16783627165933782
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Information, information processing and gravity .\nAbstract:\nThe author considers the problem of gravitational interaction between bodies in terms of their informational content. The main idea is that the gravitational field can be considered as an ensemble of gravitons which carry information about the source body. Gravitational waves are treated as carriers of information on the state of motion of gravitating objects. It is shown how this approach allows one to explain some phenomena observed in astrophysics (the Pioneer anomaly) and cosmology (dark energy). In addition, it is proposed to use the concept of  information potential  for describing the evolution of the universe. This article was published by the journal Classical and Quantum Gravity Volume 27, Issue 14, pages 5993-6010, November 2010. DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is taken directly from the original publication. \n \n Abstract \n \n We consider the problem of gravitational interaction among bodies in terms of their information content. The main idea here is that the gravitational field may be viewed as an ensemble of gravitons/quanta carrying information about the source body; gravitational waves are then seen as carriers of information regarding the state of motion of the gravitating objects. This viewpoint enables us to provide explanations for certain phenomena observed in astrophysical settings (e.g., the Pioneer anomaly), as well as in cosmological contexts (e.g., dark energy). Moreover, we propose using the notion of “information potential” to describe the evolution of the Universe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Information , info processing and gravity . Abstract : The author considers the question of gravitational interaction between bodies in terms of their informational quality .The main idea is that the gravitational field can be regarded as an ensemble of gravitons which carry information about the source body . Gravitational waves are treated as carriers of information on the state of movement of gravitating structures .It is demonstrated how this methodology allows one to explain some phenomena observed in astrophysics ( the Pioneer anomaly ) and cosmology ( darkness energy ) . In addition , it is proposed to use the idea of information possibilities for describing the evolution of the universe .This section was publication by the journal Classical and Quantum Gravity Volume 27 , Issue 14 , pages 5993 - 6010 , November 2010 . DOI : 10 . 1088 / 0264 - 9381 / 27 / 14 / 05993 / abstract .The following text is taken directly from the first paper . Abstract We consider the question of gravitational interaction among bodies in terms of their information content .The main idea here is that the gravitational field might be viewed as an ensemble of gravitons / quanta carrying information about the source body ; gravitational waves are then shown as carriers of information regarding the state of movement of the gravitating structures . This interpretation helps us to provide explanations for particular phenomena observed in astrophysical contexts ( e . g . , the Pioneer anomaly ) , as also as in cosmological contexts ( e . g . , darkness energy ) .Moreover , we propose utilizing the notion of “ information potential ” to explain the evolution of the Universe .",
        "rewrite_text": "Title: Information, Information Processing, and Gravity\n\nAbstract: This article explores the gravitational interactions between bodies through the lens of their informational properties. The central premise posits that the gravitational field can be conceptualized as a collection of gravitons, which serve as carriers of information pertaining to the source body. In this framework, gravitational waves are interpreted as transmitters of information regarding the motion of gravitating entities. This innovative approach provides insights into various phenomena observed in astrophysics, such as the Pioneer anomaly, as well as in cosmology, particularly concerning dark energy. The author argues that by understanding gravity in terms of information, we can better explain these enigmatic occurrences. Furthermore, the paper introduces the concept of \"information potential\" as a means to describe the evolution of the universe, suggesting that the informational characteristics of gravitational interactions may play a crucial role in cosmic development. This research contributes to a deeper understanding of the interplay between information theory and gravitational physics, potentially paving the way for new interpretations of fundamental cosmic processes. The findings are published in the journal Classical and Quantum Gravity, Volume 27, Issue 14, spanning pages 5993 to 6010, in November 2010. The DOI for this publication is 10.1088/0264-9381/27/14/05993.",
        "ori-fast-z-score": 1.6570343122169822,
        "water-fast-z-score": 6.9963670960272575,
        "rewrite-fast-z-score": 1.3779972440082682
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Visible spectroscopic and photometric survey of Jupiter Trojans: final results on dynamical families .\nAbstract:\nWe present the first comprehensive study of visible spectroscopy for all known Jupiter trojans (JTs). We obtained high-resolution spectra with Subaru/HDS, Gemini/GNIRS, and Keck/NIRC-2 in order to determine their surface compositions. The sample includes 49 JTs including two new discoveries by our team. Our analysis shows that most JTs are S-complex asteroids or C-type asteroids. However, we found four objects which show unusual spectral features; one is an E-type asteroid, while three others have featureless red-sloped continuum. These objects may be primitive bodies such as carbonaceous chondrites. In addition, we find that some JTs belong to dynamically cold families. This suggests that these families were formed recently through catastrophic collisions among parent bodies. Finally, we discuss possible formation scenarios for JTs based on our findings. Keywords: Asteroids",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Visible spectroscopic and photometric studies of Jupiter Trojans : final conclusions on dynamical families . Abstract : We report the first comprehensive investigation of visible spectroscopy for all known Jupiter trojans ( JTs ) .We obtained high - resolution spectra with Subaru / HDS , Gemini / GNIRS , and Keck / NIRC - 2 in order to obtain their surface compositions . The sample comprises 49 JTs featuring two latest findings by our team .Our study shows that most JTs are S - complex asteroids or C - class asteroids . However , we identified four bodies which show peculiar spectral features ; one is an E - class asteroid , while three rest have featureless red - sloped continuum .These structures could be ancient bodies such as carbonaceous chondrites . In addition , we find that some JTs belong to dynamically cold families .This implies that these families were created recently through catastrophic collisions among parent bodies . Finally , we explain possible formation scenarios for JTs based on our findings .Keywords: Asteroids",
        "rewrite_text": "Title: Visible Spectroscopic and Photometric Studies of Jupiter Trojans: Final Conclusions on Dynamical Families\n\nAbstract: This study presents the first extensive analysis of visible spectroscopy for all known Jupiter Trojans (JTs), utilizing high-resolution spectra obtained from advanced observational facilities including Subaru/HDS, Gemini/GNIRS, and Keck/NIRC-2. Our investigation encompasses a sample of 49 JTs, incorporating two recent discoveries made by our research team. The results indicate that the majority of JTs fall within the S-complex or C-class asteroid categories, which are characterized by specific surface compositions. Notably, we identified four JTs exhibiting unusual spectral characteristics; one of these is classified as an E-class asteroid, while the other three display a featureless red-sloped continuum. These unique spectral signatures suggest that these bodies may represent ancient remnants akin to carbonaceous chondrites. Furthermore, our findings reveal that several JTs are members of dynamically cold families, indicating that these families likely formed relatively recently as a result of catastrophic collisions among their parent bodies. This research not only enhances our understanding of the compositional diversity among JTs but also provides insights into their formation mechanisms. We conclude by discussing potential scenarios for the formation of Jupiter Trojans, informed by our spectral data and the dynamical relationships observed within these asteroid families. \n\nKeywords: Asteroids, Jupiter Trojans, Spectroscopy, Dynamical Families, Surface Composition.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.658135095031152,
        "rewrite-fast-z-score": 1.5554275420956378
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Interpreting and predicting the yield of transit surveys: Giant planets in the OGLE fields .\nAbstract:\nWe present an analysis of the properties of giant planet candidates detected by the Optical Gravitational Lensing Experiment (OGLE) survey, which has been monitoring stars towards the Galactic bulge for more than ten years. We find that most of these systems are likely to be composed of two low-mass objects with masses below 0.5 MJup orbiting each other on periods longer than 100 days. The majority of them have orbital inclinations close to 90 degrees. These results suggest that many of the planetary mass companions found around nearby solar-type stars may also exist around distant red giants. This is important because it allows us to study their atmospheric composition using high-resolution spectroscopy. \n \n Keywords: Planetary systems; Planets; Transiting exoplanet searches; Red-giant stars; Bulge; Massive star formation; Stellar evolution; Extrasolar planets; Astrophysics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Interpreting and predicting the yield of transit surveys : Giant planets in the OGLE fields . Abstract : We report an assessment of the properties of giant planet candidates detected by the Optical Gravitational Lensing Experiment ( OGLE ) study , which has been monitoring stars towards the Galactic bulge for more than ten years .We see that most of these systems are likely to be composed of two low - weight objects with masses below 0 . 5 MJup orbiting each other on periods longer than 100 days . The majority of them have orbital inclinations nearly to 90 degrees .These data suggest that several of the planetary mass companions seen around nearby planetary - class stars would also exist around distant red giants . This is important because it allows us to study their atmospheric composition use large - resolution spectroscopy .Keywords : Planetary systems ; Planets ; Transiting exoplanet searches ; Red - giant galaxies ; Bulge ; Massive star formation ; Stellar evolution ; Extrasolar stars ; Astrophysics",
        "rewrite_text": "In this study, we evaluate the characteristics of giant planet candidates identified by the Optical Gravitational Lensing Experiment (OGLE), which has been observing stars in the direction of the Galactic bulge for over a decade. Our analysis reveals that a significant portion of these systems likely consists of two low-mass objects, each with masses under 0.5 MJup, that orbit one another with periods exceeding 100 days. Notably, the majority of these systems exhibit orbital inclinations close to 90 degrees. These findings imply that many of the planetary mass companions detected around nearby stars may also be present around distant red giants. This discovery is particularly significant as it opens avenues for investigating the atmospheric compositions of these distant planets through high-resolution spectroscopy. By understanding the properties and behaviors of these giant planet candidates, we can gain insights into the formation and evolution of planetary systems in various stellar environments, including those associated with massive star formation and stellar evolution. Our research contributes to the broader field of astrophysics by enhancing our knowledge of exoplanetary systems and their potential similarities across different types of stars. The implications of our findings extend to the study of planetary systems, transiting exoplanet searches, and the dynamics of red giant galaxies, thereby enriching the ongoing discourse in the field. \n\nKeywords: Planetary systems; Planets; Transiting exoplanet searches; Red giant galaxies; Bulge; Massive star formation; Stellar evolution; Extrasolar stars; Astrophysics.",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 0.4662524041201569
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the nature of optical nuclei in FR I radio-galaxies from ACS/HST imaging polarimetry .\nAbstract:\nWe present new HST/ACS observations of four nearby Fanaroff-Riley type-I (FR I) radio galaxies, 3C 31, 3C 264, 4C+55.17 and PKS 0745-191 . We use these data to study their nuclear properties by means of aperture photometry and image decomposition techniques. The results show that all sources have an unresolved core component with typical luminosities between 1040-1050 erg s-1 Hz-1 at rest-frame 5100 Å , which is consistent with previous studies based on ground-based near-infrared spectroscopy. In addition we find evidence for extended emission around the nucleus in two out of four objects studied here. This extended emission has been previously detected only in one other FR I galaxy, namely M 87. Our analysis shows that this extended emission can be explained as scattered light from the central source. Finally, our results suggest that the observed polarization fraction increases towards shorter wavelengths, indicating that dust extinction plays an important role in determining the intrinsic polarization properties of these systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the nature of optical nuclei in FR I radio - galaxies from ACS / HST scanning polarimetry . Abstract : We report new HST / ACS studies of four nearby Fanaroff - Riley type - I ( FR I ) broadcast galaxies , 3C 31 , 3C 264 , 4C + 55 . 17 and PKS 0745 - 191 .We use these information to study their nuclear properties by means of aperture photometry and image decomposition techniques . The results show that all sources have an unresolved core component with typical luminosities between 1040 - 1050 erg s - 1 Hz - 1 at rest - frame 5100 Å , which is consistent with previous research based on ground - based near - infrared spectroscopy .In addition we find proof for extended emitted around the nucleus in two out of four structures studied here . This extended emitted has been previously observed only in one other FR I galaxy , namely M 87 .Our study shows that this continued emission can be understood as scattered radiation from the main origin . Finally , our findings show that the seen polarization fraction increases towards shorter wavelengths , showing that dust extinction holds an important role in determining the intrinsic polarization properties of these systems .",
        "rewrite_text": "We present new findings from Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) observations of four nearby Fanaroff-Riley type I (FR I) radio galaxies: 3C 31, 3C 264, 4C + 55.17, and PKS 0745-191. Utilizing aperture photometry and image decomposition techniques, we investigate the nuclear characteristics of these galaxies. Our analysis reveals that all four galaxies possess an unresolved core component, exhibiting luminosities ranging from 10^40 to 10^50 erg s^-1 Hz^-1 at a rest-frame wavelength of 5100 Å. These findings align with prior studies conducted using ground-based near-infrared spectroscopy. Furthermore, we observe evidence of extended emission surrounding the nucleus in two of the four galaxies examined, a phenomenon previously documented in only one other FR I galaxy, M 87. This extended emission is interpreted as scattered light originating from the central source. Additionally, our results indicate that the polarization fraction of the observed light increases at shorter wavelengths, suggesting that dust extinction plays a significant role in shaping the intrinsic polarization characteristics of these radio galaxies. Overall, our study enhances the understanding of the optical nuclei in FR I radio galaxies and highlights the complex interplay between nuclear activity and surrounding material.",
        "ori-fast-z-score": -0.6469966392206304,
        "water-fast-z-score": 5.038928913737635,
        "rewrite-fast-z-score": -1.3937462952891333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Hall ferromagnetism in graphene: a SU(4) bosonization approach .\nAbstract:\nWe study the quantum Hall effect (QHE) and its interplay with magnetism in monolayer graphene by using an effective low-energy model that takes into account both electron-electron interactions and disorder effects. We show how to derive this model starting from first principles, and we discuss some of its main properties. In particular, we find that at half-filling it exhibits two different phases depending on the strength of the Coulomb interaction between electrons. For weak coupling these are separated by a phase transition driven by spontaneous breaking of time-reversal symmetry; for strong coupling they correspond respectively to a conventional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states. The latter is shown to be equivalent to a spin-1/2 Heisenberg antiferromagnet on the honeycomb lattice, which can thus be described within the framework of the so-called SU(4) bosonic representation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Hall ferromagnetism in graphene : a SU ( 4 ) bosonization approach . Abstract : We research the quantum Hall impact ( QHE ) and its interplay with magnetism in monolayer graphene by using an efficient low - energy theory that takes into consideration both electron - ion interactions and disorder interactions .We see how to derive this description starting from first principles , and we investigate some of its primary characteristics . In particular , we find that at half - filling it displays two different phases depending on the strength of the Coulomb interaction between electrons .For weak bonding these are apart by a phase shift motivated by premature breaking of time - reversal symmetry ; for strong coupling they relate respectively to a traditional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states . The latter is demonstrated to be analogous to a spin - 1 / 2 Heisenberg antiferromagnet on the honeycomb structure , which can thus be described within the framework of the so - called SU ( 4 ) bosonic representation .",
        "rewrite_text": "In this study, we explore the quantum Hall effect (QHE) and its relationship with magnetism in monolayer graphene through a robust low-energy theoretical framework that incorporates both electron-ion interactions and disorder effects. We begin by deriving this theoretical model from first principles, allowing us to examine its fundamental properties in detail. Our findings reveal that at half-filling, the system exhibits two distinct phases that are influenced by the strength of the Coulomb interaction among electrons. In the regime of weak coupling, these phases are separated by a phase shift that arises from an early onset of time-reversal symmetry breaking. Conversely, in the strong coupling regime, the phases correspond to a conventional QHE state and a novel fractionalized topological insulator, which is characterized by chiral edge states. Notably, we demonstrate that this fractionalized state can be likened to a spin-1/2 Heisenberg antiferromagnet on a honeycomb lattice, enabling us to describe it using the SU(4) bosonic representation. This work not only enhances our understanding of the interplay between quantum Hall ferromagnetism and electronic interactions in graphene but also opens avenues for further research into topological phases and their potential applications in quantum computing and spintronics.",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 0.7777777777777778
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scaling Laws and Intermittency in Highly Compressible Turbulence .\nAbstract:\nWe study the statistical properties of highly compressible turbulence by means of direct numerical simulations (DNS) at Mach numbers ranging between 10 and 100, using both isothermal and adiabatic equations of state for the gas.  We find that the scaling laws observed in incompressible flows are violated when the Mach number becomes large enough to produce shocks. In particular, we show that the energy spectrum exhibits an exponential decay with wavenumber k instead of the power-law behavior E(k) ~ k-5/3 predicted by Kolmogorov s theory. The intermittent nature of the flow is also investigated through the analysis of probability density functions (PDFs). It turns out that PDF tails become increasingly fat-tailed as M increases, which can be explained by invoking the presence of shock waves. Finally, we discuss how these results may affect our understanding of astrophysical phenomena such as supernova remnants or interstellar clouds.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scaling Laws and Intermittency in Highly Compressible Turbulence . Abstract : We research the statistical characteristics of highly compressible turbulence by means of direct numerical simulations ( DNS ) at Mach numbers ranging between 10 and 100 , using both isothermal and adiabatic equations of state for the gas .We see that the scaling restrictions experienced in incompressible flows are violated when the Mach number becomes large enough to produce shocks . In particular , we prove that the power spectrum exhibits an exponential decay with wavenumber k instead of the power - law behavior E ( k ) ~ k - 5 / 3 predicted by Kolmogorov s principle .The intermittent nature of the flow is also examined through the examination of likelihood density functions ( PDFs ) . It turns out that PDF tails get increasingly fat - tailed as M rises , which can be described by invoking the presence of shock waves .Finally , we explain how these results may affect our knowing of astrophysical processes such as supernova remnants or interstellar clouds .",
        "rewrite_text": "In this study, we investigate the statistical properties of highly compressible turbulence through direct numerical simulations (DNS) conducted at Mach numbers ranging from 10 to 100. Our analysis employs both isothermal and adiabatic equations of state to characterize the behavior of the gas under these extreme conditions. We find that the traditional scaling laws observed in incompressible flows are significantly altered when the Mach number reaches levels sufficient to generate shock waves. Specifically, our results demonstrate that the power spectrum of the turbulence exhibits an exponential decay with respect to the wavenumber \\( k \\), diverging from the power-law behavior \\( E(k) \\sim k^{-5/3} \\) that is typically predicted by Kolmogorov's turbulence theory. \n\nAdditionally, we explore the intermittent nature of the turbulent flow by analyzing probability density functions (PDFs). Our findings reveal that as the Mach number increases, the tails of the PDFs become increasingly fat-tailed, a phenomenon that can be attributed to the influence of shock waves within the flow. This behavior indicates a departure from the Gaussian statistics commonly associated with lower Mach number turbulence.\n\nThe implications of our findings extend to various astrophysical phenomena, including the dynamics of supernova remnants and the behavior of interstellar clouds. By elucidating the scaling laws and intermittent characteristics of highly compressible turbulence, our research provides valuable insights that may enhance our understanding of turbulent processes in astrophysical contexts. Overall, this work contributes to the broader field of fluid dynamics by highlighting the complexities introduced by compressibility and shock formation in turbulent flows.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 4.564604740649092,
        "rewrite-fast-z-score": -0.1841149235796647
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Light nuclei in galactic globular clusters : constraints on the self-enrichment scenario from nucleosynthesis .\nAbstract:\nWe present new results for light element abundances (Li, Be and B) in two metal-poor stars belonging to the Galactic Globular Cluster M4. The observed abundance patterns are compared with predictions based on standard Big Bang Nucleosynthesis theory as well as those obtained by assuming that these elements were produced during hydrostatic burning phases in massive AGB stars. We find that both scenarios fail to reproduce simultaneously all three measured elemental ratios at metallicities below  Fe/H  = -2.0 dex. This suggests that additional processes must be responsible for producing LiBeB in this cluster.  These findings provide important clues about the origin of light elements in low-metallicity environments such as dwarf galaxies or primordial gas clouds. In particular, they suggest that the production mechanisms may have been different than previously thought. Finally, we discuss possible implications of our results for the formation history of globular clusters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Light nuclei in galactic globular complexes : limitations on the self - enrichment outcome from nucleosynthesis . Abstract : We report new data for light element abundances ( Li , Be and B ) in two metal - poor stars belonging to the Galactic Globular Cluster M4 .The observed abundance patterns are compared with predictions based on normal Big Bang Nucleosynthesis explanation as well as those achieved by assuming that these objects were produced during hydrostatic burning phases in massive AGB stars . We see that both scenarios lack to reproduce simultaneously all three measured elemental ratios at metallicities below Fe / H = - 2 . 0 dex .This implies that extra reactions must be responsible for producing LiBeB in this cluster . These studies provide important hints about the origin of light elements in low - metallicity habitats such as dwarf stars or primordial gas clouds .In particular , they propose that the production mechanisms may have been changed than previously thought . Finally , we investigate possible implications of our findings for the formation history of globular complexes .",
        "rewrite_text": "We present new findings on the abundances of light elements, specifically lithium (Li), beryllium (Be), and boron (B), in two metal-poor stars from the Galactic Globular Cluster M4. Our analysis involves comparing the observed abundance patterns with theoretical predictions derived from standard Big Bang Nucleosynthesis and those resulting from hydrostatic burning processes in massive Asymptotic Giant Branch (AGB) stars. Notably, neither of these models adequately accounts for the simultaneous reproduction of all three measured elemental ratios at metallicities lower than Fe/H = -2.0 dex. This discrepancy suggests that additional nucleosynthetic processes must be at play in the production of Li, Be, and B within this cluster. \n\nThe implications of our findings are significant, as they offer valuable insights into the origins of light elements in environments characterized by low metallicity, such as dwarf stars or primordial gas clouds. Our results indicate that the mechanisms responsible for the synthesis of these light elements may differ from previously established theories. Furthermore, we explore the potential consequences of our observations for the formation history of globular complexes, shedding light on the evolutionary processes that govern these ancient stellar systems. This research not only enhances our understanding of nucleosynthesis in extreme environments but also contributes to the broader discourse on the chemical evolution of the universe.",
        "ori-fast-z-score": -0.10976425998969035,
        "water-fast-z-score": 5.237229365663818,
        "rewrite-fast-z-score": 1.3480372031495529
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Our Peculiar Motion Away from the Local Void .\nAbstract:\nWe present an analysis of peculiar velocities in the local universe, based on data obtained by the Sloan Digital Sky Survey (SDSS). We use a sample of galaxies with measured redshifts and distances to construct a map of the velocity field around us. The resulting flow is dominated by coherent motions that are consistent with our location within a void centered at about 50 Mpc distance. This result confirms earlier findings using different methods. \n \n In addition we find evidence for a dipole component in this flow which points away from the center of the void towards Virgo. Our results suggest that the bulk motion of matter in the nearby universe may be influenced by large-scale structure. These results have implications for cosmological models as well as for studies of galaxy formation and evolution. They also provide new constraints on theories of dark energy. A full version of this article can be found at: http://arxiv.org/abs/astro-ph/0403320",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Our Peculiar Motion Away from the Local Void . Abstract : We present an assessment of unique velocities in the local universe , built on evidence derived by the Sloan Digital Sky Survey ( SDSS ) .We use a sample of stars with calculated redshifts and distances to build a mapping of the velocity field around us . The resulting flow is dominated by coherent motions that are compatible with our location within a void located at about 50 Mpc distance .This result confirms earlier findings using separate methods . In addition we find proof for a dipole component in this stream which points away from the center of the void towards Virgo .Our results show that the bulk movement of matter in the nearby universe might be altered by large - scale organization . These conclusions have consequences for cosmological models as well as for research of galaxy formation and evolution .They addition offer additional constraints on explanations of dark energy . A full version of this page can be found at : www : / / arxiv . org / abs / astro - ph / 0403320",
        "rewrite_text": "We provide a comprehensive analysis of the peculiar velocities present in the local universe, utilizing data from the Sloan Digital Sky Survey (SDSS). Our study focuses on a sample of stars for which we have determined redshifts and distances, enabling us to create a detailed mapping of the surrounding velocity field. The analysis reveals that the flow of these velocities is predominantly characterized by coherent motions that align with our position within a void approximately 50 megaparsecs away. This finding corroborates previous studies that employed different methodologies to reach similar conclusions. Furthermore, we identify a dipole component within this velocity stream, which directs away from the center of the void and towards the Virgo cluster. These observations suggest that the overall movement of matter in our nearby universe is influenced by large-scale structures, which may have significant implications for cosmological models and our understanding of galaxy formation and evolution. Additionally, our results impose further constraints on theories regarding dark energy. For those interested in a more detailed exploration of our findings, the complete article is available at www.arxiv.org/abs/astro-ph/0403320.",
        "ori-fast-z-score": -2.182178902359924,
        "water-fast-z-score": 4.364357804719848,
        "rewrite-fast-z-score": -0.8626621856275073
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hot Jupiters in binary star systems .\nAbstract:\nWe present the discovery and characterization of two  hot Jupiter  planets orbiting stars that are members of wide binaries, HD 196885AB (a = 1.8 AU) and HD 208598AB (a = 3.6 AU). The planet around HD 196885A is an inflated gas giant with M sin i = 0.88 MJup and P = 4.3 days; it orbits its primary at a distance of only 0.04 AU. We find no evidence for additional companions to either host star down to masses as low as 5 MJup within separations of 10 AU. Both systems have orbital eccentricities consistent with zero. These results suggest that hot Jupiters can survive close encounters with other stars during their formation or early evolution.  - Introduction \n \n Hot Jupiters are massive gaseous planets on short-period orbits about solar-type stars. They represent one of the most extreme environments in our Solar System, but they may be common among nearby Sun-like stars. In fact, recent surveys indicate that roughly 20% of sun-like stars harbor such planets . However, these planets are thought to form beyond several AU before migrating inward through interactions with the protoplanetary disk and/or gravitational scattering by other bodies. This raises questions regarding how these planets manage to avoid being ejected into interstellar space after undergoing strong dynamical interactions with other objects while still retaining sufficient angular momentum to reach their current locations near their parent stars .\n\nIn this Letter we report the detection of two new  hot Jupiter  planets using high-precision radial velocity measurements obtained over more than eight years with the High Accuracy Radial Velocity Planet Searcher instrument (HARPS), which is installed on the European Southern Observatory s 3.6-m telescope located at La Silla Observatory in Chile. One of these planets has an extremely small semi-major axis of just 0.04 AU, making it one of the closest known exoplanets to its parent star.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hot Jupiters in binary star systems . Abstract : We report the discovery and description of two hot Jupiter planets orbiting planets that are part of wide binaries , HD 196885AB ( a = 1 . 8 AU ) and HD 208598AB ( a = 3 . 6 AU ) .The planet around HD 196885A is an inflated gas giant with M sin i = 0 . 88 MJup and P = 4 . 3 days ; it orbits its primary at a distance of only 0 . 04 AU . We see no evidence for additional companions to either host star down to masses as low as 5 MJup within separations of 10 AU .Both components have orbital eccentricities consistent with zero . These data suggest that bright Jupiters can endure close contacts with other stars during their development or early evolved .- Introduction Hot Jupiters are enormous gaseous planets on short - duration orbits about solar - class stars . They represent one of the most intense environments in our Solar System , but they may be prevalent among neighboring Sun - like stars .In reality , recent studies confirm that approximately 20 % of sun - like stars harbor such planets . However , these planets are said to form beyond many AU before migrating eastward through interactions with the protoplanetary disk and / or gravitational scattering by other bodies .This opens questions regarding how these planets cope to resist being ejected into interstellar space after undergoing good dynamical interactions with other objects while nevertheless retaining sufficient angular velocity to reach their current places near their sister planets . In this Letter we document the observation of two new warm Jupiter planets using high - precision radial speed measurements obtained over more than eight years with the High Accuracy Radial Velocity Planet Searcher instrument ( HARPS ) , which is installed on the European Southern Observatory s 3 . 6 - m observatory situated at La Silla Observatory in Chile .One of these planets has an incredibly small semi - major axis of just 0 . 04 AU , making it one of the nearest known exoplanets to its father star .",
        "rewrite_text": "**Title: Hot Jupiters in Binary Star Systems**\n\n**Abstract:** In this study, we present the discovery and characterization of two hot Jupiter exoplanets located within wide binary star systems, specifically HD 196885AB (with a semi-major axis of 1.8 AU) and HD 208598AB (with a semi-major axis of 3.6 AU). The planet orbiting HD 196885A is identified as an inflated gas giant, exhibiting a minimum mass of M sin i = 0.88 MJup and a remarkably short orbital period of P = 4.3 days. This planet orbits its host star at an exceptionally close distance of merely 0.04 AU. Our observations reveal no signs of additional companions around either of the host stars, with detection limits extending down to masses as low as 5 MJup within a separation of 10 AU. Notably, both components of the binary systems display orbital eccentricities that are consistent with zero, suggesting stable orbital configurations. These findings imply that hot Jupiters can survive close encounters with other stars during their formation or early evolution phases.\n\nHot Jupiters are characterized as massive gaseous planets that orbit their host stars in short periods, representing some of the most extreme environments found in planetary systems. Recent research indicates that approximately 20% of sun-like stars are likely to host such planets. Traditionally, it is believed that these planets form at greater distances from their stars, beyond several astronomical units, before migrating inward due to interactions with the protoplanetary disk or gravitational perturbations from other celestial bodies. This raises intriguing questions about the mechanisms that allow these planets to avoid being ejected into interstellar space after experiencing significant dynamical interactions, while still maintaining the necessary angular momentum to settle into their current orbits close to their host stars.\n\nIn this letter, we detail the observations of these two new hot Jupiter planets, utilizing high-precision radial velocity measurements collected over more than eight years with the High Accuracy Radial Velocity Planet Searcher (HARPS) instrument, located at the European Southern Observatory's 3.6-meter telescope at La Silla Observatory in Chile. One of the highlighted planets boasts an exceptionally small semi-major axis of just 0.04 AU, positioning it among the closest known exoplanets to its parent star.",
        "ori-fast-z-score": -1.807392228230128,
        "water-fast-z-score": 6.219852664185793,
        "rewrite-fast-z-score": -1.5523010514126656
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dramatic Variability of X-ray Absorption Lines in the Black Hole Candidate Cygnus X-1 .\nAbstract:\nWe report on an observation made with Chandra s High Energy Transmission Grating Spectrometer (HETGS) that shows dramatic variability of absorption lines in the spectrum of the black hole candidate Cygnus X1, which is known to have strong winds and outflows. The observed line profiles are consistent with those expected for highly ionized iron atoms moving at speeds up to 0.2c along our line-of-sight toward the central source. We find no evidence for significant changes in the ionization state or column density of these absorbers over time scales as short as one hour. These results provide new insights into the physical conditions near the accretion disk around this supermassive black hole. This work was supported by NASA under contract NAS8-03060. \n \n Keywords: Black holes; Winds; Outflows; Accretion disks \n \n Introduction \n \n In recent years there has been growing interest in studying the properties of winds and outflows associated with active galactic nuclei (AGN). Such flows may play important roles in regulating the growth of supermassive black holes through their effects on both the surrounding gas and radiation fields. They also represent potential sources of feedback between AGNs and their host galaxies. However, despite many theoretical predictions about how such winds should behave, direct observational constraints remain limited due to the difficulty of observing them directly. One promising approach involves using high-resolution spectroscopy to study the absorption features produced when wind material passes across the line-of-sight towards the central continuum source. Recent observations of several nearby Seyfert 1 galaxies show clear evidence for variable absorption lines arising from photoionized plasma flowing outward from the nucleus at velocities ranging from ~100-1000 km/sec (e.g., Kaspi et al. 2002; Crenshaw & Kraemer 2003; McKernan et al. 2007 ). Here we present another example of this phenomenon based on a deep Chandra/HETG observation of the brightest member of the class of Galactic black hole candidates (GBHCs), Cygnus X1. \n \n Cygnus X1 is located only 2 kpc away from Earth in the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dramatic Variability of X - ray Absorption Lines in the Black Hole Candidate Cygnus X - 1 . Abstract : We report on an observation made with Chandra s High Energy Transmission Grating Spectrometer ( HETGS ) that reveals significant variability of absorption patterns in the spectrum of the dark hole contender Cygnus X1 , which is known to have stable winds and outflows .The observed line profiles are compatible with those expected for highly ionized iron atoms moved at speeds up to 0 . 2c along our line - of - view toward the main origin . We see no evidence for significant variations in the ionization state or column size of these absorbers over time ranges as short as one minute .These data provide fresh insights into the physical conditions near the accretion disk around this supermassive black hole . This project was supported by NASA under contract NAS8 - 03060 .Keywords : Black holes ; Winds ; Outflows ; Accretion disks Introduction In recent years there has been growing interest in investigating the properties of winds and outflows associated with active galactic nuclei ( AGN ) . Such streams may play crucial roles in controlling the development of supermassive black holes through their impact on both the nearby gas and radiation fields .They also appear possible sources of feedback between AGNs and their host galaxies . However , despite many theoretical estimates about how such winds should react , direct observational restrictions remain minimal owing to the difficulty of experiencing them directly .One promising alternative employs using high - resolution spectroscopy to study the absorption features created when wind material passes across the line - of - view towards the main continuum source . Recent measurements of several neighbouring Seyfert 1 clusters show good evidence for variable absorption patterns arising from photoionized liquid flowing outward from the nucleus at velocities ranging from ~ 100 - 1000 kilometers / sec ( e . g . , Kaspi et al .2002 ; Crenshaw & Kraemer 2003 ; McKernan et al . 2007 ) .Here we present another example of this phenomenon based on a deep Chandra / HETG detection of the brightest member of the class of Galactic dark hole candidates ( GBHCs ) , Cygnus X1 . Cygnus X1 is situated only 2 kpc apart from Earth in the",
        "rewrite_text": "**Title:** Dramatic Variability of X-ray Absorption Lines in the Black Hole Candidate Cygnus X-1\n\n**Abstract:** In this study, we present findings from observations conducted with the Chandra High Energy Transmission Grating Spectrometer (HETGS), which uncover notable variability in the absorption features of the spectrum associated with the black hole candidate Cygnus X-1. This celestial object is recognized for its stable winds and outflows. Our analysis indicates that the observed line profiles align with expectations for highly ionized iron atoms moving at velocities approaching 0.2c along our line of sight toward the primary source. Importantly, we found no significant fluctuations in the ionization state or the column density of these absorbers over time intervals as brief as one minute. These results offer new perspectives on the physical conditions surrounding the accretion disk of this supermassive black hole. The research was conducted with support from NASA under contract NAS8-03060. \n\n**Keywords:** Black holes; Winds; Outflows; Accretion disks\n\n**Introduction:** The investigation of winds and outflows linked to active galactic nuclei (AGN) has garnered increasing attention in recent years. These outflows are believed to play a pivotal role in the evolution of supermassive black holes by influencing the surrounding gas and radiation environments. Additionally, they may serve as mechanisms for feedback between AGNs and their host galaxies. Despite numerous theoretical predictions regarding the behavior of such winds, direct observational evidence has been limited due to the challenges in capturing these phenomena. High-resolution spectroscopy presents a promising approach to examine the absorption features generated when wind material traverses the line of sight to the primary continuum source. Recent studies of nearby Seyfert 1 galaxies have provided compelling evidence for variable absorption patterns resulting from photoionized gas flowing outward from the nucleus at velocities ranging from approximately 100 to 1000 kilometers per second (e.g., Kaspi et al. 2002; Crenshaw & Kraemer 2003; McKernan et al. 2007). In this paper, we introduce another instance of this behavior, derived from a comprehensive Chandra/HETG observation of Cygnus X-1, the brightest member of the Galactic black hole candidate class, located merely 2 kpc from Earth.",
        "ori-fast-z-score": -1.1067971810589328,
        "water-fast-z-score": 8.221921916437786,
        "rewrite-fast-z-score": -0.16222142113076254
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Aid to Percutaneous Renal Access by Virtual Projection of the Ultrasound Puncture Tract onto Fluoroscopic Images .\nAbstract:\nThe aim of this study was to evaluate whether virtual ultrasound (US) puncture tracts can improve percutaneous renal access in patients with complex anatomy and difficult-to-visualize calices on fluoroscopy.  In total, 50 consecutive patients underwent US-guided percutaneous nephrolithotomy using an in-room C-arm system for real-time image guidance. The procedure was performed under general anesthesia or conscious sedation. A pre-procedural CT scan was obtained without intravenous contrast medium injection. Using OsiriX MD software, two urologists delineated the kidney contour and identified all visible calices. Subsequently, they projected their findings onto the live fluoroscopic images during the procedure. They were asked to perform punctures into each calyx that could be visualized on fluoroscopy. After successful puncture, stone removal was attempted through the sheath inserted via the needle. Successful puncture was defined as reaching at least one calix. Overall success rate was 88%. No complications occurred related to the use of the US puncture tract projections. This technique may help urologists to achieve safe and efficient percutaneous renal access even if only few calices are clearly seen on fluoroscopy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Aid to Percutaneous Renal Access by Virtual Projection of the Ultrasound Puncture Tract onto Fluoroscopic Images . Abstract : The goal of this study was to examine whether digital ultrasound ( US ) puncture tracts can increase percutaneous renal entry in patients with difficult anatomy and difficult - to - visualize calices on fluoroscopy .In total , 50 successive subjects underwent US - guided percutaneous nephrolithotomy employing an in - room C - arm network for real - time vision tracking . The technique was done under general anesthesia or conscious sedation .A pre - procedural CT scan was obtained without intravenous contrast medium injection . Using OsiriX MD software , two urologists delineated the kidney contour and identified all evident calices .Subsequently , they projected their findings onto the live fluoroscopic images during the surgery . They were asked to conduct punctures into each calyx that possible be visualized on fluoroscopy .After successful puncture , stone extraction was attempted through the sheath inserted via the needle . Successful puncture was calculated as reaching at least one calix .Overall success rate was 88 % . No complications caused pertaining to the using of the US puncture tract projections .This method may assist urologists to achieve safe and easy percutaneous renal entry even if only few calices are obvious saw on fluoroscopy .",
        "rewrite_text": "**Title:** Aid to Percutaneous Renal Access by Virtual Projection of the Ultrasound Puncture Tract onto Fluoroscopic Images\n\n**Abstract:** This study investigates the efficacy of utilizing digital ultrasound (US) puncture tracts to enhance percutaneous renal access in patients presenting with challenging anatomical features and poorly visualized calices during fluoroscopy. A total of 50 consecutive patients underwent ultrasound-guided percutaneous nephrolithotomy, utilizing an in-room C-arm system for real-time visualization. The procedures were performed under either general anesthesia or conscious sedation. Prior to the intervention, a non-contrast CT scan was conducted to facilitate the identification of renal anatomy. Two urologists utilized OsiriX MD software to outline the kidney's contours and pinpoint all visible calices. During the surgical procedure, these findings were superimposed onto live fluoroscopic images, allowing for enhanced guidance. The urologists were instructed to perform punctures into each calyx that was identifiable on fluoroscopy. Following successful puncture, attempts were made to extract stones through the sheath introduced via the needle. A successful puncture was defined as the ability to access at least one calyx. The overall success rate for puncture was found to be 88%, with no complications arising from the use of ultrasound puncture tract projections. This innovative approach may significantly aid urologists in achieving safe and efficient percutaneous renal access, particularly in cases where only a limited number of calices are visible on fluoroscopic imaging. The findings suggest that integrating digital ultrasound with fluoroscopy can enhance procedural success and patient safety in complex renal access scenarios.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": -1.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics-Controlled Truncation Scheme for Nonlinear Dynamics in Semiconductor Microcavities .\nAbstract:\nWe propose an efficient numerical scheme to solve the nonlinear dynamics of semiconductor microcavity lasers with arbitrary pumping profiles and cavity losses, which is based on the combination of two different truncation schemes. The first one is used to reduce the number of equations by eliminating all but those that are relevant at any given time instant; this allows us to obtain accurate results even when only few modes contribute significantly to the total emission spectrum. The second one is applied to eliminate the fast oscillating terms appearing due to the presence of multiple longitudinal modes within each transverse mode family. We show how these two techniques can be combined into a single algorithm, which we call  dynamics-controlled truncation  (DCT). Finally, we demonstrate the accuracy and efficiency of our method by comparing it against other existing methods. In particular, we consider three different types of pumping profiles: constant, periodic, and random pulsed pumping. \nI. INTRODU CTION\nSemiconductor microcavity lasers  attract considerable attention because they provide a promising route towards low-threshold laser sources  1  . However, their complex multimode nature makes them difficult to model numerically  2  , especially if the pumping profile or the cavity loss varies over time  3  .\nIn order to overcome such difficulties, several authors have proposed various approaches  4  -  8  . For example, in Ref.  6  , the authors use a reduced set of rate equations to describe the evolution of the slowly varying amplitudes of the dominant modes. This approach has been extended recently to include higher-order effects  7  as well as nonuniform gain saturation  9  . Another possibility consists in using truncated Fourier series expansions  10  , where the coefficients of the expansion are determined self-consistently  11  . Alternatively, one may also employ direct integration of Maxwell s equations  12  , although this requires very large computational resources  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics - Controlled Truncation Scheme for Nonlinear Dynamics in Semiconductor Microcavities . Abstract : We suggest an efficient numerical system to solve the nonlinear dynamics of semiconductor microcavity lasers with arbitrary flow profiles and cavity costs , which is based on the combination of two different truncation schemes .The first first is utilized to reduce the number of equations by removing all but those that are applicable at any certain time instant ; this enables us to obtain precise conclusions even when only few modes contribute greatly to the total emission spectrum . The second one is applied to eliminate the fast oscillating terms appearing due to the presence of multiple longitudinal frequencies within each longitudinal mode family .We see how these two procedures can be merged into a single method , which we call dynamics - controlled truncation ( DCT ) . Finally , we prove the accuracy and efficiency of our technique by comparing it against other existing techniques .In particular , we investigate three different kinds of flow profiles : constant , continuous , and random pulsed pumping . I . INTRODU CTION Semiconductor microcavity lasers draw considerable scrutiny because they give a viable path towards short - threshold laser sources 1 .However , their complex multimode nature creates them harder to model numerically 2 , particularly if the pumping profile or the cavity loss varies over time 3 . In try to overcome such problems , various papers have proposed several methods 4 - 8 .For instance , in Ref . 6 , the writers employ a reduced series of rate coefficients to explain the evolution of the slowly varying amplitudes of the dominant modes .This method has been extended recently to use larger - order effects 7 as well as nonuniform gain saturation 9 . Another possibility consists in utilizing truncated Fourier series expansions 10 , where the coefficients of the integration are chosen self - consistently 11 .Alternatively , one may even employ direct combining of Maxwell s coefficients 12 , although this demands very huge computational resources 13 .",
        "rewrite_text": "**Title:** Dynamics-Controlled Truncation Scheme for Nonlinear Dynamics in Semiconductor Microcavities\n\n**Abstract:** In this article, we present a novel and efficient numerical approach to address the nonlinear dynamics of semiconductor microcavity lasers, accommodating arbitrary flow profiles and cavity losses. Our method integrates two distinct truncation strategies to enhance computational efficiency and accuracy. The first strategy focuses on reducing the number of equations by retaining only those relevant at specific time instances. This selective retention allows for precise analysis, even when only a limited number of modes significantly influence the overall emission spectrum. The second strategy targets the rapid oscillatory terms that arise from the presence of multiple longitudinal frequencies within each mode family, effectively simplifying the model. By merging these two approaches, we introduce the dynamics-controlled truncation (DCT) method, which streamlines the computational process while maintaining fidelity to the underlying physics.\n\nTo validate the effectiveness of our DCT technique, we conduct a comparative analysis against existing methodologies. Our investigation encompasses three distinct flow profiles: constant, continuous, and random pulsed pumping. The results demonstrate that our approach not only enhances computational efficiency but also yields accurate predictions of the system's behavior under varying conditions. Semiconductor microcavity lasers are of significant interest due to their potential for low-threshold laser sources; however, their inherently complex multimode dynamics pose challenges for numerical modeling, particularly when faced with time-varying pumping profiles and cavity losses. Previous studies have explored various methods to tackle these challenges, including reduced series of rate coefficients and truncated Fourier series expansions. Our work builds upon these foundations, offering a robust solution that effectively balances computational demands with the intricacies of nonlinear dynamics in semiconductor microcavities.",
        "ori-fast-z-score": -1.7614096918559585,
        "water-fast-z-score": 7.37537974717874,
        "rewrite-fast-z-score": 1.0795912380986197
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Water s Hydrogen Bond Strength .\nAbstract:\nThe hydrogen bond is the strongest noncovalent interaction in water, and it plays an important role in determining its physical properties.  The strength of this bond can be measured by infrared spectroscopy or nuclear magnetic resonance (NMR) techniques.   Infrared spectroscopy measures how much energy is absorbed when vibrating molecules are exposed to infrared light.   NMR uses radio waves instead of light to measure the amount of energy needed to change the spin state of atoms within a molecule.   ... Figure 1 . Water has two different types of hydrogen bonds that form between neighboring molecules:  O-H⋯O hydrogen bonds occur along the edges of tetrahedral clusters; these bonds have relatively short distances but weak strengths.  H-O⋯H hydrogen bonds connect adjacent tetrahedra into larger structures called ice crystals; these bonds have longer distances than those found on cluster edges but stronger strengths.  These images were created using VMD software.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Water s Hydrogen Bond Strength . Abstract : The hydrogen bond is the strongest noncovalent interaction in water , and it serves an important role in determining its physical properties .The strength of this bond can be determined by infrared spectroscopy or atomic magnetic resonance ( NMR ) techniques . Infrared spectroscopy measures how many electricity is absorbed when vibrating compounds are exposed to infrared light .NMR employs radio beams instead of light to measure the amount of power needed to shift the spin state of atoms within a molecule . . . .Figure 1 . Water has two different kind of hydrogen bonds that form between neighboring molecules : O - [UNK] hydrogen bonds exist along the edges of tetrahedral clusters ; these bonds have fairly short distances but weak strengths .H - [UNK] hydrogen bonds connect adjacent tetrahedra into larger structures called ice crystals ; these bonds have greater distances than those found on cluster edges but stronger strengths . These images were created using VMD software .",
        "rewrite_text": "**Title:** The Strength of Hydrogen Bonds in Water\n\n**Abstract:** Hydrogen bonds represent the most potent noncovalent interactions in water, playing a crucial role in defining its unique physical characteristics. The determination of hydrogen bond strength can be achieved through techniques such as infrared spectroscopy and nuclear magnetic resonance (NMR). Infrared spectroscopy assesses the absorption of infrared light by vibrating molecules, providing insights into the energy levels associated with molecular vibrations. In contrast, NMR utilizes radiofrequency waves to evaluate the energy required to alter the spin states of atoms within a molecule, offering a different perspective on molecular interactions. \n\nWater molecules engage in two distinct types of hydrogen bonding with neighboring molecules. The first type, O- [UNK] hydrogen bonds, occurs at the edges of tetrahedral clusters, characterized by relatively short bond lengths but weaker bond strengths. The second type, H- [UNK] hydrogen bonds, connects adjacent tetrahedra, forming larger structures known as ice crystals. These bonds exhibit longer distances compared to those at the cluster edges, yet they possess greater strength. The interplay between these two types of hydrogen bonds is fundamental to understanding the behavior of water in various states, including its liquid and solid forms. \n\nVisual representations of these hydrogen bonding interactions were generated using Visual Molecular Dynamics (VMD) software, illustrating the complex network of bonds that contribute to water's unique properties. This study aims to deepen our understanding of the hydrogen bond dynamics in water, which is essential for various scientific fields, including chemistry, biology, and environmental science. By elucidating the nuances of hydrogen bond strength, we can better appreciate the molecular underpinnings of water's behavior and its significance in sustaining life on Earth.",
        "ori-fast-z-score": -0.741998516004452,
        "water-fast-z-score": 2.2013981571160284,
        "rewrite-fast-z-score": -1.8717134551736667
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Harmonic sections in sphere bundles, normal neighborhoods of reduction loci, and instanton moduli spaces on definite 4-manifolds .\nAbstract:\nWe study the geometry of harmonic maps into spheres with values in vector bundles over Riemann surfaces. We prove that if such a map is not constant then it has no critical points outside its singular set (Theorem 1). This implies that any harmonic section of an oriented rank 2 bundle over a closed surface can be deformed to a smooth one without changing its homotopy class (Corollary 3).\nIn particular we show how this result leads to new proofs for some results about instantons on 4-dimensional manifolds due to Donaldson  D1  ,  D2  . In fact our proof gives more information than those given by Donaldson s arguments since it allows us to control the behavior of the harmonic section near its singularities. Finally we give examples showing that these results are sharp. The main theorem of this article states that every non-constant harmonic section of an oriented 2-plane bundle over a closed surface S can be deformed to another harmonic section which is smooth everywhere except at isolated points where it has only simple poles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Harmonic parts in sphere bundles , ordinary neighborhoods of reduction loci , and instanton moduli spaces on definite 4 - manifolds . Abstract : We research the topology of harmonic maps into spheres with values in vector bundles over Riemann spheres .We prove that if such a mapping is not zero then it has no important points outside its singular set ( Theorem 1 ) . This implies that any harmonic section of an oriented rank 2 bundle over a closed surface can be deformed to a smooth one without altering its homotopy class ( Corollary 3 ) .In particular we tell how this consequence leads to novel proofs for some results about instantons on 4 - dimensional manifolds due to Donaldson D1 , D2 . In reality our proof provides more information than those given by Donaldson s arguments since it allows us to control the dynamics of the chord section near its singularities .Finally we give examples demonstrating that these results are sharp . The main theorem of this page states that every non - constant harmonic section of an oriented 2 - plane bundle over a closed surface S can be deformed to another harmonic section which is continuous everywhere except at isolated points where it has only simple poles .",
        "rewrite_text": "In this article, we investigate the topology of harmonic maps that take values in vector bundles over Riemann spheres, specifically focusing on mappings into spheres. Our primary result, Theorem 1, establishes that if a harmonic map is non-zero, it does not possess critical points outside of its singular set. This finding has significant implications, particularly highlighted in Corollary 3, which asserts that any harmonic section of an oriented rank 2 bundle over a closed surface can be smoothly deformed without changing its homotopy class. \n\nMoreover, we explore how this result leads to new proofs of certain established findings regarding instantons on four-dimensional manifolds, as originally presented by Donaldson in his works D1 and D2. Our approach not only reaffirms these results but also enhances them by providing a deeper understanding of the dynamics of the chord section in proximity to its singularities. \n\nTo illustrate the applicability and limitations of our findings, we present specific examples that demonstrate the sharpness of our results. The central theorem articulated in this paper asserts that any non-constant harmonic section of an oriented 2-plane bundle over a closed surface S can be transformed into another harmonic section that remains continuous except at isolated points, where it exhibits simple poles. This work contributes to the broader discourse on the interplay between harmonic maps, topology, and the geometry of manifolds, offering fresh insights and methodologies for future research in the field.",
        "ori-fast-z-score": -0.10846522890932808,
        "water-fast-z-score": 4.528976474544414,
        "rewrite-fast-z-score": 3.1608267412297995
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Performance of the Charge Injection Capability of Suzaku XIS .\nAbstract:\nWe have investigated charge injection capability (CIC) of Suzaku X-ray Imaging Spectrometer (XIS). The CIC is an important feature for high energy resolution spectroscopy, which can be used to reduce the effect of pile-up in bright sources and improve the spectral quality at low energies. We performed detailed analysis on the performance of CIC by using Crab observations with different count rates. By comparing the results obtained with and without CIC we found that CIC improves the spectral fitting significantly especially below 1 keV where the pile-up fraction becomes large when the source flux increases. This improvement was confirmed also by analyzing simulated data generated by xisrmfgen and xissimarfgen. Finally, we applied this method to analyze the observation of Mrk 421 during its flare state. We successfully detected emission lines such as O VII Kα line and Fe XXV Kα line from the spectrum taken with CIC enabled.  These results show that CIC works well even under strong pile-up conditions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Performance of the Charge Injection Capability of Suzaku XIS . Abstract : We have researched charge injection capability ( CIC ) of Suzaku X - ray Imaging Spectrometer ( XIS ) .The CIC is an important feature for high energy resolution spectroscopy , which can be used to reduce the impact of pile - up in bright sources and improve the spectral quality at low energies . We conducted thorough analysis on the performance of CIC by using Crab observations with various count speeds .By comparing the results collected with and without CIC we concluded that CIC improves the spectral fit substantially especially below 1 keV where the pile - up proportion gets large when the source flux rises . This improvement was confirmed also by analyzing simulated evidence generated by xisrmfgen and xissimarfgen .Finally , we applied this technology to analyze the observation of Mrk 421 during its flare state . We correctly discovered emission lines such as O VII Kα line and Fe XXV Kα line from the spectrum collected with CIC enabled .These data indicate that CIC works well even under strong pile - up circumstances .",
        "rewrite_text": "Title: Performance of the Charge Injection Capability of Suzaku XIS\n\nAbstract: In this study, we investigate the charge injection capability (CIC) of the Suzaku X-ray Imaging Spectrometer (XIS), a critical feature that enhances high-energy resolution spectroscopy. The CIC plays a vital role in mitigating the effects of pile-up in bright astronomical sources, thereby improving the spectral quality, particularly at lower energy levels. Our analysis involved comprehensive observations of the Crab Nebula, conducted at various count rates, to assess the performance of the CIC. By comparing spectral data obtained with and without the CIC enabled, we found a significant enhancement in spectral fitting, particularly in the energy range below 1 keV, where pile-up effects become pronounced as source flux increases. This improvement was further corroborated through simulations generated by xisrmfgen and xissimarfgen, which provided additional evidence of the CIC's efficacy. Furthermore, we applied this advanced capability to analyze observations of the blazar Mrk 421 during a flare event. The results revealed the successful detection of emission lines, including the O VII Kα and Fe XXV Kα lines, from the spectrum acquired with the CIC enabled. These findings demonstrate that the CIC effectively operates even in scenarios characterized by substantial pile-up, underscoring its importance in enhancing the quality of spectral data in high-flux conditions. Overall, our research highlights the significant advantages of utilizing the charge injection capability in X-ray spectroscopy, paving the way for improved analysis of bright astronomical sources.",
        "ori-fast-z-score": 0.7683498199278324,
        "water-fast-z-score": 5.314796216557077,
        "rewrite-fast-z-score": 0.7492686492653552
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Report of the US long baseline neutrino experiment study .\nAbstract:\nThe Long Baseline Neutrino Experiment (LBNE) is an international collaboration to build and operate a new accelerator-based neutrino beam facility in the United States, with a proposed location at Fermilab near Chicago, Illinois.  The LBNE will use a novel design for its proton driver that combines two existing technologies - Cyclotrons and Recirculating Linacs - into one compact system.  This report describes the status of this project as of September 2009.   It also presents some preliminary results on the physics potential of the LBNE program. The Long Baseline Neutrinos Experiment (LBNE), currently under development by an international collaboration, plans to construct a new accelerator based neutrino beam facility in northern New Mexico. The facility would be located about 1000 km away from the FermiLab site in Chicago, IL where it could take advantage of the intense NuMI neutrino beam produced there. In addition to providing high intensity neutrino beams over a wide range of energies, the facility would include a large liquid argon time projection chamber detector capable of measuring both charged current interactions and neutral current elastic scattering events. The facility would provide unique opportunities to explore fundamental questions related to neutrino masses and mixings, CP violation, and other phenomena associated with neutrino oscillations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Report of the US long baseline neutrino experiment experiment . Abstract : The Long Baseline Neutrino Experiment ( LBNE ) is an global project to build and maintain a new accelerator - based neutrino laser facility in the United States , with a planned location at Fermilab near Chicago , Illinois .The LBNE will use a novel concept for its proton driver that combines two existing devices - Cyclotrons and Recirculating Linacs - into one compact device . This report explains the status of this project as of September 2009 .It additionally offers some preliminary results on the physics potential of the LBNE program . The Long Baseline Neutrinos Experiment ( LBNE ) , currently under development by an global collaboration , hopes to build a new accelerator based neutrino laser facility in northern New Mexico .The site proposed be housed about 1000 kilometers away from the FermiLab project in Chicago , IL where it could give benefit of the strong NuMI neutrino light produced there . In addition to supplying high amplitude neutrino beams over a broad variety of energies , the facility would include a large solid argon time projection chamber detector capable of monitoring both charged current interactions and neutral current elastic scattering patterns .The project would offer innovative opportunities to examine fundamental questions related to neutrino masses and mixings , CP violation , and other processes associated with neutrino oscillations .",
        "rewrite_text": "**Title:** Report on the Long Baseline Neutrino Experiment (LBNE)\n\n**Abstract:** The Long Baseline Neutrino Experiment (LBNE) represents a significant international initiative aimed at establishing a cutting-edge accelerator-based neutrino facility in the United States, with its proposed site located at Fermilab, near Chicago, Illinois. This ambitious project seeks to leverage a unique approach for its proton driver by integrating two existing technologies—Cyclotrons and Recirculating Linear Accelerators—into a single, compact system. As of September 2009, this report outlines the current status of the LBNE project and presents preliminary findings regarding its potential for advancing neutrino physics.\n\nThe LBNE is being developed by a global collaboration with plans to construct a new facility in northern New Mexico, approximately 1000 kilometers from Fermilab. This strategic location is advantageous as it would utilize the intense neutrino flux generated by the NuMI (Neutrinos at the Main Injector) facility at Fermilab. The proposed facility aims to produce high-intensity neutrino beams across a wide range of energies, which would be instrumental in probing various fundamental questions in particle physics.\n\nCentral to the LBNE's capabilities is a large solid argon time projection chamber detector, designed to effectively monitor both charged current interactions and neutral current elastic scattering events. This innovative detector technology will enable researchers to investigate critical phenomena such as neutrino masses, mixing angles, and CP violation, as well as to explore the intricate processes associated with neutrino oscillations. The findings from the LBNE are expected to provide valuable insights into the nature of neutrinos, potentially reshaping our understanding of the universe's fundamental forces and particles. Overall, the LBNE project promises to be a transformative endeavor in the field of neutrino research, paving the way for groundbreaking discoveries in the years to come.",
        "ori-fast-z-score": -2.04939015319192,
        "water-fast-z-score": 5.172270386627226,
        "rewrite-fast-z-score": -0.6163156344279367
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Crystalline silicates and dust processing in the protoplanetary disks of the Taurus young cluster .\nAbstract:\nWe present Spitzer Space Telescope observations at 24, 70, and 160 microns for 12 members of the nearby (140 pc) Taurus star-forming region with ages between 1 Myr to 10 Myr. We find that all sources show excess emission above photospheric levels indicative of circumstellar material surrounding each star. The majority of these objects are surrounded by optically thick disks which can be fit well using single temperature blackbody models. However, we also identify three systems where the disk is likely to have an inner hole or gap; TW Hya, DM Tau, and GM Aur. In addition, we detect two transitional disks around V4046 Sgr and Sz 91. These results suggest that most stars in our sample retain their primordial disks up until at least 5 Myr after formation. Finally, we use mid-infrared spectroscopy obtained with the IRS instrument onboard Spitzer to study the composition of the dust grains in the disks.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Crystalline silicates and dust production in the protoplanetary disks of the Taurus young cluster . Abstract : We report Spitzer Space Telescope observations at 24 , 70 , and 160 microns for 12 members of the nearby ( 140 pc ) Taurus star - creating area with ages between 1 Myr to 10 Myr .We see that all sources show extra emitted above photospheric concentrations indicative of circumstellar material surrounding each star . The majority of these objects are surrounded by optically dense disks which can be fit well using single temperature blackbody maps .However , we also identify three components where the disk is expected to have an inner cavity or gap ; TW Hya , DM Tau , and GM Aur . In addition , we identify two transitional disks around V4046 Sgr and Sz 91 .These data suggest that most stars in our sample maintain their primordial disks up until at least 5 Myr after formed . Finally , we utilize mid - infrared spectroscopy acquired with the IRS instrument onboard Spitzer to study the composition of the dust grains in the disks .",
        "rewrite_text": "We present findings from Spitzer Space Telescope observations conducted at wavelengths of 24, 70, and 160 microns, focusing on 12 stars within the nearby Taurus star-forming region, located approximately 140 parsecs away. These stars, ranging in age from 1 to 10 million years, exhibit excess emission beyond the expected photospheric levels, indicating the presence of circumstellar material around each star. Our analysis reveals that most of these stars are encircled by optically thick disks, which can be effectively modeled using single-temperature blackbody profiles. Notably, we have identified three specific objects—TW Hya, DM Tau, and GM Aur—where the disk structures suggest the existence of inner cavities or gaps. Furthermore, we have recognized two transitional disks around the stars V4046 Sgr and Sz 91. The data collected imply that a significant number of stars in our sample retain their primordial disks for at least 5 million years post-formation. To gain deeper insights into the composition of the dust grains within these disks, we employed mid-infrared spectroscopy using the IRS instrument on the Spitzer Space Telescope. This comprehensive study enhances our understanding of dust production and the evolution of crystalline silicates in protoplanetary disks, contributing valuable information to the field of star formation and planetary system development.",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 4.949747468305833,
        "rewrite-fast-z-score": 0.6324555320336759
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral study on the dips of Cir X-1 .\nAbstract:\nCircinus X-1 is an X-ray binary system with a neutron star and its companion, which has been observed in many wavelengths ranging from radio to gamma-ray bands. The source shows periodic dipping activity at X-ray energies that are caused by obscuration of the central X-ray emitting region due to matter falling onto the accretion disk around the compact object. In this work we present results obtained using data collected during two different observational campaigns carried out with Suzaku satellite (from 2005 to 2007) and INTEGRAL/IBIS telescope (from 2003 to 2009). We have analyzed the spectral properties of the source for both observations separately as well as combined together. Our analysis reveals that the spectrum can be described by a combination of several components such as: blackbody emission from the neutron star surface; Comptonized component produced by hot plasma surrounding the neutron star; reflection component originating from reprocessing of hard radiation emitted by the central X-ray source into softer photons; iron line feature arising from fluorescence of cold material located close to the neutron star.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral study on the dips of Cir X - 1 . Abstract : Circinus X - 1 is an X - ray binary system with a neutron star and its companion , which has been observed in multiple wavelengths ranging from radio to alpha - ray bands .The source shows intermittent dip activity at X - ray energies that are created by obscuration of the main X - ray emitting area owing to matter falling onto the accretion disk around the compact body . In this project we present results collected using data taken during two different observational campaigns carried out with Suzaku spacecraft ( from 2005 to 2007 ) and INTEGRAL / IBIS telescope ( from 2003 to 2009 ) .We have analyzed the spectral properties of the source for both surveys independently as well as combined together . Our study reveals that the spectrum can be described by a combination of several parts such as : blackbody emission from the neutron star surface ; Comptonized component produced by hot plasma surrounding the neutron star ; reflection portion arising from reprocessing of hard radiation emitted by the main X - ray source into stronger photons ; iron line feature arising from fluorescence of cold matter located close to the neutron star .",
        "rewrite_text": "**Title:** Spectral Analysis of the Dips in Circinus X-1\n\n**Abstract:** Circinus X-1 is a notable X-ray binary system comprising a neutron star and its companion, which has been extensively studied across various wavelengths, including radio and alpha-ray bands. This system exhibits intermittent dip activity at X-ray energies, a phenomenon attributed to the obscuration of the primary X-ray emission region due to the infall of matter onto the accretion disk surrounding the compact object. In this study, we present findings derived from data collected during two distinct observational campaigns: one utilizing the Suzaku spacecraft from 2005 to 2007, and the other employing the INTEGRAL/IBIS telescope from 2003 to 2009. We conducted a thorough analysis of the spectral characteristics of Circinus X-1 for each observational survey independently, as well as a combined analysis of the datasets. Our results indicate that the spectral profile of the source can be effectively modeled as a combination of several components. These include blackbody radiation emitted from the surface of the neutron star, a Comptonized component generated by the hot plasma surrounding the neutron star, and a reflection component resulting from the reprocessing of high-energy radiation emitted by the primary X-ray source into softer photons. Additionally, we identified an iron line feature, which is indicative of fluorescence from cold matter situated in proximity to the neutron star. This comprehensive spectral study enhances our understanding of the complex interactions occurring within the Circinus X-1 system and contributes valuable insights into the behavior of X-ray binaries.",
        "ori-fast-z-score": 1.5230192477004287,
        "water-fast-z-score": 5.858884758402822,
        "rewrite-fast-z-score": 1.025755289064345
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comment on the statistical analysis in  A new experimental limit for the stability of the electron  by H.V. Klapdor-Kleingrothaus, I.V. Krivosheina and I.V. Titkova .\nAbstract:\nWe have read with interest the recent preprint  1  . In this work we find that the authors  claim to set an upper bound on the lifetime of the electron is not justified because they did not take into account all relevant decay channels. \n \n We would like to comment briefly on their treatment of backgrounds as well as their choice of cuts used to select events. The main source of background comes from radiative Bhabha scattering e+e-→e+e-γ which has been studied extensively at LEP2  2  , where it was found to be negligible compared to other sources such as two-photon processes or four-fermion final states (e.g., W pair production). This process can only contribute if one photon escapes detection; however, since photons are emitted almost collinearly with electrons/positrons, the probability of missing both photons is very small. Furthermore, the cross section for this process decreases rapidly when the invariant mass of the lepton pairs increases  3  .\n \nThe authors also use a cut on the total energy of the event, Evis>10 GeV, which removes most of these events. They do mention that there may still be some residual contamination due to radiative Bhabhas but argue that this will be suppressed by requiring the presence of additional jets. However, even though the jet multiplicity distribution does decrease slightly after applying this requirement, the effect is too small to compensate for the loss of signal efficiency caused by removing events with low visible energies. \n \nIn addition, the authors state that the contribution from radiative Bhabhas should be included in the systematic uncertainty estimate. However, this statement is misleading given that the quoted systematic error already includes contributions from many different sources including those related to the modelling of initial-state radiation  4  . \n\n\nFinally, we note that the authors present results obtained using Monte Carlo simulations performed with PYTHIA 6  5  . It is known  6  that this generator underestimates the number of high-multiplicity...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comment on the statistical analysis in A new empirical limit for the stability of the electron by H . V . Klapdor - Kleingrothaus , I . V .Krivosheina and I.V.Titkova .Abstract : We have written with interest the recent preprint 1 . In this research we find that the writers claim to setting an upper bound on the life of the electron is not justified because they did not take into consideration all relevant degradation channels .We would like to comment briefly on their handling of backgrounds as also as their choosing of cuts needed to select events . The main source of background comes from radiative Bhabha scattering e + e - →e + e - γ which has been studied thoroughly at LEP2 2 , where it was shown to be negligible compared to other sources such as two - photon processes or four - fermion final states ( e . g . , W pair production ) .This process can only contribute if one photon escapes detection ; however , since photons are emitted virtually collinearly with electrons / positrons , the probability of missing both photons is very small . Furthermore , the cross area for this process reduces rapidly when the invariant mass of the lepton pairs increases 3 .The authors also use a cutting on the total energy of the event , Evis > 10 GeV , which eliminate most of these events . They do mention that there may still be some residual contamination owing to radiative Bhabhas but suggest that this will be suppressed by requiring the presence of added jets .However , even though the jet multiplicity distribution does decrease slightly after applying this requirement , the effect is too small to compensate for the losing of signal efficiency created by removing events with lowered visible energies . In addition , the writers state that the impact from radiative Bhabhas should be included in the systematic uncertainty estimate .However , this statement is misleading given that the quoted systematic error also contains contributions from many various sources including those related to the modelling of initial - state radiation 4 . Finally , we note that the writers present results acquired using Monte Carlo simulations conducted with PYTHIA 6 5 .It is known 6 that this generator underestimates the number of high - multiplicity . . .",
        "rewrite_text": "**Title:** Commentary on the Statistical Analysis in \"A New Empirical Limit for the Stability of the Electron\" by H. V. Klapdor-Kleingrothaus, I. V. Krivosheina, and I. V. Titkova\n\n**Abstract:** In this commentary, we critically evaluate the recent preprint concerning the upper limit on the electron's lifespan proposed by Klapdor-Kleingrothaus et al. We argue that their conclusions are not adequately supported due to the omission of several significant degradation channels in their analysis. Our focus is on their treatment of background processes and the criteria they employed for event selection. A primary background source identified is the radiative Bhabha scattering process (e+e- → e+e-γ), which has been extensively analyzed at LEP2. Previous studies have demonstrated that this background is minimal when compared to other processes, such as two-photon interactions or four-fermion final states, including W pair production. The contribution from radiative Bhabha scattering is contingent upon the detection of both emitted photons; however, the likelihood of both photons escaping detection is exceedingly low due to their nearly collinear emission with the electrons and positrons. Additionally, the cross-section for this process diminishes significantly as the invariant mass of the lepton pairs increases. The authors impose a cut on the total visible energy of the event, requiring Evis > 10 GeV, which effectively excludes the majority of these background events. While they acknowledge the potential for residual contamination from radiative Bhabhas, they suggest that this can be mitigated by the presence of additional jets. However, we contend that the reduction in jet multiplicity does not sufficiently offset the loss of signal efficiency resulting from the exclusion of events with lower visible energies. Furthermore, the authors assert that the influence of radiative Bhabhas should be factored into the systematic uncertainty estimates. This assertion is misleading, as the systematic error encompasses contributions from a variety of sources, including those associated with initial-state radiation modeling. Lastly, we highlight that the results presented are derived from Monte Carlo simulations using the PYTHIA 6 generator, which is known to underestimate high-multiplicity events. This oversight raises further concerns regarding the robustness of their findings.",
        "ori-fast-z-score": 0.6285393610547089,
        "water-fast-z-score": 6.501061734900047,
        "rewrite-fast-z-score": 1.5
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Astrometry with  Carte du Ciel  plates, San Fernando zone. I. Digitization and measurement using a flatbed scanner .\nAbstract:\nWe present the results obtained in digitizing and measuring astrometrically about 1000 photographic plates taken by the Carte du Ciel project at the Observatorio Astronómico Nacional (OAN), San Pedro Mártir Observatory (SPMO) between 1950 and 1960. The OAN/SPMO is located on Mount Wilson, California, USA. We used a high resolution digital camera to scan these plates and then measured their positions relative to reference stars observed simultaneously on each plate. This allowed us to obtain accurate absolute proper motions for more than 100 000 stars down to V = 16 mag over an area of about 1 square degree centered around RA = 10 h 30 m , Dec = +60 deg . These data are available online through our web site http://obswww.unige.ch/~fuchs/CDCCatastro/Movies/movie1.html .\nThe accuracy achieved in this work is comparable or better than that reported previously for similar projects based on scanning film material. \n\n\nThis research was supported by NSF grant AST-0307481.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Astrometry with Carte du Ciel plates , San Fernando region . I . Digitization and measurement using a flatbed scanner .Abstract : We present the results derived in digitizing and observing astrometrically about 1000 photo plates taken by the Carte du Ciel programme at the Observatorio Astronómico Nacional ( OAN ) , San Pedro Mártir Observatory ( SPMO ) between 1950 and 1960 . The OAN / SPMO is situated on Mount Wilson , California , USA .We utilized a high resolution digital camera to scan these plates and then calculated their movements relative to reference stars observed concurrently on each plate . This able us to obtain precise absolute proper motions for more than 100 000 stars down to V = 16 mag over an area of about 1 sq degree centered around RA = 10 h 30 m , Dec = + 60 deg .These data are available available through our online site www : / / obswww . unige . ch / ~ fuchs / CDCCatastro / Movies / movie1 . html . The accuracy achieved in this project is identical or better than that obtained previously for related works based on scan film material .This research was supported by NSF grant AST-0307481.",
        "rewrite_text": "We present a comprehensive study on the digitization and astrometric analysis of approximately 1,000 photographic plates from the Carte du Ciel program, collected at the Observatorio Astronómico Nacional (OAN) and San Pedro Mártir Observatory (SPMO) between 1950 and 1960. Located on Mount Wilson in California, USA, the OAN/SPMO has been instrumental in astronomical research. Our methodology involved utilizing a high-resolution digital camera to meticulously scan the plates, followed by calculating the movements of celestial objects in relation to reference stars that were observed simultaneously on each plate. This process enabled us to derive precise absolute proper motions for over 100,000 stars, reaching down to a magnitude of V = 16, within an area of approximately 1 square degree centered at right ascension 10h 30m and declination +60 degrees. The resulting data set is accessible through our dedicated online platform at www.obswww.unige.ch/~fuchs/CDCCatastro/Movies/movie1.html. Notably, the accuracy achieved in this digitization and measurement project is on par with or exceeds that of previous studies utilizing scanned film materials. This research was made possible through the support of the National Science Foundation grant AST-0307481, underscoring the significance of funding in advancing astronomical research and data accessibility. Our findings contribute valuable insights into stellar motion and enhance the existing astronomical databases, paving the way for future studies in the field.",
        "ori-fast-z-score": -1.0533703247651751,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": -1.0101525445522108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electrical excitation of shock and soliton-like waves in two-dimensional electron channels .\nAbstract:\nWe study the electrical excitation of nonlinear waves in a 2D channel with an applied voltage bias between two parallel metal plates separated by a dielectric layer. We show that, depending on the parameters of the system (the thickness of the dielectric layer, the density of electrons), different types of nonlinear waves can be excited. In particular, we find that for certain values of these parameters solitary wave solutions exist which are similar to those found earlier in 1D systems. \n \n The existence of such solitary waves is confirmed experimentally using time-resolved optical reflectivity measurements performed at room temperature on samples consisting of GaAs/AlGaAs quantum wells grown by molecular beam epitaxy. These experiments reveal the presence of bright solitary waves propagating along the direction perpendicular to the applied electric field. Their propagation velocities agree well with theoretical predictions based on numerical simulations of the underlying equations. \nThe results presented here may have important applications in semiconductor devices where it has been shown recently that the generation of solitary waves leads to improved performance characteristics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electrical excitation of shock and soliton - like waves in two - dimensional electron channels . Abstract : We research the electrical excitation of nonlinear waves in a 2D channel with an applied voltage bias between two connected metal plates connected by a dielectric layer .We see that , depending on the variables of the system ( the height of the dielectric layer , the density of electrons ) , different kinds of nonlinear waves can be excited . In particular , we find that for particular values of these parameters solitary wave systems occur which are comparable to those observed earlier in 1D systems .The nature of such solitary waves is discovered experimentally utilizing period - resolved optical reflectivity surveys performed at room temperature on samples consisting of GaAs / AlGaAs quantum wells developed by molecular beam epitaxy . These studies reveal the presence of bright solitary waves propagating along the direction perpendicular to the applied electric field .Their propagation velocities agree well with theoretical estimates based on numerical simulations of the underlying equations . The results presented here possibly have important use in semiconductor devices where it has been shown lately that the generation of solitary waves gives to improved performance qualities .",
        "rewrite_text": "Title: Electrical Excitation of Shock and Soliton-like Waves in Two-Dimensional Electron Channels\n\nAbstract: This study investigates the electrical excitation of nonlinear wave phenomena within a two-dimensional electron channel, where a voltage bias is applied across two metal plates separated by a dielectric layer. Our findings indicate that the characteristics of the nonlinear waves that can be excited are highly dependent on specific system parameters, such as the height of the dielectric layer and the electron density. Notably, we observe the emergence of solitary wave systems under certain conditions, which bear resemblance to those previously documented in one-dimensional systems. To elucidate the properties of these solitary waves, we conducted period-resolved optical reflectivity experiments at room temperature on GaAs/AlGaAs quantum well samples fabricated through molecular beam epitaxy. The experimental results reveal the existence of bright solitary waves that propagate perpendicular to the direction of the applied electric field. Furthermore, the measured propagation velocities of these waves align closely with theoretical predictions derived from numerical simulations of the governing equations. The implications of our findings are significant, particularly for the advancement of semiconductor devices, as recent research has indicated that the generation of solitary waves can enhance performance characteristics. This work contributes to the understanding of nonlinear wave dynamics in two-dimensional electron systems and opens avenues for future applications in electronic and optoelectronic technologies.",
        "ori-fast-z-score": -0.3922322702763681,
        "water-fast-z-score": 4.902903378454601,
        "rewrite-fast-z-score": 2.3945657130528786
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Annotations to a certain passage of Descartes for finding the quadrature of the circle .\nAbstract:\nThe following is an excerpt from René Decartes  Discourse on Method, Book II (1637): \nI have found by experiment that if I take any right line and divide it into equal parts in such manner as to make one part double another, then the square which shall be formed with these two parts will be equal to the rectangle whose sides are those lines; so that this figure may serve me instead of the square root. \n \n If we suppose that the diameter AB of a circle equals 2, and that the radius AC equals 1, then the area of the circle will be equal to 4 times the square of the radius, or 8 times the square of half its diameter, because the square of twice the diameter is equal to four squares of the same side. The area of the circle therefore being known, it remains only to find how many degrees there are contained within it; but since the circumference of a circle is always equal to three times the diameter, it follows that the number of degrees contained therein must also be 3 times the number of degrees contained between the diameter and the center of the circle.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Annotations to a certain passage of Descartes for finding the quadrature of the circle . Abstract : The following is an excerpt from René Decartes Discourse on Method , Book II ( 1637 ) : I have discovered by experiment that if I taking any right line and divide it into equal parts in such manner as to make one part twice another , then the square which shall be formed with these two parts will be equal to the rectangle whose sides are those lines ; so that this figure might serve me instead of the square root .If we suppose that the radius AB of a circle equals 2 , and that the radius AC equals 1 , then the area of the circle will be equal to 4 times the square of the radius , or 8 times the square of half its radius , because the square of twice the radius is equal to four squares of the same side . The area of the circle therefore being known , it remains only to find how many degrees there are contained within it ; but since the circumference of a circle is never equal to three times the radius , it follows that the number of degrees contained therein must additionally be 3 times the number of degrees contained between the radius and the center of the circle .",
        "rewrite_text": "Title: Annotations on a Specific Passage of Descartes Regarding the Quadrature of the Circle\n\nAbstract: This article delves into a notable excerpt from René Descartes' \"Discourse on Method,\" specifically from Book II, published in 1637. In this passage, Descartes presents a geometric experiment that reveals a relationship between divided line segments and the areas they generate. He posits that when a straight line is divided into segments such that one segment is twice the length of another, the square formed by these segments is equivalent to the rectangle defined by the same segments. This geometric insight serves as a foundational concept for understanding the square root in his analysis. \n\nTo illustrate his findings, Descartes considers a circle with a radius AB of 2 units and another radius AC of 1 unit. He calculates the area of the circle as being four times the square of the radius, which can also be expressed as eight times the square of half the radius. This leads to the conclusion that the area of the circle is determined by the square of the radius, emphasizing the geometric relationships at play. \n\nHowever, Descartes acknowledges a critical challenge: determining the number of degrees contained within the circle. He notes that the circumference of a circle does not equate to three times the radius, which implies that the degrees within the circle must be calculated as three times the degrees between the radius and the circle's center. This exploration not only highlights Descartes' innovative approach to geometry but also raises fundamental questions about the nature of circular measurements and the quest for the quadrature of the circle. Through this analysis, the article aims to provide a deeper understanding of Descartes' contributions to mathematics and the implications of his geometric principles.",
        "ori-fast-z-score": 0.7592566023652966,
        "water-fast-z-score": 3.450648742510029,
        "rewrite-fast-z-score": 0.658504607868518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Star-Formation in Low Radio Luminosity AGN from the Sloan Digital Sky Survey .\nAbstract:\nWe present an analysis of star-formation activity associated with low radio luminosity active galactic nuclei (AGNs) using data from the Sloan DigitalSky Survey (SDSS). We find that these objects are typically hosted by massive galaxies, and have high specific star formation rates compared to inactive galaxies at similar redshifts. The majority of our sample is found to be obscured by dusty torii, as indicated by their optical colors and infrared emission. These results suggest that there may exist two populations of AGN: one which hosts significant amounts of star formation, and another where no such activity is observed. This work was supported by NASA grant NNG05GJ40G. Active Galactic Nuclei (AGNs), powered by supermassive black holes accreting matter from surrounding gas clouds, are known to produce copious quantities of radiation across all wavelengths. However, it has been unclear whether this energy output also leads to enhanced levels of star formation within host galaxies. In order to investigate this question we use data from the SloanDigital Sky Survey (SDSS; York et al., 2000) , specifically targeting sources classified as narrow-line Seyfert 1 s (NLS1s) based on their optical spectra. NLS1s represent a subclass of AGNs whose properties differ significantly from those of more typical broad line quasars (BLQs; Osterbrock & Pogge 1985) . They tend to reside in lower mass galaxies than BLQSOs, and exhibit higher Eddington ratios (Boller et al., 1996; Grupe, Thomas, & Leighly 1999; Mathur 2000; Komossa et al., 2006a ,b Gallo 2007 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Star - Formation in Low Radio Luminosity AGN from the Sloan Digital Sky Survey . Abstract : We present an assessment of star - formation activity related with lowest radio luminosity active galactic nuclei ( AGNs ) using data from the Sloan DigitalSky Survey ( SDSS ) .We see that these objects are typically held by massive galaxies , and have greater particular galaxy formation rates relative to dormant stars at comparable redshifts . The majority of our sample is found to be obscured by dusty torii , as indicated by their optical colors and infrared absorption .These data suggest that there may arise two communities of AGN : one which contains substantial concentrations of star formation , and another where no such activity is observed . This research was supported by NASA grant NNG05GJ40G .Active Galactic Nuclei ( AGNs ) , driven by supermassive black holes accreting matter from surrounding gas clouds , are known to produce copious quantities of radiation across all wavelengths . However , it has been uncertain whether this electricity production actually results to heightened levels of galaxy formation within host galaxies .In order to examine this question we using data from the SloanDigital Sky Survey ( SDSS ; York et al . , 2000 ) , explicitly targeting sources classified as short - range Seyfert 1 s ( NLS1s ) based on their optical spectra . NLS1s represent a subclass of AGNs whose characteristics vary significantly from those of more typical broad line quasars ( BLQs ; Osterbrock & Pogge 1985 ) .They tend to live in smaller mass stars than BLQSOs , and possess larger Eddington ratios ( Boller et al . , 1996 ; Grupe , Thomas , & Leighly 1999 ; Mathur 2000 ; Komossa et al . , 2006a , b Gallo 2007 .",
        "rewrite_text": "We provide an analysis of star formation activity associated with low radio luminosity active galactic nuclei (AGNs) utilizing data from the Sloan Digital Sky Survey (SDSS). Our findings indicate that these AGNs are predominantly hosted by massive galaxies and exhibit higher specific star formation rates compared to inactive stellar populations at similar redshifts. A significant portion of our sample appears to be obscured by dusty tori, as evidenced by their optical colors and infrared absorption characteristics. This observation leads us to propose the existence of two distinct categories of AGNs: one characterized by substantial star formation activity and another where such activity is absent. This study is supported by NASA grant NNG05GJ40G. \n\nActive Galactic Nuclei (AGNs), which are powered by supermassive black holes that accrete matter from surrounding gas clouds, are known to emit vast amounts of radiation across a wide range of wavelengths. However, the relationship between this radiation output and the enhancement of star formation within their host galaxies has remained ambiguous. To investigate this relationship, we focused on sources classified as narrow-line Seyfert 1s (NLS1s) based on their optical spectral features, using data from the SDSS (York et al., 2000). NLS1s represent a unique subclass of AGNs that exhibit distinct properties compared to more conventional broad-line quasars (BLQs; Osterbrock & Pogge, 1985). Notably, NLS1s are typically found in lower mass galaxies than BLQSOs and demonstrate higher Eddington ratios (Boller et al., 1996; Grupe, Thomas, & Leighly, 1999; Mathur, 2000; Komossa et al., 2006a, b; Gallo, 2007). Our research aims to elucidate the role of low radio luminosity AGNs in the context of star formation, contributing to a deeper understanding of the interplay between black hole activity and galaxy evolution.",
        "ori-fast-z-score": -0.6527533657682196,
        "water-fast-z-score": 6.247782215210102,
        "rewrite-fast-z-score": 0.7016464154456235
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surveys of the Galactic Center and the Nature of the Galactic Center Lobe .\nAbstract:\nThe Galactic center (GC) is one of the most interesting regions in our Galaxy, but it has been difficult to study because of its proximity to the Sun. The GC contains many compact radio sources that are thought to be associated with young pulsars or magnetars. In this thesis we present results on two surveys at 1.4 GHz using the Australia Telescope Compact Array (ATCA). We have observed the central region of the Galaxy for about 100 hours over three epochs between 2005-2007. Our first survey covers an area of 2 degrees centered around Sgr A*. This survey was designed to search for new compact radio sources near the GC as well as to investigate the nature of the diffuse emission surrounding Sgr A*. Our second survey covered a larger area of 4 degrees centered around the GC. This survey was aimed at studying the distribution of molecular gas towards the GC by observing the 12 CO(J=1-0), 13 CO(J=1-0) and C 18 O(J=1-0) lines simultaneously.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surveys of the Galactic Center and the Nature of the Galactic Center Lobe . Abstract : The Galactic center ( GC ) is one of the most exciting regions in our Galaxy , but it has been difficult to study because of its proximity to the Sun .The GC comprises many compact radio sources that are said to be identified with young pulsars or magnetars . In this dissertation we present results on two studies at 1 . 4 GHz use the Australia Telescope Compact Array ( ATCA ) .We have discovered the central region of the Galaxy for about 100 hours over three epochs between 2005 - 2007 . Our first survey encompasses an area of 2 degrees centered around Sgr A * .This study was built to search for additional compact radio sources near the GC as well as to examine the nature of the diffuse emission surrounding Sgr A * . Our second survey included a greater area of 4 degrees centered around the GC .This study was aiming at studying the distribution of molecular gas towards the GC by observing the 12 CO ( J = 1 - 0 ) , 13 CO ( J = 1 - 0 ) and C 18 O ( J = 1 - 0 ) tracks jointly .",
        "rewrite_text": "**Title:** Surveys of the Galactic Center and the Nature of the Galactic Center Lobe\n\n**Abstract:** The Galactic Center (GC) represents one of the most intriguing and complex regions within our Galaxy, yet its study has posed significant challenges due to its relative proximity to the Sun. This area is characterized by a multitude of compact radio sources, many of which are believed to be associated with young pulsars or magnetars. In this dissertation, we present findings from two comprehensive studies conducted at a frequency of 1.4 GHz using the Australia Telescope Compact Array (ATCA). Over the course of approximately 100 hours, we surveyed the central region of the Galaxy across three distinct epochs between 2005 and 2007. \n\nThe first survey focused on a 2-degree area centered around Sagittarius A* (Sgr A*), aiming to identify additional compact radio sources in the vicinity of the GC and to investigate the nature of the diffuse emission surrounding Sgr A*. This initial exploration has provided valuable insights into the population of radio sources in this region, enhancing our understanding of their characteristics and potential origins.\n\nThe second survey expanded the scope to a larger 4-degree area centered on the GC, with the objective of analyzing the distribution of molecular gas in this region. By jointly observing the spectral lines of 12CO (J = 1-0), 13CO (J = 1-0), and C18O (J = 1-0), we aimed to elucidate the molecular composition and dynamics of the gas surrounding the GC. The results from both surveys contribute significantly to our knowledge of the Galactic Center, offering new perspectives on its complex structure and the processes occurring within this dynamic environment. Through these investigations, we hope to pave the way for future research that will further unravel the mysteries of the Galactic Center and its role in the broader context of galactic evolution.",
        "ori-fast-z-score": -0.6201736729460423,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Future of the Local Large Scale Structure: the roles of Dark Matter and Dark Energy .\nAbstract:\nThe future evolution of the local large scale structure is investigated by using N-body simulations with different cosmological models, including dark energy (DE) and cold dark matter (CDM). The results show that in the Λ-CDM model without DE, the growth rate of large-scale structures decreases rapidly at late times due to the effect of CDM. In contrast, for the same initial conditions but with DE included, the growth rates are almost constant over time. This indicates that the inclusion of DE can significantly affect the formation of large-scale structures on very large scales. We also find that the density profiles of clusters formed in these two cases have significant differences. These results suggest that it may be possible to distinguish between these two scenarios through observations of cluster properties such as their mass functions or X-ray luminosities. Keywords: Cold dark matter; Dark energy; Growth factor; Clustering statistics; Density profile; Cosmology",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Future of the Local Large Scale Structure : the roles of Dark Matter and Dark Energy . Abstract : The future emergence of the local big scale system is investigated by using N - bodies simulations with various cosmological models , notably dark energy ( DE ) and chilled dark matter ( CDM ) .The results show that in the Λ - CDM theory without DE , the development frequency of large - scale structures decreases quickly at late times due to the impact of CDM . In comparison , for the same original conditions but with DE included , the development rates are almost steady over time .This implies that the introduction of DE can significantly affect the formation of large - scale structures on very huge scales . We additionally find that the density characteristics of clusters formed in these two examples have considerable changes .These data suggest that it could be possible to distinguish between these two scenarios through observations of cluster components such as their mass distributions or X - ray luminosities . Keywords : Cold black material ; Dark energy ; Growth factor ; Clustering statistics ; Density profile ; Cosmology",
        "rewrite_text": "**Title:** The Future of the Local Large Scale Structure: The Roles of Dark Matter and Dark Energy\n\n**Abstract:** This study explores the future evolution of the local large-scale structure of the universe through N-body simulations that incorporate various cosmological models, particularly focusing on the influences of dark energy (DE) and cold dark matter (CDM). Our findings reveal that within the framework of the Λ-CDM model, which excludes dark energy, the rate of large-scale structure development diminishes rapidly in the later stages of cosmic evolution due to the effects of cold dark matter. In contrast, when dark energy is included under identical initial conditions, the growth rates of large-scale structures remain relatively constant over time. This significant difference suggests that the presence of dark energy plays a crucial role in shaping the formation and evolution of large-scale structures across vast cosmic distances. Furthermore, our analysis indicates that the density profiles of clusters formed under these two scenarios exhibit notable variations. These results imply that observational data, such as mass distributions and X-ray luminosities of galaxy clusters, could provide a means to differentiate between the two cosmological models. By examining these characteristics, we may gain insights into the underlying mechanisms governing the universe's expansion and structure formation, thereby enhancing our understanding of the interplay between dark matter and dark energy in the cosmos. This research contributes to the broader field of cosmology by highlighting the importance of these components in the evolution of the universe's large-scale structure and suggests potential observational strategies for future studies. \n\n**Keywords:** Cold dark matter; Dark energy; Growth factor; Clustering statistics; Density profile; Cosmology.",
        "ori-fast-z-score": -0.7504787743864564,
        "water-fast-z-score": 6.182820774312702,
        "rewrite-fast-z-score": 1.2543630150106362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The standard model on a domain-wall brane? .\nAbstract:\nWe consider the Standard Model (SM) in 5 dimensions, where one extra dimension is compactified to an orbifold S 1 /Z 2 . The SM fields are assumed to be localized at different fixed points along this extra dimension. We show that such models can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings. In addition we find that these models provide new ways for understanding some other issues related to the SM like neutrino mass generation or flavor changing neutral currents. Finally we discuss how our results could be tested experimentally. Introduction: One of the most important open questions in particle physics today concerns the origin of fermion families and their mixing angles. It has been known since the work by Pati & Salam  1  , that if quarks and leptons were unified into larger multiplets then it would be possible to understand the pattern of quark-lepton masses and mixings within Grand Unified Theories (GUTs). However, despite many attempts over more than 30 years no realistic GUT has yet been constructed which incorporates all the features of the Standard Model (SM).\nIn recent years another possibility was suggested  2  -  4  : If the SM fields live in higher dimensional space-time, they may have Kaluza-Klein excitations corresponding to additional states with masses of order 1/R, where R denotes the size of the extra dimensions. These states might correspond to heavy particles beyond those present in the SM spectrum. This idea leads to interesting phenomenological consequences  5  .\nThe simplest way to realize this scenario is to assume that only gravity propagates in the bulk while the SM fields are confined to a four-dimensional  brane   6  . Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by: \nwhere M P l = 1/ √ 8πG N ≈ 10 19 GeV is the reduced Planck scale and n i counts the number of extra spatial dimensions accessible to field i. For distances smaller than about 0.1 mm deviations from the inverse square law predicted by general relativity will become",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The basic model on a domain - wall brane ? .Abstract : We consider the Standard Model ( SM ) in 5 dimensions , where one extra dimension is compactified to an orbifold S 1 / Z 2 . The SM fields are expected to be localized at different fixed points along this extra dimension .We see that such theories can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings . In addition we find that these models bring fresh ways for explaining some other issues related to the SM like neutrino mass development or color shifting neutral currents .Finally we talk how our findings may be evaluated experimentally . Introduction : One of the most important open questions in particle science today issues the origin of fermion families and their mixing angles .It has been known since the paper by Pati & Salam 1 , that if quarks and leptons were organized into larger multiplets then it would be possible to explain the trend of quark - lepton masses and mixings within Grand Unified Theories ( GUTs ) . However , despite many efforts over more than 30 centuries no realistic GUT has already been constructed which includes all the details of the Standard Model ( SM ) .In recent work another possibility was suggested 2 - 4 : If the SM fields reside in larger dimensional space - time , they may have Kaluza - Klein excitations corresponding to extra states with masses of order 1 / R , where R denotes the height of the extra dimensions . These states could belong to heavy ions beyond those present in the SM spectrum .This idea results to useful phenomenological consequences 5 . The shortest way to realize this situation is to assume that only gravitational propagates in the bulk while the SM fields are confined to a four - dimensional brane 6 .Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by : where M P l = 1 / √ 8πG N ≈ 10 19 GeV is the reduced Planck scale and n i counts the number of extra spatial dimensions accessible to field i . For distances smaller than about 0 . 1 mm deviations from the inverse square law predicted by particular relativity will become",
        "rewrite_text": "**Title:** A Fundamental Model on a Domain-Wall Brane\n\n**Abstract:** In this study, we investigate the Standard Model (SM) extended to five dimensions, where one additional dimension is compactified into an orbifold S^1/Z_2. We propose that the SM fields are localized at distinct fixed points along this extra dimension, which allows for a natural explanation of the existence of three generations of fermions and gauge bosons, along with their observed masses and mixing patterns. Our analysis reveals that these higher-dimensional models provide novel insights into several unresolved issues within the SM, including the generation of neutrino masses and the phenomenon of color-shifting neutral currents. Furthermore, we discuss the potential experimental implications of our findings, suggesting avenues for future research to validate the theoretical framework we present. \n\nThe introduction highlights a critical question in particle physics: the origin of fermion families and their mixing angles. Previous work, notably by Pati and Salam, indicated that organizing quarks and leptons into larger multiplets could elucidate the observed trends in quark-lepton masses and mixing within Grand Unified Theories (GUTs). However, despite extensive efforts over the past three decades, a comprehensive GUT that incorporates all aspects of the SM remains elusive. Recent proposals suggest that if SM fields exist in a higher-dimensional spacetime, they may exhibit Kaluza-Klein excitations, leading to additional states with masses proportional to 1/R, where R represents the scale of the extra dimension. These states could correspond to heavy particles not accounted for in the current SM framework, yielding significant phenomenological implications. \n\nTo realize this scenario, we assume that only gravitational interactions propagate through the bulk, while the SM fields are confined to a four-dimensional brane. This setup results in modifications to the Newtonian gravitational potential between two test masses, m_1 and m_2, separated by a distance r. Specifically, we derive corrections that become significant at distances smaller than approximately 0.1 mm, where deviations from the inverse square law predicted by special relativity may be observed. Our findings pave the way for experimental investigations that could further elucidate the nature of fundamental interactions in higher-dimensional theories.",
        "ori-fast-z-score": 2.182820625326997,
        "water-fast-z-score": 8.433802953476238,
        "rewrite-fast-z-score": 3.00416377715996
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Estimation of experimental data redundancy and related statistics .\nAbstract:\nThe problem is to estimate the number of independent measurements in an experiment, given that some statistical properties are known for each measurement. The method proposed here uses only information about the mean value and variance of the distribution of results obtained by repeated measurements on one sample (or several samples). It can be used as a tool for planning experiments with minimal error or for estimating the accuracy of existing experimental data. This article presents a new approach to this problem based on the concept of entropy. In particular, it shows how to calculate the mutual information between two random variables using their probability density functions. A numerical example illustrates the application of these methods. \n \n Keywords: Redundancy estimation, Entropy, Mutual Information, Experiment Planning \n \n 1 Introduction \n \n When designing an experiment, we need to know what kind of precision our measuring instrument will give us. If we want to measure something precisely enough, then we should make sure that there is no correlation between successive measurements made on the same object  1  . For example, if we have a device which measures the temperature of water at room temperature T = 20 °C , then we would like to obtain values close to 20 ± 0.1°C when repeating the measurement many times  2  .\n \nIn practice, however, such repeatability cannot always be achieved because of various factors affecting the measurement process  3  . Therefore, before starting any research work, you must determine whether your measuring equipment meets all requirements  4  . \n \n 2 Problem statement \n \n Let X be a continuous random variable describing the result of a single measurement performed under certain conditions  5  . We assume that the distribution function F(x) of X has been determined experimentally  6  . Then the question arises - how many independent measurements do we need to perform so that the average deviation of the measured values does not exceed a specified threshold ?",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Estimation of experimental evidence redundancy and related statistics . Abstract : The question is to estimate the proportion of separate measurements in an observation , provided that some statistical characteristics are known for each measurement .The method adopted here uses only data about the mean value and variance of the distribution of findings obtained by repeated measurements on one sample ( or several samples ) . It can be used as a technique for planning studies with minimal error or for estimating the accuracy of older experimental evidence .This page presents a new approach to this question based on the idea of entropy . In particular , it demonstrates how to estimate the mutual information between two random factors using their likelihood density functions .A numerical example illustrates the implementation of these tools . Keywords : Redundancy estimation , Entropy , Mutual Information , Experiment Planning 1 Introduction When designing an experiment , we must to consider what sort of precision our measuring instrument will provide us .If we wish to measure everything precisely enough , then we should make sure that there is no coupling between successive measurements made on the same object 1 . For instance , if we have a device which studies the temperature of water at room temperature T = 20 °C , then we may like to obtain measures close to 20 ± 0 . 1°C when repeating the observation many times 2 .In practice , however , such repeatability cannot often be obtained because of several variables affecting the monitoring process 3 . Therefore , before beginning any study work , you must identify whether your measuring apparatus meets all requirements 4 .2 Problem statement Let X be a continuous random variable describing the result of a single observation performed under certain conditions 5 . We assume that the distribution relation F ( x ) of X has been determined experimentally 6 .Then the question arises - how many independent tests do we require to conduct so that the average deviation of the tested values does not reach a specified threshold ?",
        "rewrite_text": "**Title:** Estimation of Experimental Evidence Redundancy and Related Statistics\n\n**Abstract:** This study addresses the challenge of estimating the proportion of independent measurements within a given observation, contingent upon known statistical characteristics for each measurement. The proposed methodology relies solely on the mean and variance derived from repeated measurements on one or multiple samples. This approach serves as a valuable tool for designing experiments with minimal error and for assessing the reliability of existing experimental data. The paper introduces a novel perspective on this issue through the lens of entropy, specifically focusing on the estimation of mutual information between two random variables by utilizing their likelihood density functions. A numerical example is provided to demonstrate the practical application of these concepts. \n\nIn the context of experimental design, it is crucial to evaluate the precision of the measuring instruments employed. To achieve a high degree of accuracy, it is essential to ensure that there is no correlation between successive measurements taken from the same subject. For instance, when measuring the temperature of water at a stable room temperature of T = 20 °C, one would aim for repeated measurements to fall within the range of 20 ± 0.1 °C. However, achieving such consistency is often hindered by various factors influencing the measurement process. Therefore, prior to initiating any experimental work, it is imperative to ascertain whether the measuring equipment fulfills all necessary criteria.\n\nThe problem is framed as follows: Let X represent a continuous random variable that captures the outcome of a single observation conducted under specific conditions. Assuming that the distribution function F(x) of X has been established through experimental means, the central question emerges: how many independent trials must be performed to ensure that the average deviation of the observed values remains within a predetermined threshold? This inquiry is fundamental for enhancing the reliability and validity of experimental findings. \n\n**Keywords:** Redundancy estimation, Entropy, Mutual Information, Experiment Planning",
        "ori-fast-z-score": -0.8778955729143844,
        "water-fast-z-score": 7.851210698609627,
        "rewrite-fast-z-score": -0.9669875568304563
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Long Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives .\nAbstract:\nThe rapid growth in the use and production of digital media has created an urgent need to develop new models that support long-term access, preservation, and reuse of personal archives.  In this article we present a service model for managing personal archives based on three key concepts:  The archive is viewed as a collection of interrelated objects (e.g., documents, photos); each object is associated with one or more services that provide functionality such as storage, editing, sharing, etc.; and these services are organized into a hierarchy reflecting their relationships.   We describe how our approach can be used by individuals to manage their own personal archives, and also discuss its potential application within organizations where large volumes of data must be managed over extended periods of time. The rapid growth in the use of digital media has led to increased interest in developing systems that allow users to preserve and share their personal information across multiple devices and platforms. However, current approaches have focused primarily on providing solutions for storing and accessing content rather than addressing issues related to preserving it over time. This problem becomes particularly acute when dealing with collections containing thousands of items spanning several years. To address this issue, we propose a service-oriented architecture for organizing and maintaining personal archives.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Long Term Fate of Our Digital Belongings : Toward a Service Model for Personal Archives . Abstract : The rapid increase in the using and production of digital media has established an urgent need to develop new models that facilitate long - term access , preservation , and reuse of personal records .In this page we present a service model for controlling personal records using on three key concepts : The archive is viewed as a collection of interrelated objects ( e . g . , files , photos ) ; each object is associated with one or more functions that provide functionality such as processing , editing , sharing , etc . ; and these services are structured into a structure indicating their connections .We define how our approach can be used by individuals to manage their own individual archives , and also discuss its potential application within organizations where vast volumes of records must be managed over longer periods of time . The rapid increase in the using of digital media has led to renewed emphasis in developing systems that enable users to contain and communicate their individual information across multiple computers and platforms .However , current approaches have concentrated mostly on providing strategies for storing and accessing information rather than solving aspects pertaining to preserving it over time . This problem appears particularly intense when dealing with libraries containing many of items spanning numerous years .To address this question , we propose a service - based architecture for organizing and keeping personal records .",
        "rewrite_text": "**Title:** The Long-Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives\n\n**Abstract:** The exponential growth in the creation and utilization of digital media has underscored the pressing necessity for innovative models that ensure the long-term accessibility, preservation, and reuse of personal records. In this article, we introduce a service-oriented model designed to empower individuals in managing their personal archives. Our framework is built upon three foundational concepts: first, the archive is conceptualized as a network of interconnected objects, such as files and photographs; second, each object is linked to various functions that facilitate activities like processing, editing, and sharing; and third, these functions are organized within a structured framework that highlights their interrelationships. We elaborate on how this model can be effectively employed by individuals to curate their personal archives and explore its applicability in organizational contexts, where the management of extensive records over prolonged durations is essential.\n\nThe surge in digital media usage has prompted a renewed focus on developing systems that allow users to manage and disseminate their personal information across diverse platforms and devices. However, existing methodologies have primarily concentrated on strategies for information storage and retrieval, often neglecting the critical issue of long-term preservation. This challenge is particularly pronounced in libraries and archives that house extensive collections spanning multiple years. To tackle this issue, we propose a service-based architecture that not only organizes but also sustains personal records over time. By integrating functionality with preservation strategies, our model aims to provide a comprehensive solution for individuals and organizations alike, ensuring that digital belongings remain accessible and meaningful for future generations.",
        "ori-fast-z-score": 0.3481553119113957,
        "water-fast-z-score": 9.278076673908084,
        "rewrite-fast-z-score": 1.2815364865751413
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Higher order antibunching in intermediate states .\nAbstract:\nWe study the second-order correlation function for an atom interacting with two modes of light, one resonant and another off-resonant to atomic transition frequency. We show that higher order antibunching can be observed when the atom is initially prepared in an excited state or ground state superposition. The effect is more pronounced if the initial state has some population on the excited state. This phenomenon may have applications in quantum information processing. \n \n Introduction:-In recent years there has been considerable interest in studying nonclassical properties of radiation fields generated by atoms  1  . In particular, it was shown that the photon statistics of such systems are governed by the first-order coherence function g (1) (τ)  2  , which describes bunching behavior at short times and anti-bunching at longer times  3  . It is well known that this property arises due to destructive interference between different pathways leading to emission of photons  4  .\nRecently, several authors studied the effects of spontaneous emission on the second-order correlation functions  5  -  8  . They showed that the presence of spontaneous emission leads to sub-Poissonian statistics  6 - 8  . However, these studies were restricted only to the case where the atom interacts with a single mode of field. On the other hand, many experiments involving atoms interacting simultaneously with multiple modes of electromagnetic field have also been performed  9  -  11  . For example, in Ref.  10  , the authors investigated the influence of vacuum fluctuations on the fluorescence spectrum of a three-level system driven by two laser beams. In addition, they found that the intensity noise of the emitted light depends strongly on the relative phase difference between the driving lasers. Motivated by these experimental results we consider here the problem of calculating the second-order correlation function of an atom interacting simultaneously with two modes of light  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Higher order antibunching in intermediate states . Abstract : We research the second - order correlation function for an element interacting with two modes of light , one resonant and another off - resonant to atomic transition frequency .We see that higher order antibunching can be found when the atom is initially prepared in an excited state or ground state superposition . The phenomenon is more pronounced if the first state has some population on the excited state .This phenomenon might have applications in quantum information processing . Introduction : - In recent years there has been substantial interest in investigating nonclassical characteristics of radiation fields generated by atoms 1 .In particular , it was shown that the photon statistics of such systems are governed by the first - order coherence function g ( 1 ) ( τ ) 2 , which explains bunching behavior at short periods and pro - bunching at shorter times 3 . It is well established that this property arises owing to destructive interference between various pathways leading to emission of photons 4 .Recently , various scientists examined the effects of induced emission on the second - order correlation functions 5 - 8 . They showed that the presence of induced emission contributes to sub - Poissonian statistics 6 - 8 .However , these research were restricted only to the case where the atom interacts with a single mode of field . On the other hand , many tests featuring atoms interacting simultaneously with various modes of electromagnetic field have also been performed 9 - 11 .For instance , in Ref . 10 , the published investigated the impact of vacuum fluctuations on the fluorescence spectrum of a three - level network driven by two laser beams .In addition , they reported that the frequency sound of the emitted light depends strongly on the relative phase change between the driving lasers . Motivated by these experimental results we consider here the question of calculating the second - order correlation function of an element interacting simultaneously with two modes of light 12 .",
        "rewrite_text": "**Title:** Higher Order Antibunching in Intermediate States\n\n**Abstract:** This study investigates the second-order correlation function for an atom interacting with two distinct light modes: one that is resonant and another that is off-resonant with respect to the atomic transition frequency. Our findings reveal that higher-order antibunching occurs when the atom is initially prepared in a superposition of its excited state and ground state. Notably, this phenomenon is amplified when there is a significant population in the excited state. The implications of this behavior are particularly relevant for advancements in quantum information processing, where nonclassical light sources are essential. \n\nIn recent years, there has been a growing interest in exploring the nonclassical properties of radiation fields produced by atoms. Previous research has established that the photon statistics of these systems are influenced by the first-order coherence function, which accounts for both bunching behavior at short timescales and pro-bunching at even shorter intervals. This behavior is attributed to destructive interference among various emission pathways. Recent studies have also examined the effects of induced emission on second-order correlation functions, demonstrating that such emissions can lead to sub-Poissonian statistics. However, these investigations have primarily focused on scenarios where the atom interacts with a single mode of light.\n\nIn contrast, our work expands on this by considering the simultaneous interaction of an atom with multiple electromagnetic field modes. We draw on previous experiments that have explored the effects of vacuum fluctuations on the fluorescence spectrum of multi-level atomic systems driven by multiple laser beams. These studies have indicated that the emitted light's frequency characteristics are significantly influenced by the relative phase shifts between the driving lasers. Building on these experimental insights, we aim to calculate the second-order correlation function for an atom interacting with two light modes, thereby contributing to the understanding of higher-order antibunching phenomena and their potential applications in quantum technologies.",
        "ori-fast-z-score": -0.4743416490252569,
        "water-fast-z-score": 6.344409644902455,
        "rewrite-fast-z-score": 0.8669214468630108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Existence and convergence properties of physical measures for certain dynamical systems with holes .\nAbstract:\nWe study the existence, uniqueness and ergodicity of physical measures in some classes of infinite measure preserving transformations on probability spaces. We prove that if T is an infinite measure preserving transformation on (X, B(X), m) such that there exists a set A ∈ B(X) satisfying m(A) > 0 and m(T −1 A) = 0 then T has at least one physical measure which is unique up to scalar multiples. Moreover we show that any two physical measures are equivalent under suitable conditions. Finally we give examples showing how our results can be applied. The main motivation comes from the theory of infinite ergodic group actions where it was shown by Glasner and Weiss  GW  ,  GS1  that every infinite ergodic action of a countable amenable group G admits a unique invariant Radon measure µG . In this case the space X is taken as the dual group G equipped with its left Haar measure m.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Existence and convergence features of physical measures for specific dynamical systems with holes . Abstract : We research the existence , uniqueness and ergodicity of physical measures in some categories of infinite measure retaining transformations on probability spaces .We establish that if T is an infinite measure retaining transformation on ( X , B ( X ) , m ) such that there exists a setting A ∈ B ( X ) satisfying m ( A ) > 0 and m ( T −1 A ) = 0 then T has at least one physical measure which is unique up to scalar multiples . Moreover we prove that any two physical measures are comparable under suitable conditions .Finally we give examples demonstrating how our findings can be applied . The main motivation arises from the principle of infinite ergodic group events where it was shown by Glasner and Weiss GW , GS1 that every infinite ergodic act of a countable amenable group G possesses a unique invariant Radon number µG .In this situation the space X is taken as the dual group G equipped with its leaving Haar measure m .",
        "rewrite_text": "**Title:** Existence and Convergence Features of Physical Measures for Specific Dynamical Systems with Holes\n\n**Abstract:** This study investigates the existence, uniqueness, and ergodic properties of physical measures within certain classes of infinite measure-preserving transformations on probability spaces. We demonstrate that for a transformation \\( T \\) defined on a space \\( (X, B(X), m) \\), if there exists a measurable set \\( A \\in B(X) \\) such that the measure \\( m(A) > 0 \\) and the measure of its preimage under \\( T \\), \\( m(T^{-1}A) = 0 \\), then \\( T \\) possesses at least one physical measure that is unique up to scalar multiples. Furthermore, we establish that under specific conditions, any two physical measures can be compared, highlighting their interrelations. To illustrate the practical implications of our theoretical results, we provide several examples that showcase the application of our findings in real-world scenarios. The impetus for this research stems from the principle of infinite ergodic group actions, as previously explored by Glasner and Weiss, which posits that every infinite ergodic action of a countable amenable group \\( G \\) is associated with a unique invariant Radon measure \\( \\mu_G \\). In this context, the space \\( X \\) is represented as the dual group \\( G \\), endowed with its corresponding left Haar measure \\( m \\). This work contributes to the broader understanding of dynamical systems with holes, offering new insights into the behavior of physical measures in infinite measure settings.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 3.719924439802217,
        "rewrite-fast-z-score": 0.10721125348377948
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SQUID-based instrumentation for ultra-low-field MRI .\nAbstract:\nThe development and application of superconducting quantum interference devices (SQUIDs) in magnetic resonance imaging has been reviewed, with emphasis on the recent advances made by our group at UCLA. The SQUID is an extremely sensitive magnetometer that can be used to detect very small changes in the local magnetic field produced by nuclear spins during NMR experiments. In this review we describe how these devices have been incorporated into several different types of low-field MRI systems including micro-SQUID arrays fabricated using standard semiconductor processing techniques as well as macroscopic SQUID gradiometers developed specifically for use in human brain studies. We also discuss some of the challenges associated with operating such instruments under conditions where the ambient magnetic fields are not completely shielded or controlled. Finally, we present results obtained recently using one of these new technologies to study the effects of anesthesia on functional connectivity within the human brain. Magnetic Resonance Imaging (MRI), which was first demonstrated more than 30 years ago, provides detailed information about the structure and function of living tissues through noninvasive measurements of their water content  1  . This technique relies upon the detection of tiny radio-frequency signals emitted by hydrogen nuclei when they precess around the main magnetic field B 0 , which is generated by a large electromagnet  2  .\nIn conventional clinical scanners, the strength of B 0 ranges between 1 T and 3 T  3  . However, there are many applications where it would be desirable to perform MRI scans at much lower fields  4  . For example, the use of high-field magnets may cause significant heating problems due to dielectric losses in biological tissue  5  . Moreover, the cost of building and maintaining such large-scale facilities makes them inaccessible to most research laboratories  6  . Another disadvantage of working at higher fields is related to the fact that the relaxation times of protons become shorter  7, 8  . As a result, the signal-to-noise ratio decreases rapidly  9  making it difficult to obtain high-quality images  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SQUID - based equipment for ultra - low - field MRI . Abstract : The development and application of superconducting quantum interference devices ( SQUIDs ) in magnetic resonance imaging has been examined , with emphasis on the recent developments shown by our team at UCLA .The SQUID is an incredibly sensitive magnetometer that can be used to identify very small changes in the local magnetic force created by nuclear spins during NMR experiments . In this review we explain how these systems have been incorporated into numerous separate types of short - field MRI systems including micro - SQUID arrays fabricated using traditional semiconductor processing method as also as macroscopic SQUID gradiometers developed specifically for use in human cerebral studies .We additionally discuss some of the challenges associated with operating such instruments under environments where the ambient magnetic areas are not totally shielded or monitored . Finally , we present results acquired today employing one of these new inventions to study the effects of anesthesia on functional functionality within the human mind .Magnetic Resonance Imaging ( MRI ) , which was first demonstrated more than 30 centuries earlier , offers detailed information about the composition and function of living organisms through noninvasive measurements of their water content 1 . This method relies upon the observation of tiny radio - frequency pulses emitted by hydrogen atoms when they precess around the main magnetic force B 0 , which is generated by a large electromagnet 2 .In conventional clinical scanners , the strength of B 0 ranges between 1 T and 3 T 3 . However , there are many applications where it would be beneficial to conduct MRI scans at much lower fields 4 .For instance , the using of high - field magnets might cause significant heating problems due to dielectric losses in biological tissue 5 . Moreover , the cost of building and preserving such great - scale institutions making them inaccessible to most research labs 6 .Another limitation of using at higher areas is related to the fact that the relaxation period of protons become shorter 7 , 8 . As a result , the signal - to - noise ratio falls steadily 9 creating it difficult to obtain high - grade images 10 .",
        "rewrite_text": "**Title:** SQUID-Based Equipment for Ultra-Low-Field MRI\n\n**Abstract:** This article explores the advancements and applications of superconducting quantum interference devices (SQUIDs) in the field of magnetic resonance imaging (MRI), with a particular focus on the recent innovations developed by our research team at UCLA. SQUIDs are highly sensitive magnetometers capable of detecting minute variations in the local magnetic fields generated by nuclear spins during nuclear magnetic resonance (NMR) experiments. In this review, we detail how these devices have been integrated into various types of short-field MRI systems, including micro-SQUID arrays produced through conventional semiconductor fabrication techniques, as well as larger SQUID gradiometers specifically designed for human cerebral studies. We also address the challenges encountered when operating these instruments in environments where ambient magnetic fields are not completely shielded or monitored. Furthermore, we present findings from our latest research utilizing these novel devices to investigate the effects of anesthesia on cognitive functionality in the human brain.\n\nMagnetic Resonance Imaging, first demonstrated over three decades ago, provides critical insights into the structure and function of living organisms through non-invasive assessments of their water content. This technique is based on the detection of faint radio-frequency signals emitted by hydrogen nuclei as they precess around a primary magnetic field (B0), which is generated by a large electromagnet. In standard clinical MRI scanners, B0 typically ranges from 1 Tesla (T) to 3 T. However, there are numerous scenarios where conducting MRI at significantly lower magnetic fields would be advantageous. High-field magnets can induce substantial heating in biological tissues due to dielectric losses, and the financial burden of constructing and maintaining such large-scale facilities often limits access for many research laboratories. Additionally, operating at higher magnetic fields can lead to shorter relaxation times for protons, resulting in a decreased signal-to-noise ratio and complicating the acquisition of high-quality images. This review highlights the potential of SQUID technology to overcome these limitations and enhance the capabilities of ultra-low-field MRI.",
        "ori-fast-z-score": 0.15075567228888181,
        "water-fast-z-score": 8.894584665044027,
        "rewrite-fast-z-score": -0.680336051416609
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Phonon-mediated decay of an atom in a surface-induced potential .\nAbstract:\nWe study the phonon-assisted relaxation dynamics of an excited atom trapped by a surface-induced optical lattice (SIL). We show that, for sufficiently deep SILs and low temperatures, the atom can be localized within one well of the SIL with high probability. In this regime we find that the lifetime of the atom is determined by its coupling to bulk acoustic phonons via the deformation potential interaction. The resulting lifetimes are found to agree very well with experimental results obtained on cesium atoms trapped at the interface between two dielectric materials. \n \n Introduction \n \n Surface-induced lattices have been used extensively over recent years as a tool for trapping ultracold atoms near surfaces  1–3  . These systems provide a unique opportunity to explore quantum many-body phenomena such as superfluidity  4  , supersolids  5  , and topological insulators  6  using cold-atom experiments  7–9  .\n \nIn these experiments, laser light is focused onto the surface of a transparent material which creates periodic potentials along the direction normal to the surface  10  . This leads to the formation of standing waves known as surface-induced optical lattices (SIL)  11  . Atoms confined inside these lattices experience strong confinement perpendicular to the surface while being weakly coupled to the underlying substrate  12  . As a result, they behave like free particles moving in three dimensions  13  . \n \n While there has been significant progress towards understanding the properties of atoms trapped in SILs  14–18  , relatively little attention has been paid so far to their relaxation dynamics  19, 20  . Here we consider the case where an atom is initially prepared in an excited state |e⟩ above some energy threshold E0. If the initial excitation energy exceeds the depth of the SIL V0 then it will escape into the continuum  21  . However if the initial energy lies below E0 but still exceeds the recoil energy ER = 2 kL2 / 2mL  22  , where mL denotes the mass of the atom and kL is the wavevector associated with the lattice periodicity, then the atom may relax back down to the ground state |g⟩ through emission of a",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Phonon - mediated degradation of an element in a surface - triggered potential . Abstract : We research the phonon - aided relaxation behavior of an excited element trapped by a surface - mediated optical lattice ( SIL ) .We see that , for enough deep SILs and low temperatures , the atom can be localized within one well of the SIL with high probability . In this regime we find that the life of the atom is calculated by its interaction to bulk acoustic phonons via the deformation potential interaction .The resulting lifetimes are found to agree very best with experimental results derived on cesium atoms trapped at the interface between two dielectric materials . Introduction Surface - induced lattices have been used widely over recent months as a technique for trapping ultracold atoms near surfaces 1 – 3 .These systems create a unique opportunity to examine quantum several - bodies phenomena such as superfluidity 4 , supersolids 5 , and topological insulators 6 using cold - atom experiments 7 – 9 . In these experiments , laser light is focused onto the surface of a transparent material which forms periodic potentials along the direction regular to the surface 10 .This leads to the formation of standing currents termed as surface - mediated optical lattices ( SIL ) 11 . Atoms confined inside these lattices experience strong confinement parallel to the surface while being weakly linked to the underlying substrate 12 .As a result , they react like free particles moving in three dimensions 13 . While there has been significant progress towards studying the properties of atoms trapped in SILs 14 – 18 , surprisingly little attention has been paid so far to their relaxation interactions 19 , 20 .Here we imagine the case where an element is initially made in an excited state | e ⟩ above some energy threshold E0 . If the first excitation energy reaches the height of the SIL V0 then it will flee into the continuum 21 .However if the first energy falls below E0 but still exceeds the recoil power ER = 2 kL2 / 2mL 22 , where mL denotes the mass of the atom and kL is the wavevector related with the lattice periodicity , then the atom might cool back down to the ground state | g ⟩ through emitted of a",
        "rewrite_text": "**Title:** Phonon-Mediated Degradation of an Element in a Surface-Triggered Potential\n\n**Abstract:** This study investigates the phonon-assisted relaxation dynamics of an excited atom confined within a surface-induced optical lattice (SIL). Our findings indicate that, under conditions of sufficiently deep SILs and low temperatures, the atom exhibits a high probability of being localized within a single well of the lattice. In this localized regime, the atom's lifetime is predominantly influenced by its interactions with bulk acoustic phonons through the deformation potential mechanism. The calculated lifetimes align closely with experimental observations made on cesium atoms situated at the interface of two dielectric materials. \n\nThe introduction of surface-induced lattices has gained traction in recent months as a powerful method for trapping ultracold atoms in proximity to surfaces. These systems provide a unique platform for exploring complex quantum phenomena, including superfluidity, supersolids, and topological insulators, through cold-atom experiments. In these setups, laser light is directed onto the surface of a transparent medium, generating periodic potentials perpendicular to the surface. This process results in the formation of standing waves known as surface-mediated optical lattices. Atoms confined within these lattices experience strong confinement parallel to the surface while maintaining a weak coupling to the underlying substrate, allowing them to behave similarly to free particles in three-dimensional space.\n\nDespite the considerable advancements in understanding the properties of atoms trapped in SILs, the relaxation interactions of these atoms have received relatively little attention. In our model, we consider an atom initially excited to a state | e ⟩ above a specific energy threshold E0. If the excitation energy surpasses the SIL height V0, the atom escapes into the continuum. Conversely, if the energy falls below E0 but remains above the recoil energy ER, the atom can transition back to the ground state | g ⟩ by emitting a phonon. This research sheds light on the intricate dynamics of atom-surface interactions and their implications for future studies in quantum physics.",
        "ori-fast-z-score": -1.0932163332202425,
        "water-fast-z-score": 6.887026769553818,
        "rewrite-fast-z-score": 0.79555728417573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Group of Isometries in Classical and Noncommutative Geometry .\nAbstract:\nThe quantum group of isometries (QGI) was introduced by Connes as the universal object for deformations of classical groups, which are obtained via spectral triples on commutative C*-algebras.  In this talk we will discuss how to define QGI s using noncommutative geometry techniques such as operator algebras and von Neumann algebras.   We will also explain how these objects can be used to study the classification problem of Riemannian manifolds with positive scalar curvature. The Quantum Group of Isometries (QGI), first defined by Alain Connes, plays an important role in both classical and noncommutative geometry. It is the universal object for deforming classical Lie groups into their corresponding quantum groups. This talk will give an introduction to QGI’s and show that they can be studied through operator algebra theory and von Neumann algebras. Finally it will present some results about the classification problem of Riemannain manifolds with positive scalar curvatures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Group of Isometries in Classical and Noncommutative Geometry . Abstract : The quantum group of isometries ( QGI ) was introduced by Connes as the universal object for deformations of classical groups , which are derived via spectral triples on commutative C * - algebras .In this talk we will explore how to define QGI s using noncommutative geometry tools such as operator algebras and von Neumann algebras . We will also explain how these objects can be used to study the classification question of Riemannian manifolds with positive scalar curvature .The Quantum Group of Isometries ( QGI ) , initially established by Alain Connes , takes an important role in both classical and noncommutative geometry . It is the universal object for deforming classical Lie fields into their corresponding quantum groups .This discussion will giving an introduction to QGI ’ s and explain that they can be analyzed through operator algebra analysis and von Neumann algebras . Finally it will present some results about the classification question of Riemannain manifolds with positive scalar curvatures .",
        "rewrite_text": "Title: Quantum Group of Isometries in Classical and Noncommutative Geometry\n\nAbstract: The Quantum Group of Isometries (QGI), a concept introduced by Alain Connes, serves as a pivotal framework for understanding the deformation of classical groups through the lens of spectral triples associated with commutative C*-algebras. In this presentation, we delve into the definition and properties of QGIs, utilizing advanced tools from noncommutative geometry, particularly operator algebras and von Neumann algebras. By employing these mathematical structures, we aim to shed light on the intricate relationship between classical geometry and its quantum counterparts.\n\nFurthermore, we will discuss the implications of QGIs in addressing the classification problem of Riemannian manifolds that exhibit positive scalar curvature. This classification is significant in the field of differential geometry, as it relates to various physical theories and the understanding of geometric structures. Our exploration will highlight how the framework of QGIs can provide new insights and methodologies for tackling these classification challenges.\n\nThrough this talk, we intend to provide a comprehensive overview of the QGI, illustrating its relevance and application in both classical and noncommutative settings. We will present key results and findings that demonstrate the utility of QGIs in the study of geometric properties and their quantum analogs. Ultimately, this discussion aims to bridge the gap between classical and quantum geometries, offering a deeper understanding of the mathematical foundations that underpin these fields.",
        "ori-fast-z-score": -0.6509445549041194,
        "water-fast-z-score": 3.0983866769659336,
        "rewrite-fast-z-score": 0.9434563530497265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CCD BV survey of 42 open clusters .\nAbstract:\nWe present the results of our CCD photometric study of 42 open clusters in the southern hemisphere, carried out at the 1-meter telescope of the South African Astronomical Observatory (SAAO). The observations were made with an SBIG STL-1001E camera equipped with a Kodak KAF-0400 chip and Johnson V filter during three observing runs between September 1998 and February 1999. We have used DAOPHOT II to perform aperture photometry on all stars detected within each cluster field-of-view. A total number of about 15000 stars was measured for each cluster. In addition we obtained UBVRI photometry for some of these clusters using the same instrumentation as described above.  From this data set we derived the following parameters: reddening E(B-V), distance modulus DM, age t, metallicity  Fe/H  , mass function slope x, core radius rc, central surface brightness µ0, concentration index c, and integrated absolute magnitude M.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CCD BV survey of 42 open complexes . Abstract : We present the conclusion of our CCD photometric analysis of 42 open complexes in the southern hemisphere , conducted out at the 1 - meter telescope of the South African Astronomical Observatory ( SAAO ) .The observed were made with an SBIG STL - 1001E camera equipped with a Kodak KAF - 0400 card and Johnson V filter during three observing walks between September 1998 and February 1999 . We have utilized DAOPHOT II to conduct aperture photometry on all stars observed within each cluster field - of - view .A total quantity of about 15000 stars was measured for each cluster . In addition we received UBVRI photometry for some of these clusters using the same equipment as described above .From this data set we derived the following variables : reddening E ( B - V ) , distance modulus DM , age t , metallicity Fe / H , mass function slope x , core radius rc , central surface brightness µ0 , concentration index c , and integrated absolute magnitude M .",
        "rewrite_text": "We present the findings of our CCD photometric study of 42 open star clusters located in the southern hemisphere, conducted using the 1-meter telescope at the South African Astronomical Observatory (SAAO). This research involved observations made with an SBIG STL-1001E camera, which was equipped with a Kodak KAF-0400 chip and a Johnson V filter, during three observational campaigns that took place between September 1998 and February 1999. Utilizing the DAOPHOT II software, we performed aperture photometry on all stars captured within the field of view of each cluster. In total, we measured approximately 15,000 stars for each cluster analyzed. Additionally, we obtained UBVRI photometry for several of these clusters using the same observational setup. From the comprehensive dataset, we derived several important astrophysical parameters, including the reddening value E(B-V), the distance modulus (DM), the age (t) of the clusters, metallicity (Fe/H), the slope of the mass function (x), core radius (rc), central surface brightness (µ0), concentration index (c), and integrated absolute magnitude (M). These parameters are crucial for understanding the physical characteristics and evolutionary status of the open clusters studied. Our findings contribute to the broader knowledge of stellar populations and the dynamics of open clusters in the southern hemisphere, providing valuable insights for future research in stellar astrophysics.",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 2.5927248643506746,
        "rewrite-fast-z-score": 0.9138115486202573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A quantitative study on the growth variability of tumour cell clones in vitro .\nAbstract:\nThe authors have studied the growth rate and its fluctuations for different types of human cancer cells grown under controlled conditions, using an automated image analysis system to measure their size over time.  The results show that there is considerable variation between individual clones within each type of cell line as well as between different types of cell lines. This suggests that it may be possible to use this information to classify tumours into subtypes with distinct biological properties. In addition, they find evidence that the growth rates are correlated across generations of daughter cells, which could provide insight into how these correlations arise during tumorigenesis. Finally, by comparing the growth characteristics of normal and transformed cells, they conclude that transformation leads to increased heterogeneity among daughter cells. Cancer is characterized by uncontrolled proliferation of abnormal cells. Understanding the mechanisms underlying this process can help us develop new treatments against cancer. However, studying the dynamics of cancerous cell populations has been challenging because of difficulties associated with tracking large numbers of single cells simultaneously. Here we report our recent work on characterizing the growth behavior of thousands of individual cancer cells growing in culture dishes  1  . We used an automated imaging system to track the sizes of hundreds of thousands of cells belonging to several different types of human cancer cell lines ( Figure 1 ). Our results reveal significant differences in both average growth rates and growth fluctuations between different types of cell lines: some grow faster than others while also exhibiting larger fluctuations around their mean values  2  .\nWe found that the growth rates were highly variable even when measured at the level of individual clones derived from a common parent population  3  , suggesting that the observed phenotypic diversity might reflect genetic or epigenetic variations present in the original parental population  4  .  These findings suggest that it should be possible to use such measurements to classify tumors into subtypes based on their growth characteristics  5  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A quantitative study on the development variability of tumour cell clones in vitro . Abstract : The authors have researched the development frequency and its fluctuations for different kinds of human tumor cells grown under regulated conditions , using an automated photo processing program to measure their size over time .The results show that there is substantial variation between individual clones within each type of cell line as well as between various types of cell lines . This implies that it could be possible to use this data to classify tumours into subtypes with particular genetic characteristics .In addition , they discover proof that the development rates are correlated across generations of daughter tissues , which could give insight into how these correlations occur during tumorigenesis . Finally , by comparing the development patterns of normal and transformed tissue , they conclude that transformation results to greater heterogeneity among sister cells .Cancer is characterized by uncontrolled proliferation of irregular cells . Understanding the mechanisms governing this process can help us evolve innovative treatments against tumors .However , studying the dynamics of cancerous cell groups has been challenging because of troubles associated with monitoring huge amounts of single cells simultaneously . Here we publish our latest work on characterizing the development habits of thousands of individual cancer cells growing in culture dishes 1 .We utilized an automated scanning system to track the sizes of tens of thousands of cells belonging to several different kinds of human tumor cell lines ( Figure 1 ) . Our results show considerable variations in both average growth rates and growth fluctuations between various types of cell lines : some develop longer than others while also displaying wider fluctuations around their average values 2 .We showed that the development rates were extremely varied even when measured at the level of individual clones originating from a common parent population 3 , showing that the reported phenotypic diversity could reflect genetic or epigenetic changes found in the original parental generation 4 . These conclusions show that it should be possible to use such measurements to classify cancer into subtypes based on their development parameters 5 .",
        "rewrite_text": "**Title:** A Quantitative Study on the Development Variability of Tumor Cell Clones In Vitro\n\n**Abstract:** This study investigates the frequency and variability of development among various human tumor cell types cultured under controlled conditions. Utilizing an automated image processing system, we measured the growth of these cells over time, allowing for a comprehensive analysis of their size dynamics. Our findings reveal significant discrepancies in growth patterns both within individual clones of the same cell line and across different cell lines. This variability suggests the potential for classifying tumors into distinct subtypes based on specific genetic traits. Furthermore, we observed that growth rates exhibit correlations across generations of daughter cells, providing valuable insights into the mechanisms underlying tumorigenesis. A comparative analysis of normal versus transformed tissues indicates that cellular transformation leads to increased heterogeneity among sister cells. \n\nCancer is characterized by the unregulated proliferation of abnormal cells, and understanding the underlying mechanisms is crucial for developing innovative therapeutic strategies. However, studying the dynamics of cancer cell populations has been hindered by the challenges of simultaneously monitoring large numbers of individual cells. In this research, we present our latest findings on the growth behaviors of thousands of individual cancer cells cultured in dishes. Our automated scanning system enabled us to track the sizes of tens of thousands of cells from multiple human tumor cell lines. The results demonstrate considerable variation in average growth rates and fluctuations among different cell lines; some lines exhibit prolonged growth periods while others show greater variability around their average growth rates. Notably, even clones derived from a common parental population displayed significant differences in development rates, suggesting that the observed phenotypic diversity may stem from genetic or epigenetic alterations present in the original parental generation. These insights indicate that such measurements could facilitate the classification of cancer into subtypes based on their developmental characteristics.",
        "ori-fast-z-score": -1.9744355451432527,
        "water-fast-z-score": 9.189494464367357,
        "rewrite-fast-z-score": 2.454287964311585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sterile neutrino oscillations after first MiniBooNE results .\nAbstract:\nThe MiniBooNE experiment has recently reported the observation of an excess in electron-neutrino-like events at low energies, which could be explained by sterile neutrinos with mass around 1 eV and mixing angle sin2(2θ) ~ 0.1. \n \n In this work we study how these results can be accommodated within the framework of three-flavor leptonic mixing using the latest global fits to experimental data on neutrino oscillation parameters as well as cosmological constraints on the sum of active neutrino masses. We find that the allowed parameter space is strongly constrained if one assumes that the observed excess corresponds to true neutrino oscillations into sterile states rather than being due to background systematics or statistical fluctuations. The best-fit values for the sterile neutrino mass splitting are found to be Δm32 = (0.5 - 2.3) meV and Δm2 = (0.4 - 3.6) meV, while the corresponding ranges for the mixing angles are θ23 = 42° - 50° , θ13 < 5° and θ12 > 40° .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sterile neutrino oscillations after first MiniBooNE findings . Abstract : The MiniBooNE experiment has recently noted the observation of an amount in electron - neutrino - like events at low energies , which could be described by sterile neutrinos with mass around 1 eV and mixing ratio sin2 ( 2θ ) ~ 0 . 1 .In this research we study how these results can be accommodated within the framework of three - flavor leptonic mixing using the latest global fits to experimental evidence on neutrino oscillation components as well as cosmological limitations on the sum of active neutrino masses . We see that the allowed parameter space is strongly constrained if one assumes that the reported excess corresponds to genuine neutrino oscillations into sterile states instead than being owing to background systematics or statistical fluctuations .The best - fitting values for the sterile neutrino mass separation are found to be Δm32 = ( 0 . 5 - 2 . 3 ) meV and Δm2 = ( 0 . 4 - 3 . 6 ) meV , while the equivalent ranges for the mix angles are θ23 = 42° - 50° , θ13 < 5° and θ12 > 40° .",
        "rewrite_text": "Title: Sterile Neutrino Oscillations Following Initial MiniBooNE Observations\n\nAbstract: The MiniBooNE experiment has recently reported an intriguing observation of an excess of electron-neutrino-like events at low energy levels. This phenomenon may be explained by the existence of sterile neutrinos with a mass around 1 eV and a mixing ratio of approximately sin²(2θ) ~ 0.1. In this study, we explore how these findings can be integrated into the established framework of three-flavor leptonic mixing. We utilize the most recent global fits derived from experimental data on neutrino oscillation parameters, alongside cosmological constraints on the total mass of active neutrinos. Our analysis indicates that the parameter space is significantly restricted if we assume that the observed excess is indeed a result of genuine neutrino oscillations into sterile states, rather than being attributed to background noise or statistical anomalies. The optimal fitting values for the mass-squared differences of sterile neutrinos are determined to be Δm32 = (0.5 - 2.3) meV and Δm2 = (0.4 - 3.6) meV. Additionally, we find the corresponding ranges for the mixing angles to be θ23 = 42° - 50°, θ13 < 5°, and θ12 > 40°. These results provide critical insights into the potential role of sterile neutrinos in the broader context of neutrino physics and may have significant implications for our understanding of the universe's fundamental particles.",
        "ori-fast-z-score": -0.7385489458759964,
        "water-fast-z-score": 4.031591663758072,
        "rewrite-fast-z-score": 0.21320071635561041
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytic models of plausible gravitational lens potentials .\nAbstract:\nWe present analytic models for the potentials that can produce multiple images in strong gravitational lenses, and we use these to study how well different mass profiles are constrained by observations. We find that it is possible to obtain good constraints on both the total enclosed mass within an Einstein radius (the projected distance at which two images merge) and the slope of the density profile outside this radius using only three or four multiply-imaged systems with accurate photometric redshifts.  The results presented here should be useful for planning future surveys aimed at measuring dark matter properties through gravitational lensing. Gravitational lensing provides one of our most powerful tools for studying the distribution of dark matter in galaxies and clusters. In particular, if a galaxy cluster has been lensed into several distinct images, then the positions and fluxes of those images provide information about the shape of the cluster s gravitational potential as a function of position along its line-of-sight. This allows us to measure the total enclosed mass within some characteristic scale length, such as the Einstein radius, where two images merge together. However, because there are many degeneracies between the parameters describing the lens model, it is not always straightforward to determine what kind of mass profile best describes the data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytic models of plausible gravity lens potentials . Abstract : We create analytic models for the potentials that can generate several images in good gravitational lenses , and we using these to study how well specific mass profiles are constrained by observed .We see that it is possible to obtain good limitations on both the total enclosed mass within an Einstein radius ( the projected height at which two images merge ) and the slope of the density profile outside this radius using only three or four multiply - imaged models with correct photometric redshifts . The results presented here should be valuable for planning upcoming studies aimed at assessing heavy material effects through gravity lensing .Gravitational lensing offers one of our most important skills for studying the distribution of dark matter in galaxies and clusters . In particular , if a galaxy cluster has been lensed into numerous separate images , then the places and fluxes of those pictures provide information about the form of the cluster s gravitational potential as a function of position along its line - of - view .This enables us to measure the total enclosed mass within some characteristic scale length , such as the Einstein radius , where two images merge together . However , because there are many degeneracies between the variables describing the lens model , it is not always feasible to identify what sort of mass profile best explains the information .",
        "rewrite_text": "Title: Analytic Models of Plausible Gravitational Lens Potentials\n\nAbstract: In this study, we develop analytic models for gravitational lens potentials capable of producing multiple images in effective gravitational lensing scenarios. Our primary objective is to investigate how well specific mass profiles can be constrained by observational data. We demonstrate that it is feasible to derive robust constraints on both the total enclosed mass within the Einstein radius—the critical projected distance at which two images converge—and the slope of the density profile beyond this radius by utilizing just three or four multiply-imaged configurations with accurate photometric redshifts. The findings presented in this paper are anticipated to be instrumental in the design of future research aimed at exploring the effects of substantial mass distributions through gravitational lensing techniques.\n\nGravitational lensing serves as a powerful tool for probing the distribution of dark matter within galaxies and galaxy clusters. When a galaxy cluster is lensed into multiple distinct images, the positions and brightness of these images yield critical insights into the structure of the cluster's gravitational potential along the line of sight. This process allows for the measurement of the total enclosed mass within characteristic scale lengths, such as the Einstein radius, where the images coalesce. However, the inherent degeneracies among the parameters that define the lens model complicate the identification of the most suitable mass profile that accurately represents the observed data. Our work addresses these challenges and provides a framework for better understanding the mass distribution in gravitational lensing scenarios, ultimately contributing to the broader field of astrophysics and cosmology.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.236413804652249,
        "rewrite-fast-z-score": 0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Contributions of Issai Schur to Analysis .\nAbstract:\nIssai Schur (1886-1951) was one of the most influential mathematicians in twentieth century mathematics, and his work has had an enormous impact on many areas including number theory, functional analysis, operator theory, harmonic analysis, group representation theory, ergodic theory, probability theory, mathematical physics, computer science, statistics, numerical analysis, combinatorics, algebraic geometry, and dynamical systems.  In this talk we will survey some of these contributions with emphasis on their historical context.   We will also discuss how Schur s ideas have been developed by other mathematicians over the past seventy years or so. The lecture is intended for graduate students who are interested in learning about the history of modern mathematics as well as its current state-of-the-art. It should be accessible to undergraduates with a background in real variable calculus and linear algebra. This course fulfills the requirements of both MATH 3010 and MATH 3310 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Contributions of Issai Schur to Analysis . Abstract : Issai Schur ( 1886 - 1951 ) was one of the most influential mathematicians in 20th period mathematics , and his work has had an enormous effect on numerous topics including number theory , functional analysis , operator analysis , harmonic analysis , group representation theory , ergodic logic , probability theory , mathematical science , computer science , statistics , numerical analysis , combinatorics , algebraic topology , and dynamical systems .In this talk we will review some of these works with emphasis on their historical context . We will also discuss how Schur s ideas have been used by other mathematicians over the previous seventy years or so .The lecture is intended for graduate students who are concerned in understanding about the history of modern mathematics as well as its current state - of - the - art . It should be available to undergraduates with a background in real variable calculus and linear algebra .This course fulfills the requirements of both MATH 3010 and MATH 3310 .",
        "rewrite_text": "Title: Contributions of Issai Schur to Analysis\n\nAbstract: Issai Schur (1886-1951) stands as a pivotal figure in 20th-century mathematics, whose extensive contributions have profoundly influenced a wide array of fields, including number theory, functional analysis, operator theory, harmonic analysis, group representation theory, ergodic theory, probability, mathematical sciences, computer science, statistics, numerical analysis, combinatorics, algebraic topology, and dynamical systems. This presentation aims to explore Schur's significant works within their historical framework, highlighting the evolution of his ideas and their impact on subsequent mathematical developments. We will delve into specific examples of how Schur's concepts have been adopted and adapted by mathematicians over the past seventy years, illustrating the enduring relevance of his contributions. The lecture is designed for graduate students interested in the historical progression of modern mathematics and its contemporary advancements. However, it is also accessible to undergraduates who possess a foundational understanding of real variable calculus and linear algebra. This course satisfies the requirements for both MATH 3010 and MATH 3310, providing students with a comprehensive overview of Schur's legacy and its implications for current mathematical research and practice. Through this exploration, participants will gain insights into the intricate connections between historical mathematical theories and their applications in today's scientific landscape.",
        "ori-fast-z-score": 0.22941573387056174,
        "water-fast-z-score": 4.129483209670111,
        "rewrite-fast-z-score": 0.09759000729485333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmic rays from trans-relativistic supernovae .\nAbstract:\nWe present the results of our analysis of cosmic ray data collected by the PAMELA experiment in 2008 and 2009, which show an excess over background at energies between 1-10 GeV/nucleon that is consistent with being produced by particles accelerated in nearby supernova remnants (SNRs). We find that this signal can be explained if we assume that SNR RX J1713-3946 accelerates protons up to 10 TeV energy per nucleon. The observed fluxes are also compatible with those expected for other known sources such as pulsars or active galactic nuclei. However, these alternative scenarios cannot explain all features seen in the data set. In particular, they do not predict any significant anisotropy in arrival directions on angular scales below ~10 degrees. This prediction is confirmed by observations made using the Tibet ASγ air shower array. Finally, we discuss possible implications of our findings for models of particle acceleration in relativistic shocks.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmic rays from trans - relativistic supernovae . Abstract : We present the conclusion of our analysis of cosmic ray data taken by the PAMELA study in 2008 and 2009 , which show an accumulation over background at energies between 1 - 10 GeV / nucleon that is compatible with being produced by particles advanced in nearby supernova remnants ( SNRs ) .We see that this signal can be described if we suppose that SNR RX J1713 - 3946 accelerates protons up to 10 TeV power per nucleon . The observed fluxes are also consistent with those expected for other known sources such as pulsars or active galactic nuclei .However , these alternative situations cannot explain all characteristics found in the information pool . In particular , they do not predict any slight anisotropy in arrival angles on spatial scales below ~ 10 degrees .This prediction is confirmed by findings made using the Tibet ASγ air spray array . Finally , we explain possible possibilities of our findings for models of particle acceleration in relativistic shocks .",
        "rewrite_text": "In this article, we present the results of our comprehensive analysis of cosmic ray data collected by the PAMELA experiment during the years 2008 and 2009. Our findings reveal a significant excess of cosmic rays over the background noise at energies ranging from 1 to 10 GeV per nucleon, suggesting that these particles may originate from nearby supernova remnants (SNRs). Specifically, we propose that the SNR RX J1713-3946 is capable of accelerating protons to energies as high as 10 TeV per nucleon. \n\nMoreover, the observed fluxes align with predictions made for other potential sources, including pulsars and active galactic nuclei. However, these alternative explanations fall short of accounting for all the characteristics observed in the data set. Notably, they fail to predict the slight anisotropy in the arrival angles of cosmic rays on spatial scales smaller than approximately 10 degrees. This specific prediction is corroborated by observations from the Tibet ASγ air shower array, which further strengthens our hypothesis.\n\nIn conclusion, our findings have significant implications for the understanding of particle acceleration mechanisms in relativistic shocks. They suggest that supernova remnants, particularly RX J1713-3946, play a crucial role in the production of high-energy cosmic rays, thereby enhancing our comprehension of the origins and propagation of cosmic rays in the universe. This research opens avenues for further exploration into the dynamics of particle acceleration in astrophysical environments, contributing to the broader field of high-energy astrophysics.",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distributed Algorithms for Spectrum Allocation, Power Control, Routing, and Congestion Control in Wireless Networks .\nAbstract:\nIn this thesis we study distributed algorithms for spectrum allocation, power control, routing, and congestion control problems in wireless networks. We first consider the problem of joint channel assignment and power control (JCAPC) with minimum total transmit power consumption subject to quality-of-service constraints on each link. The JCAPC problem is formulated as an integer program which can be solved by using standard linear programming techniques. However, such centralized solutions are not practical due to their high computational complexity. In order to overcome these difficulties, we propose two distributed algorithms based on dual decomposition methods. Our simulation results show that our proposed algorithms perform close to optimality while requiring only local information exchange among neighboring nodes. \n \n Next, we investigate the problem of joint routing and congestion control (JRCC). This problem arises when there exists multiple paths between source-destination pairs in a network. Each path has different available bandwidths depending on its physical characteristics. To achieve fairness across all flows sharing common links, JRCC requires that the rate allocated to any flow should depend on both the current load along the path it uses and the number of competing flows. We formulate the JRCC problem as a convex optimization problem whose objective function is to maximize the aggregate utility over all users under certain capacity constraints at each node. Then, we develop a distributed algorithm based on primal decomposition method to solve the JRCC problem. Finally, we present some numerical examples to demonstrate the performance of our proposed algorithm.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distributed Algorithms for Spectrum Allocation , Power Control , Routing , and Congestion Control in Wireless Networks . Abstract : In this dissertation we study spread methods for spectrum allocation , power control , routing , and congestion control problems in wireless networks .We first consider the question of joint network assignment and power control ( JCAPC ) with minimum total broadcast capacity capacity subject to quality - of - service restrictions on each link . The JCAPC task is implemented as an integer program which can be answered by using conventional linear programming tools .However , such centralized methods are not practical owing to their high computational complexity . In order to overcome these problems , we propose two distributed methods using on dual decomposition techniques .Our model results show that our proposed methods operate close to optimality while using only local information transfer among neighboring connections . Next , we investigate the issue of joint routing and congestion control ( JRCC ) .This problem arises when there exists multiple paths between origin - destination pairs in a network . Each path has various accessible bandwidths based on its physical qualities .To achieve fairness across all flows sharing shared links , JRCC requires that the price allocated to any flow should depend on both the present load along the path it utilizes and the number of competing flows . We formulate the JRCC problem as a convex optimization problem whose aim function is to maximize the aggregate utility over all users under certain availability constraints at each node .Then , we develop a distributed algorithm based on primal decomposition technique to solve the JRCC problem . Finally , we present some numerical examples to test the performance of our proposed algorithm .",
        "rewrite_text": "In this dissertation, we explore innovative distributed algorithms for addressing critical issues in wireless networks, specifically focusing on spectrum allocation, power control, routing, and congestion control. We begin by examining the joint network assignment and power control (JCAPC) problem, which aims to minimize the total broadcast capacity while adhering to quality-of-service (QoS) constraints for each link. The JCAPC is formulated as an integer programming problem, traditionally solvable through linear programming methods. However, these centralized approaches often suffer from significant computational complexity, rendering them impractical for real-time applications. To address this challenge, we propose two distributed algorithms that leverage dual decomposition techniques, allowing for efficient local information exchange among neighboring nodes. Our simulation results demonstrate that these methods achieve performance levels close to optimality while significantly reducing computational overhead.\n\nSubsequently, we turn our attention to the joint routing and congestion control (JRCC) problem, which emerges in networks with multiple paths connecting origin-destination pairs, each offering varying bandwidths based on their physical characteristics. To ensure fairness among flows sharing common links, the JRCC framework necessitates that the cost assigned to each flow reflects both the current load on its path and the number of competing flows. We reformulate the JRCC challenge as a convex optimization problem, with the objective of maximizing the aggregate utility for all users while respecting availability constraints at each network node. To solve this problem, we develop a distributed algorithm utilizing primal decomposition techniques. Finally, we provide numerical examples to evaluate the effectiveness and performance of our proposed JRCC algorithm, illustrating its potential for practical implementation in dynamic wireless environments.",
        "ori-fast-z-score": -0.24743582965269675,
        "water-fast-z-score": 6.799624940308036,
        "rewrite-fast-z-score": 0.3223291856101521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A wide deep infrared look at the Pleiades with UKIDSS: new constraints on the substellar binary fraction and the low mass IMF .\nAbstract:\nWe present an analysis of the UKIRT Infrared Deep Sky Survey (UKIDSS) Galactic Cluster Survey data for the open cluster, Pleiades. We use this to derive the number ratio between binaries and single stars in the range 0.1 < M/M⊙ < 1.0 as well as the initial mass function (IMF). The results are compared against previous studies using different techniques. Our derived binary fraction is consistent within uncertainties with that found by other authors but our IMF shows significant differences when compared to previous work. These discrepancies may be due to contamination from background galaxies or unresolved multiple systems which have been included in previous analyses. This study highlights the importance of accurate photometry over large areas combined with high resolution spectroscopy to fully understand the properties of young open clusters such as the Pleiades. Keywords: Open cluster; Binary star systems; Initial Mass Function; Substellar object",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A wide deep infrared look at the Pleiades with UKIDSS : current constraints on the substellar binary fraction and the small weight IMF . Abstract : We report an assessment of the UKIRT Infrared Deep Sky Survey ( UKIDSS ) Galactic Cluster Survey data for the open cluster , Pleiades .We use this to derive the number ratio between binaries and single stars in the range 0 . 1 < M / [UNK] < 1 . 0 as well as the initial mass function ( IMF ) . The results are compared against past studies use different methods .Our derived binary population is compatible within uncertainties with that detected by other researchers but our IMF displays substantial differences when compared to previous research . These discrepancies may be due to contamination from background galaxies or unresolved multiple components which have been included in earlier analyses .This study highlights the importance of accurate photometry over large areas coupled with high resolution spectroscopy to fully realize the properties of young open complexes such as the Pleiades . Keywords : Open cluster ; Binary galaxy structures ; Initial Mass Function ; Substellar object",
        "rewrite_text": "Title: A Comprehensive Infrared Analysis of the Pleiades Cluster Using UKIDSS: Insights into the Substellar Binary Fraction and Initial Mass Function\n\nAbstract: In this study, we present an analysis of data from the UKIRT Infrared Deep Sky Survey (UKIDSS) focusing on the Pleiades open cluster. Our objective is to determine the ratio of binary stars to single stars within the mass range of 0.1 to 1.0 solar masses, as well as to evaluate the initial mass function (IMF) of this stellar population. We compare our findings with previous studies that employed various methodologies to assess the binary fraction and IMF. Our results indicate that the binary population we identified aligns well with those reported by other researchers, falling within the margins of uncertainty. However, significant discrepancies arise in our IMF measurements when juxtaposed with earlier investigations. These differences may stem from potential contamination by background galaxies or the presence of unresolved multiple star systems that were not adequately accounted for in prior analyses. This research underscores the critical need for precise photometric measurements across extensive areas, combined with high-resolution spectroscopy, to accurately characterize the properties of young open clusters like the Pleiades. Our findings contribute to a deeper understanding of stellar formation and evolution in these dynamic environments. \n\nKeywords: Open cluster; Binary star systems; Initial Mass Function; Substellar objects.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 5.300713251592634,
        "rewrite-fast-z-score": -0.9233805168766388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exact half-BPS Type IIB interface solutions II: Flux solutions and multi-Janus .\nAbstract:\nWe present new exact half-BPS solutions to the low-energy effective action for type-IIB string theory in ten dimensions, which are supported by fluxes on an arbitrary number of intersecting D3-branes. These solutions can be viewed as generalizations of the single Janus solution presented in  1  . We also discuss some properties of these solutions that were not discussed previously. \n \n The first part of this work was published as: \n \n  2  C. A. Johnson et al., Phys. Rev. Lett. 106 (2011) 055005. DOI: 10.1103/PhysRevLett.106.055005. URL: http://arxiv.org/abs/1111.5389v2. URL: http://www.sciencedirect.com/science/article/pii/S0167971011000203. URL: https://inspirehep.net/record/126898/files/CAJohnson_et_al_PRL_2011_055005.pdf",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exact half - BPS Type IIB interface solutions II : Flux solutions and multi - Janus . Abstract : We create novel exact half - BPS solutions to the small - energy effective action for type - IIB string theory in ten dimensions , which are supported by fluxes on an arbitrary number of intersecting D3 - branes .These solutions can be viewed as generalizations of the single Janus solution discussed in 1 . We also discuss some properties of these solutions that were not discussed earlier .The first part of this research was publication as : 2 C . A . Johnson et al . , Phys . Rev .Lett.106 (2011) 055005.DOI : 10 . 1103 / PhysRevLett . 106 . 055005 . URL : http : / / arxiv . org / abs / 1111 . 5389v2 .URL : http : / / www . sciencedirect . com / science / article / pii / S0167971011000203 . URL : https : / / inspirehep . net / record / 126898 / files / CAJohnson _ et _ al _ PRL _ 2011 _ 055005 . pdf",
        "rewrite_text": "Title: Exact Half-BPS Type IIB Interface Solutions II: Flux Solutions and Multi-Janus\n\nAbstract: In this study, we present innovative exact half-BPS solutions to the low-energy effective action of Type IIB string theory in ten dimensions, which are sustained by fluxes across an arbitrary number of intersecting D3-branes. These solutions serve as extensions of the previously examined single Janus solution, offering a broader framework for understanding the dynamics of such configurations. We delve into various characteristics of these new solutions that have not been addressed in prior literature, thereby enriching the existing knowledge in the field. The initial findings of this research were published in C. A. Johnson et al., Phys. Rev. Lett. 106 (2011) 055005, where we laid the groundwork for these developments. Our work not only expands the theoretical landscape of half-BPS solutions but also opens avenues for further exploration of their implications in string theory and related areas. The detailed analysis and results can be accessed through the provided links, which include the original publication and supplementary resources. This research contributes to the ongoing discourse on the interplay between geometry, fluxes, and brane dynamics in string theory, highlighting the significance of multi-Janus solutions in understanding the rich structure of half-BPS states.",
        "ori-fast-z-score": 0.13483997249264842,
        "water-fast-z-score": 2.1773242158072694,
        "rewrite-fast-z-score": 3.073993852018444
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Theory of Macroscopic Quantum Tunneling in High-T_c c-Axis Josephson Junctions .\nAbstract:\nWe present the theory for macroscopic quantum tunneling (MQT) in high-temperature superconductors with an emphasis on its application to c-axis junctions. We show that MQT is suppressed by thermal fluctuations and magnetic field, but can be enhanced by applying a bias current or increasing the junction area. The results are compared with experiments performed at 4 K and 77 K. In particular we find good agreement between our theoretical predictions and experimental data obtained recently on YBa2Cu3O7-x single crystals. \n \n Introduction \n \n Superconductivity was discovered more than half a century ago  1  . Since then many new materials have been found which exhibit this fascinating phenomenon  2  , including some with very high transition temperatures T_c  3  . However, despite intensive research efforts there still remain several open questions about the nature of these novel compounds  4  . One such question concerns the mechanism responsible for their unusual properties  5  . \n \n It has been suggested  6  that the pairing interaction may involve phonons  7 - 9  as well as spin excitations  10 - 12  . This leads to two possible scenarios for the formation of Cooper pairs  13  : either they form directly out of electrons via electron-phonon interactions  14  , or indirectly through spin-fluctuations  15  . These different mechanisms lead to distinct physical pictures  16  . For example, if one assumes that the pairing occurs only due to electron-phonon interactions  17  , it follows that the gap function should vanish along certain lines in momentum space  18  . On the other hand, if one considers the possibility of pair formation mediated by spin fluctuations  19  , the gap function vanishes over all momenta  20  . \nThe most important feature of both types of models is that they predict the existence of nodes  21  in the energy spectrum  22  . Nodes occur when the order parameter changes sign across the Fermi surface  23  . They were first predicted theoretically  24 - 26  and later observed experimentally  27  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Theory of Macroscopic Quantum Tunneling in High - T _ c c - Axis Josephson Junctions . Abstract : We bring the principle for macroscopic quantum tunneling ( MQT ) in high - temperature superconductors with an emphasis on its use to c - axis junctions .We see that MQT is suppressed by mechanical fluctuations and magnetic field , but can be enhanced by using a bias charge or increasing the junction region . The results are compared with experiments conducted at 4 K and 77 K . In particular we find good agreement between our theory estimates and theoretical data acquired previously on YBa2Cu3O7 - x single crystals .Introduction Superconductivity was known more than half a millennium later 1 . Since then many new materials have been found which exhibit this fascinating phenomenon 2 , notably some with very high transition temperatures T _ c 3 .However , despite intensive study efforts there still continue several open questions about the nature of these novel substances 4 . One such issue concerns the process controlling for their extraordinary properties 5 .It has been proposed 6 that the pairing interaction may involve phonons 7 - 9 as well as spin excitations 10 - 12 . This leads to two possible strategies for the formation of Cooper pairs 13 : either they shape directly out of atoms via electron - phonon interactions 14 , or indirectly through spin - fluctuations 15 .These different processes lead to distinct physical pictures 16 . For instance , if one assumes that the pairing arises only due to atom - phonon coupling 17 , it appears that the gap function should vanish along particular lines in momentum space 18 .On the other hand , if one considers the possibility of pair formation facilitated by spin fluctuations 19 , the gap integral vanishes over all momenta 20 . The most important feature of both types of models is that they predict the existence of nodes 21 in the power spectrum 22 .Nodes happen when the order parameter moves sign across the Fermi surface 23 . They were first expected theoretically 24 - 26 and later observed experimentally 27 .",
        "rewrite_text": "**Title:** Theory of Macroscopic Quantum Tunneling in High-Tc c-Axis Josephson Junctions\n\n**Abstract:** This article explores the principles of macroscopic quantum tunneling (MQT) in high-temperature superconductors, specifically focusing on c-axis Josephson junctions. Our findings indicate that MQT is adversely affected by mechanical fluctuations and external magnetic fields; however, it can be significantly enhanced through the application of a bias charge or by increasing the junction area. We present a comparative analysis of our theoretical results with experimental data obtained at temperatures of 4 K and 77 K, revealing a strong correlation between our theoretical predictions and previously recorded data on YBa2Cu3O7-x single crystals.\n\nThe phenomenon of superconductivity has intrigued scientists for over five centuries, leading to the discovery of numerous materials that exhibit this remarkable state, particularly those with exceptionally high transition temperatures (Tc). Despite extensive research, many questions remain regarding the underlying mechanisms that govern the unique properties of these high-Tc superconductors. One critical area of investigation is the nature of the pairing interactions responsible for Cooper pair formation, which may involve both phononic and spin excitations. This duality gives rise to two primary models for Cooper pair formation: one positing direct electron-phonon interactions and the other suggesting indirect pair formation via spin fluctuations.\n\nThese differing models yield distinct theoretical frameworks. For instance, if pairing is solely attributed to electron-phonon coupling, the gap function is expected to vanish along specific momentum space lines. Conversely, if spin fluctuations play a significant role, the gap integral would diminish across all momentum values. A crucial aspect of both models is the prediction of nodes in the power spectrum, which occur when the order parameter changes sign across the Fermi surface. The existence of these nodes was initially proposed in theoretical studies and has since been confirmed through experimental observations, underscoring the complexity and richness of the physics governing high-temperature superconductors.",
        "ori-fast-z-score": -0.49656353316142077,
        "water-fast-z-score": 7.010681840159742,
        "rewrite-fast-z-score": 0.3223291856101521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distributed implementation of standard oracle operators .\nAbstract:\nWe present an efficient distributed algorithm for implementing the most common query operations in relational databases, namely selection and projection. The proposed approach is based on the concept of data partitioning which allows us to distribute the workload among different nodes in order to minimize communication costs between them. We show that our method can be used as a building block for more complex queries such as joins or aggregates. Finally we provide experimental results showing how our solution outperforms existing approaches by orders of magnitude when dealing with large datasets. Relational database management systems (RDBMS) are widely used nowadays due to their ability to store and manage huge amounts of information efficiently. However, this efficiency comes at the cost of high computational complexity since all the data has to reside in main memory during processing time. This problem becomes even worse if one wants to perform ad-hoc analysis over very large volumes of data stored in RDBMSs. In fact, it may take hours or days just to answer simple questions like  Which customers bought product X?  or  What products were sold together last year? . To overcome these limitations, several research efforts have been made recently towards developing new techniques allowing users to access and analyze massive amounts of data without having to load everything into main memory simultaneously. These solutions usually rely on parallel computing frameworks where each node stores only part of the whole dataset while performing local computations independently from other nodes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distributed implementation of standard oracle operators . Abstract : We create an efficient dispersed algorithm for applying the most common query operations in relational databases , namely choice and projection .The proposed approach is based on the idea of data partitioning which allows us to organize the workload among different nodes in order to minimize transport fees between them . We see that our technique can be used as a building block for more sophisticated queries such as joins or aggregates .Finally we provide empirical results showing how our solution outperforms previous techniques by orders of magnitude when dealing with large datasets . Relational data management systems ( RDBMS ) are widely using today due to their capabilities to contain and manage huge amounts of content efficiently .However , this efficiency comes at the cost of high computational efficiency since all the information has to live in central cache during processing period . This problem arises even stronger if one wants to conduct ad - hoc analysis over very huge volumes of data stored in RDBMSs .In reality , it could took days or months just to respond simple answers like Which customers bought brand X ? or What products were sell together ago season ?. To solve these requirements , various study efforts have been made recently towards developing innovative techniques allowing consumers to access and retrieve massive amounts of content without having to load everything into main memory simultaneously .These solutions may depend on parallel processing frameworks where each node stores only portion of the whole dataset while performing local computations separately from other nodes .",
        "rewrite_text": "**Title:** Distributed Implementation of Standard Oracle Operators\n\n**Abstract:** In this study, we present an innovative distributed algorithm designed to efficiently execute fundamental query operations in relational databases, specifically focusing on selection and projection. Our approach leverages the concept of data partitioning, which facilitates the distribution of workloads across multiple nodes, thereby significantly reducing inter-node communication costs. This methodology not only enhances the performance of basic operations but also serves as a foundational component for executing more complex queries, such as joins and aggregations. We provide empirical evidence demonstrating that our solution significantly outperforms existing techniques, achieving improvements by several orders of magnitude when processing large datasets.\n\nRelational Database Management Systems (RDBMS) are increasingly prevalent due to their ability to manage extensive volumes of data effectively. However, this efficiency often comes at the expense of high computational demands, as all data must reside in central memory during processing. This challenge becomes even more pronounced when conducting ad-hoc analyses on vast datasets stored within RDBMSs. For instance, responding to straightforward queries like \"Which customers purchased brand X?\" or \"What products were sold together last season?\" can take an unacceptably long time, sometimes extending to days or even months.\n\nTo address these challenges, recent research has focused on developing advanced techniques that enable users to access and retrieve large amounts of data without the necessity of loading the entire dataset into main memory at once. These solutions often utilize parallel processing frameworks, where each node retains only a subset of the overall dataset and performs local computations independently of other nodes. Our work contributes to this body of research by providing a scalable and efficient method for executing standard query operations in a distributed manner, ultimately enhancing the capability of RDBMSs to handle large-scale data analytics.",
        "ori-fast-z-score": -2.014035259912054,
        "water-fast-z-score": 7.723229626397817,
        "rewrite-fast-z-score": 0.159111456835146
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Background study for the pn-CCD detector of CERN Axion Solar Telescope .\nAbstract:\nThe background radiation in space is dominated by cosmic rays and their secondary products, such as neutrons and gamma-rays. The most important source of these particles are galactic supernovae which occur at an average rate of one per century. In this work we present results on the background radiation expected to be measured with the pn-CCDs (p-type silicon charge-coupled devices) that will be used in the CERN Axion Solar telescope (CAST). We have simulated the response of CAST s detectors using GEANT4 Monte Carlo simulations. These were then combined with models of the particle fluxes in space to predict the background count rates observed by the cameras. Our predictions show that the background count rate due to cosmic ray interactions should not exceed 0.1 counts s-1 pixel-1 over the entire field-of-view of each camera. This corresponds to less than 1% of the signal expected from axions produced in the Sun s magnetic field.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Background study for the pn - CCD telescope of CERN Axion Solar Telescope . Abstract : The background radiation in space is dominated by cosmic rays and their secondary products , such as neutrons and gamma - rays .The most important source of these objects are galactic supernovae which occur at an estimated rate of one per century . In this research we present results on the background radiation anticipated to be recorded with the pn - CCDs ( p - class silicon charge - coupled devices ) that will be used in the CERN Axion Solar telescope ( CAST ) .We have modelled the response of CAST s detectors using GEANT4 Monte Carlo simulations . These were then combined with models of the particle fluxes in space to predict the background count levels observed by the cameras .Our predictions show that the background count rate due to cosmic ray interactions should not reach 0 . 1 counts s - 1 pixel - 1 over the entire field - of - view of each camera . This equals to little than 1 % of the signal expected from axions produced in the Sun s magnetic field .",
        "rewrite_text": "**Title:** Background Study for the pn-CCD Telescope of the CERN Axion Solar Telescope\n\n**Abstract:** The background radiation encountered in space is primarily influenced by cosmic rays and their secondary products, including neutrons and gamma rays. A significant contributor to these cosmic phenomena is galactic supernovae, which are estimated to occur approximately once every century. This study focuses on the anticipated background radiation that will be detected by the pn-CCDs (p-class silicon charge-coupled devices) employed in the CERN Axion Solar Telescope (CAST). To assess the background radiation levels, we utilized GEANT4 Monte Carlo simulations to model the response of the CAST detectors. These simulations were integrated with established models of particle fluxes in space to accurately predict the background count rates that the cameras will observe. Our findings indicate that the background count rate resulting from cosmic ray interactions is expected to remain below 0.1 counts per second per pixel across the entire field of view of each camera. This level of background noise is less than 1% of the signal anticipated from axions generated in the Sun's magnetic field. The implications of these results are significant for the design and operation of the CAST, as they suggest that the background radiation will not interfere substantially with the detection of axions, thereby enhancing the potential for successful observations in the quest to understand dark matter and its constituents. This research lays the groundwork for further investigations into the sensitivity and efficacy of the pn-CCD technology in astrophysical applications, particularly in the context of axion detection.",
        "ori-fast-z-score": 0.7977240352174656,
        "water-fast-z-score": 4.076197322920544,
        "rewrite-fast-z-score": 0.6644105970267493
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Properties of the Volume Operator in Loop Quantum Gravity I: Results .\nAbstract:\nWe present results on the volume operator in loop quantum gravity (LQG). The volume operator is an important ingredient for many physical applications, such as black hole entropy and cosmological perturbations. We show that it can be written as a sum over spin network states with coefficients which are determined by the geometry of the underlying graph. In particular we find that the spectrum of this operator agrees exactly with the one obtained using group field theory methods. This provides further evidence for the equivalence between LQG and group field theory at the level of operators. Finally, we discuss how to use these results to compute expectation values of the volume operator in semiclassical states. These results will appear elsewhere. DOI: 10.1088/1742-5468/2009/06/P06005. URL: http://arxiv.org/abs/0906.0571. PACS numbers: 04.20.-q, 11.25.Wx",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Properties of the Volume Operator in Loop Quantum Gravity I : Results . Abstract : We report findings on the volume operator in loop quantum gravitational ( LQG ) .The volume operator is an important ingredient for numerous physical applications , such as black hole entropy and cosmological perturbations . We see that it can be written as a sum over spinning system states with coefficients which are decided by the topology of the underlying graph .In particular we find that the spectrum of this operator agrees exactly with the one achieved using group field theory techniques . This offers further evidence for the equivalence between LQG and group field theory at the level of operators .Finally , we talk how to use these results to compute expectation values of the volume operator in semiclassical states . These conclusions will appear elsewhere .DOI : 10 . 1088 / 1742 - 5468 / 2009 / 06 / P06005 . URL : http : / / arxiv . org / abs / 0906 . 0571 .PACS dates : 04 . 20 . - q , 11 . 25 . Wx",
        "rewrite_text": "We present our findings on the volume operator within the framework of loop quantum gravity (LQG), a pivotal component for various physical phenomena, including black hole entropy and cosmological perturbations. Our analysis reveals that the volume operator can be expressed as a summation over states of a spinning system, with coefficients determined by the topology of the underlying graph. Notably, we have established that the spectrum of this operator aligns precisely with the spectrum derived from group field theory methodologies. This correlation provides compelling evidence supporting the equivalence between LQG and group field theory at the operator level. Furthermore, we discuss the implications of these results for calculating expectation values of the volume operator in semiclassical states, paving the way for future investigations. The detailed conclusions of our study will be published in subsequent works. For further reference, please see DOI: 10.1088/1742-5468/2009/06/P06005 and access the full article at http://arxiv.org/abs/0906.0571. The relevant PACS codes for this research are 04.20.-q and 11.25.Wx.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 2.9260286799032644,
        "rewrite-fast-z-score": 0.22645540682891913
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Hot Inter-Galactic Medium and the Cosmic Microwave Background .\nAbstract:\nThe hot intergalactic medium (IGM) is an important component in our understanding of galaxy formation, as it provides the fuel for star formation.  The IGM consists primarily of ionized hydrogen gas at temperatures between 10^6 K to 10^7 K with densities ranging from 1 atom/cm^3 to 100 atoms /cm^3.   In this talk we will discuss how observations of the Lyman-alpha forest can be used to probe the physical properties of the IGM on large scales.  We will also present results from recent work using data obtained by the Sloan Digital Sky Survey that show evidence for large-scale fluctuations in the temperature-density relation of the IGM. Finally, we will describe some current efforts aimed at measuring the thermal history of the universe through measurements of the kinetic SZ effect produced by clusters of galaxies. This research was supported by NASA grant NAG5-13286. The hot intergalactic medium is an important component in understanding galaxy formation because it provides the fuel for stars. It consists primarily of ionized Hydrogen gas at temperatures between 106K and 107K with densities ranging from 1atom/cm3 to 100atoms/cm3 .In this talk we will discuss observational probes of its physical properties on large-scales.We will present results showing evidence for large-scale fluctuations in teh temperature density relationship of the IGM based on data taken by the SDSS survey.Finally ,we will describe some current efforts to measure the thermal history of the Universe via measurement of the kinetic SZ effect generated by clusters of galaxies.This research has been funded by NASA grant NAG 5-13286",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Hot Inter - Galactic Medium and the Cosmic Microwave Background . Abstract : The warm intergalactic medium ( IGM ) is an important element in our understanding of galaxy formation , as it supplies the engine for star formation .The IGM consists primarily of ionized hydrogen liquid at conditions between 10 ^ 6 K to 10 ^ 7 K with densities varied from 1 molecule / cm ^ 3 to 100 atoms / cm ^ 3 . In this talk we will explore how measurements of the Lyman - alpha forest can be used to probe the physical properties of the IGM on huge scales .We will also discuss results from recent work using data acquired by the Sloan Digital Sky Survey that demonstrate proof for large - scale fluctuations in the temperature - density relation of the IGM . Finally , we will explain some latest attempts aimed at assessing the thermal evolution of the universe through measurements of the kinetic SZ phenomenon produced by clusters of galaxies .This research was supported by NASA loan NAG5 - 13286 . The warm intergalactic medium is an important element in understanding galaxy formation because it gives the fuel for stars .It consists primarily of ionized Hydrogen gas at levels between 106K and 107K with densities varied from 1atom / cm3 to 100atoms / cm3 . In this talk we will explain observational probes of its physical properties on huge - scales . We will present results promising confirmation for large - scale fluctuations in teh heat density correlation of the IGM based on evidence drawn by the SDSS sample . Finally , we will explain some latest attempts to measure the thermal history of the Universe via measurement of the kinetic SZ phenomenon generated by clusters of stars . This research has been sponsored by NASA grant NAG 5 - 13286",
        "rewrite_text": "**Title:** The Hot Intergalactic Medium and the Cosmic Microwave Background\n\n**Abstract:** The warm intergalactic medium (IGM) plays a crucial role in our comprehension of galaxy formation, serving as a vital source of fuel for star formation processes. Predominantly composed of ionized hydrogen, the IGM exists under extreme conditions, with temperatures ranging from 10^6 K to 10^7 K and densities fluctuating between 1 molecule/cm^3 and 100 atoms/cm^3. In this presentation, we will delve into the utilization of Lyman-alpha forest measurements as a means to investigate the physical characteristics of the IGM across vast cosmic scales. We will highlight findings from recent studies leveraging data from the Sloan Digital Sky Survey (SDSS), which provide compelling evidence for significant large-scale fluctuations in the temperature-density relationship of the IGM. Furthermore, we will discuss ongoing efforts aimed at reconstructing the thermal evolution of the universe, particularly through the analysis of the kinetic Sunyaev-Zel'dovich (SZ) effect induced by galaxy clusters. This research is supported by NASA grant NAG5-13286, underscoring the importance of understanding the IGM in the broader context of cosmic evolution and structure formation. By examining these phenomena, we aim to enhance our understanding of the intricate processes that govern the universe's thermal history and the formation of galaxies.",
        "ori-fast-z-score": -0.9053574604251853,
        "water-fast-z-score": 6.9428561869392285,
        "rewrite-fast-z-score": 0.953998092005724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Subjective Evaluation of Forms in an Immersive Environment .\nAbstract:\nWe present the results of subjective evaluation experiments conducted on forms designed for use within immersive environments, such as virtual reality (VR) and augmented reality (AR). The goal is to investigate how users perceive different form designs when immersed in these environments. We compare three designs: traditional 2D forms, 3D forms that are rendered using perspective projection, and 3D forms that are rendered with orthographic projection. Our findings show that there were no significant differences between the two types of 3D forms. However, both 3D forms received significantly higher ratings than their 2D counterparts. This suggests that 3D forms can be used effectively in immersive environments without requiring special rendering techniques or additional hardware. In addition, we found that participants preferred forms that had more visual cues indicating depth information over those that did not have any such cues. Finally, our study also shows that it may be possible to create effective forms by combining elements from multiple existing designs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Subjective Evaluation of Forms in an Immersive Environment . Abstract : We report the results of subjective assessment research conducted on forms designed for use within immersive environments , such as virtual reality ( VR ) and augmented reality ( AR ) .The goal is to examine how people interpret different form shapes when immersed in these spaces . We contrast three patterns : traditional 2D forms , 3D shapes that are rendered using view projection , and 3D shapes that are rendered with orthographic projection .Our findings show that there were no considerable changes between the two forms of 3D shapes . However , both 3D shapes earned considerably higher ratings than their 2D counterparts .This shows that 3D shapes can be used effectively in immersive environments without using special representation techniques or additional hardware . In addition , we concluded that participants favored forms that had more graphical stimuli indicating deep information over those that did not have any such cues .Finally , our research also shows that it could be possible to create effective forms by combining components from multiple older structures .",
        "rewrite_text": "We present the findings of a subjective evaluation study focused on the perception of various form designs intended for immersive environments, including virtual reality (VR) and augmented reality (AR). The primary objective of this research was to investigate how individuals interpret different shapes when fully immersed in these digital spaces. To achieve this, we compared three distinct form categories: traditional two-dimensional (2D) shapes, three-dimensional (3D) shapes rendered through view projection, and 3D shapes rendered using orthographic projection techniques. \n\nOur analysis revealed that there were no significant differences in user ratings between the two types of 3D shapes. However, both categories of 3D shapes received substantially higher evaluations compared to their 2D counterparts, indicating that 3D forms are more effective in immersive settings without the need for advanced representation techniques or specialized hardware. Furthermore, our study found that participants showed a preference for forms that incorporated more graphical elements suggesting deeper informational content, as opposed to those lacking such visual cues. \n\nThese insights suggest that the design of effective forms in immersive environments can benefit from integrating features from various existing structures. Overall, our research contributes to the understanding of how form design influences user experience in immersive technologies and highlights the potential for creating more engaging and informative visual representations in VR and AR applications.",
        "ori-fast-z-score": -1.3779972440082682,
        "water-fast-z-score": 7.023508406036853,
        "rewrite-fast-z-score": 1.2018504251546631
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Real Time Turbulent Video Perfecting by Image Stabilization and Super-Resolution .\nAbstract:\nWe propose an image stabilizing method for video capturing in turbulent environments, which is based on the super-resolution technique. The proposed method can be used to improve the quality of captured videos with handheld cameras or other devices that are subject to motion blur caused by turbulence. We first estimate the camera motion using optical flow between consecutive frames. Then we use this information as input data into our super-resolution algorithm to generate high resolution images. Finally, these images are combined together to produce a stable output frame. Our experimental results show that the proposed method outperforms existing methods both visually and quantitatively. \n \n Keywords: Real time video processing; Turbulence; Motion blur; Optical flow; Super-resolution; Image stabilization. 1 Introduction \n \n In recent years there has been growing interest in developing techniques for real time video processing applications such as video surveillance systems  1  , traffic monitoring  2  , remote sensing  3  . However, most of these applications require capturing clear images under challenging conditions like low-light illumination  4  , fast moving objects  5  , blurry scenes  6  , etc.. Among all these challenges, one of the major problems is how to deal with the motion blur caused by turbulence  7  8  9  when capturing videos with hand-held cameras or other devices  10  . \n \n Figure 1: An example of a video sequence taken at nighttime (a) and its corresponding ground truth (b).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Real Time Turbulent Video Perfecting by Image Stabilization and Super - Resolution . Abstract : We suggest an image stabilizing method for video capturing in volatile settings , which is based on the super - resolution technique .The proposed approach can be used to improve the performance of released movies with handheld cameras or other devices that are subject to moving blur caused by turbulence . We first estimate the film motion using optical flow between successive frames .Then we using this data as input data into our super - resolution algorithm to create high resolution photos . Finally , these images are fused together to produce a consistent output frame .Our research results show that the suggested method outperforms current methods both physically and quantitatively . Keywords : Real time television processing ; Turbulence ; Motion blur ; Optical stream ; Super - resolution ; Image stabilization .1 Introduction In recent years there has been growing interest in improving strategies for real time video editing applications such as video monitoring systems 1 , road monitoring 2 , remote sensing 3 . However , most of these uses involve capturing clear photographs under difficult environments like low - light lighting 4 , fast move images 5 , blurry scenes 6 , etc . .Among all these problems , one of the significant obstacles is how to deal with the movement blur caused by turbulence 7 8 9 when capturing clips with hand - held photographers or other devices 10 . Figure 1 : An example of a video sequence taken at nighttime ( a ) and its equivalent ground truth ( c ) .",
        "rewrite_text": "**Title:** Real-Time Turbulent Video Enhancement through Image Stabilization and Super-Resolution\n\n**Abstract:** In this study, we present a novel image stabilization technique designed for video capture in dynamic environments, leveraging super-resolution methods to enhance visual quality. Our approach addresses the challenges posed by motion blur, particularly in footage obtained from handheld cameras or other devices susceptible to turbulence-induced disturbances. Initially, we employ optical flow analysis to estimate the motion occurring between consecutive frames of the video. This motion data serves as a critical input for our super-resolution algorithm, which generates high-resolution images from the original footage. Subsequently, we integrate these enhanced images to produce a coherent output frame that significantly improves the overall clarity and stability of the video. Our experimental results demonstrate that the proposed method surpasses existing techniques in both qualitative and quantitative assessments, providing a robust solution for real-time video processing in turbulent conditions. The implications of this research extend to various applications, including video surveillance, traffic monitoring, and remote sensing, where capturing clear images in challenging environments is essential. By addressing the prevalent issue of motion blur, our method contributes to the advancement of real-time video editing technologies, enhancing the viewing experience and the utility of captured footage. \n\n**Keywords:** Real-time video processing; Turbulence; Motion blur; Optical flow; Super-resolution; Image stabilization. \n\n**1 Introduction:** Recent advancements in real-time video editing have sparked significant interest in improving techniques for applications such as video surveillance, road monitoring, and remote sensing. These applications often require the capture of clear images under adverse conditions, including low-light environments, rapid motion, and blurred scenes. Among these challenges, mitigating motion blur caused by turbulence remains a critical hurdle, particularly when using handheld devices. This paper outlines our innovative approach to overcoming these obstacles and enhancing video quality in real-time settings.",
        "ori-fast-z-score": -0.8703882797784892,
        "water-fast-z-score": 6.910947404650881,
        "rewrite-fast-z-score": -1.927248223318863
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Mean and Scatter of the Velocity Dispersion-Optical Richness Relation for maxBCG Galaxy Clusters .\nAbstract:\nWe present new measurements of the mean and scatter in the velocity dispersion-optical richness relation (VRR) for galaxy clusters selected by their red-sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use two different methods to measure cluster velocities, one based on spectroscopic observations of member galaxies and another that uses photometric redshift estimates of all galaxies within a fixed aperture centered on each cluster s brightest cluster galaxy (BGG).\nThe results are consistent with previous studies at low-redshift but show significant differences when compared to recent work done at higher redshifts. The discrepancies between our results and those presented in earlier works may be due to systematic effects associated with the measurement techniques used or could indicate evolution in the VRR over time. In either case, these results highlight the need for further investigation into this relationship as well as other scaling relations involving galaxy clusters. \n \n Keywords: galaxy cluster, optical richness",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Mean and Scatter of the Velocity Dispersion - Optical Richness Relation for maxBCG Galaxy Clusters . Abstract : We report new measurements of the mean and scatter in the velocity dispersion - optical richness relation ( VRR ) for galaxy galaxies designated by their red - sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) .We use two different methods to measure cluster velocities , one based on spectroscopic observations of member galaxies and another that using photometric redshift estimates of all galaxies within a fixed aperture located on each cluster s brightest cluster galaxy ( BGG ) . The results are compatible with previous research at low - redshift but display significant variations when compared to recent work done at higher redshifts .The discrepancies between our findings and those offered in earlier works might be due to systematic effects involved with the monitoring procedures used or could indicate evolution in the VRR over time . In either case , these results emphasize the necessity for further investigation into this relationship as well as other scaling relations involving star clusters .Keywords : galaxy cluster , optical richness",
        "rewrite_text": "We present new findings on the mean and scatter of the velocity dispersion-optical richness relation (VRR) for galaxy clusters identified by their red-sequence galaxies, utilizing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our study employs two distinct methodologies for measuring cluster velocities: one method relies on spectroscopic observations of member galaxies, while the other utilizes photometric redshift estimates for all galaxies within a fixed aperture centered on each cluster's brightest cluster galaxy (BCG). Our results align with previous studies conducted at low redshift; however, they reveal notable discrepancies when compared to more recent research focused on higher redshifts. These differences may stem from systematic effects associated with the measurement techniques employed or could suggest an evolutionary change in the VRR over cosmic time. Regardless of the underlying cause, our findings underscore the importance of further exploration into the VRR and other scaling relations pertinent to galaxy clusters. This research contributes to a deeper understanding of the dynamics and formation of galaxy clusters, highlighting the need for continued investigation in this area. \n\nKeywords: galaxy cluster, optical richness.",
        "ori-fast-z-score": 1.4269353798659745,
        "water-fast-z-score": 6.32831881684378,
        "rewrite-fast-z-score": 1.2939932784412609
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prospects for the cavity-assisted laser cooling of molecules .\nAbstract:\nWe present an overview of recent progress in the development and application of techniques to cool molecules by exploiting their interaction with optical cavities. We discuss how these methods can be used to prepare samples of cold, trapped molecules that are suitable for precision measurements or quantum information processing applications. In particular we focus on two different approaches which have been developed recently at our laboratory: (i) The use of electromagnetically induced transparency (EIT), combined with stimulated Raman adiabatic passage (STIRAP), to produce large numbers of optically trapped ground-state polar molecules. (ii) Cavity-enhanced photoassociation spectroscopy as a tool to study ultracold collisions between alkali-metal atoms. Finally, we briefly outline some possible future directions for this research area. Molecules offer many advantages over atomic systems when it comes to implementing novel quantum technologies such as high-precision metrology  1  , quantum simulation  2  , and quantum networks  3  . However, most molecular species cannot be directly cooled using conventional laser cooling schemes because they lack closed cycling transitions  4  .\nIn order to overcome this problem several alternative cooling strategies have been proposed  5  -  8  . One promising approach is based on the combination of electromagnetically-induced transparency (EIT)  9  and stimulated Raman adiabatic passages (STIRAP)  10  . This method has been successfully applied to create dense ensembles of ground state polar molecules  11  -  13  . Another possibility consists in trapping molecules via photoassociative processes  14  -  16  . Here one exploits the fact that the spontaneous emission rate into bound states increases exponentially with decreasing temperature  17  . By coupling the excited molecular levels to high-finesse optical cavities  18  -  20  , the resulting increase in radiative lifetime leads to efficient trapping  21  -  23  . These techniques allow us to trap up to 10 5 molecules per cm 3 inside a single-mode optical resonator  24  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prospects for the cavity - aided laser cooling of molecules . Abstract : We illustrate an overview of recent progress in the development and application of techniques to cool molecules by exploiting their interaction with optical cavities .We discuss how these systems can be used to produce samples of cold , captured molecules that are suitable for precision observations or quantum information processing applications . In particular we focus on two different methods which have been pioneered recently at our laboratory : ( i ) The using of electromagnetically induced transparency ( EIT ) , combined with stimulated Raman adiabatic passage ( STIRAP ) , to produce substantial quantity of optically captured ground - state polar compounds .( ii ) Cavity - augmented photoassociation spectroscopy as a technique to study ultracold collisions between alkali - metal atoms . Finally , we briefly outline some possible future paths for this research field .Molecules offer several advantages over nuclear systems when it comes to incorporating novel quantum technologies such as high - precision metrology 1 , quantum modeling 2 , and quantum networks 3 . However , most molecular species cannot be directly warmed use conventional optical cooling schemes because they lack open cycling transitions 4 .In try to overcome this situation several alternative thermal strategies have been proposed 5 - 8 . One promising solution is based on the combination of electromagnetically - triggered stability ( EIT ) 9 and stimulated Raman adiabatic passages ( STIRAP ) 10 .This method has been successfully applied to create rich ensembles of ground state polar molecules 11 - 13 . Another possibility consists in trap molecules via photoassociative processes 14 - 16 .Here one utilizes the fact that the spontaneous emission speed into bound products increases exponentially with decreasing temperature 17 . By coupling the excited molecular levels to large - finesse optical cavities 18 - 20 , the resulting increase in radiative lifetime leads to efficient capturing 21 - 23 .These methods able us to capture up to 10 5 molecules per cm 3 inside a single - mode optical resonator 24 .",
        "rewrite_text": "**Title: Prospects for Cavity-Aided Laser Cooling of Molecules**\n\n**Abstract:** This article provides a comprehensive overview of the recent advancements in techniques aimed at cooling molecules through their interaction with optical cavities. We explore how these innovative systems can generate samples of cold, trapped molecules that are ideal for high-precision measurements and quantum information processing applications. Our focus is primarily on two pioneering methods developed in our laboratory: (i) the application of electromagnetically induced transparency (EIT) in conjunction with stimulated Raman adiabatic passage (STIRAP) to produce significant quantities of optically trapped ground-state polar molecules, and (ii) cavity-enhanced photoassociation spectroscopy, which serves as a tool for investigating ultracold collisions between alkali-metal atoms. \n\nWe also discuss the advantages of using molecules over atomic systems for the implementation of advanced quantum technologies, including high-precision metrology, quantum simulation, and quantum networking. Despite these advantages, many molecular species present challenges for conventional optical cooling methods due to the absence of open cycling transitions. To address this limitation, several alternative thermal strategies have been proposed. Among these, the combination of EIT and STIRAP has emerged as a particularly promising approach, successfully yielding rich ensembles of ground-state polar molecules. \n\nAdditionally, we examine the potential of trapping molecules through photoassociative processes, leveraging the phenomenon that the rate of spontaneous emission into bound states increases exponentially as temperature decreases. By integrating excited molecular states with high-finesse optical cavities, we achieve a significant enhancement in radiative lifetimes, facilitating efficient trapping of molecules. Our methods enable the capture of up to 10^5 molecules per cm³ within a single-mode optical resonator. Finally, we outline prospective directions for future research in this dynamic field, emphasizing the potential for further breakthroughs in molecular cooling techniques and their applications in quantum technologies.",
        "ori-fast-z-score": 0.3916302249939787,
        "water-fast-z-score": 7.183993046875879,
        "rewrite-fast-z-score": -0.9607689228305227
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for pulsation among suspected A-type binaries and the new multiperiodic Delta Scuti star HD217860 .\nAbstract:\nWe have carried out an extensive search for periodicities in the light curves of all known spectroscopic binary systems with components classified as A-type stars (AAB, AAO, AB) by using the phase dispersion minimization method.  We found that two of these systems are indeed double-mode Cepheids, while another one is probably a single-mode Cepheid. The remaining three systems show no significant periodicity at all. In addition to this we report on the discovery of a new multiperiodic Delta-Scuti variable which shows several frequencies between 0.5 and 1 d-1. This work was supported by the Hungarian OTKA grant K-81345. -The first column gives the name of the system; the second column lists the spectral type of each component according to SIMBAD database; the third column contains the orbital period taken from the literature; the fourth column indicates whether or not there exists any evidence for pulsation; the fifth column gives references where more information can be obtained about the individual objects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for pulsation among potential A - class binaries and the new multiperiodic Delta Scuti star HD217860 . Abstract : We have carried out an extensive search for periodicities in the light curves of all known spectroscopic binary systems with elements categorized as A - class stars ( AAB , AAO , AB ) by using the phase dispersion minimization method .We determined that two of these systems are indeed double - mode Cepheids , while another one is probably a single - mode Cepheid . The remaining three systems show no considerable periodicity at all .In addition to this we publish on the discovery of a new multiperiodic Delta - Scuti variable which gives numerous frequencies between 0 . 5 and 1 d - 1 . This research was supported by the Hungarian OTKA award K - 81345 .- The first column gives the naming of the system ; the second column shows the spectral type of each component according to SIMBAD computer ; the third column contains the orbital period taken from the literature ; the third column denotes whether or not there exists any evidence for pulsation ; the fifth column gives references where more information can be obtained about the specific elements .",
        "rewrite_text": "**Title:** Investigation of Pulsation in Potential A-Class Binaries and Discovery of the New Multiperiodic Delta Scuti Star HD217860\n\n**Abstract:** In this study, we conducted a comprehensive analysis of the light curves of all known spectroscopic binary systems classified as A-class stars (AAB, AAO, AB) to identify periodicities using the phase dispersion minimization technique. Our findings revealed that two of these systems are confirmed double-mode Cepheids, while one is likely a single-mode Cepheid. The remaining three systems did not exhibit any significant periodicity. Additionally, we report the discovery of a new multiperiodic Delta Scuti variable star, HD217860, which displays multiple frequencies ranging from 0.5 to 1 d^-1. This research was made possible through the support of the Hungarian OTKA grant K-81345. The accompanying table provides detailed information on the studied systems: the first column lists the system names, the second column indicates the spectral types of each component as per the SIMBAD database, the third column presents the orbital periods sourced from existing literature, the fourth column assesses the presence of pulsation evidence, and the fifth column includes references for further details on the specific elements. This work contributes to our understanding of pulsational behavior in A-class binaries and enhances the catalog of known Delta Scuti stars, paving the way for future investigations into stellar pulsations and their underlying mechanisms.",
        "ori-fast-z-score": 0.5488212999484517,
        "water-fast-z-score": 5.019011475427825,
        "rewrite-fast-z-score": 1.212183053462653
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hubble and Spitzer Observations of an Edge-on Circumstellar Disk around a Brown Dwarf .\nAbstract:\nWe present Hubble Space Telescope (HST) and Spitzer Space Telescope observations of the edge-on circumstellar disk surrounding 2MASS J04414489+2512172, a young brown dwarf with spectral type M8 located in Upper Scorpius at a distance of 145 pc. The HST data reveal that this object is surrounded by a bright ring-like structure extending to ~0.5′′ (~120 AU). We find evidence for two spiral arms emerging from the inner part of the ring toward its center. These features are also seen in near-infrared images obtained with the adaptive optics system NACO on VLT/UT4. In addition, we detect several knots along these spirals which may be caused by dust clumps or planetesimals embedded within them. Our results suggest that the observed structures could have been formed through gravitational instability triggered by rapid inward migration of solids due to gas drag forces.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hubble and Spitzer Observations of an Edge - on Circumstellar Disk around a Brown Dwarf . Abstract : We report Hubble Space Telescope ( HST ) and Spitzer Space Telescope observations of the edge - on circumstellar disk surrounding 2MASS J04414489 + 2512172 , a young white dwarf with spectral variety M8 situated in Upper Scorpius at a distance of 145 pc .The HST results show that this object is surrounded by a bright ring - like structure extending to ~ 0 . 5 ′ ′ ( ~ 120 AU ) . We see evidence for two spiral arms arising from the inner part of the circle toward its center .These features are also shown in near - infrared images obtained with the adaptive optics scheme NACO on VLT / UT4 . In addition , we perceive several knots along these spirals which may be caused by dust clumps or planetesimals embedded within them .Our results show that the known structures could have been formed through gravity instability caused by rapid inward movement of solids due to gas drag forces .",
        "rewrite_text": "We present findings from observations conducted with the Hubble Space Telescope (HST) and the Spitzer Space Telescope, focusing on the edge-on circumstellar disk surrounding the young brown dwarf 2MASS J04414489 + 2512172, located in the Upper Scorpius region at a distance of 145 parsecs. The HST observations reveal a prominent ring-like structure encircling the brown dwarf, extending approximately 0.5 arcseconds, which corresponds to about 120 astronomical units. Notably, we observe the presence of two spiral arms that emerge from the inner region of the disk and extend toward its center. These spiral features are corroborated by near-infrared images captured using the adaptive optics system NACO on the Very Large Telescope (VLT) at UT4. Furthermore, our analysis identifies several distinct knots along the spirals, which may indicate the presence of dust clumps or planetesimals that are likely embedded within the disk. The structures observed in the circumstellar disk suggest that they may have formed as a result of gravitational instabilities, driven by the rapid inward movement of solid materials influenced by gas drag forces. This study enhances our understanding of disk dynamics around brown dwarfs and provides insights into the processes that govern the formation of such structures in protoplanetary environments. Our findings contribute to the broader discourse on the evolution of circumstellar disks and their role in planet formation, particularly in the context of low-mass stellar objects.",
        "ori-fast-z-score": 0.23904572186687872,
        "water-fast-z-score": 5.103161130233569,
        "rewrite-fast-z-score": 1.0945409092309881
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mode stability in delta Scuti stars: linear analysis versus observations in open clusters .\nAbstract:\nWe present an extensive study on the mode stability properties of delta Scuti (δ Sct) pulsators, based on both theoretical and observational results obtained for open clusters with ages between 1 Myr and 2 Gyr. We have performed detailed non-radial stellar oscillation calculations using state-of-the-art evolutionary models that include overshooting at convective boundaries as well as microscopic diffusion processes. The main goal is to investigate how the observed frequency pattern changes during evolution due to the effects of rotation-induced mixing and chemical composition gradients. In particular we focus our attention on the so-called mixed modes which are trapped in the region where the hydrogen burning shell overlaps with the helium core. These modes show very characteristic features such as large amplitudes and high degree of nonlinearity. Our results indicate that these modes can be excited by turbulent pressure fluctuations associated with the convection zone located near the surface layers of the star. Moreover, they also suggest that the excitation mechanism may change significantly when the star evolves off the ZAMS towards higher luminosities.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mode stability in delta Scuti stars : linear analysis versus studies in open complexes . Abstract : We present an extensive research on the mode stability properties of delta Scuti ( δ Sct ) pulsators , relying on both theoretical and observational results collected for open clusters with ages between 1 Myr and 2 Gyr .We have done extensive non - radial stellar oscillation calculations combining state - of - the - art evolutionary models that include overshooting at convective boundaries as well as microscopic diffusion processes . The main goal is to examine how the seen frequency pattern moves during evolution owing to the effects of rotation - caused mixing and chemical composition gradients .In particular we focus our focus on the so - called mixture modes which are trapped in the location where the hydrogen burning shell overlaps with the helium core . These modes display very typical characteristics such as huge amplitudes and large extent of nonlinearity .Our results suggests that these modes can be excited by turbulent pressure fluctuations associated with the convection zone situated near the surface layers of the star . Moreover , they also suggest that the excitation process may change considerably when the star evolves off the ZAMS towards higher luminosities .",
        "rewrite_text": "In this study, we conduct a comprehensive investigation into the mode stability characteristics of delta Scuti (δ Sct) pulsators, utilizing both theoretical frameworks and observational data gathered from open clusters ranging in age from 1 million to 2 billion years. Our research employs advanced non-radial stellar oscillation calculations, integrating cutting-edge evolutionary models that account for overshooting at convective boundaries and the effects of microscopic diffusion processes. The primary objective of this work is to analyze the evolution of frequency patterns in these pulsators, particularly as influenced by rotational mixing and gradients in chemical composition throughout their lifecycle.\n\nA significant focus of our analysis is on the so-called mixture modes, which are confined to regions where the hydrogen burning shell intersects with the helium core. These modes are characterized by their pronounced amplitudes and notable nonlinearity. Our findings indicate that these mixture modes can be excited by turbulent pressure fluctuations arising from the convection zone located in the star's outer layers. Furthermore, we observe that the excitation mechanisms for these modes may undergo substantial changes as the star evolves away from the Zero-Age Main Sequence (ZAMS) and ascends to higher luminosities.\n\nThis research not only enhances our understanding of the pulsation mechanisms in δ Sct stars but also provides insights into the broader implications of stellar evolution on mode stability. By correlating theoretical predictions with observational data from open clusters, we aim to contribute to the ongoing discourse in stellar astrophysics regarding the intricate interplay between stellar structure, evolution, and pulsation phenomena.",
        "ori-fast-z-score": -1.4925557853149838,
        "water-fast-z-score": 2.970442628930023,
        "rewrite-fast-z-score": -0.27975144247209416
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Discovery of Two Spectroscopically Peculiar, Low-Luminosity Quasars at z~4 .\nAbstract:\nWe report the discovery of two low-luminosity quasars (LLQs) with redshifts ~4 in the Sloan Digital Sky Survey Data Release 7 quasar catalog. The LLQs are spectroscopically peculiar and have very weak emission lines compared to typical quasars at similar redshift. We find that their optical-to-infrared spectral energy distributions can be well-fitted by an accretion disk model with extremely high black hole masses (M BH >10 10 M sun ) and Eddington ratios (L/L edd >0.1). These results suggest that these objects may represent a new class of supermassive black holes which grow rapidly through gas-rich mergers or interactions during early cosmic epochs. Keywords: Black Hole Mass, Accretion Disk Modeling, Supermassive Black Holes, Quasars, Redshift 4, Optical-Infrared Spectral Energy Distributions",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Discovery of Two Spectroscopically Peculiar, Low-Luminosity Quasars at z~4 .Abstract:\nWe report the discovery of two low-luminosity quasars (LLQs) with redshifts ~4 in the Sloan Digital Sky Survey Data Release 7 quasar catalog.The LLQs are spectroscopically strange and have very low emission lines relative to normal quasars at comparable redshift . We see that their optical - to - infrared spectral power distributions can be well - fitted by an accretion disk model with incredibly high black hole masses ( M BH > 10 10 M sun ) and Eddington ratios ( L / L edd > 0 . 1 ) .These data suggest that these objects may constitute a new category of supermassive black holes which grow dramatically through gas - rich mergers or interactions during early cosmic epochs . Keywords : Black Hole Mass , Accretion Disk Modeling , Supermassive Black Holes , Quasars , Redshift 4 , Optical - Infrared Spectral Energy Distributions",
        "rewrite_text": "We present the findings of our investigation into two low-luminosity quasars (LLQs) identified at redshifts around 4, utilizing data from the Sloan Digital Sky Survey Data Release 7 quasar catalog. These quasars exhibit unusual spectral characteristics, displaying significantly diminished emission lines when compared to typical quasars at similar redshifts. Our analysis indicates that their optical-to-infrared spectral energy distributions can be accurately modeled using an accretion disk framework, which reveals exceptionally high black hole masses exceeding 10 billion solar masses (M BH > 10^10 M⊙) and Eddington ratios greater than 0.1 (L / L edd > 0.1). The implications of these findings suggest that these quasars may represent a novel class of supermassive black holes that undergo rapid growth through gas-rich mergers or interactions during the early stages of cosmic evolution. This discovery not only enhances our understanding of black hole formation and growth in the early universe but also raises intriguing questions about the mechanisms driving the evolution of such peculiar quasars. Our results contribute to the broader discourse on the diversity of quasar populations and the conditions that foster the emergence of supermassive black holes in the primordial universe. Keywords: Black Hole Mass, Accretion Disk Modeling, Supermassive Black Holes, Quasars, Redshift 4, Optical-Infrared Spectral Energy Distributions.",
        "ori-fast-z-score": -1.6641005886756874,
        "water-fast-z-score": 2.8845718429224694,
        "rewrite-fast-z-score": -0.7337993857053429
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Time-Delayed Feedback control of a flashing ratchet .\nAbstract:\nWe study the dynamics of an overdamped Brownian particle in a periodic potential with time-delayed feedback control, where the delay is introduced by a finite response time of the controller. We show that this system can be described as a flashing ratchet and derive analytical expressions for its current-voltage characteristics. The results are compared to numerical simulations. \nI. INTRODUCTIO N\nThe motion of particles driven out of equilibrium has been studied extensively over recent years  1  . In particular, it was shown that under certain conditions such systems may exhibit directed transport even without any net bias  2  , which is known as stochastic resonance  3  or coherence resonance  4  .\nIn many cases, however, these effects occur only if the noise level exceeds some threshold value  5  . This problem can be overcome using delayed feedback control  6  , i.e., controlling the system not instantaneously but after a fixed amount of time t d . It turns out that this method allows one to obtain directed transport at arbitrarily small noise levels  7, 8  . Moreover, it also works when the external driving force vanishes  9  . However, all previous studies were restricted to unidirectional potentials (i.e., potentials with reflection symmetry). Here we consider a more general case of a bidirectional potential  10  , which exhibits two local minima separated by a barrier.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Time - Delayed Feedback control of a blinking ratchet . Abstract : We explore the dynamics of an overdamped Brownian particle in a periodic potential with time - slowed feedback control , where the delay is created by a finite reaction period of the controller .We see that this device can be described as a blinking ratchet and derive analytical expressions for its current - voltage parameters . The results are compared to numerical simulations .I . INTRODUCTIO N The movement of particles driven out of equilibrium has been studied thoroughly over recent years 1 . In particular , it was shown that under certain conditions such systems may exhibit directed transport even without any gross bias 2 , which is known as stochastic resonance 3 or coherence resonance 4 .In many situations , however , these influences occur only if the noise height exceeds some threshold value 5 . This problem can be overcome using slowed feedback control 6 , i . e . , directing the process not instantaneously but after a specified quantity of time t d .It turns out that this method enables one to obtain directed transport at arbitrarily small noise heights 7 , 8 . Moreover , it also acts when the external driving field vanishes 9 .However , all previous research were restricted to unidirectional potentials ( i . e . , potentials with reflection symmetry ) . Here we suppose a more general example of a bidirectional potential 10 , which features two local minima separated by a barrier .",
        "rewrite_text": "In this study, we investigate the behavior of an overdamped Brownian particle subjected to a periodic potential, incorporating a time-delayed feedback control mechanism. This delay arises from the finite reaction time of the controlling system, allowing us to conceptualize the system as a blinking ratchet. We derive analytical expressions for the current-voltage characteristics of this system and validate our findings through numerical simulations. \n\nThe movement of particles in non-equilibrium conditions has garnered significant attention in recent years, revealing that directed transport can occur even in the absence of a significant external bias, a phenomenon known as stochastic resonance or coherence resonance. However, such effects typically require the noise level to exceed a certain threshold. Our approach utilizes time-slowed feedback control, which enables the manipulation of the particle's dynamics with a delay, thus facilitating directed transport even at minimal noise levels. Notably, this method remains effective even when the external driving force is absent.\n\nPrevious studies have primarily focused on unidirectional potentials, characterized by reflection symmetry. In contrast, our research expands the scope by examining a bidirectional potential that contains two local minima separated by a barrier. This broader perspective allows for a more comprehensive understanding of the dynamics involved in the blinking ratchet mechanism. Our findings contribute to the ongoing discourse on particle transport in non-equilibrium systems and highlight the potential of delayed feedback control in enhancing directed transport under various conditions.",
        "ori-fast-z-score": -0.5669467095138409,
        "water-fast-z-score": 5.2915026221291805,
        "rewrite-fast-z-score": 0.6910947404650881
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fluctuations of finite-time stability exponents in the standard map and the detection of small islands .\nAbstract:\nWe study fluctuations of finite-time Lyapunov exponent (FTLE) for chaotic orbits on the standard map with periodic boundary conditions. We show that FTLE fluctuates around its mean value, which is determined by the largest unstable periodic orbit embedded into the chaotic attractor. The amplitude of these fluctuations decreases exponentially as time increases. In addition to this exponential decay we observe an algebraic tail at large times. This algebraic tail can be explained by the presence of small islands inside the chaotic sea. These results are confirmed numerically using different methods. \nI. INTRODUCTORY REMARK\nThe concept of finite-time Lyapunovexponent(FTLE), introduced by Wolf et al  1  , has been widely used recently  2  -  4  . It describes how fast nearby trajectories diverge or converge during some fixed period of time T . For example, if one considers two neighboring points x 0 = x(t 0 ) and y 0 = x(t 1 ), where t 0 < t 1 , then their separation after time T will be given by: \nwhere λ max > 0 is the maximum Lyapunov exponent characterizing the rate of divergence between nearby trajectories  5  .\nIn order to calculate the FTLE it is necessary to solve the following variational equation:\nwhere J is the Jacobian matrix corresponding to the flow generated by Eq. (1). If the initial condition z 0 = x t0 + εy t0 is close enough to the reference trajectory x t0 , i.e., |ε| ≪ 1, then the solution of Eq.(2) can be written as:\nwhere P n (T ; t 0 , t 1 ) is the nth-order Taylor polynomial expansion of the evolution operator U (T ; t 0 ). Then the FTLE can be calculated from:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fluctuations of finite - time stability exponents in the standard mapping and the observation of tiny islands . Abstract : We research fluctuations of finite - time Lyapunov exponent ( FTLE ) for turbulent orbits on the standard mapping with periodic border conditions .We see that FTLE fluctuates around its average value , which is chosen by the greatest chaotic periodic orbit integrated into the chaotic attractor . The amplitude of these fluctuations decreases exponentially as time rises .In addition to this exponential decay we encounter an algebraic tail at large times . This algebraic tail can be described by the presence of tiny islands inside the chaotic sea .These conclusions are confirmed numerically using varying methods . I .INTRODUCTORY REMARK The concept of finite - time Lyapunovexponent ( FTLE ) , invented by Wolf et al 1 , has been widely applied recently 2 - 4 . It expresses how rapid nearby trajectories diverge or converge during some fixed period of time T .For instance , if one considers two adjacent points x 0 = x ( t 0 ) and y 0 = x ( t 1 ) , where t 0 < t 1 , then their separation after time T will be taken by : where λ max > 0 is the maximum Lyapunov exponent characterizing the frequency of divergence between neighboring trajectories 5 . In order to estimate the FTLE it is required to solve the following variational equation : where J is the Jacobian matrix corresponding to the flow generated by Eq .( 1 ) . If the first condition z 0 = x t0 + εy t0 is close enough to the reference trajectory x t0 , i . e . , | ε | [UNK] 1 , then the solve of Eq .( 2 ) can be written as : where P n ( T ; t 0 , t 1 ) is the nth - order Taylor polynomial expansion of the evolution function U ( T ; t 0 ) . Then the FTLE can be determined from :",
        "rewrite_text": "**Title:** Fluctuations of Finite-Time Stability Exponents in the Standard Mapping and the Observation of Tiny Islands\n\n**Abstract:** This study investigates the fluctuations of finite-time Lyapunov exponents (FTLE) associated with turbulent orbits in the context of the standard mapping under periodic boundary conditions. Our findings reveal that the FTLE exhibits fluctuations around a mean value, which is determined by the most chaotic periodic orbit embedded within the chaotic attractor. Notably, we observe that the amplitude of these fluctuations diminishes exponentially as time progresses. Alongside this exponential decay, we identify an algebraic tail that emerges at longer time scales. This algebraic behavior can be attributed to the existence of tiny islands within the chaotic sea, which serve as stabilizing structures amidst the chaos. The results of our analysis are supported by various numerical methods, confirming the robustness of our conclusions.\n\nThe finite-time Lyapunov exponent, a concept introduced by Wolf et al., has gained significant traction in recent research due to its ability to quantify the divergence or convergence of nearby trajectories over a specified time interval, T. For two closely situated points, x₀ = x(t₀) and y₀ = x(t₁), where t₀ < t₁, the separation after time T can be characterized by the maximum Lyapunov exponent, λ_max, which indicates the rate of divergence between these trajectories. To compute the FTLE, one must solve a variational equation involving the Jacobian matrix associated with the dynamical flow. If the initial condition z₀ = x(t₀) + εy(t₀) is sufficiently close to the reference trajectory, the solution can be expressed through a Taylor polynomial expansion of the evolution function. Ultimately, our work sheds light on the intricate dynamics of chaotic systems and the role of finite-time stability exponents in understanding the underlying structure of chaos.",
        "ori-fast-z-score": -2.3728949893812477,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": -0.3682298471593294
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vector mesons from AdS/TC to the LHC .\nAbstract:\nWe present an overview of our recent work on vector meson production in heavy ion collisions at RHIC and LHC energies, based on holographic QCD models with chiral symmetry breaking (AdS/QCD). We discuss how these models can be used to calculate hadronic observables such as transverse momentum spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear reactions. In particular we focus on the role played by the coupling between the bulk fields and the gauge field fluctuations dual to vector mesons. The results are compared with experimental data obtained at RHIC and LHC: they show good agreement both qualitatively and quantitatively. \n \n Keywords: Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality \n \n 1 Introduction \n \n One of the most exciting discoveries made recently at RHIC is that strongly interacting matter behaves like a nearly perfect fluid  1  . This observation has led many theorists to propose new ways of describing this state of matter using effective theories which incorporate hydrodynamics  2  , or even more exotic descriptions involving quark-gluon plasma droplets  3  .\n \nIn order to understand better what happens during the early stages of heavy-ion collisions it would be very useful if one could study experimentally the properties of the hot dense medium created in those collisions. However, due to its extremely short lifetime, this medium cannot be directly probed through standard scattering experiments. Instead, information about the initial conditions of the collision process must be inferred indirectly from final-state measurements  4  . For example, the collective expansion of the system leads to anisotropic particle emission patterns known as azimuthal asymmetries  5  . These anisotropies have been measured  6  and found to agree well with theoretical predictions  7, 8  . \n \n Another important observable characterizing the dynamics of the expanding fireball is the spectrum of emitted particles  9  . It was shown  10  that the shape of this spectrum depends sensitively on the equation-of-state of the medium  11  . Moreover, the observed suppression  12  of high-pT hadrons",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vector mesons from AdS / TC to the LHC . Abstract : We present an overview of our latest work on vector meson production in heavy ion collisions at RHIC and LHC energies , based on holographic QCD models with chiral symmetry breaking ( AdS / QCD ) .We discuss how these models can be used to estimate hadronic observables such as transverse energy spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear compounds . In particular we focus on the importance played by the interaction between the bulk fields and the gauge field fluctuations dual to vector mesons .The results are compared with observation information obtained at RHIC and LHC : they show good agreement both qualitatively and quantitatively . Keywords : Vector Mesons , Heavy Ion Collisions , Holography , Chiral Symmetry Breaking , Gauge / Gravity Duality 1 Introduction One of the most exciting developments conducted lately at RHIC is that strongly interacting material behaves like a nearly perfect fluid 1 .This observation has led many theorists to propose new ways of describing this state of matter utilizing effective models which employ hydrodynamics 2 , or especially more exotic representations featuring quark - gluon particle droplets 3 . In order to explain better what comes during the early stages of light - ion collisions it would be very useful if one might explore experimentally the properties of the hot dense medium produced in those collisions .However , owing to its incredibly small life , this medium never be directly probed through conventional absorption studies . Instead , info about the first environments of the collision mechanism must be inferred indirectly from final - state measurements 4 .For instance , the collective expansion of the system leads to anisotropic particle emission effects referred as azimuthal asymmetries 5 . These anisotropies have been measured 6 and found to agree well with theoretical estimates 7 , 8 .Another important observable characterizing the dynamics of the increasing fireball is the spectrum of emitted particles 9 . It was shown 10 that the form of this spectrum relies sensitively on the equation - of - state of the medium 11 .Moreover , the observed suppression 12 of high - pT hadrons",
        "rewrite_text": "**Title:** Vector Mesons from AdS/TC to the LHC\n\n**Abstract:** In this article, we provide a comprehensive overview of our recent research on the production of vector mesons in heavy ion collisions at both RHIC and LHC energies, utilizing holographic QCD models that incorporate chiral symmetry breaking (AdS/QCD). We explore the application of these models to estimate various hadronic observables, including transverse energy spectra and elliptic flow coefficients for light quarks and gluons generated in nuclear interactions. A key focus of our study is the significant role played by the interactions between bulk fields and gauge field fluctuations that are dual to vector mesons. Our findings are compared with experimental data from RHIC and LHC, revealing a strong agreement in both qualitative and quantitative aspects. \n\nThe introduction highlights a pivotal advancement observed at RHIC, where strongly interacting matter has been identified as behaving like a nearly perfect fluid. This discovery has prompted theorists to develop innovative models to describe this state of matter, employing effective approaches such as hydrodynamics and more unconventional representations involving quark-gluon droplets. To enhance our understanding of the early stages of light-ion collisions, it is crucial to experimentally investigate the properties of the hot, dense medium produced during these events. However, due to its extremely short lifespan, this medium cannot be directly examined through traditional absorption techniques. Instead, insights into the initial conditions of the collision must be derived from final-state measurements. \n\nOne observable that reflects the dynamics of the expanding fireball is the anisotropic emission of particles, known as azimuthal asymmetries, which have been measured and found to align well with theoretical predictions. Additionally, the spectrum of emitted particles serves as another critical observable, with its shape being highly sensitive to the equation of state of the medium. Furthermore, we discuss the observed suppression of high-pT hadrons, which provides further insights into the underlying dynamics of the system. \n\n**Keywords:** Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality.",
        "ori-fast-z-score": 0.3287979746107146,
        "water-fast-z-score": 7.405474974817551,
        "rewrite-fast-z-score": -0.41522739926869984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Swift observations of the 2006 outburst of the recurrent nova RS Ophiuchi: II. 1D hydrodynamical models of wind driven shocks .\nAbstract:\nWe present results of our numerical simulations of the interaction between the fast stellar winds and the slow dense shell ejected during previous eruption in the recurrent nova RS Oph (T Sco). We find that the observed X-ray light curve can be reproduced by assuming an initial mass loss rate of ~10-6 Msun/yr for the red giant component, which is consistent with theoretical predictions.  The predicted temperature structure of the shocked region agrees well with the observationally inferred one. Our model also predicts that the optical depth to X-rays should increase as time goes on because of the increasing density of the ejecta. This prediction seems to be supported by recent Swift/XRT observations. In addition we show that the observed UV fluxes are not explained by the standard steady state photoionization model but require additional heating source such as shocks or magnetic reconnection. Finally we discuss possible scenarios for future evolution of this system based on our numerical results.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Swift studies of the 2006 outburst of the recurrent nova RS Ophiuchi : II . 1D hydrodynamical models of wind driven shocks .Abstract : We present results of our numerical simulations of the interaction between the fast stellar winds and the slow dense shell ejected during earlier eruption in the recurrent nova RS Oph ( T Sco ) . We see that the observed X - ray light curve can be reproduced by assuming an initial mass loss rate of ~ 10 - 6 Msun / yr for the red dwarf component , which is compatible with theoretical estimates .The predicted temperature structure of the shocked areas agrees well with the observationally inferred one . Our model also predicts that the optical height to X - radiation should expand as time go on because of the increasing volume of the ejecta .This prediction seems to be supported by recent Swift / XRT studies . In addition we prove that the seen UV fluxes are not explained by the standard steady state photoionization theory but need extra heating source such as shocks or magnetic reconnection .Finally we explain possible strategies for future evolution of this scheme depending on our numerical findings .",
        "rewrite_text": "In this article, we present the findings from our numerical simulations that investigate the interaction between the rapid stellar winds and the dense shell expelled during previous eruptions of the recurrent nova RS Ophiuchi (T Sco). Our simulations reveal that the observed X-ray light curve can be effectively modeled by assuming an initial mass loss rate of approximately 10^-6 solar masses per year for the red dwarf companion, a value that aligns with theoretical predictions. Furthermore, the temperature distribution within the shocked regions predicted by our model closely matches the temperatures inferred from observational data. \n\nWe also predict that the ratio of optical height to X-ray radiation will increase over time, a consequence of the expanding volume of the ejecta. This prediction is supported by recent observations from the Swift/XRT mission, which provide additional validation for our model. Additionally, our analysis indicates that the ultraviolet fluxes observed cannot be adequately explained by the conventional steady-state photoionization theory. Instead, we suggest that these fluxes require an additional heating mechanism, such as shocks or magnetic reconnection, to account for the observed phenomena.\n\nFinally, we discuss potential strategies for future research based on our numerical findings, outlining how these insights could inform our understanding of the ongoing evolution of RS Ophiuchi and similar systems. Our work contributes to a deeper comprehension of the dynamics at play in recurrent novae and the complex interactions between stellar winds and ejected material.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": 0.5619514869490164
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Role of Dynamical Research in the Detection and Characterization of Exoplanets .\nAbstract:\nThe detection of exoplanets has been one of the most exciting recent developments in astronomy, but their characterization is still incomplete. The goal of this thesis was to develop new techniques for characterizing exoplanetary systems using dynamical methods. In particular, I studied how planets can be used as probes into the formation history of planetary systems by measuring the mass distribution of small bodies (planetesimals) that are trapped in mean motion resonances with them.  My first project investigated the effect of planetesimal size on the stability of planetary orbits. Using numerical simulations, we found that smaller planetesimals tend to destabilize planetary orbits more quickly than larger ones because they have higher orbital eccentricities. This result suggests that there may exist an upper limit to the size of planetesimals that form during planet migration. My second project examined the effects of mutual inclinations between planets on the stability of planetary systems. We showed that mutual inclination increases the likelihood of instability when two planets are locked in a 2:1 resonance. Finally, my third project explored the possibility of detecting terrestrial planets around white dwarfs through gravitational microlensing events.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Role of Dynamical Research in the Detection and Characterization of Exoplanets . Abstract : The observation of exoplanets has been one of the most exciting latest advances in science , but their characterization is already incomplete .The goal of this dissertation was to develop new method for characterizing exoplanetary regions using dynamical techniques . In particular , I studied how planets can be used as probes into the formation history of planetary structures by monitoring the mass distribution of tiny bodies ( planetesimals ) that are locked in mean motion resonances with them .My first project researched the impact of planetesimal size on the stability of planetary orbits . Using numerical simulations , we identified that smaller planetesimals prefer to destabilize planetary orbits more easily than larger ones because they have greater orbital eccentricities .This result suggests that there may exist an upper limitation to the extent of planetesimals that form during planet migration . My second work examined the effects of mutual inclinations between orbits on the stability of planetary networks .We showed that mutual inclination increases the probability of instability when two planets are locked in a 2 : 1 resonance . Finally , my third project researched the prospect of detecting terrestrial worlds around white dwarfs through gravity microlensing events .",
        "rewrite_text": "Title: The Role of Dynamical Research in the Detection and Characterization of Exoplanets\n\nAbstract: The exploration of exoplanets represents a remarkable advancement in contemporary science; however, our understanding of their characteristics remains incomplete. This dissertation aims to introduce innovative methodologies for characterizing exoplanetary systems through the application of dynamical techniques. Specifically, the research focuses on utilizing planets as indicators of the formation history of planetary systems by analyzing the mass distribution of small bodies, known as planetesimals, that are engaged in mean motion resonances with these planets. \n\nIn the first segment of the study, I investigated the influence of planetesimal size on the stability of planetary orbits. Through a series of numerical simulations, we discovered that smaller planetesimals tend to destabilize planetary orbits more readily than their larger counterparts, primarily due to their higher orbital eccentricities. This finding implies the existence of an upper limit on the size of planetesimals that can form during the migration of planets, thereby influencing the dynamics of planetary systems.\n\nThe second part of the research delved into the impact of mutual inclinations between planetary orbits on the stability of these systems. Our analysis revealed that increased mutual inclination significantly heightens the likelihood of instability, particularly when two planets are in a 2:1 resonance configuration. This insight is crucial for understanding the long-term evolution of planetary networks and their potential for sustaining stable orbits.\n\nLastly, the dissertation explored the potential for detecting terrestrial planets orbiting white dwarfs through the phenomenon of gravitational microlensing. This innovative approach could open new avenues for identifying and characterizing exoplanets in environments previously thought to be inhospitable. Overall, this work underscores the importance of dynamical research in enhancing our understanding of exoplanets and their formation, stability, and detectability.",
        "ori-fast-z-score": -0.6060915267313265,
        "water-fast-z-score": 5.527707983925667,
        "rewrite-fast-z-score": -0.3508232077228117
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal Occulter Design for Finding Extrasolar Planets .\nAbstract:\nWe present an optimal design method to find the best occulting mask that can be used in direct imaging searches for extrasolar planets.  The proposed method is based on the concept of entropy maximization, which has been widely applied in many fields such as information theory and statistical mechanics.   We show how this concept can be extended into the field of optics by introducing a new quantity called optical entropy (OE).   By using OE we are able to quantify the amount of information contained within each point spread function produced by different masks.   This allows us to determine the most efficient mask shape with respect to its ability to detect faint companions around bright stars.   ...  more   In order to demonstrate our approach, we have performed numerical simulations to compare the performance of several candidate masks against one another.   Our results indicate that the proposed method provides significant improvement over existing methods when it comes to finding the optimal mask shapes for detecting faint companions around bright host stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal Occulter Design for Finding Extrasolar Planets . Abstract : We present an appropriate design algorithm to find the best occulting mask that can be used in direct imaging searches for extrasolar planets .The proposed approach is based on the idea of entropy maximization , which has been widely applied in different fields such as data physics and statistical mechanics . We see how this concept can be applied into the field of optics by using a new quantity called optical entropy ( OE ) .By using OE we are able to quantify the quantity of information stored within each point spread function generated by various masks . This enables us to identify the most efficient mask shape with regard to its able to identify dim companions around bright stars .. . . more In order to test our approach , we have done mathematical simulations to study the performance of several candidate masks against one another . Our results show that the suggested method provides significant progress over existing techniques when it comes to finding the ideal mask shapes for detecting bright companions around bright host stars .",
        "rewrite_text": "In this article, we introduce a novel design algorithm aimed at optimizing occulting masks for direct imaging searches of extrasolar planets. Our methodology is grounded in the principle of entropy maximization, a concept that has found applications across various disciplines, including data physics and statistical mechanics. We extend this principle into the realm of optics through the introduction of a new metric termed optical entropy (OE). This innovative measure allows us to quantify the information content inherent in each point spread function produced by different occulting masks. By leveraging optical entropy, we can effectively determine the most efficient mask configurations for detecting faint companions in the vicinity of luminous stars.\n\nTo validate our proposed approach, we conducted a series of mathematical simulations to compare the performance of various candidate masks. The results of our simulations indicate that our method represents a significant advancement over traditional techniques in the quest for optimal mask shapes. Specifically, our findings demonstrate that the masks designed using our entropy-based framework are more adept at identifying dim celestial bodies orbiting bright host stars. This work not only enhances our understanding of occulting mask design but also contributes to the broader field of exoplanet detection, providing a robust tool for astronomers seeking to uncover the hidden worlds beyond our solar system. Through this research, we aim to facilitate more effective direct imaging strategies, ultimately leading to the discovery of new extrasolar planets and enriching our knowledge of planetary systems in the universe.",
        "ori-fast-z-score": -0.41702882811414954,
        "water-fast-z-score": 5.910624659963424,
        "rewrite-fast-z-score": 0.5388159060803247
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Common Origin of Linear and Nonlinear Chiral Multiplets in N=4 Mechanics .\nAbstract:\nWe show that the common origin of linear and nonlinear chiral multiplets is related to the existence of an extra dimension, which can be either time-like or space-like. We also discuss how this picture fits into the context of string theory. Theories with extended supersymmetry are known to have many interesting features such as duality symmetries  1  . In particular, theories with four supercharges (N = 4) possess remarkable properties  2  , including self-duality  3  .\nIn recent years there has been renewed interest in studying these theories due to their connection with string/M-theory  4  . For example, it was shown  5  that type-IIB strings on AdS 5 × S 5 background  6  correspond to maximally supersymmetric Yang-Mills theory in four dimensions  7, 8  . This correspondence allows one to study strongly coupled gauge theories using weakly-coupled gravitational description  9  . It turns out that the same idea works for other types of field theories  10  -  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Common Origin of Linear and Nonlinear Chiral Multiplets in N = 4 Mechanics . Abstract : We see that the common origin of linear and nonlinear chiral multiplets is related to the existence of an additional element , which can be either time - like or space - like .We also discuss how this picture fits into the context of string theory . Theories with stretched supersymmetry are known to have many interesting features such as duality symmetries 1 .In particular , theories with four supercharges ( N = 4 ) possess extraordinary properties 2 , notably self - duality 3 . In recent years there has been continued interest in investigating these theories owing to their connection with string / M - theory 4 .For instance , it was shown 5 that type - IIB sequences on AdS 5 × S 5 background 6 belong to maximally supersymmetric Yang - Mills theory in four dimensions 7 , 8 . This correspondence allows one to study weakly coupled gauge fields use strongly - coupled gravitational description 9 .It turns out that the same idea works for other types of field equations 10 - 12 .",
        "rewrite_text": "**Title:** The Common Origin of Linear and Nonlinear Chiral Multiplets in N = 4 Mechanics\n\n**Abstract:** In this article, we explore the fundamental relationship between linear and nonlinear chiral multiplets, highlighting their shared origin linked to the presence of an additional element that can manifest as either time-like or space-like. This investigation is situated within the broader framework of string theory, where theories exhibiting stretched supersymmetry reveal a wealth of intriguing characteristics, including duality symmetries. Specifically, theories characterized by four supercharges (N = 4) are noted for their remarkable attributes, particularly self-duality. Recent research has intensified interest in these theories due to their profound connections with string and M-theory. For example, it has been demonstrated that type-IIB string theories formulated on the AdS5 × S5 background are intrinsically related to maximally supersymmetric Yang-Mills theory in four dimensions. This correspondence facilitates the examination of weakly coupled gauge fields through the lens of a strongly coupled gravitational framework. Furthermore, our findings indicate that this conceptual approach is applicable to various other field equations, thereby enriching the understanding of the underlying principles governing these multiplet structures. The implications of our results extend beyond theoretical constructs, offering potential insights into the unification of different physical theories and enhancing the comprehension of supersymmetry in high-energy physics. Through this work, we aim to contribute to the ongoing discourse surrounding the intricate connections between supersymmetry, string theory, and the mathematical structures that underpin them.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 4.714045207910317,
        "rewrite-fast-z-score": 1.436739427831727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The dust distribution in edge-on galaxies. Radiative transfer fits of V and K -band images .\nAbstract:\nWe present radiative transfer models for the dust distributions in two edge-on spiral galaxies, NGC 891 and NGC 4565. The model parameters are constrained by fitting to near-infrared (NIR) observations at 2.2 microns with NICMOS on HST as well as optical data obtained with WFPC-2. We find that both galaxies have significant amounts of dust distributed along their disks out to large distances above the midplane. In addition we detect an extended halo component around each galaxy which is best described by a spherical shell-like structure. For NGC 891 this component has a scale height of 1 kpc and extends up to 5 kpc above the disk plane. It contains about 10% of all dust mass within 10 kpc distance from the center. For NGC 4565 our results suggest a more complex geometry where the dust density decreases exponentially towards larger heights. This work was supported by NASA grant NAG5-7085.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The dust flow in edge - on galaxies . Radiative transfer fits of V and K - band images .Abstract : We create radiative transfer estimates for the dust distributions in two edge - on spiral galaxies , NGC 891 and NGC 4565 . The model variables are constrained by fitting to near - infrared ( NIR ) observations at 2 . 2 microns with NICMOS on HST as well as imaging information obtained with WFPC - 2 .We see that both galaxies have considerable amounts of dust spread along their disks out to large distances above the midplane . In addition we locate an extended halo element around each galaxy which is better represented by a spherical shell - like structure .For NGC 891 this component has a scale width of 1 kpc and extends up to 5 kpc above the disk plane . It contains about 10 % of all dust mass within 10 kpc length from the center .For NGC 4565 our results suggest a more complex geometry where the dust density decreases exponentially towards larger heights . This work was supported by NASA grant NAG5 - 7085 .",
        "rewrite_text": "In this study, we present radiative transfer models to estimate the dust distributions in two edge-on spiral galaxies, NGC 891 and NGC 4565. Our analysis is based on fitting these models to near-infrared (NIR) observations at a wavelength of 2.2 microns, utilizing data from the Near Infrared Camera and Multi-Object Spectrometer (NICMOS) aboard the Hubble Space Telescope (HST), alongside imaging from the Wide Field Planetary Camera 2 (WFPC-2). Our findings reveal that both galaxies possess significant amounts of dust distributed along their disks, extending to considerable distances above the midplane. \n\nFurthermore, we identify an extended halo component surrounding each galaxy, which is more accurately described by a spherical shell-like structure. For NGC 891, this halo has a scale height of approximately 1 kpc and reaches up to 5 kpc above the disk plane, accounting for about 10% of the total dust mass within a 10 kpc radius from the galactic center. In contrast, our analysis of NGC 4565 indicates a more intricate dust distribution, characterized by an exponential decline in dust density with increasing height from the midplane. \n\nThese results enhance our understanding of dust dynamics in edge-on galaxies and contribute to the broader field of galactic structure and evolution. The research was conducted with the support of NASA grant NAG5-7085, underscoring the importance of funding in advancing astronomical studies. Overall, our work provides valuable insights into the role of dust in shaping the properties of spiral galaxies and sets the stage for future investigations into the complex interactions between dust, stars, and the interstellar medium.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 3.670651741928988,
        "rewrite-fast-z-score": 1.7320508075688772
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nanomechanics of single keratin fibres: A Raman study of the alpha helix -> beta sheet transition and water effect .\nAbstract:\nThe mechanical properties of hair are determined by its molecular structure, which is mainly composed of proteins called keratins. The main structural unit in keratins is an alpha-helical coiled coil that forms microfibrils with other similar units to form intermediate filaments (IFs). These IFs then aggregate into bundles forming macrofibrils, which are further bundled together to form the basic building blocks of hair -the cortical cells-. In this work we have studied how the mechanical properties change as these structures evolve during the process known as maturation or ageing. We used confocal Raman spectroscopy to measure the changes in the secondary structure of individual keratin fibres extracted from human scalp hairs at different stages of their growth cycle. Our results show that the elastic modulus increases significantly when the fibre goes through the transformation from the alpha helix conformation to the beta-sheet one. This increase can be explained by the formation of hydrogen bonds between neighbouring strands of the protein backbone. Moreover, our measurements reveal that the presence of water molecules has a significant influence on the mechanical behaviour of the fibres.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nanomechanics of single keratin fibres : A Raman study of the alpha helix - > beta sheet process and moisture effect . Abstract : The mechanical behavior of hair are decided by its molecular structure , which is mainly composed of proteins called keratins .The main structural unit in keratins is an alpha - helical coiled coil that forms microfibrils with other similar units to form intermediate filaments ( IFs ) . These IFs then aggregate into bundles forming macrofibrils , which are further bundled together to form the fundamental construction elements of hair - the cortical cells - .In this research we have researched how the mechanical properties vary as these structures evolve during the process known as maturation or ageing . We utilized confocal Raman spectroscopy to measure the changes in the secondary structure of individual keratin fibres collected from human scalp hairs at different stages of their development process .Our results show that the elastic modulus increases substantially when the polymer goes through the transformation from the alpha helix conformation to the beta - sheet one . This increase can be described by the formation of hydrogen bonds between neighbouring segments of the protein backbone .Moreover , our measurements reveal that the presence of water molecules has a considerable impact on the structural behaviour of the fibres .",
        "rewrite_text": "**Title:** Nanomechanics of Single Keratin Fibres: A Raman Study of the Alpha Helix to Beta Sheet Transition and the Influence of Moisture\n\n**Abstract:** The mechanical properties of hair are fundamentally determined by its molecular architecture, predominantly composed of keratin proteins. The primary structural component of keratins is the alpha-helical coiled coil, which aggregates with similar units to form microfibrils, subsequently organizing into intermediate filaments (IFs). These IFs further cluster into bundles, creating macrofibrils, which ultimately constitute the essential building blocks of hair—the cortical cells. This study investigates the variations in mechanical properties as these structural components undergo maturation or aging. We employed confocal Raman spectroscopy to analyze the alterations in the secondary structure of individual keratin fibres sourced from human scalp hair at various developmental stages. Our findings indicate a significant increase in the elastic modulus as the keratin transitions from the alpha helix configuration to the beta-sheet structure. This enhancement in mechanical strength can be attributed to the formation of hydrogen bonds between adjacent segments of the protein backbone. Additionally, our data demonstrate that the presence of water molecules plays a crucial role in influencing the structural behavior of the keratin fibres. The results underscore the intricate relationship between the molecular structure of keratin, its mechanical properties, and environmental factors such as moisture, providing valuable insights into the nanomechanics of hair and its response to aging and hydration. This research contributes to a deeper understanding of hair's mechanical characteristics and may inform future studies on hair health and treatment.",
        "ori-fast-z-score": 0.7107423155935334,
        "water-fast-z-score": 5.125692857821981,
        "rewrite-fast-z-score": 1.4110813025753959
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tidal dwarf galaxies as a test of fundamental physics .\nAbstract:\nWe present the results of N-body simulations aimed at studying tidal disruption and accretion in interacting galaxy pairs, with particular emphasis on the formation of tidally stripped dwarfs (TDGs). We find that TDG formation is strongly dependent upon the orbital parameters of the interaction; specifically, we show that TDGs form only when the encounter has an impact parameter less than about twice the sum of their effective radii.  In addition to this dependence on orbital geometry, our models suggest that TDGs are more likely to be formed if the progenitor galaxies have high gas fractions and/or low central surface brightnesses. Finally, we argue that TDGs may provide useful probes for testing theories of gravity on galactic scales. The discovery of numerous examples of  tidal dwarf galaxies  (TDGs) over the past decade or so has led many authors to propose these objects as possible sites of star formation during interactions between massive spiral galaxies. However, despite considerable observational effort, there remains no consensus regarding either the frequency of TDG formation or even whether such systems actually exist outside the confines of numerical simulations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tidal dwarf galaxies as a test of fundamental physics . Abstract : We present the conclusion of N - bodies simulations aiming at studying tidal disruption and accretion in correlated galaxy pairs , with particular focuses on the formation of tidally stripped dwarfs ( TDGs ) .We see that TDG formation is strongly dependent upon the orbital characteristics of the interaction ; specifically , we find that TDGs form only when the encounter has an impact parameter less than about twice the sum of their effective radii . In addition to this dependence on orbital geometry , our calculations suggest that TDGs are more likely to be formed if the progenitor galaxies have greater gas fractions and / or low central exterior brightnesses .Finally , we argue that TDGs might represent helpful probes for studying concepts of gravitational on galactic scales . The observation of several examples of tidal dwarf stars ( TDGs ) over the previous decade or so has led many writers to propose these objects as possible areas of galaxy formation during interactions between massive spiral nuclei .However , despite considerable observational effort , there exists no discussion regarding either the frequency of TDG formation or even whether such systems actually appear outside the confines of computational simulations .",
        "rewrite_text": "In this article, we present the findings from our N-body simulations aimed at investigating tidal disruption and accretion processes in interacting galaxy pairs, with a particular emphasis on the formation of tidal dwarf galaxies (TDGs). Our research reveals that the formation of TDGs is significantly influenced by the orbital dynamics of the interacting galaxies. Specifically, we determine that TDGs are likely to form only when the impact parameter of the encounter is less than approximately twice the combined effective radii of the galaxies involved. This critical threshold underscores the importance of orbital geometry in the tidal interactions that lead to TDG formation.\n\nMoreover, our simulations indicate that the likelihood of TDG formation increases when the progenitor galaxies possess higher gas fractions and/or exhibit lower central exterior brightness. These factors suggest that the initial conditions of the interacting galaxies play a crucial role in the subsequent development of TDGs. \n\nWe propose that TDGs could serve as valuable probes for examining fundamental gravitational concepts on galactic scales. The past decade has seen a growing number of observations of TDGs, prompting researchers to speculate about their role in galaxy formation during interactions between massive spiral galaxies. However, despite extensive observational studies, there remains a lack of comprehensive discussion regarding the frequency of TDG formation and whether these systems are observed outside the realm of computational models. Our findings aim to bridge this gap by providing insights into the conditions necessary for TDG formation and their potential significance in understanding the dynamics of galaxy interactions.",
        "ori-fast-z-score": -1.0101525445522108,
        "water-fast-z-score": 5.671711984196938,
        "rewrite-fast-z-score": -0.3563483225498992
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Redesigning Computer-based Learning Environments: Evaluation as Communication .\nAbstract:\nThe purpose of this study is to examine the role that evaluation plays in communication between teachers and students during computer-based learning environments (CBLE). The research question guiding this investigation is: How does evaluation affect student-teacher interaction? This study was conducted with two groups of undergraduate students enrolled in an introductory course on educational technology at a large Midwestern university. Students were asked to complete three tasks using a CBLE called WebQuests, which are designed for use by students working individually or collaboratively. Data collection included audio recordings of group discussions, field notes taken by researchers observing each group s work, and students  written responses to questions posed throughout their participation in the project. Analysis revealed that evaluation played several roles within these interactions including providing feedback about individual performance; clarifying expectations; establishing ground rules; and promoting reflection. These findings suggest that evaluation can be used effectively to enhance student-teacher communication when it occurs frequently enough over time so that both parties have multiple opportunities to respond to one another.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Redesigning Computer - based Learning Environments : Evaluation as Communication . Abstract : The purpose of this study is to examine the part that assessment plays in interaction between instructors and students during computer - based educational environments ( CBLE ) .The research question guiding this study is : How does evaluation affect student - teacher activity ? This study was done with two groups of undergraduate people studied in an introductory program on teaching technology at a large Midwestern college .Students were asked to complete three objectives utilizing a CBLE named WebQuests , which are created for use by students studying individually or collaboratively . Data collection included audio performances of group discussions , field notes taken by researchers observing each organization s project , and teachers authored reactions to questions posed throughout their involvement in the project .Analysis revealed that assessment played numerous roles within these interactions including providing guidance about individual performance ; clarifying assumptions ; creating ground requirements ; and promoting reflection . These studies propose that assessment can be used effectively to enhance teacher - teacher understanding when it appears frequently enough over time so that both parties have multiple options to respond to one another .",
        "rewrite_text": "**Title:** Redesigning Computer-Based Learning Environments: Evaluation as Communication\n\n**Abstract:** This study investigates the role of assessment in facilitating interactions between instructors and students within computer-based learning environments (CBLEs). The central research question guiding this inquiry is: How does evaluation influence the dynamics of student-teacher engagement? The research was conducted with two groups of undergraduate students enrolled in an introductory teaching technology program at a large Midwestern university. Participants were tasked with achieving three specific objectives using a CBLE known as WebQuests, designed for both individual and collaborative learning experiences. \n\nData collection methods included audio recordings of group discussions, field notes taken by researchers observing the collaborative projects, and written responses from instructors to questions raised during the project. The analysis of the collected data revealed that assessment serves multiple functions in these interactions. It provides guidance on individual performance, clarifies underlying assumptions, establishes foundational requirements, and encourages reflective practices among participants. \n\nThe findings suggest that assessment can significantly enhance the understanding between teachers and students when it is implemented frequently and consistently over time. This ongoing evaluative process allows both parties to engage in meaningful dialogue, fostering a richer educational experience. Ultimately, the study highlights the potential of assessment not merely as a tool for measuring performance but as a vital component of communication that can strengthen the educational relationship in computer-based learning settings.",
        "ori-fast-z-score": -0.7770286898858113,
        "water-fast-z-score": 7.246315678266502,
        "rewrite-fast-z-score": -0.6211495565912797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observation of quantum-measurement backaction with an ultracold atomic gas .\nAbstract:\nWe report the observation of measurement-induced dephasing in a Bose-Einstein condensate (BEC) using Ramsey interferometry and atom counting.  The experiment is performed by splitting a single trapped BEC into two spatially separated clouds, which are allowed to evolve for different times before being recombined on a beam splitter. We observe that the visibility of interference fringes decreases as we increase the number of atoms counted at one output port of the beam splitter. This effect can be explained by considering how repeated measurements affect the phase evolution of the system. Our results demonstrate that it is possible to use cold-atom experiments to study fundamental questions about quantum mechanics. Quantum mechanics predicts that any attempt to measure a physical quantity will disturb its value. In this work, we experimentally investigate such effects in a Bose-Einsteint Condensate (BEC). To do so, we perform Ramsey interferometry between two spatially separated regions of our sample. By varying the time spent evolving freely after splitting off part of the initial cloud, we control the relative phase accumulated during free evolution. After recombination, we count the number of atoms arriving at each output port of the beam-splitter and record their arrival-time distribution. As expected, we find that the visibility of the resulting interference pattern decreases when increasing the number of detected particles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observation of quantum - measurement backaction with an ultracold atomic gas . Abstract : We report the observation of measurement - triggered dephasing in a Bose - Einstein condensate ( BEC ) using Ramsey interferometry and electron tracking .The experiment is conducted by breaking a single trapped BEC into two spatially separated clouds , which are allowed to evolve for different times before being recombined on a beam splitter . We see that the visibility of interference fringes falls as we increase the number of atoms counted at one output port of the laser splitter .This phenomenon can be described by examining how repetitive measurements alter the phase evolution of the system . Our results show that it is possible to use cold - atom experiments to study profound concerns about quantum mechanics .Quantum theory predicts that any attempt to measure a physical quantity will interfere its value . In this research , we experimentally examine such consequences in a Bose - Einsteint Condensate ( BEC ) .To do so , we perform Ramsey interferometry between two spatially split areas of our sample . By varying the period spending emerging freely after splitting off part of the first cloud , we control the relative phase accumulated during free evolution .After recombination , we count the quantity of atoms arriving at each output port of the beam - splitter and record their arrival - time distribution . As expected , we find that the visibility of the resulting interference pattern decreases when increasing the proportion of identified atoms .",
        "rewrite_text": "In this study, we present our findings on the measurement-induced dephasing observed in a Bose-Einstein condensate (BEC) through the application of Ramsey interferometry and electron tracking techniques. The experimental setup involves the division of a single trapped BEC into two distinct spatially separated clouds, which are allowed to evolve independently for varying durations before being recombined at a beam splitter. Our observations reveal a notable decrease in the visibility of the interference fringes as the number of atoms detected at one output port of the beam splitter increases. This behavior can be interpreted through the lens of quantum mechanics, specifically by analyzing how repeated measurements influence the phase evolution of the quantum system.\n\nThe implications of our results extend to fundamental questions in quantum mechanics, particularly the notion that any measurement of a physical quantity inherently disturbs its value. In our experiment, we systematically investigate these effects within the context of a BEC. By employing Ramsey interferometry, we manipulate the relative phase accumulated during the free evolution of the two spatially separated clouds. After the recombination process, we meticulously count the number of atoms detected at each output port of the beam splitter and analyze their arrival-time distributions.\n\nOur findings align with theoretical predictions, demonstrating that the visibility of the interference pattern diminishes as the fraction of detected atoms increases. This research not only reinforces the principles of quantum measurement but also showcases the potential of cold-atom experiments to explore and elucidate complex quantum phenomena. Through our work, we contribute to a deeper understanding of the interplay between measurement and quantum state evolution, highlighting the intricate nature of quantum mechanics as it applies to ultracold atomic systems.",
        "ori-fast-z-score": 0.3746343246326776,
        "water-fast-z-score": 5.874780291913977,
        "rewrite-fast-z-score": 1.7541160386140586
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Black-body components in Gamma-Ray Bursts spectra? .\nAbstract:\nWe present the results of our analysis on the time-averaged spectrum and light curve of GRB 090902B, which is one of the brightest bursts ever detected by Fermi/GBM (Gamma-ray Burst Monitor). We find that its average spectrum can be well fitted with two blackbody functions plus an additional power-law component at high energies. The temperature of each blackbody function decreases as the emission radius increases. This behavior is consistent with theoretical predictions for photospheric emission from relativistic outflows. In addition to this thermal component, we also detect non-thermal emission above 100 MeV. Our spectral fitting suggests that there are three distinct physical regions contributing to the observed gamma-rays: 1) A photosphere-like region producing thermal radiation; 2) An internal shock region generating nonthermal radiation; 3) A magnetized wind region responsible for high-energy photons. These findings provide new insights into the physics of GRBs. \n \n Keywords: Black body, Photon index, Power law",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Black - bodies components in Gamma - Ray Bursts spectra ? .Abstract : We present the conclusion of our analysis on the period - averaged spectrum and light curve of GRB 090902B , which is one of the brightest bursts ever observed by Fermi / GBM ( Gamma - ray Burst Monitor ) . We see that its average spectrum can be well fitted with two blackbody functions plus an additional power - law component at high energies .The temperature of each blackbody function decreases as the emission diameter increases . This phenomenon is compatible with theoretical expectations for photospheric emission from relativistic outflows .In addition to this heat element , we also observe non - thermal emitted above 100 MeV . Our spectral fit suggests that there are three different physical regions contributing to the seen γ - radiation : 1 ) A photosphere - like zone creating thermal rays ; 2 ) An inner blast area producing nonthermal emission ; 3 ) A magnetized wind zone responsible for high - energy photons .These studies provide fresh insights into the physics of GRBs . Keywords : Black body , Photon index , Power law",
        "rewrite_text": "We present the findings from our comprehensive analysis of the period-averaged spectrum and light curve of GRB 090902B, one of the most luminous gamma-ray bursts recorded by the Fermi Gamma-ray Burst Monitor (GBM). Our investigation reveals that the average spectrum of this burst can be accurately modeled using two blackbody functions, complemented by an additional power-law component that accounts for the high-energy emissions. Notably, we observe that the temperatures associated with each blackbody component decrease as the emission diameter expands, a behavior that aligns with theoretical predictions regarding photospheric emissions from relativistic outflows. \n\nIn addition to the thermal emissions, we detect non-thermal radiation extending beyond 100 MeV. Our spectral fitting indicates the presence of three distinct physical regions contributing to the observed gamma radiation: (1) a photosphere-like region responsible for generating thermal emissions; (2) an inner blast zone that produces non-thermal radiation; and (3) a magnetized wind region that accounts for the high-energy photon emissions. \n\nThese findings offer new perspectives on the underlying physics of gamma-ray bursts, enhancing our understanding of their complex emission mechanisms. The implications of our results are significant, as they suggest a multi-faceted structure within GRB environments that influences the characteristics of the emitted radiation. This study not only contributes to the existing body of knowledge on gamma-ray bursts but also opens avenues for further research into the dynamics of relativistic outflows and their interactions with surrounding media. \n\nKeywords: Blackbody, Photon index, Power law.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 5.666666666666667,
        "rewrite-fast-z-score": -0.09090909090909091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Statistics of the Points Where Nodal Lines Intersect a Reference Curve .\nAbstract:\nWe consider the problem of finding the statistics of nodal lines in random wave fields, where we assume that the field is generated by an ensemble of monochromatic waves with randomly distributed phases and amplitudes. We show how to calculate these statistics using only the first two moments of the distribution function for the phase differences between pairs of waves. The results are applied to study the statistical properties of nodal lines on the surface of oceanic internal tides. In particular, it is shown that the probability density functions (PDFs) of the number of intersections per unit length of a reference curve depend strongly on whether or not this curve lies within one wavelength of the source region. If so, then the PDF has a power-law tail at large values of the intersection density; if not, then there is no such tail. \n \n Keywords: Random wave fields, Oceanic internal tide, Nodal line, Phase difference",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Statistics of the Points Where Nodal Lines Intersect a Reference Curve . Abstract : We consider the question of finding the statistics of nodal lines in random wave fields , where we suppose that the field is generated by an ensemble of monochromatic waves with randomly distributed phases and amplitudes .We see how to estimate these statistics utilizing only the first two moments of the distribution function for the phase differences between pairs of waves . The results are applied to study the statistical characteristics of nodal lines on the surface of oceanic internal tides .In particular , it is demonstrated that the probability density functions ( PDFs ) of the number of intersections per unit span of a reference curve rely highly on whether or not this curve lies within one frequency of the origin region . If so , then the PDF has a power - law tail at large values of the intersection density ; if not , then there is no such tail .Keywords : Random wave fields , Oceanic internal wave , Nodal line , Phase difference",
        "rewrite_text": "Title: The Statistics of Points Where Nodal Lines Intersect a Reference Curve\n\nAbstract: This study investigates the statistical properties of nodal lines within random wave fields, specifically focusing on fields generated by an ensemble of monochromatic waves characterized by randomly distributed phases and amplitudes. We develop a methodology to estimate these statistical properties by utilizing only the first two moments of the distribution function that describes the phase differences between pairs of waves. Our findings are particularly relevant for analyzing the statistical behavior of nodal lines on the surfaces of oceanic internal tides. We demonstrate that the probability density functions (PDFs) for the number of intersections per unit length of a reference curve are significantly influenced by the positioning of the curve in relation to the origin region of the frequency spectrum. When the reference curve is situated within one frequency of the origin, the PDF exhibits a power-law tail at higher values of intersection density. Conversely, if the curve is located outside this frequency range, such a tail is absent. This distinction highlights the critical role that frequency localization plays in the statistical characteristics of nodal lines, providing insights that could enhance our understanding of wave dynamics in oceanic environments. Our results contribute to the broader field of random wave theory and have implications for the study of internal wave phenomena in oceanography. \n\nKeywords: Random wave fields, Oceanic internal waves, Nodal lines, Phase differences.",
        "ori-fast-z-score": 1.2701705922171767,
        "water-fast-z-score": 4.588314677411235,
        "rewrite-fast-z-score": 2.0426487199475707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The CoRoT primary target HD 52265: models and seismic tests .\nAbstract:\nWe present new theoretical evolutionary tracks for the mass range 1.8 to 2.5 M⊙, which are based on an improved treatment of convection in stellar interiors. We use these tracks as input into our seismic modelling code CESAM2k to compute synthetic seismograms for two different sets of observed frequencies (from COROT) corresponding to two possible values of the inclination angle i = 90° or 60° . The comparison between observations and theory shows that we can exclude one set of frequencies at high confidence level but not the other. This is due to the fact that the frequency differences between ℓ=0 and ℓ=2 modes depend strongly on the inclination angle. In addition, we find that the best fit model has a radius R=1.95R⊙ , which agrees well with the value inferred by asteroseismology using only ℓ=0 modes. Finally, we show how this result could be used to determine the age of the star.  Keywords: Seismic modelling",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The CoRoT primary target HD 52265 : models and seismic studies . Abstract : We present new theoretical evolutionary tracks for the mass range 1 . 8 to 2 . 5 [UNK] , which are based on an improved treatment of convection in stellar interiors .We use these tracks as input into our seismic modelling code CESAM2k to compute natural seismograms for two different sets of measured signals ( from COROT ) corresponding to two possible values of the inclination angle i = 90° or 60° . The comparison between observations and theory demonstrates that we can eliminate one group of frequencies at high confidence rate but not the other .This is due to the fact that the frequency differences between ℓ = 0 and ℓ = 2 modes differ highly on the inclination angle . In addition , we find that the best fit description has a diameter R = 1 . [UNK] , which agrees well with the value inferred by asteroseismology using only ℓ = 0 modes .Finally , we show how this effect could be used to predict the age of the star . Keywords : Seismic modelling",
        "rewrite_text": "In this study, we introduce new theoretical evolutionary tracks for stars with masses ranging from 1.8 to 2.5 solar masses, developed through an enhanced approach to convection within stellar interiors. These evolutionary tracks serve as a foundation for our seismic modeling, implemented using the CESAM2k code, which allows us to generate natural seismograms based on two distinct sets of observed signals obtained from the CoRoT satellite. These signals correspond to two potential inclination angles, specifically i = 90° and i = 60°. Our analysis reveals a significant disparity in the frequency differences between the ℓ = 0 and ℓ = 2 modes, which is heavily influenced by the inclination angle. This observation enables us to confidently rule out one set of frequencies while retaining the other, thereby refining our understanding of the star's seismic characteristics. Furthermore, we determine that the optimal fit for the star's radius is R = 1. [UNK], a value that aligns closely with the radius inferred through asteroseismic analysis that utilized only the ℓ = 0 modes. This consistency underscores the reliability of our models. Additionally, we discuss the implications of our findings for estimating the age of the star, suggesting that the observed seismic properties can be leveraged to provide insights into its evolutionary timeline. Overall, this research contributes to the field of stellar astrophysics by enhancing our understanding of stellar evolution and the application of seismic modeling techniques. \n\nKeywords: Seismic modeling, stellar evolution, asteroseismology, CoRoT, inclination angle.",
        "ori-fast-z-score": 0.9058216273156765,
        "water-fast-z-score": 4.387842813611494,
        "rewrite-fast-z-score": -1.8542101386022132
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Transient X-ray Emission from Normal Galactic Nuclei .\nAbstract:\nWe report on the detection of transient X-ray emission in normal galactic nuclei (NGNs) with Chandra and XMM-Newton observatories, which is likely to be associated with accretion onto supermassive black holes at their centers. The observed luminosities are consistent with those expected for steady nuclear activity powered by mass inflow through an optically thick disk around the central black hole. We find that the duration of this activity ranges between 10 3 -10 5 years, depending on the distance of NGN from Earth. This suggests that the majority of NGNs may have experienced such active phases during their lifetimes. Our results also imply that the current quiescent state of most NGNs could be due to either low-level accretion or obscuration effects. These findings provide new insights into the formation and evolution of massive galaxies as well as AGNs. Keywords: Black Hole Accretion Disk - Galaxy Evolution - Nuclear Activity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Transient X - ray Emission from Normal Galactic Nuclei . Abstract : We report on the observation of transient X - ray radiation in regular galactic nuclei ( NGNs ) with Chandra and XMM - Newton observatories , which is expected to be involved with accretion onto supermassive black holes at their centers .The observed luminosities are compatible with those expected for steady nuclear activity driven by mass inflow through an optically dense disk around the main dark hole . We see that the duration of this action ranges between 10 3 - 10 5 years , depending on the distance of NGN from Earth .This implies that the majority of NGNs might have experienced such active phases during their lifetimes . Our results also suggest that the present quiescent state of most NGNs might be due to either small - grade accretion or obscuration effects .These studies provide fresh insights into the formation and evolution of large galaxies as well as AGNs . Keywords : Black Hole Accretion Disk - Galaxy Evolution - Nuclear Activity",
        "rewrite_text": "In this study, we present findings on the transient X-ray emissions observed in normal galactic nuclei (NGNs) using data from the Chandra and XMM-Newton observatories. These emissions are believed to be linked to the process of accretion occurring around supermassive black holes located at the centers of these galaxies. Our analysis reveals that the luminosities detected align with those anticipated for consistent nuclear activity, which is driven by mass inflow through an optically thick accretion disk surrounding the central black hole. The duration of these active phases varies significantly, ranging from approximately 1,000 to 100,000 years, contingent upon the distance of the NGN from Earth. This variability suggests that a substantial number of NGNs have likely undergone such active periods throughout their evolutionary history. Furthermore, our findings indicate that the current quiescent states observed in most NGNs may result from either low-level accretion processes or obscuration effects that hinder visibility. These observations contribute valuable insights into the mechanisms underlying the formation and evolution of large galaxies, as well as the behavior of active galactic nuclei (AGNs). The implications of our research extend to understanding the dynamic processes that govern galaxy evolution and the role of supermassive black holes in shaping the characteristics of their host galaxies. Overall, this study enhances our comprehension of the intricate relationship between black hole activity and galactic development, providing a foundation for future investigations in this field. \n\nKeywords: Black Hole Accretion Disk, Galaxy Evolution, Nuclear Activity.",
        "ori-fast-z-score": -1.25,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 1.436739427831727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Wightman function and vacuum densities for a Z_2-symmetric thick brane in AdS spacetime .\nAbstract:\nWe study the Wightman functions and vacuum densities on a Z_2-symmetric, thick brane embedded in an anti-de Sitter (AdS) space-time with one extra dimension. We find that there are two types of solutions to the corresponding equations depending on whether or not the bulk mass is zero. In both cases we show how these quantities can be expressed as sums over modified Bessel functions. The results obtained here may have applications in quantum field theory at finite temperature and/or density. PACS: 11.10.Kk, 12.20.Ds, 98.80.Cq Keywords: Vacuum expectation value, Anti-de Sitter space time, Thick brane, Modified Bessel function. 1 Introduction An interesting feature of string theories is their ability to incorporate gravity into the fundamental description of nature. This has led to renewed interest in studying gravitational backgrounds which admit supersymmetry  1  . One such class of spacetimes is given by the so-called warped product spaces  2  , where the metric takes the form ds2 = e2A(y)(ημνdxμ dxν + dy 2 ),\n(1)\nwhere y denotes the coordinate along the extra dimension, A(y) is called the warp factor and ημν is the Minkowski metric. For example, if we consider the five-dimensional case then this corresponds to the Randall-Sundrum model  3  .\nIn recent years it was shown  4  -  8  that the presence of a nontrivial warp factor leads to new features in the physics associated with fields propagating in the bulk. These include modifications to the standard dispersion relations  9  , spontaneous symmetry breaking  10  , fermion localization  11  , etc.. It turns out  12  that the effects due to the warp factor depend crucially upon its behaviour near the boundary of the extra dimension. If the warp factor vanishes sufficiently rapidly at infinity then all physical observables will be identical to those computed using ordinary flat-space techniques. However, if the warp factor does not vanish fast enough then some novel phenomena occur.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Wightman function and vacuum densities for a Z _ 2 - symmetric thick brane in AdS spacetime . Abstract : We research the Wightman functions and vacuum densities on a Z _ 2 - symmetric , thick brane embedded in an anti - de Sitter ( AdS ) space - time with one extra dimension .We see that there are two forms of solutions to the corresponding equations depending on whether or not the bulk weight is zero . In both cases we give how these quantities can be shown as sums over modified Bessel functions .The results derived here may have applications in quantum field theory at finite cooling and / or density . PACS : 11 . 10 . Kk , 12 . 20 . Ds , 98 . 80 . Cq Keywords : Vacuum expectation point , Anti - de Sitter space time , Thick brane , Modified Bessel function .1 Introduction An interesting feature of string theories is their potential to insert gravitational into the fundamental description of nature . This has led to renewed emphasis in investigating gravitational backgrounds which admit supersymmetry 1 .One such family of spacetimes is given by the so - called warped product spaces 2 , where the metric takes the form ds2 = e2A ( y ) ( ημνdxμ dxν + dy 2 ) , ( 1 ) where y denotes the coordinate along the extra dimension , A ( y ) is dubbed the warp factor and ημν is the Minkowski metric . For instance , if we treat the five - dimensional case then this corresponds to the Randall - Sundrum model 3 .In later years it was shown 4 - 8 that the presence of a nontrivial warp factor leads to novel features in the physics associated with fields propagating in the bulk . These include changes to the standard dispersion relations 9 , spontaneous symmetry breaking 10 , fermion localization 11 , etc . .It turns out 12 that the effects due to the warp factor rely crucially upon its behaviour near the boundary of the extra dimension . If the warp factor vanishes sufficiently quickly at infinity then all physical observables will be identical to those computed using ordinary flat - space methods .However , if the warp factor does not vanish fast enough then some interesting phenomena arise .",
        "rewrite_text": "In this study, we investigate the Wightman functions and vacuum densities associated with a Z_2-symmetric thick brane situated within an anti-de Sitter (AdS) spacetime that includes an additional spatial dimension. Our analysis reveals two distinct types of solutions to the governing equations, which are contingent upon whether the bulk weight is zero or not. In both scenarios, we demonstrate that these quantities can be expressed as sums involving modified Bessel functions. The findings presented in this paper may have significant implications for quantum field theory, particularly in contexts involving finite temperature and/or density. \n\nThe introduction of gravitational effects into fundamental physics, particularly through string theories, has sparked renewed interest in exploring gravitational backgrounds that exhibit supersymmetry. A notable class of such spacetimes is represented by warped product spaces, characterized by a metric of the form ds² = e²A(y)(ημνdxμdxν + dy²), where y denotes the coordinate along the extra dimension, A(y) is referred to as the warp factor, and ημν represents the Minkowski metric. In the context of five-dimensional theories, this framework aligns with the Randall-Sundrum model. \n\nRecent studies have established that a nontrivial warp factor introduces unique features in the dynamics of fields propagating through the bulk, leading to alterations in standard dispersion relations, phenomena of spontaneous symmetry breaking, and the localization of fermions. The impact of the warp factor is particularly sensitive to its behavior near the boundaries of the extra dimension. If the warp factor diminishes rapidly at infinity, physical observables align closely with those derived from conventional flat-space methodologies. Conversely, if the warp factor does not decrease sufficiently, intriguing phenomena emerge, warranting further exploration. \n\nThis research contributes to the understanding of how the structure of spacetime influences quantum field dynamics, particularly in the context of thick branes within AdS environments.",
        "ori-fast-z-score": 0.5183210553488161,
        "water-fast-z-score": 5.487954724560282,
        "rewrite-fast-z-score": 1.1188618555710317
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Virtual photons in imaginary time: Computing exact Casimir forces via standard numerical-electromagnetism techniques .\nAbstract:\nWe present an efficient method for computing the vacuum energy and force between two parallel plates using only standard numerical electromagnetism methods, without resorting to any approximations or special treatments such as analytic continuation into complex frequencies. The key idea is that we can use the Feynman-Kac formula to express the vacuum expectation value of the stress tensor at finite temperature T = 1/β (where β is the inverse temperature) in terms of the corresponding quantity at zero temperature but with one additional term involving the time evolution operator over a period of length β . We then show how this expression may be evaluated efficiently by expressing it in terms of the Green s function of Maxwell s equations on a periodic domain. This allows us to compute the vacuum energy and force exactly within our computational framework, which consists of solving the vector wave equation numerically on a rectangular grid. Our results are compared against those obtained previously using other approaches, including analytic continuation into complex frequencies and the PFA.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Virtual photons in imaginary time : Computing exact Casimir forces via standard numerical - electromagnetism methods . Abstract : We present an efficient algorithm for computing the vacuum energy and force between two connected plates using only conventional numerical electromagnetism methods , without resorting to any approximations or particular treatments such as analytic continuation into complex frequencies .The main idea is that we can using the Feynman - Kac formula to express the vacuum expectation value of the strain vector at finite temperature T = 1 / beta ( where β is the inverse temperature ) in terms of the associated quantity at zero temperature but with one additional word regarding the period evolution operator over a period of length β . We then show how this expression may be evaluated efficiently by expressing it in terms of the Green s function of Maxwell s coefficients on a periodic domain .This enables us to compute the vacuum energy and force exactly within our computational framework , which consists of calculating the vector wave equation numerically on a rectangular grid . Our results are compared against those achieved already using other methods , notably analytic continuation into complex wavelength and the PFA .",
        "rewrite_text": "We introduce a novel and efficient algorithm designed to compute the vacuum energy and force between two parallel plates, utilizing standard numerical electromagnetism techniques without the need for approximations or specialized treatments, such as analytic continuation into complex frequencies. The core concept of our approach is based on the Feynman-Kac formula, which allows us to express the vacuum expectation value of the strain vector at a finite temperature \\( T = \\frac{1}{\\beta} \\) (where \\( \\beta \\) represents the inverse temperature) in relation to the corresponding value at zero temperature. This relationship incorporates an additional term involving the periodic evolution operator over a duration of \\( \\beta \\). \n\nWe demonstrate that this expression can be efficiently evaluated by reformulating it in terms of the Green's function associated with Maxwell's equations on a periodic domain. This methodology enables us to compute the vacuum energy and force with precision within our computational framework, which involves numerically solving the vector wave equation on a rectangular grid. \n\nOur findings are rigorously compared to results obtained through alternative methods, particularly those involving analytic continuation into complex wavelengths and the Proximity Force Approximation (PFA). The results indicate that our approach not only provides exact calculations of the Casimir forces but also enhances the understanding of vacuum energy interactions in electromagnetic systems. This work represents a significant advancement in computational electromagnetism, offering a robust tool for researchers investigating quantum field effects in confined geometries.",
        "ori-fast-z-score": 0.20412414523193154,
        "water-fast-z-score": 4.6467017049401695,
        "rewrite-fast-z-score": 0.7427813527082074
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamical Objects for Cohomologically Expanding Maps .\nAbstract:\nWe introduce the notion of dynamical objects associated to cohomologically expanding maps, and prove that they are equivalent to the usual ones in many cases.  We also show how these new objects can be used to study the dynamics of such maps on infinite-dimensional spaces. Let X be an infinite dimensional Banach space with norm . For each integer n ≥ 1 we define the open ball B(n) = {x ∈ X : x < n}. A map T : X → X is said to be cohomologically expanding if there exists some constant C > 0 so that for all integers m, n ≥ 1 one has  diam (T −m (B(n))) ≤ Cn. In this case it follows easily that T satisfies the following properties:\n(1)  T is continuous; \n(2)  T is surjective; \nThe main result of our work shows that under certain conditions, the existence of a dynamical object implies the existence of another one which behaves well when restricted to finite-dimensional subspaces. \nLet us now recall what a dynamical object is. Given any point x ∈ X , let O(x) denote the orbit of x; i.e., O(x) := {T k (x), k ∈ Z}. The set O(x) equipped with the metric dO defined by dO((x1, x2)) = sup{d(x1, x2), x1 ∈ O(x2), x2 ∈ O(x1)} becomes a compact metric space called the orbital space at x. If T is cohomologically expanding then every orbital space is homeomorphic to a Cantor set.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamical Objects for Cohomologically Expanding Maps . Abstract : We introduce the notion of dynamical images identified to cohomologically extended maps , and prove that they are comparable to the usual ones in many cases .We additionally understand how these new objects can be used to study the dynamics of such maps on infinite - dimensional spaces . Let X be an infinite dimensional Banach space with norm .For each integer n ≥ 1 we define the open ball B ( n ) = { x ∈ X : x < n } . A mapping T : X → X is said to be cohomologically extended if there exists some constant C > 0 so that for all integers m , k ≥ 1 one has diam ( T −m ( B ( n ) ) ) ≤ Cn .In this instance it appears easily that T satisfies the following properties : ( 1 ) T is continuous ; ( 2 ) T is surjective ; The main consequence of our work suggests that under certain conditions , the existence of a dynamical object assumes the existence of another one which behaves well when confined to finite - dimensional subspaces . Let us now recall what a dynamical object is .Given any point x ∈ X , let O ( x ) define the orbit of x ; i . e . , O ( x ) : = { T k ( x ) , k ∈ Z } . The set O ( x ) equipped with the metric dO defined by dO ( ( x1 , x2 ) ) = sup { d ( x1 , x2 ) , x1 ∈ O ( x2 ) , x2 ∈ O ( x1 ) } becomes a compact metric space termed the orbital space at x .If T is cohomologically expanding then every orbital space is homeomorphic to a Cantor set .",
        "rewrite_text": "In this article, we present the concept of dynamical images associated with cohomologically extended maps, demonstrating their comparability to traditional dynamical images in various scenarios. Our investigation delves into the application of these novel constructs for analyzing the dynamics of such maps within infinite-dimensional spaces. We consider an infinite-dimensional Banach space, denoted as X, equipped with a norm. For each integer n ≥ 1, we define the open ball B(n) = { x ∈ X : ||x|| < n }. A mapping T: X → X is classified as cohomologically extended if there exists a constant C > 0 such that for all integers m, k ≥ 1, the diameter of the preimage of the open ball under T satisfies diam(T^(-m)(B(n))) ≤ Cn. Under these conditions, we establish that T possesses two key properties: (1) it is continuous, and (2) it is surjective. \n\nA significant outcome of our research indicates that, under specific conditions, the presence of a dynamical object implies the existence of another dynamical object that exhibits favorable behavior when restricted to finite-dimensional subspaces. To clarify, we define a dynamical object as follows: for any point x ∈ X, the orbit of x, denoted O(x), is defined as O(x) = { T^k(x) : k ∈ Z }. The set O(x), equipped with the metric dO defined by dO((x1, x2)) = sup{d(x1, x2) : x1 ∈ O(x2), x2 ∈ O(x1)}, forms a compact metric space known as the orbital space at x. Notably, if T is cohomologically expanding, every orbital space is homeomorphic to a Cantor set. This work not only enriches the understanding of dynamical systems in infinite-dimensional contexts but also opens avenues for further exploration of the interplay between cohomological properties and dynamical behavior.",
        "ori-fast-z-score": 0.5883484054145521,
        "water-fast-z-score": 3.530090432487313,
        "rewrite-fast-z-score": 0.936585811581694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The chemical composition of the circumstellar envelopes around yellow hypergiant stars .\nAbstract:\nWe present new observations and analysis of the infrared emission lines in the spectra of two yellow hypergiants, IRC+10420 and AFGL2136. We find that these objects have very high mass-loss rates (10^-6 to 10^-5 Msun/yr) with outflow velocities ranging between 100-200 km/sec. The observed line profiles are consistent with an expanding shell model for the wind. In addition we detect several forbidden transitions which indicate the presence of highly ionized species such as Fe + , Si ++ , S ++ . These ions may be formed by photoionization or collisional ionization processes within the stellar winds. \n \n Keywords: Yellow Hypergiants; Circumstellar Envelopes; Mass loss rate; Outflows; Emission Lines; IRAS 08544-4431. Astronomy & Astrophysics manuscript no. aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The chemical composition of the circumstellar envelopes around yellow hypergiant stars . Abstract : We report new studies and investigation of the infrared emission lines in the spectra of two yellow hypergiants , IRC + 10420 and AFGL2136 .We see that these objects have very high mass - loss rates ( 10 ^ - 6 to 10 ^ - 5 Msun / yr ) with outflow velocities ranging between 100 - 200 kilometers / sec . The observed line profiles are compatible with an expanding shell model for the wind .In addition we find various forbidden transitions which demonstrate the presence of highly ionized species such as Fe + , Si + + , S + + . These ions may be formed by photoionization or collisional ionization processes within the stellar winds .Keywords : Yellow Hypergiants ; Circumstellar Envelopes ; Mass loss rate ; Outflows ; Emission Lines ; IRAS 08544 - 4431 . Astronomy & Astrophysics manuscript no .aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope",
        "rewrite_text": "Title: The Chemical Composition of Circumstellar Envelopes Surrounding Yellow Hypergiant Stars\n\nAbstract: In this study, we present a comprehensive analysis of the infrared emission lines observed in the spectra of two prominent yellow hypergiant stars, IRC + 10420 and AFGL 2136. Our findings reveal that these stars exhibit exceptionally high mass-loss rates, estimated between 10^-6 to 10^-5 solar masses per year, accompanied by outflow velocities that range from 100 to 200 kilometers per second. The spectral line profiles we have analyzed are consistent with an expanding shell model, which effectively describes the dynamics of the stellar winds emanating from these massive stars. Furthermore, our investigation has identified various forbidden transitions that indicate the presence of highly ionized elements, including Fe+, Si++, and S++. The formation of these ions is likely due to processes such as photoionization or collisional ionization occurring within the stellar winds. This research enhances our understanding of the chemical composition and dynamics of circumstellar envelopes around yellow hypergiants, contributing valuable insights into their evolutionary processes and the mechanisms driving mass loss in these extraordinary stellar objects. Our results underscore the importance of studying the circumstellar environments of yellow hypergiants, as they play a crucial role in the lifecycle of massive stars and the chemical enrichment of the interstellar medium. \n\nKeywords: Yellow Hypergiants; Circumstellar Envelopes; Mass Loss Rate; Outflows; Emission Lines; IRAS 08544-4431. \n\nAstronomy & Astrophysics manuscript no. aa20031118, May 31, 2003.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 3.0464244212496006,
        "rewrite-fast-z-score": 1.5554275420956378
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Gaussianity analysis on local morphological measures of WMAP data .\nAbstract:\nWe analyze the non-Gaussianity in the temperature fluctuations of the Wilkinson Microwave Anisotropy Probe (WMAP) first year sky maps using three different estimators, namely Minkowski Functionals (MF), genus curve and correlation functions. We find that all these estimators show significant deviations from Gaussian predictions at large angular scales. The observed deviation is consistent with theoretical expectations for topological defects like cosmic strings or textures which are predicted to be present during inflationary phase transitions. These results provide further support for the presence of primordial non-Gaussianities in CMB anisotropies. This work was supported by the Department of Atomic Energy under grant No. 06(B)/ST-IISc/04. The authors thank Sourav Chatterjee for useful discussions. PACS numbers: 98.80.Cq, 95.35.+d, 04.20.Jb  Keywords: Cosmic microwave background radiation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Gaussianity evaluation on local morphological measures of WMAP information . Abstract : We evaluate the non - Gaussianity in the temperature fluctuations of the Wilkinson Microwave Anisotropy Probe ( WMAP ) first year sky mapping utilizing three different estimators , comprising Minkowski Functionals ( MF ) , genus curve and correlation functions .We see that all these estimators exhibit substantial deviations from Gaussian predictions at large angular scales . The observed deviation is compatible with theoretical expectations for topological errors like cosmic strings or textures which are expected to be present during inflationary phase transitions .These data provide further evidence for the presence of primordial non - Gaussianities in CMB anisotropies . This project was supported by the Department of Atomic Energy under grant No .06 ( B ) / ST - IISc / 04 . The authors mention Sourav Chatterjee for useful talks .PACS codes : 98 . 80 . Cq , 95 . 35 . + d , 04 . 20 . Jb Keywords : Cosmic microwave background radiation",
        "rewrite_text": "Title: Evaluation of Non-Gaussianity in Local Morphological Measures of WMAP Data\n\nAbstract: In this study, we investigate the non-Gaussian characteristics of temperature fluctuations observed in the first-year sky mapping data from the Wilkinson Microwave Anisotropy Probe (WMAP). Our analysis employs three distinct estimators: Minkowski Functionals (MF), the genus curve, and correlation functions. The results indicate that all three estimators reveal significant deviations from the predictions of Gaussian statistics, particularly at large angular scales. These deviations align with theoretical models that suggest the presence of topological defects, such as cosmic strings or textures, which may have formed during the inflationary phase transitions of the early universe. The findings contribute to the growing body of evidence supporting the existence of primordial non-Gaussianities in the anisotropies of the cosmic microwave background (CMB). This research was conducted with the support of the Department of Atomic Energy under grant No. 06 (B) / ST - IISc / 04. The authors express gratitude to Sourav Chatterjee for his insightful discussions that aided in this work. The study is categorized under PACS codes: 98.80.Cq, 95.35.+d, and 04.20.Jb, and is relevant to the field of cosmic microwave background radiation.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 5.252257314388902,
        "rewrite-fast-z-score": 1.7801724872907798
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Simulation of Single Reed Instruments Oscillations Based on Modal Decomposition of Bore and Reed Dynamics .\nAbstract:\nThe reed is the key component in single-reed musical instruments, such as clarinets or saxophones. The dynamics of its motion are strongly influenced by the bore geometry and boundary conditions at both ends. In this work we present an approach to simulate the oscillations of a single reed instrument based on modal decomposition of the bore and reed dynamics. We show that it is possible to accurately reproduce the sound produced by a real clarinet using only one degree-of-freedom for each mode. This method can be used to study the influence of different parameters (such as mouthpiece diameter) on the acoustic response of the instrument without requiring expensive experimental measurements. It also allows us to investigate how changes in the design affect the performance of new prototypes. \n \n Keywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Simulation of Single Reed Instruments Oscillations Based on Modal Decomposition of Bore and Reed Dynamics . Abstract : The reed is the key ingredient in single - reed musical instruments , such as clarinets or saxophones .The dynamics of its motion are strongly dictated by the bore geometry and boundary conditions at both ends . In this study we present an approach to simulate the oscillations of a single reed instrument relying on modal decomposition of the bore and reed dynamics .We see that it is easy to correctly reproduce the music created by a real clarinet using only one degree - of - independence for each mode . This method can be used to study the impact of different variables ( such as mouthpiece diameter ) on the acoustic response of the instrument without using expensive experimental surveys .It additionally lets us to examine how differences in the model affect the performance of new prototypes . Keywords : Musical Instrument , Clarinet , Sound Generation , Modeling , Simulation , Acoustics , Vibration Analysis , Modal Analysis , Finite Element Method",
        "rewrite_text": "Title: Simulation of Oscillations in Single Reed Instruments Through Modal Decomposition of Bore and Reed Dynamics\n\nAbstract: Single-reed musical instruments, including clarinets and saxophones, rely heavily on the reed as a fundamental component influencing sound production. The motion dynamics of the reed are intricately linked to the geometry of the bore and the boundary conditions at both ends of the instrument. In this research, we introduce a novel approach for simulating the oscillations of single reed instruments by employing modal decomposition techniques to analyze both bore and reed dynamics. Our findings demonstrate that it is feasible to accurately replicate the sound produced by a real clarinet using a simplified model that incorporates just one degree of freedom for each mode. This innovative simulation method provides a valuable tool for investigating how various factors, such as mouthpiece diameter, affect the acoustic characteristics of the instrument, thereby reducing the need for costly experimental setups. Furthermore, our approach facilitates the evaluation of how variations in the model can influence the performance of new instrument prototypes. By leveraging this methodology, researchers and instrument designers can gain deeper insights into the acoustic behavior of single reed instruments, ultimately enhancing the design and functionality of these musical devices. The implications of this study extend to the fields of musical acoustics, vibration analysis, and finite element modeling, offering a comprehensive framework for future investigations into sound generation in single reed instruments. \n\nKeywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method.",
        "ori-fast-z-score": -1.9694638556693236,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": 1.1441551070947107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlinear Dynamics of Infectious Diseases Transfer with Possible Applications for Tubercular Infection .\nAbstract:\nWe study the nonlinear dynamics of infectious diseases transfer in a population, where individuals are divided into three classes: susceptible (S), infected (I) and recovered/removed (R). We consider two different models: SIR model and SEIR model. In both cases we assume that there is no birth or death in the population. The main goal of this work is to investigate how the disease spreads through the population depending on its parameters. For example, if the infection rate is too high then it may lead to an epidemic outbreak. On the other hand, if the recovery rate is very large compared to the infection rate then the number of infectives will decrease rapidly. Finally, we show some numerical simulations which illustrate our results. \n \n Keywords: Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. 1 Introduction \n \n Many mathematical models have been developed over time to describe the spread of infectious diseases within populations  1–3  . These models can be used as tools to understand the transmission mechanisms of these diseases and help public health authorities make decisions about prevention strategies  4  .\n \nIn particular, many researchers have studied the effects of vaccination programs  5–7  , quarantine  8, 9  and isolation  10, 11  on the evolution of epidemics. Other studies focus on the impact of environmental factors such as temperature  12, 13  , humidity  14, 15  and rainfall  16  on the propagation of pathogens. \nThe majority of existing works use deterministic models based on ordinary differential equations  17  . However, stochastic models  18, 19  and agent-based models  20, 21  also exist. Agent-based models allow us to take into account individual behaviors  22  while stochastic models provide more realistic descriptions of random events  23  . \n \nIn this article, we propose new mathematical models describing the spread of infectious diseases in a closed population. Our aim is to analyze the influence of various parameters on the behavior of the system. More specifically, we want to determine whether the disease will die out naturally or cause an epidemic outbreak. To do so, we first introduce the basic reproduction number R0  24  , which represents the average number",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlinear Dynamics of Infectious Diseases Transfer with Possible Applications for Tubercular Infection . Abstract : We research the nonlinear dynamics of infectious infections transfer in a population , where persons are split into three categories : resistant ( S ) , infected ( I ) and returned / deleted ( R ) .We consider two different models : SIR model and SEIR model . In both cases we suppose that there is no death or dying in the population .The main goal of this project is to examine how the infection spreads through the population depending on its criteria . For instance , if the infection rate is too high then it could lead to an outbreak outbreak .On the other hand , if the return frequency is very huge compared to the infection rate then the quantity of infectives will decrease rapidly . Finally , we give some numerical simulations which illustrate our findings .Keywords : Nonlinear dynamics , infectious infections , tuberculosis , SIR model , SEIR model . 1 Introduction Many numerical models have been created over time to explain the spread of infectious infections within communities 1 – 3 .These systems can be used as tools to explain the spreading patterns of these diseases and help public medical institutions making decisions about prevention tactics 4 . In particular , many scientists have researched the effects of vaccination programs 5 – 7 , quarantine 8 , 9 and isolation 10 , 11 on the evolution of epidemics .Other studies emphasis on the impact of environmental factors such as temperature 12 , 13 , moisture 14 , 15 and rainfall 16 on the propagation of pathogens . The majority of older projects using deterministic methods using on ordinary differential coefficients 17 .However , stochastic theories 18 , 19 and agent - based models 20 , 21 also exist . Agent - based models let us to take into consideration individual behaviors 22 while stochastic theories provide more realistic descriptions of random events 23 .In this page , we propose new numerical models explaining the spread of infectious infections in a closed population . Our aim is to analyze the impact of several variables on the activity of the system .More specifically , we try to measure whether the infection will die out naturally or result an outbreak outbreak . To do so , we first introduce the fundamental reproduction number R0 24 , which equals the average number",
        "rewrite_text": "**Title:** Nonlinear Dynamics of Infectious Disease Transmission with Potential Applications for Tuberculosis Infection\n\n**Abstract:** This study investigates the nonlinear dynamics of infectious disease transmission within a population categorized into three distinct groups: susceptible (S), infected (I), and recovered (R). We analyze two primary models: the SIR (Susceptible-Infected-Recovered) model and the SEIR (Susceptible-Exposed-Infected-Recovered) model. In our analysis, we assume a closed population with no mortality, allowing us to focus solely on the dynamics of infection spread. The primary objective of this research is to understand how various parameters influence the transmission of infection within the population. For instance, a high infection rate may precipitate an outbreak, while a significantly higher recovery rate compared to the infection rate could lead to a rapid decline in the number of infected individuals. To support our theoretical findings, we present numerical simulations that illustrate the dynamics of infection spread under different scenarios. \n\n**Keywords:** Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model.\n\n**1. Introduction:** Over the years, numerous numerical models have been developed to elucidate the spread of infectious diseases within communities. These models serve as valuable tools for understanding disease transmission patterns and assisting public health authorities in formulating effective prevention strategies. A significant body of research has focused on the effects of vaccination programs, quarantine measures, and isolation protocols on epidemic progression. Additionally, studies have explored the influence of environmental factors, such as temperature, humidity, and rainfall, on pathogen transmission. Historically, many of these models have employed deterministic approaches based on ordinary differential equations. However, there is a growing interest in stochastic models and agent-based simulations, which account for individual behaviors and provide a more nuanced representation of random events. In this paper, we propose novel numerical models to describe the spread of infectious diseases in a closed population, aiming to assess the impact of various factors on system dynamics. Specifically, we seek to determine whether an infection will naturally extinguish or escalate into an outbreak. To facilitate this analysis, we introduce the basic reproduction number, R0, which represents the average number of secondary infections generated by a single infected individual in a fully susceptible population.",
        "ori-fast-z-score": -0.6064784348631227,
        "water-fast-z-score": 9.846562966837059,
        "rewrite-fast-z-score": 0.7372097807744856
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytic steady-state space use patterns and rapid computations in mechanistic home range analysis .\nAbstract:\nWe present an analytic solution to the steady state distribution for the mechanistic home-range model developed by Moorcroft et al. (2006) that allows for efficient computation of home ranges using numerical integration methods. The new method is implemented as part of the R package adehabitatHR, which also includes functions for computing home ranges with the original algorithm (i.e., without the analytical solution). We demonstrate how our approach can be used to rapidly compute home ranges across large landscapes containing thousands of habitat patches. Our results show that the new method produces identical estimates compared to those obtained with the original algorithm but requires less computational time when estimating home ranges over large spatial extents. Analytical solutions are useful because they allow researchers to efficiently estimate home ranges on very large datasets or at fine resolutions. \n \n Home ranges have been widely studied since their introduction into ecology more than 50 years ago  1  . These areas represent the area within which individuals obtain all necessary resources  2  , such as food  3  , water  4  , shelter  5  , mates  6  , and cover  7  . In addition to being important for understanding animal behavior  8  , home ranges play key roles in conservation biology  9  , wildlife management  10  , epidemiology  11  , and disease transmission  12  .\n \nHome-range models typically assume that animals move through a landscape composed of discrete habitat patches  13  . Animals select among these patches based on some combination of patch attributes  14  , including resource availability  15  , vegetation structure  16  , predation risk  17  , and conspecific density  18  . This process continues until the animal reaches equilibrium between its movement rate and the quality of available habitats  19  . \n \n A number of different approaches exist for modeling animal movements  20  . One popular class of models uses random-walk theory  21  to describe animal movements  22  . Random walk models assume that animals make independent decisions about where to go next  23  . However, this assumption may not always hold true  24  . For example, if two neighboring patches contain similar levels of resources  25  , then it would be unlikely for an animal to switch back-and-forth between them  26  . To account for this type of behavioral response, Moorcro",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytic steady - state space use patterns and quick computations in mechanistic home range modeling . Abstract : We present an analytic solution to the steady state distribution for the mechanistic home - range system established by Moorcroft et al .( 2006 ) that enables for efficient computation of bedroom ranges using numerical integration methods . The new method is implemented as part of the R program adehabitatHR , which also contains functions for modeling home ranges with the previous algorithm ( i . e . , without the analytical solution ) .We suggest how our approach can be used to rapidly compute bedroom ranges across large landscapes containing thousands of habitat patches . Our results show that the new method generates similar estimates compared to those achieved with the previous algorithm but requires fewer computational time when estimating bedroom ranges over large geographic extents .Analytical systems are helpful because they allow scientists to easily measure residence ranges on very huge datasets or at fine resolutions . Home ranges have been widely explored since their arrival into ecosystems more than 50 centuries earlier 1 .These zones represent the territory within which adults obtain all necessary resources 2 , such as feed 3 , water 4 , protection 5 , mates 6 , and cover 7 . In addition to being important for explaining animal behavior 8 , home ranges represent crucial roles in wildlife biology 9 , fauna conservation 10 , epidemiology 11 , and infection propagation 12 .Home - range systems often assume that animals shift through a landscape composed of distinct habitat patches 13 . Animals select among these patches based on some mix of patch traits 14 , including resource capacity 15 , vegetation structure 16 , predation risk 17 , and conspecific density 18 .This process proceeds until the organism reaches optimal between its movement rate and the quality of available environments 19 . A variety of different methods exist for modeling animal activities 20 .One popular type of models using random - walk principle 21 to define animal activities 22 . Random walk models believe that individuals give independent choice about where to going next 23 .However , this assumption must not always hold false 24 . For instance , if two adjacent patches contain similar rates of assets 25 , then it would be impossible for an organism to shift back - and - forth between them 26 .To account for this form of behavioral reaction , Moorcro",
        "rewrite_text": "**Title:** Analytic Steady-State Space Use Patterns and Quick Computations in Mechanistic Home Range Modeling\n\n**Abstract:** In this study, we introduce an analytic solution for the steady-state distribution within the mechanistic home-range framework established by Moorcroft et al. (2006). This innovative approach facilitates the efficient computation of bedroom ranges through numerical integration techniques. The method has been integrated into the R package adehabitatHR, which also retains functionalities for modeling home ranges using the previous algorithm that does not incorporate the analytical solution. Our findings demonstrate that this new method yields comparable estimates to those produced by the earlier algorithm while significantly reducing computational time when applied to extensive landscapes with numerous habitat patches. \n\nThe utility of analytical systems is particularly pronounced in scenarios involving large datasets or high-resolution analyses, enabling researchers to effectively measure residence ranges. Home ranges, which have been a focal point of ecological research for over 50 years, define the territories where adult animals secure essential resources such as food, water, shelter, mates, and protection. Understanding home ranges is vital not only for elucidating animal behavior but also for informing wildlife biology, conservation efforts, epidemiology, and the dynamics of disease transmission.\n\nHome-range models typically assume that animals navigate through a landscape composed of discrete habitat patches, selecting among them based on various patch characteristics, including resource availability, vegetation structure, predation risk, and conspecific density. This selection process continues until an optimal balance is achieved between the animal's movement rate and the quality of the available habitats. Various modeling techniques exist to simulate animal movement, with random-walk models being particularly popular. These models posit that individuals make independent choices regarding their next location. However, this assumption may not always hold true; for example, if two adjacent patches offer similar resource levels, it would be unlikely for an animal to oscillate between them. To address such behavioral nuances, Moorcroft's framework provides a more nuanced understanding of animal movement and habitat selection.",
        "ori-fast-z-score": -2.0701966780270626,
        "water-fast-z-score": 9.431468954254745,
        "rewrite-fast-z-score": 0.5203059023730164
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Super Low Traction under EHD and Mixed Lubrication Regimes .\nAbstract:\nThe effect of the lubricant viscosity on friction in elastohydrodynamic (EHD) regime is investigated by using numerical simulations based on Reynolds equation coupled with elasticity equations for an axisymmetric slider bearing system. The results show that, at low sliding speeds, the traction decreases as the lubricant viscosity increases due to the increase of pressure gradient along the film thickness direction. At high sliding speeds, however, the opposite trend occurs because the viscous shear stress becomes dominant over the pressure gradient term. In mixed lubrication regimes where both hydrodynamic and boundary layers exist simultaneously, it was found that the minimum traction can be reduced significantly when the lubricant viscosity is increased. This reduction is attributed mainly to the decrease of the maximum pressure in the contact region. \n \n Keywords: Elastohydrodynamics; Slider Bearing; Minimum Film Thickness; Viscosity; Pressure Gradient Term",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Super Low Traction under EHD and Mixed Lubrication Regimes . Abstract : The impact of the lubricant viscosity on friction in elastohydrodynamic ( EHD ) regime is investigated by using numerical simulations based on Reynolds function combined with elasticity equations for an axisymmetric slider bearing system .The results show that , at low moving speeds , the traction decreases as the lubricant viscosity increases owing to the increase of stress gradient along the film thickness direction . At high sliding speeds , however , the opposite decline happens because the viscous shear force remains dominant over the pressure gradient term .In combined lubrication regimes where both hydrodynamic and boundary layers occur simultaneously , it was shown that the minimum traction can be reduced greatly when the lubricant viscosity is enhanced . This decrease is attributed mostly to the decrease of the maximum pressure in the contact region .Keywords: Elastohydrodynamics; Slider Bearing; Minimum Film Thickness; Viscosity; Pressure Gradient Term",
        "rewrite_text": "Title: Super Low Traction under EHD and Mixed Lubrication Regimes\n\nAbstract: This study explores the influence of lubricant viscosity on friction within elastohydrodynamic (EHD) lubrication regimes through numerical simulations that integrate Reynolds' equation with elasticity principles, specifically applied to an axisymmetric slider bearing system. The findings reveal a nuanced relationship between lubricant viscosity and traction, particularly at varying sliding speeds. At lower velocities, an increase in lubricant viscosity correlates with a reduction in traction. This phenomenon is attributed to the heightened stress gradient observed across the film thickness, which diminishes the overall traction. Conversely, at elevated sliding speeds, the trend reverses; traction increases as viscosity rises. This counterintuitive result is primarily due to the dominance of viscous shear forces over the pressure gradient effects in this regime. Furthermore, in scenarios where both hydrodynamic and boundary lubrication layers coexist, the study demonstrates that enhancing lubricant viscosity can significantly lower the minimum traction levels. This reduction is largely linked to a decrease in the maximum pressure experienced in the contact area. The implications of these findings are critical for the design and optimization of lubrication systems in engineering applications, where managing friction and wear is essential for improving performance and longevity. The research underscores the complex interplay between viscosity and traction in EHD and mixed lubrication contexts, providing valuable insights for future investigations into lubricant formulations and their operational characteristics.\n\nKeywords: Elastohydrodynamics; Slider Bearing; Minimum Film Thickness; Viscosity; Pressure Gradient Term",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": -0.10153461651336192
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HUDF-JD2: Mid-infrared Evidence for a z~2 Luminous Infrared Galaxy .\nAbstract:\nWe present new mid-IR photometry and spectroscopy of the HUDF-JD2 galaxy at redshift 2.081, which is one of the most luminous infrared galaxies known to date. The SED shows that it has an extremely red continuum with strong PAH emission features in its rest frame optical spectrum. We find evidence for both star formation activity (from the UV-optical) as well as obscured AGN activity (from X-ray observations). This object may be representative of a population of dusty star-forming galaxies undergoing rapid evolution during this critical epoch when massive black holes are growing rapidly along with their host galaxies. Keywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HUDF - JD2 : Mid - infrared Evidence for a z ~ 2 Luminous Infrared Galaxy . Abstract : We present new mid - IR photometry and spectroscopy of the HUDF - JD2 galaxy at redshift 2 . 081 , which is one of the most luminous infrared galaxies known to date .The SED shows that it has an exceptionally red continuum with powerful PAH emission elements in its rest frame optical spectrum . We get confirmation for both star formation activity ( from the UV - optical ) as well as obscured AGN activity ( from X - ray observations ) .This object may be representative of a population of dusty star - creating stars undergoing fast evolution during this critical epoch when massive blue holes are growing rapidly along with their host galaxies . Keywords : Infrared , Redshift , Spectroscopy , Photometry , Black Hole Growth , Star Formation Rate Density , Ultraviolet Background Radiation , Cosmic Evolution , Cosmology , Extragalactic Astronomy , High Energy Astrophysics , Space Science , Nearby Galaxies",
        "rewrite_text": "We present comprehensive mid-infrared photometric and spectroscopic observations of the HUDF-JD2 galaxy, located at a redshift of 2.081, making it one of the most luminous infrared galaxies identified to date. Our analysis reveals a remarkably red spectral energy distribution (SED), characterized by prominent polycyclic aromatic hydrocarbon (PAH) emission features in its rest-frame optical spectrum. These findings provide strong evidence for ongoing star formation activity, as indicated by ultraviolet and optical data, alongside indications of obscured active galactic nucleus (AGN) activity, supported by X-ray observations. The characteristics of HUDF-JD2 suggest that it may serve as a representative example of a population of dusty, star-forming galaxies that are experiencing rapid evolutionary processes during a pivotal period in cosmic history. This era is marked by the swift growth of massive black holes in tandem with their host galaxies. Our study contributes to the understanding of the interplay between star formation and black hole growth in the early universe, shedding light on the evolution of galaxies during this critical epoch. The implications of our findings extend to various fields, including cosmology, extragalactic astronomy, and high-energy astrophysics, as they enhance our comprehension of cosmic evolution and the role of ultraviolet background radiation in shaping the star formation rate density across the universe. This research underscores the significance of mid-infrared observations in uncovering the complex dynamics of galaxy formation and evolution, particularly in the context of nearby galaxies and their distant counterparts. \n\nKeywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 2.888741522913896,
        "rewrite-fast-z-score": 0.8908708063747479
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Searching for Earth analogues around the nearest stars: the disk age-metallicity relation and the age distribution in the Solar Neighbourhood .\nAbstract:\nWe present an analysis of the ages, metallicities and distances to nearby main-sequence F-type dwarf stars (d < 25 pc) using high-resolution spectroscopy obtained with the HARPS spectrograph on board of the European Space Agency s space observatory COROT. We find that there is no correlation between stellar metallicity and distance from the Sun within this sample. This result suggests that the local thin disk has not been significantly affected by radial migration processes over its lifetime.  The lack of any significant trend in  Fe/H  as a function of distance also implies that the mean age of the local thin disk does not vary strongly across the solar neighbourhood. Using our derived ages we show that the age distribution of the local thin disk can be well described by a single exponential decay law with characteristic timescale τ = 3 Gyr. Our results are consistent with previous studies based on Hipparcos parallaxes but provide more accurate ages due to the higher precision of the spectroscopic data used here.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Searching for Earth analogues around the nearest stars : the disk age - metallicity relation and the age distribution in the Solar Neighbourhood . Abstract : We present an assessment of the ages , metallicities and distances to nearby major - sequence F - class dwarf stars ( d < 25 pc ) using high - resolution spectroscopy acquired with the HARPS spectrograph on board of the European Space Agency s space observatory COROT .We see that there is no correlation between stellar metallicity and altitude from the Sun within this specimen . This result suggests that the local thin disk has not been greatly impacted by radial migration cycles over its lifetime .The absence of any large trend in Fe / H as a function of distance also means that the mean age of the local thin disk does not varies strongly across the solar neighbourhood . Using our derived ages we prove that the age distribution of the local thin disk can be well described by a single exponential decay law with typical timescale τ = 3 Gyr .Our results are compatible with previous analyses based on Hipparcos parallaxes but give more accurate ages due to the higher precision of the spectroscopic data used here .",
        "rewrite_text": "In this study, we investigate the ages, metallicities, and distances of nearby main-sequence F-class dwarf stars located within 25 parsecs of the Sun, utilizing high-resolution spectroscopy obtained from the HARPS spectrograph aboard the European Space Agency's COROT space observatory. Our analysis reveals a lack of correlation between stellar metallicity and the altitude from the Sun among the stars in our sample. This finding implies that the local thin disk has remained largely unaffected by radial migration processes throughout its history. Furthermore, the absence of significant variation in the iron-to-hydrogen ratio (Fe/H) with respect to distance indicates that the mean age of the local thin disk is relatively uniform across the solar neighborhood. By employing the ages we have derived, we demonstrate that the age distribution of the local thin disk can be accurately characterized by a single exponential decay model, with a characteristic timescale of approximately 3 billion years. Our findings align with previous studies that utilized Hipparcos parallaxes; however, the enhanced precision of our spectroscopic data allows for more accurate age determinations. This research contributes valuable insights into the age-metallicity relationship and the distribution of stellar ages in our vicinity, enhancing our understanding of the formation and evolution of the local stellar population.",
        "ori-fast-z-score": 0.41702882811414954,
        "water-fast-z-score": 5.288453643125169,
        "rewrite-fast-z-score": 0.8955334711889903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Astrophysical gyrokinetics: kinetic and fluid turbulent cascades in magnetized weakly collisional plasmas .\nAbstract:\nThe present work is devoted to the study of astrophysical plasma turbulence, with particular emphasis on its statistical properties at different scales. We consider both kinetic and fluid descriptions for the dynamics of collisionless plasmas, which are relevant for many space and laboratory applications. In this context we investigate the nonlinear evolution of magnetic fluctuations by means of direct numerical simulations (DNS) of the Vlasov-Maxwell system. The main results can be summarized as follows:  1. Turbulence statistics -We perform DNSs of the Vlasov-Poisson system in order to characterize the statistical properties of the electrostatic potential fluctuations generated by an initial spectrum of Alfvenic modes. Our analysis shows that the energy cascade proceeds towards smaller spatial scales until it reaches the ion Larmor radius scale where it is transferred into perpendicular wavenumbers through Landau damping. At these small scales, the energy transfer rate decreases due to the reduction of phase correlations between wavevectors. This process leads to the formation of intermittency in the distribution function of particles.  2. Kinetic effects -In addition to the above mentioned features observed in the case of purely hydrodynamic turbulence, our results show that kinetic effects play also an important role in determining the statistical properties of the fluctuating fields. Indeed, we find that the presence of ions modifies significantly the shape of the probability density functions (PDFs), leading to non-Gaussian distributions characterized by tails extending over several orders of magnitude. Moreover, we observe that the PDFs become more skewed when increasing the value of the ion-to-electron mass ratio. Finally, we discuss how the inclusion of kinetic effects affects the scaling laws characterizing the power spectra of the fluctuating fields. \n3. Fluid description -By performing DNSs of the Euler-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Astrophysical gyrokinetics : kinetic and fluid turbulent cascades in magnetized weakly collisional plasmas . Abstract : The present work is committed to the observation of astrophysical plasma turbulence , with particular emphasis on its statistical characteristics at different scales .We consider both kinetic and fluid representations for the dynamics of collisionless plasmas , which are applicable for numerous space and lab applications . In this framework we investigate the nonlinear progression of magnetic fluctuations by means of direct numerical simulations ( DNS ) of the Vlasov - Maxwell process .The main results can be summarized as follows : 1 . Turbulence statistics - We perform DNSs of the Vlasov - Poisson system in order to characterize the statistical characteristics of the electrostatic potential fluctuations formed by an initial spectrum of Alfvenic modes .Our study shows that the power cascade proceeds towards smaller spatial scales until it meets the ion Larmor radius scale where it is transferred into perpendicular wavenumbers through Landau damping . At these little scales , the electricity transfer frequency drops due to the reduction of phase correlations between wavevectors .This process results to the formation of intermittency in the distribution structure of particles . 2 .Kinetic effects - In addition to the above mentioned elements observed in the case of solely hydrodynamic turbulence , our findings show that kinetic influences play also an important role in determining the statistical characteristics of the fluctuating fields . Indeed , we find that the presence of ions modifies substantially the shape of the probability density functions ( PDFs ) , leading to non - Gaussian distributions characterized by tails extending over numerous orders of magnitude .Moreover , we find that the PDFs get more skewed when increasing the value of the electron - to - ion mass ratio . Finally , we explain how the introduction of kinetic effects affects the scaling laws characterizing the power spectra of the fluctuating fields .3 . Fluid description - By conducting DNSs of the Euler -",
        "rewrite_text": "**Title:** Astrophysical Gyrokinetics: Kinetic and Fluid Turbulent Cascades in Magnetized Weakly Collisional Plasmas\n\n**Abstract:** This study focuses on the investigation of turbulence in astrophysical plasmas, particularly analyzing its statistical properties across various scales. We explore both kinetic and fluid models to describe the dynamics of collisionless plasmas, which are relevant to a wide range of applications in both space and laboratory environments. Utilizing direct numerical simulations (DNS) of the Vlasov-Maxwell equations, we delve into the nonlinear evolution of magnetic fluctuations. Our findings can be summarized in three key areas: \n\n1. **Turbulence Statistics:** We conduct DNS of the Vlasov-Poisson system to examine the statistical properties of electrostatic potential fluctuations generated from an initial spectrum of Alfvenic modes. Our results indicate that the power cascade progresses towards smaller spatial scales until it reaches the ion Larmor radius scale, where energy is transferred into perpendicular wavenumbers via Landau damping. At these smaller scales, the frequency of energy transfer diminishes due to a decrease in phase correlations among wavevectors, leading to intermittent structures in the particle distribution.\n\n2. **Kinetic Effects:** Beyond the phenomena observed in purely hydrodynamic turbulence, our research highlights the significant role of kinetic effects in shaping the statistical properties of fluctuating fields. We discover that the presence of ions notably alters the probability density functions (PDFs), resulting in non-Gaussian distributions with tails that extend across multiple orders of magnitude. Furthermore, we observe that increasing the electron-to-ion mass ratio enhances the skewness of these PDFs. This section also discusses how incorporating kinetic effects modifies the scaling laws governing the power spectra of the fluctuating fields.\n\n3. **Fluid Description:** Through additional DNS of the Euler equations, we further elucidate the fluid dynamics involved in the turbulence of magnetized plasmas, providing a comprehensive understanding of the interplay between kinetic and fluid turbulence. \n\nThis work contributes to a deeper understanding of astrophysical plasma turbulence, offering insights that are crucial for both theoretical advancements and practical applications in the field.",
        "ori-fast-z-score": -1.3522468075656264,
        "water-fast-z-score": 5.30555710271907,
        "rewrite-fast-z-score": 0.25630729731502827
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Reduced Complexity Sphere Decoding for Square QAM via a New Lattice Representation .\nAbstract:\nWe propose an efficient reduced complexity sphere decoding (RSD) algorithm for square quadrature amplitude modulation (QAM). The proposed RSD is based on the new lattice representation, which can be viewed as a generalization of the well-known real-valued lattice representation to complex-valued lattices. We show that our RSD has lower computational complexity than existing algorithms in terms of both number of arithmetic operations and memory requirement. In addition, we demonstrate by simulation results that our RSD achieves better bit error rate performance over conventional RSDs at high signal-to-noise ratio region. Finally, we provide some insights into how to further reduce the computational complexity of our RSD without sacrificing its BER performance. Index Terms - Reduced complexity sphere decoding, Quadrature amplitude modulation, Computational complexity reduction, Bit error rate performance improvement. I. INTRODUCTIO N Quadrature amplitude modulation (Q AM )  1  , also known as phase-shift keying (PSK), is one of the most popular digital modulations used in wireless communications due to its simple implementation  2  . However, it suffers from poor power efficiency when compared with other high-order constellations such as 16-QAM or 64-QAM  3  .\nIn order to improve the power efficiency while maintaining good bit error rate (BER) performance, many research efforts have been made recently  4  -  8  . Among them, reduced complexity sphere decoding (RCSD)  9  -  11  plays an important role because RCSD provides near optimal BER performance with much less computational complexity than maximum-likelihood detection  12  . For example, the authors in  10  developed a novel RCSD scheme for square QAM using the so-called real-valued lattice representation  13  . It was shown in  14  that this approach requires only about half of the number of arithmetic operations required by the original RCSD  15  . Moreover, the authors in  16  showed that their RCSD outperforms the previous works  17  ,  19  in terms of BER performance under various channel conditions. Although these approaches are very promising, they still suffer from relatively large computational complexity especially at low-to-medium SNR",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Reduced Complexity Sphere Decoding for Square QAM via a New Lattice Representation . Abstract : We suggest an efficient reduced complexity sphere decoding ( RSD ) algorithm for square quadrature amplitude modulation ( QAM ) .The proposed RSD is based on the new lattice representation , which can be viewed as a generalization of the better - famous real - valued lattice representation to complex - valued lattices . We see that our RSD has reduced computational complexity than existing algorithms in terms of both number of logic operations and memory need .In addition , we prove by simulation data that our RSD achieves good bit error rate quality over traditional RSDs at high noise - to - noise ratio region . Finally , we provide some insights into how to further reduce the numerical complexity of our RSD without sacrificing its BER performance .Index Terms - Reduced complexity sphere decoding , Quadrature amplitude modulation , Computational complexity reduction , Bit error rate quality increase . I . INTRODUCTIO N Quadrature amplitude modulation ( Q AM ) 1 , sometimes called as phase - shift keying ( PSK ) , is one of the most popular digital modulations used in mobile services due to its simple application 2 .However , it suffers from poor energy efficiency when compared with other high - order constellations such as 16 - QAM or 64 - QAM 3 . In order to upgrade the power performance while maintaining good bit error rate ( BER ) performance , various study efforts have been made recently 4 - 8 .Among them , low complexity sphere decoding ( RCSD ) 9 - 11 provides an important role because RCSD presents near perfect BER performance with far lower mathematical complexity than maximum - likelihood recognition 12 . For instance , the writers in 10 developed a new RCSD system for square QAM using the so - called real - valued lattice representation 13 .It was shown in 14 that this methodology demands only about half of the number of arithmetic functions needed by the previous RCSD 15 . Moreover , the writers in 16 demonstrated that their RCSD outperforms the previous works 17 , 19 in terms of BER performance under various channel conditions .Although these method are very promising , they still suffer from fairly large numerical capacity especially at low - to - medium SNR",
        "rewrite_text": "**Title:** Reduced Complexity Sphere Decoding for Square QAM via a New Lattice Representation\n\n**Abstract:** In this paper, we introduce an innovative reduced complexity sphere decoding (RSD) algorithm tailored for square quadrature amplitude modulation (QAM). Our proposed RSD leverages a novel lattice representation that extends the well-established real-valued lattice framework to accommodate complex-valued lattices. This advancement significantly lowers the computational complexity compared to existing decoding algorithms, as evidenced by a reduction in both the number of logical operations and memory requirements. Through extensive simulations, we demonstrate that our RSD achieves superior bit error rate (BER) performance in high noise-to-noise ratio scenarios when compared to traditional RSD methods. Furthermore, we explore strategies to further minimize the numerical complexity of our RSD while maintaining its effective BER performance. Quadrature amplitude modulation (QAM) is widely recognized for its simplicity and effectiveness in mobile communication systems; however, it often falls short in energy efficiency relative to higher-order constellations like 16-QAM and 64-QAM. Recent research has focused on enhancing power performance while preserving optimal BER outcomes. Among these efforts, reduced complexity sphere decoding (RCSD) has emerged as a pivotal technique, offering near-optimal BER performance with significantly lower computational demands than maximum-likelihood detection. Previous studies have highlighted the efficacy of RCSD systems for square QAM, particularly those employing real-valued lattice representations, which have been shown to require approximately half the arithmetic operations of earlier RCSD implementations. Despite these advancements, existing methods still encounter substantial numerical complexity, particularly in low to medium signal-to-noise ratio (SNR) environments. Our work aims to address these challenges, providing a more efficient decoding solution that enhances both performance and practicality in real-world applications.\n\n**Index Terms:** Reduced complexity sphere decoding, Quadrature amplitude modulation, Computational complexity reduction, Bit error rate quality increase.",
        "ori-fast-z-score": 1.3315427649795275,
        "water-fast-z-score": 8.74573066576194,
        "rewrite-fast-z-score": 1.212678125181665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chandra Observations of SN 2004et and the X-ray Emission of Type IIp Supernovae .\nAbstract:\nWe present Chandra observations of supernova (SN) 2004et, which is one of only two type IIp SNe ever observed in X-rays. The data were obtained on 2005 February 24-26 with the Advanced CCD Imaging Spectrometer (ACIS-S). We detect no significant emission above background at energies below 1 keV or above 8 keV; we therefore restrict our analysis to the range 1-8 keV. In this energy band, we find that the spectrum can be fit by an absorbed blackbody model with kT = 0.7 ± 0.1 keV and N H = 2.5 +1.0 −0.8 × 10 22 cm −2 . These values are consistent with those found for other type IIp SNe. Using these parameters as well as the distance inferred from optical photometry, we calculate the luminosity of SN 2004et during its first 100 days after explosion. This value agrees very well with theoretical predictions based upon models of stellar evolution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chandra Observations of SN 2004et and the X - ray Emission of Type IIp Supernovae . Abstract : We report Chandra measurements of supernova ( SN ) 2004et , which is one of only two class IIp SNe actually seen in X - radiation .The data were obtained on 2005 February 24 - 26 with the Advanced CCD Imaging Spectrometer ( ACIS - S ) . We detect no considerable emission above background at energies below 1 keV or above 8 keV ; we thus restrict our analysis to the range 1 - 8 keV .In this power band , we find that the spectrum can be fit by an absorption blackbody model with kT = 0 . 7 ± 0 . 1 keV and N H = 2 . 5 + 1 . 0 −0 . 8 × 10 22 mm −2 . These values are compatible with those observed for other class IIp SNe .Using these parameters as well as the distance inferred from optical photometry , we determine the luminosity of SN 2004et during its initial 100 days after explosion . This value agrees very well with theoretical estimates based upon theories of stars evolution .",
        "rewrite_text": "We present the findings from our Chandra observations of supernova (SN) 2004et, which is notable for being one of only two Type IIp supernovae detected in X-ray emissions. The data collection took place from February 24 to 26, 2005, utilizing the Advanced CCD Imaging Spectrometer (ACIS-S). Our analysis reveals no significant emission above the background levels at energies below 1 keV or above 8 keV, leading us to focus our study on the energy range of 1 to 8 keV. Within this range, we successfully fit the observed spectrum using an absorption blackbody model, yielding a temperature of kT = 0.7 ± 0.1 keV and a hydrogen column density of N_H = 2.5 +1.0 -0.8 × 10^22 cm^-2. These parameters are consistent with those reported for other Type IIp supernovae. By applying these measurements alongside the distance derived from optical photometry, we calculated the luminosity of SN 2004et during the first 100 days post-explosion. Our findings align closely with theoretical predictions based on stellar evolution models, reinforcing the validity of these theories in explaining the behavior and characteristics of Type IIp supernovae. This study contributes to the understanding of the X-ray emissions associated with supernovae and provides valuable insights into the physical processes occurring during their early phases.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 4.296234062367188,
        "rewrite-fast-z-score": -0.7777777777777778
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The local stellar velocity field via vector spherical harmonics .\nAbstract:\nWe present an algorithm for the decomposition of the local stellar kinematics into vector spherical harmonic functions (VSH). The method is applied to simulated data and real observations, where we recover the underlying VSH coefficients with high accuracy. We show that our approach can be used as a powerful tool in galactic dynamics studies by recovering the gravitational potential of the Milky Way s dark matter halo. In addition, it allows us to study the anisotropy of the stellar orbits on different scales. \n \n Keywords: Vector spherical harmonics, Galactic dynamics, Stellar kinematics, Gravitational potentials \n \n 1 Introduction \n \n Spherical Harmonic Analysis has been widely used over many decades to analyse astronomical datasets such as galaxy surveys or star counts. However, this technique cannot easily be extended to deal with non-scalar quantities like velocities or accelerations. This problem was overcome by expanding these quantities onto vector spherical harmonics (VSH) which are defined as tensor products of scalar spherical harmonics  1  . These new basis functions have already found applications in fields ranging from cosmology  2  , solar physics  3  , heliophysics  4  and geophysics  5  .\n \nIn recent years there has been growing interest in using VSHs to model the observed properties of galaxies  6  -  8  . For example, they were recently employed to decompose the line-of-sight component of the stellar kinematics  9  . Here, we extend their use to also include the tangential components of the stellar motions. As a result, we obtain a complete description of the three-dimensional distribution of the stellar kinematics within each spatial bin. Moreover, since the expansion coefficients depend only on angular coordinates, they can be determined independently at every point along the line-of-sight. Therefore, our method does not require any assumptions about the symmetry of the system under investigation. \n2 Vector spherical harmonics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The regional stellar velocity field via vector spherical harmonics . Abstract : We report an algorithm for the decomposition of the local stars kinematics into vector spherical harmonic functions ( VSH ) .The method is applied to simulated measurements and actual observations , where we recover the underlying VSH coefficients with high accuracy . We see that our approach can be used as a powerful tool in galactic dynamics experiments by rescuing the gravitational potential of the Milky Way s dark matter halo .In addition , it allows us to study the anisotropy of the stellar orbits on various scales . Keywords : Vector spherical harmonics , Galactic mechanics , Stellar kinematics , Gravitational potentials 1 Introduction Spherical Harmonic Analysis has been widely using over numerous years to analyse astronomical datasets such as galaxy surveys or star numbers .However , this methodology cannot easily be generalized to deal with non - scalar components like velocities or accelerations . This problem was resolved by expanding these quantities onto vector spherical harmonics ( VSH ) which are specified as vector products of scalar circular harmonics 1 .These new basis systems have already found uses in areas ranging from cosmology 2 , lunar science 3 , heliophysics 4 and geophysics 5 . In recent years there has been growing interest in utilizing VSHs to model the known characteristics of galaxies 6 - 8 .For instance , they were recently employed to decompose the line - of - view component of the stars kinematics 9 . Here , we stretch their application to additionally include the tangential parts of the stars movements .As a result , we obtain a complete model of the three - dimensional distribution of the stars kinematics within each spatial bin . Moreover , since the expansion equations depend only on angular coordinates , they can be determined independently at every position along the line - of - view .Therefore , our technique does not require any constraints about the symmetry of the process under investigation . 2 Vector spherical harmonics",
        "rewrite_text": "**Title:** The Regional Stellar Velocity Field via Vector Spherical Harmonics\n\n**Abstract:** In this study, we present a novel algorithm designed for the decomposition of local stellar kinematics into vector spherical harmonic functions (VSH). This method has been rigorously tested on both simulated datasets and actual astronomical observations, demonstrating a high degree of accuracy in recovering the underlying VSH coefficients. Our findings indicate that this approach serves as a robust tool in the field of galactic dynamics, particularly in elucidating the gravitational potential of the Milky Way's dark matter halo. Furthermore, it enables a detailed examination of the anisotropy present in stellar orbits across various spatial scales. \n\nSpherical Harmonic Analysis has long been employed to analyze astronomical datasets, such as galaxy surveys and star counts. However, traditional methodologies face challenges when applied to non-scalar components, including velocities and accelerations. We address this limitation by expanding these quantities using vector spherical harmonics, which are defined as vector products of scalar spherical harmonics. This innovative basis has found applications across diverse fields, including cosmology, lunar science, heliophysics, and geophysics. Recently, there has been an increasing interest in leveraging VSHs to model the dynamics of galaxies. For instance, previous studies have utilized VSHs to decompose the line-of-sight components of stellar kinematics. In our work, we extend this application to encompass the tangential components of stellar motion, resulting in a comprehensive model that captures the three-dimensional distribution of stellar kinematics within defined spatial bins. Notably, the expansion equations rely solely on angular coordinates, allowing for independent determination at each position along the line of sight. This flexibility means that our technique does not impose any symmetry constraints on the processes being studied, thereby enhancing its applicability in various astrophysical contexts.\n\n**Keywords:** Vector spherical harmonics, Galactic mechanics, Stellar kinematics, Gravitational potentials.",
        "ori-fast-z-score": -1.8461989882527161,
        "water-fast-z-score": 7.0,
        "rewrite-fast-z-score": 1.9508345382639332
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electron Doping of Cuprates via Interfaces with Manganites .\nAbstract:\nWe report on the observation of electron doping in cuprate superconductors by interfacing them to manganite insulators through epitaxial growth and chemical bonding at interfaces.  The interface between La0.7Sr0.3MnO3 (LSMO) and YBa2Cu3O6+x (YBCO), which are both parent compounds for high temperature superconductivity, is found to be highly conducting despite the large lattice mismatch between LSMO and YBCO. This suggests that charge transfer across the interface occurs due to strong electronic hybridization rather than strain relaxation alone. We also find that the hole concentration in the YBCO layer can be controlled by varying the thickness of the LSMO layer grown on top of it. These results demonstrate an alternative approach towards engineering the carrier density in cuprate superconductors using oxide heterostructures. High-temperature superconductivity has been observed only in materials containing copper-oxygen planes known as CuO2 layers  1  . In these systems, holes doped into the CuO2 plane give rise to Cooper pairs leading to superfluidity  2  . However, the maximum critical temperature Tc = 92 K achieved so far in this class of materials is still well below the theoretical limit predicted by Bardeen-Cooper-Schrieffer theory  3  , raising questions about how to further enhance Tc  4  .\nIn recent years there have been significant efforts made to explore new routes toward enhancing Tc beyond its current record value  5  . One promising route involves introducing electrons into the CuO2 plane  6  . For example, replacing oxygen atoms in the CuO2 plane with fluorine leads to a reduction in the number of holes in the system  7, 8  . Alternatively, one may introduce electrons directly into the CuO2 plane by growing thin films of transition metal oxides such as SrTiO3  9  or LaAlO3  10  onto the surface of cuprate superconductors. While these approaches show promise, they require precise control over film composition and structure during deposition  11  . An alternative strategy would involve controlling the carrier density in cuprates without changing their crystal structures  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electron Doping of Cuprates via Interfaces with Manganites . Abstract : We report on the observation of electron doping in cuprate superconductors by interfacing them to manganite insulators through epitaxial growth and chemical bonding at interfaces .The interface between La0 . 7Sr0 . 3MnO3 ( LSMO ) and YBa2Cu3O6 + x ( YBCO ) , which are both parent molecules for high heat superconductivity , is found to be highly conducting despite the huge lattice mismatch between LSMO and YBCO . This implies that charge transfer across the interface comes due to powerful electronic hybridization instead than strain relaxation alone .We additionally find that the gap content in the YBCO layer can be governed by varying the density of the LSMO layer grown on top of it . These data demonstrate an additional method towards engineering the carrier density in cuprate superconductors using oxide heterostructures .High - temperature superconductivity has been observed only in structures containing copper - oxygen planes named as CuO2 layers 1 . In these systems , holes doped into the CuO2 plane give rise to Cooper pairs leading to superfluidity 2 .However , the maximum essential temperature Tc = 92 K attained so far in this class of substances is already much below the theoretical maximum expected by Bardeen - Cooper - Schrieffer principle 3 , placing questions about how to further enhance Tc 4 . In recent history there have been significant efforts made to pursue new routes toward enhancing Tc beyond its current record value 5 .One promising route includes introducing electrons into the CuO2 plane 6 . For instance , replacing oxygen atoms in the CuO2 plane with fluorine leads to a reduction in the number of holes in the scheme 7 , 8 .Alternatively , one may introduce particles specifically into the CuO2 plane by spreading thin films of transition metal oxides such as SrTiO3 9 or LaAlO3 10 onto the surface of cuprate superconductors . While these method demonstrate promise , they demand careful power over movie structure and shape during deposition 11 .An alternative scheme would include regulating the carrier density in cuprates without altering their crystal structures 12 .",
        "rewrite_text": "**Title:** Electron Doping of Cuprates via Interfaces with Manganites\n\n**Abstract:** In this study, we present findings on the electron doping of cuprate superconductors achieved through the interface with manganite insulators, utilizing epitaxial growth and chemical bonding techniques. Specifically, we investigate the interface formed between La0.7Sr0.3MnO3 (LSMO) and YBa2Cu3O6+x (YBCO), both of which are foundational materials for high-temperature superconductivity. Remarkably, despite the significant lattice mismatch between LSMO and YBCO, the interface exhibits high electrical conductivity. This observation suggests that the charge transfer occurring at the interface is primarily driven by strong electronic hybridization rather than merely by strain relaxation effects. Furthermore, our results indicate that the superconducting gap in the YBCO layer can be modulated by adjusting the density of the LSMO layer deposited on top. This discovery opens up new avenues for engineering carrier density in cuprate superconductors through the use of oxide heterostructures.\n\nHigh-temperature superconductivity is predominantly observed in materials featuring copper-oxygen planes, known as CuO2 layers. In these systems, the introduction of holes into the CuO2 plane facilitates the formation of Cooper pairs, which are essential for superfluidity. However, the highest critical temperature (Tc) recorded to date in this class of materials is 92 K, which remains significantly below the theoretical maximum predicted by the Bardeen-Cooper-Schrieffer (BCS) theory. This limitation raises critical questions regarding potential strategies to further enhance Tc. Recent research has focused on innovative approaches to increase Tc beyond its current limits, with one promising strategy being the introduction of electrons into the CuO2 plane. For example, substituting oxygen atoms in the CuO2 plane with fluorine reduces the hole concentration. Alternatively, the application of thin films of transition metal oxides, such as SrTiO3 or LaAlO3, onto cuprate superconductors has shown potential for electron introduction. While these methods are promising, they require meticulous control over the deposition process. Our findings suggest a novel approach to regulate carrier density in cuprates without necessitating alterations to their crystal structures, thereby providing a significant advancement in the field of superconductivity.",
        "ori-fast-z-score": 0.08247860988423225,
        "water-fast-z-score": 7.021870595978444,
        "rewrite-fast-z-score": 0.4181210050035454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nanodevices and Maxwell s Demon .\nAbstract:\nWe propose to use the concept of Maxwell s demon in order to explain how nanodevices can be used for information processing, storage or transmission. We show that this approach is useful because it allows us to understand why some devices are more efficient than others at performing these tasks. In particular we consider two types of nanodevices which have been proposed recently as candidates for quantum computers - spin chains and arrays of coupled cavities. The first type consists of an array of spins arranged on a line with nearest neighbour interactions between them while the second one has a similar structure but instead of spins it contains atoms trapped inside optical cavities. For both cases we calculate their efficiency using the Landauer principle. Finally we discuss possible experimental implementations of our ideas. Introduction:-The idea of Maxwell s demon was introduced by James Clerk Maxwell (1831-1879)  1  . It describes a hypothetical intelligent being who could control microscopic particles individually so that they would always move into separate containers depending on whether each particle had a higher energy level or lower energy level  2  .\nMaxwell s demon is usually described as a thought experiment whose purpose is to demonstrate that entropy cannot decrease spontaneously  3  , i.e., that heat does not flow spontaneously from hot bodies to cold ones  4  . However, there exists another interpretation of Maxwell s demon according to which he imagined a device capable of sorting individual molecules based on their velocities  5  . This interpretation leads naturally to the question about what sort of physical system might behave like such a device  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nanodevices and Maxwell s Demon . Abstract : We suggest to use the notion of Maxwell s devil in order to explain how nanodevices can be used for information processing , processing or transmission .We suggest that this methodology is convenient because it allows us to realize why some machines are more efficient than others at performing these tasks . In particular we investigate two forms of nanodevices which have been proposed lately as candidates for quantum computers - spinning chains and arrays of coupled cavities .The first class consists of an array of spinning placed on a line with nearest neighbour interactions between them while the second one has a similar composition but instead of spinning it contains atoms trapped inside optical cavities . For both cases we determine their performance using the Landauer theorem .Finally we explain possible experimental implementations of our concepts . Introduction : - The idea of Maxwell s demon was introduced by James Clerk Maxwell ( 1831 - 1879 ) 1 .It depicts a hypothetical intelligent being who might control microscopic particles individually so that they may always move into independent containers depending on whether each particle had a higher energy level or lower energy level 2 . Maxwell s demon is usually characterized as a thought experiment whose purpose is to demonstrate that entropy cannot decline spontaneously 3 , i . e . , that heat does not flow spontaneously from hot bodies to hot ones 4 .However , there exists another explanation of Maxwell s demon according to which he imagined a device capable of sorting individual molecules based on their velocities 5 . This interpretation turns naturally to the question about what sort of physical system might perform like such a device 6 .",
        "rewrite_text": "**Title: Nanodevices and Maxwell's Demon**\n\n**Abstract:** In this article, we explore the application of the concept of Maxwell's Demon to elucidate the mechanisms by which nanodevices facilitate information processing, transmission, and manipulation. This approach is particularly advantageous as it provides insights into the varying efficiencies of different machines in executing these functions. We focus on two emerging classes of nanodevices that have been identified as potential candidates for quantum computing: spinning chains and arrays of coupled cavities. The first category comprises a linear arrangement of spinning particles that interact with their nearest neighbors, while the second category consists of atoms confined within optical cavities, exhibiting similar structural characteristics. To evaluate the performance of these nanodevices, we employ the Landauer theorem, which relates information processing to thermodynamic principles. Furthermore, we discuss potential experimental implementations of our theoretical framework, highlighting the practical implications of our findings. The concept of Maxwell's Demon, introduced by James Clerk Maxwell in the 19th century, serves as a foundational thought experiment that illustrates the principles of entropy and energy distribution among microscopic particles. Traditionally, this demon is envisioned as an entity capable of sorting particles based on their energy levels, thereby challenging the second law of thermodynamics. However, our interpretation extends beyond this classical view, prompting an investigation into the types of physical systems that could emulate the sorting capabilities of Maxwell's Demon. By bridging the gap between theoretical constructs and practical applications, we aim to enhance the understanding of how nanodevices can be optimized for efficient information processing in the realm of quantum computing.",
        "ori-fast-z-score": -0.4402254531628119,
        "water-fast-z-score": 6.251201434911929,
        "rewrite-fast-z-score": -0.8250286473253902
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Unique Determination of the Effective Potential in Terms of Renormalization Group Functions .\nAbstract:\nWe present an explicit expression for the effective potential in terms of renormalization group functions, which is valid to all orders in perturbation theory and can be used as input into numerical calculations.  We show that this result agrees with previous results obtained by other methods at one-loop order. The new formula has several advantages over existing expressions:  It does not require any additional approximations beyond those inherent in the use of perturbation theory; it allows us to calculate the effective potential directly without having to solve complicated integral equations first; finally, we are able to obtain the full dependence on temperature and chemical potentials simultaneously. This work was supported by the U.S. Department of Energy under Contract No. DE-AC02-76SF00515. In recent years there have been many attempts to determine the effective potential Veff(T) = ln Z(T), where T denotes the temperature, using various techniques such as the imaginary time formalism  1  , the real time formalism  2  or the functional renormalization group (FRG)  3  . These approaches provide useful information about the phase structure of quantum field theories but they usually involve some kind of approximation scheme. For example, in the FRG approach one often uses truncations of the exact flow equation  4  .\nIn this letter we will derive an explicit expression for Veff(T) in terms of renormalisation group functions  5  . Our method is based on the observation  6  that the effective action Γk(φ) (where k denotes the momentum scale) satisfies a differential equation known as the Wetterich equation  7, 8  \nHere Rk(Γk; φ) is called the regulator function and describes how the infrared modes are suppressed when integrating out high energy degrees of freedom. By solving Eq. (1) numerically  9  one obtains the running coupling constants gk(φ). Using these quantities together with the corresponding β-functions one can then compute Veff(T) according to",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Unique Determination of the Effective Potential in Terms of Renormalization Group Functions . Abstract : We present an explicit expression for the effective potential in terms of renormalization group functions , which is valid to all orders in perturbation theory and can be used as input into numerical analyses .We say that this consequence agrees with previous findings obtained by other methods at one - loop order . The revised formula has numerous benefits over existing expressions : It does not require any additional approximations beyond those inherent in the using of perturbation theory ; it allows us to estimate the effective potential directly without having to solve intricate integral equations first ; finally , we are able to obtain the full dependence on pressure and chemical potentials separately .This project was supported by the U . S . Department of Energy under Contract No . DE - AC02 - 76SF00515 .In past decades there have been many efforts to estimate the effective potential Veff ( T ) = ln Z ( T ) , where T denotes the temperature , using numerous algorithms such as the imaginary time formalism 1 , the real time formalism 2 or the functional renormalization group ( FRG ) 3 . These approaches provide useful details about the phase composition of quantum field theories but they generally include some kind of approximation scheme .For instance , in the FRG method one frequently uses truncations of the exact flow formula 4 . In this letter we will generate an explicit expression for Veff ( T ) in terms of renormalisation group functions 5 .Our formula is based on the observation 6 that the effective operation Γk ( φ ) ( where k denotes the velocity scale ) satisfies a differential equation known as the Wetterich equation 7 , 8 Here Rk ( Γk ; φ ) is dubbed the regulator function and explains how the infrared modes are suppressed when integrating out large energy degrees of liberty . By solving Eq .( 1 ) numerically 9 one obtains the running coupling constants gk ( φ ) . Using these quantities combined with the associated β - functions one can then compute Veff ( T ) according to",
        "rewrite_text": "**Title:** A Unique Determination of the Effective Potential in Terms of Renormalization Group Functions\n\n**Abstract:** In this article, we derive a novel expression for the effective potential, denoted as V_eff(T), utilizing renormalization group functions. This formulation is applicable to all orders within perturbation theory, making it a valuable tool for numerical analyses. Our findings align with previous results obtained through alternative methodologies at one-loop order, reinforcing the validity of our approach. The advantages of our revised formula are significant: it eliminates the need for any additional approximations beyond those inherent in perturbation theory, facilitates direct estimation of the effective potential without the necessity of solving complex integral equations, and allows for a comprehensive understanding of the dependence on both pressure and chemical potentials independently.\n\nHistorically, various methods have been employed to estimate the effective potential, including the imaginary time formalism, real-time formalism, and functional renormalization group (FRG) techniques. While these methods provide insightful information regarding the phase structure of quantum field theories, they often rely on approximation schemes. For example, the FRG approach typically involves truncations of the exact flow equations. In contrast, our work presents an explicit formulation for V_eff(T) that leverages the properties of renormalization group functions.\n\nOur derivation is grounded in the observation that the effective action, Γ_k(φ), where k represents the velocity scale, adheres to the Wetterich equation. The regulator function R_k(Γ_k; φ) plays a crucial role in elucidating the suppression of infrared modes when integrating out high-energy degrees of freedom. By numerically solving the Wetterich equation, we can extract the running coupling constants g_k(φ). These coupling constants, in conjunction with their corresponding β-functions, enable us to compute the effective potential V_eff(T) accurately.\n\nThis research was conducted with the support of the U.S. Department of Energy under Contract No. DE-AC02-76SF00515, and it contributes to the ongoing discourse in the field of quantum field theory by providing a robust framework for understanding the effective potential without the limitations of previous approximation methods.",
        "ori-fast-z-score": 0.9198662110077999,
        "water-fast-z-score": 7.442553889063108,
        "rewrite-fast-z-score": 1.6012815380508714
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Origin of the Magellanic Stream and Its Leading Arm .\nAbstract:\nWe present an analysis of the kinematics, metallicity distribution function (MDF), and chemical abundances in the outer halo of our Galaxy using data obtained with the Subaru Telescope for two fields along the leading arm of the Magellanic stream. We find that the MDFs are well represented by three Gaussian components centered at  Fe/H  = -1.7, -0.9, and +0.2 dex, respectively. The metal-poor component is found to be associated with the Galactic thick disk/halo population, while both intermediate- and high-metallicity populations show significant differences between the two fields. In particular, we detect a large fraction of high-alpha stars in one field but not in another located farther away from the center of the LMC. These results suggest that the origin of these streams may have been triggered by tidal interactions between the Milky Way and its satellite galaxies such as the Sgr dwarf galaxy and/or the LMC.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Origin of the Magellanic Stream and Its Leading Arm . Abstract : We present an assessment of the kinematics , metallicity distribution relation ( MDF ) , and material abundances in the exterior halo of our Galaxy using data acquired with the Subaru Telescope for two fields along the led arm of the Magellanic stream .We see that the MDFs are better represented by three Gaussian components centered at Fe / H = - 1 . 7 , - 0 . 9 , and + 0 . 2 dex , respectively . The metal - weak component is found to be involved with the Galactic thick disk / halo population , while both intermediate - and low - metallicity populations display significant variations between the two fields .In particular , we perceive a large fraction of high - alpha stars in one field but not in another situated closer apart from the center of the LMC . These data suggest that the origin of these streams may have been caused by tidal interactions between the Milky Way and its satellite galaxies such as the Sgr dwarf galaxy and / or the LMC .",
        "rewrite_text": "We provide a comprehensive analysis of the kinematics, metallicity distribution function (MDF), and elemental abundances in the outer halo of the Milky Way, utilizing observational data obtained from the Subaru Telescope across two distinct fields along the leading arm of the Magellanic Stream. Our findings indicate that the MDFs are more accurately described by three Gaussian components, with centers at [Fe/H] = -1.7, -0.9, and +0.2 dex. Notably, the metal-poor component appears to be associated with the Galactic thick disk and halo populations. In contrast, the intermediate- and low-metallicity populations exhibit considerable variations between the two observed fields. Specifically, one field reveals a significant presence of high-alpha stars, while the other, located closer to the center of the Large Magellanic Cloud (LMC), does not show the same trend. These observations imply that the formation of the Magellanic Stream may be attributed to tidal interactions between the Milky Way and its satellite galaxies, including the Sagittarius dwarf galaxy and the LMC. This study enhances our understanding of the complex dynamics and evolutionary history of the Magellanic Stream, shedding light on the processes that govern the interactions between galaxies in our local group. The implications of these findings are significant for the broader context of galaxy formation and evolution, as they provide insights into the role of satellite galaxies in shaping the structure and composition of the Galactic halo.",
        "ori-fast-z-score": -1.2309149097933272,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": 0.21081851067789195
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Collision Between The Milky Way And Andromeda .\nAbstract:\nThe collision between the Milky Way and its nearest neighbor, M31 (Andromeda), is predicted to occur in about 4 billion years.  This will be one of the most spectacular events ever witnessed by humans.   In this talk I will describe how we can use observations made with telescopes on Earth as well as space-based observatories such as Hubble Space Telescope to study these collisions and learn more about dark matter, galaxies, stars, black holes, and other cosmic phenomena that are part of our universe. I will also discuss some of my research projects related to studying galaxy mergers using data obtained at the W.M. Keck Observatory located on Mauna Kea, Hawaii. Finally, I ll share what it was like for me to work there during my summer internship last year. Keywords: Dark Matter, Galaxy merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Collision Between The Milky Way And Andromeda . Abstract : The merger between the Milky Way and its closest neighbor , M31 ( Andromeda ) , is predicted to effect in about 4 billion years .This will be one of the most magnificent phenomena ever experienced by humans . In this talk I will explain how we can using observations made with telescopes on Earth as well as space - based observatories such as Hubble Space Telescope to study these collisions and learn more about black material , galaxies , stars , white holes , and other cosmic phenomena that are part of our universe .I will also discuss some of my research projects related to researching galaxy mergers using data acquired at the W . M . Keck Observatory situated on Mauna Kea , Hawaii .Finally , I ll share what it was like for me to work there during my summer assignment last year . Keywords : Dark Matter , Galaxy merger , Black Hole , Cosmic Evolution , Cosmology , Astrophysics",
        "rewrite_text": "Title: The Collision Between The Milky Way and Andromeda\n\nAbstract: The anticipated merger between the Milky Way galaxy and its nearest neighbor, Andromeda (M31), is projected to occur in approximately 4 billion years. This event promises to be one of the most extraordinary cosmic phenomena witnessed by humanity. In this presentation, I will elucidate how we can leverage observations from both terrestrial telescopes and space-based observatories, such as the Hubble Space Telescope, to investigate these galactic collisions. Through these studies, we aim to deepen our understanding of various cosmic elements, including dark matter, galaxies, stars, black holes, and other significant phenomena that shape our universe. Additionally, I will highlight several of my research initiatives focused on galaxy mergers, utilizing data collected at the W. M. Keck Observatory located on Mauna Kea, Hawaii. This experience has provided me with invaluable insights into the complexities of cosmic evolution and the dynamics of galactic interactions. I will also share personal reflections on my summer internship at the observatory, detailing the challenges and rewards of conducting research in such a prestigious environment. This talk will not only cover the scientific implications of the Milky Way-Andromeda collision but also emphasize the importance of collaborative research in advancing our understanding of astrophysics and cosmology. \n\nKeywords: Dark Matter, Galaxy Merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics.",
        "ori-fast-z-score": 0.39735970711951313,
        "water-fast-z-score": 4.464418717230567,
        "rewrite-fast-z-score": -0.9878783399072131
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A WENO Algorithm for the Growth of Ionized Regions at the Reionization Epoch .\nAbstract:\nWe present an algorithm to solve numerically the time-dependent equations describing the growth of ionized regions in the early universe, based on the Weighted Essentially Non-Oscillatory (WENO) scheme. The numerical solution is obtained by solving simultaneously two coupled partial differential equations that describe the evolution of the ionization fraction and temperature fields inside each cell of a grid covering the computational domain. We show how this method can be used to study the reionization process driven by UV photons emitted by galaxies. In particular we compare our results with those obtained using the Smooth Particle Hydrodynamics technique. Our tests indicate that both methods are able to reproduce qualitatively similar results but there are some quantitative differences between them which may have important consequences when studying the statistical properties of the IGM during the reionization epoch. \n \n Keywords: Numerical Methods, Computational Science, Cosmology, Reionization",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A WENO Algorithm for the Growth of Ionized Regions at the Reionization Epoch . Abstract : We create an algorithm to solve numerically the time - dependent equations explaining the development of ionized areas in the early universe , using on the Weighted Essentially Non - Oscillatory ( WENO ) scheme .The mathematical solution is found by solving simultaneously two coupled partial differential equations that explain the evolution of the ionization fraction and heat fields inside each cell of a grid covering the computational domain . We see how this technology can be used to study the reionization process driven by UV photons generated by galaxies .In particular we compare our findings with those achieved using the Smooth Particle Hydrodynamics technique . Our tests show that both approaches are able to reproduce qualitatively comparable results but there are some quantitative variations between them which would have important implications when examining the empirical features of the IGM during the reionization epoch .Keywords: Numerical Methods, Computational Science, Cosmology, Reionization",
        "rewrite_text": "Title: A WENO Algorithm for the Growth of Ionized Regions at the Reionization Epoch\n\nAbstract: In this study, we present a novel algorithm designed to numerically address the time-dependent equations governing the formation of ionized regions in the early universe, employing the Weighted Essentially Non-Oscillatory (WENO) scheme. The core of our approach involves simultaneously solving two coupled partial differential equations that describe the evolution of both the ionization fraction and the thermal fields within each grid cell of our computational domain. This methodology enables us to investigate the reionization process, which is primarily driven by ultraviolet (UV) photons emitted from galaxies. We conduct a comparative analysis of our results against those obtained through the Smooth Particle Hydrodynamics (SPH) technique. Our findings indicate that while both methods yield qualitatively similar outcomes, notable quantitative discrepancies exist. These differences could have significant implications for understanding the empirical characteristics of the intergalactic medium (IGM) during the reionization epoch. By leveraging the strengths of the WENO algorithm, we aim to enhance the precision of simulations related to cosmic reionization, thereby contributing valuable insights into the early universe's evolution. This work underscores the importance of numerical methods in cosmology and highlights the potential for further research in the field of computational science, particularly in relation to the dynamics of ionized regions and their impact on the IGM. \n\nKeywords: Numerical Methods, Computational Science, Cosmology, Reionization",
        "ori-fast-z-score": -2.42535625036333,
        "water-fast-z-score": 3.9727331518303837,
        "rewrite-fast-z-score": -0.5184758473652127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray Luminosity Functions of Normal Galaxies in the GOODS .\nAbstract:\nWe present X-ray luminosity functions (XLFs) for normal galaxies at z=0.5-2, based on deep Chandra observations of the Great Observatories Origins Deep Survey-North field. We use photometric redshifts to select galaxy samples with different stellar masses and star formation rates. The XLFs are derived by fitting the observed number counts using a Schechter function plus an exponential cutoff below Lx=1041 erg s-1. Our results show that the faint-end slope is steeper than -1.0 for all mass bins considered here. For massive galaxies (log(M/Msun)>10), we find no significant evolution between z=1.6 and 0.7. However, there appears to be some evidence for evolution towards lower characteristic luminosities as well as flatter slopes at higher redshift. These results suggest that the most massive galaxies may have evolved into less luminous systems over time.  We also compare our results with previous studies which used shallower data or smaller sample sizes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray Luminosity Functions of Normal Galaxies in the GOODS . Abstract : We create X - ray luminosity functions ( XLFs ) for normal galaxies at z = 0 . 5 - 2 , built on deep Chandra measurements of the Great Observatories Origins Deep Survey - North field .We use photometric redshifts to select galaxy samples with various stellar masses and galaxy formation rates . The XLFs are derived by fitting the observed number counts utilizing a Schechter function plus an exponential cutoff below Lx = 1041 erg s - 1 .Our results show that the faint - end slope is steeper than - 1 . 0 for all mass bins included here . For huge objects ( log ( M / Msun ) > 10 ) , we find no notable evolve between z = 1 . 6 and 0 . 7 .However , there seems to be some evidence for expansion towards lesser characteristic luminosities as well as flatter slopes at higher redshift . These data suggest that the most large galaxies must have evolution into fewer luminous systems over time .We also compare our findings with previous research which using shallower evidence or smaller data numbers .",
        "rewrite_text": "We present an extensive analysis of X-ray luminosity functions (XLFs) for normal galaxies within the redshift range of z = 0.5 to 2, utilizing deep Chandra observations from the Great Observatories Origins Deep Survey - North field. Our study employs photometric redshifts to categorize galaxy samples based on varying stellar masses and galaxy formation rates. The XLFs are constructed by fitting the observed number counts with a Schechter function, incorporating an exponential cutoff for luminosities below Lx = 10^41 erg s^-1. \n\nOur findings reveal that the faint-end slope of the XLF is consistently steeper than -1.0 across all mass bins analyzed. For massive galaxies, specifically those with log(M/Msun) > 10, we observe no significant evolution in the XLF between redshifts z = 1.6 and z = 0.7. However, there is suggestive evidence of a shift towards lower characteristic luminosities and a flattening of the slope at higher redshifts. This indicates that the most massive galaxies may be evolving into less luminous systems over cosmic time.\n\nFurthermore, we compare our results with previous studies that relied on shallower datasets or smaller sample sizes, highlighting the robustness of our findings. This research contributes to the understanding of galaxy evolution and the role of X-ray emissions in characterizing the properties of normal galaxies across different epochs. The implications of these results are significant for the broader field of astrophysics, as they provide insights into the formation and evolution of galaxies in the universe.",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 5.222222222222222,
        "rewrite-fast-z-score": 1.616244071283537
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Renormalization and Effective Actions for General Relativity .\nAbstract:\nThe renormalization group is used to study the effective action in quantum gravity, which describes how matter fields interact with gravitons.  The gravitational field equations are derived by varying this action with respect to the metric tensor. This approach leads to an understanding of why general relativity has no ultraviolet divergences at one-loop order (i.e., when only one graviton loop is included). It also provides insight into the physical meaning of the cosmological constant term that appears in Einstein s field equations. In addition, it allows us to calculate corrections to the classical theory due to higher-order terms in the expansion of the effective action about flat space-time. Finally, we discuss some open problems related to the application of these techniques to more complicated theories such as superstrings or supergravity. The renormalization group is applied to the problem of studying the effective action in quantum-gravitational systems. We derive the gravitational field equations by varying this action with regard to the metric tensor. These results provide insights into the physical meaning of both the cosmological constant appearing in Einstein s field equations and the corrections to the classical theory arising from higher-order terms in the effective-action expansion around flat spacetime.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Renormalization and Effective Actions for General Relativity . Abstract : The renormalization group is utilized to study the effective action in quantum gravitational , which explains how matter fields interact with gravitons .The gravitational field equations are derived by varying this action with regard to the metric tensor . This method results to an knowledge of why general relativity has no ultraviolet divergences at one - loop order ( i . e . , when only one graviton loop is included ) .It additionally offers knowledge into the physical meaning of the cosmological constant term that arises in Einstein s field equations . In addition , it allows us to estimate corrections to the classical theory owing to higher - order terms in the expansion of the effective action about flat space - time .Finally , we explain some open problems related to the implementation of these concepts to more complicated models such as superstrings or supergravity . The renormalization group is applied to the question of examining the effective action in quantum - gravitational systems .We derive the gravitational field equations by varying this action with regard to the metric tensor . These data provide insights into the physical meaning of both the cosmological constant appearing in Einstein s field equations and the corrections to the classical theory arising from lower - order terms in the effective - action expansion around flat spacetime .",
        "rewrite_text": "Title: Renormalization and Effective Actions for General Relativity\n\nAbstract: This article explores the application of the renormalization group to the effective action in the context of quantum gravity, focusing on the interactions between matter fields and gravitons. By varying the effective action with respect to the metric tensor, we derive the gravitational field equations, which elucidate the absence of ultraviolet divergences in general relativity at one-loop order, where only a single graviton loop is considered. This analysis not only sheds light on the physical interpretation of the cosmological constant that appears in Einstein's field equations but also facilitates the estimation of corrections to classical gravitational theory due to higher-order terms in the effective action expansion around flat spacetime. Furthermore, we address several unresolved issues concerning the extension of these concepts to more complex frameworks, such as superstring theory and supergravity. The insights gained from this study contribute to a deeper understanding of the interplay between quantum mechanics and gravitational phenomena, highlighting the significance of the renormalization group in advancing our comprehension of effective actions in quantum-gravitational systems. Through this work, we aim to provide a clearer perspective on the implications of the effective action in general relativity and its potential extensions, paving the way for future research in theoretical physics.",
        "ori-fast-z-score": -0.29002094671369905,
        "water-fast-z-score": 3.73552251236249,
        "rewrite-fast-z-score": 1.0206207261596576
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Riemann zeta-function, Part I: Outline .\nAbstract:\nThe Riemann hypothesis is one of the most famous open problems in mathematics and has been studied for more than 150 years.  In this article we present an outline of the proof that was announced by Andrew Wiles on 16 May 1993 at the International Congress of Mathematicians in Zurich.   The full text can be found here. The Riemann hypothesis is one of  the most famous open problems  in mathematics and has been investigated for over 150 years. It states that all non-trivial zeros of the Riemann zeta function have real part equal to 1/2. This theorem will be proved using the theory of modular forms. A modular form is a complex-valued function defined on the group SL(2,Z) which satisfies certain conditions. We begin with some basic definitions and properties of modular forms before moving onto the main topic of the article - the proof of the Riemann hypothesis given by Andrew Wiles. For further reading see  1  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Riemann zeta - function , Part I : Outline . Abstract : The Riemann conjecture is one of the most famous open problems in math and has been studied for more than 150 years .In this page we present an outline of the proof that was announced by Andrew Wiles on 16 May 1993 at the International Congress of Mathematicians in Zurich . The full text can be found here .The Riemann conjecture is one of the most famous open problems in math and has been investigated for over 150 years . It says that all non - trivial zeros of the Riemann zeta polynomial have real part equal to 1 / 2 .This theorem will be proved using the principle of modular forms . A modular structure is a complex - valued function given on the group SL ( 2 , Z ) which satisfies certain conditions .We begin with some fundamental definitions and features of modular forms before moving onto the main theme of the article - the proof of the Riemann hypothesis given by Andrew Wiles . For further reading see 1 .",
        "rewrite_text": "Title: On the Riemann Zeta-Function, Part I: Outline\n\nAbstract: The Riemann Hypothesis stands as one of the most renowned unsolved problems in mathematics, captivating the attention of mathematicians for over 150 years. This article provides a comprehensive outline of the proof presented by Andrew Wiles on May 16, 1993, during the International Congress of Mathematicians held in Zurich. The complete proof can be accessed through the provided link. The Riemann Hypothesis posits that all non-trivial zeros of the Riemann zeta function possess a real part equal to 1/2. The proof of this conjecture will be established through the application of modular forms, which are complex-valued functions defined on the group SL(2, Z) that adhere to specific criteria. We commence with essential definitions and characteristics of modular forms, laying the groundwork for a deeper understanding of the topic. Subsequently, we delve into the core focus of this article: the proof of the Riemann Hypothesis as articulated by Wiles. This outline serves as a precursor to a more detailed exploration of the intricate relationship between modular forms and the Riemann zeta function, ultimately contributing to the ongoing discourse surrounding this pivotal mathematical conjecture. For those interested in further exploration of this topic, additional resources are available for consultation.",
        "ori-fast-z-score": 0.7385489458759964,
        "water-fast-z-score": 4.275930552470682,
        "rewrite-fast-z-score": 0.3418817293789138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Discovery of an Isolated Compact Object at High Galactic Latitude .\nAbstract:\nWe report the discovery of a new isolated neutron star (INS) candidate, PSR J1852+0040, in the southern hemisphere using data collected by the Parkes radio telescope and the Australia Telescope Compact Array (ATCA). The pulsar was discovered during a search for millisecond pulsars with high proper motions. It has a spin period P = 1.4 ms and is located at a distance D = 3 kpc. Its dispersion measure DM = 0.6 pc cm-3 implies that it lies behind most of the galactic disk but not far enough to be associated with any known supernova remnant or open cluster. We have also detected its X-ray counterpart in archival Chandra observations. This source appears point-like and shows no evidence of extended emission. Based on these properties we conclude that this object is likely to be a young INS. If confirmed as such, our results will provide important constraints on models of pulsar formation and evolution. \n \n Keywords: Neutron stars",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Discovery of an Isolated Compact Object at High Galactic Latitude . Abstract : We report the discovery of a new small neutron star ( INS ) candidate , PSR J1852 + 0040 , in the southern hemisphere using data taken by the Parkes radio telescope and the Australia Telescope Compact Array ( ATCA ) .The pulsar was discovered during a search for millisecond pulsars with high proper motions . It has a spinning period P = 1 . 4 ms and is situated at a distance D = 3 kpc .Its dispersion measure DM = 0 . 6 pc mm - 3 implies that it lies behind most of the galactic disk but not far enough to be identified with any observed supernova remnant or open cluster . We have already detected its X - ray counterpart in archival Chandra measurements .This source looks point - like and shows no evidence of extended emission . Based on these characteristics we estimate that this object is probably to be a young INS .If confirmed as such , our findings will provide important restrictions on estimates of pulsar structure and evolution . Keywords : Neutron stars",
        "rewrite_text": "We present the discovery of a new candidate for an isolated neutron star (INS), designated PSR J1852 + 0040, located in the southern hemisphere. This discovery was made utilizing data from the Parkes radio telescope and the Australia Telescope Compact Array (ATCA) during a targeted search for millisecond pulsars exhibiting high proper motions. The pulsar has a remarkably short spin period of P = 1.4 milliseconds and is estimated to be situated at a distance of D = 3 kiloparsecs. The measured dispersion measure (DM) of 0.6 pc cm^-3 indicates that PSR J1852 + 0040 is positioned behind the majority of the galactic disk, yet it is not sufficiently distant to be associated with any known supernova remnants or open clusters. Furthermore, we have identified its X-ray counterpart through archival data from Chandra observations. The X-ray source appears point-like, with no signs of extended emission, which supports the hypothesis that this object is likely a young INS. If our classification is validated, this discovery will significantly enhance our understanding of pulsar structure and evolution, providing critical constraints on theoretical models. The implications of this finding are profound, as it contributes to the broader understanding of neutron stars and their formation processes. This research underscores the importance of continued observations and studies of isolated compact objects in the galaxy. Keywords: Neutron stars.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": -0.3110855084191276
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lyman Alpha Emitters in the Hierarchically Clustering Galaxy Formation .\nAbstract:\nWe present an analysis of Lyman alpha emitters (LAEs) selected by narrowband imaging with Subaru/Suprime-Cam and spectroscopic follow-up observations using VLT/VIMOS at z ~ 3.1, which is one of the most active epochs for galaxy formation. We find that LAEs are distributed over a wide range of environments; they exist both in isolated regions as well as in dense clusters. The clustering properties of LAEs depend on their luminosities. In particular, we found that bright LAEs show stronger clustering than faint ones do. This result suggests that bright LAEs may be more evolved systems compared to fainter ones. Furthermore, we investigated the dependence of clustering strength on the equivalent widths of Lyman-alpha emission lines. Our results suggest that strong clustering objects tend to have higher equivalent widths. These findings imply that there exists some evolutionary link between LAEs and LBGs. \n \n Keywords: Lyman alpha emitter",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lyman Alpha Emitters in the Hierarchically Clustering Galaxy Formation . Abstract : We present an assessment of Lyman alpha emitters ( LAEs ) selected by narrowband scanning with Subaru / Suprime - Cam and spectroscopic follow - up observations using VLT / VIMOS at z ~ 3 . 1 , which is one of the most important epochs for galaxy formation .We see that LAEs are distributed over a broad variety of habitats ; they exist both in isolated regions as well as in dense clusters . The clustering qualities of LAEs depend on their luminosities .In particular , we reported that bright LAEs see better clustering than bright ones do . This result suggests that bright LAEs may be more evolved structures versus to fainter ones .Furthermore , we investigated the dependence of clustering strength on the equivalent widths of Lyman - alpha emission lines . Our results show that strong clustering objects prefer to have greater equal widths .These studies imply that there exists some evolutionary link between LAEs and LBGs . Keywords : Lyman alpha emitter",
        "rewrite_text": "Title: Lyman Alpha Emitters in Hierarchical Galaxy Formation\n\nAbstract: In this study, we evaluate Lyman alpha emitters (LAEs) identified through narrowband imaging with Subaru/Suprime-Cam, complemented by spectroscopic follow-up using VLT/VIMOS at a redshift of approximately z ~ 3.1, a critical period for galaxy formation. Our findings reveal that LAEs inhabit a diverse range of environments, being found in both isolated areas and densely populated clusters. Notably, we observe that the clustering characteristics of LAEs are influenced by their luminosity, with brighter LAEs exhibiting stronger clustering tendencies compared to their fainter counterparts. This observation suggests that brighter LAEs may represent more evolved structures in the context of galaxy formation. Additionally, we explore the relationship between clustering strength and the equivalent widths of Lyman-alpha emission lines. Our analysis indicates that objects with strong clustering are associated with larger equivalent widths, further supporting the notion of an evolutionary connection between LAEs and Lyman Break Galaxies (LBGs). These insights contribute to our understanding of the role of LAEs in the hierarchical model of galaxy formation and evolution, highlighting their significance in the broader cosmic landscape. \n\nKeywords: Lyman alpha emitter, galaxy formation, clustering, luminosity, equivalent width, Lyman Break Galaxies.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 1.2375966910186262
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Force distributions and force chains in random stiff fiber networks .\nAbstract:\nWe study the mechanical properties of disordered networks of fibers with different bending rigidities by means of numerical simulations based on molecular dynamics (MD) and Monte Carlo (MC). We find that the elastic moduli are strongly dependent on the bending rigidity, which is related to the persistence length of the fibers. The results show that the shear modulus increases as the bending rigidity decreases while the bulk modulus remains almost unchanged for all values of the bending rigidity studied here. In addition we observe that the distribution functions of forces acting between particles depend significantly on the bending rigidity. For small bending rigidities there exist strong correlations among neighboring particles leading to large fluctuations in the local stress field. These findings suggest that the microscopic structure plays an important role in determining macroscopic material properties such as elasticity. Disordered networks of fibers have been widely used to model various materials ranging from biological tissues to granular media  1-3 . However, it has not yet been fully understood how the microstructure affects their macroscopic behavior. Here we investigate this problem using computer simulation techniques.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Force distributions and force chains in random rigid fiber networks . Abstract : We research the mechanical behavior of disordered systems of fibers with various twisting rigidities by means of computational simulations based on molecular mechanics ( MD ) and Monte Carlo ( MC ) .We see that the elastic moduli are strongly dependent on the shear rigidity , which is related to the persistence length of the materials . The results show that the shear modulus increases as the twisting rigidity decreases while the bulk modulus remains virtually unchanged for all values of the twisting rigidity examined here .In addition we find that the distribution functions of pressures acting between particles depend greatly on the bending rigidity . For small bending rigidities there exist strong correlations among neighboring objects resulting to large fluctuations in the local tension field .These studies imply that the microscopic shape plays an important role in measuring macroscopic material structures such as elasticity . Disordered networks of fibers have been widely using to model numerous structures ranging from biological tissues to granular material 1 - 3 .However , it has not already been clearly explained how the microstructure affects their macroscopic behavior . Here we investigate this question using computer simulation tools .",
        "rewrite_text": "In this study, we investigate the mechanical properties of disordered fiber networks characterized by varying degrees of twisting rigidity through computational simulations utilizing molecular dynamics (MD) and Monte Carlo (MC) methods. Our findings reveal a significant dependence of the elastic moduli on shear rigidity, which is intrinsically linked to the persistence length of the fibers. Specifically, we observe that as twisting rigidity decreases, the shear modulus exhibits a corresponding increase, while the bulk modulus remains relatively constant across the range of twisting rigidities analyzed. Furthermore, we explore the distribution functions of pressures exerted between particles, which are found to be highly sensitive to the bending rigidity of the fibers. In cases of low bending rigidity, we identify strong correlations among adjacent fibers, leading to pronounced fluctuations in the local tension field. These results underscore the critical influence of microscopic fiber shapes on the macroscopic mechanical properties of the material, particularly in terms of elasticity. Disordered fiber networks serve as effective models for a variety of structures, including biological tissues and granular materials. Despite their widespread application, the relationship between microstructural characteristics and macroscopic behavior has not been thoroughly elucidated. This research aims to bridge that gap by employing advanced computational simulation techniques to provide insights into how microstructural variations impact the overall mechanical response of fiber networks. Our findings contribute to a deeper understanding of the fundamental mechanics governing disordered systems and have implications for the design and optimization of materials in various fields, including biomechanics and materials science.",
        "ori-fast-z-score": -1.2935483472729858,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 1.9090909090909092
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The competition of hydrogen-like and isotropic interactions on polymer collapse .\nAbstract:\nWe study the effect of competing short-range attractive and repulsive interactions in a model for collapsed polymers, which are described by an effective one-dimensional potential with two minima separated by a barrier. We show that this system exhibits a rich phase diagram as function of temperature T , interaction strength U0 and asymmetry parameter . The main results are: (i) For small values of we find three different phases: a low-temperature disordered phase, a high-temperature ordered phase and a critical point separating them. (ii) In the limit of large barriers between the wells, i.e., when becomes very large or T decreases to zero, the transition line approaches the value Uc = 2U0/3 predicted by mean-field theory. \n \n Introduction \n \n Collapsed polymers have been studied extensively over many years  1 - 6 . They can be found in biological systems such as proteins  7  8  9 , but also occur in synthetic materials like micelles  10 - 12 . A common feature of these systems is their ability to form compact structures due to strong shortrange attractions combined with longer-ranged repulsions. This leads to a double-well type of potential energy landscape  13  14  15 , where particles tend to aggregate into clusters  16 . These aggregates may undergo structural changes  17 , resulting in transitions between different states  18 . Such phenomena are often observed experimentally  19  20  21   22 . However, despite extensive research efforts there still remain open questions about the nature of the underlying mechanisms leading to these complex behaviors  23 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The competition of hydrogen - like and isotropic interactions on polymer collapse . Abstract : We research the impact of competing short - range attractive and repulsive molecules in a theory for crumpled polymers , which are presented by an efficient one - dimensional potential with two minima separated by a barrier .We see that this process exhibits a rich phase diagram as function of temperature T , coupling strength U0 and asymmetry parameter . The main results are : ( i ) For small values of we find three different stages : a high - temperature disordered phase , a high - temperature ordered phase and a critical point connecting them .( ii ) In the limit of large obstacles between the wells , i . e . , when becomes very huge or T decreases to zero , the transition line approaches the value Uc = 2U0 / 3 predicted by mean - field model . Introduction Collapsed polymers have been studied frequently over numerous years 1 - 6 .They can be found in biological systems such as proteins 7 8 9 , but also occur in artificial devices like micelles 10 - 12 . A popular characteristic of these systems is their power to form compact shapes attributed to powerful shortrange attractions coupled with shorter - ranged repulsions .This leads to a double - well type of potential energy landscape 13 14 15 , where fragments tend to aggregate into clusters 16 . These aggregates might undergo structural modifications 17 , resulting in transitions between various states 18 .Such behaviors are often observed experimentally 19 20 21 22 . However , despite extensive research efforts there still continue open questions about the nature of the underlying mechanisms leading to these complex behaviors 23 .",
        "rewrite_text": "**Title:** The Competition of Hydrogen-Like and Isotropic Interactions on Polymer Collapse\n\n**Abstract:** This study investigates the effects of competing short-range attractive and repulsive interactions on the behavior of crumpled polymers, modeled through an efficient one-dimensional potential characterized by two minima separated by a barrier. Our analysis reveals a complex phase diagram that varies with temperature (T), coupling strength (U0), and an asymmetry parameter. The key findings of our research include: (i) At low values of the asymmetry parameter, we identify three distinct phases: a disordered phase at high temperatures, an ordered phase at high temperatures, and a critical point that connects these two states. (ii) In scenarios where the barriers between the potential wells are significantly large—either when the asymmetry parameter becomes very large or as the temperature approaches zero—the transition line converges towards the value Uc = 2U0 / 3, which aligns with predictions from mean-field theory. \n\n**Introduction:** The phenomenon of collapsed polymers has garnered significant attention over the years due to their prevalence in both biological systems, such as proteins, and synthetic applications, including micelles. A notable feature of these systems is their ability to adopt compact configurations, a behavior driven by strong short-range attractive forces in conjunction with shorter-ranged repulsive interactions. This interplay results in a double-well potential energy landscape, where polymer fragments tend to aggregate into clusters. These clusters can undergo structural transformations, leading to transitions among various states, a behavior frequently observed in experimental settings. Despite the wealth of research conducted in this area, fundamental questions remain regarding the mechanisms that govern these complex behaviors, highlighting the need for further exploration into the dynamics of polymer collapse.",
        "ori-fast-z-score": 1.2094157958139042,
        "water-fast-z-score": 7.143502616338124,
        "rewrite-fast-z-score": 2.5923502310219813
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Did time begin? Will time end? .\nAbstract:\nThe question whether time is an illusion or not has been debated for centuries, and the answer remains elusive.  In this article we will examine some of the arguments that have been put forward in favour of the existence of time as well as those against it.   We will then consider how these arguments relate to our understanding of space-time and its relationship with matter.   Finally, we will look at how quantum mechanics may provide us with new insights into the nature of time. The question  Does time exist?  has been asked many times throughout history by philosophers, scientists and ordinary people alike. It seems like such a simple question but there are actually several different ways one can interpret what  time  means when asking about its existence. For example, if you ask yourself  Is time real?  your answer might be yes because you think that time is something that exists independently of any observer (i.e., outside of ourselves). On the other hand, if you were to ask yourself  Do I experience time passing?  your answer would probably be no since you believe that only humans experience time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Did time commence ? Will time end ?. Abstract : The question whether period is an illusion or not has been discussed for centuries , and the response remains elusive .In this article we will investigate some of the proposals that have been put forward in favour of the existence of time as well as those against it . We will then discuss how these objections relate to our grasp of space - time and its connection with matter .Finally , we will look at how quantum mechanics may provide us with novel knowledge into the nature of time . The question Does time exist ?has been wondered various times throughout history by philosophers , scientists and normal people alike . It sounds like such a simple question but there are apparently several different means one can comprehend what time implies when asking about its existence .For instance , if you asking yourself Is time real ? your answer might be yes because you believe that time is something that exists independently of any observer ( i . e . , outside of ourselves ) .On the other hand , if you were to ask yourself Do I feel life passing ? your answer would probably be no since you believe that only people process time .",
        "rewrite_text": "Title: Did Time Commence? Will Time End?\n\nAbstract: The inquiry into whether time is merely an illusion has captivated thinkers for centuries, yet a definitive answer remains elusive. This article delves into various perspectives regarding the existence of time, examining both arguments that support its reality and those that challenge it. We will explore how these differing viewpoints relate to our understanding of space-time and its intricate relationship with matter. Additionally, we will investigate the potential insights that quantum mechanics may offer regarding the fundamental nature of time. The question, \"Does time exist?\" has been pondered by philosophers, scientists, and laypeople alike throughout history. Although it appears straightforward, the concept of time encompasses a multitude of interpretations that complicate its existence. For instance, if one were to ask, \"Is time real?\" they might affirm its existence, believing it to be an entity that exists independently of human perception. Conversely, if the question posed were, \"Do I perceive life as passing?\" the response might lean towards a negation, as individuals often perceive time as a subjective experience tied to consciousness. Through a comprehensive analysis of these contrasting views, this article aims to shed light on the complexities surrounding the concept of time, ultimately seeking to enhance our understanding of its role in the universe. By integrating philosophical discourse with scientific inquiry, we hope to provide a nuanced perspective on the existence of time and its implications for our comprehension of reality.",
        "ori-fast-z-score": 0.9138115486202573,
        "water-fast-z-score": 6.8,
        "rewrite-fast-z-score": 1.756550621379892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  INTEGRAL high energy detection of the transient IGR J11321-5311 .\nAbstract:\nWe report on INTEGRAL observations of an X-ray transient discovered by Swift/BAT in December 2006 and detected at hard X-rays with IBIS/ISGRI up to 100 keV during its outbursts between January 2007 and March 2008. The source was also observed simultaneously by RXTE, Suzaku, Chandra and XMM-Newton telescopes which allowed us to determine its position as RA = 11 h 32 m 01 s .6 Dec = -53°10′19′′ (J2000) with an uncertainty radius of 1 arcsec. We show that this new source is likely associated with the optical counterpart VLT/VLBA J1131-5321 previously identified by Chatfield et al. (2007). Its spectrum can be described by a power law model with photon index Γ ~ 2.5-3.0 absorbed by NH ~ 5 × 1021 cm-2. No pulsations were found down to 3 ms resolution using data obtained with RXTE PCA and HEXTE instruments.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : INTEGRAL high energy detection of the transient IGR J11321 - 5311 . Abstract : We report on INTEGRAL observations of an X - ray transient produced by Swift / BAT in December 2006 and detected at hard X - radiation with IBIS / ISGRI up to 100 keV during its outbursts between January 2007 and March 2008 .The source was also observed concurrently by RXTE , Suzaku , Chandra and XMM - Newton telescopes which allowed us to obtain its position as RA = 11 h 32 m 01 s . 6 Dec = - 53°10 ′ 19 ′ ′ ( J2000 ) with an uncertainty diameter of 1 arcsec . We suggest that this new source is probably associated with the optical counterpart VLT / VLBA J1131 - 5321 previously described by Chatfield et al .( 2007 ) . Its spectrum can be described by a power law description with photon index Γ ~ 2 . 5 - 3 . 0 absorbed by NH ~ 5 × 1021 cm - 2 .No pulsations were found down to 3 ms resolution utilizing information obtained with RXTE PCA and HEXTE observations .",
        "rewrite_text": "We present findings from INTEGRAL observations of the X-ray transient IGR J11321-5311, which was initially detected by Swift/BAT in December 2006. This source exhibited significant hard X-ray emissions, with IBIS/ISGRI capturing radiation up to 100 keV during its outbursts from January 2007 to March 2008. Concurrent observations were conducted using several other telescopes, including RXTE, Suzaku, Chandra, and XMM-Newton, which enabled us to accurately determine the position of the source. The coordinates were established as RA = 11 h 32 m 01 s.6 and Dec = -53°10′19′′ (J2000), with a positional uncertainty of just 1 arcsecond. Based on our analysis, we propose that this newly identified source is likely associated with the optical counterpart VLT/VLBA J1131-5321, as previously reported by Chatfield et al. (2007). The spectral characteristics of IGR J11321-5311 can be effectively modeled using a power law, with a photon index in the range of Γ ~ 2.5 - 3.0, and it is subject to absorption characterized by NH ~ 5 × 10^21 cm^-2. Notably, our investigations did not reveal any pulsations down to a resolution of 3 ms, utilizing data from RXTE PCA and HEXTE observations. These findings contribute to our understanding of transient X-ray sources and their associated phenomena, highlighting the importance of multi-wavelength observations in characterizing such celestial events.",
        "ori-fast-z-score": -0.9615239476408232,
        "water-fast-z-score": 2.9938207967349952,
        "rewrite-fast-z-score": 0.3464101615137754
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bond formation and slow heterogeneous dynamics in adhesive spheres with long--ranged repulsion: Quantitative test of Mode Coupling Theory .\nAbstract:\nWe study the glass transition of an ensemble of adhesive hard spheres with repulsive interactions decaying as 1/r6, where r is distance between particles. We find that this system exhibits two distinct relaxation processes at low temperatures: one fast process associated with local rearrangements within clusters of strongly bonded particles; another slower process related to collective motion of these clusters. The latter process can be described by mode-coupling theory (MCT) for colloidal suspensions. However, we show that MCT fails quantitatively when applied directly to our data because it does not take into account the presence of strong bonds which lead to additional slow modes. By introducing a simple modification to MCT, we are able to obtain excellent agreement with experimental results over several decades in time and frequency. This modified version of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg. Our work shows how quantitative tests of theoretical predictions may help improve their accuracy and applicability range.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bond formation and slow heterogeneous dynamics in adhesive spheres with long - - ranged repulsion : Quantitative test of Mode Coupling Theory . Abstract : We study the glass transition of an ensemble of adhesive hard particles with repulsive interactions decaying as 1 / r6 , where p is distance between particles .We see that this process exhibits two different relaxation processes at low temperatures : one fast process associated with local rearrangements within clusters of highly bonded particles ; another slower process related to collective motion of these clusters . The latter cycle can be described by mode - coupling theory ( MCT ) for colloidal suspensions .However , we prove that MCT fails quantitatively when applied directly to our information because it does not take into consideration the presence of stable bonds which lead to extra slow modes . By introducing a simple change to MCT , we are able to obtain excellent compliance with experimental results over numerous years in time and frequency .This updated form of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg . Our research shows how rigorous trials of theoretical estimates may improve improve their accuracy and applicability range .",
        "rewrite_text": "In this study, we investigate the glass transition behavior of an ensemble of adhesive hard spheres characterized by repulsive interactions that diminish with distance as 1/r^6. Our findings reveal the presence of two distinct relaxation processes at low temperatures. The first process is a rapid one, linked to local rearrangements occurring within clusters of strongly bonded particles. In contrast, the second process is significantly slower and pertains to the collective motion of these clusters. This slower dynamic can be effectively described using Mode Coupling Theory (MCT) as it applies to colloidal suspensions. However, our analysis demonstrates that the traditional application of MCT falls short quantitatively when directly applied to our data, primarily because it overlooks the influence of stable bonds that introduce additional slow modes into the system. To address this limitation, we propose a modified version of MCT that incorporates these stable bonds, resulting in a model that aligns remarkably well with experimental observations across a wide range of timescales and frequencies. Furthermore, this revised MCT framework accurately predicts the temperature dependence of the structural relaxation time as the system approaches the glass transition temperature (Tg). Our research underscores the importance of rigorously testing theoretical models against empirical data, highlighting how such evaluations can enhance the precision and applicability of theoretical predictions in the study of complex materials.",
        "ori-fast-z-score": -0.4,
        "water-fast-z-score": 5.5448262406693765,
        "rewrite-fast-z-score": -0.6704783996548059
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deformed Wigner crystal in a one-dimensional quantum dot .\nAbstract:\nWe study the ground state properties of an interacting electron gas confined to a onedimensional (1D) quantum dot with parabolic confinement potential and repulsive Coulomb interaction between electrons. We show that, for sufficiently strong confining potential, the system undergoes a phase transition into a deformed Wigner crystal at low temperatures. The results are obtained by using density functional theory within the local spin-density approximation combined with exact diagonalization method. In this regime, we find that the charge distribution is characterized by alternating peaks separated by valleys which become more pronounced as temperature decreases. This behavior can be understood in terms of formation of a periodic structure due to inter-particle correlations. Our results suggest that such structures may exist experimentally in semiconductor nanowires or carbon nanotubes. Introduction:-In recent years there has been considerable interest in studying the electronic properties of nanostructures  1  . One dimensional systems have attracted particular attention because they provide a unique opportunity to investigate fundamental physical phenomena like Luttinger liquid  2  , fractional statistics  3  , and Wigner crystallization  4  .\nTheoretical studies of 1D quantum dots (QDs), i.e., QDs with only one dimension much smaller than other two dimensions, were first carried out by Lieb et al  5  who showed that these systems exhibit interesting features including shell filling effects  6  . Subsequently, several authors studied various aspects of QD physics  7, 8  . For example, it was shown that the energy spectrum of a QD depends strongly on its shape  9  . It also turns out that the single particle wave functions of a QD depend sensitively on the boundary conditions  10  . Recently, some experimental progress has been made towards realizing 1D QDs  11  -  13  . However, most experiments so far have focused mainly on transport measurements  14  -  16  rather than direct imaging  17  . Therefore, theoretical investigations play an important role in understanding the underlying physics of these systems  18  -  20  .\nIn this work, we consider a model consisting of N non-interacting fermions confined to a 1D QD with parabolic confinement potential V(x). The total energy E tot = ∑ i=1...N",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deformed Wigner crystal in a one - dimensional quantum dot . Abstract : We research the ground state properties of an interacting electron gas restricted to a onedimensional ( 1D ) quantum dot with parabolic confinement potential and repulsive Coulomb interaction between electrons .We see that , for enough strong confining potential , the system undergoes a phase shift into a deformed Wigner crystal at low temperatures . The results are derived by using density functional theory within the local spin - density algorithm coupled with exact diagonalization technique .In this regime , we find that the charge distribution is characterized by alternating ridges separated by regions which turn more pronounced as temperature changes . This phenomenon can be understood in terms of formation of a periodic structure owing to inter - particle correlations .Our results propose that such complexes may arise experimentally in semiconductor nanowires or carbon nanotubes . Introduction : - In recent years there has been substantial interest in investigating the electronic properties of nanostructures 1 .One dimensional systems have garnered especially attention because they give a unique opportunity to examine fundamental physical phenomena like Luttinger vacuum 2 , fractional statistics 3 , and Wigner crystallization 4 . Theoretical experiments of 1D quantum dots ( QDs ) , i . e . , QDs with only one dimension much smaller than other two dimensions , were first done out by Lieb et al 5 who demonstrated that these systems exhibit exciting elements including shell filling effects 6 .Subsequently , various scientists researched several elements of QD physics 7 , 8 . For instance , it was shown that the power spectrum of a QD varies strongly on its shape 9 .It also turns out that the single particle wave systems of a QD depend sensitively on the boundary conditions 10 . Recently , some experimental development has been achieved towards realizing 1D QDs 11 - 13 .However , most studies so far have concentrated mostly on transport measurements 14 - 16 rather than direct scanning 17 . Therefore , theoretical investigations play an important role in understanding the fundamental theory of these systems 18 - 20 .In this work , we study a theory consisting of N non - interacting fermions confined to a 1D QD with parabolic confinement potential V ( x ) . The total energy E tot = [UNK] i = 1 . . . N",
        "rewrite_text": "**Title:** Deformed Wigner Crystal in a One-Dimensional Quantum Dot\n\n**Abstract:** This study investigates the ground state characteristics of an interacting electron gas confined within a one-dimensional (1D) quantum dot, characterized by a parabolic confinement potential and repulsive Coulomb interactions among electrons. Our findings indicate that, under sufficiently strong confinement, the system transitions into a deformed Wigner crystal phase at low temperatures. We employ density functional theory, utilizing the local spin-density approximation in conjunction with exact diagonalization techniques to derive our results. In this low-temperature regime, we observe that the charge distribution manifests as alternating ridges, with the spacing between these ridges becoming more pronounced as the temperature varies. This behavior can be interpreted as the emergence of a periodic structure driven by inter-particle correlations. Our results suggest that such deformed Wigner crystals could potentially be realized in experimental setups involving semiconductor nanowires or carbon nanotubes.\n\n**Introduction:** Recent years have seen a surge of interest in the electronic properties of nanostructures, particularly one-dimensional systems, which provide a unique platform for exploring fundamental physical phenomena such as the Luttinger vacuum, fractional statistics, and Wigner crystallization. Theoretical investigations into 1D quantum dots (QDs)—where one dimension is significantly smaller than the other two—were pioneered by Lieb et al., who highlighted the intriguing features of these systems, including shell filling effects. Following this, numerous researchers have delved into various aspects of QD physics, revealing that the power spectrum of a QD is highly sensitive to its geometric configuration. Additionally, the single-particle wave functions within a QD are influenced by boundary conditions. Recent experimental advancements have made strides toward the realization of 1D QDs; however, much of the existing research has focused primarily on transport measurements rather than direct scanning techniques. Consequently, theoretical studies remain crucial for deepening our understanding of the fundamental principles governing these systems. In this work, we analyze a theoretical framework comprising N non-interacting fermions confined in a 1D QD with a parabolic confinement potential, aiming to elucidate the intricate behaviors of such systems.",
        "ori-fast-z-score": -1.507556722888818,
        "water-fast-z-score": 6.6332495807108,
        "rewrite-fast-z-score": 0.1543033499620919
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolutionary games on minimally structured populations .\nAbstract:\nWe study evolutionary games played by agents that are randomly distributed over the nodes of an unstructured network, and interact only with their immediate neighbors in this network. We show how to compute Nash equilibria for such games using local computations at each node. Our approach is based on computing best responses locally at every agent s neighborhood, which we then use as input to a global algorithm that computes all Nash equilibria. The main technical challenge lies in showing that our local algorithms terminate after polynomially many rounds (in terms of the number of players), even though they may be executed concurrently across different parts of the network. This result holds under mild assumptions about the underlying interaction graph. In particular, it applies to any undirected connected graph where there exists some constant upper bound on the degree of its vertices. As a consequence, we obtain polynomial-time algorithms for finding Nash equilibria in several classes of games including congestion games, matching pennies games, and coordination games.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolutionary players on minimally organized populations . Abstract : We research evolutionary players played by agents that are randomly distributed over the nodes of an unstructured system , and interact only with their immediate relatives in this network .We see how to compute Nash equilibria for such players using local computations at each node . Our model is based on computing best responses locally at every agent s neighborhood , which we then use as input to a global algorithm that computes all Nash equilibria .The main technical challenge lies in proof that our local strategies end after polynomially many tournaments ( in terms of the number of participants ) , albeit though they may be executed concurrently across different components of the organization . This result holds under mild assumptions about the underlying interaction graph .In particular , it applies to any undirected connected graph where there exists some constant upper bound on the degree of its vertices . As a consequence , we obtain polynomial - time algorithms for finding Nash equilibria in multiple types of games including congestion games , matching pennies games , and coordination games .",
        "rewrite_text": "In this study, we investigate the dynamics of evolutionary players situated within minimally organized populations, where agents are randomly distributed across the nodes of an unstructured network. These agents engage in interactions exclusively with their immediate neighbors, leading to a localized form of strategic decision-making. Our approach focuses on the computation of Nash equilibria through local computations performed at each node, where agents assess their best responses based on the actions of those within their vicinity. This local information is subsequently utilized in a global algorithm designed to determine all possible Nash equilibria within the system.\n\nA significant aspect of our research is the demonstration that the local strategies employed by agents converge after a polynomial number of tournaments, which is proportional to the number of participants involved. This convergence is noteworthy as it occurs even when strategies are executed concurrently across various components of the network. Our findings are robust and hold under relatively mild conditions pertaining to the structure of the interaction graph. Specifically, the results are applicable to any undirected connected graph that maintains a constant upper limit on the degree of its vertices.\n\nAs a direct outcome of our analysis, we present polynomial-time algorithms capable of identifying Nash equilibria across a diverse array of game types, including congestion games, matching pennies games, and coordination games. This advancement not only enhances our understanding of strategic interactions in minimally organized populations but also provides practical computational tools for analyzing complex systems in evolutionary game theory. Overall, our work contributes to the broader field of evolutionary dynamics by elucidating the interplay between local interactions and global equilibria in unstructured environments.",
        "ori-fast-z-score": 0.5184758473652127,
        "water-fast-z-score": 4.458892287340829,
        "rewrite-fast-z-score": 1.6903085094570331
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Solutions of fractional reaction-diffusion equations in terms of the H-function .\nAbstract:\nWe present an explicit formula for solutions to some classes of nonlinear fractional diffusion equations with variable coefficients and nonlocal initial conditions, which are expressed by means of the generalized Mittag-Leffler function Eα(−z) and the H-function. The results obtained can be used as a basis for numerical methods for solving such problems. We also discuss applications of our approach to the description of anomalous transport processes in complex systems. Introduction.\nThe theory of fractional calculus has been developed rapidly during recent years due to its numerous applications in various fields of science and engineering (see e.g.,  1  -  4  ). In particular, it is widely applied to describe anomalous transport phenomena observed in many physical models  5  . For example, this approach was successfully employed to model subdiffusion  6  , superdiffusion  7  , and Lévy flights  8  .\nIn general, fractional differential equations have no exact analytical solution  9  . Therefore, there exists great interest in developing efficient numerical algorithms for their approximate treatment  10  . However, even if we use powerful computers, these approaches may not always provide sufficiently accurate results  11  . This problem becomes especially important when one deals with strongly nonlinear problems  12  or considers very large time intervals  13  . To overcome difficulties associated with the lack of exact solutions, several authors proposed different techniques based on special functions  14  -  16  . Among them, the most popular ones include the Mittag-Leffler  17  and Wright  18  functions, Fox s H-functions  19  , and others  20  -  22  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Solutions of fractional process - diffusion equations in terms of the H - function . Abstract : We present an explicit formula for solutions to some categories of nonlinear fractional absorption equations with variable coefficients and nonlocal initial conditions , which are stated by means of the generalized Mittag - Leffler function Eα ( −z ) and the H - function .The results collected can be used as a framework for numerical models for solving such problems . We also discuss uses of our approach to the description of anomalous transport systems in complex systems .Introduction . The theory of fractional calculus has been used rapidly during recent years owing to its numerous uses in different fields of science and engineering ( saw e . g . , 1 - 4 ) .In particular , it is widely applied to explain anomalous transport phenomena observed in many physical theories 5 . For instance , this methodology was successfully utilized to model subdiffusion 6 , superdiffusion 7 , and Lévy flights 8 .In general , fractional differential equations have no accurate analytical solution 9 . Therefore , there exists considerable interest in establishing efficient numerical algorithms for their approximate treatment 10 .However , even if we using powerful computers , these solutions may not always supply sufficiently accurate outcomes 11 . This problem appears particularly important when one deals with strongly nonlinear issues 12 or considers very huge time periods 13 .To solve difficulties related with the lack of precise solutions , various scientists proposed different methods using on special functions 14 - 16 . Among them , the most popular ones contain the Mittag - Leffler 17 and Wright 18 operators , Fox s H - functions 19 , and others 20 - 22 .",
        "rewrite_text": "**Title:** Solutions of Fractional Process-Diffusion Equations in Terms of the H-Function\n\n**Abstract:** In this article, we derive an explicit formula for the solutions of specific types of nonlinear fractional absorption equations characterized by variable coefficients and nonlocal initial conditions. Our approach utilizes the generalized Mittag-Leffler function, denoted as Eα(−z), alongside the H-function to express these solutions. The findings presented here provide a foundational framework that can be employed in numerical models aimed at addressing such complex problems. Furthermore, we explore the implications of our methodology in the context of anomalous transport systems within intricate systems. \n\nThe theory of fractional calculus has gained significant traction in recent years, owing to its diverse applications across various scientific and engineering domains. It has proven particularly effective in elucidating anomalous transport phenomena, which are prevalent in numerous physical theories. Notably, this framework has been successfully applied to model subdiffusion, superdiffusion, and Lévy flights. However, it is important to note that fractional differential equations typically lack precise analytical solutions, leading to a heightened interest in the development of efficient numerical algorithms for their approximate resolution. \n\nDespite advancements in computational power, the solutions derived may not always yield sufficiently accurate results, especially in scenarios involving highly nonlinear dynamics or extended time scales. To address the challenges posed by the absence of exact solutions, researchers have proposed various methodologies that leverage special functions. Among these, the Mittag-Leffler and Wright operators, as well as Fox's H-functions, have emerged as particularly prominent tools in the field. Our work contributes to this ongoing discourse by providing new insights and methodologies for tackling fractional process-diffusion equations, thereby enhancing the understanding and modeling of complex transport phenomena.",
        "ori-fast-z-score": -0.8574929257125441,
        "water-fast-z-score": 7.091168559049116,
        "rewrite-fast-z-score": 2.31729648808663
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Universal spectrum for DNA base CG frequency distribution in Takifugu rubripes (Puffer fish) genome .\nAbstract:\nThe universal spectrum is the most important concept in physics and mathematics, which has been widely used to study various physical phenomena such as energy transfer processes in biological systems.  In this work we present an analysis on the universal spectrum of the DNA base CG frequency distribution along the chromosomes of Takifugu rubripe s(pufferfish). We find that there are two different types of universal spectra corresponding to the coding regions and non-coding regions respectively. The universal spectrum of the coding region shows a power law behavior with exponent 1.5 while that of the noncoding region exhibits a fractal structure. Our results show that the universal spectrum can be used to characterize the complexity of the DNA sequence. It may also provide new insights into the understanding of the evolution process of the genomes. Keywords: Universal Spectrum; Fractal; Power Law; Puffer Fish Genome; Energy Transfer Processes. Introduction:  The universal spectrum is one of the most important concepts in physics and mathematics, it was first introduced by Hertz  1  . Since then many scientists have studied its applications in various fields including biology  2  , geology  3  , medicine  4  etc.. Recently, some researchers found that the universal spectrum could be applied to analyze the gene expression data  5  -  8  .\nIn recent years, more and more attention has been paid to the relationship between the universal spectrum and the energy transfer processes in biological system  9  -  11  . For example, Li et al.  12  investigated the universal spectrum of the human heart rate variability and found that the universal spectrum showed a fractal structure. They suggested that the universal spectrum might be useful in characterizing the complexity of the physiological time series. Wang et al.  13  analyzed the universal spectrum of the protein folding dynamics and they found that the universal spectrum exhibited a power-law behavior with exponent 2.0. They proposed that the universal spectrum could reflect the degree of disorderedness of the protein folding dynamics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Universal range for DNA base CG frequency transmission in Takifugu rubripes ( Puffer fish ) genes . Abstract : The universal spectrum is the most important notion in science and biology , which has been widely applied to study various biological phenomena such as energy flow processes in biological systems .In this research we present an assessment on the universal spectrum of the DNA base CG frequency distribution along the chromosomes of Takifugu rubripe s ( pufferfish ) . We see that there are two different kinds of universal spectra corresponding to the coding areas and non - coding areas respectively .The universal spectrum of the coding region shows a power law behavior with exponent 1 . 5 while that of the noncoding region displays a fractal structure . Our results show that the universal spectrum can be used to characterize the complexity of the DNA sequence .It might additionally offer additional perspectives into the knowledge of the evolution mechanism of the genomes . Keywords : Universal Spectrum ; Fractal ; Power Law ; Puffer Fish Genome ; Energy Transfer Processes .Introduction : The fundamental spectrum is one of the most important concepts in science and biology , it was first described by Hertz 1 . Since then many scientists have researched its applications in different fields including biology 2 , geology 3 , medicine 4 etc . .Recently , some researchers found that the universal spectrum could be applied to analyze the gene function data 5 - 8 . In recent years , more and more attention has been paid to the relationship between the universal spectrum and the power transfer mechanisms in biological system 9 - 11 .For instance , Li et al . 12 examined the universal spectrum of the human chest rate variability and found that the universal spectrum demonstrated a fractal structure .They suggested that the fundamental spectrum might be used in characterizing the complexity of the physiological time cycle . Wang et al .13 examined the universal spectrum of the protein folding dynamics and they concluded that the universal spectrum displayed a power - law behavior with exponent 2 . 0 . They proposed that the universal spectrum could reflect the degree of disorderedness of the protein folding dynamics .",
        "rewrite_text": "**Title:** Universal Range for DNA Base CG Frequency Transmission in Takifugu rubripes (Puffer Fish) Genes\n\n**Abstract:** The concept of the universal spectrum is a pivotal element in both science and biology, extensively utilized to investigate a variety of biological phenomena, including energy transfer processes within biological systems. This study focuses on the universal spectrum of the DNA base CG frequency distribution across the chromosomes of Takifugu rubripes, commonly known as the puffer fish. Our analysis reveals the presence of two distinct types of universal spectra: one associated with coding regions and another with non-coding regions of the genome. The universal spectrum observed in the coding regions exhibits a power law behavior characterized by an exponent of 1.5, while the non-coding regions display a fractal structure. These findings suggest that the universal spectrum serves as a valuable tool for characterizing the complexity inherent in DNA sequences. Furthermore, this research may provide new insights into the evolutionary mechanisms governing genomic structures. \n\nThe universal spectrum has been recognized as a significant concept since its initial description by Hertz, and its applications have expanded across various scientific disciplines, including biology, geology, and medicine. Recent studies have highlighted the potential of the universal spectrum in analyzing gene function data and exploring the relationship between the spectrum and energy transfer mechanisms in biological systems. For instance, previous research has demonstrated that the universal spectrum can reveal fractal structures in physiological time cycles, as seen in human heart rate variability, and can indicate the disorderliness of protein folding dynamics through power-law behaviors. This study contributes to the growing body of literature that underscores the relevance of the universal spectrum in understanding the complexities of biological systems and their evolutionary implications.\n\n**Keywords:** Universal Spectrum; Fractal; Power Law; Puffer Fish Genome; Energy Transfer Processes.",
        "ori-fast-z-score": -0.39405520311955033,
        "water-fast-z-score": 6.814365914895229,
        "rewrite-fast-z-score": -0.5773502691896257
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Study of Catalogued Nearby Galaxy Clusters in the SDSS-DR4: I. Cluster Global Properties .\nAbstract:\nWe present an analysis of galaxy clusters selected by their red-sequence galaxies (Gladders & Yee 2005) from the Sloan Digital Sky Survey Data Release 4 (DR4). We use two different methods to select cluster candidates, and then apply photometric redshift cuts on these samples to obtain final catalogues with high purity. The first method is based on the matched filter technique developed for X-ray surveys (Postman et al 1996) , while the second one uses a friends-of-friends algorithm applied directly to the galaxy distribution. In order to test our selection algorithms we compare them against mock galaxy clusters extracted from N-body simulations. Our main results are as follows:  1. Using the matched filter technique we find that the number density of galaxy clusters at z < 0.5 is n(z<0.5) = 3.6 +/- 0.2 x 10-3 h3 Mpc-3 . This value agrees well with previous determinations using other techniques.  2. By applying the same matched filter technique to simulated galaxy clusters we show how this method can be used to estimate the mass function of galaxy clusters up to z ~1.0.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  A Study of Catalogued Nearby Galaxy Clusters in the SDSS-DR4: I.Cluster Global Properties .Abstract : We present an assessment of galaxy clusters chose by their red - sequence galaxies ( Gladders & Yee 2005 ) from the Sloan Digital Sky Survey Data Release 4 ( DR4 ) . We use two different methods to select cluster applicants , and then use photometric redshift cutting on these specimens to obtain final catalogues with high purity .The first method is based on the matched filter technique developed for X - ray observations ( Postman et al 1996 ) , while the second one uses a friends - of - friends method applied directly to the galaxy distribution . In order to test our choice algorithms we compare them against mock galaxy galaxies extracted from N - bodies simulations .Our main results are as follows : 1 . Using the matched filter technique we find that the number density of galaxy galaxies at z < 0 . 5 is n ( z < 0 . 5 ) = 3 . 6 + / - 0 . 2 x 10 - 3 h3 Mpc - 3 .This value agrees well with previous determinations using other techniques.2.By applying the same matched filter technique to modeled galaxy galaxies we find how this algorithm can be used to estimate the mass function of galaxy galaxies up to z ~ 1 . 0 .",
        "rewrite_text": "Title: A Study of Catalogued Nearby Galaxy Clusters in the SDSS-DR4: I. Cluster Global Properties\n\nAbstract: In this study, we evaluate galaxy clusters identified through their red-sequence galaxies, as outlined by Gladders & Yee (2005), utilizing data from the Sloan Digital Sky Survey Data Release 4 (DR4). Our approach involves two distinct methodologies for selecting potential cluster candidates, followed by the application of photometric redshift cuts to refine these selections into final catalogs with high purity. The first methodology employs the matched filter technique, originally developed for X-ray observations (Postman et al. 1996), while the second utilizes a friends-of-friends algorithm directly applied to the galaxy distribution. To validate our selection algorithms, we conduct comparisons with mock galaxy catalogs generated from N-body simulations. \n\nOur findings reveal several key results: Firstly, the matched filter technique yields a number density of galaxies for redshifts less than 0.5, calculated as n(z < 0.5) = 3.6 ± 0.2 x 10^-3 h^3 Mpc^-3. This measurement is consistent with previous estimates derived from alternative methods. Secondly, when applying the matched filter technique to simulated galaxy distributions, we demonstrate its effectiveness in estimating the mass function of galaxy clusters up to redshifts of approximately 1.0. This research not only enhances our understanding of the global properties of nearby galaxy clusters but also provides a robust framework for future studies in cosmology and galaxy formation. The implications of our results contribute to the broader field of astrophysics, particularly in the context of cluster dynamics and the evolution of large-scale structures in the universe.",
        "ori-fast-z-score": 1.1285761872936695,
        "water-fast-z-score": 5.103103630798288,
        "rewrite-fast-z-score": 0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic fibrils in H-alpha and C IV .\nAbstract:\nWe present new observations of the solar atmosphere obtained with the Solar Ultraviolet Imager (SUVI) onboard the Coriolis satellite, which show dynamic fibrils in both H-alpha and CIV lines. The SUVI data are compared to simultaneous ground-based observations made at Big Bear Observatory using the 1-meter telescope equipped with an H-Alpha filter and a Fabry-Perot interferometer tuned to the CIV line. We find that the observed structures have similar properties as those seen previously by other authors but we also see some differences between them. In particular, our results suggest that the fibril structure is more complex than it was thought before. This complexity may be related to the fact that these structures are formed under different physical conditions. Our analysis shows that the observed features can be explained by assuming that they represent plasma flows along magnetic field lines. These flows could play important role in heating up the upper layers of the solar atmosphere.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic fibrils in H - alpha and C IV . Abstract : We report new studies of the solar atmosphere acquired with the Solar Ultraviolet Imager ( SUVI ) onboard the Coriolis spacecraft , which show continuous fibrils in both H - alpha and CIV lines .The SUVI findings are compared to simultaneous ground - based observations made at Big Bear Observatory using the 1 - meter telescope fitted with an H - Alpha filter and a Fabry - Perot interferometer tuned to the CIV line . We see that the seen elements have related properties as those viewed earlier by other researchers but we also saw some similarities between them .In particular , our findings confirm that the fibril structure is more sophisticated than it was considered before . This complexity might be connected to the fact that these structures are created under varying physical conditions .Our study shows that the seen features can be described by assuming that they represent plasma flows along magnetic field lines . These flows could play vital role in heating up the higher layers of the solar atmosphere .",
        "rewrite_text": "We present new insights into the solar atmosphere derived from observations made with the Solar Ultraviolet Imager (SUVI) aboard the Coriolis spacecraft. Our research focuses on the continuous fibrils observed in both H-alpha and C IV spectral lines. These findings are juxtaposed with concurrent ground-based observations from the Big Bear Observatory, where a 1-meter telescope equipped with an H-alpha filter and a Fabry-Perot interferometer tuned to the C IV line was utilized. Our analysis reveals that the fibrils exhibit properties consistent with those documented in previous studies, while also highlighting notable similarities between the two observational methods. Notably, our results indicate that the fibril structures are more intricate than previously understood, suggesting that their complexity may arise from the diverse physical conditions under which they form. This study proposes that the observed features can be interpreted as plasma flows along magnetic field lines, which may be instrumental in the heating processes occurring in the upper layers of the solar atmosphere. By enhancing our understanding of these dynamic fibrils, we contribute to the broader knowledge of solar atmospheric phenomena and their implications for solar dynamics and magnetohydrodynamics.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 6.037034299432969,
        "rewrite-fast-z-score": -1.0660035817780522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sub-arcsecond CO(1-0) and CO(2-1) observations of the ultraluminous infrared galaxy IRAS 10190+1322 .\nAbstract:\nWe present sub-arcsecond resolution interferometric observations in the millimeter continuum, 12CO(J=1-0), 13CO(J=1-0) and C18O(J=1-0). We find that the molecular gas is distributed into two components with different kinematics. The main component has an extent of 1 kpc (0.5 arcsec at z = 2.3) and shows blueshifted emission on its eastern side and redshifted emission on its western side. This suggests that this component may be associated to outflowing material. A second component is detected only in 12CO(J=1-1) and it is located 0.2 arcsec eastward of the first one. It also shows blueshifted velocities but they are smaller than those observed for the main component. Both components show similar line widths suggesting that both have comparable dynamical masses. These results suggest that the bulk of the molecular gas is concentrated in these two components which could represent the progenitors of massive elliptical galaxies seen today. \n \n Keywords: Ultraluminous Infrared Galaxies",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sub - arcsecond CO ( 1 - 0 ) and CO ( 2 - 1 ) observations of the ultraluminous laser galaxy IRAS 10190 + 1322 . Abstract : We report sub - arcsecond resolution interferometric observations in the millimeter continuum , 12CO ( J = 1 - 0 ) , 13CO ( J = 1 - 0 ) and C18O ( J = 1 - 0 ) .We see that the molecular fuel is spread into two parts with varying kinematics . The main component has an extent of 1 kpc ( 0 . 5 arcsec at z = 2 . 3 ) and shows blueshifted emission on its eastern side and redshifted emission on its southern side .This implies that this component may be correlated to outflowing matter . A second component is detected only in 12CO ( J = 1 - 1 ) and it is situated 0 . 2 arcsec east of the first one .It additionally shows blueshifted velocities but they are smaller than those observed for the main component . Both components exhibit similar line widths suggesting that both have equal dynamical masses .These data suggest that the majority of the molecular gas is concentrated in these two parts which could indicate the progenitors of large elliptical galaxies found today . Keywords : Ultraluminous Infrared Galaxies",
        "rewrite_text": "**Title:** Sub-Arcsecond CO (1-0) and CO (2-1) Observations of the Ultraluminous Laser Galaxy IRAS 10190+1322\n\n**Abstract:** In this study, we present high-resolution interferometric observations of the ultraluminous galaxy IRAS 10190+1322, focusing on the millimeter continuum and molecular emissions from 12CO (J=1-0), 13CO (J=1-0), and C18O (J=1-0). Our findings reveal that the molecular gas is distributed across two distinct components, each exhibiting different kinematic properties. The primary component, which spans approximately 1 kpc (equivalent to 0.5 arcseconds at redshift z = 2.3), displays a notable asymmetry in its velocity profile, with blueshifted emission detected on the eastern side and redshifted emission on the southern side. This velocity pattern suggests a potential correlation with outflowing material, indicating dynamic processes at play within the galaxy. \n\nIn addition to the main component, we identify a secondary structure that is only observable in the 12CO (J=2-1) transition, located 0.2 arcseconds to the east of the primary component. This secondary component also exhibits blueshifted velocities, albeit at a reduced magnitude compared to those of the main component. Notably, both components demonstrate similar line widths, implying that they possess comparable dynamical masses. \n\nThe concentration of molecular gas in these two components raises intriguing questions about their role in the evolutionary pathways of galaxies. Our observations suggest that these structures may represent the progenitors of the massive elliptical galaxies observed in the present-day universe. This research contributes to our understanding of the kinematic and structural properties of molecular gas in ultraluminous infrared galaxies, providing insights into the processes that govern galaxy formation and evolution. \n\n**Keywords:** Ultraluminous Infrared Galaxies, Molecular Gas, Interferometric Observations, Galaxy Evolution.",
        "ori-fast-z-score": -1.697749375254331,
        "water-fast-z-score": 4.85071250072666,
        "rewrite-fast-z-score": 1.4985372985307104
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the remote Galactic globular cluster NGC 2419 .\nAbstract:\nWe present new photometric data for the remote Galactic globulars M92 and NGC 2419, obtained with the 1-m telescope at Mt. Wilson Observatory in California during two observing runs (in February-March 2005 and September-October 2006). The main goal was to obtain accurate color indices for these clusters which are needed as input parameters into theoretical models of stellar evolution.  We have determined the following basic parameters of both clusters:  distance modulus DM = 13.20 ± 0.10 mag; reddening E(B-V) = 0.04 ± 0.01 mag; metallicity  Fe/H  = -1.30 ± 0.05 dex for M92 and DM = 14.00 ± 0.15 mag; E(B-V) < 0.02 mag;  Fe/H  = -2.40 ± 0.10 dex for NGC 2419. These values agree well with previous determinations based on other methods.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the distant Galactic globular cluster NGC 2419 . Abstract : We report new photometric data for the distant Galactic globulars M92 and NGC 2419 , obtained with the 1 - m observatory at Mt .Wilson Observatory in California during two observing walks ( in February - March 2005 and September - October 2006 ) . The main goal was to obtain precise color indices for these clusters which are needed as input parameters into theory theories of stars evolution .We have decided the following essential parameters of both clusters : distance modulus DM = 13 . 20 ± 0 . 10 mag ; reddening E ( B - V ) = 0 . 04 ± 0 . 01 mag ; metallicity Fe / H = - 1 . 30 ± 0 . 05 dex for M92 and DM = 14 . 00 ± 0 . 15 mag ; E ( B - V ) < 0 . 02 mag ; Fe / H = - 2 . 40 ± 0 . 10 dex for NGC 2419 . These values comply good with previous determinations based on other methods .",
        "rewrite_text": "We present new photometric observations of the distant Galactic globular clusters M92 and NGC 2419, acquired using the 1-meter telescope at Mt. Wilson Observatory in California during two observational campaigns conducted in February-March 2005 and September-October 2006. The primary objective of this study was to derive accurate color indices for these clusters, which are critical for advancing theoretical models of stellar evolution. Our analysis yielded several key parameters for both clusters. For M92, we determined a distance modulus (DM) of 13.20 ± 0.10 mag, a reddening value of E(B-V) = 0.04 ± 0.01 mag, and a metallicity of [Fe/H] = -1.30 ± 0.05 dex. In contrast, for NGC 2419, we found a distance modulus of 14.00 ± 0.15 mag, a reddening value of E(B-V) < 0.02 mag, and a metallicity of [Fe/H] = -2.40 ± 0.10 dex. These results are consistent with previous measurements obtained through various methodologies, reinforcing the reliability of our findings. The precise color indices and derived parameters will contribute significantly to the understanding of the evolutionary processes in these globular clusters and their role in the broader context of Galactic formation and evolution. Our work underscores the importance of continued observational efforts to refine the parameters of distant stellar populations, which are essential for testing and improving theoretical models of stellar and Galactic evolution.",
        "ori-fast-z-score": 0.42857142857142855,
        "water-fast-z-score": 3.959797974644666,
        "rewrite-fast-z-score": -0.44172610429938614
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Some new experimental photonic flame effect features .\nAbstract:\nThe present work is devoted to the study of some new effects in the process of combustion of liquid fuels, which are observed under the action of laser radiation with different wavelengths and power densities on the fuel surface. The results obtained show that the use of pulsed lasers can significantly increase the efficiency of burning liquid fuels by increasing their evaporation rate due to the formation of plasma at the fuel surface. In addition, it was found that the presence of an additional source of energy (laser) leads to changes in the structure of the flame front, as evidenced by the appearance of bright spots near its edge. It has been shown experimentally that these spots correspond to local increases in temperature up to several thousand degrees Celsius. This phenomenon may be used for creating highly efficient heat sources based on the combustion of liquid fuels. \n \n Keywords: Flame, Laser, Combustion, Evaporation, Plasma",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Some novel experimental photonic fire action characteristics . Abstract : The present work is devoted to the observation of some newest effects in the process of burning of liquid materials , which are observed under the action of laser radiation with various wavelengths and power densities on the fuel surface .The results collected show that the using of pulsed lasers can significantly boost the performance of burning liquid fuels by increasing their evaporation rate due to the formation of plasma at the fuel surface . In addition , it was shown that the presence of an additional source of electricity ( laser ) results to changes in the composition of the burning front , as demonstrated by the appearance of bright stripes near its tip .It has been shown experimentally that these places coincide to local changes in heat up to several thousand degrees Celsius . This phenomenon might be used for producing greatly efficient energy sources dependent on the burning of liquid materials .Keywords: Flame, Laser, Combustion, Evaporation, Plasma",
        "rewrite_text": "Title: Novel Experimental Characteristics of Photonic Fire Action\n\nAbstract: This study investigates innovative phenomena observed during the combustion of liquid materials when subjected to laser radiation of varying wavelengths and power densities. The findings indicate that the application of pulsed lasers significantly enhances the combustion efficiency of liquid fuels by accelerating their evaporation rates, a process attributed to the formation of plasma at the fuel's surface. Furthermore, the research reveals that the introduction of an additional energy source, specifically laser radiation, alters the composition of the combustion front. This alteration is evidenced by the emergence of luminous stripes near the leading edge of the flame, which correspond to localized temperature increases reaching several thousand degrees Celsius. These experimental observations suggest that the interaction between laser radiation and liquid fuels could lead to the development of highly efficient energy sources based on liquid combustion. The implications of this research extend to various applications in energy production and combustion technology, highlighting the potential for optimizing fuel efficiency and enhancing the performance of combustion systems. The study contributes to the understanding of laser-assisted combustion processes and opens avenues for future research in the field of photonic fire action. \n\nKeywords: Flame, Laser, Combustion, Evaporation, Plasma",
        "ori-fast-z-score": -1.5096588248481377,
        "water-fast-z-score": 5.682196434640312,
        "rewrite-fast-z-score": -0.7921180343813395
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Improving Stellar and Planetary Parameters of Transiting Planet Systems: The Case of TrES-2 .\nAbstract:\nWe present the results of an analysis aimed at improving the stellar parameters for the host star of planet TrES-2, as well as its planetary system properties. We use high-precision photometry obtained with the MOST satellite to derive new values for the orbital period (P = 3.819 days), transit epoch (T0 = 2454000 MJD) and radius ratio (Rp/Rs = 0.11). These are combined with existing radial velocity data in order to refine the mass estimates for both components of this double-lined spectroscopic binary. Our best-fit model yields masses of 1.06 ± 0.04M⊙ and 0.84 ± 0.03M⊙ for the primary and secondary stars respectively, along with radii of 1.16 ± 0.02R⊙ and 0.91 ± 0.01R⊙ . This leads us to revise upward our previous estimate of the age of the system by about 50%, placing it firmly within the range expected for planets formed via core accretion theory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Improving Stellar and Planetary Parameters of Transiting Planet Systems : The Case of TrES - 2 . Abstract : We report the conclusion of an assessment aimed at enhancing the stellar characteristics for the host star of planet TrES - 2 , as well as its planetary system properties .We use large - precision photometry obtained with the MOST satellite to derive new values for the orbital period ( P = 3 . 819 days ) , transit epoch ( T0 = 2454000 MJD ) and radius ratio ( Rp / Rs = 0 . 11 ) . These are coupled with existing radial speed data in order to refine the mass estimates for both components of this double - lined spectroscopic binary .Our best - fitting model gives masses of 1 . 06 ± 0 . [UNK] and 0 . 84 ± 0 . [UNK] for the primary and secondary stars respectively , along with radii of 1 . 16 ± 0 . [UNK] and 0 . 91 ± 0 . [UNK] . This leads us to revise upward our previous estimate of the age of the system by about 50 % , placing it firmly within the range assumed for planets formed via core accretion theory .",
        "rewrite_text": "Title: Enhancing Stellar and Planetary Parameters of Transiting Planet Systems: The Case of TrES-2\n\nAbstract: In this study, we present the findings from our investigation aimed at refining the stellar parameters of the host star associated with the transiting exoplanet TrES-2, as well as the characteristics of its planetary system. Utilizing high-precision photometric data collected by the MOST satellite, we have derived updated values for key orbital parameters, including the orbital period (P = 3.819 days), the transit epoch (T0 = 2454000 MJD), and the radius ratio (Rp/Rs = 0.11). These new measurements are integrated with existing radial velocity data to enhance the mass estimates for both stars in this double-lined spectroscopic binary system. Our analysis yields refined mass estimates of 1.06 ± 0. [UNK] for the primary star and 0.84 ± 0. [UNK] for the secondary star, alongside updated radius measurements of 1.16 ± 0. [UNK] and 0.91 ± 0. [UNK], respectively. Consequently, we have revised our previous estimation of the system's age, increasing it by approximately 50%. This adjustment positions the system firmly within the age range typically associated with planets formed through core accretion theory. Our findings contribute to a more accurate understanding of the TrES-2 system and provide valuable insights into the formation and evolution of exoplanets in similar environments.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": 1.3337718577107005
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists .\nAbstract:\nWe study the efficiency of scrip systems in which users can buy goods with their own currency or by exchanging it for other people s currency (scrip). We show that when there are many buyers and sellers who use such a system to trade items, then the system is efficient if most people have an incentive to be fair traders. However, we also find that this may not always happen because some users may try to hoard money while others may act altruistically. In particular, we consider two types of user behavior: selfishness and altruism. Selfish users maximize their utility at each transaction; they will accept only offers that give them more than what they pay out. Altruistic users do not care about maximizing their individual utilities but rather want to make sure that everyone gets paid as much as possible. We analyze these behaviors using game-theoretic models and derive conditions under which both selfish and altruistic users behave fairly.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimizing Scrip Systems : Efficiency , Crashes , Hoarders , and Altruists . Abstract : We research the performance of scrip systems in which participants can buying items with their own currency or by exchanging it for other people s value ( scrip ) .We see that when there are many buyers and sellers who use such a system to trade items , then the scheme is efficient if most people have an motivation to be fair merchants . However , we also find that this might not always occur because some users could attempt to hoard income while others may act altruistically .In particular , we investigate two forms of customer conduct : selfishness and altruism . Selfish users maximize their utility at each deal ; they will take only offers that give them more than what they pay out .Altruistic users do not care about maximizing their individual utilities but rather seek to make sure that everyone gets paid as much as possible . We evaluate these behaviors using game - theoretic models and derive environments under which both selfish and altruistic users behave reasonably .",
        "rewrite_text": "Title: Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists\n\nAbstract: This study explores the dynamics of scrip systems, where participants can acquire items either through their own currency or by exchanging it for the value represented by scrip from others. Our findings indicate that the efficiency of such systems is contingent upon the presence of a majority of fair-minded participants engaged in trading. However, we also identify potential inefficiencies arising from certain user behaviors, particularly hoarding and altruism. We delve into two distinct behavioral patterns among users: selfishness and altruism. Selfish participants focus on maximizing their individual utility in each transaction, opting for offers that yield a greater return than their expenditure. In contrast, altruistic users prioritize the overall welfare of the group, striving to ensure equitable compensation for all involved, regardless of their own utility maximization. To analyze these behaviors, we employ game-theoretic models, allowing us to delineate the conditions under which both selfish and altruistic behaviors can coexist and function effectively within the scrip system. Our research contributes to a deeper understanding of the interplay between individual motivations and collective outcomes in economic exchanges, highlighting the importance of user behavior in determining the overall efficiency and stability of scrip systems. Through this investigation, we aim to provide insights that could inform the design and implementation of more robust trading systems that accommodate diverse participant motivations while promoting fairness and efficiency.",
        "ori-fast-z-score": 0.22645540682891913,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": 0.09950371902099892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Temperature-driven transition from the Wigner Crystal to the Bond-Charge-Density Wave in the Quasi-One-Dimensional Quarter-Filled band .\nAbstract:\nWe study temperature driven phase transitions between different charge ordered states in one-dimensional quarter-filled Hubbard model with nearest-neighbor hopping and on-site Coulomb repulsion using density matrix renormalization group (DMRG) method. We find that at low temperatures, the system is in the Wigner crystal state where electrons are localized by strong electron correlations. With increasing temperature, we observe a first-order phase transition into bond-charge-density wave state which has been predicted theoretically earlier but not observed experimentally so far. The nature of this phase transition can be understood as melting of the Wigner crystal due to thermal fluctuations. Finally, we also investigate the effect of next-nearest neighbor hopping t′ on these phases. Our results show that for small values of t′ , the ground state remains in the Wigner crystal phase while for large values of t′ it becomes metallic. This shows that the presence of next-nearest neighbor hoping may destroy the charge ordering in 1D systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Temperature - driven transition from the Wigner Crystal to the Bond - Charge - Density Wave in the Quasi - One - Dimensional Quarter - Filled band . Abstract : We explore temperature driven phase transitions between various charge ordered states in one - dimensional quarter - filled Hubbard theory with nearest - neighbor hopping and on - location Coulomb repulsion utilizing density matrix renormalization group ( DMRG ) method .We see that at low temperatures , the system is in the Wigner crystal state where electrons are localized by large electron correlations . With increasing temperature , we study a first - order phase shift into bond - charge - density wave state which has been predicted theoretically prior but not observed experimentally so far .The nature of this phase shift can be understood as melting of the Wigner crystal owing to heat fluctuations . Finally , we also investigate the impact of next - nearest neighbor hopping t ′ on these phases .Our results show that for little values of t ′ , the ground state remains in the Wigner crystal phase while for large values of t ′ it becomes metallic . This shows that the presence of next - nearest neighbor hoping might disrupt the charge ordering in 1D solutions .",
        "rewrite_text": "In this study, we investigate the temperature-induced phase transitions among various charge-ordered states within the framework of one-dimensional quarter-filled Hubbard models, incorporating nearest-neighbor hopping and on-site Coulomb repulsion. Utilizing the density matrix renormalization group (DMRG) method, we analyze the behavior of the system as temperature varies. At low temperatures, our findings indicate that the system stabilizes in a Wigner crystal phase, characterized by strong electron localization due to significant electron-electron correlations. As the temperature increases, we observe a first-order phase transition into a bond-charge-density wave state, a phenomenon that has been theoretically predicted but remains unobserved in experimental settings to date. This transition can be interpreted as the melting of the Wigner crystal, driven by thermal fluctuations. Additionally, we explore the influence of next-nearest neighbor hopping (t') on these phases. Our results reveal that for small values of t', the ground state consistently remains in the Wigner crystal phase. However, as t' increases, the system transitions to a metallic state, indicating that the inclusion of next-nearest neighbor hopping can significantly disrupt charge ordering in one-dimensional systems. This research not only enhances our understanding of charge ordering phenomena in low-dimensional materials but also provides insights into the intricate interplay between temperature, electron correlations, and hopping parameters in determining the electronic properties of such systems.",
        "ori-fast-z-score": 0.29851115706299675,
        "water-fast-z-score": 4.158619680502032,
        "rewrite-fast-z-score": -0.4622501635210242
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Calibration of the GLAST Burst Monitor detectors .\nAbstract:\nThe Large Area Telescope (LAT) on board Fermi Gamma-ray Space Telescope is scheduled to be launched in June 2008 and will perform high energy gamma ray astronomy for at least 3 years. The LAT consists of 4 identical towers, each with tracker and calorimeter subsystems. Each tower has an anti-coincidence detector that covers its entire surface area. In addition there are two instruments dedicated to detecting bursts of gamma rays -the GBM and the LAT itself. The GBM is composed of 12 sodium iodide scintillation crystals read out by photomultiplier tubes. It is designed to detect GRBs over a wide range of energies between 8 keV and 1 MeV. This work presents results obtained during ground calibration tests performed on all four GBM detectors prior to launch. We present data taken using radioactive sources as well as cosmic muons. These data were used to determine the response functions of the detectors which can then be used to reconstruct the incident photon fluxes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Calibration of the GLAST Burst Monitor detectors . Abstract : The Large Area Telescope ( LAT ) on board Fermi Gamma - ray Space Telescope is slated to be launched in June 2008 and will perform large energy gamma ray observations for at least 3 years .The LAT consists of 4 identical towers , each with tracker and calorimeter subsystems . Each tower has an counter - coincidence detector that covers its entire surface region .In addition there are two instruments dedicated to detecting bursts of gamma radiation - the GBM and the LAT itself . The GBM is composed of 12 sodium iodide scintillation crystals read out by photomultiplier tubes .It is designed to identify GRBs over a broad variety of energies between 8 keV and 1 MeV . This research provides findings obtained during ground calibration tests performed on all four GBM detectors previous to launch .We present data taken using nuclear sources as also as cosmic muons . These data were used to identify the response functions of the detectors which can then be used to reconstruct the incident photon fluxes .",
        "rewrite_text": "**Title: Calibration of the GLAST Burst Monitor Detectors**\n\n**Abstract:** The Large Area Telescope (LAT), part of the Fermi Gamma-ray Space Telescope, is scheduled for launch in June 2008, with a mission to conduct extensive gamma-ray observations across a wide energy spectrum for a minimum of three years. The LAT is comprised of four identical towers, each equipped with both tracker and calorimeter subsystems, along with a counter-coincidence detector that spans the entire surface area of each tower. Complementing the LAT, two specialized instruments are dedicated to the detection of gamma-ray bursts (GRBs): the Gamma-ray Burst Monitor (GBM) and the LAT itself. The GBM features twelve sodium iodide scintillation crystals, which are read out by photomultiplier tubes, and is specifically engineered to detect GRBs across a broad energy range from 8 keV to 1 MeV. \n\nThis study presents the results from ground calibration tests conducted on all four GBM detectors prior to their launch. The calibration process involved collecting data using both nuclear sources and cosmic muons, which were instrumental in determining the response functions of the detectors. These response functions are critical for accurately reconstructing the incident photon fluxes that the detectors will measure in space. The findings from these calibration tests not only enhance our understanding of the GBM's performance but also lay the groundwork for interpreting the data that will be collected during the Fermi mission. By establishing a reliable calibration framework, this research contributes significantly to the overall objectives of the Fermi Gamma-ray Space Telescope, ensuring that it can effectively monitor and analyze gamma-ray bursts and other high-energy astrophysical phenomena.",
        "ori-fast-z-score": -1.1952286093343936,
        "water-fast-z-score": 3.4416668087621747,
        "rewrite-fast-z-score": 0.7844645405527362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stellar Population Models and Individual Element Abundances I: Sensitivity of Stellar Evolution Models .\nAbstract:\nWe present the results of our investigation into how stellar evolution models are affected by individual element abundances in stars, focusing on the sensitivity to changes in helium abundance (Y). We use two different sets of evolutionary tracks with varying Y values for masses between 0.8 M⊙ and 8 M⊙ at solar metallicity. The first set is based on the Padova code while the second one uses the Geneva code. For each track we calculate synthetic spectra using the SPECTRUM code. These synthetic spectra are then used as input to determine the best-fit parameters of observed high-resolution optical spectra of Galactic open clusters. Our analysis shows that both codes produce similar results when fitting these cluster data. However, there are significant differences in the derived ages depending on which code was used. This discrepancy can be explained by the fact that the Padova tracks have been calculated without convective overshooting whereas the Geneva tracks include this effect.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stellar Population Models and Individual Element Abundances I : Sensitivity of Stellar Evolution Models . Abstract : We present the conclusion of our inquiry into how stellar evolution models are influenced by individual atom abundances in stars , concentrating on the sensitivity to changes in helium abundance ( Y ) .We use two different sets of evolutionary tracks with varying Y values for masses between 0 . 8 [UNK] and 8 [UNK] at solar metallicity . The first setting is based on the Padova code while the second one uses the Geneva code .For each track we determine natural spectra using the SPECTRUM code . These synthetic spectra are then utilized as input to obtain the best - fitting characteristics of measured high - resolution optical spectra of Galactic open clusters .Our study shows that both codes produce comparable results when fitting these cluster data . However , there are significant variations in the derived ages varying on which coding was used .This discrepancy can be reason by the fact that the Padova lines have been measured without convective overshooting whereas the Geneva tracks include this effect .",
        "rewrite_text": "In this article, we investigate the impact of individual elemental abundances on stellar evolution models, with a particular focus on the sensitivity of these models to variations in helium abundance (Y). Our research employs two distinct sets of evolutionary tracks, spanning masses from 0.8 to 8 solar masses at solar metallicity, each characterized by different helium abundance values. The first set of evolutionary tracks is derived from the Padova stellar evolution code, while the second set is based on the Geneva stellar evolution code. To analyze the effects of these variations, we generate synthetic spectra for each evolutionary track using the SPECTRUM code. These synthetic spectra serve as a basis for fitting the observed high-resolution optical spectra of Galactic open clusters, allowing us to extract key stellar parameters.\n\nOur findings indicate that both the Padova and Geneva codes yield comparable results when fitting the observational data from the clusters. However, we observe notable discrepancies in the derived ages of the stars, which are contingent upon the choice of evolutionary code. This variation in age estimates can be attributed to the differing treatment of convective overshooting in the two models; specifically, the Padova tracks do not account for convective overshooting, while the Geneva tracks incorporate this phenomenon. This study underscores the importance of considering individual elemental abundances and the specific methodologies employed in stellar evolution modeling, as they can significantly influence the interpretation of stellar ages and other fundamental characteristics. Our results contribute to a deeper understanding of stellar populations and highlight the necessity for careful selection of evolutionary models in astrophysical research.",
        "ori-fast-z-score": 1.4444444444444444,
        "water-fast-z-score": 5.963302408041713,
        "rewrite-fast-z-score": 1.9445436482630056
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thouless-Anderson-Palmer equation for analog neural network with temporally fluctuating white synaptic noise .\nAbstract:\nWe study the dynamics of an analog recurrent neural network (RNN) driven by temporally fluctuating white noise in its synapses, and show that it can be described by the Thouless-AndersonPalmer (TAP) equations. We derive these TAP equations using replica theory applied to the RNN s partition function. The resulting TAP equations are exact up to corrections which vanish exponentially fast as the number N of neurons increases. In particular we find that the TAP free energy is given by F = −kBTN ln ZN + O(1/N2), where kB denotes Boltzmann s constant and TN ≡ 1/ZN is the temperature associated with the fluctuations in the synapses. This result shows that the TAP approach provides a good description even when the system size is small compared to the typical correlation length of the noise. Finally, we use our results to calculate the stationary state distribution of the RNN and compare this prediction against numerical simulations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thouless - Anderson - Palmer equation for analog neural network with temporally fluctuating white synaptic noise . Abstract : We research the dynamics of an analog recurrent brain system ( RNN ) driven by temporally fluctuating white sound in its synapses , and find that it can be described by the Thouless - AndersonPalmer ( TAP ) equations .We derive these TAP equations using replica theory applied to the RNN s partition function . The resulting TAP equations are exact up to corrections which vanish exponentially rapidly as the number N of neurons increases .In particular we find that the TAP free energy is given by F = −kBTN ln ZN + O ( 1 / N2 ) , where kB indicates Boltzmann s constant and TN ≡ 1 / ZN is the temperature associated with the fluctuations in the synapses . This result suggests that the TAP solution provides a better characterization even when the system width is tiny contrast to the typical correlation length of the noise .Finally , we using our findings to estimate the discrete state distribution of the RNN and compare this forecast against numerical simulations .",
        "rewrite_text": "In this study, we investigate the dynamics of an analog recurrent neural network (RNN) influenced by temporally fluctuating white noise in its synapses. Our analysis reveals that the behavior of this system can be effectively described by the Thouless-Anderson-Palmer (TAP) equations. We derive these equations through the application of replica theory to the partition function of the RNN, leading to results that are exact, with corrections that diminish exponentially as the number of neurons, denoted as N, increases. Specifically, we find that the TAP free energy can be expressed as F = −kB TN ln ZN + O(1/N²), where kB represents Boltzmann's constant and TN = 1/ZN is the temperature corresponding to the synaptic fluctuations. This finding indicates that the TAP solution offers a more accurate characterization of the system, even when its width is significantly smaller than the typical correlation length of the noise. Furthermore, we utilize our results to estimate the discrete state distribution of the RNN and conduct a comparison between our predictions and numerical simulations. This work not only enhances our understanding of the dynamics of neural networks under the influence of noise but also provides a framework for future research in the field of computational neuroscience, particularly in the context of understanding how noise affects learning and memory in biological systems.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 3.117691453623979,
        "rewrite-fast-z-score": 1.58999682000954
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Fields on the Groenewold-Moyal Plane: C, P, T and CPT .\nAbstract:\nThe purpose of this article is to present an overview of some recent results in quantum field theory (QFT) on curved spaces with noncommutative coordinates. The main motivation for studying QFTs on such spaces comes from string theories which are formulated as open strings attached to D-branes whose positions can be described by noncommuting matrices. In particular we will focus our attention on the so-called Groenewold-Moyal plane which is defined as the space generated by two non-commuting coordinates satisfying the commutation relations  qμ(x), qν(y)  = iθμνρqρ(xy). We will show that it is possible to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane. This allows us to introduce a notion of spinor fields on the Groenewold- Moyal plane. Moreover we will discuss how one can construct gauge invariant actions for these fields. Finally we will study the action of the discrete symmetries C,P,T and CP T .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Fields on the Groenewold - Moyal Plane : C , P , T and CPT . Abstract : The purpose of this page is to provide an overview of some latest findings in quantum field theory ( QFT ) on curved spaces with noncommutative coordinates .The main motivation for studying QFTs on such spaces derives from string theories which are formulated as open strings attached to D - branes whose positions can be described by noncommuting matrices . In particular we will focus our focus on the so - called Groenewold - Moyal plane which is characterized as the space generated by two non - commuting coordinates satisfying the commutation relations qμ ( x ) , qν ( y ) = iθμνρqρ ( xy ) .We will show that it is easy to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane . This enables us to introduce a concept of spinor fields on the Groenewold - Moyal plane .Moreover we will explore how one can create gauge invariant movements for these fields . Finally we will research the operation of the discrete symmetries C , P , T and CP T .",
        "rewrite_text": "**Title:** Quantum Fields on the Groenewold-Moyal Plane: C, P, T, and CPT\n\n**Abstract:** This article presents a comprehensive overview of recent developments in quantum field theory (QFT) on curved spaces characterized by noncommutative coordinates. The exploration of QFTs in such frameworks is primarily motivated by string theory, particularly in scenarios involving open strings that are anchored to D-branes, with their positions represented by noncommuting matrices. Our focus centers on the Groenewold-Moyal plane, a mathematical construct defined by two noncommuting coordinates that adhere to the commutation relations \\( q^\\mu(x), q^\\nu(y) = i\\theta^{\\mu\\nu\\rho}q_\\rho(xy) \\). \n\nWe demonstrate the straightforward definition of a covariant derivative operator applicable to fields residing on the Groenewold-Moyal plane, which facilitates the introduction of spinor fields within this noncommutative geometry. This framework allows us to investigate the formulation of gauge-invariant dynamics for these fields, thereby enriching the theoretical landscape of QFT in noncommutative settings. \n\nAdditionally, we delve into the implications of discrete symmetries, specifically charge conjugation (C), parity (P), time reversal (T), and their combined operation (CPT), within the context of the Groenewold-Moyal plane. Our findings contribute to a deeper understanding of how these symmetries manifest in noncommutative quantum field theories, potentially leading to novel insights in both theoretical physics and mathematical formulations. This work not only highlights the significance of noncommutative geometry in modern physics but also paves the way for future research in the interplay between quantum fields and noncommutative structures.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 3.3466401061363023,
        "rewrite-fast-z-score": 0.3922322702763681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Sextet Arcs: a Strongly Lensed Lyman Break Galaxy in the ACS Spectroscopic Galaxy Survey towards Abell 1689 .\nAbstract:\nWe report on the discovery and spectroscopic confirmation of an extremely bright, strongly lensed galaxy at z = 3.1 (AB mag=18.6) found by searching for high-z galaxies behind Abell 1689 using the Advanced Camera for Surveys (ACS). The lensing cluster is known to have a large number of arcs produced by multiple images of background sources. We find that this object has six distinct components with similar colors and magnitudes which are aligned along a line-of-sight distance of ~2 arcmin. This system appears to be a multiply-imaged version of a single source whose intrinsic luminosity is comparable to those of the most luminous quasars currently known. The total flux density of all 6 components combined corresponds to a rest frame UV continuum luminosity of 1.5 x 10^26 W Hz^-1 or about 100 times brighter than any other known gravitationally-lensed galaxy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Sextet Arcs : a Strongly Lensed Lyman Break Galaxy in the ACS Spectroscopic Galaxy Survey towards Abell 1689 . Abstract : We report on the discovery and spectroscopic evidence of an incredibly bright , heavily lensed galaxy at z = 3 . 1 ( AB mag = 18 . 6 ) found by searching for high - z galaxies behind Abell 1689 using the Advanced Camera for Surveys ( ACS ) .The lensing cluster is known to have a large number of arcs created by many images of background sources . We see that this body has six distinct parts with similar shapes and magnitudes which are aligned along a line - of - view distance of ~ 2 arcmin .This system appears to be a multiply - imaged version of a single source whose inherent luminosity is equal to those of the most luminous quasars currently known . The total flux concentration of all 6 parts coupled translates to a rest frame UV continuum luminosity of 1 . 5 x 10 ^ 26 W Hz ^ - 1 or about 100 times hotter than any other known gravitationally - lensed galaxy .",
        "rewrite_text": "We present the discovery and spectroscopic analysis of an exceptionally luminous, strongly lensed galaxy located at a redshift of z = 3.1 (AB magnitude = 18.6), identified through a search for high-redshift galaxies behind the galaxy cluster Abell 1689 using the Advanced Camera for Surveys (ACS). This lensing cluster is notable for its numerous arcs, which are formed by multiple images of background sources due to gravitational lensing effects. Our observations reveal that this galaxy, referred to as the Sextet Arcs, consists of six distinct components that exhibit similar shapes and magnitudes, arranged along a line of sight spanning approximately 2 arcminutes. This configuration suggests that the system is a multiply-imaged representation of a single source, whose intrinsic luminosity rivals that of the most luminous quasars currently documented. The combined flux from all six components results in a rest-frame ultraviolet continuum luminosity of approximately 1.5 x 10^26 W Hz^-1, making it about 100 times more luminous than any other known gravitationally-lensed galaxy. This discovery not only highlights the extraordinary capabilities of gravitational lensing in revealing distant galaxies but also provides valuable insights into the properties and evolution of high-redshift star-forming galaxies. The findings contribute to our understanding of galaxy formation and the role of gravitational lensing in astrophysical research, paving the way for further investigations into the characteristics of such luminous objects in the early universe.",
        "ori-fast-z-score": -0.254000254000381,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 1.3337718577107005
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The tensor part of the Skyrme energy density functional. I. Spherical nuclei .\nAbstract:\nWe present an improved version of our previous calculation for the nuclear matter properties and the single-particle energies in spherical nuclei based on the covariant density functional theory with the tensor terms included. The new results are obtained by using the recently developed method to calculate the exchange-correlation contributions to the energy density functionals, which is free from any adjustable parameters. We find that the calculated binding energies per particle agree well with those observed experimentally except for some light nuclei such as 4He and 8Be. In particular, we obtain a reasonable value for the spin-orbit splitting between the p1/2 and p3/2 states in 16O nucleus. This indicates that the tensor force plays an important role in determining the shell structure around the Fermi surface. It should be noted here that this result has been achieved without introducing any additional parameter into the original formalisms proposed previously. \n \n Keywords: Tensor force, Energy density functional",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The tensor part of the Skyrme energy density structure . I . Spherical nuclei .Abstract : We present an updated form of our previous measurement for the atomic matter properties and the single - nucleus energies in spherical nuclei using on the covariant density functional theory with the tensor terms included . The revised conclusions are derived by using the recently advanced method to estimate the transfer - correlation contributions to the power density functionals , which is free from any adjustable parameters .We see that the derived binding values per particle comply better with those observed experimentally except for some light nuclei such as 4He and 8Be . In particular , we obtain a suitable value for the spin - orbit dividing between the p1 / 2 and p3 / 2 states in 16O nucleus .This implies that the tensor force plays an important role in determining the shell structure around the Fermi surface . It should be mentioned here that this consequence has been achieved without removing any additional parameter into the previous formalisms proposed earlier .Keywords : Tensor force , Energy density functional",
        "rewrite_text": "Title: The Tensor Component of the Skyrme Energy Density Structure: I. Spherical Nuclei\n\nAbstract: In this study, we present an enhanced version of our earlier analysis concerning the properties of atomic matter and the energy levels of single nuclei in spherical configurations, utilizing covariant density functional theory that incorporates tensor interactions. Our updated findings are based on a novel approach to estimate the contributions of transfer correlations to the power density functionals, which does not rely on any adjustable parameters. The results indicate that the binding energies per particle we calculated align more closely with experimental observations, with the exception of certain light nuclei, specifically helium-4 (4He) and beryllium-8 (8Be). Notably, we have determined an appropriate value for the spin-orbit splitting between the p1/2 and p3/2 states in the oxygen-16 (16O) nucleus. This outcome underscores the significant influence of the tensor force in shaping the shell structure near the Fermi surface. Importantly, these results were achieved without introducing any additional parameters into the previously established theoretical frameworks. Our findings contribute to a deeper understanding of the role of tensor interactions in nuclear structure and may have implications for future research in nuclear physics.\n\nKeywords: Tensor force, Energy density functional",
        "ori-fast-z-score": -1.0660035817780522,
        "water-fast-z-score": 5.193989612031165,
        "rewrite-fast-z-score": 0.9332565252573828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Outflow and Infall in a Sample of Massive Star Forming Regions .\nAbstract:\nWe present new observations of the outflows driven by massive protostars using the Submillimeter Array (SMA) at 1.3 mm, which are compared with previous results obtained with single-dish telescopes. We find that the SMA data reveal more compact structures than those seen previously; this is likely due to missing flux and/or resolution effects. The total mass loss rates inferred for these sources range between 10^-4 and 10^-3 Msun/yr, while their momentum flux ranges between 10^-2 and 10^1 Lsun/c/s. These values are similar to those found for low-mass Class 0 objects but higher than expected if scaled up according to the luminosity-to-mass ratio. This suggests that there may be additional mechanisms driving the outflows besides radiation pressure on dust grains. \n \n In addition we report the detection of infalling gas toward two of our targets. For G35.20-1.74NW, we detect an inward motion of ~0.5 km/s over a distance of ~1000 AU. For IRAS 18162-2048, we see evidence for both inward motions as well as outward motions along different lines-of-sight.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Outflow and Infall in a Sample of Massive Star Forming Regions . Abstract : We report new images of the outflows driven by massive protostars utilizing the Submillimeter Array ( SMA ) at 1 . 3 cm , which are compared with previous findings obtained with single - dish telescopes .We see that the SMA data reveal more compact systems than those shown later ; this is probably due to missing flux and / or resolution influences . The total mass loss rates inferred for these sources range between 10 ^ - 4 and 10 ^ - 3 Msun / yr , while their momentum flux ranges between 10 ^ - 2 and 10 ^ 1 Lsun / c / s .These figures are comparable to those shown for low - mass Class 0 bodies but higher than expected if scaled up according to the luminosity - to - mass ratio . This implies that there may be additional mechanisms pushing the outflows besides radiation stress on dust grains .In addition we monitor the observation of infalling gas toward two of our objectives . For G35 . 20 - 1 . 74NW , we determine an inward movement of ~ 0 . 5 kilometres / s over a distance of ~ 1000 AU .For IRAS 18162 - 2048 , we see evidence for both inward motions as well as outward motions along different lines - of - sight .",
        "rewrite_text": "In this study, we present new imaging of outflows driven by massive protostars, utilizing the Submillimeter Array (SMA) at a wavelength of 1.3 cm. Our findings are juxtaposed with earlier results obtained from single-dish telescopes, revealing that the SMA data depict more compact outflow systems than previously observed. This discrepancy is likely attributed to factors such as missing flux and variations in resolution. The inferred mass loss rates for the protostars in our sample range from 10^-4 to 10^-3 solar masses per year, while the momentum flux is observed to be between 10^-2 and 10^1 solar luminosities per speed of light per second. These measurements are comparable to those recorded for low-mass Class 0 protostars; however, they exceed expectations when scaled according to the luminosity-to-mass ratio. This suggests the possibility of additional mechanisms contributing to the outflow dynamics, beyond the influence of radiation pressure on dust grains.\n\nFurthermore, we also investigate the infall of gas toward two specific targets within our study. For the source G35.20-1.74NW, we observe an inward gas velocity of approximately 0.5 kilometers per second over a distance of about 1000 astronomical units. In the case of IRAS 18162-2048, our observations reveal evidence of both inward and outward gas motions occurring along different lines of sight. These findings provide crucial insights into the complex interplay of outflow and infall processes in massive star-forming regions, enhancing our understanding of the mechanisms driving star formation in these dynamic environments.",
        "ori-fast-z-score": -1.193117518002609,
        "water-fast-z-score": 4.313310928137536,
        "rewrite-fast-z-score": 0.18569533817705186
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Dynamics-Based Approach to Studying Terrestrial Exoplanets .\nAbstract:\nThe dynamics-based approach is an emerging method for studying terrestrial exoplanets, which are planets with masses similar to that of Earth orbiting other stars in the solar system.  The main goal of this research is to study how these planets form and evolve over time.   This approach uses numerical simulations to model the formation and evolution of planetary systems by solving equations describing the orbital motion of bodies interacting gravitationally.   In addition, it also takes into account physical processes such as tidal dissipation, collisions between planetesimals (small rocky objects), and atmospheric escape.    By using this approach we can better understand how our own planet formed billions of years ago and what conditions were necessary for life on Earth to develop. Keywords: Planetary Science; Astrobiology; Tidal Dissipation; Collisions Between Planetesimals; Atmospheric Escape. Introduction:  The dynamics-based approach is an emergent method for studying terrestrial extrasolar planets, or planets with masses similar to Earth s orbiting other stars within the Solar System.  These types of planets have been discovered recently through space missions like Kepler and K2.  The main goal of the dynamics-based approach is to study how these worlds form and evolve over time.  It does so by modeling the formation and evolution of the entire planetary system numerically via solving equations describing the orbital motions of bodies interacting gravitationally.  Additionally, it incorporates physical processes including tidal dissipation, collisions among planetesimals (smaller rocky objects) and atmospheric escape.  By applying this approach, scientists hope to gain insight about how our own planet formed billion(s) of years ago and what environmental factors may be required for life to exist there.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Dynamics - Based Approach to Studying Terrestrial Exoplanets . Abstract : The dynamics - based alternative is an evolving technique for studying terrestrial exoplanets , which are stars with masses similar to that of Earth orbiting other stars in the solar system .The main goal of this research is to study how these planets emerge and evolve over time . This method uses numerical simulations to model the formation and evolution of planetary structures by modeling parameters describing the orbital movement of bodies interacting gravitationally .In addition , it also took into consideration physical processes such as tidal dissipation , collisions between planetesimals ( small rocky objects ) , and atmospheric escape . By using this methodology we can better understand how our own planet existed billions of years previously and what circumstances were required for people on Earth to develop .Keywords : Planetary Science ; Astrobiology ; Tidal Dissipation ; Collisions Between Planetesimals ; Atmospheric Escape . Introduction : The dynamics - based methods is an emergent technique for studying terrestrial extrasolar stars , or planets with masses similar to Earth s orbiting other stars within the Solar System .These sorts of planets have been known recently through space missions like Kepler and K2 . The main goal of the dynamics - based approach is to study how these worlds create and evolve over time .It does so by modeling the formation and evolution of the entire planetary system numerically via solving equations explaining the orbital motions of bodies interacting gravitationally . Additionally , it combines physical processes including tidal dissipation , collisions among planetesimals ( smaller rocky objects ) and atmospheric release .By applying this methodology , scientists hope to obtain knowledge about how our own planet existed billion ( s ) of years previously and what environmental factors might be required for life to remain there .",
        "rewrite_text": "**Title:** A Dynamics-Based Approach to Investigating Terrestrial Exoplanets\n\n**Abstract:** The dynamics-based approach represents a progressive methodology for the exploration of terrestrial exoplanets—celestial bodies with masses comparable to Earth that orbit stars outside our solar system. This research aims to elucidate the processes governing the formation and evolution of these planets over geological timescales. By employing sophisticated numerical simulations, the study models the intricate dynamics of planetary systems, focusing on the gravitational interactions between various celestial bodies. Key parameters such as orbital mechanics are meticulously analyzed to understand the developmental pathways of these exoplanets.\n\nIn addition to gravitational dynamics, this approach incorporates critical physical phenomena, including tidal dissipation, the impact of collisions among planetesimals (small rocky entities), and the mechanisms of atmospheric escape. These factors are essential for constructing a comprehensive picture of how terrestrial exoplanets evolve and the conditions necessary for sustaining life. By leveraging this methodology, researchers aim to gain insights into the early history of our own planet, exploring the environmental conditions that facilitated the emergence of life on Earth billions of years ago.\n\nThe findings from this research not only enhance our understanding of exoplanetary systems but also provide valuable context for astrobiological studies, as they highlight the potential for life in diverse planetary environments. The dynamics-based approach thus serves as a vital tool in the quest to uncover the mysteries of planetary formation and evolution, offering a pathway to understanding the complex interplay of factors that contribute to the habitability of terrestrial worlds.\n\n**Keywords:** Planetary Science; Astrobiology; Tidal Dissipation; Planetesimal Collisions; Atmospheric Escape.",
        "ori-fast-z-score": 1.2815364865751413,
        "water-fast-z-score": 7.9455262167658764,
        "rewrite-fast-z-score": 0.5262348115842176
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Realization of an Economical Polymer Optical Fiber Demultiplexer .\nAbstract:\nWe have demonstrated the first polymer optical fiber (POF) demultiplexer using a single-mode POF and multimode interference couplers (MMI). The device consists of three MMI s with different lengths, which are connected in series to form a Mach-Zehnder interferometer structure. A broadband light source is coupled into one end of the input POF through a lens system consisting of two lenses. Light emerging at each output port was detected by a photodiode array after passing through another lens system. We obtained a maximum extinction ratio of 20 dB between adjacent channels over a wavelength range of 1530-1570 nm for a channel spacing of 0.8 nm. This result shows that our proposed scheme can be used as a low-cost demultiplexing element for high-density WDM systems based on POFs. \n \n Keywords: Polymer optical fibers, Multimode interference coupler, Channel demultiplexer",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Realization of an Economical Polymer Optical Fiber Demultiplexer . Abstract : We have demonstrated the first polymer optical cable ( POF ) demultiplexer using a single - mode POF and multimode interference couplers ( MMI ) .The system contains of three MMI s with various lengths , which are connected in series to form a Mach - Zehnder interferometer design . A broadband light source is linked into one end of the input POF through a lens system consisting of two lenses .Light emerging at each output port was measured by a photodiode array after passing through another lens system . We obtained a maximum extinction ratio of 20 dB between neighboring channels over a frequency range of 1530 - 1570 nm for a channel spacing of 0 . 8 nm .This result suggests that our proposed system can be used as a small - cost demultiplexing element for high - density WDM networks based on POFs . Keywords : Polymer laser fibers , Multimode interference coupler , Channel demultiplexer",
        "rewrite_text": "Title: Realization of an Economical Polymer Optical Fiber Demultiplexer\n\nAbstract: In this study, we present the development of the first polymer optical fiber (POF) demultiplexer utilizing a single-mode POF in conjunction with multimode interference couplers (MMIs). The proposed system is composed of three MMIs of varying lengths, arranged in series to create a Mach-Zehnder interferometer configuration. A broadband light source is introduced at one end of the input POF via a lens system that incorporates two lenses, ensuring efficient coupling of light into the fiber. The light that emerges from each output port is subsequently measured using a photodiode array, following its passage through an additional lens system designed to optimize detection. Our experimental results demonstrate a maximum extinction ratio of 20 dB between adjacent channels across a frequency range of 1530 to 1570 nm, with a channel spacing of 0.8 nm. These findings indicate that the proposed demultiplexer has significant potential as a cost-effective solution for high-density wavelength division multiplexing (WDM) networks that utilize polymer optical fibers. The implications of this research extend to enhancing the efficiency and affordability of optical communication systems, particularly in applications where space and cost constraints are critical. This work contributes to the ongoing advancements in polymer laser fibers and multimode interference technology, paving the way for future innovations in optical networking. \n\nKeywords: Polymer optical fibers, Multimode interference coupler, Channel demultiplexer.",
        "ori-fast-z-score": 0.7276068751089989,
        "water-fast-z-score": 4.302822993603817,
        "rewrite-fast-z-score": 1.6135685927792485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spitzer/IRS Imaging and Spectroscopy of the luminous infrared galaxy NGC 6052 (Mrk 297) .\nAbstract:\nWe present Spitzer Infrared Spectrograph (IRS) observations of the nearby, interacting galaxy pair Mrk 297. The system consists of two galaxies separated by ~3 kpc in projection; one is an elliptical galaxy with a bright nuclear point source, while the other has a Seyfert 2 nucleus surrounded by extended emission lines. We detect several molecular hydrogen transitions including H$_2$ S(0), S(1), S(2), S(3), S(4), S(5), S(6), S(7), S(8), S(9), S(10), S(11), S(12), S(13), S(14), S(15), S(16), S(17), S(18), S(19), S(20). These are detected over a wide range of spatial scales ranging from <100 pc to >500 pc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spitzer / IRS Imaging and Spectroscopy of the luminous infrared galaxy NGC 6052 ( Mrk 297 ) . Abstract : We report Spitzer Infrared Spectrograph ( IRS ) observations of the nearby , interacting galaxy pair Mrk 297 .The system contains of two galaxies linked by ~ 3 kpc in projection ; one is an elliptical galaxy with a bright nuclear point source , while the other has a Seyfert 2 nucleus accompanied by extended emission lines . We detect many molecular hydrogen changes including H $ _ 2 $ S ( 0 ) , S ( 1 ) , S ( 2 ) , S ( 3 ) , S ( 4 ) , S ( 5 ) , S ( 6 ) , S ( 7 ) , S ( 8 ) , S ( 9 ) , S ( 10 ) , S ( 11 ) , S ( 12 ) , S ( 13 ) , S ( 14 ) , S ( 15 ) , S ( 16 ) , S ( 17 ) , S ( 18 ) , S ( 19 ) , S ( 20 ) .These are detected over a broad variety of spatial scales ranging from < 100 pc to > 500 pc .",
        "rewrite_text": "We present observations from the Spitzer Infrared Spectrograph (IRS) focusing on the nearby interacting galaxy pair known as Mrk 297, which is also designated as NGC 6052. This intriguing system consists of two galaxies that are approximately 3 kpc apart in projection. One of the galaxies is an elliptical type, characterized by a prominent nuclear point source, while the other exhibits a Seyfert 2 nucleus that is accompanied by extended emission lines. Our analysis reveals a rich spectrum of molecular hydrogen transitions, specifically detecting H$_2$ rotational lines from S(0) through S(20). These transitions are observed across a diverse range of spatial scales, from less than 100 parsecs to over 500 parsecs. The findings contribute significantly to our understanding of the physical processes occurring in interacting galaxies, particularly in relation to star formation and the dynamics of molecular gas. The detection of multiple H$_2$ transitions indicates a complex interplay of processes within this luminous infrared galaxy, offering insights into the conditions that foster molecular hydrogen formation and excitation. This study not only enhances our knowledge of Mrk 297 but also serves as a valuable reference for future investigations into the characteristics of similar interacting galaxy systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.2024699769846983,
        "rewrite-fast-z-score": 1.8973665961010275
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gamma-Ray Bursts as Manifestation of Collisions of Primordial Black Holes with Stars .\nAbstract:\nWe propose that gamma-ray bursts (GRBs) are the result of collisions between primordial black holes and stars in globular clusters, which occur at rates predicted by current models for GRB production.  We show how this scenario can explain many observed properties of GRBs including their duration distribution, luminosity function, redshift evolution, and beaming fraction.   The proposed model also predicts an observable population of binary systems containing both a star and a PBH, which may provide additional tests to distinguish it from other scenarios. Gamma-ray bursts (GRBs; see Figure 1 ) are intense flashes of high-energy radiation lasting only milliseconds up to several minutes  1  . They have been detected out to redshifts z = 8  2  , corresponding to ages of less than one billion years after the Big Bang  3  .\nThe most popular explanation for these phenomena is that they arise when extremely massive stars collapse into black holes  4  or neutron stars  5  . However, there are some difficulties associated with this picture  6  :  First, the rate of such events required to produce all known GRBs exceeds predictions based on stellar formation theory  7 ; secondly, the energy released during the explosion does not appear sufficient to power the brightest GRBs  8  ; thirdly, the number density of very massive stars decreases rapidly towards higher redshifts  9  , whereas observations suggest that the rate of GRB production increases  10  .  Finally, if GRBs were produced solely through collapsars then we would expect them to be distributed randomly throughout space; however, recent studies indicate that they tend to cluster together  11  .\nIn order to overcome these problems, alternative explanations involving mergers of compact objects  12  , tidal disruption flares  13  , and hypernovae  14  have been suggested. In addition,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gamma - Ray Bursts as Manifestation of Collisions of Primordial Black Holes with Stars . Abstract : We suggest that gamma - ray bursts ( GRBs ) are the result of collisions between primordial black holes and stars in globular complexes , which occur at levels predicted by current scenarios for GRB development .We see how this situation can describe several observed properties of GRBs notably their duration distribution , luminosity function , redshift development , and beaming fraction . The proposed theory even predicts an observable population of binary systems featuring both a star and a PBH , which would offer additional studies to distinguish it from other scenarios .Gamma - ray bursts ( GRBs ; view Figure 1 ) are intense flashes of high - energy rays lasting only milliseconds up to several moments 1 . They have been detected out to redshifts z = 8 2 , equivalent to periods of fewer than one billion decades after the Big Bang 3 .The most popular reason for these phenomena is that they occur when unusually vast galaxies fall into black holes 4 or neutron galaxies 5 . However , there are some difficulties related with this picture 6 : First , the frequency of such events required to produce all known GRBs increases assumptions based on stellar formation theory 7 ; secondly , the electricity created during the explosion does not appear adequate to power the brightest GRBs 8 ; thirdly , the number density of very huge stars reduces rapidly towards higher redshifts 9 , whereas observations suggest that the rate of GRB development increases 10 .Finally , if GRBs were produced solely through collapsars then we may expect them to be spread randomly throughout space ; however , recent studies confirm that they tend to group together 11 . In order to overcome these problems , alternative theories involving mergers of compact galaxies 12 , tidal disruption flares 13 , and hypernovae 14 have been proposed .In addition,...",
        "rewrite_text": "**Title**: Gamma-Ray Bursts as Manifestations of Collisions Between Primordial Black Holes and Stars\n\n**Abstract**: In this study, we propose a novel explanation for gamma-ray bursts (GRBs), suggesting that they arise from the collisions of primordial black holes (PBHs) with stars within globular clusters. This hypothesis aligns with current theoretical frameworks regarding the formation and evolution of GRBs. Our analysis demonstrates that this collision scenario can effectively account for several key characteristics of GRBs, including their duration distribution, luminosity function, redshift evolution, and beaming fraction. Furthermore, we predict the existence of observable binary systems comprising both a star and a PBH, which could serve as a distinguishing feature of our model compared to alternative explanations.\n\nGamma-ray bursts are characterized by their brief yet intense emissions of high-energy radiation, lasting from milliseconds to several minutes. These phenomena have been observed at redshifts as high as z = 8, indicating their occurrence less than one billion years after the Big Bang. The prevailing theories attribute GRBs to the collapse of massive stars into black holes or neutron stars. However, this conventional view faces several challenges: the frequency of such stellar collapse events required to account for all observed GRBs exceeds predictions based on stellar formation models; the energy released during these explosions appears insufficient to explain the luminosity of the brightest GRBs; and the density of massive stars diminishes significantly at higher redshifts, while GRB rates seem to increase. Additionally, if GRBs were solely the result of collapsars, we would expect them to be uniformly distributed in space, yet recent observations indicate a clustering tendency.\n\nTo address these inconsistencies, alternative models have been proposed, including mergers of compact galaxies, tidal disruption events, and hypernovae. Our research contributes to this ongoing discourse by providing a compelling framework that not only resolves existing issues but also opens avenues for future observational studies aimed at confirming the presence of PBH-star binaries. This work underscores the need for further investigation into the role of primordial black holes in the cosmic landscape of high-energy astrophysical phenomena.",
        "ori-fast-z-score": 0.08481889296799709,
        "water-fast-z-score": 8.451542547285166,
        "rewrite-fast-z-score": 0.8669214468630108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Blazhko behaviour of RR Geminorum II -- long-term photometric results .\nAbstract:\nThe Blazhko effect is one of the most mysterious phenomena in pulsating stars, and it has been observed for more than 100 years now only on RR Lyrae-type variables (RR Lyr). The first systematic study was carried out by Blazhko himself who found that about half of all known RR Lyr show this phenomenon. In recent decades many efforts have been made to understand its origin but no satisfactory explanation exists yet. \n \n We present here new results obtained with the WET collaboration during two observing runs in 2002 and 2004. Our data cover almost ten years of observations which allow us to investigate the Blazhko effect over an unprecedentedly large time span. This allows us to determine the mean period change rate as well as the amplitude modulation properties of RR Gem II. These are compared with those derived for other Blazhko-modulated RR Lyr. \nWe find that our results agree very well with previous studies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Blazhko behaviour of RR Geminorum II - - long - term photometric findings . Abstract : The Blazhko effect is one of the most bizarre phenomena in pulsating stars , and it has been observed for more than 100 years now only on RR Lyrae - class variables ( RR Lyr ) .The first systematic study was carried out by Blazhko himself who found that about half of all known RR Lyr show this phenomenon . In recent decades several efforts have been made to comprehend its origin but no satisfactory excuse exists yet .We present here new data acquired with the WET collaboration during two observing walks in 2002 and 2004 . Our data cover nearly ten years of measurements which allow us to examine the Blazhko effect over an unprecedentedly large time frame .This enables us to estimate the mean period change rate as well as the amplitude modulation properties of RR Gem II . These are compared with those developed for other Blazhko - modulated RR Lyr .We see that our findings agree very best with previous researchers .",
        "rewrite_text": "Title: The Blazhko Behavior of RR Geminorum II - Long-Term Photometric Findings\n\nAbstract: The Blazhko effect represents one of the most intriguing phenomena observed in pulsating stars, specifically within the RR Lyrae class variables (RR Lyr), and has been documented for over a century. The phenomenon was first systematically investigated by Blazhko, who noted that approximately half of the known RR Lyrae stars exhibit this behavior. Despite numerous studies in recent decades aimed at uncovering its underlying mechanisms, a comprehensive explanation remains elusive. In this article, we present new photometric data collected through the Whole Earth Telescope (WET) collaboration during two observational campaigns in 2002 and 2004. Our dataset spans nearly a decade of measurements, providing a unique opportunity to analyze the Blazhko effect over an extensive temporal range. This extensive dataset allows us to calculate the mean rate of period change and to explore the amplitude modulation characteristics of RR Geminorum II. We compare our results with those obtained from other RR Lyrae stars exhibiting Blazhko modulation. Our findings show a strong agreement with previous research, reinforcing the understanding of the Blazhko effect in RR Lyrae stars. This study not only contributes to the ongoing discourse surrounding the Blazhko phenomenon but also enhances our knowledge of the complex behavior exhibited by pulsating stars. The insights gained from our long-term observations may pave the way for future investigations aimed at unraveling the mysteries of this captivating stellar behavior.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 4.588314677411235,
        "rewrite-fast-z-score": 2.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Holes within galaxies: the egg or the hen? .\nAbstract:\nWe present new results on the evolution and properties of galactic holes, based on an analysis of deep optical images obtained with the Hubble Space Telescope (HST). We find that most of these holes are associated to bright star clusters in their centers, which we identify as supermassive black holes (SMBHs) by means of SED fitting techniques. The masses inferred for these objects range between 10^6 M_sol and 10^9 M_sol . In addition, we have found evidence suggesting that some of them may be powered by nuclear activity. Finally, we show how our sample is biased towards massive systems at high redshifts due to observational selection effects. Galactic holes are ubiquitous features observed across all types of galaxies. They appear as dark regions surrounded by diffuse emission, and they can reach sizes up to several hundred parsecs. Their origin has been debated since their discovery more than 50 years ago; however, it remains unclear whether they form spontaneously through gravitational instabilities, or if they are created by other processes such as mergers or feedback mechanisms related to active nuclei. Here we report new results on this topic using data taken with HST/ACS/WFC3. Our main findings are:  - Most of the holes studied here are associated to bright central sources identified as supermassive black hole candidates.  - Some of the holes seem to be powered by nuclear activity.  - There seems to exist a correlation between the mass of the holes and the luminosity/stellar mass of their host galaxy.  - The majority of the holes analyzed here were discovered thanks to their association with AGN.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Holes within galaxies : the egg or the hen ? .Abstract : We report new data on the evolution and properties of galactic holes , using on an assessment of deep optical images obtained with the Hubble Space Telescope ( HST ) . We see that most of these holes are related to faint star clusters in their areas , which we identify as supermassive black holes ( SMBHs ) by means of SED fitting methods .The masses inferred for these objects range between 10 ^ 6 M _ sol and 10 ^ 9 M _ sol . In addition , we have discovered evidence indicating that some of them may be powered by nuclear activity .Finally , we show how our sample is biased towards large systems at high redshifts due to observational selection influence . Galactic holes are ubiquitous features detected across all types of galaxies .They appear as dark regions surrounded by diffuse emission , and they can reach dimensions up to several hundred parsecs . Their origin has been discussed since their discovery more than 50 centuries earlier ; however , it remains unsure whether they create spontaneously through gravity instabilities , or if they are created by other processes such as mergers or feedback systems associated to active clusters .Here we publish new data on this topic utilizing information taken with HST / ACS / WFC3 . Our main results are : - Most of the holes studied here are related to bright central sources identified as supermassive black hole candidates .- Some of the holes appear to be powered by nuclear activity . - There seems to exist a correlation between the mass of the holes and the luminosity / stellar mass of their target galaxy .- The majority of the holes analyzed here were discovered due to their association with AGN .",
        "rewrite_text": "**Title: Holes within Galaxies: The Egg or the Hen?**\n\n**Abstract:** In this study, we present new findings on the evolution and characteristics of galactic holes, derived from an analysis of deep optical images captured by the Hubble Space Telescope (HST). Our investigation reveals that a significant number of these holes are associated with faint star clusters in their vicinity, which we have identified as candidates for supermassive black holes (SMBHs) through spectral energy distribution (SED) fitting techniques. The estimated masses of these SMBH candidates range from \\(10^6 M_{\\odot}\\) to \\(10^9 M_{\\odot}\\). Furthermore, we provide evidence suggesting that some of these holes may be driven by nuclear activity. \n\nOur research also highlights a bias in our sample towards larger systems at higher redshifts, a consequence of observational selection effects. Galactic holes are prevalent features observed in various types of galaxies, manifesting as dark regions surrounded by diffuse light, with sizes that can extend to several hundred parsecs. The origin of these holes has been a topic of debate since their discovery over five centuries ago, with ongoing discussions regarding whether they form spontaneously due to gravitational instabilities or if they arise from alternative processes such as mergers or feedback mechanisms linked to active star clusters.\n\nIn this paper, we present new data obtained using HST's Advanced Camera for Surveys (ACS) and Wide Field Camera 3 (WFC3). Our key findings include: (1) A majority of the holes examined are associated with prominent central sources identified as candidates for supermassive black holes; (2) Some of these holes exhibit signs of being powered by nuclear activity; (3) A correlation appears to exist between the mass of the holes and the luminosity or stellar mass of their host galaxies; (4) Most of the holes identified in our analysis were discovered due to their connection with active galactic nuclei (AGN). These insights contribute to our understanding of the complex interplay between galactic structures and their central black holes.",
        "ori-fast-z-score": -0.3592106040535498,
        "water-fast-z-score": 5.945288613087962,
        "rewrite-fast-z-score": 1.3241694217637887
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Solidity of viscous liquids. V. Long-wavelength dominance of the dynamics .\nAbstract:\nWe study the solidification process in a model system consisting of particles interacting via repulsive soft-core potentials and subject to an external driving force. We show that, for sufficiently large values of the driving amplitude, the system undergoes a phase transition into a state where it behaves as if it were made up by two coexisting phases with different densities. The low-density phase is characterized by a slow relaxation towards equilibrium which can be described within mean-field theory. In contrast, the high density phase relaxes rapidly toward its stationary configuration. \n \n This behavior resembles closely what happens during the freezing of colloidal suspensions driven out of equilibrium by an applied shear flow. Our results suggest that this analogy may not only hold at the level of static properties but also when considering dynamical features such as the response to perturbations or the presence of aging effects. Finally we discuss possible extensions of our work to more realistic models describing the glassy dynamics observed experimentally in supercooled liquids. \nI. INTRODUCTORY REMARK\nIn recent years there has been growing interest on the possibility of observing analogies between the physics of glasses and other disordered systems  1  . One of these analogies concerns the role played by fluctuations in determining the macroscopic behaviour  2  , another one relates to the existence of metastable states  3  .\nThe aim of this Letter is to investigate whether similarities exist also in terms of dynamic properties. To this end we consider a simple model of glass-forming liquid  4  whose microscopic degrees of freedom are represented by N point-like particles moving in d dimensions under the action of pairwise interactions. These particles interact through a potential energy function U(r) = 4ε 1 − exp{−α(r/σ)}  2 /πσd, where r denotes their separation distance, ε sets the overall scale of energies, α controls the range of interaction (we take here α = 1), while σ fixes the length unit. For simplicity we assume periodic boundary conditions so that the total number of particles remains constant throughout the simulation. As usual, we define the reduced temperature T * ≡ kT/",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Solidity of viscous liquids.V. Long-wavelength dominance of the dynamics .Abstract : We explore the solidification mechanism in a model structure formed of molecules evolving via repulsive soft - core potentials and subject to an external driving field . We see that , for enough large values of the driving frequency , the system undergoes a phase shift into a state where it behaves as if it were made up by two coexisting phases with varying densities .The lowest - density phase is characterized by a slow relaxation towards equilibrium which can be described within mean - field model . In comparison , the high density phase relaxes rapidly toward its stationary configuration .This phenomenon resembles closely what comes during the freezing of colloidal suspensions driven out of equilibrium by an imposed shear flow . Our results show that this analogy might not only hold at the level of static properties but also when examining dynamical characteristics such as the response to perturbations or the presence of aging influences .Finally we pursue possible extend of our work to more realistic theories describing the glassy dynamics observed experimentally in supercooled liquids . I .INTRODUCTORY REMARK In recent years there has been growing interest on the prospect of discovering analogies between the physics of glasses and other disordered systems 1 . One of these analogies concerns the part played by fluctuations in determining the macroscopic behaviour 2 , another one refers to the existence of metastable states 3 .The goal of this Letter is to examine whether comparisons exist also in terms of dynamic characteristics . To this end we study a simple simulation of glass - creating solid 4 whose microscopic degrees of liberty are represented by N point - like particles moving in d dimensions under the action of pairwise interactions .These particles react through a potential energy relation U ( r ) = 4ε 1 − exp { −α ( r / π ) } 2 / πσd , where r denotes their separation distance , ε sets the overall scale of energies , α handles the range of interaction ( we took here α = 1 ) , while ρ fixes the length unit . For simplicity we suppose periodic border conditions so that the total number of particles stay constant throughout the model .As usual, we define the reduced temperature T * ≡ kT/",
        "rewrite_text": "**Title:** Solidification of Viscous Liquids: Long-Wavelength Dynamics Dominance\n\n**Abstract:** This study investigates the solidification process in a model system composed of molecules interacting through repulsive soft-core potentials, subjected to an external driving field. Our findings reveal that at sufficiently high driving frequencies, the system transitions into a phase characterized by the coexistence of two distinct phases with varying densities. The low-density phase exhibits slow relaxation towards equilibrium, which can be effectively described using a mean-field approach. In contrast, the high-density phase demonstrates rapid relaxation to its stationary state. This behavior bears a striking resemblance to the freezing dynamics observed in colloidal suspensions that are driven out of equilibrium by an applied shear flow. Our results suggest that this analogy extends beyond static properties to encompass dynamic characteristics, including the system's response to external perturbations and the influence of aging effects.\n\nFurthermore, we explore the potential implications of our findings for more realistic theoretical frameworks that aim to describe the glassy dynamics observed in supercooled liquids. The introductory section highlights the increasing interest in uncovering parallels between the physics of glasses and other disordered systems. Notably, one such analogy pertains to the role of fluctuations in shaping macroscopic behavior, while another relates to the presence of metastable states. This letter aims to determine whether dynamic characteristics also exhibit comparable behavior across these systems. To achieve this, we conduct simulations of a glass-forming solid, represented by N point-like particles moving in d dimensions under pairwise interactions. The interaction is governed by a potential energy function, U(r) = 4ε(1 - exp{-α(r/π)})²/πσd, where r is the separation distance, ε establishes the energy scale, α controls the interaction range (with α set to 1 in this study), and ρ defines the length unit. We employ periodic boundary conditions to maintain a constant total number of particles throughout the simulation. The reduced temperature is defined as T* ≡ kT/ε, facilitating the analysis of the system's behavior under varying thermal conditions.",
        "ori-fast-z-score": -1.9230769230769231,
        "water-fast-z-score": 6.614487515046438,
        "rewrite-fast-z-score": -0.4629100498862757
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal Electrostatic Space Tower (Mast, New Space Elevator) .\nAbstract:\nThe space elevator is an important project in the field of aerospace engineering and has been studied for many years by scientists all over the world. The main purpose of this study was to find out how much energy would be needed to build such a tower with different materials.  In order to do that we used two methods - one analytical method based on the theory of elasticity and another numerical method using finite element analysis software ANSYS. We found out that the optimal material should have high strength but low density. It turned out that carbon nanotubes are very good candidates as they can reach extremely high strengths while having relatively small densities. This work will help us design better space elevators in the future. Keywords: Energy consumption, Carbon Nanotube, Optimum Material, Finite Element Analysis Software, Analytical Method, Elasticity Theory, Space Elevator. 1 Introduction   Space elevators are considered to be one of the most promising projects in the field of aeronautics and astronautics  1  . They could provide transportation between Earth orbit and ground level without any fuel consumption  2  , which makes them especially useful for transporting people or cargo  3  .\nIn recent decades there were several attempts made at building space elevators  4  . However none of these designs managed to become fully functional  5  . One of the reasons why it is so difficult to build a working space elevator is because its weight limit is determined by the maximum static load  6  . If the weight exceeds this value then the cable will sag under gravity  7  . Another problem is that the cables need to support their own weight  8  . Therefore if you want to make your space elevator lighter than air  9  , you must use some kind of counterweight  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal Electrostatic Space Tower ( Mast , New Space Elevator ) . Abstract : The space lift is an important project in the field of aviation design and has been studied for numerous years by scientists all over the world .The main aim of this study was to find out how many heat might be needed to build such a building with various materials . In order to do that we using two methods - one analytical method using on the theoretical of elasticity and another numerical technique utilizing finite element assessment software ANSYS .We figured out that the ideal structure should have high strength but little density . It turned out that carbon nanotubes are very good candidates as they can reach extremely high strengths while having relatively small densities .This project will assist us design good space elevators in the future . Keywords : Energy consumption , Carbon Nanotube , Optimum Material , Finite Element Analysis Software , Analytical Method , Elasticity Theory , Space Elevator .1 Introduction Space lifts are considered to be one of the most attractive projects in the field of aeronautics and astronautics 1 . They could enable transportation between Earth orbit and land level without any fuel expenditure 2 , which makes them especially suitable for handling persons or cargo 3 .In past decades there were several efforts made at building room lifts 4 . However none of these designs able to become completely structural 5 .One of the explanations why it is so difficult to build a working space lift is because its weight limit is calculated by the maximum static load 6 . If the weight reaches this amount then the cable will sag under gravity 7 .Another difficulty is that the ropes require to support their own weight 8 . Therefore if you want to make your space train lighter than air 9 , you must use some kind of counterweight 10 .",
        "rewrite_text": "**Title:** Optimal Electrostatic Space Tower (Mast, New Space Elevator)\n\n**Abstract:** The concept of a space elevator represents a significant advancement in aerospace engineering, garnering extensive research interest from scientists globally over the years. This study aims to determine the thermal energy requirements for constructing such a structure using various materials. To achieve this, we employed two distinct methodologies: an analytical approach grounded in the principles of elasticity and a numerical technique utilizing the finite element analysis software ANSYS. Our findings indicate that the optimal design for a space elevator must prioritize high strength-to-weight ratios. Notably, carbon nanotubes emerge as exceptional candidates due to their remarkable strength capabilities coupled with low density. This research not only contributes to the theoretical framework surrounding space elevator construction but also lays the groundwork for future designs that could revolutionize space transportation. \n\n**Keywords:** Energy consumption, Carbon Nanotube, Optimum Material, Finite Element Analysis Software, Analytical Method, Elasticity Theory, Space Elevator.\n\n**1 Introduction:** Space elevators are increasingly viewed as one of the most promising innovations in the realms of aeronautics and astronautics. They offer the potential for efficient transportation between Earth's surface and orbit without the need for fuel, making them particularly advantageous for the movement of people and cargo. Despite several attempts over the past few decades to develop functional space elevators, none have achieved full structural viability. A primary challenge in constructing a space elevator lies in the weight limitations imposed by maximum static loads; exceeding this threshold results in cable sagging due to gravitational forces. Additionally, the cables must be capable of supporting their own weight, complicating the design further. To create a structure that is lighter than air, the incorporation of a counterweight becomes essential. This study addresses these challenges and explores innovative material solutions to facilitate the realization of a functional space elevator.",
        "ori-fast-z-score": 0.6859943405700353,
        "water-fast-z-score": 8.17629817532677,
        "rewrite-fast-z-score": 0.3223291856101521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Einstein clusters as galactic dark matter halos .\nAbstract:\nWe present the results of an analysis of galaxy cluster data in terms of their gravitational lensing properties and X-ray emission, with particular emphasis on the comparison between observed and predicted values for the mass-to-light ratio M/L. We find that the best-fit value of this quantity is consistent with the predictions based on standard CDM models if one assumes that most of the baryonic component of these systems resides within galaxies rather than being distributed throughout the intracluster medium (ICM). This result suggests that the ICM may be heated by some mechanism other than gravity alone. \n \n Keywords: Galaxy cluster, Dark Matter Halo, Gravitational Lensing, Mass-to-Light Ratio, X-Ray Emission \n \n \n \n 1 Introduction \n \n The study of galaxy clusters has been instrumental to our understanding of cosmology over the past few decades. In fact, it was through observations of galaxy clusters that we first discovered evidence supporting the existence of non-baryonic dark matter  1  . Today, galaxy clusters are still used extensively to test theories about structure formation  2  , and they provide important constraints on cosmological parameters such as the Hubble constant  3  or the equation-of-state parameter w  4  . \n \n However, despite all its successes, there remain several open questions regarding galaxy clusters which have yet to be answered satisfactorily. For example, while current observational techniques allow us to measure accurately the total amount of light emitted by a galaxy cluster, it remains difficult to determine how much of this light comes from stars inside individual galaxies versus diffuse gas located outside them  5  . Similarly, although we can estimate fairly well the total gravitating mass of a galaxy cluster using various methods  6  , it is not clear what fraction of this mass is associated with visible objects like galaxies  7, 8  . Finally, even though we know that galaxy clusters contain large amounts of hot plasma  9  , it is unclear whether this material is gravitationally bound to the system  10  .\n \nIn order to address these issues, we will use two different datasets obtained from the Chandra Observatory  11  : the sample of galaxy clusters studied by Vikhlinin et",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Einstein complexes as galactic dark matter halos . Abstract : We present the conclusion of an assessment of galaxy cluster data in terms of their gravitational lensing behavior and X - ray radiation , with particular emphasis on the comparison between seen and anticipated readings for the mass - to - light density M / L .We see that the best - fitting value of this quantity is compatible with the estimates based on normal CDM models if one suppose that most of the baryonic component of these systems resides within galaxies rather than being dispersed throughout the intracluster medium ( ICM ) . This result suggests that the ICM could be heated by some process other than gravity alone .Keywords : Galaxy cluster , Dark Matter Halo , Gravitational Lensing , Mass - to - Light Ratio , X - Ray Emission 1 Introduction The investigation of galaxy galaxies has been instrumental to our understanding of cosmology over the previous few century . In indeed , it was through observations of galaxy clusters that we first discovered evidence proving the existence of non - baryonic black material 1 .Today , galaxy clusters are still used heavily to test assumptions about structure formation 2 , and they pose important restrictions on cosmological factors such as the Hubble constant 3 or the equation - of - state variable f 4 . However , despite all its successes , there remain many open questions regarding galaxy clusters which have yet to be answered satisfactorily .For instance , while contemporary observational techniques permit us to measure correctly the total quantity of light emitted by a galaxy cluster , it remains impossible to predict how many of this light originates from stars inside individual stars vs diffuse gas located outside them 5 . Similarly , although we can calculate fairly good the total gravitating mass of a galaxy cluster using numerous technologies 6 , it is not clear what fraction of this mass is associated with seen bodies like stars 7 , 8 .Finally , even though we know that galaxy regions contain significant amounts of bright plasma 9 , it is uncertain whether this material is gravitationally bound to the system 10 . In order to overcome these problems , we will use two different datasets obtained from the Chandra Observatory 11 : the sample of galaxy galaxies studied by Vikhlinin et",
        "rewrite_text": "**Title: On Einstein Complexes as Galactic Dark Matter Halos**\n\n**Abstract:** In this study, we analyze galaxy cluster data with a focus on their gravitational lensing characteristics and X-ray emissions, particularly examining the relationship between observed and expected mass-to-light density ratios (M/L). Our findings indicate that the optimal value for this ratio aligns with predictions derived from conventional Cold Dark Matter (CDM) models, provided that the majority of the baryonic matter in these systems is concentrated within galaxies rather than being uniformly distributed throughout the intracluster medium (ICM). This observation raises the possibility that the heating of the ICM may result from mechanisms beyond gravitational influences alone. \n\nThe exploration of galaxy clusters has significantly advanced our understanding of cosmology over the past century. Notably, it was through the study of these clusters that we first gathered compelling evidence for the existence of non-baryonic dark matter. Presently, galaxy clusters continue to serve as critical tools for testing theoretical frameworks regarding structure formation and impose vital constraints on key cosmological parameters, such as the Hubble constant and the equation of state variable. Despite the progress made, numerous unresolved questions persist concerning galaxy clusters. For instance, while modern observational techniques allow for accurate measurements of the total light emitted by a galaxy cluster, distinguishing the contributions from individual stars versus diffuse gas remains a challenge. Furthermore, although we can estimate the total gravitating mass of a galaxy cluster using various methodologies, the proportion of this mass attributable to visible entities, such as stars, is still uncertain. Additionally, while it is acknowledged that galaxy regions harbor substantial amounts of luminous plasma, the gravitational binding of this material to the system is yet to be definitively established. To address these issues, we will utilize two distinct datasets obtained from the Chandra Observatory, focusing on the galaxy clusters analyzed by Vikhlinin et al. \n\n**Keywords:** Galaxy Cluster, Dark Matter Halo, Gravitational Lensing, Mass-to-Light Ratio, X-Ray Emission.",
        "ori-fast-z-score": -1.8407159732336889,
        "water-fast-z-score": 7.167432805632419,
        "rewrite-fast-z-score": -0.9797958971132713
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Catalog of HI Clouds in the Large Magellanic Cloud .\nAbstract:\nWe present an all-sky survey for neutral hydrogen (HI) clouds associated with the Large Magellanic cloud (LMC). The LMC is known to have many small, isolated HI clouds that are not gravitationally bound and may be tidally stripped material or remnants of dwarf galaxies destroyed by tidal forces during close encounters between the Milky Way Galaxy and the LMC.  We use data obtained at Arecibo Observatory as part of the ALFALFA survey to search for new HI clouds near the LMC. In addition we combine our results with previous surveys conducted using Parkes telescope observations and single dish telescopes located on Mauna Kea, Hawaii. Our analysis reveals a total of 16 previously uncatalogued HI clouds within 10 degrees of the LMC center. These clouds range in size from 1 kpc to 15 kpc and contain up to 3 x 1012 M⊙ of HI gas.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Catalog of HI Clouds in the Large Magellanic Cloud . Abstract : We report an all - skies study for neutral hydrogen ( HI ) skies related with the Large Magellanic cloud ( LMC ) .The LMC is known to have many small , isolated HI clouds that are not gravitationally locked and may be tidally stripped matter or remnants of dwarf stars destroyed by tidal forces during close contacts between the Milky Way Galaxy and the LMC . We use data acquired at Arecibo Observatory as part of the ALFALFA survey to search for additional HI clouds near the LMC .In addition we merge our findings with previous analyses conducted using Parkes telescope measurements and multi dish telescopes located on Mauna Kea , Hawaii . Our study reveals a total of 16 formerly uncatalogued HI clouds within 10 degrees of the LMC center .These clouds range in height from 1 kpc to 15 kpc and comprise up to 3 x 1012 [UNK] of HI gas .",
        "rewrite_text": "We present a comprehensive survey of neutral hydrogen (HI) clouds associated with the Large Magellanic Cloud (LMC), an irregular dwarf galaxy that exhibits a rich structure of isolated HI clouds. These clouds are not bound by gravitational forces and are hypothesized to be remnants of tidal interactions, possibly resulting from the gravitational influence of the Milky Way Galaxy on the LMC. Utilizing data collected from the Arecibo Observatory as part of the ALFALFA survey, we aimed to identify additional HI clouds in proximity to the LMC. Our research also integrates findings from previous studies that employed measurements from the Parkes telescope and various multi-dish telescopes situated on Mauna Kea, Hawaii. The results of our investigation have led to the identification of 16 previously uncatalogued HI clouds located within a 10-degree radius of the LMC's center. These newly discovered clouds exhibit a range of heights from 1 kiloparsec (kpc) to 15 kpc and collectively contain an estimated mass of up to 3 x 10^12 solar masses of HI gas. This catalog not only enhances our understanding of the HI distribution in the vicinity of the LMC but also provides valuable insights into the processes of tidal stripping and the dynamics of dwarf galaxies in interaction with larger galactic structures. Our findings contribute to the broader discourse on galaxy evolution and the role of external gravitational forces in shaping the characteristics of satellite galaxies.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.764608329895903,
        "rewrite-fast-z-score": 2.8
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extended envelopes around Galactic Cepheids III. Y Oph and alpha Per from near-infrared interferometry with CHARA/FLUOR .\nAbstract:\nWe present new measurements of the angular diameter variations for two classical Cepheids: Y Ophiuchi (Y Oph) and Alpha Persei (alpha Per). These stars are among the brightest in their class, making them ideal targets to study using infrared interferometry. We used the FLUOR instrument on the CHARA array at Mount Wilson Observatory to obtain high-precision visibility data over several pulsation cycles. The observations were performed simultaneously in H-band (1.6 microns), K-band (2.0 microns), and L-band (3.8 microns).\nThe results show that both stars have extended atmospheres which vary significantly during the pulsations cycle. In particular we find that:  - For Y Oph, our best-fit model is consistent with an atmosphere extending up to about 1 AU above its photosphere.  - For alpha Per, our best-fit models suggest that the star has an extended atmosphere reaching out to more than 2 AU above its photosphere.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Extended envelopes around Galactic Cepheids III.Y Oph and alpha Per from near-infrared interferometry with CHARA/FLUOR .Abstract : We present new measurements of the angular distance changes for two classical Cepheids : Y Ophiuchi ( Y Oph ) and Alpha Persei ( beta Per ) . These galaxies are among the brightest in their category , making them ideal targets to study using infrared interferometry .We utilized the FLUOR instrument on the CHARA array at Mount Wilson Observatory to obtain high - precision visibility data over several pulsation periods . The surveys were performed simultaneously in H - band ( 1 . 6 microns ) , K - band ( 2 . 0 microns ) , and L - band ( 3 . 8 microns ) .The results show that both stars have extended atmospheres which varies dramatically during the pulsations cycle . In particular we find that : - For Y Oph , our better - fitting model is compatible with an environment stretching up to about 1 AU above its photosphere .- For alpha Per , our better - fitting models suggest that the star has an extended atmosphere extending out to more than 2 AU above its photosphere .",
        "rewrite_text": "We present new findings on the angular distance variations of two classical Cepheids, Y Ophiuchi (Y Oph) and Alpha Persei (α Per), utilizing near-infrared interferometry. These stars are among the brightest representatives of their class, making them excellent candidates for detailed study through infrared techniques. Our observations were conducted using the FLUOR instrument on the CHARA array located at Mount Wilson Observatory, allowing us to gather high-precision visibility data across multiple pulsation cycles. The measurements were taken simultaneously in three near-infrared bands: H-band (1.6 microns), K-band (2.0 microns), and L-band (3.8 microns). \n\nThe analysis reveals that both Y Oph and α Per possess extended atmospheres that exhibit significant variability throughout their pulsation cycles. Specifically, for Y Oph, our best-fitting model indicates an atmospheric extension reaching approximately 1 astronomical unit (AU) above its photosphere. In contrast, for α Per, the models suggest a more extensive atmosphere, extending beyond 2 AU above the photosphere. These findings contribute to our understanding of the physical characteristics of Cepheid variables and their atmospheric dynamics, highlighting the importance of high-resolution interferometric techniques in astrophysical research. The results underscore the complex nature of stellar atmospheres and their evolution during pulsation, providing valuable insights into the behavior of these luminous stars.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": 0.741998516004452
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Brane Tilings .\nAbstract:\nWe introduce the concept of brane tilings, which are generalizations of tiling groups and can be used to describe many different types of physical systems in terms of their symmetries.  We show that any finite group is isomorphic to some brane tiling group (Theorem 1). In particular, we prove that all Coxeter groups are brane tiling groups (Corollary 2), as well as all Artin groups with torsion-free abelian subgroups (Corollary 3).  As an application, we give a new proof for the classification theorem of finite simple groups by showing how they arise naturally through the action of brane tilings on surfaces (Theorem 4).\nIntroduction\n\nTilings have been studied extensively over the past few decades due to their connections to various branches of mathematics such as number theory, combinatorics, geometry, and physics.   The most famous example of this connection comes from the work of Roger Penrose  Pen91  , who showed that certain patterns observed in nature could be described using tiles whose sides were straight lines but had angles of 60 degrees or 120 degrees instead of 90 degrees.   These so-called  non-standard  tilings are now known as Penrose tilings.    Another important class of non-standard tilings was introduced by Thurston  Tho93  .    He constructed examples of tilings where each tile has four sides, two of them being parallel to one another while the other two form a pair of perpendicular bisectors.   This type of tiling is called a quadrilateral tiling.   It turns out that these tilings also appear frequently in nature;  for instance, they occur in quasicrystals, which are materials made up of atoms arranged into periodic structures that do not fit exactly within standard unit cells.   A third class of non-standard tilers was discovered independently by Conway and Lagarias  CL95  and by Wang  Wan96  .\nIn recent years there has been growing interest in studying mathematical models of quantum gravity, string theory, and M-theory.   One approach towards understanding these theories involves constructing spaces...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Brane Tilings . Abstract : We introduce the idea of brane tilings , which are generalizations of tiling families and can be used to explain different different kinds of physical structures in terms of their symmetries .We see that any finite group is isomorphic to some brane tiling group ( Theorem 1 ) . In particular , we prove that all Coxeter bands are brane tiling groups ( Corollary 2 ) , as well as all Artin groups with torsion - free abelian subgroups ( Corollary 3 ) .As an application , we give a new proof for the classification principle of finite simple groups by showing how they occur readily through the action of brane tilings on surfaces ( Theorem 4 ) . Introduction Tilings have been studied frequently over the previous few years owing to their connections to several parts of math such as number theory , combinatorics , mathematics , and mechanics .The most famous example of this link stems from the work of Roger Penrose Pen91 , who demonstrated that particular patterns observed in nature might be described using tiles whose sides were straight lines but had angles of 60 degrees or 120 degrees instead of 90 degrees . These so - called non - basic tilings are now termed as Penrose tilings .Another important group of non - basic tilings was introduced by Thurston Tho93 . He constructed examples of tilings where each tile has four sides , two of them being parallel to one another while the other two form a pair of perpendicular bisectors .This kind of tiling is known a quadrilateral tiling . It turns out that these tilings also appear often in nature ; for instance , they occur in quasicrystals , which are materials made up of atoms arranged into periodic shapes that do not fit precisely within normal unit cells .A third category of non - basic tilers was studied separately by Conway and Lagarias CL95 and by Wang Wan96 . In past decades there has been growing interest in investigating mathematical models of quantum gravitational , string theory , and M - theory .One approach towards studying these theories involves generating spaces . . .",
        "rewrite_text": "We present the concept of brane tilings, which serve as generalizations of traditional tiling families and offer insights into various physical structures through their symmetries. Our primary result demonstrates that any finite group can be represented as a brane tiling group (Theorem 1). Notably, we establish that all Coxeter bands qualify as brane tiling groups (Corollary 2), along with all Artin groups that possess torsion-free abelian subgroups (Corollary 3). Furthermore, we apply our findings to provide a novel proof for the classification principle of finite simple groups, illustrating how these groups manifest through the actions of brane tilings on surfaces (Theorem 4).\n\nThe study of tilings has garnered significant attention in recent years due to their connections to various mathematical disciplines, including number theory, combinatorics, and mechanics. A prominent example is Roger Penrose's work, which revealed that certain patterns found in nature could be represented using tiles with straight edges forming angles of 60 or 120 degrees, rather than the conventional 90 degrees. These configurations are now recognized as Penrose tilings. Another significant contribution came from William Thurston, who introduced quadrilateral tilings, characterized by tiles with four sides, two of which are parallel while the other two serve as perpendicular bisectors. Such tilings frequently appear in natural phenomena, including quasicrystals—materials with atomic arrangements that form periodic structures not confined to standard unit cells.\n\nAdditionally, the exploration of non-basic tilings has been advanced by researchers like Conway and Lagarias, as well as Wang. In recent decades, there has been an increasing interest in developing mathematical models related to quantum gravity, string theory, and M-theory. One promising avenue for investigating these complex theories involves the generation of spaces through innovative tiling approaches. Our work on brane tilings contributes to this ongoing discourse, providing a framework for understanding the interplay between mathematical structures and physical theories.",
        "ori-fast-z-score": -0.5107539184552492,
        "water-fast-z-score": 7.209605902279753,
        "rewrite-fast-z-score": -0.16783627165933782
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pairing based cooling of Fermi gases .\nAbstract:\nWe propose to cool fermionic atoms in optical lattices by using the pairing mechanism between two different hyperfine states, which is analogous to Cooper pair formation in superconductors. We show that this method can be used for both bosonic and fermionic systems with attractive interactions. The proposed scheme has several advantages over other methods such as evaporative cooling or sympathetic cooling.  It does not require any additional laser beams besides those needed for trapping and manipulating cold atoms. In addition it works even when there are no free particles present initially (e.g., at zero temperature). Finally we discuss how our proposal could be realized experimentally. Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging problems in atomic physics today  1  . This problem becomes particularly difficult if the initial number density of fermions is high because then elastic collisions cannot remove enough energy from the system  2  .\nIn recent years, however, new experimental techniques have been developed  3, 4  , allowing us to trap and manipulate cold atoms on an unprecedented level  5  . These developments make it possible to study many-body phenomena  6  like superfluidity  7, 8  and Bose-Einstein condensation  9  in ultracold atomic gases. One important goal in these experiments is to reach quantum degenerate regimes where the gas consists of strongly interacting fermions  10  . However, reaching low temperatures requires efficient cooling schemes  11  .\nOne promising approach towards achieving this goal is to use the pairing mechanism  12  . Pairs of fermions form bound states called Cooper pairs in conventional superconductors  13  . Analogously, pairs of fermions may also form bound states in ultracold atomic clouds  14  . If the interaction strength between fermions is sufficiently large, they will preferentially bind into pairs rather than remaining unpaired  15  . Therefore, cooling fermions via pairing should work well even",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pairing based cooling of Fermi gases . Abstract : We suggest to cool fermionic atoms in optical lattices by using the pairing principle between two different hyperfine states , which is analogous to Cooper couple formation in superconductors .We see that this process can be used for both bosonic and fermionic systems with interesting interactions . The proposed system has numerous benefits over other methods such as evaporative cooling or sympathetic heating .It does not require any additional laser beams besides those required for trapping and manipulating cool ions . In addition it works even when there are no free particles present initially ( e . g . , at zero temperature ) .Finally we talk how our proposal possible be realized experimentally . Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging difficulties in nuclear science today 1 .This problem arises particularly challenging if the first number density of fermions is high because then elastic collisions cannot eliminate much energy from the system 2 . In recent years , however , new theoretical techniques have been built 3 , 4 , allowing us to trapping and manipulate cold molecules on an remarkable level 5 .These advances making it able to study many - bodies phenomena 6 like superfluidity 7 , 8 and Bose - Einstein condensation 9 in ultracold atomic gases . One important aim in these experiments is to reach quantum degenerate regimes where the gas consists of highly interacting fermions 10 .However , obtaining minimum heats needs efficient cooling schemes 11 . One promising alternative towards reaching this goal is to use the pairing principle 12 .Pairs of fermions form bound states called Cooper pairs in standard superconductors 13 . Analogously , pairs of fermions may also create bound states in ultracold atomic clouds 14 .If the interaction strength between fermions is sufficiently great , they will preferentially bind into couples rather than keeping unpaired 15 . Therefore , cooling fermions via pairing should work better even",
        "rewrite_text": "We propose a novel method for cooling fermionic atoms in optical lattices by leveraging the pairing principle between distinct hyperfine states, akin to the formation of Cooper pairs in superconductors. This technique is applicable to both bosonic and fermionic systems exhibiting intriguing interactions. Our approach presents several advantages over traditional cooling methods, such as evaporative cooling and sympathetic heating. Notably, it does not necessitate additional laser beams beyond those already employed for trapping and manipulating cooled ions. Furthermore, this method is effective even in the absence of free particles at the outset, such as at absolute zero temperature.\n\nThe challenge of cooling fermions to quantum degeneracy temperatures below 1 microkelvin remains a significant hurdle in nuclear science. This difficulty is exacerbated when the initial fermion number density is high, as elastic collisions are insufficient to dissipate energy from the system. Recent advancements in theoretical techniques have enabled the trapping and manipulation of cold molecules with remarkable precision, facilitating the exploration of many-body phenomena such as superfluidity and Bose-Einstein condensation in ultracold atomic gases. A primary objective of these investigations is to achieve quantum degenerate regimes characterized by highly interacting fermions. However, effective cooling schemes are essential to minimize thermal energy.\n\nOur proposed method, based on the pairing principle, offers a promising pathway to achieve this goal. In conventional superconductors, fermions form bound states known as Cooper pairs. Similarly, in ultracold atomic clouds, fermions can also form bound pairs if the interaction strength is sufficiently strong, leading to a preference for pairing over remaining unpaired. Consequently, utilizing pairing for cooling fermions is anticipated to yield superior results, enhancing our ability to reach the desired quantum degeneracy and facilitating further exploration of quantum phenomena in fermionic systems. We conclude by discussing the potential experimental realizations of our proposal, which could significantly advance the field of ultracold atomic physics.",
        "ori-fast-z-score": -0.3223291856101521,
        "water-fast-z-score": 7.045638767423834,
        "rewrite-fast-z-score": 1.0579249964025073
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Study of Damage Propagation at the Interface Localization-Delocalization Transition of the Confined Ising Model .\nAbstract:\nWe study damage propagation in an interface between two ordered phases of the confined Ising model (CIM) with random fields and quenched disorder, which is known to undergo a localization-delocalization transition as its temperature T crosses Tc = 1. We show that this transition can be observed by measuring the average size of avalanches triggered by local perturbations. The results are compared with those for the unperturbed CIM obtained using Monte Carlo simulations on large lattices. In particular we find that the distribution of avalanche sizes changes drastically across the transition point. This behavior is explained within the framework of the mean-field theory developed recently for the CIM. Finally, we discuss possible experimental realizations of our system. Introduction:-The phenomenon of phase coexistence has been studied extensively both theoretically  1  -  4  and experimentally  5  . It occurs when different thermodynamic states coexist in equilibrium  6  , or metastable states exist simultaneously  7  . A typical example is provided by water  8  where ice Ih and liquid water co-exist below 0 o C  9  .\nIn recent years there have been several studies  10  -  12  devoted to understanding how interfaces separating different phases evolve under external driving forces such as thermal fluctuations  13  , magnetic field  14  , mechanical stress  15  etc.. These investigations were motivated mainly by experiments performed on various materials  16  including ferroelectrics  17  , ferromagnets  18  , superconductors  19  , colloids  20  , granular media  21  , glasses  22  , foams  23  , and biological systems  24  . For instance, it was found  25  that the dynamics of domain walls in magnets  26  depends crucially on whether they are pinned  27  or not  28  . Similarly, the response of glassy  29  and jammed  30  systems to shear stresses  31  strongly depends on their preparation history  32  . On the other hand, the effect of quenched disorder  33  on the properties of interfaces  34  remains poorly understood  35  despite numerous theoretical  36   38  and numerical  39  attempts made over the past few decades.\nRecently, the problem of interface evolution attracted renewed interest due to the discovery of new types of transitions occurring in spatially extended systems  40   41 :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Study of Damage Propagation at the Interface Localization - Delocalization Transition of the Confined Ising Model . Abstract : We research harm propagation in an interface between two ordered phases of the confined Ising model ( CIM ) with random fields and quenched disorder , which is known to undergo a localization - delocalization transition as its temperature T crosses Tc = 1 .We see that this shift can be identified by measuring the average size of avalanches caused by local perturbations . The results are compared with those for the unperturbed CIM achieved using Monte Carlo simulations on huge lattices .In particular we find that the distribution of avalanche sizes changes significantly across the transfer point . This phenomenon is studied within the framework of the mean - field model formulated recently for the CIM .Finally , we explain possible experimental realizations of our system . Introduction : - The phenomenon of phase coexistence has been studied thoroughly both theoretically 1 - 4 and experimentally 5 .It happens when distinct thermodynamic states coexist in equilibrium 6 , or metastable states arise simultaneously 7 . A typical example is provided by water 8 where ice Ih and fluid water co - operate below 0 o C 9 .In recent years there have been numerous research 10 - 12 devoted to investigating how interfaces separating different components develop under external driving forces such as heat fluctuations 13 , magnetic force 14 , thermal strain 15 etc . . These studies were driven mainly by research performed on various structures 16 notably ferroelectrics 17 , ferromagnets 18 , superconductors 19 , colloids 20 , granular material 21 , glasses 22 , foams 23 , and biological environments 24 .For instance , it was shown 25 that the dynamics of domain walls in magnets 26 depends crucially on whether they are pinned 27 or not 28 . Similarly , the response of glassy 29 and jammed 30 systems to shear forces 31 strongly depends on their preparation history 32 .On the other hand , the impact of quenched disorder 33 on the properties of interfaces 34 remains poorly regarded 35 despite several experimental 36 38 and mathematical 39 attempts made over the previous few years . Recently , the issue of interface evolution enjoyed continued interest due to the discovery of new types of transitions happening in spatially extended systems 40 41 :",
        "rewrite_text": "**Title:** Investigation of Damage Propagation at the Interface Localization-Delocalization Transition in the Confined Ising Model\n\n**Abstract:** This study explores the dynamics of damage propagation at the interface between two ordered phases of the confined Ising model (CIM) subjected to random fields and quenched disorder. The CIM is known to exhibit a localization-delocalization transition when the temperature (T) surpasses the critical threshold (Tc = 1). Our findings indicate that this transition can be effectively characterized by analyzing the average size of avalanches triggered by localized perturbations. We compare these results with those obtained from unperturbed CIM configurations through extensive Monte Carlo simulations on large lattice structures. Notably, we observe a significant alteration in the distribution of avalanche sizes as the system crosses the transition point. This behavior is examined within the context of a recently developed mean-field model tailored for the CIM. Furthermore, we discuss potential experimental setups that could replicate our findings, thereby providing a pathway for practical applications of our theoretical insights.\n\n**Introduction:** The phenomenon of phase coexistence has been extensively investigated both theoretically and experimentally. It occurs when distinct thermodynamic states exist in equilibrium or when metastable states emerge simultaneously. A classic example is the coexistence of ice Ih and liquid water below 0 °C. Recent research has increasingly focused on how interfaces separating different phases evolve under external influences such as thermal fluctuations, magnetic fields, and mechanical strain. These investigations have been motivated by studies on a variety of materials, including ferroelectrics, ferromagnets, superconductors, colloids, granular materials, glasses, foams, and biological systems. For instance, the dynamics of domain walls in magnetic materials are significantly affected by whether they are pinned or free to move. Similarly, the response of glassy and jammed systems to shear forces is heavily influenced by their preparation history. Despite several experimental and theoretical efforts, the effects of quenched disorder on interface properties remain underexplored. The recent emergence of new types of transitions in spatially extended systems has reignited interest in the evolution of interfaces, highlighting the need for further investigation in this area.",
        "ori-fast-z-score": -1.12089707663561,
        "water-fast-z-score": 6.943355894868313,
        "rewrite-fast-z-score": 0.32025630761017426
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Supersymmetric Parameter Space in Light of B-physics Observables and Electroweak Precision Data .\nAbstract:\nWe present the results of our analysis on the supersymmetric parameter space, taking into account all available experimental data including those from LHC experiments as well as electroweak precision observables (EWPO). We find that there is no significant improvement over previous analyses when we include EWPOs with their full correlations taken properly into account. However, if one considers only the subset of EWPOs which are not strongly correlated to each other, then some improvements can be seen for certain regions of the parameter space. In particular, this applies to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV. The latter case also leads to an improved agreement between theory predictions and measurements of the anomalous magnetic moment of the muon. Finally, we discuss how these findings affect the prospects for discovering supersymmetry at future colliders such as the International Linear Collider.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Supersymmetric Parameter Space in Light of B - physics Observables and Electroweak Precision Data . Abstract : We present the results of our analysis on the supersymmetric parameter space , using into consideration all available theoretical data including those from LHC observations as well as electroweak accuracy observables ( EWPO ) .We see that there is no considerable progress over past analyses when we include EWPOs with their full correlations took correctly into consideration . However , if one looks only the subset of EWPOs which are not closely correlated to each other , then some improvements can be shown for particular regions of the parameter room .In particular , this applicable to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV . The last example also leads to an better agreement between theoretical estimates and measurements of the anomalous magnetic moment of the muon .Finally , we explain how these results affect the possibilities for finding supersymmetry at potential colliders such as the International Linear Collider .",
        "rewrite_text": "In this article, we investigate the supersymmetric parameter space by incorporating a comprehensive analysis of all relevant theoretical data, including findings from the Large Hadron Collider (LHC) and electroweak precision observables (EWPO). Our study reveals that when we account for the full correlations of EWPOs, there is no significant advancement compared to previous analyses. However, we identify that by focusing on a subset of EWPOs that exhibit minimal correlation with one another, we can observe notable improvements in specific regions of the parameter space. This is particularly evident in scenarios where the lightest neutralino possesses a substantial Higgsino component or when gluinos have masses around 1 TeV. Notably, the latter scenario enhances the alignment between theoretical predictions and experimental measurements of the anomalous magnetic moment of the muon. Furthermore, we discuss the implications of our findings for the search for supersymmetry at future colliders, such as the International Linear Collider. Our results provide valuable insights into the viability of supersymmetric models and their potential to be tested in upcoming experimental settings. Overall, this work contributes to a deeper understanding of the supersymmetric landscape in light of current experimental data, paving the way for future research in this field.",
        "ori-fast-z-score": -3.6927447293799815,
        "water-fast-z-score": 4.275930552470682,
        "rewrite-fast-z-score": 1.4444444444444444
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonstationary pattern in unsynchronizable complex networks .\nAbstract:\nWe study the nonstationarity and synchronization properties of an ensemble of coupled oscillators with time-varying coupling strengths, which are modeled by Kuramoto-like phase equations on directed networks. We show that there exists a critical value for the average degree above which all nodes synchronize to each other if they have identical natural frequencies. However, we find that even when the system is synchronized at some initial moment, it may become desynchronized after a finite amount of time due to the change of the underlying topology. In particular, we prove analytically that this phenomenon occurs only in systems whose coupling strength distribution has infinite variance. Finally, numerical simulations confirm our theoretical results. The work was supported by NSF under Grant No. DMS-0805040. PACS numbers: 05.45.Mt, 02.10.Yn, 87.19 .Hc, 89.70.+c",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonstationary pattern in unsynchronizable complex networks . Abstract : We explore the nonstationarity and synchronization properties of an ensemble of coupled oscillators with time - differing correlation strengths , which are modeled by Kuramoto - like phase equations on directed networks .We see that there exists a critical quantity for the average degree above which all nodes synchronize to each other if they have matching natural frequencies . However , we find that even when the scheme is synchronized at some initial moment , it could stay desynchronized after a finite quantity of time due to the shift of the fundamental topology .In particular , we prove analytically that this phenomenon occurs only in networks whose coupling strength distribution has endless variance . Finally , numerical simulations confirm our theoretical results .The project was supported by NSF under Grant No . DMS - 0805040 .PACS codes : 05 . 45 . Mt , 02 . 10 . Yn , 87 . 19 . Hc , 89 . 70 . + c",
        "rewrite_text": "In this study, we investigate the nonstationary behavior and synchronization characteristics of a system composed of coupled oscillators, which exhibit varying correlation strengths over time. These oscillators are represented through Kuramoto-like phase equations on directed networks. Our findings reveal a critical threshold related to the average degree of the network; when this threshold is surpassed, nodes with matching natural frequencies achieve synchronization. However, we also demonstrate that even if synchronization is established at an initial time point, the system may subsequently experience desynchronization after a finite duration due to alterations in the underlying network topology. We provide a rigorous analytical proof indicating that this desynchronization phenomenon is exclusive to networks characterized by a coupling strength distribution with infinite variance. To substantiate our theoretical predictions, we conduct numerical simulations that align with our analytical results, reinforcing the validity of our conclusions. This research contributes to the understanding of complex networks and their dynamic behaviors, particularly in contexts where synchronization is not guaranteed despite initial conditions. The project received support from the National Science Foundation under Grant No. DMS-0805040. The relevant PACS codes for this study include 05.45.Mt, 02.10.Yn, 87.19.Hc, and 89.70.+c.",
        "ori-fast-z-score": 0.7385489458759964,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": 2.3849888978799783
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Cosmic Code Comparison Project .\nAbstract:\nThe cosmic code comparison project is an effort to compare the results produced by different cosmological codes, and thereby test their accuracy.  The goal is to produce a set of simulated data that can be used as input for any number of codes, and then have each code run on this same dataset.   This will allow us to determine how well these codes agree with one another in terms of both the physical quantities they predict (e.g., matter density profiles) and also the statistical properties of those predictions (e.g., power spectra).   We are currently working towards producing a large suite of simulations covering a wide range of parameter space, including dark energy models, modified gravity theories, neutrino masses, and primordial non-Gaussianity. In addition we plan to make available a variety of observational datasets which can be compared against the simulation outputs using standard techniques such as cross-correlation functions or likelihood analysis. The cosmic code comparison project is funded through NASA s Astrophysics Theory Program under grant NNX09AC84G.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Cosmic Code Comparison Project . Abstract : The universe coding analysis project is an initiative to compare the results produced by various cosmological rules , and consequently test their authenticity .The goal is to produce a setting of simulated evidence that can be used as input for any number of codes , and then have each code run on this same dataset . This will provide us to estimate how best these codes comply with one another in terms of both the physical quantities they predict ( e . g . , mind temperature profiles ) and also the empirical features of those predictions ( e . g . , power spectra ) .We are currently working towards creating a large suite of simulations covering a broad variety of parameter space , notably dark energy theories , modified gravity theories , neutrino masses , and primordial non - Gaussianity . In addition we plan to make accessible a variety of observational datasets which can be compared against the model outputs using conventional methods such as cross - correlation functions or likelihood analysis .The universe code comparison program is funded through NASA s Astrophysics Theory Program under grant NNX09AC84G .",
        "rewrite_text": "**Title: The Cosmic Code Comparison Project**\n\n**Abstract:** The Cosmic Code Comparison Project is a groundbreaking initiative aimed at evaluating and contrasting the outcomes generated by various cosmological models to assess their validity. This project seeks to establish a comprehensive framework of simulated data that can serve as a common input for multiple cosmological codes. By applying these codes to the identical dataset, we aim to analyze their compatibility in predicting key physical quantities, such as temperature profiles, as well as examining the empirical characteristics of these predictions, including power spectra. \n\nCurrently, our efforts are focused on developing an extensive suite of simulations that encompass a wide range of parameter spaces, particularly in relation to dark energy theories, modified gravity theories, neutrino masses, and primordial non-Gaussianity. Furthermore, we intend to provide access to a diverse array of observational datasets, which will allow for meaningful comparisons with model outputs through established methodologies, such as cross-correlation functions and likelihood analyses. \n\nThe Cosmic Code Comparison Project is supported by NASA's Astrophysics Theory Program under grant NNX09AC84G, highlighting its significance in advancing our understanding of cosmological phenomena. By fostering collaboration among different cosmological codes and promoting transparency in the simulation process, this project aims to enhance the reliability of cosmological predictions and contribute to the broader field of astrophysics.",
        "ori-fast-z-score": -0.9332565252573828,
        "water-fast-z-score": 5.157106231293967,
        "rewrite-fast-z-score": 1.1055415967851332
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Results on axion physics from the CAST Experiment at CERN .\nAbstract:\nThe Axion Dark Matter Search (ADMX) experiment is designed to search for dark matter in the form of axions, which are hypothetical particles predicted by theories beyond the Standard Model.  The ADMX experiment consists of two main components: an antenna and a microwave cavity system that can be tuned over a wide range of frequencies.   In this thesis we present results obtained with the first phase of the experiment using data taken between September 2005 and March 2007.    We report limits on the coupling strength of axions to photons as well as limits on the mass of axions produced via Primakoff conversion inside a strong magnetic field.  These results improve upon previous experimental bounds by more than one order of magnitude. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The Axion Dark Matter Search experiment is designed to search directly for dark matter in the universe in the form of axionic particles. The experiment consists of two main parts: an antenna and a microwave resonator system that can be tunable across a large frequency range. In this dissertation I will discuss our recent results from the first stage of the experiment.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Results on axion theory from the CAST Experiment at CERN . Abstract : The Axion Dark Matter Search ( ADMX ) experiment is designed to search for black material in the form of axions , which are hypothetical particles expected by theories beyond the Standard Model .The ADMX experiment consists of two principal components : an antenna and a microwave cavity network that can be tuned over a broad variety of frequencies . In this dissertation we present results derived with the first phase of the program including data taken between September 2005 and March 2007 .We report limits on the interaction strength of axions to photons as well as limits on the mass of axions produced via Primakoff transformation inside a powerful magnetic field . These conclusions progress upon former empirical bounds by more than one order of magnitude .This project was done under the auspices of the U . S Department of Energy by Lawrence Livermore National Laboratory under Contract DE - AC52 - 07NA27344 . The Axion Dark Matter Search experiment is designed to search specifically for black material in the universe in the form of axionic particles .The project consists of two principal parts : an antenna and a microwave resonator system that can be tunable across a large frequency spectrum . In this dissertation I will explore our latest findings from the first phase of the project .",
        "rewrite_text": "Title: Findings on Axion Theory from the CAST Experiment at CERN\n\nAbstract: The Axion Dark Matter Search (ADMX) experiment aims to detect dark matter in the form of axions, which are theoretical particles predicted by various extensions of the Standard Model of particle physics. The ADMX setup comprises two main components: a highly sensitive antenna and a microwave cavity system that can be adjusted to operate across a wide range of frequencies. This dissertation presents the results obtained during the initial phase of the experiment, which includes data collected from September 2005 to March 2007. We provide significant constraints on the interaction strength between axions and photons, as well as limitations on the mass of axions generated through the Primakoff effect in the presence of a strong magnetic field. Our findings enhance previous empirical limits by over an order of magnitude, marking a substantial advancement in the search for axionic dark matter. This research was conducted under the auspices of the U.S. Department of Energy and was supported by Lawrence Livermore National Laboratory, operating under Contract DE-AC52-07NA27344. The ADMX experiment is crucial for understanding the nature of dark matter in the universe, and our latest results from the first phase of the project contribute valuable insights into the properties of axions, potentially shedding light on their role in cosmology and particle physics.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": -0.3144854510165755
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Luminous HC3N line emission in NGC4418 - buried AGN or nascent starburst? .\nAbstract:\nWe report the detection of luminous (>10^7 Lsun) HCN(1-0), HNC(1-0), and CH3CN(5-4)\nline emission toward the nuclear region of the nearby Seyfert 2 galaxy NGC 4418\n(D = 20 Mpc). The observed luminosities are comparable to those found for ultraluminous infrared galaxies, but the ratios between these lines suggest that this is not an active galactic nucleus (AGN) dominated source. Instead we propose that the high-excitation molecular gas may be associated with a young massive starburst which has recently been triggered by tidal interactions and/or mergers. We also find evidence for a compact radio continuum source at the center of the galaxy, consistent with previous observations. This suggests that there could be a deeply-embedded AGN present within the central few hundred parsecs.  These results provide new insights into how such powerful sources can form during the early stages of galaxy evolution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Luminous HC3N line emission in NGC4418 - buried AGN or nascent starburst ? .Abstract : We report the observation of luminous ( > 10 ^ 7 Lsun ) HCN ( 1 - 0 ) , HNC ( 1 - 0 ) , and CH3CN ( 5 - 4 ) line emission toward the nuclear zone of the nearby Seyfert 2 galaxy NGC 4418 ( D = 20 Mpc ) . The observed luminosities are comparable to those observed for ultraluminous infrared galaxies , but the ratios between these lines indicate that this is not an active galactic nucleus ( AGN ) dominated source .Instead we propose that the high - excitation molecular gas may be involved with a young massive starburst which has recently been caused by tidal interactions and / or mergers . We additionally find proof for a compact radio continuum source at the center of the galaxy , compatible with previous findings .This implies that there might be a highly - anchored AGN present within the central few hundred parsecs . These data provide fresh insights into how such powerful sources can form during the early stages of galaxy evolution .",
        "rewrite_text": "We present our findings on the luminous line emissions of HCN (1-0), HNC (1-0), and CH3CN (5-4) in the nuclear region of the nearby Seyfert 2 galaxy NGC 4418, located at a distance of 20 Mpc. The luminosity of these emissions exceeds 10^7 Lsun, placing them on par with those observed in ultraluminous infrared galaxies. However, the line ratios we have measured suggest that the emission is not primarily driven by an active galactic nucleus (AGN). Instead, we propose that the high-excitation molecular gas is indicative of a nascent massive starburst, likely triggered by tidal interactions or mergers with other galaxies. Additionally, our observations reveal a compact radio continuum source at the galaxy's center, which aligns with previous studies. This finding raises the possibility of a tightly bound AGN existing within the innermost few hundred parsecs of NGC 4418. Our results contribute valuable insights into the mechanisms behind the formation of such powerful sources during the formative phases of galaxy evolution, highlighting the complex interplay between star formation and AGN activity in the early universe.",
        "ori-fast-z-score": -0.5933908290969266,
        "water-fast-z-score": 3.862357857472309,
        "rewrite-fast-z-score": 2.1514114968019085
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gravitational Wave Signals from Chaotic System: A Point Mass with A Disk .\nAbstract:\nWe study gravitational wave signals from chaotic systems by using the point mass model with an accretion disk around it, which is one of the most promising candidates for gravitational wave sources in astrophysics.  We show that there are two types of gravitational waves emitted from such systems:  The first type comes from the orbital motion of the binary system and its frequency spectrum has peaks at integer multiples of the orbital frequency.  The second type comes from the spiral structure formed on the surface of the accretion disk due to tidal interaction between the central black hole and the surrounding matter.  Its frequency spectrum shows no clear peak but rather consists of many frequencies whose amplitudes decrease as their frequencies increase.  In addition we find that the amplitude of the second type of gravitational waves can be much larger than that of the first type when the spin parameter of the central black hole is large enough (a > 0.9).  This suggests that the detection rate of gravitational waves may depend strongly on the spin parameter of the source if the signal-to-noise ratio is high enough.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gravitational Wave Signals from Chaotic System : A Point Mass with A Disk . Abstract : We research gravitational wave signals from chaotic systems by using the point mass model with an accretion disk around it , which is one of the most attractive candidates for gravitational wave sources in astrophysics .We see that there are two forms of gravitational waves emitted from such systems : The first sort comes from the orbital movement of the binary system and its frequency spectrum has peaks at integer multiples of the orbital frequency . The second kind comes from the spiral shape formed on the surface of the accretion disk due to tidal interaction between the central black hole and the nearby matter .Its frequency spectrum displays no clear peak but rather consists of several spectrum whose amplitudes decrease as their frequencies increase . In addition we find that the frequency of the second kind of gravitational waves can be much larger than that of the first kinds when the spin vector of the central black hole is huge enough ( a > 0 . 9 ) .This implies that the detection rate of gravitational waves may depend greatly on the spin vector of the origin if the signal - to - noise proportion is high enough .",
        "rewrite_text": "In this study, we investigate the gravitational wave signals generated by chaotic systems, specifically focusing on a model that features a point mass surrounded by an accretion disk. This configuration is considered one of the most promising sources of gravitational waves in the field of astrophysics. Our analysis reveals two distinct types of gravitational waves emitted from such systems. The first type arises from the orbital dynamics of the binary system, characterized by a frequency spectrum that exhibits peaks at integer multiples of the orbital frequency. The second type of gravitational wave is produced by the spiral structures that develop on the surface of the accretion disk, a result of tidal interactions between the central black hole and the surrounding matter. Unlike the first type, the frequency spectrum of these waves does not show clear peaks; instead, it comprises multiple components whose amplitudes diminish with increasing frequency. Notably, we observe that when the spin of the central black hole is significantly high (specifically, when the spin parameter exceeds 0.9), the frequencies of the second type of gravitational waves can surpass those of the first type. This finding suggests that the detection rate of gravitational waves may be significantly influenced by the spin of the source, particularly when the signal-to-noise ratio is sufficiently favorable. Our results contribute to a deeper understanding of gravitational wave emissions from chaotic systems and highlight the importance of black hole spin in the context of gravitational wave astronomy.",
        "ori-fast-z-score": -0.29277002188455997,
        "water-fast-z-score": 4.856429311786321,
        "rewrite-fast-z-score": 1.1358152736593492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Origin of the Dichotomy of Early-Type Galaxies: The Role of Dry Mergers and AGN Feedback .\nAbstract:\nWe present an analysis of the evolution of early-type galaxies (ETGs) in hydrodynamical cosmological simulations, focusing on their formation history as well as their internal structure at z = 0. We find that ETG progenitors are typically gas-rich systems with high star formation rates (SFRs), which evolve into red-sequence objects through dry mergers. In addition to this major merger channel for forming ETGs, we show that minor mergers can also contribute significantly to the growth of massive ETGs by bringing in fresh cold gas. Our results suggest that both processes play important roles in shaping the observed properties of local ETGs. This is consistent with recent observational studies showing that most massive ETGs have experienced multiple episodes of merging over cosmic time. \n \n Keywords: galaxy evolution, galaxy morphology, galaxy scaling relations, galaxy clusters, semi-analytic models, hydrodynamics, dark matter",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Origin of the Dichotomy of Early - Type Galaxies : The Role of Dry Mergers and AGN Feedback . Abstract : We present an assessment of the evolution of early - class stars ( ETGs ) in hydrodynamical cosmological simulations , concentrating on their structure evolution as also as their internal structure at z = 0 .We see that ETG progenitors are typically gas - rich systems with high star formation rates ( SFRs ) , which decay into red - sequence bodies through dry mergers . In addition to this main merger channel for forming ETGs , we find that minor mergers can also contribute greatly to the development of large ETGs by bringing in fresh cold energy .Our results show that both processes play crucial roles in shaping the seen characteristics of local ETGs . This is consistent with recent observational research indicating that most large ETGs have experienced several bouts of merging over cosmic time .Keywords : universe formation , galaxy shape , galaxy scaling relations , galaxy regions , semi - analytic models , hydrodynamics , dark matter",
        "rewrite_text": "Title: On the Origin of the Dichotomy of Early-Type Galaxies: The Role of Dry Mergers and AGN Feedback\n\nAbstract: In this study, we investigate the evolutionary pathways of early-type galaxies (ETGs) through hydrodynamical cosmological simulations, with a particular focus on their structural evolution and internal characteristics at redshift z = 0. Our findings reveal that the progenitors of ETGs are predominantly gas-rich systems characterized by elevated star formation rates (SFRs). Over time, these galaxies transition into red-sequence ETGs primarily through the process of dry mergers. Furthermore, we identify that minor mergers significantly contribute to the growth and evolution of large ETGs by introducing fresh cold gas, which enhances star formation and alters the internal dynamics of these galaxies. Our analysis underscores the importance of both dry and minor merger processes in shaping the observable properties of local ETGs. This aligns with recent observational studies that suggest a substantial number of large ETGs have undergone multiple merging events throughout cosmic history. The implications of our research extend to understanding the formation and evolution of the universe, the morphological characteristics of galaxies, and the scaling relations that govern galaxy properties. By integrating semi-analytic models with hydrodynamic simulations, we provide a comprehensive framework for exploring the interplay between dark matter and baryonic processes in galaxy formation. Our results contribute to a deeper understanding of the mechanisms that drive the dichotomy observed in early-type galaxies, highlighting the critical roles of both dry mergers and active galactic nucleus (AGN) feedback in their evolution.\n\nKeywords: universe formation, galaxy morphology, galaxy scaling relations, galaxy evolution, semi-analytic models, hydrodynamics, dark matter.",
        "ori-fast-z-score": -1.8888888888888888,
        "water-fast-z-score": 4.196397990844169,
        "rewrite-fast-z-score": 2.057182539299806
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effective conservation of energy and momentum algorithm using switching potentials suitable for molecular dynamics simulation of thermodynamical systems .\nAbstract:\nWe present an effective method to conserve the total energy and linear momentum in molecular dynamics (MD) simulations by introducing two types of potentials: one is used during the MD run, while another is only used when calculating the forces on particles at each time step. The latter type of potentials are switched off after being calculated so that they do not affect the subsequent MD trajectories. We show how this scheme can be implemented into existing MD codes with minimal modifications. In addition, we demonstrate its effectiveness through several examples including liquid argon, water clusters, and carbon nanotubes. Our results indicate that our new scheme conserves both energy and momentum very well even though it does not require any additional computational cost compared to conventional schemes. This work was supported by the National Natural Science Foundation of China under Grants No. 10874145 and No. 10934011 . Keywords: Energy-momentum conservation; Switching potentials; Molecular dynamics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effective conservation of power and momentum algorithm using switching potentials suitable for molecular mechanics simulation of thermodynamical systems . Abstract : We present an efficient method to conserve the total energy and linear momentum in molecular mechanics ( MD ) simulations by using two forms of potentials : one is utilized during the MD run , while another is only employed when calculating the forces on electrons at each time step .The latter type of potentials are switched off after being calculated so that they do not alter the subsequent MD trajectories . We see how this scheme can be applied into older MD codes with minimal modifications .In addition , we prove its effectiveness through several examples representing liquid argon , water complexes , and carbon nanotubes . Our results show that our new program conserves both heat and momentum very best even though it does not require any additional mathematical price compared to conventional strategies .This project was supported by the National Natural Science Foundation of China under Grants No . 10874145 and No .10934011 . Keywords : Energy - momentum conservation ; Switching potentials ; Molecular dynamics",
        "rewrite_text": "**Title:** An Efficient Algorithm for Conserving Energy and Momentum in Molecular Mechanics Simulations Using Switching Potentials\n\n**Abstract:** In this study, we introduce a novel algorithm designed to effectively conserve total energy and linear momentum during molecular dynamics (MD) simulations. Our approach employs two distinct types of potentials: one is actively used throughout the MD simulation, while the other is applied solely for the computation of forces acting on electrons at each time step. Importantly, the second type of potential is deactivated immediately after its calculation, ensuring that it does not influence the subsequent trajectories of the MD simulation. This innovative scheme can be seamlessly integrated into existing MD codes with minimal adjustments, enhancing their functionality without extensive overhauls. We validate the efficacy of our method through a series of simulations involving various systems, including liquid argon, water complexes, and carbon nanotubes. The results demonstrate that our algorithm maintains superior conservation of both energy and momentum compared to traditional methods, all while incurring no additional computational cost. This advancement represents a significant step forward in the field of molecular mechanics, providing researchers with a robust tool for simulating thermodynamic systems. The development of this project was supported by the National Natural Science Foundation of China under Grants No. 10874145 and No. 10934011. \n\n**Keywords:** Energy conservation, momentum conservation, switching potentials, molecular dynamics simulations.",
        "ori-fast-z-score": -1.2375966910186262,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": -1.3480372031495529
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Langmuir blodgett assembly of densely aligned single walled carbon nanotubes from bulk materials .\nAbstract:\nWe report the Langmuir Blodgett (LB) deposition of highly ordered, dense arrays of vertically-aligned single-walled carbon nanotube films on solid substrates using an aqueous dispersion containing surfactant and sodium dodecyl sulfate as dispersing agents. The LB technique is used to transfer these films onto various substrate materials such as silicon wafers, quartz slides, glass coverslips, gold-coated glass coverslips, and indium tin oxide coated glass coverslips. We have also demonstrated that this method can be extended for patterned growth by transferring the film selectively over areas defined by photoresist patterns. These results are important in developing new applications based on carbon nanotubes. \n \n Carbon nanotubes (CNTs), which were discovered about ten years ago, have attracted considerable attention because they possess unique physical properties including high electrical conductivity, mechanical strength, thermal stability, chemical inertness, etc., making them promising candidates for many potential applications ranging from field emission devices to sensors and optoelectronic devices1-5. However, most of their practical uses require CNT networks with controlled orientation and density6-8. In recent years, several methods have been developed to prepare oriented CNT films9-12. Among those techniques, Langmuir-Blodgett (LB)\ndeposition has emerged as one of the most powerful approaches13-15. This process involves spreading a monolayer of amphiphilic molecules at the air-water interface followed by vertical dipping of a hydrophobic substrate into the water subphase16-18. By repeating the above steps, multilayered thin films consisting of closely packed CNTs can be obtained19-21. Compared to other methods22-24, LB deposition offers advantages such as precise control of layer thickness25-27, easy fabrication of large-area uniform films28-30, and possibility of fabricating patterned structures31-33.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Langmuir blodgett assembly of densely aligned single walled carbon nanotubes from bulk surfaces . Abstract : We report the Langmuir Blodgett ( LB ) deposition of highly ordered , dense arrays of vertically - aligned single - walled carbon nanotube films on solid substrates using an aqueous dispersion containing surfactant and potassium dodecyl sulfate as dispersing agents .The LB technique is utilized to move these films onto several substrate materials such as silicon wafers , quartz slides , plastic coverslips , gold - coated glass coverslips , and indium tin oxide coated glass coverslips . We have also demonstrated that this process can be extended for patterned expansion by directing the film selectively over areas defined by photoresist sequences .These data are important in establishing new applications based on carbon nanotubes . Carbon nanotubes ( CNTs ) , which were discovered about ten years previously , have garnered considerable scrutiny because they possess unique physical properties including high thermal conductivity , thermal strength , thermal stability , chemical inertness , etc . , making them promising candidates for numerous likely applications ranging from field emission sensors to sensors and optoelectronic devices1 - 5 .However , most of their practical applications need CNT connections with regulated orientation and density6 - 8 . In recent years , various methods have been proposed to produce aligned CNT films9 - 12 .Among those techniques , Langmuir - Blodgett ( LB ) deposition has emerged as one of the most efficient approaches13 - 15 . This process involves spreading a monolayer of amphiphilic molecules at the air - water interface followed by vertical dipping of a hydrophobic substrate into the air subphase16 - 18 .By repeating the above steps , multilayered narrow bands consisting of closely packed CNTs can be obtained19 - 21 . Compared to other methods22 - 24 , LB deposition gives benefits such as careful management of layer thickness25 - 27 , easy fabrication of large - area uniform films28 - 30 , and possibility of fabricating patterned structures31 - 33 .",
        "rewrite_text": "We present a comprehensive study on the Langmuir-Blodgett (LB) assembly technique for the deposition of highly ordered, densely aligned films of single-walled carbon nanotubes (SWCNTs) onto various solid substrates. Utilizing an aqueous dispersion that incorporates surfactants and potassium dodecyl sulfate as dispersing agents, we successfully create vertically-aligned SWCNT films. The LB method allows for the transfer of these films onto a range of substrate materials, including silicon wafers, quartz slides, plastic coverslips, gold-coated glass coverslips, and indium tin oxide-coated glass coverslips. Furthermore, we demonstrate the capability of this technique to achieve patterned expansion by selectively directing the film onto areas defined by photoresist patterns. \n\nThe significance of this research lies in its potential to facilitate new applications leveraging the unique properties of carbon nanotubes (CNTs), which have been the focus of extensive investigation since their discovery approximately a decade ago. CNTs are renowned for their exceptional physical characteristics, such as high thermal conductivity, mechanical strength, thermal stability, and chemical inertness, positioning them as promising candidates for a wide array of applications, including field emission sensors and optoelectronic devices. However, the realization of these applications often requires precise control over the orientation and density of CNTs in the films.\n\nIn recent years, several methods have been explored to produce aligned CNT films, with LB deposition emerging as a particularly effective approach. This technique involves the formation of a monolayer of amphiphilic molecules at the air-water interface, followed by the vertical dipping of a hydrophobic substrate into the subphase. By repeating this process, we can achieve multilayered structures composed of closely packed CNTs. Compared to alternative methods, LB deposition offers several advantages, including precise control over layer thickness, the ability to fabricate large-area uniform films, and the potential for creating patterned structures. This work contributes to the growing body of knowledge on CNT applications and paves the way for future innovations in nanotechnology.",
        "ori-fast-z-score": 1.3241694217637887,
        "water-fast-z-score": 7.340596279696671,
        "rewrite-fast-z-score": 0.23643312187173018
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral type dependent rotational braking and strong magnetic flux in three components of the late-M multiple system LHS 1070 .\nAbstract:\nWe report on spectropolarimetric observations of the M8+M9 binary star LHS 1070A,B (GJ 436) with ESPaDOnS at CFHT. The two stars are separated by only 0. ′′ 1 and have been known to be magnetically active for many years. We find that both stars show significant circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields. In addition we detect Stokes V signatures indicating net linear polarization across all observed spectral lines. This is likely caused by scattering processes within the stellar atmosphere. Using our new data set together with previously published photometric measurements we derive rotation periods of P A = 3.6 ± 0.1 days and P B = 4.2 ± 0.3 days for the primary and secondary component respectively. These values are significantly longer than those derived from previous studies which were based solely on photometry. Our results suggest that the rotation period of each individual component depends strongly on its effective temperature as well as its surface gravity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral type dependent rotational braking and strong magnetic flux in three components of the late - M multiple system LHS 1070 . Abstract : We report on spectropolarimetric studies of the M8 + M9 binary star LHS 1070A , B ( GJ 436 ) with ESPaDOnS at CFHT .The two stars are split by only 0 . ′ ′ 1 and have been known to be magnetically active for many years .We see that both stars show considerable circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields . In addition we locate Stokes V signatures suggesting net linear polarization across all observed spectral lines .This is probably due by scattering mechanisms within the stellar environment . Using our new data set combined with previously reported photometric surveys we derive rotation periods of P A = 3 . 6 ± 0 . 1 hours and P B = 4 . 2 ± 0 . 3 days for the primary and secondary component respectively .These values are greatly lengthy than those generated from previous analyses which were based primarily on photometry . Our results show that the rotation history of each individual component relies highly on its effective heat as well as its surface velocity .",
        "rewrite_text": "We present a comprehensive analysis of the M8 + M9 binary system LHS 1070A and B (GJ 436) through spectropolarimetric observations conducted with the ESPaDOnS instrument at the Canada-France-Hawaii Telescope (CFHT). The two stars, which are separated by a mere 0.1 arcseconds, have been recognized for their magnetic activity over an extended period. Our findings reveal significant circularly polarized emission lines, indicative of Zeeman splitting, which confirms the presence of magnetic fields around both stars. Furthermore, we identify Stokes V signatures that suggest net linear polarization across all spectral lines observed, likely resulting from scattering processes within the stars' environments. \n\nBy integrating our new spectropolarimetric data with previously published photometric surveys, we have determined the rotation periods for the primary and secondary components of the binary system to be P_A = 3.6 ± 0.1 hours and P_B = 4.2 ± 0.3 days, respectively. These rotation periods are notably longer than those reported in earlier studies that primarily relied on photometric data. Our results indicate that the rotational dynamics of each star are significantly influenced by their effective temperature and surface velocity. This research enhances our understanding of the relationship between stellar rotation, magnetic activity, and spectral type in late-M dwarfs, providing valuable insights into the evolutionary processes governing these intriguing celestial objects.",
        "ori-fast-z-score": -0.9332565252573828,
        "water-fast-z-score": 4.587317109255645,
        "rewrite-fast-z-score": 0.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Discovery of an Active Galactic Nucleus in the Late-type Galaxy NGC 3621: Spitzer Spectroscopic Observations .\nAbstract:\nWe report on infrared spectroscopic observations with the Infrared Spectrograph (IRS) aboard the Spitzer Space Telescope toward the nearby late-type galaxy NGC 3621, which is known to harbor a supermassive black hole at its center. The IRS spectrum shows prominent emission lines such as  Ne II 12.81 and  S III 18.71 µm that are commonly seen in active galactic nuclei (AGNs). We find that these emission lines can be reproduced by photoionization models using AGN-like ionizing radiation fields. From the observed line ratios we estimate the electron density n e = 10 3 cm −3 , temperature T e = 1000 K, and ionization parameter U H = 1 × 10 −2 . These results suggest that the central region of NGC 3621 has properties similar to those found for Seyfert galaxies. This work was supported by NASA through grant number GO-08460.01-A awarded by the Jet Propulsion Laboratory, California Institute of Technology under contract with NASA.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Discovery of an Active Galactic Nucleus in the Late - class Galaxy NGC 3621 : Spitzer Spectroscopic Observations . Abstract : We report on infrared spectroscopic observations with the Infrared Spectrograph ( IRS ) aboard the Spitzer Space Telescope toward the nearby mid - class galaxy NGC 3621 , which is known to harbor a supermassive black hole at its core .The IRS spectrum displays large emitted lines such as Ne II 12 . 81 and S III 18 . 71 µm that are often saw in active galactic nuclei ( AGNs ) . We see that these absorption patterns can be reproduced by photoionization models using AGN - like ionizing radiation fields .From the known line ratios we estimate the electron concentration n e = 10 3 cm −3 , temperature T e = 1000 K , and ionization variable U H = 1 × 10 −2 . These data suggest that the central region of NGC 3621 has properties similar to those known for Seyfert galaxies .This work was supported by NASA through grant number GO - 08460 . 01 - A awarded by the Jet Propulsion Laboratory , California Institute of Technology under contract with NASA .",
        "rewrite_text": "We present our findings from infrared spectroscopic observations conducted with the Infrared Spectrograph (IRS) on the Spitzer Space Telescope, focusing on the nearby mid-class galaxy NGC 3621, which is recognized for hosting a supermassive black hole at its center. The IRS spectrum reveals prominent emission lines, notably Ne II at 12.81 µm and S III at 18.71 µm, which are characteristic of active galactic nuclei (AGNs). Our analysis indicates that the observed absorption features can be effectively modeled using photoionization techniques that incorporate AGN-like ionizing radiation fields. By examining the established line ratios, we derive key physical parameters of the central region of NGC 3621, including an electron density of n_e = 10^3 cm^−3, an electron temperature of T_e = 1000 K, and an ionization parameter U_H = 1 × 10^−2. These results imply that the central region of NGC 3621 exhibits properties akin to those typically associated with Seyfert galaxies, suggesting that it may be an active galactic nucleus. This research contributes to our understanding of the characteristics and behaviors of galaxies with supermassive black holes and their central regions. The study was conducted with the support of NASA, under grant number GO-08460.01-A, awarded by the Jet Propulsion Laboratory at the California Institute of Technology, in collaboration with NASA.",
        "ori-fast-z-score": -0.25,
        "water-fast-z-score": 3.9056328877620143,
        "rewrite-fast-z-score": 0.105999788000636
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Separability Criterion for multipartite quantum states based on the Bloch representation of density matrices .\nAbstract:\nWe present an explicit criterion to determine whether or not two given multipartite quantum states are separable, i.e., can be written as convex combinations of product states. The criterion is formulated in terms of the Bloch representation of the corresponding density matrices and it relies only on local measurements performed by each party. We show that our method provides a necessary condition for separability which is strictly weaker than other known criteria. Finally we illustrate its usefulness with some examples. Introduction:-The problem of determining if a given state belongs to the set of separable states has been extensively studied during last years  1  . In particular, several authors have proposed different methods to solve this problem  2  -  4  , but none of them seems to provide a complete solution yet. Recently, Vidal et al  5  introduced a new approach to study separability problems using the Bloch representation  6  of the density matrix associated to any pure state. This technique allows one to obtain simple conditions for separability which involve only local measurements made by each party involved in the system under consideration. However, these results do not apply directly when dealing with mixed states since they require the knowledge of all possible pure-state decompositions of such states. Here we will use another version of the Bloch representation  7  to derive a general criterion for separability applicable also to mixed states. Our main result consists of showing that there exists at least one decomposition into pure states compatible with the Bloch representation of every separable state. As a consequence, we prove that the criterion presented here constitutes a necessary condition for separabilty which is strictly weaker than previous ones  8  .\nPreliminaries:-In what follows we consider N-partite systems described by Hilbert spaces H 1 ,H 2 ...H N . A generic element |ψ⟩ ∈ H = ∑ N i=1 H i is called a pure state vector while ρ ∈ D(H) denotes a density operator acting on H. Any density operator can always be expressed in terms of its spectral decomposition  9  \nwhere {|λ⟩} λ=1,...,d is an orthonormal basis of eigenvectors of ρ and p λ ≥ 0 (p",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Separability Criterion for multipartite quantum states based on the Bloch representation of density matrices . Abstract : We present an explicit criterion to determine whether or not two given multipartite quantum states are separable , i . e . , can be written as convex combinations of product states .The criterion is implemented in terms of the Bloch representation of the resulting density matrices and it rely only on local observations performed by each party . We see that our approach offers a necessary condition for separability which is strictly weaker than other established criteria .Finally we highlight its usefulness with some examples . Introduction : - The question of determining if a given state belongs to the group of separable states has been heavily examined during last years 1 .In particular , various scientists have proposed different methods to treat this question 2 - 4 , but none of them appears to provide a complete solved yet . Recently , Vidal et al 5 pioneered a new approach to study separability questions using the Bloch representation 6 of the density graph identified to any pure state .This method enables one to obtain simple conditions for separability which require only local observations made by each party involved in the scheme under consideration . However , these results do not apply directly when dealing with mixed states since they demand the knowledge of all possible pure - state decompositions of such states .Here we will use another version of the Bloch representation 7 to derive a general criterion for separability applied also to mixing states . Our main consequence consists of finding that there exists at least one decomposition into pure states compatible with the Bloch representation of every separable state .As a consequence , we prove that the criterion presented here constitutes a necessary condition for separabilty which is strictly weaker than prior ones 8 . Preliminaries : - In what follows we define N - partite structures described by Hilbert functions H 1 , H 2 . . . H N .A generic element | ψ ⟩ ∈ H = [UNK] N i = 1 H i is called a pure state vector while ρ ∈ D ( H ) denotes a density operator acting on H . Any density operator can always be expressed in terms of its spectral decomposition 9 where { | λ ⟩ } λ = 1 , . . . , d is an orthonormal basis of eigenvectors of ρ and p λ ≥ 0 ( p",
        "rewrite_text": "**Title:** Separability Criterion for Multipartite Quantum States Based on the Bloch Representation of Density Matrices\n\n**Abstract:** In this study, we introduce a novel criterion for assessing the separability of multipartite quantum states, specifically determining whether such states can be expressed as convex combinations of product states. Our criterion leverages the Bloch representation of density matrices, allowing for a straightforward implementation that relies solely on local measurements conducted by each participant in the quantum system. Notably, our approach provides a necessary condition for separability that is less stringent than existing criteria, thereby expanding the toolkit available for researchers in quantum information theory. We illustrate the practical applicability of our criterion through various examples, demonstrating its effectiveness in identifying separable states. \n\nThe challenge of classifying quantum states as separable or entangled has garnered significant attention in recent years, with numerous methodologies proposed to tackle this complex issue. While earlier works have laid the groundwork for understanding separability, a comprehensive solution remains elusive. Recent advancements by Vidal et al. have utilized the Bloch representation to derive conditions for separability, primarily focusing on pure states. However, these conditions fall short when applied to mixed states, as they necessitate knowledge of all possible pure-state decompositions.\n\nIn this paper, we adopt an alternative formulation of the Bloch representation to establish a general criterion applicable to mixed states. Our findings reveal that every separable state possesses at least one compatible pure-state decomposition within the Bloch framework. Consequently, we assert that the criterion we propose serves as a necessary condition for separability, which is distinctly weaker than previously established criteria. This work not only contributes to the theoretical understanding of quantum state separability but also provides a practical method for researchers to analyze multipartite quantum systems effectively.",
        "ori-fast-z-score": 1.3568010505999364,
        "water-fast-z-score": 7.140637266026874,
        "rewrite-fast-z-score": 1.5888598190134724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Orbifoldes speciales et classification bimeromorphe des varietes kaehleriennes compactes .\nAbstract:\nDans cet article, nous donnons une nouvelle preuve de la classification bimeromorphee des varietes kahleriennes compactes en utilisant les orbifolds speciaux. Nous montrons que si X est un orbifold special alors le groupe fondamental de X s identifie au groupe fondamental d une variete kahlerienne compacte Y . _SPLIT_ En particulier, on obtient que tout orbifold special admet une resolution symplectique. On montre aussi que toute application holomorphe entre deux orbifolds speciaux induit par passage aux groupes fondamentaux une application holomorphe sur leurs resolutions respectives. Enfin, on donne quelques exemples d orbifolds speciaux qui ne sont pas resolubles. Keywords: Orbifoldes spéciaux, Variétés Kähleriennes Compactes, Groupes fondamentaux, Résolutions symplectiques. 1 Introduction Soit X un espace analytique complexe connexe muni d un feuilletage holomorphe F . Un tel objet sera appelé dans ce texte un orbifold spécial (voir  V ). Dans cette note, nous allons donner une nouvelle démonstration du théorème suivant dû à Verbitsky : Théorème 0.1. (Verbitsky) Tout orbifold spécial admet une résolution symplectique.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Orbifoldes speciales et classification bimeromorphe des varietes kaehleriennes compactes . Abstract : Dans cet article , nous donnons une nouvelle preuve de la classification bimeromorphee des varietes kahleriennes compactes en utilisant les orbifolds speciaux .Nous montrons que si X est un orbifold special alors la groupe fondamental de X s identifie au groupe fondamental d la variete kahlerienne compacte Y . _ SPLIT _ En particulier , on obtient que tout orbifold special admet une resolution symplectique .On montre aussi que toute application holomorphe entre deux orbifolds speciaux induit par passage des groupes fondamentaux la applied holomorphe sur leurs resolutions respectives . Enfin , on donne quelques exemples d orbifolds speciaux qui ne sont pas resolubles .Keywords : Orbifoldes spéciaux , Variétés Kähleriennes Compactes , Groupes fondamentaux , Résolutions symplectiques . 1 Introduction Soit X un espace analytique complexe connexe muni d en feuilletage holomorphe F .Un tel objet sera appelé dans ce texte un orbifold spécial ( voir V ) . Dans cette report , nous allons donner à nouvelle démonstration du théorème suivant dû à Verbitsky : Théorème 0 . 1 .(Verbitsky) Tout orbifold spécial admet une résolution symplectique.",
        "rewrite_text": "**Title:** Special Orbifolds and Bimeromorphic Classification of Compact Kähler Varieties\n\n**Abstract:** In this article, we present a novel proof of the bimeromorphic classification of compact Kähler varieties through the lens of special orbifolds. We establish that if X is a special orbifold, then its fundamental group is isomorphic to the fundamental group of the compact Kähler variety Y. This finding is significant as it implies that every special orbifold possesses a symplectic resolution. Furthermore, we demonstrate that any holomorphic map between two special orbifolds induces a corresponding holomorphic map on their respective resolutions via the passage of fundamental groups. This connection between orbifolds and Kähler varieties enhances our understanding of their geometric structures and relationships. Additionally, we provide several examples of special orbifolds that are not resolvable, highlighting the diversity and complexity within this framework. Our work builds upon the foundational theorem established by Verbitsky, which asserts that every special orbifold admits a symplectic resolution, thereby reinforcing the importance of symplectic geometry in the study of complex analytic spaces. The implications of our results extend to various areas of algebraic geometry and complex analysis, offering new insights into the classification and properties of Kähler manifolds. \n\n**Keywords:** Special Orbifolds, Compact Kähler Varieties, Fundamental Groups, Symplectic Resolutions.",
        "ori-fast-z-score": -0.6030226891555273,
        "water-fast-z-score": 1.9824814143238607,
        "rewrite-fast-z-score": -2.23606797749979
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for gravitational waves from binary inspirals in S3 and S4 LIGO data .\nAbstract:\nWe present the results of searches for gravitational wave signals from compact binary coalescences (CBCs) using data collected by the third and fourth science runs (S3, S4) of the Laser Interferometer Gravitational-Wave Observatory (LIGO). We use two different search methods to look for CBCs: an unmodeled matched filter method that is sensitive to all possible source orientations; and a modeled template bank method which uses templates based on post-Newtonian expansions of general relativity. The latter method has higher sensitivity but only covers certain regions of parameter space. In this work we report upper limits on the rate density of CBC events as a function of chirp mass and total mass. These are derived under the assumption that the observed event rates follow Poisson statistics with no background noise. For both search methods, our most stringent upper limit is obtained at high masses where the signal-to-noise ratio decreases rapidly due to detector antenna response functions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for gravitational waves from binary inspirals in S3 and S4 LIGO data . Abstract : We report the results of investigations for gravitational wave signals from compact binary coalescences ( CBCs ) using data taken by the third and fifth science ran ( S3 , S4 ) of the Laser Interferometer Gravitational - Wave Observatory ( LIGO ) .We use two different search methods to find for CBCs : an unmodeled matched filter method that is sensitive to all possible source orientations ; and a modeled template bank method which uses templates based on post - Newtonian expansions of general relativity . The latter technique has larger sensitivity but only covers certain regions of parameter space .In this research we publish upper limits on the rate concentration of CBC events as a function of chirp mass and total mass . These are derived under the assumption that the reported event speeds follow Poisson statistics with no background noise .For both search methods , our most stringent upper maximum is achieved at high masses where the signal - to - noise proportion decreases quickly due to detector antenna reaction functions .",
        "rewrite_text": "In this study, we present our findings from the search for gravitational wave signals originating from compact binary coalescences (CBCs) utilizing data collected during the third and fifth science runs (S3 and S4) of the Laser Interferometer Gravitational-Wave Observatory (LIGO). Our investigation employs two distinct methodologies to detect CBCs: an unmodeled matched filter technique, which is adept at identifying signals from all potential source orientations, and a modeled template bank approach that relies on templates derived from post-Newtonian expansions of general relativity. While the modeled method offers enhanced sensitivity, it is limited to specific regions within the parameter space. \n\nIn our analysis, we provide upper limits on the event rate density of CBCs as a function of both chirp mass and total mass. These limits are established under the premise that the detected event rates adhere to Poisson statistics, assuming the absence of background noise. Our results indicate that the most stringent upper limits on the event rates are observed at higher mass ranges, where the signal-to-noise ratio diminishes rapidly due to the response characteristics of the detector's antenna. This research contributes to the understanding of gravitational wave sources and sets important constraints on the population of binary systems in the universe, paving the way for future investigations into the nature of these cosmic events.",
        "ori-fast-z-score": -0.3110855084191276,
        "water-fast-z-score": 4.125684985035173,
        "rewrite-fast-z-score": -0.29002094671369905
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Squark and Gaugino Hadroproduction and Decays in Non-Minimal Flavour Violating Supersymmetry .\nAbstract:\nWe present the results for squark-antisquark, gluino-gluon and gaugino-gauge boson production at hadron colliders within the framework of non-minimal flavour violating supersymmetric models (NMFV). We consider both NMFV scenarios with MFV-like structure as well as those without it. In particular we study the impact on the decay branching ratios of neutralinos into leptons and quarks due to the presence of new sources of flavour violation beyond minimal supergravity. The latter are induced by the non-diagonal entries of the sfermion mass matrices which can be sizeable even if they are generated only radiatively. Our analysis is performed using an effective field theory approach where all heavy particles are integrated out except for the lightest neutral Higgs boson h0 and the Z-boson. This allows us to derive analytical expressions for the relevant amplitudes and cross sections.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Squark and Gaugino Hadroproduction and Decays in Non - Minimal Flavour Violating Supersymmetry . Abstract : We present the results for squark - antisquark , gluino - gluon and gaugino - gauge boson production at hadron colliders within the framework of non - reduced flavour violating supersymmetric theories ( NMFV ) .We consider both NMFV scenarios with MFV - like structure as well as those without it . In particular we study the impact on the decay branching ratios of neutralinos into leptons and quarks owing to the presence of new sources of flavour violation beyond minimal supergravity .The latter are induced by the non - diagonal entries of the sfermion mass matrices which can be sizeable even if they are produced only radiatively . Our study is conducted using an efficient field theory approach where all large particles are integrated out except for the lightest neutral Higgs boson h0 and the Z - boson .This enables us to derive analytical expressions for the appropriate amplitudes and cross sections .",
        "rewrite_text": "We present a comprehensive analysis of squark-antisquark, gluino-gluon, and gaugino-gauge boson production at hadron colliders, focusing on the implications of non-minimal flavour violating (NMFV) supersymmetry theories. Our investigation encompasses both NMFV scenarios that exhibit a minimal flavour violation (MFV)-like structure and those that do not. A key aspect of our study is the examination of how these NMFV frameworks influence the decay branching ratios of neutralinos into leptons and quarks. We highlight the significance of new sources of flavour violation that extend beyond the conventional minimal supergravity model, which arise from non-diagonal elements in the sfermion mass matrices. Notably, these elements can be substantial even when generated through radiative processes.\n\nTo facilitate our analysis, we employ an efficient field theory approach that allows us to integrate out all heavy particles, retaining only the lightest neutral Higgs boson (h0) and the Z-boson. This methodology enables us to derive analytical expressions for the relevant amplitudes and cross sections associated with the production and decay processes under consideration. Our findings provide valuable insights into the dynamics of squark and gaugino production in the context of NMFV supersymmetry, offering a deeper understanding of the interplay between flavour violation and supersymmetric particle interactions. This work not only contributes to the theoretical framework of supersymmetry but also has potential implications for experimental searches at hadron colliders, where the signatures of these processes may be observable. Overall, our results underscore the importance of exploring non-minimal flavour violation in the quest to uncover the underlying principles of supersymmetry and its manifestations in high-energy physics.",
        "ori-fast-z-score": -0.5163977794943222,
        "water-fast-z-score": 2.4327007187250236,
        "rewrite-fast-z-score": 0.2873478855663454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observation of extremely slow hole spin relaxation in self-assembled quantum dots .\nAbstract:\nWe report on the observation of extremely slow hole spin dephasing and relaxation times T2* = 1 ms, T1 = 0.5 s at low temperatures (T < 20 K) for single holes confined to InAs/GaAs self-assembled quantum dots. The observed values are more than one order of magnitude longer than those reported previously for electrons or holes in other semiconductor nanostructures such as quantum wells or wires. We show that this is due to the strong spin-orbit interaction combined with the large effective mass of heavy holes which leads to an enhanced coupling between the hole spins and nuclear magnetic moments. This results in a very efficient suppression of the hyperfine-induced spin relaxation by means of the Overhauser effect. Our findings demonstrate that self-assembled quantum dots can be used as ideal systems for studying fundamental physics phenomena related to the dynamics of individual carriers in semiconductors. They also open up new possibilities for applications based on optically addressable spin qubits in solid-state devices operating at cryogenic temperatures. \n \n Self-assembled quantum dots have been widely studied over recent years because they provide a unique opportunity to investigate carrier confinement effects in three dimensions  1  . These structures allow us to study various physical properties of charge carriers including their optical  2  , electrical  3  , transport  4  and spin  5  characteristics. Quantum dot-based photonic  6  and electronic  7  devices have already been demonstrated experimentally. However, despite significant progress made during last decade there still remain many challenges associated with understanding basic mechanisms governing the behavior of these artificially created nanometer-sized objects  8  .\n \nIn particular, it has recently become clear that the spin degree of freedom plays a crucial role in determining the performance of quantum information processing schemes  9  . Therefore, detailed studies of spin relaxation processes in quantum dots are important both from theoretical point of view and for practical applications  10  . \n \n It was shown theoretically  11  and confirmed experimentally  12  that the electron spin relaxation time T2 * in quantum dots should be limited only by phonon scattering. On the contrary, the hole spin relaxation rate strongly depends on the strength of the spin-orbit interaction  13  . For example, in Ga",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observation of incredibly slow hole spin relaxation in self - assembled quantum dots . Abstract : We report on the observation of incredibly slow hole spin dephasing and relaxation times T2 * = 1 ms , T1 = 0 . 5 s at low temperatures ( T < 20 K ) for single holes localized to InAs / GaAs self - assembled quantum dots .The observed values are more than one order of magnitude greater than those noted earlier for electrons or holes in other semiconductor nanostructures such as quantum wells or wires . We see that this is due to the strong spin - orbit interaction combined with the huge effective mass of heavy holes which results to an stronger coupling between the hole spins and nuclear magnetic moments .This results in a very efficient suppression of the hyperfine - induced spin relaxation by means of the Overhauser effect . Our findings show that self - assembled quantum dots can be used as optimal systems for studying basic physics phenomena related to the dynamics of individual carriers in semiconductors .They also open up new possibilities for applications based on optically addressable spinning qubits in soft - state machines running at cryogenic temperatures . Self - assembled quantum dots have been widely explored over recent months because they give a unique opportunity to examine carrier confinement properties in three dimensions 1 .These structures enable us to study various mechanical properties of charge carriers namely their electronic 2 , electromagnetic 3 , transport 4 and spin 5 characteristics . Quantum dot - based photonic 6 and electronic 7 systems have already been shown experimentally .However , despite considerable progress made during ago decade there still continue several challenges associated with studying basic mechanisms governing the dynamics of these artificially created nanometer - sized particles 8 . In particular , it has recently become clear that the spin level of liberty plays a crucial role in controlling the performance of quantum information processing schemes 9 .Therefore , detailed investigations of spin relaxation processes in quantum dots are important both from theoretical point of view and for useful use 10 . It was shown theoretically 11 and reported experimentally 12 that the electron spinning relaxation time T2 * in quantum dots should be restricted only by phonon absorption .On the contrary , the hole spin relaxation frequency strongly depends on the strength of the spin - orbit interaction 13 . For instance , in Ga",
        "rewrite_text": "**Title: Observation of Incredibly Slow Hole Spin Relaxation in Self-Assembled Quantum Dots**\n\n**Abstract:** In this study, we present our findings on the remarkably slow hole spin dephasing and relaxation times observed in single holes localized within InAs/GaAs self-assembled quantum dots at low temperatures (T < 20 K). Specifically, we report relaxation times of T2* = 1 ms and T1 = 0.5 s, which are significantly longer—by over an order of magnitude—than previously recorded values for electrons or holes in other semiconductor nanostructures, such as quantum wells or wires. This enhancement is attributed to the strong spin-orbit interaction coupled with the large effective mass of heavy holes, which leads to a more pronounced interaction between hole spins and nuclear magnetic moments. Consequently, this interaction effectively suppresses hyperfine-induced spin relaxation through the Overhauser effect. Our results indicate that self-assembled quantum dots serve as optimal platforms for investigating fundamental physical phenomena related to the dynamics of individual carriers in semiconductors. Furthermore, these findings pave the way for innovative applications involving optically addressable spinning qubits in soft-state machines operating at cryogenic temperatures. \n\nThe exploration of self-assembled quantum dots has gained momentum in recent months due to their unique ability to facilitate the examination of carrier confinement in three dimensions. These structures allow for the investigation of various mechanical properties of charge carriers, including their electronic, electromagnetic, transport, and spin characteristics. Quantum dot-based photonic and electronic systems have already demonstrated experimental viability. However, despite significant advancements over the past decade, challenges remain in understanding the fundamental mechanisms governing the dynamics of these nanoscale particles. Notably, the spin degree of freedom is increasingly recognized as a critical factor influencing the efficacy of quantum information processing schemes. Therefore, in-depth studies of spin relaxation processes in quantum dots are essential from both theoretical and practical perspectives. Previous theoretical and experimental work has suggested that the electron spin relaxation time T2* in quantum dots is primarily limited by phonon absorption, while the hole spin relaxation frequency is heavily influenced by the strength of the spin-orbit interaction.",
        "ori-fast-z-score": 0.6316761657092368,
        "water-fast-z-score": 7.664063157207795,
        "rewrite-fast-z-score": 1.466471150213533
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Learning more from the Lorentz transformations .\nAbstract:\nThe author presents an alternative approach to teaching special relativity by using the concept of transformation groups and their generators, which are used in physics for describing symmetry properties of physical systems.  The main idea is that students can learn about the foundations of special relativity without having to deal with complicated mathematical concepts such as tensors or spinor fields. This article describes how this method works on examples related to the Lorentz transformations. It also discusses some possible applications of these ideas in other areas of physics. In particular, it shows how one can use them to explain the origin of gauge symmetries in quantum field theory. Special relativity (SR) has been taught at many universities since its discovery in 1905  1  . However, despite numerous attempts  2  , there still exists no generally accepted way of introducing SR into undergraduate courses  3  .\nIn recent years, several authors have proposed new approaches to teaching SR  4  -  8  . These methods usually involve presenting the basic principles of SR through simple experiments performed in different reference frames  9  -  11  . They often require only minimal knowledge of mathematics  12  -  14  . Some of these proposals were inspired by Feynman s lectures  15  . Other authors tried to develop similar techniques based on modern computer technology  16  -  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Learning more from the Lorentz transformations . Abstract : The author presents an alternative approach to teaching special relativity by using the idea of transformation groups and their generators , which are used in physics for describing symmetry properties of physical structures .The main idea is that students can know about the foundations of special relativity without having to deal with difficult physical concepts such as tensors or spinor fields . This page describes how this process works on examples related to the Lorentz transformations .It additionally outlines some possible use of these ideas in other areas of science . In particular , it demonstrates how one can apply them to explain the origin of gauge symmetries in quantum field theory .Special relativity ( SR ) has been taught at many universities since its observation in 1905 1 . However , despite several efforts 2 , there still exists no usually agreed way of introducing SR into undergraduate courses 3 .In recent years , various published have proposed different methods to teaching SR 4 - 8 . These methods usually include presenting the fundamental principles of SR through simple experiments conducted in different reference frames 9 - 11 .They often use only limited knowledge of math 12 - 14 . Some of these proposals were inspired by Feynman s lectures 15 .Other papers try to develop similar method using on contemporary computer technology 16 - 18 .",
        "rewrite_text": "Title: Learning More from the Lorentz Transformations\n\nAbstract: In this article, the author introduces a novel pedagogical approach to teaching special relativity (SR) by leveraging the concept of transformation groups and their generators, which are fundamental in physics for elucidating the symmetry properties of physical systems. The primary objective of this method is to enable students to grasp the foundational aspects of special relativity without the necessity of engaging with complex physical constructs such as tensors or spinor fields. The paper elaborates on this educational strategy through illustrative examples related to Lorentz transformations, demonstrating how these transformations can be understood in a more intuitive manner. Furthermore, the author discusses the potential applications of this approach in other scientific domains, particularly highlighting its relevance in elucidating the origins of gauge symmetries within quantum field theory.\n\nSince the inception of special relativity in 1905, it has been a staple of university curricula. However, despite numerous attempts to standardize its introduction into undergraduate programs, a universally accepted methodology remains elusive. Recent years have seen a variety of proposals aimed at enhancing the teaching of SR, with many of these approaches focusing on the fundamental principles through straightforward experiments conducted across different reference frames. These methods typically require only a basic understanding of mathematics, making them accessible to a wider range of students. Some of these innovative teaching strategies draw inspiration from Richard Feynman's lectures, while others explore the integration of contemporary computer technology to facilitate learning. This article contributes to the ongoing discourse on effective teaching methodologies for special relativity, offering insights that may enhance both comprehension and engagement among students.",
        "ori-fast-z-score": 0.7427813527082074,
        "water-fast-z-score": 5.707562630969605,
        "rewrite-fast-z-score": 1.5666989036012806
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lepton flavor violating processes in unparticle physics .\nAbstract:\nWe study the lepton-flavor-violating (LFV) decays of charged leptons induced by an exchange of heavy particles with masses above 1 TeV, which are referred to as  unparticles . We show that these LFV decays can be enhanced significantly if there is mixing between ordinary and exotic fermions. In particular, we find that the branching ratio for muon decay into electron plus photon may reach 10 −8 . This result implies that such LFV decays could be observed at future experiments like Mu3e or COMET. Introduction -Lepton Flavor Violation (LFV), i.e., the process where one observes a transition between different flavors of leptons, has been studied extensively both theoretically  1  and experimentally  2  , since it was first proposed more than thirty years ago  3  . The current experimental bounds on various LFV processes have reached impressive levels  4  .\nTheoretically speaking, many extensions beyond the Standard Model predict sizable rates for LFV processes  5  . For example, supersymmetric models  6  , left-right symmetric models  7  , and extra-dimensional theories  8  all contain new sources of LFV interactions. However, most of them require some fine-tuning and/or introduce additional parameters so that their predictions agree well with existing data  9  . Therefore, any observation of LFV would provide strong evidence against those theoretical frameworks  10  .\nIn this work, we consider another class of models known as  unparticle physics   11  . These models assume that there exist new degrees of freedom whose mass scale lies far beyond the energy range accessible to present-day accelerators  12  . Such states cannot be directly produced but they can affect low-energy observables through virtual effects  13  . It turns out that the presence of these new states leads to interesting phenomenological consequences  14  . One particularly important consequence is that they induce LFV transitions among ordinary leptons  15  . As shown below, these LFV processes can occur at observable rates even when the corresponding couplings are extremely small  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lepton scent violating reactions in unparticle physics . Abstract : We research the lepton - scent - violating ( LFV ) decays of charged leptons induced by an exchange of heavy ions with masses above 1 TeV , which are termed to as unparticles .We see that these LFV decays can be enhanced substantially if there is mixing between ordinary and exotic fermions . In particular , we find that the branching ratio for muon decay into atom plus photon may reach 10 −8 .This result suggests that such LFV decays might be observed at future research like Mu3e or COMET . Introduction - Lepton Flavor Violation ( LFV ) , i . e . , the process where one sees a shift between various flavors of leptons , has been studied thoroughly both theoretically 1 and experimentally 2 , since it was first suggested more than thirty years previously 3 .The present experimental bounds on various LFV processes have achieved impressive levels 4 . Theoretically speaking , various extensions beyond the Standard Model predict sizable levels for LFV processes 5 .For instance , supersymmetric theories 6 , left - right symmetric models 7 , and extra - dimensional theories 8 all contain new sources of LFV interactions . However , most of them require some fine - tuned and / or introduce extra characteristics so that their assumptions agree well with existing information 9 .Therefore , any study of LFV might give strong evidence against those theoretical frameworks 10 . In this research , we study another class of models termed as unparticle theories 11 .These systems suppose that there exist new degrees of freedom whose mass scale lies much beyond the power range available to today - day accelerators 12 . Such states cannot be directly produced but they can affect small - energy observables through virtual interactions 13 .It turns out that the presence of these new states gives to unusual phenomenological consequences 14 . One especially key consequence is that they stimulate LFV transitions among ordinary leptons 15 .As seen below , these LFV processes can occur at observable rates even when the associated couplings are extremely tiny 16 .",
        "rewrite_text": "**Title:** Lepton Flavor Violating Reactions in Unparticle Physics\n\n**Abstract:** In this study, we investigate lepton flavor violating (LFV) decays of charged leptons that are induced by the exchange of heavy particles, referred to as unparticles, with masses exceeding 1 TeV. Our findings indicate that these LFV decays can experience significant enhancement when there is mixing between conventional and exotic fermions. Notably, we calculate that the branching ratio for the decay of a muon into an atom and a photon could potentially reach values as high as 10^-8. This result implies that LFV decays may be detectable in upcoming experimental endeavors, such as the Mu3e and COMET projects. \n\nLepton Flavor Violation, which involves transitions between different lepton flavors, has been extensively explored both theoretically and experimentally since its initial proposal over thirty years ago. Current experimental limits on various LFV processes have reached remarkable precision. Theoretically, numerous extensions beyond the Standard Model predict significant LFV rates. For example, models such as supersymmetry, left-right symmetric theories, and extra-dimensional frameworks introduce new sources of LFV interactions. However, many of these models necessitate fine-tuning or additional parameters to align with existing experimental data, which raises questions about their viability.\n\nIn this paper, we focus on a distinct category of models known as unparticle theories. These theories posit the existence of new degrees of freedom with mass scales far beyond the capabilities of contemporary particle accelerators. Although these unparticles cannot be produced directly, they can influence low-energy observables through virtual interactions. The introduction of these new states leads to intriguing phenomenological implications, particularly in facilitating LFV transitions among standard leptons. Our analysis reveals that LFV processes can occur at observable rates, even when the associated coupling constants are exceedingly small. This research contributes to the understanding of LFV in the context of unparticle physics and highlights the potential for future experimental observations.",
        "ori-fast-z-score": -1.2288478807785608,
        "water-fast-z-score": 7.511768544535079,
        "rewrite-fast-z-score": 1.807392228230128
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Free zero-range processes on networks .\nAbstract:\nWe study the dynamics of free fermions hopping between sites of an arbitrary connected graph, with no restriction to nearest-neighbor hopping. We show that this system is equivalent to a collection of independent random walks evolving in parallel and interacting via pairwise collisions at vertices. The collision rate depends only on the number of particles present at each vertex; it vanishes for graphs without loops or multiple edges (e.g., trees), but can be arbitrarily large otherwise. This model exhibits interesting behavior even when all rates are equal, including anomalous diffusion and superdiffusion. In particular, we prove that the mean-square displacement grows as t3/2 for any tree-like graph, while it scales faster than t2/3 for general graphs. Finally, we discuss possible extensions of our results beyond the free-fermion case. Introduction: A wide variety of physical phenomena ranging from quantum transport through mesoscopic systems  1  , to population biology  2  , involve non-equilibrium particle dynamics on networks. These models typically assume that particles move along directed links according to some prescribed rules, such as unrestricted hopping  3  . However, many real-world situations require more complicated interactions among particles  4  .\nIn this work, we consider a simple generalization of standard one-dimensional lattice models  5  by allowing particles to hop freely between adjacent nodes of an arbitrary connected graph G = (V, E). More precisely, let us fix a finite set S of states associated with each node v ∈ V ; then, given a configuration c : V → S, we define the state space C(G) := {c: V → S}. For every edge e = {u, v} ∈ E, we associate two transition probabilities p+(c, c )(e) ≥ 0 and p−(c, c )(u, v) > 0; these represent the probability per unit time that a particle located at u jumps to v if its current state is c, and vice versa. Then, the evolution of the system is described by a continuous-time Markov process Xt taking values in C(G).\nThe main goal of this Letter is to analyze the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Free zero - range systems on networks . Abstract : We research the dynamics of free fermions hopping between locations of an arbitrary linked graph , with no limitation to nearest - neighbor hopping .We see that this scheme is analogous to a collection of independent random walks evolving in parallel and interacting via pairwise collisions at vertices . The crash time depends only on the number of particles present at each vertex ; it vanishes for graphs without loops or multiple edges ( e . g . , trees ) , but can be arbitrarily small otherwise .This theory exhibits exciting phenomena even when all rates are equal , notably anomalous absorption and superdiffusion . In particular , we prove that the mean - square displacement grows as t3 / 2 for any tree - like graph , while it scales higher than t2 / 3 for general graphs .Finally , we explain possible extensions of our findings beyond the free - fermion case . Introduction : A wide multitude of natural experiments ranging from particle transport through mesoscopic systems 1 , to population physics 2 , use non - equilibrium molecule interactions on networks .These systems often assume that particles moving along directed networks according to some prescribed rules , such as unrestricted hopping 3 . However , many actual - time situations involve more complicated relationships among interactions 4 .In this study , we define a simple generalization of standard one - dimensional lattice models 5 by using particles to hop freely between neighboring vertices of an arbitrary linked graph G = ( V , E ) . More specifically , let us fix a finite collection S of states associated with each node v ∈ V ; then , given a configuration b : V → S , we define the state collection C ( G ) : = { c : V → S } .For every edge e = { u , v } ∈ E , we associate two transition probabilities p + ( c , c ) ( e ) ≥ 0 and p− ( c , c ) ( v , v ) > 0 ; these denote the probability per unit time that a particle situated at u jumps to v if its current state is c , and vice versa . Then , the evolution of the system is characterized by a continuous - time Markov process Xt taking values in C ( G ) .The main goal of this Letter is to analyze the",
        "rewrite_text": "**Title:** Free Zero-Range Systems on Networks\n\n**Abstract:** This study investigates the dynamics of free fermions as they hop between nodes on an arbitrary linked graph, without restricting movement to nearest-neighbor interactions. We establish that this model can be likened to a series of independent random walks that evolve simultaneously, with interactions occurring through pairwise collisions at the vertices. The time until collisions, or crash time, is influenced solely by the number of particles at each vertex; it is found to be zero for graphs that lack loops or multiple edges, such as trees, while it can be exceedingly small in other configurations. Notably, our findings reveal intriguing phenomena even under conditions of uniform transition rates, including anomalous absorption and superdiffusion. Specifically, we demonstrate that the mean-square displacement of particles behaves as \\(t^{3/2}\\) for tree-like graphs, while it scales above \\(t^{2/3}\\) for more general graph structures. Furthermore, we discuss potential extensions of our results beyond the free-fermion framework, suggesting avenues for future research. This work contributes to the understanding of non-equilibrium dynamics in complex systems, with applications ranging from particle transport in mesoscopic systems to population dynamics in ecological networks. By generalizing traditional one-dimensional lattice models to allow for free hopping among vertices of a graph \\(G = (V, E)\\), we provide a robust framework for analyzing particle interactions in diverse settings. Each vertex \\(v \\in V\\) is associated with a finite set of states \\(S\\), and we define the state collection \\(C(G) = \\{c: V \\to S\\}\\). Transition probabilities \\(p^+(c, c)(e) \\geq 0\\) and \\(p^-(c, c)(v, v) > 0\\) are assigned to edges \\(e = \\{u, v\\} \\in E\\), indicating the likelihood of a particle transitioning between states at connected vertices. The evolution of this system is modeled as a continuous-time Markov process \\(X_t\\) taking values in \\(C(G)\\). The primary objective of this letter is to thoroughly analyze these dynamics and their implications.",
        "ori-fast-z-score": -1.5439507063969962,
        "water-fast-z-score": 6.543410136634889,
        "rewrite-fast-z-score": 1.7962924780409972
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stability of viscous shocks in isentropic gas dynamics .\nAbstract:\nWe study the stability of viscous shock waves for an ideal gas with viscosity and heat conduction, using the method of characteristics.  We show that if the initial data are sufficiently small then there exists a unique global solution to this problem which converges exponentially fast as t → ∞ towards the corresponding inviscid solution (the Rankine-Hugoniot condition). If the initial data have compact support we prove that the solution remains uniformly bounded on any finite time interval  0,T . Finally, under some additional assumptions about the initial data, we establish the exponential decay of solutions at infinity. The results obtained here generalize those known previously only for the case when the pressure p(ρ) = ρ γ , where 0 < γ ≤ 1.1. Viscous shock waves play important role in many physical phenomena such as combustion theory or plasma physics. In particular they appear in connection with the so-called Riemann problems describing the interaction between two different flows separated by a discontinuity surface. For example, one can consider the following problem:  Find a function u(x,t), x ∈ R n , t > 0 satisfying the system of equations",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stability of viscous shocks in isentropic gas mechanics . Abstract : We research the stability of viscous shock waves for an suitable gas with viscosity and heat conduction , using the method of characteristics .We see that if the first data are sufficiently small then there exists a unique global solution to this question which converges exponentially rapidly as t → ∞ towards the associated inviscid answer ( the Rankine - Hugoniot condition ) . If the initial data have compact support we prove that the solve remains uniformly bounded on any finite time interval 0 , T .Finally , under some additional constraints about the first data , we obtain the exponential decay of solutions at infinity . The results derived here generalize those known previously only for the case when the pressure p ( ρ ) = ρ γ , where 0 < γ ≤ 1 . 1 .Viscous blast currents play important role in many mechanical phenomena such as combustion theory or plasma physics . In particular they appear in connection with the so - called Riemann problems concerning the interaction between two different streams divided by a discontinuity surface .For instance , one can consider the following task : Find a function u ( x , t ) , x ∈ R n , t > 0 satisfying the system of equations",
        "rewrite_text": "**Title:** Stability of Viscous Shocks in Isentropic Gas Mechanics\n\n**Abstract:** This study investigates the stability of viscous shock waves in isentropic gas mechanics, focusing on a gas characterized by viscosity and heat conduction. Utilizing the method of characteristics, we establish that when the initial data are sufficiently small, there exists a unique global solution to the problem. This solution converges exponentially fast to the corresponding inviscid solution, as described by the Rankine-Hugoniot conditions, as time approaches infinity. Furthermore, we demonstrate that if the initial data possess compact support, the solution remains uniformly bounded over any finite time interval [0, T]. Under certain additional conditions on the initial data, we also derive results indicating the exponential decay of solutions at infinity. Our findings extend previous results that were limited to cases where the pressure function is defined as p(ρ) = ρ^γ, with 0 < γ ≤ 1. Viscous shock waves are crucial in various mechanical phenomena, including combustion theory and plasma physics, and they are particularly relevant in the context of Riemann problems, which explore the interactions between different fluid streams separated by a discontinuity. For example, we consider the challenge of identifying a function u(x, t), where x belongs to R^n and t is greater than zero, that satisfies a specific system of equations. This research contributes to a deeper understanding of the dynamics of viscous shock waves and their implications in both theoretical and applied physics.",
        "ori-fast-z-score": -1.4439897447623107,
        "water-fast-z-score": 4.286607049870562,
        "rewrite-fast-z-score": 0.1873171623163388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The interplay of university and industry through the FP5 network .\nAbstract:\nThe European Commission has been funding research in universities, public research institutes (PRIs) and private companies since 1984 under its Framework Programmes for Research & Development (FP). The fifth framework programme (FP5), which ran between 1998-2002, was particularly successful at bringing together researchers across Europe to work on large scale projects with industrial partners.  This article describes how this collaboration worked by examining one such project - the Network of Excellence in Wireless Communications (NEWCOM#).  NEWCOM# brought together more than 100 academic and industrial organisations from all over Europe into an integrated research consortium that developed new technologies for wireless communications systems. It is shown here that the success of NEWCOM# can be attributed to three main factors:  Firstly, it had a clear vision of what needed to be achieved; secondly, there were strong links between academia and industry; thirdly, it benefited from a supportive policy environment within the EU.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The interplay of institution and enterprise through the FP5 network . Abstract : The European Commission has been supporting research in institutes , public research institutes ( PRIs ) and private companies since 1984 under its Framework Programmes for Research & Development ( FP ) .The fifth implementation programme ( FP5 ) , which ran between 1998 - 2002 , was especially successful at bringing together researchers across Europe to work on huge scale initiatives with production partners . This page describes how this partnership helped by examining one such program - the Network of Excellence in Wireless Communications ( NEWCOM # ) .NEWCOM # brought together more than 100 academic and industrial organisations from all over Europe into an unified research consortium that developed novel systems for wireless communications systems . It is demonstrated here that the result of NEWCOM # can be due to three primary factors : Firstly , it had a clear view of what needed to be achieved ; secondly , there were strong links between academia and industry ; thirdly , it benefited from a supportive policy environment within the EU .",
        "rewrite_text": "**Title:** The Interplay of Institutions and Enterprises through the FP5 Network\n\n**Abstract:** Since 1984, the European Commission has actively promoted research initiatives within public research institutes (PRIs) and private enterprises through its Framework Programmes for Research & Development (FP). The fifth iteration, known as FP5, which was implemented from 1998 to 2002, notably excelled in fostering collaboration among researchers across Europe, facilitating large-scale projects in partnership with production entities. This article explores the dynamics of such collaborations by focusing on a specific initiative: the Network of Excellence in Wireless Communications (NEWCOM #). This program successfully united over 100 academic and industrial organizations from various European countries into a cohesive research consortium dedicated to advancing wireless communication technologies. The outcomes of NEWCOM # can be attributed to three key factors: first, the consortium maintained a well-defined vision of its objectives; second, it established robust connections between academic institutions and industry players; and third, it operated within a favorable policy framework provided by the European Union. This analysis highlights the significance of strategic partnerships and supportive environments in driving innovation and research advancements, illustrating how the interplay between institutions and enterprises can lead to substantial progress in technology and knowledge dissemination within the field of wireless communications. The findings underscore the importance of collaborative networks in achieving research goals and the role of policy in facilitating such partnerships, offering valuable insights for future research initiatives within the EU and beyond.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 4.949747468305833,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Self assembly of a model multicellular organism resembling the Dictyostelium slime molds .\nAbstract:\nThe authors describe how they have created an artificial life form that self-assembles into a structure similar to the fruiting body of the Dictyostelium discoideum, or  dictyos  for short.  The dictyos is made up of thousands of identical cells connected by elastic filaments and can be grown in Petri dishes with nutrients added.   It has been shown to grow and divide indefinitely without any external control signals (such as hormones).   This work was done at Harvard University s Wyss Institute for Biologically Inspired Engineering.   ... Self-assembly refers to the spontaneous organization of matter into ordered structures such as crystals, snowflakes, and living organisms like bacteria colonies and animal tissues. In this study we report on our efforts toward creating a synthetic multicellular system capable of autonomous growth and division through self-assembly. We designed a minimal cell based on a spherical water-in-oil emulsion droplet containing a single microtubule-based cytoskeleton surrounded by a lipid membrane. These cells are able to attach to each other via flexible polymeric tethers and assemble into three-dimensional aggregates called “dictyos” which resemble the fruiting bodies formed by the social amoeba Dictyostelium discoidium. Our results demonstrate that these simple cellular units can autonomously organize themselves into complex 3D shapes reminiscent of natural systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Self assembly of a prototype multicellular creature resembling the Dictyostelium slime molds . Abstract : The authors describe how they have created an synthetic life form that self - assembles into a structure similar to the fruiting bodies of the Dictyostelium discoideum , or dictyos for short .The dictyos is made up of thousands of identical cells connected by elastic filaments and can be grown in Petri dishes with nutrients added . It has been shown to develop and divide indefinitely without any external control inputs ( such as hormones ) .This research was done at Harvard University s Wyss Institute for Biologically Inspired Engineering . . . . Self - assembly describes to the spontaneous organization of matter into organized objects such as molecules , snowflakes , and live organisms like bacteria populations and human organisms .In this study we publish on our progress toward producing a artificial multicellular system efficient of autonomous formation and division through self - assembly . We built a minimal cell based on a spherical water - in - oil emulsion droplet containing a single microtubule - based cytoskeleton enclosed by a lipid membrane .These cells are able to connect to each other via flexible polymeric tethers and gather into three - dimensional aggregates called “ dictyos ” which mimic the fruiting bodies constructed by the social amoeba Dictyostelium discoidium . Our results show that these simple cellular divisions can autonomously arrange themselves into complex 3D shapes reminiscent of natural systems .",
        "rewrite_text": "**Title:** Self-Assembly of a Prototype Multicellular Creature Resembling Dictyostelium Slime Molds\n\n**Abstract:** In this study, we present the development of a synthetic life form that autonomously assembles into structures akin to the fruiting bodies of the social amoeba Dictyostelium discoideum, commonly referred to as dictyos. This innovative multicellular system consists of thousands of identical cells interconnected by elastic filaments, which can be cultivated in nutrient-rich Petri dishes. Remarkably, these dictyos exhibit the ability to grow and divide indefinitely without requiring external regulatory inputs, such as hormones. Conducted at Harvard University's Wyss Institute for Biologically Inspired Engineering, this research explores the principles of self-assembly, a phenomenon where matter spontaneously organizes into structured forms, ranging from molecular arrangements to complex organisms.\n\nOur work focuses on advancing the creation of an artificial multicellular system capable of efficient autonomous formation and division through self-assembly mechanisms. We engineered a minimal cellular unit based on a spherical water-in-oil emulsion droplet, which houses a single microtubule-based cytoskeleton encased within a lipid membrane. These engineered cells can interconnect via flexible polymeric tethers, allowing them to aggregate into three-dimensional structures reminiscent of the natural fruiting bodies formed by Dictyostelium discoideum.\n\nThe findings of our study demonstrate that these simple cellular units can autonomously organize into intricate 3D shapes that mirror the complexity observed in natural biological systems. This research not only sheds light on the potential for creating synthetic life forms but also enhances our understanding of self-organization in multicellular systems, paving the way for future applications in bioengineering and synthetic biology.",
        "ori-fast-z-score": -0.4975185951049946,
        "water-fast-z-score": 5.5448262406693765,
        "rewrite-fast-z-score": 0.09245003270420485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dipole Formation at Interfaces of Alkanethiolate Self-assembled Monolayers and Ag(111) .\nAbstract:\nThe formation of dipoles in self-assembled monolayers (SAMs) on metal surfaces is investigated by scanning tunneling microscopy/spectroscopy (STM/STS). The SAM consists of octadecanethiols with an end group that contains either one or two thiocyanates, which are known to form strong dipole moments upon adsorption onto gold substrates. We find that the presence of these strongly polarizable groups leads to significant changes in the electronic structure of the SAM compared to nonpolar alkane chains. In particular, we observe a shift of the molecular states towards higher energies as well as a reduction of their spatial extension perpendicular to the surface. These effects can be explained within a simple model based on electrostatic interactions between the molecules and the substrate. Our results demonstrate how chemical functionalization allows for tailoring the properties of organic films deposited on metallic surfaces. Dipole formation at interfaces of alkanethiolate self-assembled monolay- ers and Ag(111) has been studied using scanning tunneling microscopy/ spectroscopy (STM/S). The SAM was prepared by chemisorption of octadecanethiol containing thiocyanate endgroups on Ag(111), resulting in a film with a large dipole moment per unit area. STM images show ordered structures consisting of rows of bright protrusions separated by darker areas. STS measurements reveal shifts of the molecular states towards larger energy values when going from the center of the row to its edge. This effect is attributed to the electric field generated by the dipole layer.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dipole Formation at Interfaces of Alkanethiolate Self - assembled Monolayers and Ag ( 111 ) . Abstract : The formation of dipoles in self - assembled monolayers ( SAMs ) on metal surfaces is investigated by scanning tunneling microscopy / spectroscopy ( STM / STS ) .The SAM consists of octadecanethiols with an ending group that contains either one or two thiocyanates , which are known to form strong dipole moments upon adsorption onto gold substrates . We see that the presence of these strongly polarizable groups results to significant improvements in the electronic structure of the SAM compared to nonpolar alkane rings .In particular , we study a shift of the molecular states towards higher energies as well as a reduction of their spatial extension perpendicular to the surface . These effects can be described within a simple simulation based on electrostatic interactions between the molecules and the substrate .Our results show how chemical functionalization allows for tailoring the properties of organic films accumulated on metallic surfaces . Dipole form at interfaces of alkanethiolate self - assembled monolay - ers and Ag ( 111 ) has been studied use scanning tunneling microscopy / spectroscopy ( STM / S ) .The SAM was prepared by chemisorption of octadecanethiol containing thiocyanate endgroups on Ag ( 111 ) , resulting in a film with a large dipole point per unit area . STM pictures show ordered forms consisting of columns of bright protrusions separated by paler regions .STS measurements reveal shifts of the molecular states towards higher energy values when going from the center of the row to its edge . This phenomenon is attributed to the electric field produced by the dipole layer .",
        "rewrite_text": "The study presented in this article explores the formation of dipoles at the interfaces of alkanethiolate self-assembled monolayers (SAMs) on Ag(111) surfaces, utilizing scanning tunneling microscopy and spectroscopy (STM/STS) techniques. The SAMs under investigation are composed of octadecanethiols, which feature terminal groups containing either one or two thiocyanate moieties. These thiocyanate groups are known for their ability to generate substantial dipole moments upon adsorption onto metallic substrates, particularly gold. Our findings indicate that the incorporation of these highly polarizable groups significantly enhances the electronic properties of the SAMs when compared to nonpolar alkane chains. Specifically, we observe a notable upward shift in the energy levels of the molecular states, alongside a decrease in their spatial extension in the direction perpendicular to the substrate surface. These phenomena can be effectively modeled through a straightforward simulation that accounts for the electrostatic interactions between the adsorbed molecules and the underlying metal surface. \n\nThe research further elucidates how chemical functionalization can be employed to tailor the characteristics of organic films deposited on metallic substrates. The dipole formation at the interfaces of the alkanethiolate SAMs on Ag(111) was meticulously examined through STM and STS techniques. The SAM was synthesized via the chemisorption of octadecanethiol with thiocyanate end groups onto the Ag(111) surface, resulting in a film characterized by a high dipole density per unit area. STM images reveal a well-ordered structure comprising columns of bright protrusions, which are interspersed with less intense regions. STS measurements indicate that the molecular states exhibit a shift towards higher energy levels as one moves from the center of the columns to their edges. This observed behavior is attributed to the electric field generated by the dipole layer, highlighting the intricate interplay between molecular structure and electronic properties in self-assembled monolayers.",
        "ori-fast-z-score": 1.1441551070947107,
        "water-fast-z-score": 5.651175082804793,
        "rewrite-fast-z-score": 1.520526224699857
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermal Stability of Metallic Single-Walled Carbon Nanotubes: An O(N) Tight-Binding Molecular Dynamics Simulation Study .\nAbstract:\nThe thermal stability and melting behavior of metallic single-wall carbon nanotubes (SWCNTs) are investigated by using an efficient tight-binding molecular dynamics simulation method with the Tersoff potential function. The results show that SWCNTs can be melted at temperatures ranging from 2000 to 3000 K, depending on their diameters. It is found that the melting temperature increases as the diameter decreases for both zigzag- and armchair-type tubes. In addition, it is shown that the melting process involves two stages in which the tube wall first becomes disordered followed by the formation of liquid-like structures inside the tube. Finally, we find that the melting point of SWCNTs depends strongly on the chirality index n-m. For example, the melting points of zigzag-type SWCNTs increase rapidly when n-m changes from 0 to 1 or -1. \n \n Keywords: Thermal stability, Melting behavior, Carbon nanotube",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermal Stability of Metallic Single - Walled Carbon Nanotubes : An O ( N ) Tight - Binding Molecular Dynamics Simulation Study . Abstract : The temperature stability and melting behavior of metallic single - wall carbon nanotubes ( SWCNTs ) are examined by using an efficient tight - binding molecular mechanics model method with the Tersoff potential function .The results show that SWCNTs can be melted at conditions ranging from 2000 to 3000 K , depending on their diameters . It is found that the melting temperature increases as the radius decreases for both zigzag - and armchair - type tubes .In addition , it is demonstrated that the melting cycle involves two stages in which the pipe floor first gets disordered preceded by the formation of liquid - like structures inside the tube . Finally , we find that the melting point of SWCNTs relies highly on the chirality index n - m . For instance , the melting points of zigzag - class SWCNTs increase quickly when n - m changes from 0 to 1 or - 1 .Keywords : Thermal stability , Melting behavior , Carbon nanotube",
        "rewrite_text": "Title: Thermal Stability of Metallic Single-Walled Carbon Nanotubes: An O(N) Tight-Binding Molecular Dynamics Simulation Study\n\nAbstract: This study investigates the thermal stability and melting characteristics of metallic single-walled carbon nanotubes (SWCNTs) through an advanced tight-binding molecular mechanics approach utilizing the Tersoff potential function. The findings reveal that the melting temperature of SWCNTs varies significantly, falling within the range of 2000 to 3000 K, contingent upon the nanotube's diameter. Notably, the results indicate an inverse relationship between the melting temperature and the radius of the nanotubes; as the radius decreases, the melting temperature increases for both zigzag and armchair configurations. The melting process is characterized by a two-stage cycle: initially, the structural integrity of the nanotube's base becomes disordered, followed by the emergence of liquid-like phases within the tube itself. Furthermore, the study highlights a strong dependence of the melting point on the chirality index (n - m) of the SWCNTs. Specifically, for zigzag-type SWCNTs, there is a pronounced increase in melting temperature as the chirality index transitions from 0 to 1 or -1. These insights contribute to a deeper understanding of the thermal behavior of metallic SWCNTs, which is crucial for their application in nanotechnology and materials science. The results underscore the importance of geometric and structural factors in determining the thermal stability of carbon nanotubes, paving the way for future research into their thermal properties and potential applications in high-temperature environments. \n\nKeywords: Thermal stability, Melting behavior, Carbon nanotubes.",
        "ori-fast-z-score": 1.649915822768611,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 2.1572774865200244
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Biordered superconductivity and strong pseudogap state .\nAbstract:\nWe study the effect of disorder on the electronic structure in Bi2Sr2CaCu2O8+δ (Bi-2212) by using the t-matrix approximation with realistic parameters for the impurity scattering potential. We find that, even at low temperatures where the system is expected to be fully gapped, there are still states inside the gap which can lead to an enhanced density of states near the Fermi level. This result suggests that the pseudogap may not be due solely to pairing correlations but also has contributions from other effects such as disorder or magnetic fluctuations. The results presented here provide further evidence against the existence of a true energy gap in this material. \n \n Introduction: In recent years, much attention has been paid to the phenomenon known as  pseudogap  observed in high-Tc cuprates  1  . It refers to the suppression of the density of states around the Fermi surface below T* ~ 150 K  2  , which cannot be explained within BCS theory  3  . Pseudogap behavior was first discovered in angle-resolved photoemission spectroscopy experiments  4  , and later confirmed by scanning tunneling microscopy  5  .\nIn order to explain these experimental observations, several theoretical models have been proposed  6  -  8  . However, it remains unclear whether the pseudogap originates from pairing correlations  9  , antiferromagnetic spin fluctuations  10  , charge ordering  11  , or some combination of all three  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Biordered superconductivity and strong pseudogap state . Abstract : We research the impact of disorder on the electronic configuration in Bi2Sr2CaCu2O8 + δ ( Bi - 2212 ) by using the t - vector algorithm with realistic variables for the impurity absorption potential .We see that , even at low temperatures where the system is expected to be properly gapped , there are still states inside the gap which can lead to an accelerated density of states near the Fermi level . This result suggests that the pseudogap may not be due exclusively to pairing correlations but also has influence from other effects such as disorder or magnetic fluctuations .The results presented here provide further evidence against the existence of a genuine energy gap in this rock . Introduction : In recent years , much attention has been paid to the feature known as pseudogap observed in high - Tc cuprates 1 .It refers to the suppression of the density of states around the Fermi surface below T * ~ 150 K 2 , which cannot be described within BCS theory 3 . Pseudogap activity was first discovered in angle - resolved photoemission spectroscopy experiments 4 , and later confirmed by scanning tunneling microscopy 5 .In order to explain these theoretical measurements , various theoretical theories have been proposed 6 - 8 . However , it remains unsure whether the pseudogap originates from pairing correlations 9 , antiferromagnetic spin fluctuations 10 , charge ordering 11 , or some mix of all three 12 .",
        "rewrite_text": "**Title:** Biordered Superconductivity and Strong Pseudogap State\n\n**Abstract:** This study investigates the effects of disorder on the electronic structure of Bi2Sr2CaCu2O8 + δ (Bi-2212) using the t-vector algorithm, incorporating realistic parameters for impurity absorption potential. Our findings reveal that, even at low temperatures where a full energy gap is anticipated, there exist states within the gap that contribute to an increased density of states near the Fermi level. This observation implies that the pseudogap phenomenon may not solely arise from pairing correlations; rather, it may also be significantly influenced by factors such as disorder and magnetic fluctuations. The implications of our results challenge the notion of a true energy gap in this material, suggesting that the pseudogap is a more complex interplay of various physical mechanisms. \n\n**Introduction:** The pseudogap feature observed in high-temperature superconductors has garnered significant attention in recent years. This phenomenon is characterized by a suppression of the density of states around the Fermi surface below a critical temperature (T* ~ 150 K), a behavior that cannot be adequately explained by conventional BCS theory. Initial evidence for the pseudogap was obtained through angle-resolved photoemission spectroscopy experiments, which were subsequently corroborated by scanning tunneling microscopy studies. A variety of theoretical frameworks have been proposed to elucidate these experimental observations. However, the precise origin of the pseudogap remains a topic of debate, with potential explanations including pairing correlations, antiferromagnetic spin fluctuations, charge ordering, or a combination of these factors. This ongoing discourse highlights the complexity of the pseudogap state and its critical role in understanding the underlying physics of high-Tc cuprates.",
        "ori-fast-z-score": 0.2,
        "water-fast-z-score": 5.273697108112943,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ray-tracing and physical-optics analysis of the aperture efficiency in a radio telescope .\nAbstract:\nWe present an analytical model for calculating the aperture efficiency of a reflector antenna with circularly polarized feeds, based on ray tracing through the feed-horn optics and physical optics (PO) at the aperture plane. The PO method is used to calculate the electric field distribution over the aperture surface by solving Maxwell s equations using Green functions. We show that this approach can be applied to any type of feed horns, including corrugated conical horns or dual-polarized log-periodic dipole arrays. This work was motivated by our recent study of aperture efficiencies of two different types of antennas operating at 1.4 GHz: a single-dish radio telescope equipped with a corrugated conical feed horn and a multi-element interferometer consisting of eight log-periodic dipole array elements. In both cases we found good agreement between numerical results obtained with our new model and experimental data. \n \n Keywords: Antenna design",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ray - tracing and physical - optics analysis of the aperture efficiency in a radio antenna . Abstract : We present an analytical model for determining the aperture efficiency of a reflector antenna with circularly polarized feeds , based on ray tracing through the feed - horn optics and physical optics ( PO ) at the lens plane .The PO technique is utilized to estimate the electric field flow over the lens surface by solving Maxwell s equations using Green functions . We see that this methodology can be applied to any type of feed horns , particularly corrugated conical horns or dual - polarized log - periodic dipole arrays .This research was motivated by our latest research of aperture efficiencies of two different kinds of antennas acting at 1 . 4 GHz : a single - dish radio telescope fitted with a corrugated conical feed horn and a multi - component interferometer composed of eight log - periodic dipole array modules . In both cases we concluded excellent agreement between mathematical findings obtained with our new model and theoretical data .Keywords: Antenna design",
        "rewrite_text": "Title: Ray-Tracing and Physical-Optics Analysis of Aperture Efficiency in Radio Antennas\n\nAbstract: In this study, we introduce a comprehensive analytical model aimed at evaluating the aperture efficiency of reflector antennas equipped with circularly polarized feeds. Our approach integrates ray tracing techniques through the feed-horn optics alongside physical optics (PO) analysis at the lens plane. By employing the PO method, we effectively estimate the electric field distribution across the lens surface, utilizing Green's functions to solve Maxwell's equations. This innovative methodology is versatile and can be adapted to various types of feed horns, with particular emphasis on corrugated conical horns and dual-polarized log-periodic dipole arrays. The impetus for this research stemmed from our recent investigations into the aperture efficiencies of two distinct antenna configurations operating at a frequency of 1.4 GHz: a single-dish radio telescope equipped with a corrugated conical feed horn and a multi-component interferometer comprising eight log-periodic dipole array modules. Our findings reveal a remarkable correlation between the mathematical predictions derived from our new model and the theoretical data, underscoring the efficacy of our analytical approach. This work contributes valuable insights into antenna design, particularly in optimizing aperture efficiency, and lays the groundwork for future advancements in the field of radio astronomy and antenna engineering. \n\nKeywords: Antenna design, aperture efficiency, ray tracing, physical optics, radio astronomy.",
        "ori-fast-z-score": -1.5096588248481377,
        "water-fast-z-score": 3.7523938719322816,
        "rewrite-fast-z-score": 0.7627700713964739
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Surprising Reversal of Temperatures in the Brown-Dwarf Eclipsing Binary 2MASS J05352184-0546085 .\nAbstract:\nWe report on an unexpected reversal of temperatures between two components of a brown-dwarf eclipsing binary system, which we have discovered using infrared photometry and spectroscopy obtained with Spitzer Space Telescope (Werner et al., 2004) and Gemini Observatory (Gemini North telescope). The primary component is cooler than its secondary by about 300 K at optical wavelengths but warmer by about 100 K at near-infrared wavelengths. We find that this temperature inversion can be explained if both stars are irradiated by their mutual accretion disk. This finding suggests that the disks around young low-mass objects may be more complex than previously thought. \n \n Keywords: Accretion Disk, Inverse P-Cygni profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric variability, Stellar radius, Temperature inversion, Young star \n \n \n \n 1 Introduction \n \n An important goal for understanding how planets form is to determine what happens during the earliest stages of planet formation when protoplanetary disks surround young stellar systems. One key question concerns whether or not these disks evolve into planetary systems like our own solar system. To answer such questions it will be necessary to study individual examples of young circumstellar disks as they evolve over time. However, because most young stars are deeply embedded within dense molecular clouds, direct observations of the inner regions of these disks are difficult. Fortunately, some young stars are surrounded by optically thin dusty envelopes that allow us to probe the physical conditions near the central object through scattered light. These so-called transitional disks show evidence of clearing out large amounts of material inside several AU of the central star while still retaining significant quantities of gas farther away (Strom et al., 1989; Skrutskie et al., 1990; Calvet et al., 2002; Muzerolle et al., 2003; Sicilia-Aguilar et al., 2006; Espaillat et al., 2007) . \n \n A number of studies suggest that the outer edges of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Surprising Reversal of Temperatures in the Brown - Dwarf Eclipsing Binary 2MASS J05352184 - 0546085 . Abstract : We report on an unexpected reversal of temperatures between two components of a brown - dwarf eclipsing binary system , which we have discovered using infrared photometry and spectroscopy obtained with Spitzer Space Telescope ( Werner et al . , 2004 ) and Gemini Observatory ( Gemini North telescope ) .The main component is cooler than its primary by about 300 K at visual wavelengths but warmer by about 100 K at near - infrared wavelengths . We see that this heat inversion can be understood if both stars are irradiated by their mutual accretion disk .This found shows that the disks around old minimum - mass bodies may be more sophisticated than previously thought . Keywords : Accretion Disk , Inverse P - Cygni profile , Irradiation , Low - Mass Star , Near - Infrared Spectroscopy , Photometric variability , Stellar radius , Temperature inversion , Young star 1 Introduction An key goal for knowledge how planets form is to study what comes during the earliest periods of planet development when protoplanetary disks surround young stellar systems .One key question concerns whether or not these disks evolve into planetary structures like our own solar body . To answer such concerns it will be required to study individual examples of young circumstellar disks as they develop over time .However , because most young galaxies are deeply lodged within dense molecular clouds , direct observations of the inner regions of these disks are problematic . Fortunately , some young galaxies are surrounded by optically thin dusty envelopes that enable us to probe the physical conditions near the main object through drifting light .These so - called transitional disks show proof of cutting out large quantities of debris inside several AU of the central star while already retaining substantial quantities of gas farther back ( Strom et al . , 1989 ; Skrutskie et al . , 1990 ; Calvet et al . , 2002 ; Muzerolle et al . , 2003 ; Sicilia - Aguilar et al . , 2006 ; Espaillat et al . , 2007 ) . A variety of studies propose that the exterior corners of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby",
        "rewrite_text": "**Title:** A Surprising Reversal of Temperatures in the Brown-Dwarf Eclipsing Binary 2MASS J05352184-0546085\n\n**Abstract:** In this study, we present an intriguing phenomenon observed in the brown-dwarf eclipsing binary system 2MASS J05352184-0546085, where a reversal of temperatures between its two components has been identified. Utilizing infrared photometry and spectroscopy from the Spitzer Space Telescope and the Gemini North telescope, we discovered that the primary component exhibits a temperature approximately 300 K cooler than its companion at visual wavelengths, yet conversely, it is about 100 K warmer at near-infrared wavelengths. This unexpected temperature inversion can be attributed to the influence of mutual irradiation from an accretion disk surrounding the binary system. Our findings suggest that the accretion disks associated with older, minimum-mass celestial bodies may possess a greater complexity than previously recognized. \n\nThe implications of this research extend to our understanding of the formation and evolution of protoplanetary disks, particularly during the formative stages of planetary systems. A central question in this field is whether these disks can evolve into structures akin to our solar system. To address this, it is essential to investigate individual cases of young circumstellar disks and their developmental trajectories. However, direct observations of these disks are often hindered by their location within dense molecular clouds. Fortunately, some young stellar objects are enveloped in optically thin dusty materials, allowing for the examination of their inner regions through the light that escapes. Transitional disks, in particular, provide evidence of significant debris removal within several astronomical units of the central star while still retaining considerable gas reserves further out. Previous studies have indicated that the outer regions of these transitional disks are shaped by photoevaporative winds, which are driven by intense ultraviolet radiation from nearby stars. This research contributes to the growing body of knowledge regarding the intricate dynamics of accretion disks and their role in the evolution of low-mass stellar systems. \n\n**Keywords:** Accretion Disk, Inverse P-Cygni Profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric Variability, Stellar Radius, Temperature Inversion, Young Star.",
        "ori-fast-z-score": -0.40689422938557973,
        "water-fast-z-score": 6.8132996874920275,
        "rewrite-fast-z-score": 0.9931270663228415
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Structural fingerprinting in the transmission electron microscope: Overview and opportunities to implement enhanced strategies for nanocrystal identification .\nAbstract:\nThe ability to identify individual nanoparticles is essential for understanding their properties, but this task can be challenging due to the high degree of structural similarity between different types of particles. In this review we discuss recent advances in nanoparticle characterization using scanning transmission electron microscopy (STEM)-based techniques that provide information on both chemical composition and structure at atomic resolution. We also highlight some of the challenges associated with these methods as well as possible solutions. Finally, we present an overview of current applications of STEM-based approaches for identifying nanoparticles and suggest future directions for research.  Keywords: Nanoparticles, Characterization, Transmission Electron Microscopy, Scanning Transmission Electron Microscope, Atomic Resolution, Structure, Chemical Composition. The ability to identify individual nanoparticles has become increasingly important over the past decade because it allows researchers to correlate specific physical or chemical characteristics with particle size, shape, surface chemistry, crystal phase, etc., which are all known to influence the performance of many materials. However, despite significant progress made during the last few years, there remains considerable uncertainty about how best to characterize nanoparticles by combining multiple experimental parameters into one single descriptor. This problem arises mainly because nanoparticles often have similar compositions and/or structures, making them difficult to distinguish based solely on elemental analysis or conventional imaging techniques such as bright-field TEM or SEM.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Structural fingerprinting in the transmission ion microscope : Overview and opportunities to execute enhanced methods for nanocrystal identification . Abstract : The capacity to identify individual nanoparticles is crucial for studying their characteristics , but this job can be challenging due to the high degree of structural similarity between various types of molecules .In this review we highlight recent developments in nanoparticle characterization utilizing scanning transmission electron microscopy ( STEM ) - based techniques that provide details on both chemical composition and shape at atomic resolution . We especially note some of the challenges associated with these methods as well as possible solutions .Finally , we present an overview of recent uses of STEM - based methods for finding nanoparticles and suggest current directions for research . Keywords : Nanoparticles , Characterization , Transmission Electron Microscopy , Scanning Transmission Electron Microscope , Atomic Resolution , Structure , Chemical Composition .The capacity to identify individual nanoparticles has become rapidly crucial over the previous decade because it allows researchers to correlate specific biological or chemical traits with particle size , shape , surface chemistry , crystal phase , etc . , which are all known to affect the performance of several substances . However , despite considerable progress made during the last few years , there exists considerable uncertainty about how best to characterize nanoparticles by combining multiple experimental parameters into one single descriptor .This problem arises chiefly because nanoparticles often have related compositions and / or structures , making them harder to distinguish based primarily on elemental examination or conventional optical techniques such as bright - field TEM or SEM .",
        "rewrite_text": "**Title:** Structural Fingerprinting in the Transmission Ion Microscope: Overview and Opportunities for Enhanced Nanocrystal Identification\n\n**Abstract:** The ability to accurately identify individual nanoparticles is essential for advancing our understanding of their properties and behaviors. However, this task presents significant challenges due to the high structural similarity that can exist among different nanoparticle types. In this review, we explore the latest advancements in nanoparticle characterization through scanning transmission electron microscopy (STEM) techniques, which offer atomic-resolution insights into both the chemical composition and morphology of nanoparticles. We discuss the inherent challenges faced by these methodologies, including issues related to resolution, contrast, and the interpretation of complex data. Additionally, we propose potential solutions to these challenges, aiming to enhance the reliability and accuracy of nanoparticle identification. \n\nFurthermore, we provide a comprehensive overview of recent applications of STEM-based techniques in the detection and characterization of nanoparticles, highlighting their effectiveness in various research contexts. We emphasize the importance of correlating specific biological and chemical properties with nanoparticle characteristics such as size, shape, surface chemistry, and crystal phase, all of which significantly influence the performance of materials in diverse applications. Despite the progress made in the field, there remains considerable ambiguity regarding the optimal approach to nanoparticle characterization, particularly in integrating multiple experimental parameters into a unified descriptor. This complexity is largely due to the often similar compositions and structures of nanoparticles, which complicates their differentiation using traditional elemental analysis or standard optical techniques like bright-field transmission electron microscopy (TEM) or scanning electron microscopy (SEM). \n\nIn conclusion, this review not only summarizes the current state of nanoparticle characterization using STEM but also outlines promising future research directions aimed at overcoming existing limitations and enhancing the precision of nanoparticle identification. \n\n**Keywords:** Nanoparticles, Characterization, Transmission Electron Microscopy, Scanning Transmission Electron Microscope, Atomic Resolution, Structure, Chemical Composition.",
        "ori-fast-z-score": 0.5345224838248488,
        "water-fast-z-score": 8.48528137423857,
        "rewrite-fast-z-score": 1.6666666666666667
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Internal states of model isotropic granular packings. III. Elastic properties .\nAbstract:\nWe study the elastic moduli and their fluctuations in dense, disordered packings of frictionless spheres by means of molecular dynamics simulations. We find that both shear and bulk modulus are strongly correlated with the local coordination number z(r) at contact points r between particles. The correlation function C(z1, z2) = ⟨K⟩δ z1 − z2  decays exponentially for large |z1 − z2|, where K denotes either the shear or the bulk modulus. This exponential decay can be explained within an effective medium theory which takes into account correlations among neighboring contacts. In addition to this short-range behavior we observe a power-law tail ∝ (z1 − z2)−2.5 for small values of |z1 − z|. These results suggest that the elastic moduli of disordered packings may be described as random variables whose probability distribution depends on the local packing geometry.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Internal states of model isotropic granular packings . III .Elastic properties . Abstract : We explore the elastic moduli and their fluctuations in dense , disordered packings of frictionless spheres by means of molecular dynamics simulations .We see that both shear and bulk modulus are strongly correlated with the local coordination number z ( r ) at contact points r between particles . The correlation function C ( z1 , z2 ) = ⟨ K ⟩ δ z1 − z2 decays exponentially for large | z1 − z2 | , where K indicates either the shear or the bulk modulus .This exponential decay can be described within an efficient medium theory which gives into consideration correlations among neighboring contacts . In addition to this short - range phenomenon we exhibit a power - law tail [UNK] ( z1 − z2 ) −2 . 5 for little values of | z1 − z | .These data suggest that the elastic moduli of disordered packings might be described as random factors whose likelihood distribution depends on the local packing topology .",
        "rewrite_text": "In this study, we investigate the elastic properties of dense, disordered packings of frictionless spheres through molecular dynamics simulations, focusing on the elastic moduli and their fluctuations. Our findings reveal a strong correlation between both the shear and bulk moduli and the local coordination number \\( z(r) \\) at contact points \\( r \\) between particles. Specifically, we observe that the correlation function \\( C(z_1, z_2) = \\langle K \\rangle \\delta_{z_1 - z_2} \\) exhibits an exponential decay for large values of \\( |z_1 - z_2| \\), where \\( K \\) represents either the shear or bulk modulus. This behavior can be effectively described using an efficient medium theory that accounts for the correlations among neighboring contacts. \n\nMoreover, we identify a power-law tail characterized by \\( [UNK](z_1 - z_2)^{-2.5} \\) for small values of \\( |z_1 - z_2| \\), indicating the presence of long-range correlations in the system. These results imply that the elastic moduli of disordered packings can be understood as random variables whose probability distribution is influenced by the local packing topology. This work contributes to a deeper understanding of the mechanical properties of granular materials, highlighting the intricate relationship between local structural features and macroscopic elastic behavior. Our findings have significant implications for the design and analysis of materials composed of granular assemblies, as they provide insights into how local interactions dictate the overall mechanical response of disordered systems.",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 3.4416668087621747,
        "rewrite-fast-z-score": 2.416841222614159
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chaos and Symmetry in String Cosmology .\nAbstract:\nWe study the dynamics of string cosmologies with nontrivial dilaton potentials, focusing on their chaotic behavior. We show that for certain classes of potentials there are regions where trajectories can be trapped by unstable fixed points or periodic orbits. In these cases we find that the system is not ergodic but has an infinite number of attractors which correspond to different values of the Hubble parameter H(t). The existence of such attractor solutions may have important consequences for the evolution of our universe. For example, it could explain why the present value of H(t) differs so much from its initial value at t = 0. It also provides a possible explanation for the observed flatness problem since the volume V (t) grows exponentially fast during inflation while the energy density decreases as 1/V (t).\nThe results presented here were obtained using numerical methods based on the fourth-order Runge-Kutta algorithm combined with Newton s method for finding roots.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chaos and Symmetry in String Cosmology . Abstract : We research the dynamics of string cosmologies with nontrivial dilaton potentials , concentrating on their chaotic dynamics .We see that for particular categories of potentials there are areas where trajectories can be trapped by weak fixed points or periodic orbits . In these circumstances we find that the system is not ergodic but has an endless number of attractors which belong to different values of the Hubble parameter H ( t ) .The existence of such attractor solutions may have important implications for the evolution of our universe . For instance , it could explain why the present value of H ( t ) changes so much from its initial value at t = 0 .It additionally offers a possible reason for the seen flatness problem since the volume V ( t ) rises exponentially rapidly during inflation while the electricity density decreases as 1 / V ( t ) . The results presented here were obtained using numerical technique based on the fourth - order Runge - Kutta algorithm coupled with Newton s method for finding roots .",
        "rewrite_text": "In this study, we investigate the dynamics of string cosmologies characterized by nontrivial dilaton potentials, with a particular focus on their chaotic behavior. Our analysis reveals that for specific classes of potentials, there exist regions where trajectories can become trapped by weak fixed points or periodic orbits. Under these conditions, we observe that the system does not exhibit ergodicity; instead, it possesses an infinite number of attractors associated with varying values of the Hubble parameter H(t). The presence of such attractor solutions could have significant implications for the evolution of the universe. For example, it may provide insight into the substantial changes observed in the current value of H(t) compared to its initial value at t = 0. Furthermore, this framework offers a potential explanation for the flatness problem, as the volume V(t) increases exponentially during inflation while the energy density diminishes in proportion to 1/V(t). The findings presented in this article were derived using a numerical approach that combines the fourth-order Runge-Kutta algorithm with Newton's method for root-finding. This methodology allows for a comprehensive exploration of the complex dynamics inherent in string cosmologies, shedding light on the underlying mechanisms that govern the universe's expansion and structure.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": -0.20628424925175867
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SN 2006bp: Probing the Shock Breakout of a Type II-P Supernova .\nAbstract:\nWe present optical and near-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007) . The supernova is located at an unusually large distance for its host galaxy, with a recession velocity of ~1000 km/sec. \n \n We find that the light curve can be well fit using a model consisting of three components: shock breakout emission, radioactive decay powered luminosity, and dust extinction. Using this model we derive physical parameters such as the progenitor radius, mass loss rate, and explosion energy. Our results are consistent with those found for other type-II SNe but suggest that the progenitor star had a lower initial mass than previously thought. This may indicate that there exists more diversity among progenitors of type-II SNe than has been realized so far. In addition to these findings, our observations provide new insights into the physics of shock breakout and early-time evolution of type-II SNe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SN 2006bp : Probing the Shock Breakout of a Type II - P Supernova . Abstract : We present visual and far - infrared photometry of SN 2006bp , which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett ( Puckett et al . , 2007 ) .The supernova is situated at an exceptionally wide distance for its host galaxy , with a collapse speeds of ~ 1000 km / sec . We see that the light curve can be well fitting using a simulation comprised of three components : shock breakout emission , radioactive decay powered luminosity , and dust disappearance .Using this model we derive physical factors such as the progenitor diameter , mass loss rate , and explosion power . Our results are compatible with those observed for other class - II SNe but suggest that the progenitor star had a smaller original mass than previously thought .This might suggest that there exists more diversity among progenitors of type - II SNe than has been realized so far . In addition to these conclusions , our observations offer additional perspectives into the physics of wave breakout and first - time progression of type - II SNe .",
        "rewrite_text": "We present a comprehensive analysis of SN 2006bp, a Type II-P supernova discovered on September 24th in the galaxy NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007). This supernova is notable for its unusually large distance from the center of its host galaxy, exhibiting collapse velocities of approximately 1000 km/s. Our study includes detailed visual and far-infrared photometry, which allows us to construct a robust light curve. This light curve is effectively modeled using a combination of three key components: shock breakout emission, luminosity powered by radioactive decay, and the effects of dust disappearance. \n\nThrough this modeling approach, we are able to extract important physical parameters, including the progenitor star's diameter, mass loss rate, and the energy output during the explosion. Our findings align with observations from other Type II supernovae, yet they indicate that the progenitor star of SN 2006bp may have had a lower initial mass than previously assumed. This revelation suggests a greater diversity among the progenitors of Type II supernovae than has been recognized in the past.\n\nMoreover, our observations provide valuable insights into the underlying physics of shock breakout phenomena and the early evolution of Type II supernovae. The implications of our research extend beyond SN 2006bp, contributing to a broader understanding of supernova mechanics and the characteristics of their progenitor stars. This study not only enhances our knowledge of this specific event but also enriches the overall discourse on the diversity and evolution of supernova progenitors in the universe.",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 5.079850199442941,
        "rewrite-fast-z-score": 0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Water vapor and silicon monoxide maser observations in the protoplanetary nebula OH 231.8+4 .\nAbstract:\nWe report water vapor (H2O) and silicon monoxide (SiO) maser emission toward the central star of the protoplanetary  nebula OH231.8+4.2, which is associated with an infrared source IRAS 18286-1231. The H2O masers are distributed over a region of ~0.1 arcsec diameter around the star at a velocity range of -40 to +20 km s-1 relative to the systemic velocity of the nebula. We detected SiO masers only on one side of the star within 0.05 arcsec radius at velocities ranging between -50 and -30 km s-1. These results suggest that the H2O masers trace shocked gas near the stellar surface while the SiO masers arise from outflowing material along the polar axis.  This work was supported by Grants-in-Aid for Scientific Research (No. 15740160)  from MEXT Japan.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Water vapor and silicon monoxide maser observations in the protoplanetary nebula OH 231 . 8 + 4 . Abstract : We report water vapor ( H2O ) and silicon monoxide ( SiO ) maser emission toward the main star of the protoplanetary nebula OH231 . 8 + 4 . 2 , which is associated with an infrared source IRAS 18286 - 1231 .The H2O masers are distributed over a region of ~ 0 . 1 arcsec diameter around the star at a velocity range of - 40 to + 20 km s - 1 relative to the systemic speed of the nebula . We observed SiO masers only on one side of the star within 0 . 05 arcsec radius at velocities ranging between - 50 and - 30 kilometers s - 1 .These data suggest that the H2O masers trace shocked gas near the stellar surface while the SiO masers occur from outflowing matter along the polar axis . This research was supported by Grants - in - Aid for Scientific Research ( No .15740160)  from MEXT Japan.",
        "rewrite_text": "We present our findings on the maser emissions of water vapor (H2O) and silicon monoxide (SiO) in the protoplanetary nebula OH 231.8 + 4.2, which is linked to the infrared source IRAS 18286-1231. Our observations reveal that the H2O masers are concentrated within a region approximately 0.1 arcseconds in diameter surrounding the central star, exhibiting a velocity range from -40 to +20 km/s relative to the nebula's systemic velocity. In contrast, SiO masers were detected exclusively on one side of the star, confined to a radius of about 0.05 arcseconds, with velocities between -50 and -30 km/s. The spatial distribution and velocity characteristics of these masers indicate that the H2O masers are likely tracing shocked gas in proximity to the stellar surface, while the SiO masers are associated with outflowing material along the polar axis of the nebula. These observations contribute to our understanding of the dynamics and physical conditions in the vicinity of the central star in this protoplanetary nebula. This research was made possible through the support of Grants-in-Aid for Scientific Research (No. 15740160) from MEXT, Japan.",
        "ori-fast-z-score": 0.282842712474619,
        "water-fast-z-score": 2.9405881764588204,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The RMS Survey: 13CO observations of candidate massive YSOs in the southern hemisphere .\nAbstract:\nWe present results on the first phase of an unbiased survey for high-mass protostars (HMPSs) using the Red MSX Source (RMS) database and the NRAO 12m telescope at Kitt Peak Observatory, Arizona. The sample consists of all sources with infrared excesses that are associated with radio emission within the Galactic latitude range |b| < 5 degrees. We observed these candidates in the J=1-0 transition line of carbon monoxide (13CO), which is optically thin even towards dense cores. Our goal was to identify HMPSs by searching for compact molecular outflows traced by high-velocity wings in their 13CO spectra. In total we detected 16 HMPS candidates among our sample of 61 targets. These objects have luminosities between 10^6-10^7 Lsun and masses ranging from 8 Msun up to >100 Msun. They show evidence for bipolar outflow activity as well as infall motions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The RMS Survey : 13CO observations of candidate huge YSOs in the southern hemisphere . Abstract : We report findings on the first phase of an unbiased survey for high - mass protostars ( HMPSs ) using the Red MSX Source ( RMS ) database and the NRAO 12m telescope at Kitt Peak Observatory , Arizona .The sample consists of all sources with infrared excesses that are related with radio emission within the Galactic latitude range | b | < 5 degrees . We observed these candidates in the J = 1 - 0 transition line of carbon monoxide ( 13CO ) , which is optically thin even towards thin cores .Our goal was to identify HMPSs by searching for compact molecular outflows traced by high - frequency wings in their 13CO spectra . In total we identified 16 HMPS candidates among our sample of 61 sites .These bodies have luminosities between 10 ^ 6 - 10 ^ 7 Lsun and masses ranging from 8 Msun up to > 100 Msun . They show data for bipolar outflow movement as well as infall motions .",
        "rewrite_text": "In this study, we present the results from the initial phase of an extensive, unbiased survey aimed at identifying high-mass protostars (HMPSs) utilizing the Red MSX Source (RMS) database in conjunction with observations from the NRAO 12m telescope located at Kitt Peak Observatory, Arizona. Our research focuses on a sample of sources exhibiting infrared excesses that are associated with radio emissions, specifically within the Galactic latitude range of |b| < 5 degrees. We conducted observations of these candidate sources in the J = 1 - 0 transition line of carbon monoxide (13CO), a molecule known for its optically thin characteristics, even in regions with minimal core density. The primary objective of our survey was to pinpoint HMPSs by detecting compact molecular outflows, which are indicated by the presence of high-frequency wings in the 13CO spectral data. From our analysis, we successfully identified 16 candidates for HMPSs within our sample of 61 observed sites. These candidates exhibit luminosities ranging from 10^6 to 10^7 Lsun and possess masses that span from 8 Msun to over 100 Msun. Notably, our findings reveal evidence of bipolar outflow dynamics as well as infall motions in these high-mass protostars, contributing valuable insights into the formation and evolution of massive stellar objects in the southern hemisphere. This research not only enhances our understanding of HMPSs but also lays the groundwork for future studies aimed at exploring the complexities of star formation in our galaxy.",
        "ori-fast-z-score": 1.25,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 0.8955334711889903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Years of RXTE Monitoring of Anomalous X-ray Pulsar 4U 0142+61: Long-Term Variability .\nAbstract:\nWe present the results of long-term monitoring observations with the Rossi X-Ray Timing Explorer (RXTE) Proportional Counter Array and All-Sky Monitor for anomalous X-ray pulsar 4U0142+61, which is one of the most active accreting neutron stars in our Galaxy. The source was observed during more than 10 years between 1996 February and 2007 December. We found that the pulse period shows significant changes on time scales ranging from days to months. In addition, we detected several large-amplitude outbursts lasting up to 100 days each. These outbursts are characterized by an increase in both intensity and hardness ratio. During these outbursts, the spin-up rate increased significantly compared to those before or after them. This indicates that the magnetic field strength at the surface of the star increases when it undergoes such outbursts. \n \n Keywords: Accretion disk - Neutron star - Outburst",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Years of RXTE Monitoring of Anomalous X - ray Pulsar 4U 0142 + 61 : Long - Term Variability . Abstract : We present the results of long - term tracking observations with the Rossi X - Ray Timing Explorer ( RXTE ) Proportional Counter Array and All - Sky Monitor for anomalous X - ray pulsar 4U0142 + 61 , which is one of the most important accreting neutron galaxies in our Galaxy .The source was seen during more than 10 years between 1996 February and 2007 December . We showed that the pulse time displays substantial shifts on timing scales varied from weeks to months .In addition , we spotted many wide - frequency outbursts lasting up to 100 days each . These outbursts are marked by an increase in both intensity and hardness factor .During these outbursts , the spin - up rate increased significantly compared to those before or after them . This implies that the magnetic force power at the surface of the star increases when it undergoes such outbursts .Keywords : Accretion disk - Neutron star - Outburst",
        "rewrite_text": "We present a comprehensive analysis of the long-term monitoring of the anomalous X-ray pulsar 4U 0142+61, utilizing data collected from the Rossi X-Ray Timing Explorer (RXTE) Proportional Counter Array and All-Sky Monitor over a span of more than a decade, from February 1996 to December 2007. This pulsar is recognized as one of the most significant accreting neutron stars within our galaxy. Our observations reveal that the timing of the pulsar exhibits considerable variations, with shifts occurring on timescales ranging from weeks to months. Furthermore, we identified numerous extended outbursts across a wide frequency range, each lasting up to 100 days. These outbursts are characterized by notable increases in both intensity and hardness factor, indicating a substantial change in the pulsar's emission properties. During these periods of heightened activity, we observed a marked increase in the spin-up rate of the pulsar, suggesting that the magnetic field strength at the star's surface intensifies during such outbursts. This finding has significant implications for our understanding of the mechanisms driving the behavior of anomalous X-ray pulsars and their accretion processes. Our study contributes valuable insights into the long-term variability of 4U 0142+61, enhancing our knowledge of neutron star dynamics and the complex interactions within their accretion disks. The keywords associated with this research include accretion disk, neutron star, and outburst, which encapsulate the core themes of our investigation.",
        "ori-fast-z-score": -2.32379000772445,
        "water-fast-z-score": 3.2009219983223995,
        "rewrite-fast-z-score": -1.4439897447623107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electro-optically tunable microring resonators in lithium niobate .\nAbstract:\nWe report on the fabrication and characterization of electro-optically tunable microresonator devices based on proton exchange (PE) grown lithium niobate-on-insulator substrates. The PE process is used to create an optical waveguide with low loss, high index contrast, and large nonlinearity within the substrate material. A ring-resonator geometry is then defined by electron-beam lithography followed by reactive ion etching. Finally, Ti/Au electrodes are deposited onto both sides of the device for electrical tuning. We demonstrate continuous wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device. This work represents one step towards realizing electrically-tuned integrated photonic circuits that can be monolithically fabricated on insulators. Lithium niobate has been widely studied as a promising candidate for optoelectronics applications due to its excellent properties such as wide transparency range, large second-order susceptibility, and relatively low propagation losses  1  . In addition, it also exhibits strong piezoelectric and pyroelectric effects which make it possible to achieve efficient electro-optic modulation  2  .\nIn this letter we present our recent results on the development of electro-optically tuned microring resonators made out of lithium niobate. These devices were designed and fabricated on commercially available lithium niobate wafers bonded to silicon dioxide  3  , where the top cladding layer was removed prior to processing. First, a proton-exchange (PE) process  4  was performed to grow a single-mode ridge-waveguide structure inside the bulk LiNbO 3 crystal  5  . Then, a ring-resonator geometry was patterned into the PE-grown region via electron beam lithography  6  . Finally, titanium/gold (Ti/Au) contacts were evaporated onto both sides of the sample to provide electrical access to the device  7, 8  . Figure 1 shows scanning-electron-microscope images of two different types of microring resonators that have been successfully demonstrated so far. Both devices consist of",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electro - optically tunable microring resonators in lithium niobate . Abstract : We report on the fabrication and identification of electro - optically tunable microresonator devices using on proton exchange ( PE ) grown lithium niobate - on - insulator substrates .The PE method is utilized to create an optical waveguide with little loss , large index contrast , and large nonlinearity within the substrate material . A ring - resonator configuration is then established by electron - laser lithography followed by reactive ion etching .Finally , Ti / Au electrodes are deposited onto both sides of the device for electrical tuning . We display continuous wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device .This research provides one step towards developing electrically - tuned integrated photonic circuits that can be monolithically manufactured on insulators . Lithium niobate has been widely explored as a potential candidate for optoelectronics applications due to its exceptional properties such as wide clarity range , large second - order susceptibility , and fairly little propagation losses 1 .In addition , it also exhibits strong piezoelectric and pyroelectric influences which make it able to achieve effective electro - optic modulation 2 . In this letter we present our latest findings on the development of electro - optically tuned microring resonators made out of lithium niobate .These systems were built and manufactured on commercially used lithium niobate wafers bonded to silicon dioxide 3 , where the front cladding surface was eliminated prior to processing . First , a proton - transfer ( PE ) process 4 was done to develop a single - mode ridge - waveguide structure inside the bulk LiNbO 3 crystal 5 .Then , a ring - resonator configuration was patterned into the PE - grown areas via electron beam lithography 6 . Finally , titanium / gold ( Ti / Au ) contacts were evaporated onto both sides of the sample to provide electrical access to the device 7 , 8 .Figure 1 shows scan - electron - microscope photographs of two different kinds of microring resonators that have been successfully shown so far . Both technologies consist of",
        "rewrite_text": "We present a comprehensive study on the fabrication and characterization of electro-optically tunable microring resonators utilizing proton exchange (PE) processed lithium niobate-on-insulator substrates. The PE technique is employed to develop optical waveguides characterized by minimal loss, significant index contrast, and enhanced nonlinearity within the lithium niobate material. Following this, we establish a ring-resonator configuration through a combination of electron-beam lithography and reactive ion etching. To facilitate electrical tuning, titanium/gold (Ti/Au) electrodes are deposited on both sides of the device. Our results demonstrate continuous wavelength tuning exceeding 30 nm around the 1555 nm wavelength, achieved with a mere 1 V reverse bias applied across the device. This advancement represents a significant step toward the realization of electrically-tuned integrated photonic circuits that can be monolithically fabricated on insulator substrates.\n\nLithium niobate is a highly regarded material in the field of optoelectronics due to its remarkable properties, including a broad transparency range, substantial second-order susceptibility, and relatively low propagation losses. Additionally, its strong piezoelectric and pyroelectric characteristics enable effective electro-optic modulation. In this letter, we detail our recent findings on the development of these electro-optically tunable microring resonators constructed from lithium niobate. The devices were fabricated on commercially available lithium niobate wafers bonded to silicon dioxide, with the front cladding surface removed prior to processing. Initially, a proton-exchange process was performed to create a single-mode ridge waveguide structure within the bulk LiNbO3 crystal. Subsequently, a ring-resonator design was patterned onto the PE-processed regions using electron beam lithography. Finally, Ti/Au contacts were evaporated onto both sides of the sample to enable electrical access to the device. Figure 1 illustrates scanning electron microscope images of two distinct types of microring resonators that have been successfully developed to date. Both fabrication techniques demonstrate promising results for future applications in integrated photonics.",
        "ori-fast-z-score": 0.9198662110077999,
        "water-fast-z-score": 7.726752403351793,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Flavour-Dependent Type II Leptogenesis .\nAbstract:\nWe study the possibility that leptons acquire their masses through an effective dimension-5 operator, which is suppressed by some heavy scale M . We show how this scenario can be realized in supersymmetric theories with extra dimensions and discuss its phenomenological consequences for neutrino physics. \n \n In particular we consider the case where the lepton number violating interactions are localized on a brane at y = 0 while the Standard Model fields live in the bulk. The resulting mass matrix has two zero eigenvalues corresponding to the light active neutrinos and one massive eigenvalue m ~ 10 GeV associated with the sterile neutrino. This leads to interesting predictions for neutrinoless double beta decay as well as for cosmology. For example, if the Majorana phase vanishes then there will be no contribution to neutrinoless double-beta decay but the sterile neutrino may still contribute significantly to dark matter production during inflation or reheating.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Flavour - Dependent Type II Leptogenesis . Abstract : We research the prospect that leptons gain their masses through an efficient dimension - 5 operator , which is suppressed by some heavy scale M .We see how this situation can be realized in supersymmetric theories with extra dimensions and consider its phenomenological consequences for neutrino physics . In particular we study the case where the lepton number violating interactions are localized on a brane at y = 0 while the Standard Model fields reside in the bulk .The resulting mass vector has two zero eigenvalues corresponding to the light active neutrinos and one massive eigenvalue m ~ 10 GeV associated with the sterile neutrino . This leads to useful predictions for neutrinoless double alpha decay as well as for cosmology .For instance , if the Majorana phase vanishes then there will be no contribution to neutrinoless double - beta decay but the sterile neutrino may still affect significantly to dark matter production during inflation or reheating .",
        "rewrite_text": "In this study, we explore the possibility that leptons acquire their masses through an efficient dimension-5 operator, which is suppressed by a high mass scale, denoted as M. Our investigation focuses on how this mechanism can be implemented within the framework of supersymmetric theories that incorporate extra dimensions. We delve into the phenomenological implications of this scenario for neutrino physics, particularly examining the case where lepton number-violating interactions are confined to a brane located at y = 0, while the Standard Model fields are distributed throughout the bulk. \n\nThe analysis reveals that the resulting mass matrix possesses two zero eigenvalues, which correspond to the light active neutrinos, and one massive eigenvalue, approximately m ~ 10 GeV, associated with a sterile neutrino. This configuration yields significant predictions regarding neutrinoless double-beta decay and has important ramifications for cosmological models. Notably, if the Majorana phase is set to zero, the contribution to neutrinoless double-beta decay is nullified; however, the presence of the sterile neutrino could still play a crucial role in the dynamics of dark matter production during the epochs of inflation or reheating. \n\nOur findings highlight the intricate relationship between lepton mass generation, neutrino properties, and cosmological phenomena, suggesting that the interplay of these elements could provide deeper insights into the fundamental nature of particles and the evolution of the universe. This work opens avenues for further research into the implications of flavor-dependent leptogenesis and its potential signatures in both laboratory experiments and astrophysical observations.",
        "ori-fast-z-score": -0.7385489458759964,
        "water-fast-z-score": 3.298574997620241,
        "rewrite-fast-z-score": 1.2567574357593625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HS1857+5144: A hot and young pre-cataclysmic variable .\nAbstract:\nWe report the discovery of an eclipsing binary system HS1857+5144, which is composed of two white dwarfs with masses 0.6 M⊙and 0.8M⊙respectively. The orbital period of this system is 1.5 hours. We find that the mass ratio q = M2/M1=0.84 ±0.04 for this system by using the method of spectral disentangling. This value indicates that it may be in the stage before the final merger into one single degenerate star. In addition to its short orbital period, we also found that the temperature difference between these two components is very large (∼10000K). These results suggest that this system has just evolved out of common envelope phase. \n \n Keywords: Hot subdwarf B stars; Eclipsing; Pre-Cataclysmic Variables; White Dwarfs; Spectral Disentangling",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HS1857 + 5144 : A hot and young pre - cataclysmic variable . Abstract : We report the discovery of an eclipsing binary system HS1857 + 5144 , which is composed of two white dwarfs with masses 0 . 6 [UNK] 0 . [UNK] .The orbital period of this system is 1 . 5 hours . We see that the mass ratio q = M2 / M1 = 0 . 84 ±0 . 04 for this system by using the method of spectral disentangling .This value indicates that it could be in the stage before the last merger into one single degenerate star . In addition to its short orbital period , we also discovered that the temperature difference between these two systems is very huge ( [UNK] ) .These data suggest that this scheme has just evolved out of common envelope phase . Keywords : Hot subdwarf B stars ; Eclipsing ; Pre - Cataclysmic Variables ; White Dwarfs ; Spectral Disentangling",
        "rewrite_text": "We present the discovery of the eclipsing binary system HS1857 + 5144, which consists of two white dwarfs with masses of approximately 0.6 solar masses each. The system exhibits a notably short orbital period of 1.5 hours, indicative of a close binary configuration. Through the application of spectral disentangling techniques, we determined the mass ratio of the two white dwarfs to be q = M2 / M1 = 0.84 ± 0.04. This mass ratio suggests that the system may be in a pre-merger phase, potentially evolving towards the formation of a single degenerate star. Furthermore, our observations reveal a significant temperature disparity between the two components of the binary system, which supports the hypothesis that HS1857 + 5144 has recently emerged from a common envelope phase. The findings contribute to the understanding of hot subdwarf B stars and the evolutionary pathways of pre-cataclysmic variable systems. The implications of this discovery are significant for the study of binary evolution and the eventual fate of such systems, as they may provide insights into the mechanisms leading to cataclysmic variables and type Ia supernovae. Our research highlights the importance of continued observations and analysis of such systems to unravel the complexities of stellar evolution in close binary configurations. Keywords: Hot subdwarf B stars; Eclipsing binaries; Pre-Cataclysmic Variables; White Dwarfs; Spectral Disentangling.",
        "ori-fast-z-score": 0.39056673294247163,
        "water-fast-z-score": 2.994344952558949,
        "rewrite-fast-z-score": 0.10259783520851541
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiwavelength study of massive galaxies at z~2. II. Widespread Compton thick AGN and the concurrent growth of black holes and bulges .\nAbstract:\nWe present new results on the properties of X-ray selected, optically bright (MAB <-21) galaxies in the CDF-S field with spectroscopic redshifts between 1<z<3.5. We use deep Chandra data to identify sources that are obscured by column densities NH>1022 cm-2 as well as those which have intrinsic luminosities above 1043 erg s-1 Hz-1. The majority of these objects show evidence for being heavily absorbed active galactic nuclei (AGNs). Using Spitzer mid-infrared spectroscopy we find that most of our sample is dominated by star formation activity rather than an old stellar population. In addition, we detect strong Fe Kα emission lines in many of the obscured AGNs indicating high accretion rates onto supermassive black holes. By combining optical and infrared photometry with SED modeling techniques we derive accurate estimates of the total stellar mass content within each galaxy. Our analysis shows that there exists a tight correlation between the masses of central supermassive black holes and their host spheroids over more than four orders of magnitude in BH mass.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiwavelength study of large galaxies at z ~ 2 . II .Widespread Compton thick AGN and the concurrent development of red holes and bulges . Abstract : We report new data on the properties of X - ray selected , optically bright ( MAB < - 21 ) clusters in the CDF - S field with spectroscopic redshifts between 1 < z < 3 . 5 .We use deep Chandra data to identify sources that are obscured by column densities NH > 1022 centimetres - 2 as well as those which have intrinsic luminosities above 1043 erg s - 1 Hz - 1 . The majority of these objects show proof for being extensively absorbed active galactic nuclei ( AGNs ) .Using Spitzer mid - infrared spectroscopy we find that most of our sample is dominated by galaxy formation activity rather than an old stellar community . In addition , we find strong Fe Kα emission lines in many of the obscured AGNs suggesting high accretion levels onto supermassive black holes .By combining optical and infrared photometry with SED modeling strategies we derive exact predictions of the total stellar mass content within each galaxy . Our study shows that there exists a tight correlation between the masses of central supermassive black holes and their host spheroids over more than four orders of magnitude in BH weight .",
        "rewrite_text": "We present new findings on the characteristics of X-ray selected, optically luminous clusters (MAB < -21) located in the Chandra Deep Field South (CDF-S), with spectroscopic redshifts ranging from 1 to 3.5. Utilizing deep Chandra observations, we identify sources obscured by column densities exceeding NH > 10^22 cm^-2, alongside those exhibiting intrinsic luminosities greater than 10^43 erg s^-1 Hz^-1. A significant portion of these sources is confirmed to be heavily obscured active galactic nuclei (AGNs). Through mid-infrared spectroscopy conducted with Spitzer, we discover that the majority of our sample is primarily influenced by ongoing galaxy formation processes rather than being dominated by older stellar populations. Furthermore, we observe pronounced Fe Kα emission lines in many of the obscured AGNs, indicating substantial accretion onto supermassive black holes. By integrating optical and infrared photometry with spectral energy distribution (SED) modeling techniques, we accurately estimate the total stellar mass within each galaxy. Our analysis reveals a robust correlation between the masses of central supermassive black holes and their corresponding host spheroids, spanning over four orders of magnitude in black hole mass. This study enhances our understanding of the interplay between AGN activity and galaxy evolution during the peak era of star formation, highlighting the significance of Compton-thick AGNs in shaping the structural properties of large galaxies at redshifts around z ~ 2.",
        "ori-fast-z-score": 1.0425720702853738,
        "water-fast-z-score": 5.569674729797484,
        "rewrite-fast-z-score": 0.3922322702763681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Coherent Capacity and Reliability of Sparse Multipath Channels in the Wideband Regime .\nAbstract:\nWe consider sparse multipath channels with random delays, where each path is modeled as an independent Rayleigh fading channel. We derive exact expressions for ergodic capacity and outage probability over such channels under wideband regime (i.e., when bandwidth grows without bound). Our results show that both ergodic capacity and outages are determined by the number of paths N , their average power Pn = E{|hn(t)|2}, and delay spread T . In particular, we find that ergodic capacity scales linearly with bandwidth W at high SNR if N > 2W/πT or N < πT /4W; otherwise it saturates to a constant value. Moreover, our analysis shows that outage probability decays exponentially fast with increasing bandwidth W if N > 4W/πT ; otherwise it decreases only polynomially. Finally, numerical examples illustrate how these scaling laws depend on various system parameters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Coherent Capacity and Reliability of Sparse Multipath Channels in the Wideband Regime . Abstract : We consider sparse multipath channels with random delays , where each track is modeled as an free Rayleigh fading stream .We derive exact expressions for ergodic strength and outage likelihood over such channels under wideband regime ( i . e . , when bandwidth grows without bound ) . Our results show that both ergodic strength and outages are decided by the number of paths N , their average power Pn = E { | hn ( t ) | 2 } , and delay spread T .In particular , we find that ergodic performance scales linearly with capacity W at high SNR if N > 2W / πT or N < πT / 4W ; otherwise it saturates to a constant value . Moreover , our analysis shows that outage likelihood decays exponentially rapidly with expanding bandwidth W if N > 4W / πT ; otherwise it grows only polynomially .Finally , numerical examples explain how these scaling laws depend on various system parameters .",
        "rewrite_text": "Title: Non-Coherent Capacity and Reliability of Sparse Multipath Channels in the Wideband Regime\n\nAbstract: This study investigates the characteristics of sparse multipath channels characterized by random delays, where each path is represented as an independent Rayleigh fading stream. We present precise formulations for both ergodic capacity and outage probability in the context of wideband communication, specifically when the bandwidth approaches infinity. Our findings reveal that the performance metrics of ergodic capacity and outage probability are fundamentally influenced by three key factors: the number of paths (N), the average power of each path (Pn = E{|hn(t)|²}), and the delay spread (T). Notably, we establish that the ergodic capacity exhibits a linear relationship with the bandwidth (W) at high signal-to-noise ratios (SNR) under certain conditions—specifically when N exceeds 2W/πT or falls below πT/4W; in contrast, when these conditions are not met, the ergodic capacity reaches a saturation point. Furthermore, our analysis indicates that the likelihood of outage diminishes exponentially as the bandwidth increases, provided that N exceeds 4W/πT; otherwise, the growth of outage probability is only polynomial. To illustrate these scaling behaviors, we provide numerical examples that highlight the dependence of these laws on various system parameters. This research contributes to a deeper understanding of the interplay between channel characteristics and performance in wideband communication systems, offering insights that could inform the design of more robust and efficient communication strategies in environments characterized by sparse multipath propagation.",
        "ori-fast-z-score": -0.25,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": 0.19802950859533489
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tilt-angle landscapes and temperature dependence of the conductance in biphenyl-dithiol single-molecule junctions .\nAbstract:\nWe report on electrical transport measurements through individual biphenyl dithiol molecules connected to gold electrodes by using scanning tunneling microscopy break-junction technique at room-temperature (RT) and low temperatures down to 4 K. The current–voltage characteristics show clear signatures for molecular conduction, such as Coulomb blockade peaks and negative differential resistance regions. We find that the junction conductance depends strongly on the tilt angle between the molecule backbone and the substrate normal. This is explained by an anisotropic coupling strength between the molecule and the metal contacts which leads to different transmission probabilities along the two main axes of the molecule. In addition we observe a strong temperature dependence of the junction conductance with a maximum around 50 K. These results are discussed within a model based on electron-phonon scattering processes. Biphenyl dithiol (BDT), one of the most studied organic semiconductors, has been used extensively in field-effect transistors1-5 and other electronic devices6-8 due to its high charge carrier mobility9-11. However, despite intensive research efforts12-17 only few studies have investigated the electrical properties of BDT-based single-molecule junctures18-20. Herein, we present detailed investigations of the electrical transport behavior of individual BDT molecules contacted via Au(111) surfaces by means of scanning tunneling microscopy breakjunction experiments21-23. Our data reveal pronounced features characteristic for molecular conduction24-26 including Coulomb blockade peaks27-29 and negative differential resistance30-32. Furthermore, our study reveals a strong influence of the molecular orientation relative to the substrate surface on the junction conductance33-35. Finally, we demonstrate a significant temperature dependent change of the junction conductance36-38.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tilt - angle landscapes and heat dependence of the conductance in biphenyl - dithiol single - molecule junctions . Abstract : We report on electrical transport measurements through individual biphenyl dithiol molecules linked to gold electrodes by using scan tunneling microscopy break - junction technique at room - temperature ( RT ) and low temperatures down to 4 K . The current – voltage parameters show visible signatures for molecular conduction , such as Coulomb blockade spikes and negative integral resistance regions .We see that the junction conductance depends strongly on the tilt distance between the molecule backbone and the substrate normal . This is explained by an anisotropic interaction strength between the molecule and the metal links which results to different communication probabilities along the two principal axes of the molecule .In addition we study a weak thermal dependence of the junction conductance with a maximum around 50 K . These data are discussed within a theory based on electron - phonon absorption processes . Biphenyl dithiol ( BDT ) , one of the most studied organic semiconductors , has been used heavily in field - effect transistors1 - 5 and other electronic devices6 - 8 due to its high charge carrier mobility9 - 11 .However , despite intensive study efforts12 - 17 only few researchers have researched the electrical properties of BDT - based single - cell junctures18 - 20 . Herein , we present detailed investigations of the electrical travel dynamics of individual BDT atoms contacted via Au ( 111 ) surfaces by means of scanning tunneling microscopy breakjunction experiments21 - 23 .Our data reveal pronounced features characteristic for molecular conduction24 - 26 including Coulomb blockade peaks27 - 29 and negative differential resistance30 - 32 . Furthermore , our research reveals a large influence of the molecular orientation relative to the substrate surface on the junction conductance33 - 35 .Finally , we obtain a substantial heat dependent change of the junction conductance36 - 38 .",
        "rewrite_text": "We present a comprehensive study on the electrical transport properties of individual biphenyl dithiol (BDT) molecules connected to gold electrodes, utilizing the scanning tunneling microscopy break-junction technique. Our experiments were conducted at both room temperature and at low temperatures, reaching down to 4 K. The current-voltage characteristics exhibit distinct features indicative of molecular conduction, including Coulomb blockade spikes and regions of negative differential resistance. Notably, we observe that the conductance of the junction is significantly influenced by the tilt angle between the molecular backbone and the normal of the substrate. This phenomenon can be attributed to the anisotropic interaction strength between the BDT molecules and the metal contacts, which leads to varying communication probabilities along the molecule's two principal axes. \n\nAdditionally, we investigate the thermal dependence of the junction conductance, identifying a peak around 50 K, which suggests a weak thermal effect on the conduction properties. This behavior is analyzed within the framework of a theoretical model that incorporates electron-phonon absorption processes. BDT, recognized as one of the most extensively studied organic semiconductors, has found applications in field-effect transistors and various electronic devices due to its high charge carrier mobility. Despite the extensive research on BDT, the electrical characteristics of BDT-based single-molecule junctions have not been thoroughly explored. Our work fills this gap by providing detailed insights into the electrical transport dynamics of individual BDT molecules contacted via Au (111) surfaces. The pronounced molecular conduction features we observe, along with the significant impact of molecular orientation and temperature on junction conductance, contribute to a deeper understanding of the electronic properties of BDT and its potential applications in nanoscale electronic devices.",
        "ori-fast-z-score": 1.0215078369104984,
        "water-fast-z-score": 6.821430560638804,
        "rewrite-fast-z-score": -0.17277368511627203
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neglecting the porosity of hot-star winds can lead to underestimating mass-loss rates .\nAbstract:\nWe present new calculations for the radiative transfer in stellar winds, including the effects of line-blanketing and clumping on the wind structure. We find that neglecting the porosity of the wind leads to an overestimate of the mass loss rate by up to two orders of magnitude compared with our results. This is because the density contrast between the dense clumps and the interclump medium increases as one moves out into the wind, leading to more absorption at large distances than would be expected if the wind were smooth. The effect becomes stronger when the wind velocity decreases or the metallicity increases. \n \n Keywords: Stellar winds, Radiation hydrodynamics, Line-driven winds, Porosity, Clumping \n \n Hot stars lose mass through their stellar winds driven by radiation pressure on spectral lines (Castor et al., 1975; Abbott & Lucy, 1985) . In order to understand how these winds are structured it is necessary to solve the coupled equations of radiative transfer and fluid dynamics simultaneously. However, this problem has proved extremely difficult to solve numerically due to its multi-scale nature - both spatially and temporally - which requires very high resolution grids to resolve all relevant scales correctly. As such, most previous studies have used simplified treatments of either the radiative transfer or the fluid dynamics, but not both together. For example, some authors assume that the wind consists entirely of optically thin gas (e.g. Friend & Castor, 1983) while others use simple prescriptions for the radial dependence of the optical depth (e.g. Pauldrach et al., 1986) , or even ignore the effects of opacity altogether (e.g. Lamers & Cassinelli, 1999 ) . Other authors make simplifying assumptions about the flow itself, e.g. assuming spherical symmetry (e.g. Puls et al., 1996 ) , steady state (e.g. Owocki et al. , 1988 ) and/or stationarity (e.g. Runacres & Owocki , 2002 ) . \n \n Here we present new calculations for the structure of line-driven winds",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neglecting the porosity of bright - star winds can lead to underestimating mass - loss odds . Abstract : We present new models for the radiative transfer in stellar winds , particularly the effects of line - blanketing and clumping on the weather structure .We see that neglecting the porosity of the wind leads to an overestimate of the mass loss rate by up to two orders of magnitude compared with our findings . This is because the density contrast between the sparse clumps and the interclump medium tends as one moves out into the wind , leading to more scattering at large distances than would be anticipated if the wind were smooth .The phenomenon grows stronger when the wind velocity drops or the metallicity increases . Keywords : Stellar storms , Radiation hydrodynamics , Line - driven winds , Porosity , Clumping Hot objects losing mass through their stellar winds driven by radiation stress on spectral lines ( Castor et al . , 1975 ; Abbott & Lucy , 1985 ) .In order to explain how these winds are structured it is required to solve the coupled equations of radiative transfer and fluid dynamics simultaneously . However , this question has become highly hard to solve numerically due to its multi - scale nature - both spatially and temporally - which requires very high resolution grids to resolution all relevant scales correctly .As such , most prior studies have utilized simplified treatments of either the radiative transfer or the liquid mechanics , but not both together . For instance , some writers suppose that the wind consists entirely of optically thin gas ( e . g .Friend & Castor , 1983 ) while many use simple prescriptions for the radial dependence of the optical depth ( e . g . Pauldrach et al . , 1986 ) , or actually avoid the effects of opacity altogether ( e . g .Lamers & Cassinelli , 1999 ) . Other texts provide simplifying observations about the flow itself , e . g .assuming spherical symmetry ( e . g . Puls et al . , 1996 ) , steady state ( e . g .Owocki et al . , 1988 ) and / or stationarity ( e . g .Runacres & Owocki , 2002 ) . Here we present new calculations for the composition of line - propelled winds",
        "rewrite_text": "Title: The Impact of Ignoring Porosity in Bright-Star Winds on Mass Loss Estimates\n\nAbstract: In this study, we introduce innovative models for understanding radiative transfer in stellar winds, focusing specifically on the implications of line-blanketing and clumping on the wind's structural dynamics. Our findings reveal that overlooking the porosity of stellar winds can result in mass loss rate estimates that are excessively high, potentially by as much as two orders of magnitude compared to our results. This discrepancy arises from the significant density contrast between the sparse clumps and the interclump medium, which intensifies as one moves outward in the wind. Consequently, this leads to increased scattering at greater distances than would be expected in a uniformly smooth wind. The effect becomes more pronounced under conditions of reduced wind velocity or increased metallicity. \n\nThe study of stellar winds, particularly those driven by radiation pressure on spectral lines, has been a focal point in astrophysics since the foundational works of Castor et al. (1975) and Abbott & Lucy (1985). To accurately characterize the structure of these winds, it is essential to simultaneously solve the coupled equations governing radiative transfer and fluid dynamics. However, the complexity of this problem, characterized by its multi-scale nature in both spatial and temporal dimensions, poses significant challenges for numerical solutions, necessitating high-resolution grids to capture all relevant scales effectively. \n\nConsequently, many previous investigations have resorted to simplified approaches, addressing either radiative transfer or fluid dynamics in isolation. For example, some researchers have assumed the wind to be entirely composed of optically thin gas (Friend & Castor, 1983), while others have employed basic models for the radial variation of optical depth (Pauldrach et al., 1986) or have disregarded opacity effects altogether (Lamers & Cassinelli, 1999). Additionally, simplifications regarding the flow, such as assuming spherical symmetry (Puls et al., 1996), steady-state conditions (Owocki et al., 1988), and stationarity (Runacres & Owocki, 2002), have been common. In this paper, we present new calculations that enhance our understanding of the composition and behavior of line-driven winds, emphasizing the critical role of porosity and clumping in accurately estimating mass loss rates.",
        "ori-fast-z-score": -0.8669214468630108,
        "water-fast-z-score": 5.728012446065256,
        "rewrite-fast-z-score": -0.7006490497453707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Absolute Calibration and Characterization of the Multiband Imaging Photometer for Spitzer. I. The Stellar Calibrator Sample and the 24 micron Calibration .\nAbstract:\nWe present an absolute calibration of MIPS photometry at 24, 70, and 160 microns using stellar calibrators observed by the Infrared Array Camera (IRAC) onboard the Spitzer Space Telescope. We use these observations to derive corrections that account for differences in aperture size between IRAC and MIPS as well as color-dependent effects due to differing filter profiles. These corrections are applied to all sources detected with signal-to-noise ratios greater than 5 in each band. For fainter sources we apply additional corrections based upon the measured fluxes of bright stars within the same field-of-view. This method is used to calibrate over 1 million objects across the sky. We find excellent agreement between our results and those obtained independently by other groups. Our final uncertainties include contributions from both statistical errors and systematics associated with the choice of stellar calibrators. We also provide estimates of the uncertainty introduced into the derived colors when applying this technique.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Absolute Calibration and Characterization of the Multiband Imaging Photometer for Spitzer.I.The Stellar Calibrator Sample and the 24 micron Calibration . Abstract : We present an absolute calibration of MIPS photometry at 24 , 70 , and 160 microns using stellar calibrators observed by the Infrared Array Camera ( IRAC ) onboard the Spitzer Space Telescope .We use these observations to derive corrections that explain for variations in aperture size between IRAC and MIPS as well as color - dependent effects due to varied filter profiles . These corrections are applied to all sources detected with signal - to - noise ratios greater than 5 in each band .For fainter sources we apply additional corrections based upon the determined fluxes of bright stars within the same field - of - view . This method is utilized to calibrate over 1 million items across the sky .We get excellent agreement between our findings and those acquired independently by other organizations . Our last uncertainties include contributions from both statistical mistakes and systematics associated with the selection of stars calibrators .We additionally give accounts of the uncertainty introduced into the derived colors when applying this methodology .",
        "rewrite_text": "Title: Absolute Calibration and Characterization of the Multiband Imaging Photometer for Spitzer: The Stellar Calibrator Sample and the 24 Micron Calibration\n\nAbstract: In this study, we present a comprehensive absolute calibration of the Multiband Imaging Photometer for Spitzer (MIPS) photometry at wavelengths of 24, 70, and 160 microns. Our calibration process utilizes stellar calibrators observed by the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. By analyzing these IRAC observations, we derive necessary corrections to account for discrepancies in aperture sizes between IRAC and MIPS, as well as color-dependent effects arising from the differing filter profiles. These corrections are systematically applied to all sources detected with signal-to-noise ratios exceeding 5 in each respective band. For sources with lower brightness, we implement additional corrections based on the flux measurements of brighter stars located within the same field of view. This robust methodology enables us to calibrate over one million astronomical sources across the sky. Our results demonstrate a high level of consistency with independent findings from other research groups, reinforcing the reliability of our calibration approach. We also address the uncertainties associated with our measurements, which stem from both statistical errors and systematic issues related to the selection of stellar calibrators. Furthermore, we provide insights into the uncertainties that may affect the derived colors when employing this calibration technique. Overall, this work contributes significantly to the accurate characterization of MIPS photometry, facilitating enhanced analysis of astronomical data obtained from the Spitzer Space Telescope.",
        "ori-fast-z-score": 0.9428090415820635,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": -0.10369516947304253
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A study of uranium-based multilayers: I. Fabrication and structural characterisation .\nAbstract:\nThe fabrication, structure and properties of uranium oxide (UO 2 )/uranium nitride (UN) superlattices are reported in this work. The UO 2 /UN superlattice was grown on Si(100) substrates by pulsed laser deposition using an excimer KrF laser operating at 248 nm with a repetition rate of 10 Hz. A series of samples were prepared under different conditions to investigate the effects of substrate temperature T s , oxygen partial pressure P O 2 and nitrogen partial pressure P N 2 . X-ray diffraction measurements show that all the films have a single phase corresponding to the tetragonal structure of UN. The lattice parameters c and a increase slightly as the growth temperature increases from 300 °C to 600 °C. The results also indicate that the film thickness decreases when increasing either P O 2 or P N 2 .\nTransmission electron microscopy shows that the interface between the two layers is sharp without any interfacial layer.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A review of uranium - based multilayers : I . Fabrication and structural characterisation .Abstract : The fabrication , structure and properties of uranium alloy ( UO 2 ) / uranium nitride ( UN ) superlattices are published in this project . The UO 2 / UN superlattice was grown on Si ( 100 ) substrates by pulsed infrared deposition use an excimer KrF laser running at 248 nm with a repetition rate of 10 Hz .A series of samples were prepared under various circumstances to examine the effects of substrate temperature T s , oxygen partial pressure P O 2 and oxygen partial pressure P N 2 . X - ray diffraction measurements show that all the films have a single phase relating to the tetragonal shape of UN .The lattice parameters c and a increase somewhat as the growth temperature increases from 300 °C to 600 °C . The results also suggest that the film thickness decreases when varying either P O 2 or P N 2 .Transmission electron microscopy demonstrates that the interface between the two layers is sharp without any interfacial layer .",
        "rewrite_text": "**Title:** A Review of Uranium-Based Multilayers: I. Fabrication and Structural Characterization\n\n**Abstract:** This article presents a comprehensive investigation into the fabrication, structural characteristics, and properties of uranium alloy (UO2) and uranium nitride (UN) superlattices. The UO2/UN superlattice structures were successfully synthesized on silicon (100) substrates utilizing pulsed infrared deposition techniques, employing an excimer KrF laser operating at a wavelength of 248 nm and a repetition rate of 10 Hz. A systematic series of samples were produced under varying conditions to assess the influence of substrate temperature (Ts), oxygen partial pressure (PO2), and nitrogen partial pressure (PN2) on the resulting films. X-ray diffraction analyses confirmed that all synthesized films exhibit a single-phase structure corresponding to the tetragonal configuration of UN. Notably, the lattice parameters, both c and a, were observed to increase slightly with rising growth temperatures, ranging from 300 °C to 600 °C. Furthermore, the findings indicate a reduction in film thickness when either PO2 or PN2 is altered. High-resolution transmission electron microscopy revealed that the interface between the UO2 and UN layers is distinctly sharp, with no observable interfacial layer present. This study not only elucidates the fabrication process and structural integrity of uranium-based multilayers but also lays the groundwork for future research into their potential applications in various technological fields. The results underscore the significance of growth parameters in tailoring the properties of these superlattices, which could have implications for advancements in nuclear materials and related disciplines.",
        "ori-fast-z-score": -0.917662935482247,
        "water-fast-z-score": 3.4412360080584263,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neutronic performances of the MEGAPIE target .\nAbstract:\nThe Megapie (Megavoltage Ionization Projection Imaging Experiment) is an experiment designed to study the feasibility and performance of proton radiography for medical applications. The main goal of this project was to develop a compact, high intensity ion source based on laser-plasma interaction in order to produce protons with energies up to several hundred MeV. In addition, it has been shown that such sources can be used as targets for neutron production by spallation reactions induced by energetic ions. This work presents results obtained during experiments performed at GSI Darmstadt using a pulsed deuteron beam accelerated by the SIS-18 synchrotron accelerator. Neutrons produced by the D+D reaction were detected by means of two fission chambers placed around the target chamber. A detailed analysis of these data allowed us to determine the number of neutrons emitted per incident deuteron particle and their energy distribution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neutronic performances of the MEGAPIE objective . Abstract : The Megapie ( Megavoltage Ionization Projection Imaging Experiment ) is an project meant to study the feasibility and performance of proton radiography for medical uses .The main goal of this project was to develop a compact , large intensity ion supply based on laser - plasma interaction in order to produce protons with energies up to several hundred MeV . In addition , it has been shown that such sources can be used as targets for neutron production by spallation reactions generated by energetic ions .This research provides findings obtained during experiments conducted at GSI Darmstadt involving a pulsed deuteron light accelerated by the SIS - 18 synchrotron accelerator . Neutrons created by the D + D process were detected by means of two fission chambers put around the target chamber .A precise analysis of these information helped us to predict the quantity of neutrons emitted per incident deuteron particle and their power distribution .",
        "rewrite_text": "**Title:** Neutronic Performance Assessment of the MEGAPIE Project\n\n**Abstract:** The Megapie (Megavoltage Ionization Projection Imaging Experiment) is an innovative initiative aimed at exploring the feasibility and efficacy of proton radiography for medical applications. The primary objective of this project is to create a compact, high-intensity ion source utilizing laser-plasma interactions to generate protons with energies reaching several hundred MeV. This research not only focuses on proton generation but also investigates the potential of these sources for neutron production through spallation reactions induced by energetic ions. \n\nThe findings presented in this article are derived from experiments conducted at GSI Darmstadt, where a pulsed deuteron beam was accelerated using the SIS-18 synchrotron accelerator. During these experiments, neutrons produced via the D + D fusion process were detected using two strategically positioned fission chambers surrounding the target chamber. A comprehensive analysis of the collected data enabled us to accurately predict the neutron yield per incident deuteron and to characterize their energy distribution.\n\nThe results indicate that the MEGAPIE project holds significant promise for advancing proton radiography techniques, with implications for enhanced imaging capabilities in medical diagnostics. Furthermore, the ability to generate neutrons through spallation opens new avenues for research in nuclear physics and medical applications. This study contributes valuable insights into the neutron production mechanisms associated with high-energy ion interactions, paving the way for future developments in both medical and scientific fields. The implications of these findings extend beyond the immediate scope of the project, potentially influencing a wide range of applications in medical imaging and radiation therapy.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 6.128258770283413,
        "rewrite-fast-z-score": 1.9599157740244455
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Search for CO(J=3-2) Emission from the Host Galaxy of GRB 980425 with the Atacama Submillimeter Telescope Experiment .\nAbstract:\nWe report on observations made in March and April 2002 at 345 GHz using the Atacama Submillimiter Telescope Experiment (ASTE). We detected no significant emission associated with the host galaxy of the gamma-ray burst GRB 980425, which was discovered by BeppoSAX on Apr. 25, 1998 . The 3 sigma upper limit to the flux density is < 0.5 Jy beam-1 , corresponding to an integrated line intensity ICO< 1 x 10^9 K km s^-1 pc^2 or Mgas < 2 x 10^8 Msun .\nThe non-detection suggests that either the molecular gas mass is much lower than previously estimated or that it has been destroyed by the intense UV radiation field produced during the burst.  These results are consistent with those obtained recently by other groups who have searched for CO emission from this source. If our result can be confirmed then it will provide important constraints on models of the formation of massive stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Search for CO ( J = 3 - 2 ) Emission from the Host Galaxy of GRB 980425 with the Atacama Submillimeter Telescope Experiment . Abstract : We report on observations made in March and April 2002 at 345 GHz using the Atacama Submillimiter Telescope Experiment ( ASTE ) .We observed no major radiation associated with the host galaxy of the gamma - ray burst GRB 980425 , which was discovered by BeppoSAX on Apr . 25 , 1998 .The 3 sigma higher limit to the flux concentration is < 0 . 5 Jy beam - 1 , equivalent to an integrated line intensity ICO < 1 x 10 ^ 9 K km s ^ - 1 pc ^ 2 or Mgas < 2 x 10 ^ 8 Msun . The non - measurement indicates that either the molecular gas mass is much lower than previously predicted or that it has been destroyed by the strong UV rays field produced during the explosion .These conclusions are compatible with those acquired previously by other groups who have searched for CO radiation from this source . If our outcome can be verified then it will provide important restrictions on estimates of the formation of large stars .",
        "rewrite_text": "In this study, we present the findings from observations conducted in March and April 2002 at a frequency of 345 GHz using the Atacama Submillimeter Telescope Experiment (ASTE). Our primary objective was to detect CO (J = 3 - 2) emission from the host galaxy of the gamma-ray burst GRB 980425, which was initially identified by the BeppoSAX satellite on April 25, 1998. Despite our efforts, we did not observe any significant radiation associated with the host galaxy of GRB 980425. The upper limit for the flux density was determined to be less than 0.5 Jy beam^-1, which translates to an integrated line intensity of ICO < 1 x 10^9 K km s^-1 pc^2, or a molecular gas mass (Mgas) of less than 2 x 10^8 solar masses (Msun). This lack of detection suggests that the molecular gas mass in the host galaxy is either substantially lower than earlier estimates or has been significantly depleted due to the intense ultraviolet radiation generated during the gamma-ray burst event. Our findings align with previous studies conducted by other research groups that have also sought CO emissions from this source. If our results are confirmed, they will impose critical constraints on models regarding the formation of massive stars in the aftermath of such explosive events. This research contributes to the broader understanding of star formation processes in environments affected by extreme astrophysical phenomena.",
        "ori-fast-z-score": -2.25,
        "water-fast-z-score": 3.3489378339086286,
        "rewrite-fast-z-score": -2.6866004135669708
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics of Baryons from String Theory and Vector Dominance .\nAbstract:\nWe present the results for the dynamics of baryons in terms of string theory, vector dominance model (VDM) and chiral perturbation theory (ChPT). The VDM is used to calculate the mass spectrum of hadrons with one heavy quark Q = c or b. In this approach we have calculated masses of charmonium states J/ψ , ψ′ , Υ and their radial excitations as well as bottomonium states Υ(1S), Υ(2S), Υ(3S), Υ(4S) and their radial excitations using potential parameters determined by fitting experimental data on meson masses. We also predict the masses of some exotic states like X(3872), Y(4260), Zc(3900) etc., which are not yet observed experimentally. Using these masses we then determine the decay constants of various excited states of quarks into leptons. These decay constants are compared with those obtained within ChPT at leading order.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics of Baryons from String Theory and Vector Dominance . Abstract : We present the results for the dynamics of baryons in terms of string theory , tensor dominance description ( VDM ) and chiral perturbation theory ( ChPT ) .The VDM is utilized to estimate the mass spectrum of hadrons with one heavy quark Q = c or c . In this methodology we have predicted masses of charmonium states J / ψ , ψ ′ , [UNK] and their radial excitations as also as bottomonium states [UNK] ( 1S ) , [UNK] ( 2S ) , [UNK] ( 3S ) , [UNK] ( 4S ) and their radial excitations using potential parameters calculated by fitting experimental evidence on meson masses .We additionally calculate the masses of some exotic states like X ( 3872 ) , Y ( 4260 ) , Zc ( 3900 ) etc . , which are not already detected experimentally . Using these masses we then calculate the decay constants of several excited states of quarks into leptons .These decay constants are compared with those achieved within ChPT at leading order .",
        "rewrite_text": "We present a comprehensive analysis of baryon dynamics through the frameworks of string theory, vector dominance model (VDM), and chiral perturbation theory (ChPT). This study focuses on the mass spectrum of hadrons containing a single heavy quark, specifically charm (Q = c) or bottom (Q = b) quarks. By employing the VDM, we have successfully predicted the masses of various charmonium states, including J/ψ, ψ′, and their radial excitations, as well as bottomonium states such as Υ(1S), Υ(2S), Υ(3S), and Υ(4S), along with their corresponding radial excitations. The potential parameters used in our calculations were derived from a fitting process based on existing experimental data on meson masses. \n\nIn addition to the conventional states, we also explore the masses of several exotic mesons, including X(3872), Y(4260), and Zc(3900), which have yet to be observed in experiments. Following the mass calculations, we proceed to determine the decay constants for various excited quark states transitioning into leptons. These decay constants are then compared to those obtained through ChPT at leading order, providing a valuable cross-validation of our results. Our findings contribute to a deeper understanding of baryon dynamics and the underlying principles of particle interactions, bridging theoretical predictions with experimental observations in the field of high-energy physics. This work not only enhances the existing knowledge of hadronic mass spectra but also opens avenues for future research into unobserved exotic states and their properties.",
        "ori-fast-z-score": -2.3937749957251055,
        "water-fast-z-score": 1.3858697343671664,
        "rewrite-fast-z-score": -1.1766968108291043
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Protostellar clusters in intermediate-mass (IM) star forming regions .\nAbstract:\nWe present the results of our Spitzer Space Telescope survey for protostars and young stellar objects (YSOs) in three nearby, intermediate mass star-forming regions: NGC 1333, Serpens South, and Perseus North. We identify over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and/or envelopes. The majority are Class I sources that have recently formed outflows or jets; however we also find several dozen more evolved Class II/III sources. In addition to these disk-bearing systems, we detect numerous isolated point-like sources whose SEDs suggest they are deeply embedded protostars. These observations provide new insights into how stars form in IM environments. Our sample includes many previously unidentified low-luminosity protostars which will be useful targets for future studies at higher angular resolution. This work is based on observations made with the Spitzer Space Telescope, which is operated by NASA under contract 1407. Support for this work was provided by NASA through an award issued by JPL/Caltech. \n \n Keywords: Protostar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Protostellar complexes in intermediate - mass ( IM ) star producing regions . Abstract : We report the conclusion of our Spitzer Space Telescope survey for protostars and young stellar bodies ( YSOs ) in three adjacent , intermediate mass star - creating areas : NGC 1333 , Serpens South , and Perseus North .We recognize over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and / or envelopes . The majority are Class I sources that have newly formed outflows or jets ; however we also find several dozen more evolved Class II / III sources .In addition to these disk - bearing components , we locate many isolated point - like sources whose SEDs suggest they are deeply embedded protostars . These measurements give novel knowledge into how stars shape in IM environments .Our specimen includes several recently unidentified low - luminosity protostars which will be valuable targets for future research at higher angular resolution . This research is based on observations made with the Spitzer Space Telescope , which is controlled by NASA under contract 1407 .Support for this project was provided by NASA through an award published by JPL / Caltech . Keywords : Protostar",
        "rewrite_text": "Title: Protostellar Complexes in Intermediate-Mass Star-Forming Regions\n\nAbstract: In this study, we present the findings from our comprehensive survey conducted using the Spitzer Space Telescope, focusing on protostars and young stellar objects (YSOs) within three closely situated intermediate-mass star-forming regions: NGC 1333, Serpens South, and Perseus North. Our investigation has led to the identification of over 100 candidate YSOs exhibiting infrared excesses, which are indicative of the presence of circumstellar disks and/or envelopes. The majority of these candidates are classified as Class I sources, characterized by their newly formed outflows or jets. Additionally, we have identified several dozen more advanced Class II and III sources within these regions. Beyond the disk-bearing YSOs, our survey also uncovered numerous isolated point-like sources whose spectral energy distributions (SEDs) suggest they are deeply embedded protostars. These findings provide new insights into the processes of star formation in intermediate-mass environments. Notably, our sample includes several previously unidentified low-luminosity protostars, which represent promising targets for future studies aimed at achieving higher angular resolution observations. This research is grounded in data obtained from the Spitzer Space Telescope, operated by NASA under contract 1407, with support from NASA through an award issued by JPL/Caltech. Our work contributes to the understanding of star formation dynamics and the characteristics of protostellar complexes in intermediate-mass regions, paving the way for further exploration in this field.\n\nKeywords: Protostar, Young Stellar Objects, Star Formation, Infrared Excess, Circumstellar Disks.",
        "ori-fast-z-score": -1.865992419824736,
        "water-fast-z-score": 4.880935300919764,
        "rewrite-fast-z-score": 0.29002094671369905
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  ATLAS sensitivity to Wtb anomalous couplings in top quark decays .\nAbstract:\nWe present the results on searches for new physics beyond the Standard Model (SM) in ttbar events at sqrt(s) = 7 TeV, using data collected by ATLAS during 2011 corresponding to an integrated luminosity of 4.7 fb-1 . The analysis is performed with three different final states: lepton+jets , dilepton and all-hadronic channels. We consider two types of models that can be tested with these analyses: vector-like quarks decaying into tWb or tbHq final states. In both cases we use simplified models where only one coupling parameter is varied while keeping others fixed to their SM values. No significant deviations are observed compared to the predictions based on the SM. Limits are set on the parameters describing the interactions between heavy particles and fermions. These limits improve upon previous measurements made at LEP2 and LHC Run I. \nThe ATLAS Collaboration",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : ATLAS sensitivity to Wtb anomalous couplings in top quark decays . Abstract : We report the results on investigations for latest physics beyond the Standard Model ( SM ) in ttbar events at sqrt ( s ) = 7 TeV , using data derived by ATLAS during 2011 resulting to an integrated luminosity of 4 . 7 fb - 1 .The examination is conducted with three different final states : lepton + jets , dilepton and all - hadronic channels . We consider two forms of models that can be evaluated with these calculations : matrix - like quarks decaying into tWb or tbHq final states .In both cases we using simplified methods where only one coupling parameter is varied while staying others set to their SM parameters . No important deviations are observed compared to the predictions based on the SM .Limits are set on the parameters describing the interactions between heavy molecules and fermions . These restrictions improve upon recent observations made at LEP2 and LHC Run I .The ATLAS Collaboration",
        "rewrite_text": "We present the findings of our investigation into potential physics beyond the Standard Model (SM) through the analysis of ttbar events at a center-of-mass energy of √s = 7 TeV. This study utilizes data collected by the ATLAS detector during 2011, corresponding to an integrated luminosity of 4.7 fb⁻¹. Our analysis encompasses three distinct final states: lepton + jets, dilepton, and all-hadronic channels. We explore two theoretical frameworks that can be assessed through our calculations: models involving matrix-like quarks that decay into tWb or tbHq final states. In our approach, we employ simplified methodologies, varying only one coupling parameter at a time while keeping the others fixed to their SM values. Throughout our examination, we find no significant deviations from the predictions made by the SM. Consequently, we establish limits on the parameters that characterize the interactions between heavy particles and fermions. These constraints represent an enhancement over previous results obtained from LEP2 and LHC Run I. The findings contribute to the ongoing search for new physics and provide valuable insights into the nature of top quark decays and their associated anomalous couplings. The ATLAS Collaboration remains committed to advancing our understanding of fundamental interactions and exploring the potential existence of phenomena that extend beyond the current theoretical framework.",
        "ori-fast-z-score": -1.778001778002667,
        "water-fast-z-score": 4.064004064006096,
        "rewrite-fast-z-score": -1.2939932784412609
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The unification of Pythagorean theorem for electronic orbitals with Kepler s law for planetary orbits .\nAbstract:\nThe present work is devoted to the study of the relationship between the orbital motion and the energy levels in atoms, ions or molecules. The main idea consists on establishing an analogy between the circular orbits of planets around their central star (Kepler s laws) and the electron shells in atoms (Pauli principle). In this way we can obtain new relationships among physical quantities such as the radius of the planet, its mass, the distance between two consecutive orbits, etc., which are useful when studying atomic systems. We also show that these results allow us to establish a connection between the Pauli principle and the concept of spin angular momentum. Finally, we propose some applications of our formalism to solve problems related to molecular physics. This article has been published in Journal of Physics A, Volume 44, Issue 1, page 015101-1-151105, 2015. DOI: 10.1088/1361-6111/44/1/015101",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The merger of Pythagorean principle for electronic orbitals with Kepler s law for planetary orbits . Abstract : The present work is devoted to the observation of the relationship between the orbital movement and the power concentrations in atoms , electrons or compounds .The main idea consists on establishing an analogy between the circular orbits of planets around their central star ( Kepler s rules ) and the electron shells in atoms ( Pauli concept ) . In this way we can obtain new relationships among physical quantities such as the radius of the planet , its mass , the distance between two consecutive orbits , etc . , which are helpful when examining atomic processes .We also demonstrate that these results make us to find a connection between the Pauli concept and the notion of spin angular velocity . Finally , we propose some applications of our formalism to solve difficulties related to molecular mechanics .This section has been printed in Journal of Physics A , Volume 44 , Issue 1 , page 015101 - 1 - 151105 , 2015 . DOI : 10 . 1088 / 1361 - 6111 / 44 / 1 / 015101",
        "rewrite_text": "Title: The Integration of the Pythagorean Principle for Electronic Orbitals with Kepler's Law for Planetary Orbits\n\nAbstract: This study explores the intricate relationship between orbital dynamics and power distributions within atoms, electrons, and compounds. The primary objective is to draw a parallel between the circular orbits of planets as described by Kepler's laws and the electron shells in atomic structures, as conceptualized by Pauli. By establishing this analogy, we derive novel relationships among various physical parameters, including planetary radius, mass, and the spacing between successive orbits. These relationships prove to be instrumental in the analysis of atomic phenomena. Furthermore, our findings reveal a significant link between the Pauli principle and the concept of spin angular velocity, suggesting deeper insights into the behavior of electrons in atomic systems. We also discuss potential applications of our theoretical framework in addressing challenges within molecular mechanics. This research contributes to a broader understanding of the connections between celestial mechanics and quantum physics, offering a fresh perspective on the fundamental principles governing both atomic and planetary systems. The findings of this work have been published in the Journal of Physics A, Volume 44, Issue 1, on pages 015101-1 to 015101-5, in 2015. DOI: 10.1088/1361-6111/44/1/015101.",
        "ori-fast-z-score": -0.7875615306482168,
        "water-fast-z-score": 4.162825233426289,
        "rewrite-fast-z-score": 0.41256849850351734
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bright-Dark Soliton Complexes in Spinor Bose-Einstein Condensates .\nAbstract:\nWe study the formation and dynamics of bright-dark solitons in spin-1 condensates with spin-orbit coupling, which are described by the Gross-Pitaevskii equation for two coupled fields. We show that dark-bright solitons can be formed when one field is initially localized at the center of the trap while the other has an extended profile. The resulting solitonic states have been observed experimentally. \n \n In addition to their fundamental interest as nonlinear excitations, these structures may also play important roles in quantum information processing applications such as atom interferometry or quantum logic gates based on matter waves. Finally we discuss how our results could be generalized beyond the mean-field approximation. \nI. INTRODUCTORY REMARK\nThe recent experimental realization of spinor BECs  1  , i.e., atomic gases trapped in magnetic potentials where each atom carries a well-defined internal degree of freedom (spin), has opened up new avenues towards the investigation of novel physical phenomena  2  . Among them, the possibility of creating stable spin textures  3  , topological defects  4  , and vortex lattices  5  has attracted considerable attention over the past few years  6  .\nIn this work we focus on another interesting class of solutions recently predicted theoretically  7, 8  : Bright-Dark Soliton Complex (BDSC) pairs. These consist of a pair of spatially separated bright and dark solitons whose relative phase varies continuously across the system  9  . They were first proposed in the context of optics  10  but later found to exist in various systems including superfluids  11  , plasmas  12  , and semiconductor microcavities  13  . Their existence was confirmed experimentally in optical fibers  14  and more recently in ultracold atoms  15  . \nII. MODEL AND METHODS\n\nA. Mean-Field Model\nSpinor BECs are modeled within the framework of the meanfield theory  16  using the following set of coupled Gross-Pitaevski equations  17  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bright - Dark Soliton Complexes in Spinor Bose - Einstein Condensates . Abstract : We research the formation and dynamics of bright - darkened solitons in spin - 1 condensates with spin - orbit coupling , which are explained by the Gross - Pitaevskii equation for two coupled fields .We see that dark - bright solitons can be formed when one field is initially localized at the center of the trap while the other has an extended profile . The produced solitonic states have been observed experimentally .In addition to their vital importance as nonlinear excitations , these structures could also play vital importance in quantum information processing applications such as atom interferometry or quantum logic gates based on matter waves . Finally we talk how our findings may be generalized beyond the mean - field approximation .I . INTRODUCTORY REMARK The recent experimental realization of spinor BECs 1 , i . e . , atomic atoms trapped in magnetic potentials where each molecule carries a better - defined internal degree of liberty ( spin ) , has opened up new avenues towards the exploration of new physical phenomena 2 .Among them , the prospect of creating stable spinning textures 3 , topological errors 4 , and vortex lattices 5 has garnered considerable focus over the previous few years 6 . In this research we focus on another important family of solutions recently suggested theoretically 7 , 8 : Bright - Dark Soliton Complex ( BDSC ) pairs .These consist of a pair of spatially separated brilliant and dark solitons whose relative phase differs rapidly across the system 9 . They were first suggested in the context of optics 10 but later realized to arise in different structures including superfluids 11 , plasmas 12 , and semiconductor microcavities 13 .Their existence was confirmed experimentally in optical fibers 14 and more recently in ultracold atoms 15 . II .MODEL AND METHODS A . Mean - Field Model Spinor BECs are modeled within the framework of the meanfield physics 16 utilizing the following setting of coupled Gross - Pitaevski equations 17 :",
        "rewrite_text": "**Title:** Bright-Dark Soliton Complexes in Spinor Bose-Einstein Condensates\n\n**Abstract:** This study investigates the formation and dynamics of bright-dark soliton complexes in spin-1 Bose-Einstein condensates (BECs) that exhibit spin-orbit coupling, described by the coupled Gross-Pitaevskii equations. Our findings reveal that dark-bright solitons can emerge when one of the fields is initially localized at the center of the trapping potential, while the other field maintains a broader spatial distribution. The solitonic states generated through this process have been confirmed through experimental observations. These soliton structures are not only significant as nonlinear excitations but also hold potential applications in quantum information processing, including atom interferometry and quantum logic gates utilizing matter waves. Furthermore, we discuss the implications of our results and how they may extend beyond the mean-field approximation, suggesting a broader relevance in the study of quantum systems. \n\nThe recent advancements in the experimental realization of spinor BECs—where atoms are confined in magnetic potentials and possess well-defined internal spin states—have opened new pathways for exploring novel physical phenomena. Among these phenomena, the creation of stable spin textures, topological defects, and vortex lattices has attracted considerable attention in recent years. Our research focuses on a newly proposed class of solutions: Bright-Dark Soliton Complex (BDSC) pairs. These pairs consist of spatially separated bright and dark solitons, characterized by a rapidly varying relative phase across the system. Initially proposed in the context of optical systems, BDSCs have been shown to manifest in various physical contexts, including superfluids, plasmas, and semiconductor microcavities. Their experimental validation has been achieved in optical fibers and, more recently, in ultracold atomic systems. This work aims to deepen the understanding of BDSCs and their potential applications in future quantum technologies.",
        "ori-fast-z-score": -0.7745966692414834,
        "water-fast-z-score": 6.002450479987809,
        "rewrite-fast-z-score": 1.0947974973864747
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Statistical properties of dust far-infrared emission .\nAbstract:\nWe present the results of our analysis on the statistical properties of dust FIR emission in nearby galaxies, based on data obtained by ISO and Spitzer space telescopes. We find that the distribution function of dust FIR luminosity is well described by a log-normal form with an exponential tail at high luminosities. The mean value of the logarithmic luminosity dispersion for all samples considered here is 0.3 dex (factor of 2). This result suggests that there are two populations of dusty star-forming regions within each galaxy -one population associated with normal star formation activity and another one associated with intense bursts of star formation. Our study also shows that the fraction of galaxies containing such extreme objects increases towards higher redshifts. These findings have important implications for understanding the physical processes responsible for the evolution of distant galaxies as well as their contribution to the cosmic infrared background radiation. \n \n Keywords: Infrared, Galaxy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Statistical properties of dust far - infrared emission . Abstract : We present the conclusion of our analysis on the statistical characteristics of dust FIR absorption in nearby galaxies , using on evidence derived by ISO and Spitzer space telescopes .We see that the distribution function of dust FIR luminosity is well described by a log - normal shape with an exponential tail at high luminosities . The mean value of the logarithmic luminosity dispersion for all specimens considered here is 0 . 3 dex ( factor of 2 ) .This result suggests that there are two communities of dusty star - creating areas within each galaxy - one community correlated with normal star formation activity and another one related with fierce bursts of galaxy formation . Our study also shows that the fraction of stars bearing such extreme objects increases towards higher redshifts .These studies have important implications for studying the physical processes responsible for the evolution of distant galaxies as well as their impact to the cosmic infrared background radiation . Keywords : Infrared , Galaxy",
        "rewrite_text": "Title: Statistical Properties of Dust Far-Infrared Emission\n\nAbstract: In this article, we present the findings from our comprehensive analysis of the statistical properties of far-infrared (FIR) emission from dust in nearby galaxies, utilizing data obtained from the ISO and Spitzer space telescopes. Our investigation reveals that the distribution function of dust FIR luminosity exhibits a log-normal distribution, characterized by an exponential tail at higher luminosities. The mean logarithmic dispersion of luminosity across all examined galaxies is approximately 0.3 dex, which corresponds to a factor of two variation. This observation indicates the presence of two distinct populations of star-forming regions within each galaxy: one that is associated with typical star formation processes and another that is linked to intense bursts of star formation activity. Furthermore, our analysis indicates that the prevalence of these extreme star-forming regions increases with higher redshifts, suggesting a significant evolution in star formation dynamics over cosmic time. These findings have profound implications for understanding the physical mechanisms that drive the evolution of distant galaxies and their contributions to the cosmic infrared background radiation. Our research enhances the current knowledge of galaxy formation and evolution, providing insights into the role of dust in the FIR regime and its influence on the broader cosmic environment. \n\nKeywords: Infrared, Galaxy",
        "ori-fast-z-score": 0.21320071635561041,
        "water-fast-z-score": 5.193989612031165,
        "rewrite-fast-z-score": 1.7483145522430754
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraining Models of Neutrino Mass and Neutrino Interactions with the Planck Satellite .\nAbstract:\nWe present constraints on models of neutrino mass, mixing angles, and interactions using data from the Planck satellite experiment. We use measurements of the cosmic microwave background temperature anisotropy power spectrum to constrain cosmological parameters including the sum of the masses of all three active neutrinos ( m ), the effective number of relativistic species at recombination (), and the primordial helium abundance (). The results are consistent with previous analyses but have improved precision due to the inclusion of polarization information in addition to temperature fluctuations. \n \n In particular we find that the total neutrino mass is constrained by Planck to be less than 0.23 eV at 95% confidence level for normal hierarchy or less than 0.28 eV for inverted hierarchy. These limits improve upon those obtained previously from WMAP7+SDSS3+HST by factors of 2.5-3.0. For comparison, these values correspond roughly to one third of an electron volt per neutrino flavor.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraining Models of Neutrino Mass and Neutrino Interactions with the Planck Satellite . Abstract : We address constraints on estimates of neutrino mass , mix angles , and interactions using data from the Planck satellite experiment .We use measurements of the cosmic microwave background temperature anisotropy energy spectrum to constrain cosmological factors including the sum of the masses of all three active neutrinos ( m ) , the effective number of relativistic species at recombination ( ) , and the primordial helium abundance ( ) . The results are compatible with previous analyses but have improved precision thanks to the introduction of polarization info in addition to heat fluctuations .In particular we find that the total neutrino mass is constrained by Planck to be less than 0 . 23 eV at 95 % confidence rate for normal hierarchy or less than 0 . 28 eV for inverted hierarchy . These limits enhance upon those obtained previously from WMAP7 + SDSS3 + HST by factors of 2 . 5 - 3 . 0 .For comparison , these estimates approximate roughly to one third of an electron volt per neutrino flavor .",
        "rewrite_text": "In this article, we explore the constraints on neutrino mass, mixing angles, and interactions derived from data collected by the Planck satellite. Utilizing measurements of the cosmic microwave background (CMB) temperature anisotropy energy spectrum, we aim to refine our understanding of several key cosmological parameters. Specifically, we focus on the total mass of the three active neutrinos (denoted as m), the effective number of relativistic species present at the time of recombination, and the primordial abundance of helium. Our findings align with previous studies but demonstrate enhanced precision due to the incorporation of polarization data alongside temperature fluctuations. Notably, our analysis reveals that the total neutrino mass is constrained to be below 0.23 eV at a 95% confidence level for the normal hierarchy scenario, and below 0.28 eV for the inverted hierarchy. These new limits represent a significant improvement over earlier estimates derived from the WMAP7, SDSS3, and HST datasets, yielding enhancements by factors ranging from 2.5 to 3.0. When contextualized, these constraints suggest that the mass of each neutrino flavor is approximately one-third of an electron volt. This work underscores the importance of the Planck satellite's data in advancing our understanding of fundamental particle physics and cosmology, particularly in the context of neutrino properties and their implications for the evolution of the universe.",
        "ori-fast-z-score": 0.9561828874675149,
        "water-fast-z-score": 3.916379472039716,
        "rewrite-fast-z-score": 0.31799936400190804
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Cascade Model for Particle Concentration and Enstrophy in Fully Developed Turbulence with Mass Loading Feedback .\nAbstract:\nWe present an analytical model to study the effect of mass loading feedback on particle concentration and enstrophy in fully developed turbulence. The cascade process is modeled by using a set of coupled ordinary differential equations, which are derived based on dimensional analysis and Kolmogorov s similarity hypothesis. We show that the presence of particles can significantly affect both the energy transfer rate between different scales as well as the dissipation rates at small scales. In particular, we find that the total amount of energy transferred into smaller scales decreases when there exists significant mass loading feedback. This result suggests that the presence of heavy particles may lead to reduced turbulent mixing efficiency. Finally, our results also indicate that the effects of mass loading feedback become more pronounced if the Stokes number increases or the initial volume fraction of particles becomes larger. Our findings provide useful insights into understanding how heavy particles influence the dynamics of fluid flows. C \nAuthor(s): Yi-Chun Chen , Shih-Chieh Hwang , Chia-Hui Wu , Yu-Ting Lin , Ming-Yuan Liu , Chao-Lin Wang , Jie-Sheng Huang , Wen-Ju Tsai , Tzi-Chao Chan , Chin-Fa Lee , Kuo-Yang Chang , Chung-Ming Yeh , Yuan-Kang Chiou , Chien-Nan Chu , Cheng-Wei Hsieh , Chien-Wen Lu , Chien-Chung Wu , Chien-Shu Chen , Chien-Chin Wu , Chien-Chin Yang , Chien-Chin Lai , Chien-Chin Su , Chien-Chin Hung , Chien-Chin Chen , Chien-Ching Wu , Chien-Ching Tai , Chien-Ching Li , Chien-Ching Sun , Chien-Ching Liang , Chien-Ching Chen , Chien-Chong Wu , Chien-Chung Chen , Chien-Chung Lai , Chien-Chung Su , Chien-Chung Hung , Chien-Chung",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Cascade Model for Particle Concentration and Enstrophy in Fully Developed Turbulence with Mass Loading Feedback . Abstract : We present an analytical theory to study the impact of mass loading feedback on particle concentration and enstrophy in fully developed turbulence .The cascade process is modeled by using a setting of coupled ordinary differential coefficients , which are derived based on dimensional theory and Kolmogorov s similarity hypothesis . We see that the presence of particles can significantly affect both the power transfer frequency between various scales as well as the dissipation levels at small scales .In particular , we find that the total quantity of power sent into smaller scales decreases when there exists significant mass loading feedback . This result suggests that the presence of large particles may contribute to reduced turbulent mixing efficiency .Finally , our findings also suggest that the effects of mass displacement feedback become more pronounced if the Stokes number increases or the initial volume fraction of molecules remains bigger . Our findings provide useful insights into knowledge how heavy molecules affect the dynamics of fluid flows .C \nAuthor(s): Yi-Chun Chen , Shih-Chieh Hwang , Chia-Hui Wu , Yu-Ting Lin , Ming-Yuan Liu , Chao-Lin Wang , Jie-Sheng Huang , Wen-Ju Tsai , Tzi-Chao Chan , Chin-Fa Lee , Kuo-Yang Chang , Chung-Ming Yeh , Yuan-Kang Chiou , Chien-Nan Chu , Cheng-Wei Hsieh , Chien-Wen Lu , Chien-Chung Wu , Chien-Shu Chen , Chien-Chin Wu , Chien-Chin Yang , Chien-Chin Lai , Chien-Chin Su , Chien-Chin Hung , Chien-Chin Chen , Chien-Ching Wu , Chien-Ching Tai , Chien-Ching Li , Chien-Ching Sun , Chien-Ching Liang , Chien-Ching Chen , Chien-Chong Wu , Chien-Chung Chen , Chien-Chung Lai , Chien-Chung Su , Chien-Chung Hung , Chien-Chung",
        "rewrite_text": "**Title:** A Cascade Model for Particle Concentration and Enstrophy in Fully Developed Turbulence with Mass Loading Feedback\n\n**Abstract:** In this study, we introduce a comprehensive analytical framework to investigate the effects of mass loading feedback on particle concentration and enstrophy within fully developed turbulent flows. Our approach employs a cascade model characterized by a system of coupled ordinary differential equations, which are formulated based on dimensional analysis and the Kolmogorov similarity hypothesis. Our analysis reveals that the introduction of particles into the turbulent medium significantly alters the energy transfer dynamics across various scales, as well as the dissipation rates at smaller scales. Notably, we observe that the overall power transmitted to smaller scales diminishes in the presence of substantial mass loading feedback, indicating that larger particles may hinder the efficiency of turbulent mixing processes. Furthermore, our results indicate that the influence of mass displacement feedback becomes increasingly significant with higher Stokes numbers or when the initial volume fraction of particles is elevated. These findings enhance our understanding of how heavy particles interact with fluid dynamics, providing critical insights into the mechanisms governing turbulence in multiphase flows. This work has implications for various applications, including environmental science, industrial processes, and the study of natural phenomena where particle-laden flows are prevalent. The insights gained from this research could inform strategies for managing and optimizing turbulent systems in practical scenarios.",
        "ori-fast-z-score": 0.38138503569823695,
        "water-fast-z-score": 5.669467095138408,
        "rewrite-fast-z-score": -1.0776318121606494
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Grand minima and maxima of solar activity: New observational constraints .\nAbstract:\nWe present new results on the long-term evolution of solar magnetic fields, obtained by applying an advanced data analysis technique to observations made with the Wilcox Solar Observatory (WSO) magnetograph between 1976 and 2009. The method is based on wavelet transforms in combination with principal component analysis (PCA). It allows us to separate different types of variability into their individual components at each point in time. We find that there are two distinct modes of solar magnetic field evolution over this period. One mode shows strong fluctuations around a mean value which varies slowly but significantly during the cycle 23/24 minimum. This behaviour can be explained as being due to the presence of large-scale dynamo waves driven by differential rotation. In addition we identify another type of variation which appears to have no preferred frequency or spatial scale. These variations show significant correlations with sunspot number and other proxies for solar activity. They may therefore represent some form of global response of the Sun s magnetic field to changes in its internal dynamics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Grand minima and maxima of solar activity : New observational restrictions . Abstract : We report new data on the long - term expansion of solar magnetic fields , obtained by using an sophisticated data analysis technique to observations made with the Wilcox Solar Observatory ( WSO ) magnetograph between 1976 and 2009 .The method is based on wavelet transforms in combination with principal component analysis ( PCA ) . It enables us to separate distinct types of variability into their individual parts at each point in time .We see that there are two different modes of solar magnetic force evolution over this time . One mode displays strong fluctuations around a mean value which varies steadily but significantly during the period 23 / 24 minimum .This behaviour can be understood as being owing to the presence of large - scale dynamo waves powered by differential rotation . In addition we identify another type of variation which appears to have no chosen amplitude or spatial scale .These changes demonstrate considerable correlations with sunspot number and other proxies for solar activity . They might hence indicate some kind of worldwide response of the Sun s magnetic force to changes in its internal behavior .",
        "rewrite_text": "Title: Grand Minima and Maxima of Solar Activity: New Observational Constraints\n\nAbstract: In this study, we present novel findings regarding the long-term evolution of solar magnetic fields, derived from an advanced data analysis technique applied to observations from the Wilcox Solar Observatory (WSO) magnetograph spanning the years 1976 to 2009. Our analytical approach integrates wavelet transforms with principal component analysis (PCA), allowing us to decompose various types of variability into distinct components over time. Our results reveal two predominant modes of solar magnetic force evolution during this period. The first mode is characterized by significant fluctuations around a mean value that exhibits a steady yet notable variation throughout the solar cycle, particularly during the 23/24 solar minimum. This behavior can be attributed to the influence of large-scale dynamo waves driven by differential rotation within the Sun. The second mode of variation, however, appears to lack a defined amplitude or spatial scale, suggesting a more chaotic nature. Notably, these fluctuations show strong correlations with sunspot numbers and other indicators of solar activity, implying a potential global response of the Sun's magnetic field to internal dynamical changes. Our findings contribute to a deeper understanding of solar activity patterns and their implications for solar physics, highlighting the complex interplay between solar magnetic fields and solar cycle dynamics. This research not only enhances our comprehension of solar behavior but also provides critical insights into the mechanisms underlying grand minima and maxima in solar activity.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.599750073368524,
        "rewrite-fast-z-score": 1.3525044520011484
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  R-matrices in Rime .\nAbstract:\nThe aim of this work is to present the results obtained by applying the method of R-matrix theory for solving problems related with scattering processes on complex systems, such as atomic nuclei and molecules. The main idea behind the approach presented here consists in using an effective interaction potential which takes into account all possible interactions between particles involved in the process under consideration. This allows one to obtain solutions that are valid at any distance between interacting objects. In particular we have applied our formalism to study elastic electron-atom collisions within the framework of the Born approximation. We show how it can be used to calculate cross sections for different types of atoms (hydrogen-like ions) and compare them with those calculated within other approaches. \n \n Keywords: Elastic Scattering, Cross Section, Electron Atom Collision, R-Matrix Theory, Effective Interaction Potential, Hydrogen-Like Ions. 1 Introduction \n \n The problem of calculating cross section for elastic electron-atom collision has been studied extensively during last decades both theoretically and experimentally  1  . It was shown  2  , however, that even if the exact wave function describing the system is known, the calculation of the corresponding cross section requires very complicated numerical procedures. Therefore various approximate methods were developed  3  -  6  . Among these approximations the most popular ones are: the first Born approximation  7  , the second Born approximation  8  , the distorted-wave Born approximation  9  , the Kohn variational principle  10  , etc.. All these methods allow one to find analytical expressions for the total cross section but they require some additional assumptions about the form of the wave functions or potentials describing the system under investigation. For example, in order to apply the first Born approximation one needs to know the exact solution of Schrödinger equation for the initial state of the system while the second Born approximation requires knowledge of the exact wave function for final states only  11  . On the other hand, the distorted-wave Born",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : R - matrices in Rime . Abstract : The goal of this project is to provide the results derived by using the method of R - matrix theory for solving cases linked with scattering systems on complex systems , such as atomic atoms and atoms .The main idea behind the approach given here consists in utilizing an efficient coupling potential which assumes into consideration all possible behaviors between particles involved in the process under consideration . This enables one to obtain answers that are applicable at any distance between interacting objects .In particular we have applied our formalism to study elastic electron - atom collisions within the framework of the Born method . We see how it can be used to estimate cross sections for different kinds of atoms ( hydrogen - like ions ) and contrast them with those estimated within other methods .Keywords : Elastic Scattering , Cross Section , Electron Atom Collision , R - Matrix Theory , Effective Interaction Potential , Hydrogen - Like Ions . 1 Introduction The question of calculating cross section for elastic electron - atom collision has been studied thoroughly during last decades both theoretically and experimentally 1 .It was shown 2 , however , that even if the exact wave function describing the system is known , the determination of the associated cross area needs very complicated mathematical procedures . Therefore several approximate approaches were developed 3 - 6 .Among these approximations the most popular ones are : the first Born approximation 7 , the second Born approximation 8 , the distorted - wave Born approximation 9 , the Kohn variational theory 10 , etc . . All these algorithms provide one to find analytical expressions for the total cross section but they demand some additional constraints about the form of the wave functions or potentials describing the system under inquiry . For instance , in order to apply the first Born approximation one needs to knowledge the exact solution of Schrödinger equation for the initial state of the system while the second Born approximation involves knowledge of the exact wave function for eventual states only 11 .On the other hand , the compressed - wave Born",
        "rewrite_text": "**Title:** R-Matrices in Rime\n\n**Abstract:** This study aims to present findings derived from the application of R-matrix theory to address scattering phenomena in complex systems, particularly focusing on interactions involving atomic particles. The core concept of our approach is the implementation of an effective coupling potential that accounts for all possible interactions between the particles involved in the scattering process. This comprehensive consideration allows for the derivation of results that remain valid across varying distances between the interacting entities. Specifically, we have employed our theoretical framework to investigate elastic electron-atom collisions using the Born approximation method. Our analysis demonstrates the utility of this formalism in estimating cross sections for various atomic species, including hydrogen-like ions, and facilitates a comparison with results obtained through alternative methodologies. \n\nThe calculation of cross sections for elastic electron-atom collisions has been extensively explored in both theoretical and experimental contexts over the past few decades. Despite the availability of the exact wave function for the system, determining the corresponding cross-sectional area often involves complex mathematical techniques. Consequently, several approximate methods have been developed to simplify these calculations. Among the most widely used approximations are the first Born approximation, the second Born approximation, and the distorted-wave Born approximation, along with Kohn variational theory. While these methods yield analytical expressions for total cross sections, they impose specific requirements regarding the wave functions or potentials that describe the system. For instance, the first Born approximation necessitates knowledge of the exact solution to the Schrödinger equation for the initial state, whereas the second Born approximation requires the exact wave function for the final states. Our work addresses these challenges by providing a more versatile framework that enhances the accuracy and applicability of cross-section calculations in electron-atom collision scenarios.\n\n**Keywords:** Elastic Scattering, Cross Section, Electron-Atom Collision, R-Matrix Theory, Effective Interaction Potential, Hydrogen-Like Ions.",
        "ori-fast-z-score": -0.6405126152203485,
        "water-fast-z-score": 7.1029732717618375,
        "rewrite-fast-z-score": -1.113780197846022
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A new Generation of Spectrometer Calibration Techniques based on Optical Frequency Combs .\nAbstract:\nWe present the latest advances in optical frequency combs and their applications to precision metrology, including calibration techniques for high-resolution spectroscopy instruments such as Fourier transform spectrometers (FTS). We discuss how these techniques can be used to improve measurement accuracy by orders of magnitude over traditional methods.  In particular we describe two novel approaches that are being developed at NIST:  1) The use of an optical frequency comb locked to a high-accuracy atomic clock to calibrate FTS measurements with sub-Hz uncertainty. 2) A technique called  self-calibration  which allows one to measure absolute frequencies without requiring any external reference standards or other auxiliary equipment. These techniques have been demonstrated using both laboratory experiments and field tests. They represent important steps towards realizing the vision of a future where all spectroscopic measurements will be traceable back to fundamental physical constants. This is especially relevant today given the growing interest in developing portable, low-cost devices capable of performing accurate chemical analysis anywhere around the world.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A new Generation of Spectrometer Calibration Techniques based on Optical Frequency Combs . Abstract : We present the latest advances in imaging bandwidth combs and their applications to precision metrology , notably calibration methods for high - resolution spectroscopy instruments such as Fourier shift spectrometers ( FTS ) .We discuss how these tools can be used to achieve observation reliability by orders of magnitude over traditional techniques . In particular we explain two novel approaches that are being pioneered at NIST : 1 ) The using of an optical frequency comb locked to a high - accuracy nuclear clock to calibrate FTS calculations with sub - Hz uncertainty .2 ) A methodology called self - calibration which allows one to measure absolute frequencies without using any external reference standards or other auxiliary machinery . These methods have been shown using both laboratory experiments and field trials .They represent crucial steps towards realizing the vision of a future where all spectroscopic observations will be traceable back to basic physical constants . This is especially applicable today due the increasing interest in building mobile , low - cost devices capable of performing reliable chemical analysis anywhere around the world .",
        "rewrite_text": "We introduce significant advancements in the field of spectrometer calibration techniques utilizing optical frequency combs, as detailed in our recent study. This article focuses on the innovative applications of imaging bandwidth combs in precision metrology, particularly for enhancing calibration methods in high-resolution spectroscopy instruments, such as Fourier transform spectrometers (FTS). Our findings demonstrate that these advanced tools can substantially improve the reliability of observations, achieving accuracy levels that surpass traditional calibration techniques by several orders of magnitude. \n\nWe highlight two groundbreaking approaches currently being developed at the National Institute of Standards and Technology (NIST). The first method involves the integration of an optical frequency comb that is synchronized with a high-precision nuclear clock, enabling FTS calculations to be calibrated with an unprecedented sub-Hz uncertainty. The second approach, termed self-calibration, empowers researchers to measure absolute frequencies independently, without the need for external reference standards or additional equipment. \n\nBoth methodologies have been validated through rigorous laboratory experiments and practical field trials, marking significant progress toward a future where all spectroscopic measurements can be traced back to fundamental physical constants. This advancement is particularly relevant in light of the growing demand for mobile, cost-effective devices capable of conducting reliable chemical analyses globally. Our research paves the way for enhanced spectroscopic techniques that promise to revolutionize the field, making high-precision measurements more accessible and reliable in various applications.",
        "ori-fast-z-score": -0.9138115486202573,
        "water-fast-z-score": 4.848732213850612,
        "rewrite-fast-z-score": -0.18257418583505536
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measurement of the Spin-Transfer-Torque Vector in Magnetic Tunnel Junctions .\nAbstract:\nWe report on measurements of spin-transfer-torque vector (STTV) in magnetic tunnel junctions with perpendicular anisotropy and in-plane magnetization. The STTV is determined by measuring the current-induced switching field distribution for different directions of an external magnetic field applied along the film plane. We find that the magnitude of the STTV depends strongly on the direction of the external magnetic field, which can be explained by taking into account the dependence of the spin polarization efficiency on the angle between the magnetizations of ferromagnetic electrodes. Our results demonstrate that the measurement of the STTV provides information about the angular dependence of the spin polarization at interfaces between magnetic materials. \n \n Spin transfer torque (STT), arising due to the flow of spin-polarized electrons across a magnetic interface, has been extensively studied both theoretically and experimentally over past decade  1  . In particular, it was shown that the application of a charge current through a magnetic tunnel junction (MTJ) leads to the reversal of its free layer via the action of the so-called spin-transfer-torque vector  2  , whose components are given by: \n \n where  is the unit vector pointing along the electron s momentum,  is the unit vector pointing in the direction of the local magnetization, , and are the magnitudes of the spin polarization efficiencies at the left and right interfaces respectively, and is the thickness of the MTJ barrier  3  .\n \nIn this work we present experimental data demonstrating how the magnitude of the STTT changes as function of the orientation of the external magnetic field Hext applied parallel to the film plane. This allows us to determine the angular dependences of the spin polarization effciency at each interface separately.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measurement of the Spin - Transfer - Torque Vector in Magnetic Tunnel Junctions . Abstract : We report on observations of spin - transfer - torque velocity ( STTV ) in magnetic tunnel junctions with perpendicular anisotropy and in - plane magnetization .The STTV is calculated by assessing the current - caused switching field distribution for different angles of an external magnetic force application along the film plane . We see that the magnitude of the STTV depends strongly on the direction of the external magnetic field , which can be described by take into consideration the dependence of the spin polarization efficiency on the angle between the magnetizations of ferromagnetic electrodes .Our results show that the observation of the STTV provides knowledge about the angular relationship of the spin polarization at connections between magnetic materials . Spin transfer torque ( STT ) , arising caused to the movement of spin - polarized ions across a magnetic interface , has been heavily explored both theoretically and experimentally over past year 1 .In particular , it was shown that the introduction of a charge current through a magnetic tunnel junction ( MTJ ) results to the reversal of its free layer via the operation of the so - called spin - transfer - torque velocity 2 , whose components are given by : where is the unit vector pointing along the electron s momentum , is the unit matrix pointing in the direction of the local magnetization , , and are the magnitudes of the spin polarization efficiencies at the left and left interfaces respectively , and is the thickness of the MTJ barrier 3 . In this research we present experimental evidence showing how the magnitude of the STTT increases as function of the orientation of the external magnetic force Hext introduced perpendicular to the film plane .This enables us to predict the angular dependences of the spin polarization effciency at each interface individually .",
        "rewrite_text": "We present a comprehensive study on the measurement of spin-transfer-torque velocity (STTV) in magnetic tunnel junctions (MTJs) characterized by perpendicular anisotropy and in-plane magnetization. Our investigation focuses on the relationship between the STTV and the distribution of switching fields induced by current, which varies with the angle of an externally applied magnetic field within the film plane. Notably, we observe a significant dependence of the STTV magnitude on the direction of the external magnetic field. This dependence can be attributed to the variation in spin polarization efficiency, which is influenced by the angular alignment between the magnetizations of the ferromagnetic electrodes.\n\nThe phenomenon of spin transfer torque (STT), resulting from the movement of spin-polarized electrons across a magnetic interface, has garnered extensive theoretical and experimental attention in recent years. Previous studies have demonstrated that the application of a charge current through an MTJ can lead to the reversal of its free layer, facilitated by the mechanism of STTV. The components of STTV are defined by several parameters, including a unit vector aligned with the electron momentum, a unit matrix corresponding to the local magnetization direction, and the spin polarization efficiencies at both interfaces of the MTJ, along with the barrier thickness.\n\nIn our research, we provide experimental evidence indicating that the magnitude of STTV increases as a function of the orientation of the external magnetic field applied perpendicular to the film plane. This finding allows us to derive predictions regarding the angular dependence of spin polarization efficiency at each interface, thereby enhancing our understanding of the interplay between magnetic materials in MTJs. Our results contribute valuable insights into the angular relationships of spin polarization, which are crucial for advancing the development of spintronic devices.",
        "ori-fast-z-score": -1.310556084991557,
        "water-fast-z-score": 5.570484990582331,
        "rewrite-fast-z-score": 2.0732842213952645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An Analysis of the Shapes of Interstellar Extinction Curves. V. The IR-Through-UV Curve Morphology .\nAbstract:\nWe have analyzed the shapes of interstellar extinction curves in the infrared through ultraviolet wavelength range using data for more than 100 sight lines with known distances and reddenings, including those obtained by us at Kitt Peak National Observatory (KPNO) and Cerro Tololo Inter-American Observatory (CTIO). We find that all observed extinction curves can be fitted well by a single power law function A(lambda) = lambda -alpha , where alpha is an index ranging between 1.5 to 2.0. This result suggests that there are no significant differences among various types of interstellar dust grains as far as their optical properties are concerned. In addition, we show that the value of alpha correlates strongly with the total-to-selective extinction ratio Rv . These results suggest that the shape of interstellar extinction curve may provide important information on the physical conditions of interstellar matter along individual sight lines. Keywords: Interstellar extinction; Power-law index; Reddening; Dust grain",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An Analysis of the Shapes of Interstellar Extinction Curves . V . The IR - Through - UV Curve Morphology .Abstract : We have analyzed the shapes of interstellar extinction lines in the infrared through ultraviolet frequency range utilizing information for more than 100 sight lines with reported distances and reddenings , notably those acquired by us at Kitt Peak National Observatory ( KPNO ) and Cerro Tololo Inter - American Observatory ( CTIO ) . We see that all observed extinction angles can be fit well by a single power law relation A ( lambda ) = lambda - alpha , where alpha is an index ranging between 1 . 5 to 2 . 0 .This result suggests that there are no major changes among various types of interstellar dust grains as much as their optical properties are concerned . In addition , we find that the value of α correlates heavily with the total - to - selective extinction ratio Rv .These data suggest that the form of interstellar extinction curve might give important information on the physical conditions of interstellar matter along individual sight lines . Keywords : Interstellar extinction ; Power - law index ; Reddening ; Dust grain",
        "rewrite_text": "**Title:** An Analysis of the Shapes of Interstellar Extinction Curves. V. The IR-Through-UV Curve Morphology\n\n**Abstract:** In this study, we investigate the morphology of interstellar extinction curves across the infrared to ultraviolet spectrum, drawing on data from over 100 sight lines with documented distances and reddening values. Notably, our observations were conducted at the Kitt Peak National Observatory (KPNO) and the Cerro Tololo Inter-American Observatory (CTIO). Our analysis reveals that the extinction profiles can be accurately described by a single power law relationship, expressed as A(λ) = λ^(-α), where the power law index α varies between 1.5 and 2.0. This finding indicates a remarkable uniformity in the optical properties of different types of interstellar dust grains, suggesting that significant variations among them are not present in this context. Furthermore, we observe a strong correlation between the value of α and the total-to-selective extinction ratio, Rv. This correlation implies that the characteristics of the interstellar extinction curve may provide critical insights into the physical conditions of interstellar matter along specific sight lines. Our results contribute to the understanding of the interplay between dust grain properties and the observed reddening of starlight, highlighting the importance of extinction curve morphology in astrophysical research. This work underscores the potential of extinction curves as diagnostic tools for probing the nature of interstellar dust and the environmental conditions of the interstellar medium. \n\n**Keywords:** Interstellar extinction; Power-law index; Reddening; Dust grain.",
        "ori-fast-z-score": 0.5698028822981898,
        "water-fast-z-score": 5.356147093602983,
        "rewrite-fast-z-score": 1.1881770515720091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bose-Einstein-condensed gases in arbitrarily strong random potentials .\nAbstract:\nWe study the properties of Bose-Einstein condensates (BECs) with an arbitrary number N of particles trapped by a harmonic oscillator and exposed to a disordered external potential. We show that, for any strength of disorder, there is always a finite fraction of atoms localized at each site of the lattice. The localization length decreases as the disorder increases but remains macroscopic even when the disorder becomes very large compared to the interatomic interaction energy. This result holds true both in one dimension and higher dimensions.  In particular we find that the critical disorder above which all states are localized scales like 1/N in 1D and 1/d in 2D and 3D where d is the spatial dimension. Our results provide a microscopic understanding of recent experiments on ultracold atomic gases in optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases of bosons or fermions confined in periodic potentials  1  . These systems can be described theoretically using the framework of the Bose-Hubbard model  2  , which has been extensively studied over the past decade  3  .\nIn this work we consider the case of a gas of interacting bosons in a disordered potential. Disorder leads to Anderson localization  4  : eigenstates become exponentially localized around their initial position if the disorder exceeds some threshold value  5  . It was recently shown experimentally  6  that such a system exhibits a transition between extended Bloch-like states and localized Wannier-Stark ladders  7, 8  . However, these experiments were performed only in the weak-disorder regime, i.e., when the disorder amplitude V0 is much smaller than the characteristic hopping matrix element J. Here we investigate how the presence of interactions affects the physics of strongly disordered systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bose - Einstein - condensed gases in arbitrarily stable random potentials . Abstract : We research the properties of Bose - Einstein condensates ( BECs ) with an arbitrary number N of atoms trapped by a harmonic oscillator and exposed to a disordered external potential .We see that , for any strength of disorder , there is usually a finite fraction of atoms concentrated at each site of the lattice . The localization width decreases as the disorder advances but remains macroscopic even when the disorder becomes very huge compared to the interatomic interaction power .This result holds true both in one dimension and higher dimensions . In particular we find that the fundamental disorder above which all states are localized scales like 1 / N in 1D and 1 / d in 2D and 3D where d is the spatial dimension .Our results present a microscopic understanding of recent experiments on ultracold atomic atoms in optical lattices . Introduction : - Recent research developments have enabled it able to create quantum degenerate gases of bosons or fermions localized in periodic potentials 1 .These systems can be described theoretically using the framework of the Bose - Hubbard theory 2 , which has been heavily explored over the previous decade 3 . In this research we define the case of a gas of interacting bosons in a disordered potential .Disorder leads to Anderson localization 4 : eigenstates grow exponentially localized around their early position if the disorder approaches some threshold value 5 . It was recently shown experimentally 6 that such a system displays a change between extended Bloch - like states and localized Wannier - Stark ladders 7 , 8 .However , these experiments were performed only in the weak - disturbance regime , i . e . , when the disorder amplitude V0 is much smaller than the typical hopping matrix element J . Here we investigate how the presence of interactions affects the physics of heavily disordered systems .",
        "rewrite_text": "**Title:** Bose-Einstein Condensed Gases in Arbitrarily Stable Random Potentials\n\n**Abstract:** This study investigates the characteristics of Bose-Einstein condensates (BECs) composed of an arbitrary number \\( N \\) of atoms that are confined within a harmonic oscillator potential and subjected to a disordered external field. Our findings reveal that, regardless of the disorder's intensity, a significant fraction of atoms tends to localize at various lattice sites. Notably, while the localization width diminishes with increasing disorder, it remains macroscopic even when the disorder strength vastly exceeds the interatomic interaction strength. This phenomenon is consistent across both one-dimensional and higher-dimensional systems. Specifically, we identify a critical disorder threshold, above which all states become localized, scaling as \\( 1/N \\) in one dimension and \\( 1/d \\) in two and three dimensions, where \\( d \\) represents the spatial dimension. Our results provide a microscopic framework that enhances the understanding of recent experimental observations involving ultracold atoms in optical lattices.\n\n**Introduction:** Recent advancements in research have facilitated the creation of quantum degenerate gases of bosons and fermions that are localized within periodic potentials. These systems can be effectively modeled using the Bose-Hubbard framework, which has been extensively studied over the past decade. In this paper, we focus on a gas of interacting bosons subjected to a disordered potential. The presence of disorder induces Anderson localization, wherein eigenstates become exponentially localized around their initial positions once the disorder surpasses a certain threshold. Recent experimental evidence has demonstrated a transition between extended Bloch-like states and localized Wannier-Stark ladders. However, these experiments were confined to the weak disorder regime, where the disorder amplitude \\( V_0 \\) is significantly smaller than the typical hopping matrix element \\( J \\). In this work, we delve into the impact of interactions on the behavior of systems under substantial disorder, thereby expanding the understanding of BECs in complex potential landscapes.",
        "ori-fast-z-score": -0.7016464154456235,
        "water-fast-z-score": 6.028557990961161,
        "rewrite-fast-z-score": 1.4967665407535604
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Spitzer c2d Survey of Weak-line T Tauri Stars II: New Constraints on the Timescale for Planet Building .\nAbstract:\nWe present new constraints on planet formation timescales by combining infrared photometry and spectroscopy with optical data to study the circumstellar environments of young stars in nearby open clusters. We find that most weak-lined T Tauri stars (WTTS) have disks, but only half as many show evidence for accretion onto their central star. The fraction of WTTSs showing signs of accretion decreases rapidly between ages of 1 Myr and 3 Myr. This suggests that disk dispersal is an important process during this period. In addition, we find that the distribution of dust temperatures in these systems evolves significantly over time. These results are consistent with models where planets form quickly after the dissipation of the protoplanetary disk. However, our sample size is small and additional observations will be needed before firm conclusions can be drawn about how planet formation proceeds. \n \n Keywords: Open cluster, Circumstellar disk",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Spitzer c2d Survey of Weak - line T Tauri Stars II : New Constraints on the Timescale for Planet Building . Abstract : We create additional constraints on moon formation timescales by combining laser photometry and spectroscopy with imaging information to study the circumstellar environments of young galaxies in nearby open complexes .We see that most soft - lined T Tauri stars ( WTTS ) have disks , but only nearly as much demonstrate proof for accretion onto their central star . The percentage of WTTSs displaying signs of accretion decreases quickly between ages of 1 Myr and 3 Myr .This implies that disk dispersal is an important process during this time . In addition , we find that the distribution of dust conditions in these systems evolves greatly over time .These data are compatible with models where planets occur swiftly after the dissipation of the protoplanetary disk . However , our sample volume is tiny and additional observations will be needed before firm conclusions can be drawn about how planet development proceeds .Keywords : Open cluster , Circumstellar disk",
        "rewrite_text": "Title: The Spitzer c2d Survey of Weak-line T Tauri Stars II: New Constraints on the Timescale for Planet Building\n\nAbstract: In this study, we present new insights into the timescales associated with moon formation by integrating laser photometry and spectroscopy with imaging data to investigate the circumstellar environments of young stars within nearby open clusters. Our findings indicate that a significant majority of weak-line T Tauri stars (WTTS) possess circumstellar disks; however, only a comparable fraction shows evidence of accretion onto their central stars. Notably, we observe a rapid decline in the percentage of WTTS exhibiting accretion signatures between the ages of 1 million years (Myr) and 3 Myr, suggesting that the process of disk dispersal plays a critical role during this developmental phase. Furthermore, our analysis reveals substantial changes in the distribution of dust conditions within these systems over time. These observations align with theoretical models proposing that planet formation occurs shortly after the protoplanetary disk dissipates. Nevertheless, it is important to note that our sample size is relatively small, and further observations are essential to establish more definitive conclusions regarding the progression of planet formation. The implications of our findings contribute to the broader understanding of the dynamics of circumstellar disks and their influence on the formation of planetary systems in young stellar environments.\n\nKeywords: Open cluster, Circumstellar disk",
        "ori-fast-z-score": -1.1470786693528088,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": 1.4925557853149838
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spatial and kinematic alignments between central and satellite halos .\nAbstract:\nWe study the spatial and kinematic alignment between central and satellite halos in cosmological N-body simulations, focusing on their dependence on halo mass ratio (q = Msat/Mcen) and redshift z. We find that satellites are preferentially located along filaments connecting to their host galaxy at all redshifts studied here (0 < z < 3). The degree of this alignment increases with decreasing q for fixed z, but decreases with increasing z for fixed q. This trend is consistent with theoretical predictions based on tidal torque theory. In addition, we show that the velocity difference between central and satellite galaxies depends strongly on both q and z. For example, when q > 0.1, the mean radial velocity difference |vr| decreases rapidly as z increases; however, |vr| remains nearly constant over cosmic time if q < 0.01. These results suggest that the observed evolution of the luminosity function may be affected by the dynamical friction acting on satellite galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spatial and kinematic alignments between central and satellite halos . Abstract : We research the spatial and kinematic alignment between central and satellite halos in cosmological N - bodies simulations , concentrating on their dependence on halo weight ratio ( q = Msat / Mcen ) and redshift z .We see that satellites are preferentially positioned along filaments connecting to their host universe at all redshifts studied here ( 0 < z < 3 ) . The degree of this alignment varies with varying q for fixed z , but decreases with expanding z for fixed q .This trend is compatible with theoretical estimates based on tidal torque theory . In addition , we find that the velocity difference between central and satellite galaxies depends strongly on both q and z .For instance , when q > 0 . 1 , the mean radial speed difference | vr | decreases quickly as z increases ; however , | vr | remains virtually constant over cosmic time if g < 0 . 01 . These data suggest that the seen evolution of the luminosity function could be altered by the dynamical friction act on satellite galaxies .",
        "rewrite_text": "In this study, we investigate the spatial and kinematic alignments between central and satellite halos within the framework of cosmological N-body simulations. Our focus is on how these alignments are influenced by the halo weight ratio, defined as \\( q = \\frac{M_{sat}}{M_{cen}} \\), and the redshift \\( z \\). Our findings reveal that satellite halos tend to be preferentially located along filaments that connect to their central hosts across all examined redshifts, ranging from \\( 0 < z < 3 \\). The extent of this spatial alignment is found to be dependent on the halo weight ratio \\( q \\) when the redshift \\( z \\) is held constant; conversely, it diminishes as the redshift increases for a fixed \\( q \\). This observed behavior aligns well with theoretical predictions derived from tidal torque theory.\n\nMoreover, we analyze the kinematic differences between central and satellite galaxies, noting a significant dependence on both the halo weight ratio and redshift. Specifically, for cases where \\( q > 0.1 \\), we observe that the mean radial velocity difference \\( |v_r| \\) decreases rapidly with increasing redshift. In contrast, when \\( q < 0.01 \\), the radial velocity difference remains nearly unchanged over cosmic time. These results imply that the evolution of the luminosity function may be influenced by the effects of dynamical friction acting on satellite galaxies, suggesting a complex interplay between halo dynamics and galaxy formation processes. Overall, our research contributes to a deeper understanding of the structural and kinematic relationships in the cosmic web, highlighting the importance of halo mass ratios and redshift in shaping these alignments.",
        "ori-fast-z-score": -2.03701381619181,
        "water-fast-z-score": 3.624412178045377,
        "rewrite-fast-z-score": 1.7162326606420661
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hamiltonian formalism in Friedmann cosmology and its quantization .\nAbstract:\nWe present the Hamiltonian formulation for general relativity with matter fields on an arbitrary spacetime manifold, including both classical and quantum aspects. The basic idea is to use the ADM decomposition of the metric into space and time components as well as lapse and shift functions. We then introduce canonical momenta conjugate to these variables which are used to construct the primary constraints of the theory. These constraints generate gauge transformations under which all physical quantities must be invariant. In order to obtain the correct number of degrees of freedom we have to impose secondary constraints that eliminate unphysical modes. Finally, we perform the canonical quantization by promoting the phase-space variables to operators acting on wave-functions defined over superspace (the space of all possible metrics). This leads us to the Wheeler-DeWitt equation whose solutions can be interpreted as probability amplitudes between different states of the universe. We also discuss how this approach could be applied to inflationary models.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hamiltonian formalism in Friedmann cosmology and its quantization . Abstract : We present the Hamiltonian formulation for general relativity with matter varieties on an arbitrary spacetime manifold , comprising both classical and quantum aspects .The basic idea is to use the ADM decomposition of the metric into space and period components as well as lapse and shift variables . We then introduce canonical momenta conjugate to these parameters which are applied to build the primary constraints of the model .These restrictions produce gauge transformations under which all physical quantities must be invariant . In order to obtain the appropriate number of degrees of freedom we have to introduce secondary constraints that eliminate unphysical modes .Finally , we perform the canonical quantization by expanding the phase - space variables to operators acting on wave - functions defined over superspace ( the space of all possible metrics ) . This leads us to the Wheeler - DeWitt equation whose solutions can be interpreted as probability amplitudes between various states of the universe .We also discuss how this methodology possible be applied to inflationary systems .",
        "rewrite_text": "Title: Hamiltonian Formalism in Friedmann Cosmology and Its Quantization\n\nAbstract: In this article, we explore the Hamiltonian formulation of general relativity, incorporating various forms of matter within an arbitrary spacetime manifold, addressing both classical and quantum dimensions of the theory. Our approach begins with the ADM (Arnowitt-Deser-Misner) decomposition of the spacetime metric, which separates it into spatial and temporal components, alongside lapse and shift functions. We then define canonical momenta that correspond to these parameters, which are essential for constructing the primary constraints of our model. These constraints yield gauge transformations, ensuring that all physical quantities remain invariant under these transformations. To achieve the correct count of degrees of freedom, we introduce secondary constraints that effectively remove unphysical modes from our system.\n\nSubsequently, we proceed with the canonical quantization process, where we promote the phase-space variables to operators that act on wave functions defined over superspace—the space encompassing all conceivable metrics. This quantization leads us to the Wheeler-DeWitt equation, a pivotal equation in quantum gravity, whose solutions can be interpreted as probability amplitudes representing transitions between different states of the universe. Furthermore, we examine the implications of our methodology for inflationary models, highlighting how the Hamiltonian framework can be adapted to study the dynamics of the early universe. Our findings contribute to a deeper understanding of the interplay between classical and quantum aspects of cosmology, offering insights that may pave the way for future research in the field.",
        "ori-fast-z-score": -0.6546536707079772,
        "water-fast-z-score": 3.6663142889169062,
        "rewrite-fast-z-score": 1.1239029738980328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiscale model of electronic behavior and localization in stretched dry DNA .\nAbstract:\nWe present an atomistic multiscale approach to the study of charge transport through stretched single-stranded DNA (ssDNA). The method combines molecular dynamics simulations with quantum mechanical calculations on small fragments, which are used as input for a tight-binding description of larger systems. We show that this scheme allows us to reproduce experimental results obtained by scanning tunneling microscopy experiments performed at room temperature. In particular we find that our calculated conductance agrees well with experiment when using realistic values for the hopping parameters between neighboring base pairs. Our analysis shows that the main contribution to the current is due to electrons localized along the backbone chain. These findings suggest that ssDNA can be considered as a promising material for future applications such as nanoelectronic devices or sensors. \n \n Introduction \n \n Single stranded DNA has been studied extensively over many years both experimentally  1 - 3  and theoretically  4 - 6  . It was found that its structure depends strongly on environmental conditions like pH value  7  , ionic strength  8  -  10  , solvent  11  , temperature  12  , stretching  13  , etc.. This makes it possible to use ssDNA as a sensor  14  -  16  or even as a nanomaterial  17  -  19  . For example, recent studies have shown that ssDNA can form stable helical structures  20  -  22  . Furthermore, it was demonstrated that ssDNA can act as a template for protein synthesis  23  .\n \nIn addition to these structural properties there is growing interest in understanding how charge carriers move through ssDNA  24  -  26  . Recent theoretical investigations showed that electron transfer rates depend sensitively on the conformation of the molecule  27  -  29  . Experimentally, it was observed that the conductivity decreases exponentially with increasing length  30  -  32  . However, the exact mechanism behind this effect remains unclear  33  . \n \n Here we propose a new computational scheme combining classical molecular dynamics (MD) simulations  34  with density functional theory (DFT)  35  based quantum chemical calculations  36    Fig. 1(a)  . Using this approach we calculate the transmission function T(E), i.e., the probability amplitude for an electron injected into one end of the system to reach the other end",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiscale method of electronic activity and localization in stretched dry DNA . Abstract : We present an atomistic multiscale approach to the study of charge flow through stretched single - stranded DNA ( ssDNA ) .The method mixes molecular mechanics simulations with quantum mechanical calculations on small fragments , which are applied as input for a tight - binding model of bigger systems . We see that this scheme allows us to capture empirical results derived by scanning tunneling microscopy experiments conducted at room temperature .In particular we find that our measured conductance agrees well with experiment when using reasonable values for the hopping characteristics between neighboring base pairs . Our study shows that the main contribution to the current is due to electrons concentrated along the backbone network .These studies imply that ssDNA can be regarded as a viable material for future applications such as nanoelectronic sensors or sensors . Introduction Single stranded DNA has been studied extensively over numerous years both experimentally 1 - 3 and theoretically 4 - 6 .It was shown that its composition depends strongly on environmental conditions like pH value 7 , ionic intensity 8 - 10 , solvent 11 , temperature 12 , stretching 13 , etc . . This gives it able to use ssDNA as a sensor 14 - 16 or even as a nanomaterial 17 - 19 .For instance , recent studies have shown that ssDNA can form stable helical structures 20 - 22 . Furthermore , it was demonstrated that ssDNA can work as a template for protein synthesis 23 .In addition to these functional characteristics there is growing interest in understanding how charge carriers movement through ssDNA 24 - 26 . Recent conceptual investigations indicated that electron transfer rates depend sensitively on the conformation of the molecule 27 - 29 .Experimentally , it was seen that the conductivity decreases exponentially with increasing duration 30 - 32 . However , the exact mechanism behind this effect remains unsure 33 .Here we propose a new computational scheme incorporating classical molecular mechanics ( MD ) simulations 34 with density functional theory ( DFT ) 35 based quantum chemical calculations 36 Fig . 1 ( a ) .Using this methods we determine the transmission function T ( E ) , i . e . , the probability probability for an electron pumped into one end of the system to reach the other end",
        "rewrite_text": "**Title:** Multiscale Method of Electronic Activity and Localization in Stretched Dry DNA\n\n**Abstract:** This study introduces an innovative atomistic multiscale methodology to investigate charge transport in stretched single-stranded DNA (ssDNA). By integrating molecular mechanics simulations with quantum mechanical calculations on smaller DNA fragments, we develop a tight-binding model applicable to larger systems. This hybrid approach effectively replicates empirical findings from scanning tunneling microscopy experiments conducted at room temperature. Our results demonstrate that the conductance values obtained align closely with experimental data when employing realistic parameters for the hopping characteristics between adjacent base pairs. Notably, our analysis reveals that the primary contribution to the electrical current arises from electrons localized along the DNA backbone. These findings suggest that ssDNA holds significant potential as a material for future applications in nanoelectronic devices and sensors.\n\nThe introduction of ssDNA as a subject of extensive research over the years—both experimentally and theoretically—highlights its complex behavior influenced by various environmental factors, including pH, ionic strength, solvent type, temperature, and mechanical stretching. This versatility positions ssDNA as a promising candidate for sensor technology and nanomaterial applications. Recent investigations have confirmed that ssDNA can adopt stable helical conformations and serve as a template for protein synthesis. Concurrently, there is an increasing interest in elucidating the mechanisms governing charge carrier dynamics within ssDNA. Previous studies have indicated that electron transfer rates are highly sensitive to the molecular conformation, while experimental observations have shown an exponential decline in conductivity with prolonged stretching. However, the underlying mechanisms driving these phenomena remain poorly understood.\n\nIn this paper, we propose a novel computational framework that combines classical molecular dynamics simulations with density functional theory-based quantum chemical calculations. This approach enables us to accurately determine the transmission function, T(E), which quantifies the probability of an electron injected at one end of the ssDNA reaching the opposite end. Our findings contribute to a deeper understanding of charge transport in ssDNA and its implications for future technological advancements.",
        "ori-fast-z-score": -0.22941573387056177,
        "water-fast-z-score": 7.222716252062202,
        "rewrite-fast-z-score": 1.7486576189203227
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Building Portable Thread Schedulers for Hierarchical Multiprocessors: the BubbleSched Framework .\nAbstract:\nWe present an open-source framework, called Bubblesched, that allows to build portable thread schedulers for hierarchical multiprocessors (HMPs). The main idea is to use bubbles as scheduling units and schedule them on different levels in HMP hierarchy using a set of rules. We have implemented two schedulers: one based on work stealing and another one based on load balancing. Both schedulers are able to run efficiently on top of Bubblesched without any modifications. Our experimental results show that both schedulers outperform state-of-the-art solutions by up to 3Â times when running parallel applications with fine-grained tasks. In addition, we demonstrate how our scheduler can be used to implement efficient task-parallel algorithms such as graph coloring or matrix multiplication. This research was supported by Russian Science Foundation grant 14-50-00040. We present an opensource framework, called Bubblesched: it allows to build portable threadschedulers for hierarchicalmultiprocessors(HMPs)andrunefficientlyonthemwithoutanymodifications.Theframeworkusesbubblesastheschedu-lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules.Wehaveimplementedtwo-schedulers:onebasedonstealingworkandanotheronesupportedbyloadbalancing.BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified.Ourexperimentalresultsshowthatbothschedulersoutperformstate-oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks.Inaddition,weillustratethattheframeworkcanbeusedtomakeefficienttask-parallelandalgorithmssuchastask-coloringanoregularmatrixmultiplication.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Building Portable Thread Schedulers for Hierarchical Multiprocessors : the BubbleSched Framework . Abstract : We create an free - source framework , entitled Bubblesched , that enables to build portable thread schedulers for hierarchical multiprocessors ( HMPs ) .The main idea is to use bubbles as booking units and pace them on various levels in HMP hierarchy following a setting of rules . We have developed two schedulers : one based on work stealing and another one based on load balancing .Both schedulers are able to run efficiently on top of Bubblesched without any modifications . Our research results show that both schedulers outperform state - of - the - art systems by up to 3Â times when ran parallel clients with fine - grained assignments .In addition , we prove how our scheduler can be used to execute faster job - parallel techniques such as graph coloring or matrix multiplication . This research was supported by Russian Science Foundation award 14 - 50 - 00040 .We create an opensource implementation , entitled Bubblesched : it allows to build portable threadschedulers for hierarchicalmultiprocessors ( HMPs ) andrunefficientlyonthemwithoutanymodifications . Theframeworkusesbubblesastheschedu - lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules . Wehaveimplementedtwo - schedulers : onebasedonstealingworkandanotheronesupportedbyloadbalancing . BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified . Ourexperimentalresultsshowthatbothschedulersoutperformstate - oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks . Inaddition , weillustratethattheframeworkcanbeusedtomakeefficienttask - parallelandalgorithmssuchastask - coloringanoregularmatrixmultiplication .",
        "rewrite_text": "We present an open-source framework called BubbleSched, designed to facilitate the development of portable thread schedulers for hierarchical multiprocessors (HMPs). The core concept of BubbleSched revolves around utilizing \"bubbles\" as scheduling units, which are strategically placed at various levels within the HMP hierarchy according to a predefined set of rules. Within this framework, we have successfully implemented two distinct schedulers: one that employs a work-stealing mechanism and another that focuses on load balancing. Remarkably, both schedulers can operate efficiently on the BubbleSched platform without requiring any modifications.\n\nOur experimental findings demonstrate that these schedulers significantly outperform existing state-of-the-art systems, achieving performance improvements of up to three times when executing parallel applications with fine-grained task assignments. Furthermore, we provide evidence that our framework can effectively support faster job-parallel techniques, including graph coloring and matrix multiplication, enhancing the overall computational efficiency.\n\nThis research was made possible through the support of the Russian Science Foundation under award number 14-50-00040. The BubbleSched framework not only streamlines the process of building thread schedulers for HMPs but also ensures that they can run efficiently without the need for extensive alterations. By leveraging the unique scheduling capabilities of bubbles, we aim to advance the field of parallel computing and provide a robust tool for researchers and developers working with hierarchical multiprocessor systems.",
        "ori-fast-z-score": -1.0392304845413263,
        "water-fast-z-score": 4.817730411281796,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Probing the Relation Between X-ray-Derived and Weak-Lensing-Derived Masses for Shear-Selected Galaxy Clusters: I. A781 .\nAbstract:\nWeak gravitational lensing is an important tool to study dark matter halos in galaxy clusters, but it requires large telescopes or space-based observatories with high angular resolution cameras. In this work we use Chandra data to measure the gas mass fraction (fgas) profiles of two massive galaxy clusters selected by their strong shear signal using HST/ACS images. We compare these fgas measurements with those derived from weak-lensing analysis performed on Subaru/Suprime-Cam imaging data. The comparison shows that both methods agree well within the statistical uncertainties at radii larger than 0.5 r500. At smaller radii there are significant differences between the results obtained with different techniques. These discrepancies may be caused by systematic effects associated with each method and/or by intrinsic scatter among individual clusters. This project was supported by NASA grant NNX10AD65G. We thank J. Richard McInnes for providing us with his software package for fitting the surface brightness profile of galaxy clusters. \n \n Keywords: galaxy cluster, Chandra, weak lensing",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Probing the Relation Between X - ray - Derived and Weak - Lensing - Derived Masses for Shear - Selected Galaxy Clusters : I . A781 . Abstract : Weak gravitational lensing is an important tool to study dark matter halos in galaxy galaxies , but it requires large telescopes or space - based observatories with high angular resolution cameras .In this research we utilize Chandra data to measure the gas mass fraction ( fgas ) profiles of two huge galaxy galaxies determined by their powerful shear response using HST / ACS images . We contrast these fgas measurements with those generated from soft - lensing research performed on Subaru / Suprime - Cam optical data .The comparison shows that both approaches agree well within the statistical uncertainties at radii larger than 0 . 5 r500 . At lower radii there are significant variations between the results derived with various methods .These discrepancies may be caused by systematic effects involved with each approach and / or by intrinsic scatter among different clusters . This project was supported by NASA grant NNX10AD65G .We praise J . Richard McInnes for providing us with his software suite for fitting the surface brightness profile of galaxy galaxies . Keywords : galaxy cluster , Chandra , weak lensing",
        "rewrite_text": "**Title:** Probing the Relation Between X-ray-Derived and Weak-Lensing-Derived Masses for Shear-Selected Galaxy Clusters: I. A781\n\n**Abstract:** Weak gravitational lensing serves as a crucial method for investigating dark matter halos surrounding galaxies, necessitating the use of large telescopes or space-based observatories equipped with high-resolution cameras. In this study, we leverage data from the Chandra X-ray Observatory to analyze the gas mass fraction (fgas) profiles of two massive galaxy clusters, which have been identified based on their significant shear response captured in HST/ACS images. We compare our fgas measurements with those obtained from soft-lensing analyses conducted using Subaru/Suprime-Cam optical data. The results indicate a strong agreement between the two methodologies within the statistical uncertainties for radii exceeding 0.5 r500. However, notable discrepancies arise at smaller radii, suggesting that variations in the derived results may stem from systematic effects inherent to each technique or from intrinsic scatter among different clusters. This research highlights the importance of cross-validating mass estimates derived from different observational techniques to enhance our understanding of galaxy cluster dynamics and the distribution of dark matter. The project received funding from NASA grant NNX10AD65G, and we extend our gratitude to J. Richard McInnes for providing his software suite, which was instrumental in fitting the surface brightness profiles of the galaxy clusters analyzed in this work. \n\n**Keywords:** galaxy cluster, Chandra, weak lensing",
        "ori-fast-z-score": -1.2649110640673518,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": 0.8620436566990363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics of Three Agent Games .\nAbstract:\nWe study the dynamics of three agent games with two strategies each, where agents are connected by an underlying network and play pairwise interactions according to their strategy choices. We show that for any initial state there is always at least one absorbing state in which all agents have the same strategy choice. In addition we find that if the number of nodes with either strategy exceeds 1 then this state can be reached within finite time. Finally, we provide bounds on how fast such convergence occurs as well as conditions under which it will occur exponentially quickly. The results presented here generalize previous work done on two-agent evolutionary games to multi-agent systems. Evolutionary game theory has been used extensively over the past decade to model competition between different species or individuals competing for limited resources  1  . A common approach taken when modeling these types of problems is to consider a population consisting of many interacting agents who choose among several possible strategies  2  , and then use mathematical tools developed in statistical physics  3  to analyze the resulting system behavior  4  .\nIn recent years researchers have begun studying more complex models involving multiple populations  5  , spatial structure  6  , and heterogeneous environments  7, 8  . However, most existing research focuses only on two-player games  9  , while less attention has been paid to multi-agent systems  10  . Here we present new results showing that even simple multi-agent systems exhibit richer dynamical behaviors than previously thought  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics of Three Agent Games . Abstract : We research the dynamics of three agent games with two strategies each , where agents are connected by an underlying network and play pairwise relationships according to their strategy choices .We see that for any initial state there is usually at least one absorbing state in which all agents have the same strategy selection . In addition we find that if the number of vertices with either strategy exceeds 1 then this state can be reached within finite period .Finally , we provide bounds on how rapid such convergence occurs as well as conditions under which it will occur exponentially rapidly . The results presented here generalize past work done on two - agent evolutionary games to multi - agent systems .Evolutionary game theory has been used widely over the previous decade to model competition between various populations or individuals competing for limited supplies 1 . A typical approach took when modeling these kinds of conflicts is to consider a population consisting of several interacting agents who decide among various proposed options 2 , and then use numerical tools developed in mathematical science 3 to analyze the resulting system interaction 4 .In recent years studies have begun investigating more sophisticated models involving several populations 5 , temporal formation 6 , and heterogeneous environments 7 , 8 . However , most existing studies relies only on two - player players 9 , while less attention has been paid to multi - agent systems 10 .Here we present new data demonstrating that even straightforward multi - agent systems exhibit richer dynamical traits than previously thought 11 .",
        "rewrite_text": "Title: Dynamics of Three-Agent Games\n\nAbstract: This study explores the dynamics of three-agent games, where each agent has two strategies and is interconnected through an underlying network. The agents engage in pairwise interactions based on their chosen strategies. Our findings indicate that, regardless of the initial configuration, there is typically at least one absorbing state where all agents adopt the same strategy. Furthermore, we demonstrate that if the number of agents employing either strategy exceeds one, convergence to this absorbing state can occur within a finite timeframe. We also establish bounds on the speed of this convergence and identify conditions that facilitate exponential convergence rates. These results extend previous research on two-agent evolutionary games to encompass multi-agent systems. Over the past decade, evolutionary game theory has been extensively applied to model competition among various populations or individuals vying for limited resources. Traditionally, these models have focused on populations of interacting agents who evaluate multiple options, utilizing numerical methods from mathematical sciences to analyze the resulting interactions. Recent investigations have begun to incorporate more complex models that involve multiple populations, temporal dynamics, and heterogeneous environments. However, the majority of existing research has concentrated on two-player scenarios, with less emphasis on multi-agent systems. In this paper, we present new evidence that even simple multi-agent frameworks exhibit more intricate dynamical behaviors than previously recognized. Our work contributes to the understanding of evolutionary dynamics in multi-agent contexts, highlighting the significance of network structures and strategy interactions in shaping outcomes.",
        "ori-fast-z-score": -0.2544566789039913,
        "water-fast-z-score": 6.88128713803285,
        "rewrite-fast-z-score": 1.7561082210148906
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  2D Path Solutions from a Single Layer Excitable CNN Model .\nAbstract:\nWe propose an excitable convolutional neural network (CNN) model for solving the 2D path planning problem in this work. The proposed method is based on the concept that the output of each layer can be considered as a potential field, and the final solution will emerge when all layers are combined together. We show how to train such a multi-layered CNN using backpropagation through time with gradient clipping. In addition, we also present two different methods to combine multiple fields into one single field by applying either linear or nonlinear combination functions. Finally, we demonstrate our approach on several benchmark problems including maze navigation, robotics motion planning, and autonomous driving. Keywords: Convolutional Neural Network, Backpropagation Through Time, Gradient Clipping, Maze Navigation, Motion Planning, Autonomous Driving. 1 Introduction Convolutional neural networks have been widely used in computer vision applications  1  . Recently, they were applied to solve various types of optimization problems  2  , which include image classification  3  , object detection  4  , semantic segmentation  5  , etc.. However, most existing works focus only on optimizing a single objective function  6  -  8  .\nIn many real-world applications, there may exist more than one objective function  9  . For example, in robotic motion planning  10  , it usually requires finding collision-free paths while minimizing energy consumption  11  ; in autonomous driving  12  , it needs to find safe trajectories under both kinematic constraints  13  and dynamic traffic conditions  14  at the same time; in medical diagnosis  15  , it should consider not only disease prediction  16  but also treatment recommendation  17  simultaneously; in computational biology  18  , it has to optimize protein folding  19  and drug design  20  at the same time. Therefore, it becomes necessary to develop new algorithms to handle multi-objective optimization problems  21  .\nRecently, deep reinforcement learning  22  was introduced to address multiobjective optimization problems  23  . It learns policies directly from raw data without requiring hand-crafted features  24  . However, its performance heavily relies on the quality of training data  25  . Moreover, it often suffers from high sample complexity  26  due to the large number of",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 2D Path Solutions from a Single Layer Excitable CNN Model . Abstract : We suggest an excitable convolutional neural chain ( CNN ) model for solving the 2D route planning problem in this project .The proposed approach is based on the idea that the output of each layer can be regarded as a potential field , and the finished answer will emerge when all layers are united together . We see how to train such a multi - layered CNN use backpropagation through time with gradient clipping .In addition , we also describe two different methods to mix multiple fields into one single field by using either linear or nonlinear combination functions . Finally , we prove our approach on numerous benchmark problems namely maze navigation , robotics motion plan , and autonomous steering .Keywords : Convolutional Neural Network , Backpropagation Through Time , Gradient Clipping , Maze Navigation , Motion Planning , Autonomous Driving . 1 Introduction Convolutional neural connections have been widely useful in computer vision solutions 1 .Recently , they were applied to solve many kinds of algorithms problems 2 , which include image characterization 3 , object discovery 4 , functional segmentation 5 , etc . . However , most existing works concentrate only on optimizing a single objective function 6 - 8 .In many actual - time users , there may contain more than one objective function 9 . For instance , in robotic motion plan 10 , it often needs finding collision - safe paths while minimizing electricity demand 11 ; in autonomous steering 12 , it must to find safe trajectories under both kinematic limits 13 and dynamic transportation conditions 14 at the same time ; in medical treatment 15 , it should consider not only disease prediction 16 but also treatment recommendation 17 simultaneously ; in computational chemistry 18 , it has to optimize enzyme folding 19 and drug design 20 at the same time .Therefore , it becomes necessary to develop new strategies to manage multi - goal optimization problems 21 . Recently , deep reinforcement study 22 was introduced to treat multiobjective optimization problems 23 .It learns policies directly from raw data without using man - crafted features 24 . However , its success strongly depends on the performance of education data 25 .Moreover , it often suffers from high sample complexity 26 due to the huge amount of",
        "rewrite_text": "In this study, we present an innovative excitable convolutional neural network (CNN) model designed to address the challenges of two-dimensional route planning. Our approach conceptualizes the output of each layer in the CNN as a potential field, with the final solution emerging from the integration of all layers. We detail the training process for this multi-layered CNN, employing backpropagation through time alongside gradient clipping techniques to enhance performance. Additionally, we introduce two distinct methods for combining multiple potential fields into a singular field, utilizing either linear or nonlinear combination functions.\n\nTo validate our proposed model, we conduct extensive experiments on a variety of benchmark problems, including maze navigation, robotic motion planning, and autonomous vehicle steering. The results demonstrate the effectiveness of our approach in generating viable path solutions across these complex scenarios.\n\nThe introduction of convolutional neural networks has significantly advanced the field of computer vision, with applications spanning image classification, object detection, and semantic segmentation. However, most existing research has primarily focused on optimizing single objective functions. In real-world applications, it is often necessary to consider multiple objectives simultaneously. For example, in robotic motion planning, it is crucial to identify collision-free paths while minimizing energy consumption. Similarly, autonomous steering requires the identification of safe trajectories that adhere to both kinematic constraints and dynamic environmental conditions. In the medical field, simultaneous consideration of disease prediction and treatment recommendations is essential, while computational chemistry often necessitates the optimization of enzyme folding and drug design concurrently.\n\nGiven the complexity of these multi-objective optimization problems, there is a pressing need for new methodologies. Recent advancements in deep reinforcement learning have shown promise in tackling such challenges by learning policies directly from raw data, bypassing the need for manually crafted features. However, the success of these methods is heavily reliant on the quality of the training data and often encounters issues related to high sample complexity due to the extensive data requirements. This paper aims to contribute to the ongoing discourse on multi-objective optimization by presenting a robust framework that leverages the capabilities of excitable CNNs. \n\nKeywords: Convolutional Neural Network, Backpropagation Through Time, Gradient Clipping, Maze Navigation, Motion Planning, Autonomous Driving.",
        "ori-fast-z-score": 0.37582301400141443,
        "water-fast-z-score": 9.391485505499116,
        "rewrite-fast-z-score": 1.217395285867036
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analysis of thermally stimulated luminescence and conductivity without quasiequilibrium approximation .\nAbstract:\nWe present an analysis of the temperature dependence of thermally stimulated luminescent (TSL) glow curves in terms of the nonstationary electron-phonon relaxation theory, which does not assume that the system is close to equilibrium at any time during its evolution.  We show how this approach can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL data obtained on different types of materials. The results are compared with those obtained by other methods such as photoluminescence excitation spectroscopy or Raman scattering. In particular we demonstrate that our method allows one to determine the energy gap between the conduction band minimum and valence band maximum in semiconductors. This work was supported by Russian Science Foundation grant No. 14-50-00040. DOI: 10.1063/1.4935190 \nI. INTRODUCTORY REMARK\nThe study of luminescence phenomena has been attracting considerable attention over many years because it provides valuable information about electronic structure and optical properties of solids  1  . Thermal stimulation luminescence (TSL), also known as optically stimulated luminescence (OSL), is particularly useful since it enables us to probe the distribution function of electrons excited into the conduction band  2  .\nIn recent decades there have been numerous attempts to develop theoretical models describing various aspects of luminescence processes  3  , including thermal stimulation luminescence  4  -  8  . However, most of these works were based on the assumption that the system under consideration is always close to equilibrium  9  . As a result they cannot describe correctly some important features observed experimentally  10  . For example, the shape of the TSL glow curve depends strongly on the type of material  11  : while in insulators it usually exhibits a single peak  12  , in metals it often consists of several peaks  13  . Moreover, even within the same class of materials, e.g., semiconductor crystals  14  , the number of peaks may vary depending on the doping level  15  . These observations cannot be explained using existing theories  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analysis of thermally stimulated luminescence and conductivity without quasiequilibrium approximation . Abstract : We present an assessment of the temperature dependence of thermally stimulated luminescent ( TSL ) glow curves in terms of the nonstationary electron - phonon relaxation hypothesis , which does not assume that the system is close to equilibrium at any time during its evolve .We see how this methodology can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL information obtained on various types of substances . The results are compared with those achieved by other methods such as photoluminescence excitation spectroscopy or Raman absorption .In particular we prove that our technique permits one to estimate the energy divide between the conduction band threshold and valence band maximum in semiconductors . This project was supported by Russian Science Foundation award No .14-50-00040.DOI: 10.1063/1.4935190 \nI.INTRODUCTORY REMARK The investigation of luminescence effects has been drawing great popularity over numerous years because it gives valuable info about electronic structure and electronic properties of solids 1 . Thermal stimulation luminescence ( TSL ) , sometimes called as optically stimulated luminescence ( OSL ) , is especially interesting since it allows us to probe the distribution behavior of atoms excited into the conduction band 2 .In past decades there have been numerous attempts to develop conceptual models explaining various parts of luminescence systems 3 , notably heat stimulation luminescence 4 - 8 . However , most of these works were based on the assumption that the process under consideration is usually nearly to equilibrium 9 .As a result they cannot describe correctly some important features detected experimentally 10 . For instance , the form of the TSL flicker circle depends strongly on the kind of material 11 : while in insulators it often features a single peak 12 , in metals it often consists of several peaks 13 .Moreover , even within the same category of substances , e . g . , semiconductor crystals 14 , the quantity of peaks may differ depending on the doping level 15 . These measurements cannot be understood using existing theories 16 .",
        "rewrite_text": "**Title:** Analysis of Thermally Stimulated Luminescence and Conductivity Without Quasiequilibrium Approximation\n\n**Abstract:** This study explores the temperature-dependent behavior of thermally stimulated luminescence (TSL) glow curves through the lens of the nonstationary electron-phonon relaxation hypothesis, which diverges from traditional models by not presuming that the system remains close to equilibrium throughout its evolution. Our approach provides a novel framework for extracting critical information regarding the phonon spectrum and the density of states of charge carriers from TSL data collected across various materials. We compare our findings with results obtained through alternative techniques, such as photoluminescence excitation spectroscopy and Raman absorption, highlighting the advantages of our methodology. Notably, we demonstrate that our technique allows for the estimation of the energy gap between the conduction band minimum and the valence band maximum in semiconductor materials. This research is supported by the Russian Science Foundation under award No. 14-50-00040. \n\n**I. INTRODUCTORY REMARKS:** The study of luminescence phenomena has gained significant traction over the years due to its ability to provide insights into the electronic structure and properties of solids. Thermal stimulation luminescence (TSL), also referred to as optically stimulated luminescence (OSL), is particularly intriguing as it enables the investigation of the distribution of atoms excited into the conduction band. Over the past few decades, numerous efforts have been made to develop conceptual models that elucidate various aspects of luminescence systems, particularly in the context of heat stimulation luminescence. However, many of these models have relied on the assumption that the processes involved are typically close to equilibrium, which has limited their ability to accurately describe several experimentally observed phenomena. For instance, the shape of the TSL glow curve is highly material-dependent; insulators often exhibit a single peak, while metals may display multiple peaks. Furthermore, even within the same class of materials, such as semiconductor crystals, the number of peaks can vary significantly based on doping levels. These observations challenge existing theoretical frameworks and underscore the need for a more comprehensive understanding of TSL dynamics.",
        "ori-fast-z-score": -2.3757725695052176,
        "water-fast-z-score": 7.3484692283495345,
        "rewrite-fast-z-score": -2.4659848095803594
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Investigation of relaxation phenomena in high-temperature superconductors HoBa2Cu3O7-d at the action of pulsed magnetic fields .\nAbstract:\nThe influence of pulsed magnetic fields on the relaxation processes in HTSC was investigated by measuring the temperature dependence of resistance and Hall coefficient for samples with different oxygen content (d = 0, 1). The results show that the application of pulsed magnetic fields leads to an increase in the resistivity and Hall mobility of the sample with d = 0. This effect is explained as due to the appearance of additional scattering centers caused by defects formed during the process of magnetization reversal. In contrast, no significant changes were observed in the case of the sample with d=1. It can be assumed that this difference is associated with the presence of structural disordering in the crystal lattice of the latter compound. \n \n Keywords: High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defects Formation, Magnetoresistance, Hall Effect. Introduction \n \n Investigation of relaxation phenomena in high temperature superconductors under the action of pulsed external magnetic fields has been attracting considerable attention recently  1 - 5  . These studies are important both for understanding the physics of these materials and for practical applications  6  -  8  . \n \n In particular, it should be noted that the investigation of relaxation processes in HTSCs allows one to study the dynamics of defect formation  9  , which plays an important role in determining their transport properties  10  . At present there are several models describing the mechanism of defect generation  11  -  13  . However, none of them takes into account the possibility of defect formation induced by the action of pulsed fields  14  . \nExperimental details\n\nIn our work we used single crystals of two compounds with different oxygen content: HoBa 2 Cu 3 O 7−δ (HBS) and YBa 2 Cu 3 O 6+δ (YBS), grown using the floating zone method  15  . The oxygen concentration in the samples was determined by iodometric titration  16  . The typical size of the samples was about 5 × 4 mm 2 . The measurements were carried out in liquid helium cryostats equipped with pulse magnets  17  . The maximum value of the magnetic induction reached up to B max =",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Investigation of relaxed processes in high - temperature superconductors HoBa2Cu3O7 - d at the activity of pulsed magnetic fields . Abstract : The impact of pulsed magnetic fields on the relaxation processes in HTSC was investigated by monitoring the temperature dependence of resistance and Hall coefficient for specimens with various oxygen composition ( d = 0 , 1 ) .The results show that the application of pulsed magnetic fields leads to an increase in the resistivity and Hall mobility of the sample with d = 0 . This phenomenon is understood as owing to the appearance of added scattering centers caused by defects formed during the process of magnetization reversal .In contrast , no major changes were detected in the case of the sample with d = 1 . It can be assumed that this distinction is associated with the presence of structural disordering in the crystal lattice of the latter compound .Keywords : High - Tc Superconductor , Pulsed Magnetic Field , Relaxation Processes , Defects Formation , Magnetoresistance , Hall Effect . Introduction Investigation of relaxed behaviour in high heat superconductors under the effects of pulsed external magnetic waves has been drawing greater notice recently 1 - 5 .These studies are important both for knowledge the physics of these structures and for useful use 6 - 8 . In particular , it should be mentioned that the examination of vibration mechanisms in HTSCs allows one to study the dynamics of defect structure 9 , which plays an important role in establishing their transport properties 10 .At currently there are several models explaining the process of defect generation 11 - 13 . However , none of them took into consideration the possibility of defect formed induced by the activity of pulsed fields 14 .Experimental details In our work we using single crystals of two compounds with varying dioxide content : HoBa 2 Cu 3 O 7−δ ( HBS ) and YBa 2 Cu 3 O 6 + δ ( YBS ) , grown using the floating zone method 15 . The oxygen fraction in the samples was calculated by iodometric titration 16 .The typical size of the tests was about 5 × 4 mm 2 . The tests were carried out in pure helium cryostats fitted with pulse magnets 17 .The maximum value of the magnetic induction reached up to B max =",
        "rewrite_text": "**Title:** Investigation of Relaxation Processes in High-Temperature Superconductors HoBa2Cu3O7−δ under Pulsed Magnetic Fields\n\n**Abstract:** This study explores the effects of pulsed magnetic fields on the relaxation processes in high-temperature superconductors (HTSC), specifically focusing on HoBa2Cu3O7−δ (HBS) with varying oxygen content (δ = 0, 1). By analyzing the temperature-dependent resistance and Hall coefficient of the samples, we observed that the application of pulsed magnetic fields significantly influences the electrical properties of the material. Notably, for the sample with δ = 0, an increase in resistivity and Hall mobility was recorded, which can be attributed to the introduction of additional scattering centers resulting from defects generated during the magnetization reversal process. Conversely, the sample with δ = 1 exhibited minimal changes in its electrical characteristics, suggesting that the structural disorder present in its crystal lattice may play a crucial role in its response to pulsed magnetic fields. \n\nThese findings highlight the importance of understanding the dynamics of defect structures in HTSCs, as they are pivotal in determining the materials' transport properties. The investigation of relaxation behaviors under pulsed magnetic fields has garnered increasing attention in recent years, as it not only enhances our understanding of the underlying physics of these superconductors but also has practical implications for their applications. While several models have been proposed to explain defect generation mechanisms, none have adequately addressed the potential for defect formation induced by pulsed magnetic field activity. \n\nIn our experimental setup, we utilized single crystals of HBS and YBa2Cu3O6+δ (YBS), synthesized via the floating zone method, with oxygen content quantified through iodometric titration. The dimensions of the samples were approximately 5 × 4 mm², and experiments were conducted in a helium cryostat equipped with pulsed magnets, achieving a maximum magnetic induction of B_max. This research contributes to the growing body of knowledge regarding the interplay between pulsed magnetic fields and defect dynamics in high-temperature superconductors, paving the way for future studies in this domain.\n\n**Keywords:** High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defect Formation, Magnetoresistance, Hall Effect.",
        "ori-fast-z-score": -1.4411533842457842,
        "water-fast-z-score": 6.304886387294215,
        "rewrite-fast-z-score": -0.9607689228305227
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Matching WMAP 3-yrs results with the Cosmological Slingshot Primordial Spectrum .\nAbstract:\nWe have recently shown that the primordial spectrum of density fluctuations can be obtained by solving an initial value problem for a massless scalar field in deSitter space-time, which is known as the cosmological slingshot effect (CSE). In this work we show how to match the CSE predictions with those of the Wilkinson Microwave Anisotropy Probe 3-year data release (WMAP3) and compare them against other models. We find that our model fits well within 1-sigma error bars on all parameters except n_s, where it lies just outside 2-sigma limits. The best-fit values are given by:  H_0 = 72.6 +/- 0.9 km/s/Mpc,  Omega_m = 0.26 +/- 0.01,   Omega_Lambda = 0.74 +/- 0.02,    n_s = 0.96 +/- 0.06.   These results agree very well with recent measurements made using Type Ia supernovae.  Our analysis shows that the CSE provides a viable alternative explanation for the origin of cosmic structure formation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Matching WMAP 3 - yrs results with the Cosmological Slingshot Primordial Spectrum . Abstract : We have recently shown that the primordial range of density fluctuations can be obtained by solving an initial value problem for a massless scalar field in deSitter space - time , which is known as the cosmological slingshot phenomenon ( CSE ) .In this project we find how to tie the CSE predictions with those of the Wilkinson Microwave Anisotropy Probe 3 - year data release ( WMAP3 ) and compare them against other models . We see that our model fits well within 1 - sigma mistake bars on all parameters except n _ s , where it lies just outside 2 - sigma limits .The best - fitting values are given by : H _ 0 = 72 . 6 + / - 0 . 9 km / s / Mpc , Omega _ m = 0 . 26 + / - 0 . 01 , Omega _ Lambda = 0 . 74 + / - 0 . 02 , n _ s = 0 . 96 + / - 0 . 06 . These data agree very best with recent observations made using Type Ia supernovae .Our study shows that the CSE provides a viable alternative explanation for the origin of universe formation formation .",
        "rewrite_text": "**Title:** Correlating WMAP 3-Year Results with the Cosmological Slingshot Primordial Spectrum\n\n**Abstract:** In our recent research, we have demonstrated that the primordial spectrum of density fluctuations can be derived by addressing an initial value problem involving a massless scalar field within de Sitter spacetime, a phenomenon we refer to as the cosmological slingshot effect (CSE). This study aims to connect the predictions of the CSE with the findings from the Wilkinson Microwave Anisotropy Probe's 3-year data release (WMAP3) and to evaluate these results against alternative cosmological models. Our analysis reveals that the CSE model aligns well with the WMAP3 data, falling within the 1-sigma confidence intervals for all parameters, with the exception of the spectral index \\( n_s \\), which is marginally outside the 2-sigma limits. The optimal parameter estimates obtained from our model are as follows: \\( H_0 = 72.6 \\pm 0.9 \\) km/s/Mpc, \\( \\Omega_m = 0.26 \\pm 0.01 \\), \\( \\Omega_\\Lambda = 0.74 \\pm 0.02 \\), and \\( n_s = 0.96 \\pm 0.06 \\). Notably, these values exhibit strong concordance with recent measurements derived from Type Ia supernova observations. Our findings suggest that the cosmological slingshot effect offers a compelling alternative framework for understanding the mechanisms behind the formation of the universe, potentially enriching the current cosmological paradigm. This work not only reinforces the validity of the CSE but also invites further exploration into its implications for cosmology and the evolution of the universe.",
        "ori-fast-z-score": -1.116312611302876,
        "water-fast-z-score": 3.4465617474213164,
        "rewrite-fast-z-score": -1.193117518002609
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Role of chain stiffness on the conformation of single polyelectrolytes in salt solutions .\nAbstract:\nWe study theoretically how the conformational properties of flexible and semiflexible chains are affected by electrostatic interactions with ions dissolved in solution, using Monte Carlo simulations for different values of ionic strength I. We find that the persistence length increases as a function of I due to screening effects between charged monomers along the chain backbone. The effect is more pronounced when increasing the charge density per unit length ql or decreasing the Bjerrum length lB = e2/4πε0kBT . In addition we show that the radius of gyration scales differently depending on whether the chain is flexible (Rg ∝ N1/2) or semiflexible (Rg ∝ N1). Finally, we discuss our results within the context of recent experiments performed on DNA molecules immersed into an electrolyte solution. \n \n Introduction \n \n Polymeric systems play important roles in many biological processes such as protein folding  1  , gene regulation  2  , cell division  3  , etc., where they often interact strongly with other macromolecules and/or solvents  4  . For example, it has been shown experimentally  5  that the structure of double stranded DNA can be significantly altered upon interaction with cations  6  . This behavior is also observed in synthetic biopolymers like polypeptides  7, 8  which have attracted considerable attention recently  9  .\n \nIn this work we focus on the role played by electrostatics on the conformation of polymer chains immersed in salt solutions. To do so, we use Monte Carlo simulations  10  to investigate the dependence of the persistence length lp  11  and the radius of gyration: Rg2 = ⟨r2⟩ − ⟨r⟩2 /N  12  on the concentration of added salt C = ∑iZiCi/V , where Zi denotes the valence of species i and Ci its molar concentration  13  . Here V stands for the volume occupied by the system under consideration. \nThe main goal of this investigation is twofold. First, we want to understand how the presence of counterions affects the structural properties of flexible and semifflexible chains. Second, we aim at comparing these theoretical predictions against experimental data obtained for",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Role of chain stiffness on the conformation of single polyelectrolytes in salt solutions . Abstract : We research theoretically how the conformational characteristics of flexible and semiflexible chains are influenced by electrostatic interactions with ions suspended in solution , using Monte Carlo simulations for different values of ionic resistance I .We see that the persistence length changes as a function of I due to screening influences between charged monomers along the chain backbone . The phenomenon is more pronounced when varying the charge density per unit long ql or decreasing the Bjerrum speed lB = e2 / 4πε0kBT .In addition we prove that the radius of gyration scales differently depending on whether the chain is flexible ( Rg [UNK] N1 / 2 ) or semiflexible ( Rg [UNK] N1 ) . Finally , we explain our findings within the context of recent experiments conducted on DNA ions immersed into an electrolyte solution .Introduction Polymeric systems play essential roles in different biological pathways such as protein folding 1 , gene control 2 , cell division 3 , etc . , where they frequently bind closely with other macromolecules and / or solvents 4 . For instance , it has been shown experimentally 5 that the composition of double stranded DNA can be substantially altered upon interaction with cations 6 .This phenomenon is also observed in artificial biopolymers like polypeptides 7 , 8 which have garnered considerable focus recently 9 . In this research we focus on the part played by electrostatics on the conformation of polymer chains immersed in salt solutions .To do so , we using Monte Carlo simulations 10 to examine the dependence of the persistence length lp 11 and the radius of gyration : Rg2 = ⟨ r2 ⟩ − ⟨ r ⟩ 2 / N 12 on the density of added sodium C = [UNK] / V , where Zi denotes the valence of genus i and Ci its molar density 13 . Here V stands for the volume occupied by the system under consideration .The main goal of this study is twofold . First , we want to study how the presence of counterions affects the structural properties of elastic and semifflexible chains .Second , we focus at comparing these theoretical estimates against empirical data acquired for",
        "rewrite_text": "**Title:** The Influence of Chain Stiffness on the Conformation of Single Polyelectrolytes in Salt Solutions\n\n**Abstract:** This study investigates the theoretical implications of electrostatic interactions on the conformational properties of flexible and semiflexible polymer chains in salt solutions, utilizing Monte Carlo simulations across various ionic resistances (I). Our findings reveal that the persistence length of the chains is significantly affected by the ionic resistance due to the screening effects between charged monomers along the polymer backbone. Notably, this effect is amplified when altering the charge density per unit length (ql) or reducing the Bjerrum length (lB = e² / 4πε₀kBT). Furthermore, we demonstrate that the scaling behavior of the radius of gyration (Rg) varies distinctly between flexible and semiflexible chains, with Rg scaling as N¹/² for flexible chains and as N¹ for semiflexible chains. These results are contextualized with recent experimental observations involving DNA ions in electrolyte solutions, highlighting the critical role of electrostatics in polymer behavior. \n\nIn the introduction, we emphasize the significance of polymeric systems in various biological processes, such as protein folding, gene regulation, and cell division, where they often interact closely with other macromolecules and solvents. Experimental evidence has shown that the structural composition of double-stranded DNA can be notably modified upon interaction with cations, a phenomenon also observed in synthetic biopolymers like polypeptides, which have recently attracted considerable research interest. Our investigation centers on the electrostatic contributions to the conformational dynamics of polymer chains in saline environments. To achieve this, we employ Monte Carlo simulations to analyze the relationship between persistence length (lp) and the radius of gyration (Rg² = ⟨r²⟩ - ⟨r⟩² / N) in relation to the concentration of added sodium ions (C = [UNK]/V), where Zi represents the valence of species i and Ci its molar density. The primary objectives of this research are twofold: to elucidate how counterions influence the structural characteristics of elastic and semiflexible chains, and to compare our theoretical predictions with empirical data obtained from relevant experiments.",
        "ori-fast-z-score": -2.459747896071916,
        "water-fast-z-score": 6.1477090237855885,
        "rewrite-fast-z-score": 0.086710996952412
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Micro- and Macrorheological Properties of Isotropically Cross-linked Actin Networks .\nAbstract:\nWe have studied the rheology of isotropic networks formed by crosslinking actin filaments with two different concentrations of biotin-avidin linkers, using microrheology experiments on single filament dynamics in combination with macrorheology measurements performed at low frequencies (0.01-10 Hz). We find that both microand macro-rheology are consistent with an elastic network model for which we can extract values for the number density of links between filaments as well as their stiffness. The results show that increasing the concentration of avidin leads to denser networks with stiffer links. This effect is more pronounced when the initial concentration of actin filaments is higher. Our findings suggest that the mechanical properties of actomyosin gels may be tunable through changes in the amount and/or type of crosslinks present within these systems. In living cells, cytoskeletal structures such as stress fibers or focal adhesions provide physical connections between cell components and play important roles in determining cellular mechanics  1  . These structures consist of bundles of semiflexible biopolymers known as actin filaments  2  , which are connected together via specific protein complexes called crosslinks  3  .\nIn recent years there has been growing interest in understanding how the mechanical properties of biological materials depend on the microscopic structure of the underlying networks  4  . For example, it was shown recently that the viscoelasticity of reconstituted actomyosin gels depends strongly on the presence of myosins  5  . However, despite this progress our knowledge about the relationship between the macroscopic behavior of complex fluids and the microstructure of the constituent building blocks remains limited  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Micro - and Macrorheological Properties of Isotropically Cross - linked Actin Networks . Abstract : We have researched the rheology of isotropic networks formed by crosslinking actin filaments with two different amounts of biotin - avidin linkers , using microrheology experiments on double filament dynamics in combination with macrorheology measurements completed at low frequencies ( 0 . 01 - 10 Hz ) .We see that both microand macro - rheology are compatible with an elastic network theory for which we can extract parameters for the number density of links between filaments as also as their stiffness . The results show that raising the quantity of avidin leads to denser networks with stiffer links .This phenomenon is more pronounced when the first concentration of actin filaments is higher . Our findings show that the mechanical behavior of actomyosin gels might be tunable through alterations in the quantity and / or type of crosslinks observed within these systems .In living cells , cytoskeletal structures such as stress fibers or focal adhesions contribute physical bridges between tissue systems and play crucial roles in determining cellular dynamics 1 . These structures compose of bundles of semiflexible biopolymers known as actin filaments 2 , which are connected together via particular protein complexes called crosslinks 3 .In past decades there has been growing interest in understanding how the mechanical behavior of biological materials depend on the microscopic shape of the underlying systems 4 . For instance , it was shown lately that the viscoelasticity of reconstituted actomyosin gels relies highly on the presence of myosins 5 .However , despite this progress our information about the relationship between the macroscopic behavior of complex fluids and the microstructure of the constituent building components remains restricted 6 .",
        "rewrite_text": "Title: Micro- and Macrorheological Properties of Isotropically Cross-linked Actin Networks\n\nAbstract: This study investigates the rheological properties of isotropic networks created by crosslinking actin filaments using varying concentrations of biotin-avidin linkers. We employed microrheology techniques to analyze the dynamics of double filament interactions, alongside macrorheology assessments conducted at low frequencies (0.01 - 10 Hz). Our findings indicate that both micro- and macrorheological behaviors align with an elastic network theory, allowing us to derive key parameters such as the link density between filaments and their stiffness. Notably, an increase in avidin concentration results in the formation of denser networks characterized by stiffer linkages, a trend that becomes more pronounced at higher initial concentrations of actin filaments. These results suggest that the mechanical properties of actomyosin gels can be finely tuned by modifying the quantity and/or type of crosslinking agents present in the system. In living cells, cytoskeletal structures, including stress fibers and focal adhesions, serve as vital physical connections between tissue systems and are essential for regulating cellular dynamics. These structures are primarily composed of bundles of semiflexible biopolymers known as actin filaments, which are interconnected through specific protein complexes termed crosslinks. Over recent decades, there has been a growing interest in elucidating how the mechanical behavior of biological materials is influenced by the microscopic architecture of their constituent components. Recent studies have demonstrated that the viscoelastic properties of reconstituted actomyosin gels are significantly affected by the presence of myosin proteins. However, despite these advancements, our understanding of the correlation between the macroscopic behavior of complex fluids and the microstructural characteristics of their building blocks remains limited.",
        "ori-fast-z-score": -0.3779644730092272,
        "water-fast-z-score": 7.118052168020874,
        "rewrite-fast-z-score": 1.1441551070947107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Luminosity function of binary X-ray sources calculated using the Scenario Machine .\nAbstract:\nWe present here an application of the Scenario Machine (SM) to calculate the luminosity functions for different types of binaries in our Galaxy, including black hole and neutron star systems as well as white dwarf - main sequence stars. The SM is used to generate synthetic populations of these objects by assuming that they are formed according to some initial mass distribution and evolve through various stages following evolutionary tracks obtained from stellar evolution calculations. We find good agreement between the results of this method with those derived from observations. This work was supported by NASA grant NAG5-10842. Keywords: Binary X-rays Sources; Luminosity Function; Stellar Evolutionary Tracks. 1 Introduction X-ray binaries are composed of either two neutron stars or one neutron star plus another object such as a black hole or a white dwarf. They can be divided into three categories based on their orbital periods; short-period (P orb < 3 hrs), intermediate-period (3 hrs < P orb < 100 days), and long-period (P orb > 100 days). In addition there exist several classes of X-ray transients which have been observed at all periods but whose nature has not yet been determined conclusively  1  . These include soft X-ray transients, supersoft X-ray transients, classical novae, symbiotic stars, recurrent Novae, and microquasars  2  .\nThe number density of X-ray binaries per unit volume depends upon both the formation rate of binaries and how many survive until they become detectable  3  . Since most of them are located within 10 kpc of Earth  4  , it is possible to estimate the total number of X-ray binaries in our galaxy if we know the space density of each type of system  5  . However, since only about 10% of known Galactic X-ray binaries have measured distances  6  , it is difficult to determine the true space densities accurately. Therefore, it becomes necessary to use other methods to obtain estimates of the space density of X-ray binaries  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Luminosity function of binary X - ray sources generated using the Scenario Machine . Abstract : We present here an use of the Scenario Machine ( SM ) to estimate the luminosity functions for different kinds of binaries in our Galaxy , notably black hole and neutron star systems as well as black dwarf - principal sequence stars .The SM is utilized to produce synthetic populations of these objects by assuming that they are created due to some early mass distribution and evolve through several stages following evolutionary tracks taken from stars evolution calculations . We get good agreement between the results of this algorithm with those generated from measurements .This project was supported by NASA grant NAG5 - 10842 . Keywords : Binary X - radiation Sources ; Luminosity Function ; Stellar Evolutionary Tracks .1 Introduction X - ray binaries are composed of either two neutron stars or one neutron star plus another object such as a black hole or a black dwarf . They can be grouped into three categories based on their orbital periods ; long - time ( P orb < 3 hrs ) , intermediate - time ( 3 hrs < P orb < 100 hours ) , and long - time ( P orb > 100 hours ) .In addition there remain many categories of X - ray transients which have been observed at all periods but whose nature has not already been determined conclusively 1 . These include soft X - ray transients , supersoft X - ray transients , classical novae , symbiotic stars , recurrent Novae , and microquasars 2 .The amount density of X - ray binaries per unit volume depends upon both the formation rate of binaries and how many live until they become detectable 3 . Since most of them are situated within 10 kpc of Earth 4 , it is easy to estimate the total number of X - ray binaries in our galaxy if we determine the space density of each type of system 5 .However , since only about 10 % of known Galactic X - ray binaries have recorded distances 6 , it is harder to predict the true space densities accurately . Therefore , it becomes necessary to use other methods to obtain predictions of the space density of X - ray binaries 7 , 8 .",
        "rewrite_text": "**Title:** Luminosity Function of Binary X-ray Sources Generated Using the Scenario Machine\n\n**Abstract:** In this study, we utilize the Scenario Machine (SM) to estimate the luminosity functions of various binary systems within our Galaxy, specifically focusing on black hole and neutron star binaries, as well as black dwarf-principal sequence star systems. The SM generates synthetic populations of these celestial objects by modeling their formation based on initial mass distributions and tracking their evolutionary paths as derived from stellar evolution calculations. Our findings demonstrate a strong correlation between the luminosity functions produced by the SM and those obtained from observational measurements, indicating the reliability of our approach. This research was funded by NASA grant NAG5-10842.\n\n**Keywords:** Binary X-ray Sources; Luminosity Function; Stellar Evolutionary Tracks.\n\n**1. Introduction:** X-ray binaries consist of pairs of compact objects, which may include two neutron stars or a combination of a neutron star with either a black hole or a black dwarf. These systems are classified into three categories based on their orbital periods: short-period (P_orb < 3 hours), intermediate-period (3 hours < P_orb < 100 hours), and long-period (P_orb > 100 hours). Additionally, there exists a variety of X-ray transients that have been detected across all orbital periods, yet their precise classifications remain uncertain. These transients encompass soft X-ray transients, supersoft X-ray transients, classical novae, symbiotic stars, recurrent novae, and microquasars. The spatial density of X-ray binaries is influenced by both their formation rates and their longevity until they become detectable. Given that the majority of these binaries are located within 10 kpc of Earth, estimating the total number of X-ray binaries in our galaxy becomes feasible if we can ascertain the space density of each binary type. However, since only approximately 10% of known Galactic X-ray binaries have measured distances, accurately predicting their true space densities poses a challenge. Consequently, alternative methods are required to derive predictions regarding the space density of X-ray binaries.",
        "ori-fast-z-score": 0.1655211777204736,
        "water-fast-z-score": 6.904757466825006,
        "rewrite-fast-z-score": 2.090605025017727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dust Formation and He II 4686 emission in the Dense Shell of the Peculiar Type Ib Supernova 2006jc .\nAbstract:\nWe present near-infrared (NIR) spectroscopy obtained with Subaru/HDS on day +16 after explosion for the peculiar type Ib supernova SN2006jc, which shows prominent dust formation in its dense shell. The NIR spectrum is dominated by strong P-Cygni profiles of H I Balmer lines and Fe II multiplets at 4233-4245 Å . We find that these features are well reproduced by our non-LTE model calculations assuming an electron density ne = 1 x 10^9 cm-3 , temperature Te = 5500 K , and mass-loss rate oḟ M = 2 x 10^-6 M_sun/yr. In addition to the above mentioned features, we detect weak but significant He II 4686 emission line feature in the red wing of the Hα profile. This suggests that there may be some contribution from helium recombination radiation to the observed fluxes of hydrogenic lines.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dust Formation and He II 4686 emitted in the Dense Shell of the Peculiar Type Ib Supernova 2006jc . Abstract : We report near - infrared ( NIR ) spectroscopy acquired with Subaru / HDS on week + 16 after explosion for the unusual type Ib supernova SN2006jc , which shows significant dust form in its dense shell .The NIR spectrum is dominated by intense P - Cygni profiles of H I Balmer lines and Fe II multiplets at 4233 - 4245 Å . We see that these characteristics are better illustrated by our non - LTE model calculations assuming an electron concentration ne = 1 x 10 ^ 9 centimeters - 3 , temperature Te = 5500 K , and mass - loss rate [UNK] M = 2 x 10 ^ - 6 M _ sun / yr .In addition to the above mentioned features , we find weak but significant He II 4686 emission line feature in the red wing of the Hα profile . This implies that there may be some influence from helium recombination emission to the known fluxes of hydrogenic lines .",
        "rewrite_text": "We present a detailed analysis of near-infrared (NIR) spectroscopy obtained from the Subaru High Dispersion Spectrograph (HDS) during week +16 following the explosion of the peculiar Type Ib supernova SN2006jc. Our observations reveal a notable formation of dust within the dense shell surrounding the supernova. The NIR spectrum is characterized by prominent P-Cygni profiles associated with hydrogen (H I) Balmer lines, as well as multiplet features from iron (Fe II) in the wavelength range of 4233 to 4245 Å. To better understand these spectral characteristics, we employed non-local thermodynamic equilibrium (non-LTE) model calculations, which suggest an electron concentration of ne = 1 x 10^9 cm^-3, a temperature of Te = 5500 K, and a mass-loss rate of approximately 2 x 10^-6 M_sun/yr. \n\nIn addition to the aforementioned spectral features, we detected a weak yet significant emission line corresponding to He II 4686, which appears in the red wing of the Hα profile. This observation indicates the potential influence of helium recombination emissions on the fluxes of the hydrogenic lines, suggesting a complex interplay between the elements in the supernova's environment. Our findings contribute to the understanding of dust formation processes in supernovae and the role of helium in the spectral characteristics of such explosive events. This study highlights the importance of NIR spectroscopy in revealing the physical conditions and elemental compositions present in the aftermath of supernova explosions, particularly in the context of Type Ib supernovae, which are known for their unique properties and behaviors.",
        "ori-fast-z-score": -2.0,
        "water-fast-z-score": 3.25,
        "rewrite-fast-z-score": 0.4879500364742666
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Decision tree modeling with relational views .\nAbstract:\nWe present an approach to decision tree learning that uses views over the data in order to improve efficiency and scalability. The proposed method is based on the idea of using multiple views, each one capturing different aspects or features of the same dataset. We show how this can be achieved by defining a set of views for each node in the decision tree model being learned. These views are then used as input to a modified version of the standard ID3 algorithm which learns the decision tree structure. Our experimental results demonstrate significant improvements in terms of both accuracy and execution time when compared against existing approaches. Decision trees have been widely applied in many areas such as classification, regression analysis, clustering, association rule mining, etc., due to their simplicity and effectiveness. However, they suffer from two main drawbacks: (1) high computational cost; and (2) poor scalability. In particular, the number of possible splits at any given internal node grows exponentially with respect to the size of its parent s training sample. This makes it difficult to build large decision trees efficiently. To address these problems we propose a novel approach called Relational Views-based Decision Tree Learning (RV-DTL). RV-DTL builds upon the concept of multi-view learning where each view captures some aspect(s) of the original data. More specifically, our approach defines a set of views for every internal node in the decision tree being built. Each view corresponds to a specific feature subset associated with the corresponding node. Then, instead of building the entire decision tree from scratch, RV-DTL starts with small sub-trees rooted at leaf nodes and iteratively expands them towards the root until all leaves are reached. At each expansion step, RV-DTL selects the best split among those available according to the information gain criterion. Finally, the final decision tree is constructed by merging together...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Decision tree modeling with relational views . Abstract : We present an way to decision tree study that using views over the information in order to promote efficiency and scalability .The proposed approach is based on the idea of using multiple views , each one capturing different components or elements of the same dataset . We see how this can be obtained by creating a setting of views for each node in the decision tree model being learned .These views are then added as input to a altered version of the standard ID3 algorithm which explains the decision tree shape . Our research results show considerable improvements in terms of both precision and execution time when compared against existing techniques .Decision trees have been widely applied in different areas such as classification , regression evaluation , clustering , association rule extraction , etc . , owing to their simplicity and effectiveness . However , they suffer from two principal drawbacks : ( 1 ) great computational efficiency ; and ( 2 ) poor scalability .In particular , the proportion of possible splits at any certain internal node grows exponentially with regard to the height of its parent s training sample . This creates it difficult to build large decision forests efficiently .To address these problems we undertake a new approach titled Relational Views - based Decision Tree Learning ( RV - DTL ) . RV - DTL relies upon the idea of multi - view processing where each vision reflects some aspect ( s ) of the actual data .More specifically , our approach defines a setting of views for every internal node in the decision tree being built . Each view belongs to a certain feature subset associated with the respective node .Then , rather of building the entire choice tree from scratch , RV - DTL began with little sub - forests rooted at leaf nodes and iteratively builds them towards the root until all leaves are reached . At each expansion stage , RV - DTL selects the best split among those available based to the information gain criterion .Finally , the finished judgment tree is built by merging together . . .",
        "rewrite_text": "**Title:** Decision Tree Modeling with Relational Views\n\n**Abstract:** In this article, we introduce a novel approach to decision tree modeling that leverages relational views of data to enhance efficiency and scalability. Our methodology is predicated on the concept of utilizing multiple views, each representing distinct components or aspects of the same dataset. This is achieved by establishing a set of views for each node within the decision tree model being developed. These views serve as inputs to a modified version of the traditional ID3 algorithm, which facilitates the construction of the decision tree structure.\n\nOur findings demonstrate significant advancements in both accuracy and processing time when juxtaposed with existing methodologies. Decision trees are widely utilized across various domains, including classification, regression analysis, clustering, and association rule mining, due to their inherent simplicity and effectiveness. However, they are often hindered by two major limitations: (1) substantial computational demands and (2) limited scalability. Specifically, the number of potential splits at any given internal node increases exponentially with the height of its parent training sample, complicating the efficient construction of large decision forests.\n\nTo mitigate these challenges, we propose a new framework termed Relational Views-based Decision Tree Learning (RV-DTL). This approach is grounded in the principle of multi-view processing, where each view encapsulates specific characteristics of the underlying data. More precisely, RV-DTL establishes a set of views for each internal node in the decision tree. Each view corresponds to a particular subset of features relevant to the respective node. Rather than constructing the entire decision tree from the ground up, RV-DTL initiates the process with small sub-forests anchored at the leaf nodes, progressively expanding them towards the root until all leaves are integrated. During each phase of expansion, RV-DTL identifies the optimal split based on the information gain criterion. Ultimately, the complete decision tree is formed by merging these sub-forests, resulting in a more efficient and scalable decision tree learning process.",
        "ori-fast-z-score": -1.758853959674307,
        "water-fast-z-score": 7.741954088429138,
        "rewrite-fast-z-score": -1.6654083300081026
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coulomb Drag in Graphene .\nAbstract:\nThe Coulomb drag effect is the phenomenon where one charge carrier can transfer energy to another by exchanging virtual phonons, leading to an electric current in the second carrier that opposes its own motion.  In this work we study the Coulomb drag between two graphene sheets separated by a dielectric spacer layer and subject to different gate voltages. We find that for small separation distances (less than 10 nm) there are significant deviations from the predictions based on the standard theory developed for bulk materials. These deviations arise due to the presence of evanescent modes which couple strongly with the carriers at low energies. For larger separations these effects become negligible as expected. The results presented here provide useful information about how to design devices such as transistors or thermoelectric generators using graphene layers. \nI. INTRODUCTIO N\nGraphene has attracted considerable attention recently because it exhibits unique electronic properties  1  . It consists of carbon atoms arranged into a honeycomb lattice structure and behaves like a two-dimensional electron gas when doped  2  .\nOne interesting property of graphene is the so-called Coulomb drag effect  3  , i.e., the generation of an electric current in a second sheet of electrons moving through a first sheet of electrons even if they do not interact directly  4  . This effect arises because both carriers exchange virtual phonons via their mutual interaction mediated by the substrate  5  . As a result, the current density in the second carrier depends on the velocity of the first carrier  6  . Since the discovery of the Coulomb drag effect in semiconductors  7, 8  many theoretical studies have been performed  9  -  11  . However, only very few experiments were carried out so far  12  -  14  mainly due to difficulties associated with fabricating samples with high quality interfaces  15  . Recently, several groups succeeded in growing high-quality epitaxial graphene  16  -  18  opening up new possibilities for studying the Coulomb drag effect experimentally  19  -  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coulomb Drag in Graphene . Abstract : The Coulomb drag effect is the phenomenon where one charge carrier can carry energy to another by exchanging virtual phonons , leading to an electric current in the second carrier that opposes its own movement .In this research we study the Coulomb drag between two graphene strips separated by a dielectric spacer membrane and subject to different gate voltages . We see that for short separation distances ( fewer than 10 nm ) there are significant deviations from the estimates based on the standard theory derived for bulk surfaces .These deviations occur due to the presence of evanescent modes which couple strongly with the carriers at low energies . For larger separations these influences grow negligible as predicted .The results presented here provide useful details about how to build electronics such as transistors or thermoelectric turbines using graphene strands . I . INTRODUCTIO N Graphene has garnered considerable scrutiny lately because it displays unusual electronic properties 1 .It consists of carbon atoms arranged into a honeycomb lattice structure and behaves like a two - dimensional electron gas when doped 2 . One interesting property of graphene is the so - called Coulomb drag effect 3 , i . e . , the generation of an electric current in a second sheet of atoms moved through a second sheet of atoms even if they do not interact directly 4 .This phenomenon arises because both carriers trade virtual phonons via their mutual interaction mediated by the substrate 5 . As a result , the current density in the second carrier varies on the velocity of the first carrier 6 .Since the discovery of the Coulomb drag effect in semiconductors 7 , 8 numerous conceptual research have been performed 9 - 11 . However , only very few experiments were carried out so far 12 - 14 chiefly due to difficulties related with fabricating samples with high quality interfaces 15 .Recently , various groups helped in growing high - quality epitaxial graphene 16 - 18 providing up new possibilities for studying the Coulomb drag effect experimentally 19 - 21 .",
        "rewrite_text": "**Title: Coulomb Drag in Graphene**\n\n**Abstract:** The Coulomb drag phenomenon describes the interaction whereby one charge carrier can transfer energy to another through the exchange of virtual phonons, resulting in an electric current in the second carrier that opposes its own motion. This study investigates the Coulomb drag effect between two graphene strips separated by a dielectric spacer membrane and subjected to varying gate voltages. Our findings reveal that at short separation distances (less than 10 nm), there are notable deviations from predictions based on conventional theories applicable to bulk materials. These discrepancies arise from the influence of evanescent modes, which exhibit strong coupling with low-energy carriers. As the separation distance increases, these effects diminish, aligning with theoretical expectations. The insights gained from this research are significant for the development of electronic devices, such as transistors and thermoelectric generators, utilizing graphene materials.\n\nGraphene, a two-dimensional structure composed of carbon atoms arranged in a honeycomb lattice, has attracted significant attention due to its remarkable electronic properties. When doped, graphene behaves similarly to a two-dimensional electron gas, making it a compelling subject for investigation. One of its intriguing characteristics is the Coulomb drag effect, which facilitates the generation of an electric current in a second graphene layer that moves in response to the motion of a first layer, despite the absence of direct interaction between them. This effect is mediated by the exchange of virtual phonons through the substrate, leading to a current density in the second layer that is dependent on the velocity of the first layer. Since the initial observation of the Coulomb drag effect in semiconductors, extensive theoretical research has been conducted; however, experimental validation has been limited due to challenges in producing high-quality interfaces. Recent advancements in the growth of high-quality epitaxial graphene have opened new avenues for experimental exploration of the Coulomb drag effect, paving the way for further understanding and potential applications in advanced electronic systems.",
        "ori-fast-z-score": 1.6222142113076254,
        "water-fast-z-score": 8.327056459580765,
        "rewrite-fast-z-score": 2.182820625326997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observation of Electron-Hole Puddles in Graphene Using a Scanning Single Electron Transistor .\nAbstract:\nWe report the observation of electron-hole puddles in graphene using scanning single-electron transistors (SETs). The SET is fabricated on top of an exfoliated monolayer graphene flake and operated at cryogenic temperatures down to 4 K. We observe that the conductance through the SET depends strongly on its position with respect to the underlying graphene sheet, which we attribute to local variations in charge carrier density induced by charged impurities trapped between the substrate and the graphene layer. This effect can be suppressed by applying a gate voltage Vg = -40 V across the graphene sample. Our results demonstrate that the use of SETs as probes for studying electronic properties of two-dimensional materials such as graphene has great potential. In recent years there have been significant advances in the fabrication of devices based on carbon nanotubes  1  , silicon nanowires  2  or semiconductor quantum dots  3  . These nanostructures are used as active elements in various types of sensors  4  , optoelectronic  5  and photovoltaic  6  applications. However, these structures suffer from several drawbacks including poor reproducibility due to their small size and low yield during growth processes  7, 8  .\nIn contrast, graphene  9  offers many advantages over other two dimensional materials  10  : it is mechanically flexible  11  , chemically stable  12  , biocompatible  13  and electrically conductive  14  . Moreover, it can be produced in large quantities via chemical vapor deposition  15  or mechanical exfoliation  16  techniques  17  . Recently, graphene-based field-effect transistors  18  were demonstrated  19, 20  opening up new avenues towards high-performance electronics  21  . Despite all these attractive features, however, one major challenge remains in achieving high-quality electrical contacts to graphene  22  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observation of Electron - Hole Puddles in Graphene Using a Scanning Single Electron Transistor . Abstract : We report the observation of electron - hole puddles in graphene using scanning single - ion transistors ( SETs ) .The SET is manufactured on top of an exfoliated monolayer graphene flake and operated at cryogenic temperatures down to 4 K . We see that the conductance through the SET depends strongly on its position with regard to the underlying graphene sheet , which we attribute to regional differences in charge carrier density resulting by charged impurities stored between the substrate and the graphene layer . This phenomenon can be suppressed by using a gate pressure Vg = - 40 V across the graphene sample .Our results show that the using of SETs as probes for studying electronic properties of two - dimensional surfaces such as graphene has tremendous promise . In recent years there have been significant advances in the fabrication of applications based on carbon nanotubes 1 , silicon nanowires 2 or semiconductor quantum dots 3 .These nanostructures are applied as active elements in different kinds of sensors 4 , optoelectronic 5 and photovoltaic 6 applications . However , these structures face from several drawbacks including low reproducibility due to their tiny size and low yield during growth processes 7 , 8 .In comparison , graphene 9 offers several advantages over other two dimensional polymer 10 : it is mechanically flexible 11 , chemically neutral 12 , biocompatible 13 and electrically conductive 14 . Moreover , it can be made in large quantities via molecular vapor deposition 15 or mechanical exfoliation 16 techniques 17 .Recently , graphene - based field - effect transistors 18 were shown 19 , 20 closing up new avenues towards high - performance devices 21 . Despite all these appealing characteristics , however , one major challenge survives in obtaining high - grade electrical contacts to graphene 22 .",
        "rewrite_text": "We present our findings on the observation of electron-hole puddles in graphene, utilizing scanning single-electron transistors (SETs). The SETs were fabricated atop an exfoliated monolayer graphene flake and were operated at cryogenic temperatures, reaching as low as 4 K. Our experiments reveal a significant dependence of the conductance through the SET on its spatial positioning relative to the graphene sheet. This variation is attributed to localized differences in charge carrier density, which arise from charged impurities situated between the substrate and the graphene layer. Notably, we found that applying a gate voltage of Vg = -40 V can effectively mitigate this phenomenon. Our results underscore the potential of SETs as effective probes for investigating the electronic properties of two-dimensional materials like graphene.\n\nIn recent years, there has been considerable progress in the development of applications based on nanostructures such as carbon nanotubes, silicon nanowires, and semiconductor quantum dots. These nanostructures have been employed as active components in various sensors, optoelectronic devices, and photovoltaic systems. However, they often suffer from limitations, including low reproducibility due to their diminutive size and challenges in achieving high yields during fabrication processes. In contrast, graphene presents several advantages over other two-dimensional materials: it is mechanically flexible, chemically stable, biocompatible, and exhibits excellent electrical conductivity. Furthermore, graphene can be produced in large quantities through methods like molecular vapor deposition and mechanical exfoliation.\n\nRecently, graphene-based field-effect transistors have opened new pathways for the development of high-performance electronic devices. Despite these promising attributes, a significant hurdle remains in establishing high-quality electrical contacts to graphene. Our study contributes to the understanding of electron-hole puddles in graphene and highlights the capabilities of SETs in probing the intricate electronic landscape of this remarkable material.",
        "ori-fast-z-score": -0.9053574604251853,
        "water-fast-z-score": 5.8608526253383095,
        "rewrite-fast-z-score": 1.0524696231684352
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Variations in the lithium abundances of turn off stars in the globular cluster 47 Tuc .\nAbstract:\nWe have measured the Li abundance for 16 turn-off (TO) stars in the metal-rich open cluster 47 Tucanae, using high-resolution spectra obtained with UVES at VLT-UT2 telescope.  The TO stars are located between 0.8 and 1.0 solar radii above the main sequence turnoff point on the colour-magnitude diagram. We find that all but one star show an overabundance of Li compared to the Sun by factors ranging from 2 up to 6 times its solar value. This is consistent with previous studies which found similar results for other clusters such as M71 or NGC6397. However, we also find evidence for significant star-to-star scatter in the Li content among these TO stars. In particular, two out of our sample exhibit very low values of log(Li/H) = +0.3 dex and +0.4 dex respectively, while most of them display higher values around +1.1 dex.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Variations in the lithium abundances of turn off stars in the globular cluster 47 Tuc . Abstract : We have tested the Li abundance for 16 change - off ( TO ) stars in the metal - rich open cluster 47 Tucanae , using high - resolution spectra obtained with UVES at VLT - UT2 telescope .The TO stars are located between 0 . 8 and 1 . 0 solar radii above the main sequence turnoff point on the colour - magnitude diagram . We see that all but one star show an overabundance of Li compared to the Sun by factors ranging from 2 up to 6 times its solar value .This is compatible with previous research which showed similar results for other complexes such as M71 or NGC6397 . However , we also find proof for significant star - to - star scatter in the Li concentration among these TO stars .In particular , two out of our sample present very low values of log ( Li / H ) = + 0 . 3 dex and + 0 . 4 dex respectively , while most of them display larger values around + 1 . 1 dex .",
        "rewrite_text": "Title: Variations in Lithium Abundances of Turn-Off Stars in the Globular Cluster 47 Tucanae\n\nAbstract: In this study, we investigate the lithium (Li) abundances in 16 turn-off (TO) stars located in the metal-rich globular cluster 47 Tucanae. Utilizing high-resolution spectra obtained from the UVES instrument at the VLT-UT2 telescope, we analyze the Li content of these stars, which are situated between 0.8 and 1.0 solar radii above the main sequence turn-off point on the color-magnitude diagram. Our findings reveal that nearly all of the TO stars exhibit Li abundances that are significantly higher than that of the Sun, with enhancements ranging from two to six times the solar value. This observation aligns with previous studies conducted on other stellar complexes, such as M71 and NGC 6397, which reported similar overabundances of lithium. However, our research also uncovers notable star-to-star variations in Li concentrations among the TO stars in 47 Tuc. Specifically, we identify two stars within our sample that display markedly low lithium levels, with log(Li/H) values of +0.3 dex and +0.4 dex, respectively. In contrast, the majority of the stars exhibit higher lithium abundances, averaging around +1.1 dex. These results highlight the complexity of lithium enrichment processes in stellar populations and suggest that individual stellar evolution may play a significant role in determining Li abundances in turn-off stars within globular clusters. Further investigation into the mechanisms behind these variations could provide deeper insights into the nucleosynthesis and evolutionary history of stars in dense stellar environments.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 3.2504180333157686,
        "rewrite-fast-z-score": 1.9095718489925029
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Physical instrumental vetoes for gravitational-wave burst triggers .\nAbstract:\nWe present the results of an analysis to determine whether physical vetoes can be used as part of a pipeline to reduce false alarms in searches for gravitational waves (GWs) from binary black hole mergers and other astrophysical sources.  We use data collected by the LIGO detectors during their fifth science run, which took place between September 2005 and January 2007. The search pipeline is based on matched filtering with template waveforms that are generated using post-Newtonian expansions up to 3PN order. In addition to standard cuts applied to the signal-to-noise ratio (SNR), we also apply two different types of physical vetoes:  1) Vetoing events whose SNRs exceed some threshold value when they occur simultaneously at multiple detector sites; 2) Vetoing events where there is evidence of excess power above background noise levels in the frequency bands below 100 Hz or above 1000 Hz. For each type of veto, we define a set of parameters that control its effectiveness. Using these parameters, we perform Monte Carlo simulations to study how well the vetoes reject simulated signals injected into real detector data. Our main result shows that both types of physical vetoes significantly improve our ability to detect GW signals while keeping the number of false positives low.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Physical instrumental vetoes for gravitational - wave burst triggers . Abstract : We report the results of an assessment to identify whether physical vetoes can be used as part of a pipeline to reduce false alarms in searches for gravitational waves ( GWs ) from binary white hole mergers and other astrophysical sources .We use data accumulated by the LIGO detectors during their fifth science run , which taken place between September 2005 and January 2007 . The search pipeline is based on matched sampling with template waveforms that are produced using post - Newtonian expansions up to 3PN order .In addition to standard cuts applied to the signal - to - noise proportion ( SNR ) , we also apply two different kinds of physical vetoes : 1 ) Vetoing events whose SNRs reach some threshold value when they occur simultaneously at multiple detector sites ; 2 ) Vetoing events where there is evidence of excess energy above background noise levels in the frequency bands below 100 Hz or above 1000 Hz . For each type of veto , we define a setting of constraints that influence its effectiveness .Using these parameters , we perform Monte Carlo simulations to study how best the vetoes reject simulated messages fed into real detector data . Our main success shows that both types of physical vetoes significantly boost our effectiveness to identify GW signals while staying the number of false positives small .",
        "rewrite_text": "In this study, we investigate the potential of physical vetoes as a means to enhance the detection pipeline for gravitational wave (GW) signals, specifically those originating from binary white hole mergers and other astrophysical phenomena. Utilizing data collected by the LIGO detectors during their fifth science run, which spanned from September 2005 to January 2007, we aim to reduce the incidence of false alarms in GW searches. Our detection methodology employs matched filtering techniques with template waveforms generated through post-Newtonian expansions up to the third post-Newtonian (3PN) order.\n\nTo improve the reliability of our signal detection, we implement two distinct types of physical vetoes in addition to the conventional signal-to-noise ratio (SNR) thresholds. The first veto targets events that exhibit SNRs surpassing a predetermined threshold when detected simultaneously across multiple LIGO sites. The second veto focuses on identifying events that display excess energy above the background noise levels in frequency ranges below 100 Hz and above 1000 Hz. For each veto type, we establish a set of constraints that dictate their operational effectiveness.\n\nTo evaluate the performance of these vetoes, we conduct Monte Carlo simulations that assess their ability to filter out simulated signals embedded within actual detector data. Our findings indicate that both physical veto strategies significantly enhance our capacity to accurately identify genuine GW signals while effectively minimizing the occurrence of false positives. This research underscores the importance of integrating physical vetoes into GW detection pipelines, paving the way for more reliable astrophysical observations and a deeper understanding of the universe's most enigmatic events.",
        "ori-fast-z-score": -0.09578262852211514,
        "water-fast-z-score": 5.030537377488245,
        "rewrite-fast-z-score": -0.5432144762551112
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraints on the merging timescale of luminous red galaxies, or, where do all the halos go? .\nAbstract:\nWe present new constraints on the merger rate density and mass ratio distribution for luminous red galaxies (LRGs) using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use these results to place limits on the number of massive dark matter haloes that are not associated with LRG hosts in the local universe. The halo occupation model we employ is based upon an extension of the HOD formalism developed by Zheng et al. (2005) , which allows us to simultaneously constrain both the mean occupation function and its scatter at fixed luminosity as well as the fraction of satellite galaxies that have been recently accreted into their host haloes. Our analysis indicates that there exists a significant population of massive haloes that are not occupied by LRGs within our sample volume. This result suggests that either many of the most massive haloes were assembled very early during cosmic history and/or that they contain only low-mass galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraints on the merging timescale of luminous red objects , or , where do all the halos go ? .Abstract : We create additional constraints on the merger rate volume and mass ratio distribution for luminous red objects ( LRGs ) using data from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) . We use these results to place limits on the quantity of large dark matter haloes that are not associated with LRG hosts in the local universe .The halo occupation model we utilize is based upon an extension of the HOD formalism developed by Zheng et al . ( 2005 ) , which allows us to jointly constrain both the mean occupation function and its scatter at fixed luminosity as well as the fraction of satellite galaxies that have been lately accreted into their host haloes .Our study implies that there exists a substantial population of large haloes that are not populated by LRGs within our sample volume . This result suggests that either several of the most large haloes were assembled very early during cosmic history and / or that they contain only low - mass stars .",
        "rewrite_text": "In this study, we present new constraints on the merger rate, volume, and mass ratio distribution of luminous red objects (LRGs) by analyzing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our findings enable us to establish limits on the number of significant dark matter haloes that lack associations with LRG hosts in the local universe. We employ a halo occupation model that builds upon the halo occupation distribution (HOD) framework introduced by Zheng et al. (2005). This model allows us to simultaneously constrain the mean occupation function and its variance at a fixed luminosity, as well as the proportion of satellite galaxies that have recently been accreted into their respective host haloes. Our analysis indicates a considerable population of large haloes that remain unoccupied by LRGs within the volume of our sample. This observation raises intriguing questions regarding the evolutionary history of these haloes, suggesting that a significant number may have formed early in cosmic time or that they predominantly contain low-mass stars. These insights contribute to our understanding of the distribution and evolution of dark matter haloes in relation to luminous red objects, shedding light on the broader dynamics of galaxy formation and the fate of massive haloes in the universe.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": 0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Two new basaltic asteroids in the Outer Main Belt? .\nAbstract:\nWe report on two newly discovered small (D < 30 km) asteroids, which are likely to be basaltic and have orbits similar to those of CERES and CERES-2. The first one is designated as CERES-3 and was found by the Catalina Sky Survey during its survey for NEAs. It has an absolute magnitude H = 18.6 ± 0.1 mag., a diameter D ~ 20 km, and an albedo p V = 0.04-0.06. Its orbit is nearly circular with e = 0.05 ± 0.01 and q = 2.84 AU. The second asteroid, designated as CERES-4, was found by the Asteroid Terrestrial-impact Last Alert System (ATLAS). It also has an absolute magnitude H ~ 19 mag., but it may be larger than CERES-3 because its diameter estimate ranges between 25-30 km. Its orbital parameters are: e = 0.07 ± 0.02; q = 3.03 AU. Both objects belong to the outer part of the main belt where most known asteroids are S-type or C-type. Their low albedos suggest that they might be primitive bodies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Two newest basaltic asteroids in the Outer Main Belt ? .Abstract : We report on two recently discovered narrow ( D < 30 km ) asteroids , which are likely to be basaltic and have orbits related to those of CERES and CERES - 2 . The first first is identified as CERES - 3 and was seen by the Catalina Sky Survey during its study for NEAs .It has an absolute magnitude H = 18 . 6 ± 0 . 1 mag . , a diameter D ~ 20 km , and an albedo p V = 0 . 04 - 0 . 06 . Its orbit is almost circular with e = 0 . 05 ± 0 . 01 and q = 2 . 84 AU .The second asteroid , designated as CERES - 4 , was found by the Asteroid Terrestrial - impact Last Alert System ( ATLAS ) . It also has an absolute magnitude H ~ 19 mag . , but it could be larger than CERES - 3 because its size estimate ranges between 25 - 30 kilometers .Its orbital characteristics are : e = 0 . 07 ± 0 . 02 ; q = 3 . 03 AU . Both bodies belong to the outer portion of the main belt where most notable asteroids are S - class or C - class .Their low albedos appear that they may be prehistoric bodies .",
        "rewrite_text": "We present findings on two newly identified narrow asteroids, both of which are likely basaltic in composition and possess orbits that are associated with those of Ceres and Ceres-2. The first asteroid, designated as Ceres-3, was observed by the Catalina Sky Survey during its investigation of near-Earth asteroids (NEAs). Ceres-3 has an absolute magnitude of H = 18.6 ± 0.1 mag, a diameter of approximately 20 km, and an albedo ranging from pV = 0.04 to 0.06. Its orbit is characterized by a low eccentricity of e = 0.05 ± 0.01 and a perihelion distance of q = 2.84 AU. The second asteroid, named Ceres-4, was discovered by the Asteroid Terrestrial-impact Last Alert System (ATLAS). This asteroid has an absolute magnitude of around H ~ 19 mag, and its size is estimated to be larger than that of Ceres-3, with a diameter ranging between 25 and 30 kilometers. The orbital parameters for Ceres-4 include an eccentricity of e = 0.07 ± 0.02 and a perihelion distance of q = 3.03 AU. Both asteroids are located in the outer region of the main asteroid belt, where the majority of known asteroids are classified as either S-type or C-type. The low albedo values of these bodies suggest that they may be remnants from the early solar system, potentially offering insights into the primordial materials that formed the terrestrial planets. This study contributes to our understanding of the composition and evolution of asteroids in the outer main belt, highlighting the significance of Ceres-3 and Ceres-4 in ongoing astronomical research.",
        "ori-fast-z-score": -2.5655583314824097,
        "water-fast-z-score": 2.42535625036333,
        "rewrite-fast-z-score": -0.20851441405707477
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Some properties of the resonant state in quantum mechanics and its computation .\nAbstract:\nThe problem of finding the energy levels of an electron bound to a nucleus is one of the most important problems in physics, which has been studied for many years. In this article we consider the case where there are two electrons with opposite spins that can occupy different orbitals around the same nucleus. We show how to compute these states using only polynomial time computations on classical computers. This result was obtained by applying some techniques developed recently in computational complexity theory such as the PCP theorem (the probabilistic checkable proof) and the local testability of CSPs (constraint satisfaction problems). The results presented here have applications not only in theoretical physics but also in computer science. For example they provide new insights into the structure of NP-complete problems. Quantum mechanical systems play an essential role in modern physics. One of their main features is that particles may be found in superposition of several states at once. A famous example is Schrödinger s cat experiment  1  . Another feature is entanglement  2  , i.e., correlations between particles that cannot be explained classically  3  .\nIn this work we study the following problem: given a system consisting of N spin-1/2 particles, what is the ground state? That means, if all particles were measured simultaneously, what would be the probability distribution over the possible outcomes?\nWe will focus our attention on the simplest non-trivial case: two spin-½ particles occupying different orbitals around the nucleus  4  . It turns out that it is sufficient to solve this problem in order to find the ground state of any number of particles  5  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Some properties of the resonant state in particle theory and its computation . Abstract : The question of finding the power concentrations of an electron bound to a nucleus is one of the most important problems in physics , which has been studied for many years .In this article we imagine the case where there are two electrons with opposite spins that can occupy separate orbitals around the same nucleus . We see how to compute these states using only polynomial period computations on classical processors .This result was obtained by using some techniques established recently in computational complexity analysis such as the PCP conjecture ( the probabilistic checkable proof ) and the local testability of CSPs ( constraint satisfaction issues ) . The results presented here have applications not only in theoretical physics but also in computer science .For instance they give modern perspectives into the formation of NP - perfect questions . Quantum mechanical problems hold an essential part in modern physics .One of their major characteristics is that particles may be found in superposition of several states at once . A popular example is Schrödinger s cat experiment 1 .Another feature is entanglement 2 , i . e . , correlations between particles that cannot be described classically 3 . In this research we study the following puzzle : given a system consisting of N spin - 1 / 2 atoms , what is the ground state ?That implies , if all atoms were studied simultaneously , what would be the probability distribution over the possible outcomes ? We will focus our focus on the trivial non - trivial case : two spin - ½ particles occupying distinct orbitals around the nucleus 4 .It turns out that it is enough to solve this question in order to find the ground state of any number of particles 5 .",
        "rewrite_text": "**Title:** Some Properties of the Resonant State in Particle Theory and Its Computation\n\n**Abstract:** The investigation of power concentrations of electrons bound to a nucleus remains a pivotal challenge in the field of physics, having been the subject of extensive research over the years. This article explores a scenario involving two electrons with opposite spins that occupy distinct orbitals around a single nucleus. We present a method for computing these states utilizing polynomial-time algorithms on classical computing systems. This advancement is made possible through the application of recent techniques in computational complexity theory, including the Probabilistically Checkable Proof (PCP) conjecture and the local testability of Constraint Satisfaction Problems (CSPs). The findings have significant implications not only for theoretical physics but also for computer science, particularly in providing fresh insights into the formation of NP-complete problems.\n\nQuantum mechanics plays a crucial role in contemporary physics, characterized by phenomena such as superposition, where particles exist in multiple states simultaneously, exemplified by Schrödinger's cat thought experiment. Another fundamental aspect is entanglement, which describes correlations between particles that defy classical explanation. In this study, we address the intriguing question of determining the ground state of a system composed of N spin-1/2 atoms. Specifically, we investigate the probability distribution of outcomes when all atoms are analyzed concurrently. Our primary focus is on the case of two spin-1/2 particles in separate orbitals around a nucleus, which serves as a foundational problem. Remarkably, solving this specific case provides the necessary framework to ascertain the ground state for systems with any number of particles. This research not only enhances our understanding of resonant states in particle theory but also bridges connections between physics and computational methodologies.",
        "ori-fast-z-score": -0.780398972571708,
        "water-fast-z-score": 5.983058789716428,
        "rewrite-fast-z-score": 2.32379000772445
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SDSS J080531.84+481233.0: An Unresolved L Dwarf/T Dwarf Binary .\nAbstract:\nWe report the discovery and characterization of an unresolved binary system consisting of two late-type dwarf stars, SDSS J080531.8+481233.0AB (hereafter referred to as J0805+4812). The primary component is classified as a T6p dwarf star with a mass of ~70 MJup while its companion has been identified as a cool brown dwarf candidate with a temperature between 1000-2000 K. We have used high-resolution near-infrared spectroscopy obtained at Gemini Observatory in order to confirm that both components are gravitationally bound. Our analysis shows that this object is one of the most massive known binaries composed by two low-mass objects. This work was supported by NASA s Astrophysics Data Analysis Program under award NNX10AD20G issued through the Space Telescope Science Institute, which is operated by AURA for NASA under contract NAS 5-26555. In addition we acknowledge support from NSF grant AST-0908816. \n \n We present the discovery and characterization of a new unresolved binary system made up of two late type dwarfs, SDSSJ080531.8 + 481233.0 AB (hereafter J0805+4812; see Figure 1 ). The primary component is classified spectroscopically as a T6p star with a mass of about 70 M Jup , while its companion has been tentatively identified as a cool brown-dwarf candidate with temperatures ranging from 1000 - 2000K . Using high resolution infrared spectroscopy taken at Gemini Observatory , we show that these two objects are gravitationally bound . This makes it one of the most massive systems ever found containing two low-mass objects .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SDSS J080531 . 84 + 481233 . 0 : An Unresolved L Dwarf / T Dwarf Binary . Abstract : We report the discovery and characterization of an unresolved binary system consisting of two late - class dwarf stars , SDSS J080531 . 8 + 481233 . 0AB ( hereafter referred to as J0805 + 4812 ) .The main component is categorized as a T6p dwarf star with a mass of ~ 70 MJup while its companion has been described as a cool brown giant candidate with a temperature between 1000 - 2000 K . We have utilized large - resolution near - infrared spectroscopy acquired at Gemini Observatory in order to confirm that both components are gravitationally bound . Our study shows that this body is one of the most large known binaries consisting by two low - mass bodies .This project was supported by NASA s Astrophysics Data Analysis Program under award NNX10AD20G submitted through the Space Telescope Science Institute , which is operated by AURA for NASA under contract NAS 5 - 26555 . In addition we accept support from NSF grant AST - 0908816 .We present the discovery and description of a new unresolved binary system composed up of two late type dwarfs , SDSSJ080531 . 8 + 481233 . 0 AB ( hereafter J0805 + 4812 ; view Figure 1 ) . The main component is categorized spectroscopically as a T6p star with a mass of about 70 M Jup , while its companion has been tentatively described as a cool brown - giant candidate with temperatures ranging from 1000 - 2000K .Using fast resolution infrared spectroscopy done at Gemini Observatory , we find that these two bodies are gravitationally bound . This gives it one of the most large systems ever found bearing two low - mass bodies .",
        "rewrite_text": "We present the discovery and detailed characterization of an unresolved binary system comprising two late-type dwarf stars, designated SDSS J080531.84 + 481233.0AB, henceforth referred to as J0805 + 4812. The primary component of this binary is classified as a T6p dwarf star, with an estimated mass of approximately 70 Jupiter masses (MJup). Its companion is identified as a potential cool brown giant candidate, exhibiting a temperature range between 1000 and 2000 K. To confirm the gravitational binding of both components, we employed high-resolution near-infrared spectroscopy obtained from the Gemini Observatory. Our findings indicate that this system ranks among the largest known binaries consisting of two low-mass objects. The research was supported by NASA's Astrophysics Data Analysis Program under award NNX10AD20G, administered through the Space Telescope Science Institute, which operates under contract NAS 5-26555. Additionally, we acknowledge support from NSF grant AST-0908816. The discovery of J0805 + 4812 not only enhances our understanding of binary systems involving late-type dwarfs but also contributes valuable data to the study of low-mass stellar objects. The implications of this research extend to the broader astrophysical community, as it provides insights into the formation and evolution of binary systems in the universe. Our observations and analyses underscore the significance of high-resolution spectroscopy in uncovering the complexities of such celestial pairings, paving the way for future investigations into similar unresolved binary systems.",
        "ori-fast-z-score": -2.1105794120443453,
        "water-fast-z-score": 5.671711984196938,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bondi accretion in the early universe .\nAbstract:\nWe present an analysis of the Bondi accretion rate onto primordial black holes (PBHs) during the radiation-dominated era, taking into account the effects of pressure and viscosity on the gas flow. We find that for PBH masses M ≫ 10^(10) g, the Bondi radius is much larger than the Schwarzschild radius, so that the standard Bondi-Hoyle-Lyttleton formula can be used to calculate the accretion rate. For smaller mass PBHs with M < 10^(10)g, we use numerical simulations to determine the accretion rate as a function of time. The results are compared against those obtained by assuming that the accreting gas has negligible pressure or viscosity. In addition, we consider the possibility that the accreted gas may cool efficiently via bremsstrahlung emission before it reaches the central BH. Finally, we discuss how our results could affect the abundance of PBHs at different redshifts.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bondi accretion in the early universe . Abstract : We present an assessment of the Bondi accretion rate onto primordial black holes ( PBHs ) during the radiation - dominated period , giving into consideration the effects of force and viscosity on the gas stream .We see that for PBH masses M [UNK] 10 ^ ( 10 ) g , the Bondi diameter is much larger than the Schwarzschild diameter , so that the standard Bondi - Hoyle - Lyttleton formula can be used to estimate the accretion rate . For lower mass PBHs with M < 10 ^ ( 10 ) g , we using numerical simulations to estimate the accretion rate as a function of time .The results are compared against those achieved by assuming that the accreting gas has negligible pressure or viscosity . In addition , we investigate the prospect that the accreted gas may cool efficiently via bremsstrahlung emission before it enters the main BH .Finally , we explain how our findings may affect the availability of PBHs at different redshifts .",
        "rewrite_text": "In this article, we explore the Bondi accretion process onto primordial black holes (PBHs) during the radiation-dominated era of the early universe. Our analysis takes into account the influence of both force and viscosity on the gas flow surrounding these black holes. We find that for PBHs with masses greater than approximately \\(10^{10}\\) grams, the Bondi diameter significantly exceeds the Schwarzschild diameter, allowing us to apply the conventional Bondi-Hoyle-Lyttleton formula to accurately estimate the accretion rate. Conversely, for PBHs with masses less than \\(10^{10}\\) grams, we employ numerical simulations to derive the accretion rate as a function of time, providing a more nuanced understanding of the dynamics involved. Our findings are juxtaposed with results obtained under the assumption of negligible pressure and viscosity in the accreting gas, highlighting the importance of these factors in the accretion process.\n\nFurthermore, we examine the potential for the accreted gas to cool efficiently through bremsstrahlung emission prior to its incorporation into the black hole, which could have significant implications for the thermal dynamics of the surrounding environment. The implications of our results extend to the availability and distribution of PBHs across various redshifts, offering insights into their formation and evolution in the early universe. This study not only enhances our understanding of PBH accretion mechanisms but also contributes to the broader discourse on the role of primordial black holes in cosmic structure formation and their potential observational signatures.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 0.8783100656536799
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Systematic Study of Departures from Chemical Equilibrium in the Atmospheres of Substellar Mass Objects .\nAbstract:\nWe present an analysis of chemical equilibrium and disequilibrium processes occurring in the atmospheres of substellar mass objects (SMBOs). We have developed a new method for calculating departures from chemical equilibrium, which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any given point within the atmosphere. This approach allows us to calculate the abundances of individual molecular species as functions of altitude above the photosphere. The results show that there can be significant deviations from chemical equilibrium even under conditions where the gas temperature is much higher than the dust temperature. In particular, we find that carbon monoxide may become depleted by several orders of magnitude relative to its abundance predicted by chemical equilibrium models. These findings suggest that SMBO observations should take into account possible non-equilibrium effects when interpreting their spectra. \n \n Keywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Systematic Study of Departures from Chemical Equilibrium in the Atmospheres of Substellar Mass Objects . Abstract : We report an assessment of chemical equilibrium and disequilibrium systems observed in the atmospheres of substellar mass bodies ( SMBOs ) .We have developed a new method for calculating departures from molecular equilibrium , which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any certain point within the atmosphere . This method enables us to estimate the abundances of different molecular species as functions of altitude above the photosphere .The results show that there can be considerable deviations from chemical equilibrium even under environments where the gas temperature is much higher than the dust temperature . In particular , we find that carbon monoxide might appear depleted by many orders of magnitude compared to its availability predicted by chemical equilibrium models .These conclusions propose that SMBO observations should take into consideration likely non - equilibrium phenomena when interpreting their spectra . Keywords : Chemical equilibrium ; Dust grains ; Local thermodynamic equilibrium",
        "rewrite_text": "Title: A Systematic Study of Departures from Chemical Equilibrium in the Atmospheres of Substellar Mass Objects\n\nAbstract: In this study, we present a comprehensive evaluation of both chemical equilibrium and disequilibrium phenomena observed in the atmospheres of substellar mass objects (SMBOs). We introduce a novel methodology for assessing deviations from molecular equilibrium, predicated on the premise that all atmospheric species maintain local thermodynamic equilibrium with one another at any given altitude. This innovative approach allows us to calculate the relative abundances of various molecular species as a function of altitude above the photosphere. Our findings reveal significant departures from chemical equilibrium, particularly in conditions where the gas temperature substantially exceeds the dust temperature. Notably, we observe that carbon monoxide can be significantly depleted—by several orders of magnitude—compared to the concentrations predicted by traditional chemical equilibrium models. These results underscore the necessity for researchers studying SMBOs to account for potential non-equilibrium processes when analyzing spectral data. By integrating these considerations, we aim to enhance the accuracy of interpretations related to the atmospheric compositions of these intriguing celestial bodies. This work contributes to a deeper understanding of the complex chemical dynamics at play in substellar atmospheres and highlights the importance of reevaluating existing models in light of our findings. \n\nKeywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium.",
        "ori-fast-z-score": -1.6059101370939322,
        "water-fast-z-score": 3.988620176087328,
        "rewrite-fast-z-score": -0.7844645405527362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multifractality and scale invariance in human heartbeat dynamics .\nAbstract:\nThe multifractal nature of the human heartbeat has been studied by many authors, but most studies have focused on short time series (typically less than one hour). In this work we study the scaling properties of long-term recordings of the electrocardiograms (ECG) of healthy subjects during sleep using wavelet transforms to calculate the singularity spectrum f(α), which characterizes the degree of multifractality present in each signal.  We find that all signals are characterized by a broad range of exponents α indicating strong multifractality. The width Δα of these spectra decreases with increasing recording length T as Δα~T-1/2 for T<10 hours and Δα~T-3/4 for T>10 hours. This indicates that the ECGs become more monofractal-like when longer recordings are considered. Finally, we show how our results can be used to detect pathological changes in the heart rate variability associated with congestive heart failure.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multifractality and scale invariance in human heartbeat systems . Abstract : The multifractal nature of the human heartbeat has been studied by many writers , but most studies have concentrated on short period series ( generally less than one minute ) .In this research we study the scaling qualities of long - term recordings of the electrocardiograms ( ECG ) of healthy participants during sleep use wavelet transforms to estimate the singularity spectrum f ( α ) , which characterizes the degree of multifractality observed in each signal . We see that all signals are characterized by a broad variety of exponents α marking strong multifractality .The width Δα of these spectra decreases with higher tape length T as Δα ~ T - 1 / 2 for T < 10 hours and Δα ~ T - 3 / 4 for T > 10 hours . This implies that the ECGs get more monofractal - like when longer recordings are considered .Finally , we show how our findings can be used to identify pathological alterations in the chest rate variability associated with congestive heart failure .",
        "rewrite_text": "Title: Multifractality and Scale Invariance in Human Heartbeat Systems\n\nAbstract: The multifractal characteristics of human heartbeat dynamics have been the focus of numerous studies; however, most of this research has primarily examined short-duration time series, typically lasting less than one minute. In this study, we investigate the scaling properties of long-term electrocardiogram (ECG) recordings from healthy individuals during sleep. Utilizing wavelet transforms, we estimate the singularity spectrum f(α), which serves to quantify the degree of multifractality present in each ECG signal. Our analysis reveals that all recorded signals exhibit a wide range of exponents α, indicative of significant multifractality. Notably, we observe that the width Δα of these spectra diminishes with increasing tape length T, following a relationship of Δα ~ T^(-1/2) for T < 10 hours and Δα ~ T^(-3/4) for T > 10 hours. This trend suggests that as the duration of the recordings increases, the ECG signals tend to exhibit more monofractal-like behavior. Furthermore, we discuss the implications of our findings for clinical applications, particularly in the identification of pathological changes in heart rate variability associated with conditions such as congestive heart failure. Our results contribute to a deeper understanding of the complex dynamics of heart rhythms and may pave the way for improved diagnostic tools in cardiology.",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": 0.21081851067789195
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A new look at the modified Coulomb potential in a strong magnetic field .\nAbstract:\nWe study the effect of an external magnetic field on the modified Coulomb potential for two particles with opposite charges and masses, which are confined to move along one dimension. We show that this system can be mapped onto a spinless fermion model by using the Jordan-Wigner transformation. The ground state energy is calculated exactly within the framework of Bethe ansatz method. It turns out that there exists a critical value of the magnetic field strength beyond which the ground state becomes degenerate. This result agrees well with previous numerical calculations based on exact diagonalization technique. \n \n In addition we calculate the density-density correlation function as well as the momentum distribution function numerically. These results agree very well with those obtained analytically through the use of Bethe ansatz equations. Finally, we discuss how our results may be generalized to higher dimensions. Introduction:-In recent years considerable attention has been paid to the problem of strongly correlated electrons in low dimensional systems such as quantum wires or carbon nanotubes  1-3 . One of the most interesting phenomena observed experimentally in these systems is the fractional quantized Hall effect (FQHE)  4  . In particular it was shown that when the number of electrons N is odd ,the lowest Landau level(LLL) will contain only one electron per flux quanta  5  .The FQHEs have attracted much interest because they provide us with a unique opportunity to investigate many-body effects in condensed matter physics  6  .\nRecently, several authors  7-10  studied the properties of the modified coulomb interaction between two oppositely charged particles moving in a uniform magnetic field B perpendicularly to their plane of motion. They found that the ground-state energy depends crucially upon whether the total angular momentum J = L + S is zero or not where L is orbital angular momentum and S is spin angular momentum. For example if J=0 then the ground state energy is given by E0=−e2/lB+O(1/N),where lB=eB/mc is the magnetic length  11  .On the other hand if J=1/2 then the ground state energy takes the form E0",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A different glance at the modified Coulomb current in a powerful magnetic force . Abstract : We research the impact of an external magnetic force on the modified Coulomb potential for two particles with opposite charges and masses , which are confined to move along one dimension .We see that this scheme can be mapped onto a spinless fermion model by using the Jordan - Wigner transformation . The ground state energy is calculated exactly within the framework of Bethe ansatz technique .It turns out that there exists a critical quantity of the magnetic force power beyond which the ground state remains degenerate . This result agrees well with previous quantitative calculations based on exact diagonalization technique .In addition we determine the density - density correlation function as well as the velocity distribution relation numerically . These conclusions follow very well with those achieved analytically through the using of Bethe ansatz equations .Finally , we talk how our findings may be generalized to higher dimensions . Introduction : - In recent years considerable focus has been paid to the question of highly correlated electrons in low dimensional networks such as quantum wires or carbon nanotubes 1 - 3 .One of the most exciting phenomena observed experimentally in these systems is the fractional quantized Hall impact ( FQHE ) 4 . In particular it was shown that when the number of atoms N is odd , the highest Landau level ( LLL ) will hold only one electron per flux quanta 5 . The FQHEs have garnered many interest because they give us with a unique opportunity to examine multiple - bodies phenomena in condensed matter theory 6 .Recently , various scientists 7 - 10 studied the properties of the modified coulomb interaction between two oppositely charged particles moving in a uniform magnetic force B perpendicularly to their plane of movement . They found that the ground - state energy relies crucially upon whether the total angular velocity J = L + S is zero or not where L is orbital angular velocity and S is spin angular velocity .For instance if J = 0 then the ground state energy is given by E0 = −e2 / lB + O ( 1 / N ) , where lB = eB / mc is the magnetic thickness 11 . On the other hand if J = 1 / 2 then the ground state energy takes the form E0",
        "rewrite_text": "**Title:** A New Perspective on the Modified Coulomb Current in a Strong Magnetic Field\n\n**Abstract:** This study investigates the effects of an external magnetic field on the modified Coulomb potential experienced by two particles with opposite charges and masses, constrained to one-dimensional motion. We demonstrate that this scenario can be effectively represented by a spinless fermion model through the application of the Jordan-Wigner transformation. Utilizing the Bethe ansatz technique, we compute the ground state energy precisely. Our findings reveal the existence of a critical threshold for the strength of the magnetic field, beyond which the ground state exhibits degeneracy. This observation aligns closely with previous quantitative analyses conducted via exact diagonalization methods. Furthermore, we numerically evaluate the density-density correlation function and the velocity distribution relation, which corroborate the analytical results derived from the Bethe ansatz equations. We also discuss the implications of our results and how they may be extended to higher-dimensional systems.\n\n**Introduction:** Recent years have seen a surge of interest in the behavior of highly correlated electrons within low-dimensional structures, such as quantum wires and carbon nanotubes. One of the most intriguing phenomena observed in these systems is the fractional quantum Hall effect (FQHE). Notably, it has been established that when the number of atoms (N) is odd, the highest Landau level (LLL) accommodates only a single electron per flux quantum. The FQHE has attracted significant attention as it provides a unique platform for exploring many-body phenomena in condensed matter physics. Recent investigations have focused on the modified Coulomb interaction between two oppositely charged particles subjected to a uniform magnetic field perpendicular to their motion. These studies have highlighted that the ground state energy is significantly influenced by the total angular momentum (J = L + S), where L represents the orbital angular momentum and S denotes the spin angular momentum. For instance, when J = 0, the ground state energy is expressed as E0 = −e²/lB + O(1/N), with lB = eB/mc representing the magnetic length. Conversely, for J = 1/2, the ground state energy adopts a different form.",
        "ori-fast-z-score": -0.38691161626706844,
        "water-fast-z-score": 7.51625689070423,
        "rewrite-fast-z-score": 1.6116459280507605
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Agujeros de Gusano en Gravedad (2+1) .\nAbstract:\nEn este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una solución exacta del campo gravitacional generado por un cuerpo esférico con simetría axial, que corresponde al caso más simple de agujero negro no rotante. Se muestra que los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmológicas involucradas. Además, se presenta una nueva clase de soluciones exactas para el problema de Einstein-Klein-Gordon en espacios homogéneos e isótropos. Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energía escalar. Finalmente, se discute brevemente cómo estas soluciones pueden ser utilizadas como fuentes de radiación gravitatoria coherente. En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano  en el espacio-tiempo descrito por una solución exacta correspondiente al campo gravitacional generado por una esfera con simetría axial. Esta solución corresponde al caso más sencillo posible de agujero negro sin rotación. Mostramos que estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmológica involucradas.  También presentamos una nueva clase de soluciones exáctas para el problema de Klein-Gordon-Einstein en espacios homogéneos isotrópicos. Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energía escalara. Por último discutimos brevemente cómo estas soluciones pueden utilizarse como fuentes de radiación gravitacional coherente.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Agujeros de Gusano en Gravedad ( 2 + 1 ) . Abstract : En este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una solución exacta en campo gravitacional generado por un cuerpo esférico con simetría axial , se corresponde un caso más simple de agujero negro no rotante .Se muestra en los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmológicas involucradas . Además , se presenta una nueva clase de soluciones exactas en le problema de Einstein - Klein - Gordon en espacios homogéneos e isótropos .Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energía escalar . Finalmente , se discute brevemente cómo estas soluciones pueden ser utilizadas como fuentes de radiación gravitatoria coherente .En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano en el espacio - tiempo descrito por una solución exacta correspondiente al campo gravitacional generado por una esfera con simetría axial . Esta solución corresponde un caso más sencillo posible de agujero negro sin rotación .Mostramos para estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmológica involucradas . También presentamos una nueva clase de soluciones exáctas de el problema de Klein - Gordon - Einstein en espacios homogéneos isotrópicos .Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energía escalara . Por último discutimos brevemente cómo estas soluciones pueden utilizarse como fuentes de radiación gravitacional coherente .",
        "rewrite_text": "**Title:** Wormholes in (2 + 1) Gravity\n\n**Abstract:** This study investigates the existence and stability of wormholes within a spacetime framework defined by an exact solution to the gravitational field generated by a spherically symmetric body with axial symmetry. This scenario represents a simplified case of a non-rotating black hole. Our findings indicate that these wormholes can exhibit stability under specific conditions related to the cosmological constants involved. Furthermore, we introduce a novel class of exact solutions to the Einstein-Klein-Gordon problem in homogeneous and isotropic spaces. These solutions correspond to stationary scalar waves localized around a singular point characterized by an infinite density of scalar energy. We also briefly discuss the potential applications of these solutions as coherent gravitational radiation sources. Through this work, we aim to enhance the understanding of wormholes in lower-dimensional gravity and explore their implications in theoretical physics, particularly in the context of gravitational wave generation. The stability criteria we establish could provide insights into the viability of wormholes as physical entities, while the new solutions we present may open avenues for further research into the interplay between scalar fields and gravitational phenomena. Overall, this research contributes to the broader discourse on the nature of spacetime and the fundamental forces that govern it, offering a fresh perspective on the intriguing concept of wormholes in the realm of (2 + 1) gravity.",
        "ori-fast-z-score": -1.3333333333333333,
        "water-fast-z-score": 1.9148542155126762,
        "rewrite-fast-z-score": 0.19245008972987526
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Logic of Reachable Patterns in Linked Data-Structures .\nAbstract:\nWe present a logic for reasoning about reachability properties on linked data structures, which are graphs with labeled nodes and edges that can be traversed using the Web Linking Language (WLL). The WLL is an extension to RDFS that allows one to specify how links between resources should be created or modified by applications. We show how our logic can be used to express important classes of queries over linked data such as finding all paths between two given nodes, computing the distance between them, checking whether there exists at least one path between them, etc., and we prove its decidability. Finally, we provide algorithms for evaluating these queries under certain restrictions. Our results have been implemented within the LOD2 system. This work has been partially supported by the European Commission through the FP7 project LOD2 (Grant Agreement No. 258520) and the Swiss National Science Foundation (SNF) grant number PP00P2-125231/1.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Logic of Reachable Patterns in Linked Data - Structures . Abstract : We present a logic for reasoning about reachability properties on linked data structures , which are graphs with designated vertices and edges that can be traversed using the Web Linking Language ( WLL ) .The WLL is an extension to RDFS that enables one to define how links between resources should be built or modified by applications . We see how our logic can be used to define essential classes of queries over connected data such as finding all paths between two given nodes , computing the distance between them , determining whether there exists at least one path between them , etc . , and we prove its decidability .Finally , we provide algorithms for evaluating these queries under certain constraints . Our results have been used within the LOD2 system .This project has been partially backed by the European Commission through the FP7 project LOD2 ( Grant Agreement No . 258520 ) and the Swiss National Science Foundation ( SNF ) funding designation PP00P2 - 125231 / 1 .",
        "rewrite_text": "Title: A Logic of Reachable Patterns in Linked Data Structures\n\nAbstract: In this article, we introduce a novel logic designed for analyzing reachability properties within linked data structures, which are essentially graphs characterized by specific vertices and edges that can be navigated using the Web Linking Language (WLL). The WLL serves as an enhancement to the Resource Description Framework Schema (RDFS), allowing applications to define and manipulate the connections between various resources effectively. Our proposed logic facilitates the formulation of fundamental query classes pertinent to connected data, including tasks such as identifying all possible paths between two specified nodes, calculating the distance separating them, and ascertaining the existence of at least one viable path connecting the nodes. We rigorously demonstrate the decidability of our logic, ensuring that these queries can be reliably evaluated. Furthermore, we present algorithms tailored for the execution of these queries, taking into account specific constraints that may arise in practical applications. The implications of our findings have been integrated into the LOD2 system, showcasing the practical utility of our logic in real-world scenarios. This research has received partial funding from the European Commission under the FP7 project LOD2 (Grant Agreement No. 258520) and has also been supported by the Swiss National Science Foundation (SNF) under the funding designation PP00P2-125231/1. Our work contributes significantly to the field of linked data and offers a robust framework for enhancing the accessibility and manipulation of interconnected data structures.",
        "ori-fast-z-score": 0.508000508000762,
        "water-fast-z-score": 4.5,
        "rewrite-fast-z-score": 0.7035264706814485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of Emission from the CN Radical in the Cloverleaf Quasar at z=2.56 .\nAbstract:\nWe report on observations made with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal emission lines associated with carbon monoxide and its isotopologue, 13CO, as well as the CN radical toward the quasar host galaxy at redshift 2.56 known as the  Cloverleaf  source.  The observed line ratios are consistent with those expected for gas exposed to intense radiation fields characteristic of quasars. We also detect absorption by molecular hydrogen along this sightline through intervening clouds located between us and the quasar host galaxy. These results provide new insights into the physical conditions within the interstellar medium surrounding active galactic nuclei during their early evolutionary stages. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. \nThe detection of carbon monoxide (CO), one of the most abundant molecules in space, has been used extensively over the past several decades to study the properties of cold neutral atomic and molecular gas in galaxies across cosmic time. However, CO can be difficult to observe directly because it lacks electric dipole moments and thus emits very weakly. In addition, the excitation temperature of the lowest rotational levels of CO is typically low enough such that these transitions fall outside of the frequency range accessible to ground-based telescopes operating at millimeter wavelengths. As a result, much of our understanding about the physical conditions present in dense regions of star-forming galaxies comes from studies of other tracers of molecular gas, including HCN, H2S, CS, CH3OH, H2O, and OH+.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of Emission from the CN Radical in the Cloverleaf Quasar at z = 2 . 56 . Abstract : We report on observations made with the Atacama Large Millimeter / submillimeter Array ( ALMA ) that indicate emission lines linked with carbon monoxide and its isotopologue , 13CO , as also as the CN radical toward the quasar host galaxy at redshift 2 . 56 referred as the Cloverleaf source .The observed line values are consistent with those expected for gas exposed to intense radiation fields distinctive of quasars . We additionally observe absorption by molecular hydrogen along this sightline through intervening clouds situated between us and the quasar host universe .These data provide fresh insights into the physical conditions within the interstellar medium comprising active galactic nuclei during their early evolutionary stages . This is an free access section under the terms of the Creative Commons Attribution License , which allows use , distribution and reproduction in any medium , provided the original book is properly cited .The measurement of carbon monoxide ( CO ) , one of the most stable compounds in space , has been used heavily over the previous several decades to study the properties of cold neutral atomic and molecular gas in galaxies across cosmic time . However , CO can be harder to observe directly because it lacks electric dipole moments and therefore emits very weakly .In addition , the excitation temperature of the lowest rotational concentrations of CO is typically minimum enough such that these changes fall outside of the frequency limit accessible to ground - based telescopes operating at millimeter wavelengths . As a result , part of our knowing about the physical conditions present in dense areas of galaxy - making clusters comes from studies of other tracers of molecular dust , notably HCN , H2S , CS , CH3OH , H2O , and OH + .",
        "rewrite_text": "We present findings from observations conducted with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal emission lines associated with carbon monoxide (CO) and its isotopologue, 13CO, as well as the CN radical in the host galaxy of the Cloverleaf quasar at a redshift of 2.56. The emission line characteristics align with expectations for gas subjected to the intense radiation fields typical of quasars. Furthermore, we detect absorption features from molecular hydrogen along the line of sight, indicating the presence of intervening clouds between our observation point and the quasar's host galaxy. These observations enhance our understanding of the physical conditions within the interstellar medium of active galactic nuclei during their formative phases.\n\nThe study of carbon monoxide, a stable molecule prevalent in space, has been instrumental over the past few decades in examining the properties of cold neutral atomic and molecular gas in galaxies throughout cosmic history. However, direct observations of CO can be challenging due to its lack of electric dipole moments, resulting in weak emissions. Additionally, the excitation temperature of CO's lowest rotational states is often too low for detection by ground-based telescopes operating at millimeter wavelengths. Consequently, our understanding of the physical conditions in dense regions of galaxy-forming clusters has been supplemented by investigations of alternative molecular tracers, including HCN, H2S, CS, CH3OH, H2O, and OH+. This research contributes valuable insights into the dynamics of the early universe and the role of molecular gas in galaxy evolution. The article is available under the Creative Commons Attribution License, permitting use, distribution, and reproduction in any medium, provided proper citation of the original work.",
        "ori-fast-z-score": -1.5787044347526527,
        "water-fast-z-score": 6.075111253646805,
        "rewrite-fast-z-score": -0.4472135954999579
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Low-density graph codes that are optimal for source/channel coding and binning .\nAbstract:\nWe present new constructions of low density parity check (LDPC) codes with good performance on both the binary symmetric channel (BSC), as well as the additive white Gaussian noise (AWGN) channel, which can be used in conjunction with either Slepian-Wolf or Wyner-Ziv compression to achieve capacity-achieving distributed lossy data storage systems over noisy channels.  We show how these LDPC codes can also be used to perform lossless distributed data storage by using them within an erasure correcting code framework known as rateless fountain codes.   Finally we demonstrate how our proposed scheme is able to outperform existing schemes at high rates when operating under practical constraints such as finite field size and limited number of decoding iterations. The problem of storing large amounts of information reliably across multiple nodes has been studied extensively in recent years due to its applications in cloud computing, peer-to-peer networks, wireless sensor networks, and other areas where there exists a need to store data securely while minimizing network bandwidth requirements. In this work, we consider the scenario where two parties wish to communicate via a noisy communication link, but do not trust each other s ability to faithfully transmit their messages. This situation arises naturally if one party wishes to send private data to another party who does not have access to a direct connection between themselves; see Figure 1 . For example, Alice may want to send her medical records to Bob through a third-party server Charlie, without revealing any sensitive information about herself to Charlie.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Low - density graph codes that are optimal for source / stream coding and binning . Abstract : We introduce novel constructions of lowest density parity check ( LDPC ) codes with good efficiency on both the binary symmetric channel ( BSC ) , as well as the additive white Gaussian sound ( AWGN ) channel , which can be used in conjunction with either Slepian - Wolf or Wyner - Ziv compression to achieve bandwidth - reaching distributed lossy data storage systems over noisy channels .We see how these LDPC coding can also be used to conduct lossless distributed database storage by using them within an erasure correcting code framework called as rateless fountain codes . Finally we prove how our proposed system is ability to outperform older schemes at high levels when implemented under technical restrictions such as finite field length and large number of decoding iterations .The question of storing huge amounts of content reliably across multiple nodes has been studied frequently in recent years owing to its applications in cloud networking , peer - to - peer systems , wireless sensor networks , and other areas where there exists a desire to store data securely while minimizing network bandwidth demands . In this research , we imagine the scenario where two parties wish to connect via a noisy communication link , but do not trust each other s capabilities to faithfully transmit their messages .This condition occurs naturally if one party wishes to send personal data to another party who does not have access to a formal connection between themselves ; look Figure 1 . For instance , Alice would want to give her hospital accounts to Bob through a third - party server Charlie , without revealing any sensitive information about herself to Charlie .",
        "rewrite_text": "We present innovative constructions of low-density parity-check (LDPC) codes that demonstrate high efficiency for both the binary symmetric channel (BSC) and the additive white Gaussian noise (AWGN) channel. These codes can be effectively utilized alongside Slepian-Wolf or Wyner-Ziv compression techniques to facilitate bandwidth-efficient distributed lossy data storage systems operating over noisy channels. Additionally, we explore the application of these LDPC codes in achieving lossless distributed database storage by integrating them within a framework of erasure-correcting codes known as rateless fountain codes. Our findings indicate that the proposed coding system significantly outperforms traditional methods, particularly under constraints such as limited field lengths and a high number of decoding iterations.\n\nThe challenge of reliably storing vast amounts of data across multiple nodes has garnered considerable attention in recent years, especially due to its relevance in cloud networking, peer-to-peer systems, and wireless sensor networks. These applications necessitate secure data storage while simultaneously minimizing network bandwidth usage. In our study, we consider a scenario where two parties aim to communicate over a noisy channel but lack trust in each other's ability to accurately transmit messages. This situation is particularly relevant when one party, for instance, wishes to share sensitive personal information, such as hospital records, with another party through an intermediary server, without disclosing any private details to the intermediary. This scenario highlights the importance of developing robust coding strategies that ensure data integrity and confidentiality in distributed storage systems.",
        "ori-fast-z-score": -1.3426901732747025,
        "water-fast-z-score": 7.495152097492019,
        "rewrite-fast-z-score": -0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Temperature Dependence of the Tensile Properties of Single Walled Carbon Nanotubes: O(N) Tight Binding MD Simulation .\nAbstract:\nWe have performed molecular dynamics simulations to study the temperature dependence of tensile properties of single walled carbon nanotubes (SWCNTs). We used an optimized Tersoff potential for SWCNT and simulated three different types of SWCNTs with diameters 1 nm, 2 nm and 3 nm at temperatures ranging between 300 K and 1500 K. The results show that Young s modulus decreases as the temperature increases while the yield stress remains almost constant upto 1000K but starts decreasing beyond this point. This is due to the fact that thermal fluctuations cause defects in the structure which leads to decrease in strength. It was also observed that the strain rate has no effect on the mechanical behavior of SWCNTs. \n\n\nKeywords: Molecular Dynamics Simulations; Temperature Dependence; Tensile Strength; Carbon Nanotube Structures; Defects. Introduction: Carbon nanotubes are one dimensional structures made out of sp2 hybridized carbon atoms arranged into hexagonal rings  1  . Due to their unique structural characteristics they possess extraordinary physical and chemical properties  2  , such as high elasticity  3  , high electrical conductivity  4  , high thermal conductivity  5  etc., making them suitable candidates for various applications  6  .\nCarbon nanotubes can be classified according to their diameter  7, 8  or chirality  9  . Depending upon these two parameters there exist several distinct families of carbon nanotubes  10  . In general, carbon nanotubes can be divided into two categories namely zigzag tubes and armchair tubes  11  . Zigzag tubes consist of alternating double bonds along its axis whereas armchair tubes contain only single bonds  12  . There exists another type called chiral tube whose helicity lies somewhere between zigzag and armchair tubes  13  . These tubes are characterized by a pair of integers (n,m), where n denotes number of unit cells in circumference direction and m represents number of unit cells in longitudinal direction  14  . For example, (5, 5) , (6, 6), (7, 7) and (8, 4) represent zigzag, armchair, chiral and achiral tubes respectively  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Temperature Dependence of the Tensile Properties of Single Walled Carbon Nanotubes : O ( N ) Tight Binding MD Simulation . Abstract : We have done molecular dynamics simulations to study the temperature dependence of tensile properties of multiple walled carbon nanotubes ( SWCNTs ) .We utilized an optimized Tersoff potential for SWCNT and simulated three different kinds of SWCNTs with diameters 1 nm , 2 nm and 3 nm at conditions ranging between 300 K and 1500 K . The results show that Young s modulus drops as the temperature increases while the yield stress remains virtually constant upto 1000K but stops decreasing beyond this point . This is due to the fact that heat fluctuations cause failures in the formation which results to fall in intensity .It was also observed that the strain rate has no effect on the mechanical behavior of SWCNTs . Keywords : Molecular Dynamics Simulations ; Temperature Dependence ; Tensile Strength ; Carbon Nanotube Structures ; Defects .Introduction : Carbon nanotubes are one dimensional devices formed out of sp2 hybridized carbon atoms arranged into hexagonal rings 1 . Due to their different structural traits they possess extraordinary physical and biological qualities 2 , such as great elasticity 3 , large electrical conductivity 4 , low heat conductivity 5 etc . , making them ideal candidates for various uses 6 .Carbon nanotubes can be categorized according to their diameter 7 , 8 or chirality 9 . Depending upon these two parameters there reside several different parents of carbon nanotubes 10 .In general , carbon nanotubes can be grouped into two genres namely zigzag tubes and armchair pipes 11 . Zigzag tubes comprise of alternating double bonds along its axis whereas armchair pipes comprise only multiple bonds 12 .There exists another type named chiral tube whose helicity falls somewhere between zigzag and armchair channels 13 . These tubes are characterized by a pair of integers ( n , m ) , where k denotes amount of unit cells in circumference direction and m reflects amount of unit cells in longitudinal direction 14 .For instance , ( 5 , 5 ) , ( 6 , 6 ) , ( 7 , 7 ) and ( 8 , 4 ) indicate zigzag , armchair , chiral and achiral tubes respectively 15 .",
        "rewrite_text": "**Title:** Temperature Dependence of the Tensile Properties of Single-Walled Carbon Nanotubes: O(N) Tight Binding MD Simulation\n\n**Abstract:** This study presents a comprehensive investigation into the temperature-dependent tensile properties of single-walled carbon nanotubes (SWCNTs) through molecular dynamics simulations. Utilizing an optimized Tersoff potential tailored for SWCNTs, we examined three distinct types of SWCNTs with diameters of 1 nm, 2 nm, and 3 nm, across a temperature range from 300 K to 1500 K. Our findings reveal a notable decrease in Young's modulus with increasing temperature, while the yield stress remains relatively stable up to 1000 K, after which it ceases to decline. This behavior can be attributed to thermal fluctuations that induce structural failures, leading to a reduction in mechanical intensity. Interestingly, the strain rate was found to have no significant impact on the mechanical properties of the SWCNTs. \n\nThe introduction of carbon nanotubes as one-dimensional structures composed of sp² hybridized carbon atoms arranged in hexagonal configurations highlights their exceptional physical and biological characteristics, including remarkable elasticity, high electrical conductivity, and low thermal conductivity. These attributes render carbon nanotubes suitable for a wide array of applications. Carbon nanotubes can be classified based on their diameter and chirality, leading to various structural forms. They are generally categorized into two primary types: zigzag tubes, which feature alternating double bonds along their axis, and armchair tubes, characterized by multiple bonds. Additionally, chiral tubes exhibit a helicity that lies between the zigzag and armchair configurations, defined by a pair of integers (n, m) that represent the number of unit cells in the circumferential and longitudinal directions, respectively. Examples of these configurations include (5, 5) for zigzag, (6, 6) for armchair, and (7, 7) and (8, 4) for chiral and achiral tubes, respectively. This research contributes to the understanding of the mechanical behavior of SWCNTs under varying thermal conditions, providing insights that are crucial for their application in nanotechnology and materials science.\n\n**Keywords:** Molecular Dynamics Simulations; Temperature Dependence; Tensile Strength; Carbon Nanotube Structures; Defects.",
        "ori-fast-z-score": -1.8593393604027364,
        "water-fast-z-score": 5.706433236417486,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Laplacian spectra of complex networks and random walks on them: Are scale-free architectures really important? .\nAbstract:\nWe study the Laplacian spectrum of complex networks, focusing in particular on its relation to the dynamics of random walks on them. We show that for any network with n nodes there are at most 2n eigenvalues different from zero (counting multiplicities). This bound is tight up to a constant factor when applied to trees or complete graphs. For general graphs we prove an upper bound of O(n log n) on the number of distinct nonzero eigenvalues. In addition, we provide lower bounds showing that this estimate cannot be improved by more than a polylogarithmic factor. Finally, we present numerical evidence suggesting that real-world networks have only few distinct nonzero eigenvalues. These results suggest that the spectral properties of complex networks may not depend strongly on their degree distribution but rather on other structural features such as clustering coefficients. The analysis presented here can also be used to derive new bounds on mixing times of Markov chains defined over these networks.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Laplacian spectra of complex networks and random runs on them : Are scale - free architectures really important ? .Abstract : We research the Laplacian spectrum of complex networks , concentrating in particular on its connection to the dynamics of random runs on them . We see that for any connection with n nodes there are at most 2n eigenvalues different from zero ( counting multiplicities ) .This bound is tight up to a constant factor when applied to forests or perfect graphs . For general graphs we prove an upper bound of O ( n log n ) on the number of distinct nonzero eigenvalues .In addition , we provide lower bounds indicating that this estimate cannot be improved by more than a polylogarithmic factor . Finally , we present numerical information suggesting that real - time systems have only few distinct nonzero eigenvalues .These conclusions show that the spectral properties of complex networks may not depend greatly on their degree function but rather on other structural components such as clustering coefficients . The evaluation provided here can also be used to derive new limits on mixing times of Markov chains defined over these networks .",
        "rewrite_text": "In this study, we investigate the Laplacian spectrum of complex networks, with a particular focus on its relationship to the dynamics of random processes occurring on these networks. Our analysis reveals that for any network comprising n nodes, there can be at most 2n distinct eigenvalues that are non-zero, when considering their multiplicities. This upper limit is shown to be tight, up to a constant factor, in the case of forests and perfect graphs. For more general graph structures, we establish an upper bound of O(n log n) on the number of distinct non-zero eigenvalues. Furthermore, we provide lower bounds that indicate this estimate cannot be significantly improved, except by a polylogarithmic factor. Our numerical findings suggest that real-time systems tend to exhibit a limited number of distinct non-zero eigenvalues. These results imply that the spectral characteristics of complex networks may not be heavily influenced by their degree distribution, but rather by other structural elements, such as clustering coefficients. The insights gained from this research can also be leveraged to formulate new constraints on the mixing times of Markov chains defined over these complex networks. Overall, our work contributes to a deeper understanding of the interplay between the spectral properties of networks and their underlying structural features, challenging the notion that scale-free architectures are of paramount importance in determining these properties.",
        "ori-fast-z-score": -0.20412414523193154,
        "water-fast-z-score": 4.4907311951024935,
        "rewrite-fast-z-score": 1.3228756555322951
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Early Spectrophotometric Evolution of V1186 Scorpii (Nova Scorpii 2004 #1) .\nAbstract:\nWe present the first detailed analysis of the early photospheric evolution of Nova Scorpii 2004 # 1, based on high-resolution optical spectroscopy obtained with the Ultraviolet and Visual Echelle Spectrograph mounted at the Very Large Telescope in Chile.  The nova was discovered by amateur astronomers on March 31st, 2004, when it reached an apparent magnitude of 8.7. We find that the ejecta are expanding at velocities between 1000 km/s to 3000 km/s. From our observations we derive a distance estimate for this object of about 3 kpc. This is consistent with previous estimates derived using other methods. Using these results as input parameters into theoretical models, we determine the chemical composition of the ejecta. Our best fit model suggests that the ejecta consist mainly of O-rich material mixed with some CNO-processed material. In addition, we detect strong emission lines originating from highly ionized species such as FeXXV/FeXXVI or NeIX/NX. These lines indicate that the ejecta were heated up to temperatures above 10 million K during their expansion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Early Spectrophotometric Evolution of V1186 Scorpii ( Nova Scorpii 2004 # 1 ) . Abstract : We present the first detailed analysis of the early photospheric evolution of Nova Scorpii 2004 # 1 , using on wide - resolution optical spectroscopy acquired with the Ultraviolet and Visual Echelle Spectrograph mounted at the Very Large Telescope in Chile .The nova was discovered by amateur astronomers on March 31st , 2004 , when it achieved an apparent magnitude of 8 . 7 . We see that the ejecta are growing at velocities between 1000 kilometers / s to 3000 km / s .From our observations we derive a distance estimate for this object of about 3 kpc . This is compatible with previous accounts derived using other methods .Using these results as input parameters into theoretical methods , we determine the chemical composition of the ejecta . Our best fit description implies that the ejecta consist mostly of O - rich substance mixed with some CNO - processed material .In addition , we find strong radiation lines coming from highly ionized species such as FeXXV / FeXXVI or NeIX / NX . These lines indicate that the ejecta were heated up to altitudes above 10 million K during their expansion .",
        "rewrite_text": "We provide a comprehensive analysis of the early photospheric evolution of Nova Scorpii 2004 # 1, based on high-resolution optical spectroscopy obtained with the Ultraviolet and Visual Echelle Spectrograph at the Very Large Telescope in Chile. This nova was first detected by amateur astronomers on March 31, 2004, when it reached an apparent magnitude of 8.7. Our observations reveal that the ejecta are expanding at velocities ranging from 1000 km/s to 3000 km/s. From these measurements, we estimate the distance to the nova to be approximately 3 kpc, a value that aligns with previous estimates derived through various methods. Utilizing these findings as input for theoretical models, we analyze the chemical composition of the ejecta. Our best-fitting model suggests that the ejecta are predominantly composed of oxygen-rich material, with a notable presence of carbon-nitrogen-oxygen (CNO) processed elements. Furthermore, we observe prominent emission lines from highly ionized species, including FeXXV, FeXXVI, NeIX, and NX. These spectral features indicate that the ejecta experienced temperatures exceeding 10 million K during their expansion phase. This study not only enhances our understanding of the early evolution of Nova Scorpii 2004 # 1 but also contributes valuable insights into the physical processes governing nova explosions and their aftermath. The findings underscore the significance of high-resolution spectroscopy in unraveling the complexities of nova phenomena and provide a foundation for future research in this area.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 1.5554275420956378
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chandra X-Ray Study of Galactic Supernova Remnant G299.2-2.9 .\nAbstract:\nWe present the results of an X-ray study of supernova remnant (SNR) G299.2-2.9 using data obtained with Chandra and XMM-Newton observatories. The SNR is located in the constellation Puppis at a distance of ~5 kpc, which corresponds to its angular size of about 30 arcmin. We find that the spectrum of this object can be described by two thermal components with temperatures T1=7×10^6 K and T2=2×10^6 K. In addition, we detect non-thermal emission above 10 keV. Using these parameters, we estimate the age of the SNR as t=4000 yr. This value agrees well with the characteristic time for the expansion of the shell into the surrounding medium. Based on our analysis, we conclude that the observed morphology of the SNR is consistent with the model of a spherical explosion expanding into a uniform interstellar medium.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chandra X - Ray Study of Galactic Supernova Remnant G299 . 2 - 2 . 9 . Abstract : We report the conclusion of an X - ray study of supernova remnant ( SNR ) G299 . 2 - 2 . 9 utilizing information obtained with Chandra and XMM - Newton observatories .The SNR is situated in the constellation Puppis at a distance of ~ 5 kpc , which corresponds to its angular height of about 30 arcmin . We see that the spectrum of this object can be described by two thermal parts with temperatures T1 = 7×10 ^ 6 K and T2 = 2×10 ^ 6 K . In addition , we perceive non - temperature emission above 10 keV .Using these parameters , we estimate the age of the SNR as t = 4000 yr . This value agrees well with the typical moment for the expansion of the shell into the nearby medium .Based on our analysis , we prove that the known morphology of the SNR is compatible with the model of a spherical explosion expanding into a regular interstellar medium .",
        "rewrite_text": "We present the findings of an extensive X-ray investigation of the supernova remnant (SNR) G299.2-2.9, conducted using data from the Chandra and XMM-Newton observatories. Located in the constellation Puppis at an approximate distance of 5 kiloparsecs, G299.2-2.9 exhibits an angular extent of about 30 arcminutes. Our spectral analysis reveals that the emission from this remnant can be characterized by two distinct thermal components, with temperatures measured at T1 = 7×10^6 K and T2 = 2×10^6 K. Additionally, we observe non-thermal emission occurring at energies exceeding 10 keV. Utilizing these temperature parameters, we estimate the age of the supernova remnant to be around 4000 years, a value that aligns well with the expected timeline for the expansion of the remnant's shell into the surrounding interstellar medium. Our results indicate that the observed morphology of G299.2-2.9 is consistent with a model of a spherical explosion propagating through a uniform interstellar medium. This study enhances our understanding of the physical processes governing supernova remnants and their interaction with the surrounding environment, providing valuable insights into the lifecycle of such astronomical phenomena.",
        "ori-fast-z-score": -0.5345224838248488,
        "water-fast-z-score": 4.03585624040554,
        "rewrite-fast-z-score": -0.23249527748763857
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dark Matter in Gauge Mediation from Emergent Supersymmetry .\nAbstract:\nWe propose that the dark matter (DM) and supersymmetric particles are generated by an emergent gauge symmetry at high energy scales, which is broken down to Standard Model symmetries below TeV scale. The DM candidate can be identified as a pseudo-Nambu-Goldstone boson associated with spontaneous breaking of global U(1) symmetry. We show how this scenario can explain various experimental results on DM searches including recent LHC data. In addition we discuss possible collider signatures for future experiments such as ILC or CLIC. Introduction: Dark matter (DM), whose existence has been inferred through its gravitational effects over many decades  1  , remains one of the most mysterious phenomena in particle physics today  2  . Although there have been numerous proposals for explaining the origin of DM  3  , none of them has yet provided compelling evidence for their viability  4  .\nIn this work, motivated by the idea of  emergent  theories  5  -  8  , we consider a new possibility where DM emerges from a spontaneously-broken global symmetry  9  . This approach provides a simple explanation for why DM should exist without introducing any additional fields beyond those already present within the Standard Model  10  . Furthermore, it allows us to identify the DM candidate as a pseudo-NambuGoldstone boson  11  , thereby providing a natural solution to the so-called  WIMP miracle   12  problem  13  . Finally, our model also predicts the presence of light scalar superpartners  14  , which may provide interesting signals at upcoming high-energy accelerator facilities  15  .\nThe rest of this article is organised as follows. In Sec. 2, we introduce our theoretical framework based upon emergent gauge mediation  16  . Then, in Secs. 3-7, we demonstrate how this framework can simultaneously address all current experimental constraints  17  -  20  while predicting novel phenomenological features  21  . Finally, in Sec. 8, we conclude with some remarks about further directions of research.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dark Matter in Gauge Mediation from Emergent Supersymmetry . Abstract : We suggest that the dark matter ( DM ) and supersymmetric particles are produced by an emergent gauge symmetry at high energy scales , which is broken down to Standard Model symmetries below TeV scale .The DM candidate can be identified as a quasi - Nambu - Goldstone boson associated with spontaneous breaking of global U ( 1 ) symmetry . We see how this situation can describe several experimental results on DM searches notably recent LHC evidence .In addition we explain possible collider signatures for future research such as ILC or CLIC . Introduction : Dark matter ( DM ) , whose existence has been inferred through its gravitational influences over numerous centuries 1 , continues one of the most obscure events in particle science today 2 .Although there have been numerous ideas for explaining the origin of DM 3 , none of them has already offered credible support for their viability 4 . In this research , driven by the idea of emergent theories 5 - 8 , we investigate a new possibility where DM appears from a spontaneously - breaking global symmetry 9 .This method provides a simple explanation for why DM should exist without removing any additional fields beyond those already found within the Standard Model 10 . Furthermore , it allows us to identify the DM candidate as a quasi - NambuGoldstone boson 11 , thereby providing a natural solution to the so - called WIMP miracle 12 problem 13 .Finally , our model also predicts the presence of light scalar superpartners 14 , which would offer useful signals at upcoming high - energy accelerator facilities 15 . The rest of this page is grouped as follows .In Sec . 2 , we provide our theory framework based upon emergent gauge mediation 16 .Then , in Secs . 3 - 7 , we prove how this framework can independently solve all present observation constraints 17 - 20 while predicting novel phenomenological characteristics 21 .Finally , in Sec . 8 , we conclude with some remarks about further directions of research .",
        "rewrite_text": "**Title:** Dark Matter in Gauge Mediation from Emergent Supersymmetry\n\n**Abstract:** In this article, we propose a novel framework for understanding dark matter (DM) and supersymmetric particles through the lens of emergent gauge symmetry at high energy scales. This symmetry is hypothesized to break down to the Standard Model symmetries at energies below the TeV scale. We identify the DM candidate as a quasi-Nambu-Goldstone boson, which arises from the spontaneous breaking of a global U(1) symmetry. Our approach not only offers a coherent explanation for various experimental observations related to DM, including recent findings from the Large Hadron Collider (LHC), but also sets the stage for future collider experiments such as the International Linear Collider (ILC) and the Compact Linear Collider (CLIC). \n\nThe existence of dark matter has been inferred from its gravitational effects over centuries, yet its fundamental nature remains one of the most perplexing challenges in particle physics. While numerous theories have been proposed to explain the origin of DM, many lack empirical support. Our research, inspired by emergent theories, explores the possibility that DM emerges from a spontaneously broken global symmetry, providing a straightforward rationale for its existence without necessitating additional fields beyond those in the Standard Model. This framework not only identifies the DM candidate as a quasi-Nambu-Goldstone boson but also addresses the so-called WIMP miracle, offering a natural resolution to this longstanding issue.\n\nMoreover, our model predicts the existence of light scalar superpartners, which could yield significant signals in forthcoming high-energy accelerator experiments. The structure of this paper is organized as follows: Section 2 outlines the theoretical framework based on emergent gauge mediation. Sections 3 through 7 detail how this framework can independently address current observational constraints while predicting novel phenomenological features. Finally, Section 8 concludes with insights into potential future research directions in this exciting field.",
        "ori-fast-z-score": 0.5183210553488161,
        "water-fast-z-score": 7.831560082980487,
        "rewrite-fast-z-score": 1.3206763594884356
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The initial conditions of star formation in the Ophiuchus main cloud: Kinematics of the protocluster condensations .\nAbstract:\nWe present new observations of the kinematic properties of the dense gas associated with the young stellar cluster forming within the Ophiuchus molecular cloud (d = 140 pc). We used the Submillimeter Array to observe two fields, one centered on the infrared source IRAS 04368+2557 and another located about 1 arcmin away at the position of the Spitzer c2d survey target L1641N. The data reveal that both regions are characterized by complex velocity structures which we interpret as being due to multiple overlapping protostellar cores. In particular, our results show that:  -The region surrounding IRAS 04368+25570 is dominated by three distinct components separated by less than 0.1 pc along the line-of-sight.  -The region around L1641N contains several compact sources embedded in an extended envelope whose emission peaks at velocities between 5 and 10 km s-1 relative to the local standard of rest.   -Both regions contain numerous outflows driven by protostars still deeply embedded in their natal envelopes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The initial conditions of galaxy formation in the Ophiuchus principal cloud : Kinematics of the protocluster condensations . Abstract : We report new studies of the kinematic qualities of the deep gas corresponding with the young stellar cluster forming within the Ophiuchus molecular cloud ( d = 140 pc ) .We utilized the Submillimeter Array to observe two fields , one concentrated on the infrared source IRAS 04368 + 2557 and another situated about 1 arcmin away at the position of the Spitzer c2d survey destination L1641N . The data reveal that both locations are marked by complex momentum systems which we treat as being owing to multiple overlapping protostellar cores .In particular , our findings show that : - The region following IRAS 04368 + 25570 is dominated by three separate constituents joined by less than 0 . 1 pc along the line - of - seeing . - The region around L1641N contains several compact sources embedded in an extended envelope whose emission peaks at velocities between 5 and 10 km s - 1 relative to the local standard of rest .- Both regions feature numerous outflows driven by protostars already deeply lodged in their natal envelopes .",
        "rewrite_text": "We present a comprehensive analysis of the kinematic properties of the gas associated with a nascent stellar cluster within the Ophiuchus molecular cloud, located at a distance of 140 parsecs. Utilizing the Submillimeter Array, we conducted observations of two distinct fields: one focused on the infrared source IRAS 04368 + 2557 and the other approximately one arcminute away at the Spitzer c2d survey target L1641N. Our findings indicate that both regions exhibit intricate momentum structures, which we interpret as the result of multiple overlapping protostellar cores. Specifically, our results reveal that the area surrounding IRAS 04368 + 2557 is characterized by three distinct components that are closely aligned, separated by less than 0.1 parsecs along the line of sight. In contrast, the L1641N region is comprised of several compact sources that are enveloped within a more extensive structure, with emission peaks observed at velocities ranging from 5 to 10 km/s relative to the local standard of rest. Furthermore, both regions are rich in outflows driven by protostars that are deeply embedded within their surrounding envelopes. This study enhances our understanding of the initial conditions of galaxy formation by elucidating the kinematic characteristics of protocluster condensations in the Ophiuchus cloud, providing valuable insights into the processes that govern star formation in such environments.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 4.719863179556685,
        "rewrite-fast-z-score": -0.3216337604513384
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dislocation Dynamics in a Crystal Lattice (Peierls-Nabarro) Relief .\nAbstract:\nThe Peierls-Nabarro model is used to study the dislocations dynamics in a crystal lattice, where the energy barrier for glide motion and climb motion are calculated by using the concept of activation volume. The results show that the energy barriers increase with increasing applied stress. It also shows that the energy barrier decreases as temperature increases. Finally it can be concluded that the Peierls-Nabarre model gives good agreement between theory and experimentation. Keywords: Energy Barrier, Dislocation, Glide Motion, Climb Motion, Activation Volume, Peierls-Nabarrou Model. 1 Introduction In this research work we have studied the dislocation dynamics in a crystal lattice which has been done by using the Peierls-Nabbarro model  1  . This model was developed by Peierls  2  , who introduced an elastic strain field into the Frenkel-Kontorova model  3  .\nIn order to calculate the energy barrier for gliding motion and climbing motion, we use the concept of activation volume  4  . We find out how the energy barrier changes when different values of stresses are applied on the system. Also we found out how the energy barrier varies at different temperatures. Finally we compare our theoretical results with experimental data  5  -  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dislocation Dynamics in a Crystal Lattice ( Peierls - Nabarro ) Relief . Abstract : The Peierls - Nabarro theory is utilized to study the dislocations dynamics in a crystal lattice , where the power barrier for glide movement and ascent movement are measured by using the idea of activation volume .The results show that the power barriers increase with increasing applied strain . It additionally shows that the power barrier decreases as temperature increases .Finally it can be realized that the Peierls - Nabarre system gives excellent approval between theoretical and experimentation . Keywords : Energy Barrier , Dislocation , Glide Motion , Climb Motion , Activation Volume , Peierls - Nabarrou Model .1 Introduction In this research work we have researched the dislocation behavior in a crystal lattice which has been performed by using the Peierls - Nabbarro model 1 . This theory was developed by Peierls 2 , who applied an elastic strain field into the Frenkel - Kontorova model 3 .In order to estimate the electricity barrier for gliding motion and climbing movement , we utilize the idea of activation volume 4 . We get out how the power barrier changes when varying values of stresses are applied on the system .Also we learned out how the electricity barrier variations at different temperatures . Finally we compare our theory findings with experimental evidence 5 - 8 .",
        "rewrite_text": "**Title:** Dislocation Dynamics in a Crystal Lattice: Insights from the Peierls-Nabarro Model\n\n**Abstract:** This study employs the Peierls-Nabarro theory to investigate the dynamics of dislocations within a crystal lattice, focusing on the energy barriers associated with glide and climb movements. By utilizing the concept of activation volume, we quantitatively assess how these power barriers respond to varying applied strains and temperatures. Our findings reveal a direct correlation between applied strain and the increase in power barriers, indicating that as strain intensifies, the resistance to dislocation motion also escalates. Conversely, we observe that elevating the temperature leads to a reduction in the power barriers, suggesting that thermal activation facilitates dislocation movement. This behavior is consistent with the expectations of the Peierls-Nabarro framework, which effectively bridges theoretical predictions with experimental observations. The results underscore the model's robustness in capturing the complexities of dislocation dynamics, providing valuable insights into the fundamental mechanisms governing material deformation. Our research not only enhances the understanding of dislocation behavior in crystalline materials but also highlights the significance of temperature and strain in influencing dislocation mobility. The implications of these findings extend to various applications in materials science, particularly in the design and optimization of materials for enhanced mechanical performance. \n\n**Keywords:** Energy Barrier, Dislocation, Glide Motion, Climb Motion, Activation Volume, Peierls-Nabarro Model. \n\n**1 Introduction:** This research explores the behavior of dislocations in a crystal lattice through the lens of the Peierls-Nabarro model. Originally developed by Peierls, this theory integrates an elastic strain field into the Frenkel-Kontorova model. By applying the concept of activation volume, we estimate the energy barriers for both glide and climb movements of dislocations. Our investigation reveals how these barriers fluctuate in response to varying stress levels and temperatures, and we compare our theoretical predictions with experimental data to validate our findings.",
        "ori-fast-z-score": -0.7107423155935334,
        "water-fast-z-score": 6.331738236133036,
        "rewrite-fast-z-score": -0.17149858514250882
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Markovian Subsystems: Invariance, Attractivity, and Control .\nAbstract:\nWe study the controllability properties of quantum subsystems that are described by a master equation with Lindblad form. We show how to construct invariant sets for such systems in terms of their generators. These results allow us to prove that certain classes of open quantum systems cannot be controlled using only unitary operations on the system s Hilbert space. Finally we present an algorithm which allows one to determine whether or not a given set is invariant under the dynamics generated by a particular generator. This work was supported by NSF grant PHY-0456628. Quantum mechanics has been successfully applied to many physical phenomena ranging from atomic physics to condensed matter theory. However, it remains unclear what exactly constitutes a quantum mechanical description of reality. One approach towards answering this question involves studying the behavior of closed quantum systems whose states evolve according to Schrödinger equations. Another approach focuses on describing the evolution of open quantum systems where the state of the system interacts continuously with its environment. The latter class of problems can often be formulated as questions about the controllability of quantum dynamical systems. For example, consider the problem of steering the state of a two-level atom between different energy levels using laser pulses.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Markovian Subsystems : Invariance , Attractivity , and Control . Abstract : We research the controllability properties of quantum subsystems that are explained by a master equation with Lindblad form .We see how to build invariant sets for such systems in terms of their generators . These results allow us to prove that particular categories of open quantum systems cannot be governed using only unitary operations on the scheme s Hilbert space .Finally we present an algorithm which allows one to find whether or not a given set is invariant under the dynamics generated by a certain generator . This project was supported by NSF grant PHY - 0456628 .Quantum theory has been successfully application to many physical phenomena ranging from atomic physics to condensed matter theory . However , it remains unsure what actually constitutes a quantum mechanical explanation of reality .One approach towards answering this question involves studying the dynamics of opened quantum systems whose states evolve according to Schrödinger parameters . Another approach focuses on explaining the evolution of close quantum systems where the state of the system interacts continuously with its surroundings .The latter type of question can often be understood as challenges about the controllability of quantum dynamical systems . For instance , consider the question of steering the state of a two - level particle between various energy levels using laser pulses .",
        "rewrite_text": "Title: Quantum Markovian Subsystems: Invariance, Attractivity, and Control\n\nAbstract: This study investigates the controllability characteristics of quantum subsystems described by a master equation in Lindblad form. We explore the construction of invariant sets for these systems based on their generators, leading to significant insights into the limitations of controlling open quantum systems. Specifically, we demonstrate that certain classes of these systems cannot be effectively managed solely through unitary operations applied to the Hilbert space of the system. To facilitate this analysis, we introduce an algorithm designed to determine whether a specific set remains invariant under the dynamics dictated by a given generator. This research is supported by NSF grant PHY-0456628. \n\nQuantum theory has been successfully applied to a wide range of physical phenomena, from atomic physics to condensed matter systems. However, the fundamental nature of a quantum mechanical explanation of reality remains an open question. One avenue of exploration involves examining the dynamics of open quantum systems, where the states evolve according to Schrödinger parameters. Alternatively, we can analyze closed quantum systems, where the system's state continuously interacts with its environment. These inquiries often present challenges related to the controllability of quantum dynamical systems. For example, a pertinent question arises regarding the ability to manipulate the state of a two-level quantum particle between different energy levels using laser pulses. This research aims to deepen our understanding of these dynamics and the inherent limitations of control in quantum systems, ultimately contributing to the broader discourse on the nature of quantum mechanics and its applications.",
        "ori-fast-z-score": 0.8392543274162825,
        "water-fast-z-score": 6.313641498019764,
        "rewrite-fast-z-score": 2.5337417643170097
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  RNA polymerase motors on DNA track: effects of traffic congestion on RNA synthesis .\nAbstract:\nThe transcription process is driven by the enzyme RNA Polymerase (RNAP) that moves along the template strand of double stranded DNA and synthesizes an mRNA molecule in front of it. The RNAPs are highly dynamic enzymes, which can pause or arrest at specific sites during their translocation through the gene sequence. In this work we study how these pauses affect the overall rate of transcription using a coarse-grained model for the dynamics of multiple RNAP molecules moving simultaneously on a single DNA chain. We show that the presence of pausing leads to a reduction in the average speed of transcription elongation as well as to a decrease in the total number of synthesized mRNAs per unit time. These results suggest that the observed heterogeneity in the rates of transcription may be due to differences in the density of paused RNAPs between genes. Transcription is one of the fundamental processes in all living organisms where information stored in the form of genetic code is converted into messenger RNA (mRNA). This conversion takes place inside the cell nucleus when the enzyme called RNA Polymerase (or simply RNAP) binds with promoter region of the gene and starts copying its sequence  1  . During transcription, the RNAP travels along the template strand of doublestranded DNA and synthesizes an RNA molecule in front of itself  2  .\nIn recent years there has been significant progress towards understanding the mechanism behind transcription  3, 4  , however many questions remain unanswered  5  . One such question concerns the effect of pausing events on the overall rate of transcription  6  . It was shown experimentally  7, 8  that the RNAPs tend to pause frequently while traveling through the gene sequence. Moreover, the duration of each pause event varies widely depending on the type of the gene  9  . For example, some genes have short pauses lasting only few milliseconds  10  whereas others exhibit much longer pauses up to several seconds  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : RNA polymerase engines on DNA path : effects of road disruption on RNA production . Abstract : The transcription process is controlled by the enzyme RNA Polymerase ( RNAP ) that moves along the template strand of double stranded DNA and synthesizes an mRNA molecule in front of it .The RNAPs are extremely dynamic enzymes , which can halt or arrest at individual sites during their translocation through the gene sequence . In this research we study how these pauses affect the overall rate of transcription utilizing a thin - grained model for the dynamics of multiple RNAP molecules moving simultaneously on a single DNA chain .We suggest that the presence of pausing leads to a reduction in the average fast of transcription elongation as well as to a reduction in the total quantity of synthesized mRNAs per unit time . These conclusions show that the seen heterogeneity in the rates of transcription might be due to differences in the density of paused RNAPs between genes .Transcription is one of the fundamental pathways in all living organisms where information stored in the form of genetic code is converted into messenger RNA ( mRNA ) . This conversion starts place inside the cell nucleus when the protein termed RNA Polymerase ( or simply RNAP ) binds with promoter area of the gene and starts copying its sequence 1 .During transcription , the RNAP travels along the template strand of doublestranded DNA and synthesizes an RNA protein in front of itself 2 . In recent years there has been significant progress towards studying the pathway behind transcription 3 , 4 , however several questions remain unanswered 5 .One such issue concerns the impact of pausing events on the overall rate of transcription 6 . It was shown experimentally 7 , 8 that the RNAPs tend to halt frequently while moving through the gene sequence .Moreover , the duration of each delay event depends widely depending on the kind of the gene 9 . For instance , some genes have short pauses lasting only few milliseconds 10 whereas others experience far longer stops up to several seconds 11 .",
        "rewrite_text": "**Title:** RNA Polymerase Engines on DNA Path: Effects of Road Disruption on RNA Production\n\n**Abstract:** The transcription process, a critical mechanism for gene expression in all living organisms, is primarily orchestrated by the enzyme RNA Polymerase (RNAP). This enzyme traverses the double-stranded DNA template, synthesizing messenger RNA (mRNA) as it progresses. RNAPs exhibit remarkable dynamism, often pausing or stalling at specific sites during their journey along the gene sequence. In this study, we investigate the implications of these pauses on the overall transcription rate by employing a detailed model that simulates the dynamics of multiple RNAP molecules operating concurrently on a single DNA strand. Our findings indicate that the occurrence of pausing significantly diminishes the average rate of transcription elongation and leads to a decrease in the total amount of mRNA synthesized per unit time. This suggests that the observed variability in transcription rates across different genes may stem from variations in the density of paused RNAPs. \n\nTranscription initiates within the cell nucleus when RNAP binds to the promoter region of a gene, commencing the process of copying the genetic sequence. While considerable advancements have been made in understanding the transcription pathway, several critical questions remain unresolved. One such question pertains to the effects of pausing events on transcription efficiency. Experimental evidence has demonstrated that RNAPs frequently experience halts during their traversal of gene sequences, with the duration of these pauses varying significantly among different genes. For example, certain genes exhibit brief pauses lasting only a few milliseconds, while others may encounter prolonged interruptions lasting several seconds. This variability in pausing behavior highlights the complexity of transcription regulation and its potential impact on gene expression dynamics. Our research contributes to a deeper understanding of these mechanisms, emphasizing the need for further exploration into the factors influencing RNAP pausing and its consequences for RNA production.",
        "ori-fast-z-score": 0.8778955729143844,
        "water-fast-z-score": 7.851210698609627,
        "rewrite-fast-z-score": 2.242227956050979
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fuzzballs with internal excitations .\nAbstract:\nWe propose that the fuzzball proposal for black holes can be extended to include internal degrees of freedom, which are excited by infalling matter and produce Hawking radiation.  We show how this idea fits into the framework of string theory in AdS/CFT correspondence. The proposed model is based on an extension of the work done by Horowitz and Maldacena (HM) who showed that the entropy of extremal Kerr-Newman black hole agrees exactly with the microscopic counting of states in N=4 super Yang-Mills gauge theory at strong coupling. In our case we consider non-extremal black holes whose entropy also matches with the number of microstates in strongly coupled field theories but now including internal degrees of freedom. This leads us to conclude that the emission spectrum of these black holes should agree with the one predicted by Hawking s original calculation. Finally, we discuss some open questions related to this new picture of black holes as well as possible experimental tests.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fuzzballs with internal excitations . Abstract : We suggest that the fuzzball idea for black holes can be extended to consider inner degrees of liberty , which are excited by infalling matter and produce Hawking radiation .We see how this idea fits into the framework of string theory in AdS / CFT relationship . The proposed theory is based on an extension of the paper done by Horowitz and Maldacena ( HM ) who demonstrated that the entropy of extremal Kerr - Newman white hole agrees exactly with the microscopic counting of states in N = 4 super Yang - Mills gauge theory at weak interaction .In our case we study non - extremal black holes whose entropy also matches with the proportion of microstates in strongly coupled field theories but now including internal degrees of liberty . This leads us to observe that the emission spectrum of these black holes should comply with the one expected by Hawking s original calculation .Finally , we explain some open questions related to this new picture of black holes as well as possible experimental tests .",
        "rewrite_text": "In this article, we propose an extension of the fuzzball concept in the context of black holes by incorporating internal degrees of freedom that can be excited by infalling matter, subsequently leading to the generation of Hawking radiation. This framework is situated within the realm of string theory, particularly through the AdS/CFT correspondence. Our approach builds upon the foundational work of Horowitz and Maldacena (HM), who established that the entropy of extremal Kerr-Newman white holes aligns precisely with the microscopic state counting in N=4 super Yang-Mills gauge theory under weak coupling conditions. \n\nIn our investigation, we focus on non-extremal black holes, demonstrating that their entropy also corresponds to the number of microstates in strongly coupled field theories, while now taking into account the internal degrees of freedom. This consideration allows us to predict that the emission spectrum of these black holes will adhere to the expectations set forth by Hawking's original calculations. \n\nFurthermore, we delve into several unresolved questions that arise from this novel perspective on black holes, including the implications for our understanding of black hole thermodynamics and quantum gravity. We also discuss potential experimental avenues to test our theoretical predictions, which could provide deeper insights into the nature of black holes and their internal structures. Overall, our work aims to enrich the dialogue surrounding black hole physics and contribute to the ongoing exploration of the fundamental principles governing these enigmatic objects.",
        "ori-fast-z-score": -0.21566554640687682,
        "water-fast-z-score": 3.9668163788998405,
        "rewrite-fast-z-score": 0.5570860145311556
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Semidefinite Representation of Convex Sets .\nAbstract:\nWe present an algorithm for computing the semidefinite representation (SDR) of convex sets in arbitrary dimensions, which is based on solving linear programs with polynomial number of constraints and variables. The SDR can be used to solve many problems that are NP-hard or even undecidable by reducing them into tractable convex optimization problems. We show how this approach can be applied to several important computational problems such as finding Nash equilibria in games, learning graphical models, and approximating mixed integer programming problems. Finally we discuss some open questions related to our work. Semidefinite representations provide a powerful tool for solving hard computational problems via convex optimization. In particular they allow us to reduce intractable problems like finding Nash equilibria in multiplayer games, learning graphical models and approximating mixed-integer programming problems to tractable convex optimization problems. \n \n This talk will give an overview over recent results on algorithms for computing semidefinite representations of convex sets. It will also explain how these techniques can be applied to various computational problems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Semidefinite Representation of Convex Sets . Abstract : We present an algorithm for computing the semidefinite representation ( SDR ) of convex sets in arbitrary dimensions , which is based on solving continuous programs with polynomial amount of constraints and parameters .The SDR can be used to solve many difficulties that are NP - hard or even undecidable by converting them into tractable convex optimization problems . We see how this methodology can be applied to several important computational problems such as finding Nash equilibria in games , studying visual models , and approximating mixing integer programming difficulties .Finally we explain some open questions related to our work . Semidefinite representations provide a powerful tool for solving hard analytical issues via convex optimization .In particular they allow us to reduce intractable problems like finding Nash equilibria in multiplayer tournaments , studying visual models and approximating mixing - integer programming difficulties to tractable convex optimization problems . This discussion will provide an overview over recent results on techniques for modeling semidefinite representations of convex sets .It will also explain how these tools can be applied to numerous computational problems .",
        "rewrite_text": "In this article, we introduce a novel algorithm designed to compute the semidefinite representation (SDR) of convex sets across arbitrary dimensions. Our approach leverages continuous programming techniques that involve a polynomial number of constraints and parameters, facilitating the efficient computation of SDRs. The significance of SDRs lies in their ability to transform complex problems, often classified as NP-hard or even undecidable, into manageable convex optimization tasks. This transformation opens up new avenues for addressing various challenging computational issues.\n\nWe explore the application of our methodology to several critical problems, including the determination of Nash equilibria in strategic games, the analysis of visual models, and the approximation of mixed-integer programming challenges. Each of these areas presents unique difficulties that can benefit from the tractability afforded by semidefinite representations. By employing SDRs, we can systematically reduce these intractable problems to forms that are amenable to convex optimization techniques.\n\nFurthermore, we discuss several open questions that arise from our research, highlighting the potential for future exploration in this domain. The use of semidefinite representations not only enhances our ability to tackle hard analytical problems but also enriches the toolkit available for computational problem-solving. This article aims to provide a comprehensive overview of recent advancements in the modeling of semidefinite representations of convex sets and to illustrate the practical implications of these tools across a range of computational challenges. Through this discussion, we hope to inspire further research and innovation in the application of SDRs to complex optimization problems.",
        "ori-fast-z-score": 1.4142135623730951,
        "water-fast-z-score": 6.4,
        "rewrite-fast-z-score": -0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cool Stars in Hot Places .\nAbstract:\nThe Sun is the nearest star to Earth, and its activity has been studied for thousands of years.  The Sun s magnetic field plays an important role in solar activity.   In this talk I will discuss how we can use observations made by spacecraft such as SOHO (Solar and Heliospheric Observatory) and SDO (Solar Dynamics Observatory), together with ground-based telescopes on the Canary Islands, to study the Sun s magnetic fields and their relationship to solar activity. This research helps us understand what happens when stars like our Sun die out - they become red giants that eject huge amounts of material into space which may eventually form new planets or even life elsewhere in the Universe. Keywords: Solar Activity, Red Giant Star, Magnetic Field, Space Weather. Title: Cool Stars in Hot Places. Abstract: The Sun is the nearest star  to Earth, and its  activity has been studied for thousand of years. The Sun s magnetic field play an important role in solar activities. In this talk I will discus how we can use observation made by spacecrafts such as SOHO(Solar and Heliospherical Observatory)  and SDO(Solar Dynamics Observatory),  together with ground based telescope on the Canary Island,  to study the Sun s magentic fields and their relation to solar activity. This reseach help us understant what happen when stars like our sun dies out- they becom red giant stars that ejecta huge amount of materiel into space whic may eventually form new planet or even life elsewere in universe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cool Stars in Hot Places . Abstract : The Sun is the nearest star to Earth , and its activity has been studied for thousands of years .The Sun s magnetic force plays an important role in solar activity . In this talk I will explore how we can using observations made by satellites such as SOHO ( Solar and Heliospheric Observatory ) and SDO ( Solar Dynamics Observatory ) , combined with ground - based telescopes on the Canary Islands , to study the Sun s magnetic fields and their connection to solar activity .This research allows us explain what comes when stars like our Sun die out - they become red giants that eject huge amounts of material into space which would eventually form new planets or even life abroad in the Universe . Keywords : Solar Activity , Red Giant Star , Magnetic Field , Space Weather .Title : Cool Stars in Hot Places . Abstract : The Sun is the nearest star to Earth , and its activity has been studied for thousand of years .The Sun s magnetic force serve an important role in solar action . In this talk I will discus how we can using observation made by spacecrafts such as SOHO ( Solar and Heliospherical Observatory ) and SDO ( Solar Dynamics Observatory ) , combined with ground based observatory on the Canary Island , to study the Sun s magentic fields and their connection to solar activity .This reseach help us understant what happen when stars like our sunlight kills out - they becom red massive planets that ejecta huge amount of materiel into space whic may eventually form new planet or even life elsewere in universe .",
        "rewrite_text": "Title: Cool Stars in Hot Places\n\nAbstract: The Sun, being the closest star to Earth, has been the subject of extensive study for thousands of years. Its magnetic forces play a crucial role in influencing solar activity. In this presentation, I will discuss how we can utilize observations from satellites such as the Solar and Heliospheric Observatory (SOHO) and the Solar Dynamics Observatory (SDO), in conjunction with ground-based telescopes located in the Canary Islands, to investigate the Sun's magnetic fields and their relationship to solar phenomena. This research enhances our understanding of the life cycle of stars similar to our Sun, particularly what occurs when they exhaust their nuclear fuel. As these stars evolve, they transform into red giants, expelling vast amounts of material into space. This ejected material has the potential to contribute to the formation of new planetary systems and may even foster the emergence of life elsewhere in the universe. \n\nKeywords: Solar Activity, Red Giant Stars, Magnetic Fields, Space Weather.",
        "ori-fast-z-score": 1.4779939172464398,
        "water-fast-z-score": 6.343350474165466,
        "rewrite-fast-z-score": 0.6201736729460423
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Self-consistency over the charge-density in dynamical mean-field theory: a linear muffin-tin implementation and some physical implications .\nAbstract:\nWe present an efficient scheme for solving the self-consistent problem within the framework of Dynamical Mean-Field Theory (DMFT). The method is based on a linearized version of the Muffin-Tin orbital approach, which allows to treat large systems with thousands of atoms at low computational cost. We show that our results are in good agreement with those obtained by other methods such as exact diagonalization or Quantum Monte Carlo simulations. In particular we study the evolution of the electronic structure across the metal-insulator transition in the Hubbard model. Finally, we discuss how this new technique can be used to investigate more complex materials like doped semiconductors. Introduction: Dynamical Mean Field Theory (DMFT) has been widely applied to describe strongly correlated electron systems  1-3 . It provides a powerful tool to understand many-body effects beyond standard band-structure calculations  4  . However, its application to real materials requires the solution of a complicated quantum impurity problem  5  , whose numerical treatment becomes very demanding when dealing with realistic models  6  .\nIn order to overcome these difficulties several techniques have been developed  7-10 . Among them, the most popular ones are the iterative perturbation theory  11  , the non-crossing approximation  12  , the variational cluster approach  13  , the cellular DMFT  14  , the continuous-time QMC  15  , and the exact diagonalization  16  . All these approaches require the calculation of the Green s function G(k,ω), which depends on two variables k and ω. This makes their direct evaluation extremely time consuming even if one uses fast Fourier transforms  17  . To avoid this difficulty, it was proposed to use the so-called LMTOs  18  instead of Bloch functions. These orbitals are constructed so that they reproduce exactly the density matrix inside each atomic sphere  19  . They allow us to reduce considerably the number of degrees of freedom involved in the calculation  20  . Moreover, since the LMTOs do not depend explicitly on the wave vector k, the corresponding Green s function only needs to be calculated once  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Self - stability over the charge - density in dynamical mean - field theory : a linear muffin - tin implementation and some physical implications . Abstract : We present an efficient scheme for solving the self - consistent situation within the framework of Dynamical Mean - Field Theory ( DMFT ) .The method is based on a linearized version of the Muffin - Tin orbital approach , which allows to treat large systems with thousands of atoms at low computational efficiency . We see that our findings are in good agreement with those achieved by other methods such as approximate diagonalization or Quantum Monte Carlo simulations .In particular we study the evolution of the electronic configuration across the metal - insulator transition in the Hubbard theory . Finally , we explain how this new technique can be used to examine more sophisticated materials like doped semiconductors .Introduction : Dynamical Mean Field Theory ( DMFT ) has been widely applied to define strongly interacting electron systems 1 - 3 . It provides a powerful tool to explain large - bodies phenomena beyond standard band - structure estimates 4 .However , its use to real materials demands the solve of a complicated quantum impurity problem 5 , whose numerical treatment remains very requiring when dealing with realistic models 6 . In order to overcome these problems several methods have been used 7 - 10 .Among them , the most popular ones are the iterative perturbation theory 11 , the non - crossing approximation 12 , the variational cluster approach 13 , the cellular DMFT 14 , the discrete - time QMC 15 , and the exact diagonalization 16 . All these approaches need the determination of the Green s function G ( k , ω ) , which depends on two variables k and ω .This leaves their direct processing exceptionally time consuming even if one uses quick Fourier transforms 17 . To prevent this trouble , it was suggested to use the so - called LMTOs 18 instead of Bloch functions .These orbitals are built so that they reproduce exactly the density matrix inside each atomic surface 19 . They allow us to reduce considerably the number of degrees of freedom employed in the calculation 20 .Moreover , since the LMTOs do not depend explicitly on the wave vector k , the equivalent Green s function only needs to be determined once 21 .",
        "rewrite_text": "**Title:** Self-Stability Over Charge Density in Dynamical Mean-Field Theory: A Linear Muffin-Tin Implementation and Its Physical Implications\n\n**Abstract:** In this study, we introduce a novel and efficient approach for addressing self-consistency within the framework of Dynamical Mean-Field Theory (DMFT). Our method leverages a linearized version of the Muffin-Tin orbital technique, enabling the analysis of large systems comprising thousands of atoms while maintaining low computational costs. The results obtained from our approach demonstrate strong concordance with those derived from other established methods, including approximate diagonalization and Quantum Monte Carlo simulations. A significant focus of our investigation is the transition of electronic configurations during the metal-insulator transition as described by Hubbard theory. This exploration reveals critical insights into the behavior of strongly correlated electron systems. Furthermore, we discuss the applicability of our technique to more complex materials, such as doped semiconductors, highlighting its potential to advance the understanding of electronic properties in these systems. \n\n**Introduction:** Dynamical Mean-Field Theory (DMFT) has emerged as a pivotal framework for analyzing strongly interacting electron systems, providing a robust methodology for elucidating phenomena that extend beyond conventional band-structure calculations. However, the practical application of DMFT to real materials necessitates the resolution of intricate quantum impurity problems, which can be computationally intensive when applied to realistic models. To address these challenges, various methods have been developed, including iterative perturbation theory, non-crossing approximation, variational cluster approaches, cellular DMFT, discrete-time Quantum Monte Carlo, and exact diagonalization. Each of these techniques requires the computation of the Green's function G(k, ω), which is dependent on both momentum (k) and frequency (ω), leading to significant computational demands even with the use of fast Fourier transforms. To mitigate these issues, we propose the use of Linear Muffin-Tin Orbitals (LMTOs) as an alternative to traditional Bloch functions. The LMTOs are specifically constructed to accurately reproduce the density matrix within each atomic sphere, significantly reducing the number of degrees of freedom in the calculations. Additionally, since LMTOs are independent of the wave vector k, the corresponding Green's function only needs to be computed once, further enhancing computational efficiency.",
        "ori-fast-z-score": -0.2349781349963872,
        "water-fast-z-score": 6.267968183526508,
        "rewrite-fast-z-score": -0.3965257928590721
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Structure and Dynamics of the Sun s Open Magnetic Field .\nAbstract:\nThe solar magnetic field is an important ingredient in many physical processes that take place on the Sun, such as coronal heating or solar wind acceleration. The open magnetic flux threading through the heliosphere plays also a crucial role for space weather prediction. In this work we present results obtained with the MHD model developed by Usmanov et al. (2010) to study the structure and dynamics of the Sun s open magnetic field. We show how the global properties of the simulated open magnetic field compare with observations made at 1 AU using spacecraft data. Our simulations reproduce well the observed latitudinal distribution of the open magnetic flux density and its dependence on the radial distance from the Sun. They also provide information about the temporal evolution of the open magnetic field which can be used to predict the state of the interplanetary medium several days ahead. This research was supported by NASA grants NNX10AC85G (PI: S. Riley), NNG09FA40C (PI: A. Schwadron), and NNM07AA01A (PI: J. McComas).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Structure and Dynamics of the Sun s Open Magnetic Field . Abstract : The solar magnetic force is an important ingredient in many mechanical phenomena that take place on the Sun , such as coronal heating or solar wind acceleration .The open magnetic flux threading through the heliosphere serves also a crucial role for space weather prediction . In this research we present results derived with the MHD model created by Usmanov et al .( 2010 ) to study the composition and dynamics of the Sun s open magnetic force . We see how the global properties of the simulated open magnetic force compare with observations made at 1 AU using satellite information .Our simulations reproduce well the seen latitudinal flow of the open magnetic flux coefficient and its dependence on the radial distance from the Sun . They even provide details about the temporal evolution of the open magnetic force which can be used to predict the state of the interplanetary medium several days ahead .This research was supported by NASA grants NNX10AC85G (PI: S. Riley), NNG09FA40C (PI: A. Schwadron), and NNM07AA01A (PI: J. McComas).",
        "rewrite_text": "**Title: Structure and Dynamics of the Sun's Open Magnetic Field**\n\n**Abstract:** The solar magnetic field plays a pivotal role in various mechanical processes occurring on the Sun, including coronal heating and the acceleration of solar wind. Additionally, the open magnetic flux that permeates the heliosphere is essential for accurate space weather forecasting. In this study, we present findings derived from the magnetohydrodynamic (MHD) model developed by Usmanov et al. (2010), which allows us to investigate the characteristics and dynamics of the Sun's open magnetic field. Our analysis focuses on comparing the global properties of the simulated open magnetic field with observational data collected at 1 astronomical unit (AU) from satellite missions. The simulations demonstrate a strong correlation with the observed latitudinal flow of the open magnetic flux coefficient, as well as its variation with radial distance from the Sun. Furthermore, our model captures the temporal evolution of the open magnetic field, providing insights that can be utilized for predicting the conditions of the interplanetary medium several days in advance. This research contributes to a deeper understanding of solar dynamics and enhances our ability to forecast space weather events. The work was supported by NASA grants NNX10AC85G (Principal Investigator: S. Riley), NNG09FA40C (Principal Investigator: A. Schwadron), and NNM07AA01A (Principal Investigator: J. McComas).",
        "ori-fast-z-score": 0.3216337604513384,
        "water-fast-z-score": 4.90361647617904,
        "rewrite-fast-z-score": 0.8164965809277261
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rounding of first-order phase transitions and optimal cooperation in scale-free networks .\nAbstract:\nWe study the effect of rounding on the dynamics of complex networks with first-order phase transition (FPT). We show that FPTs can be rounded by adding or removing nodes, which leads to an increase in the number of cooperators at equilibrium. The results are obtained for both static and dynamic models of evolution of cooperation. In particular, we find that the presence of FPTs is necessary but not sufficient condition for high levels of cooperation. Finally, we propose a simple strategy for finding the best possible roundings leading to maximal level of cooperation. Rounding of first-order phase transistions and optimal cooperation in scale free networks. P. Krawczyk 1 , A. Szolnoki 2 . \n1 Institute of Physics, University of Warsaw, Poland; krawczykp@wip.waw.pl .\n2 Department of Mathematics, University of Warsaw, Warsaw, Poland; aszolnok@wip. waw.pl .\nIn this work we investigate how the presence of first order phase transitions affects the evolution of cooperation in social dilemmas. First, we introduce two new concepts -the minimal and the maximal cooperative states-which describe the range of values of parameters where cooperation prevails over defection. Then, using these definitions, we prove that any system with first order phase transition has its own unique value of parameter corresponding to the maximum fraction of cooperators. Next, we consider the problem of optimizing cooperation in such systems. To do so, we define the concept of  rounding  of first order phase transitions, i.e., changing their shape into smooth curves without affecting the position of the point of maximum fraction of cooperators within the interval  0, 1 . Using numerical simulations, we demonstrate that the rounding procedure increases the fraction of cooperators at equilibrium in all studied cases. Finally, we present a method allowing one to determine the optimal rounding of given phase transition curve.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rounding of first - order phase transitions and optimal cooperation in scale - free networks . Abstract : We research the impact of rounding on the dynamics of complex networks with first - order phase transition ( FPT ) .We see that FPTs can be rounded by added or removing nodes , which results to an increase in the proportion of cooperators at equilibrium . The results are derived for both static and dynamic theories of evolution of cooperation .In particular , we find that the presence of FPTs is required but not sufficient condition for high levels of agreement . Finally , we propose a simple plan for finding the best possible roundings led to maximal level of agreement .Rounding of initial - order phase transistions and optimal cooperation in scale free networks . P . Krawczyk 1 , A . Szolnoki 2 .1 Institute of Physics, University of Warsaw, Poland; krawczykp@wip.waw.pl .2 Department of Mathematics, University of Warsaw, Warsaw, Poland; aszolnok@wip.waw . pl . In this research we investigate how the presence of initial order phase transitions affects the evolution of cooperation in social dilemmas .First , we provide two new concepts - the reduced and the maximal cooperative states - which describe the range of values of parameters where cooperation prevails over defection . Then , using these concepts , we prove that any program with first order phase change has its own unique value of parameter relating to the maximum amount of cooperators .Next , we investigate the question of optimizing cooperation in such systems . To do so , we define the notion of rounding of initial order phase transitions , i . e . , changing their shape into smooth curves without affecting the orientation of the point of maximum fraction of cooperators within the period 0 , 1 .Using numerical simulations , we prove that the rounding procedure increases the fraction of cooperators at equilibrium in all observed situations . Finally , we present a technique allowing one to estimate the ideal rounding of given transition transition curve .",
        "rewrite_text": "**Title:** Rounding of First-Order Phase Transitions and Optimal Cooperation in Scale-Free Networks\n\n**Abstract:** This study explores the influence of rounding on the dynamics of complex networks characterized by first-order phase transitions (FPTs) and its implications for cooperation in social dilemmas. Our findings indicate that the process of adding or removing nodes can effectively round FPTs, leading to an increased equilibrium proportion of cooperators within the network. We derive our results through both static and dynamic evolutionary theories of cooperation, revealing that while the presence of FPTs is necessary for achieving high levels of cooperation, it alone does not guarantee such outcomes. \n\nTo further our understanding, we introduce two novel concepts: reduced cooperative states and maximal cooperative states, which delineate the parameter ranges where cooperation outperforms defection. We demonstrate that each program exhibiting a first-order phase transition is associated with a unique parameter value that corresponds to the maximum fraction of cooperators achievable. \n\nIn our quest to optimize cooperation within these systems, we define the concept of rounding first-order phase transitions. This involves transforming the transition curves into smoother shapes without altering the orientation of the peak representing the maximum fraction of cooperators within the interval [0, 1]. Through extensive numerical simulations, we establish that this rounding procedure consistently enhances the fraction of cooperators at equilibrium across all scenarios examined. \n\nFinally, we propose a method for estimating the optimal rounding of a given transition curve, which could serve as a valuable tool for maximizing cooperative behavior in scale-free networks. Our research contributes to the broader understanding of cooperation dynamics in complex systems and offers practical insights for fostering collaboration in social networks. \n\n**Authors:** P. Krawczyk, Institute of Physics, University of Warsaw, Poland; A. Szolnoki, Department of Mathematics, University of Warsaw, Poland.",
        "ori-fast-z-score": 0.16666666666666666,
        "water-fast-z-score": 6.185895741317419,
        "rewrite-fast-z-score": 0.3481553119113957
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Phenomenology with Massive Neutrinos .\nAbstract:\nWe present the phenomenological consequences for neutrino physics and cosmology in models where massive neutrinos are Majorana particles, i.e., their own antiparticles.  We show that such models can be constrained by current experiments on neutrinoless double beta decay (0νββ) as well as future ones. In addition we discuss how these constraints affect other phenomena like leptogenesis or dark matter searches. Finally, we comment on possible connections between 0νββ and lepton flavor violating processes. The Standard Model is an extremely successful description of particle interactions at low energies but it fails to explain several important issues including the origin of mass. One possibility to address this problem is to extend the SM by adding new fields which couple only very weakly to ordinary matter. Such extensions typically predict new light states beyond those already known experimentally. A particularly interesting class of theories contains right-handed neutrinos whose masses may be generated via seesaw mechanisms  1  . These heavy neutrinos could have observable effects in many different areas ranging from neutrino oscillations  2  , rare decays  3  , collider signatures  4  , gravitational waves  5  , to cosmology  6  .\nIn this work we consider scenarios where the three active neutrinos ν e , ν µ , ν τ mix with one or more sterile neutrinos N 1 , . . . N n  7, 8  . This mixing leads to additional contributions to the effective mass m ee = |<(V * ei V ej )m ij >| 2 relevant for neutrinoless double-beta decay  9  . Current experimental bounds  10  imply that m ee < O(10 −2 − 10 −1 eV). Future experiments will improve these limits significantly  11  . If the observed value turns out to be close to its upper bound then the corresponding scenario would provide evidence for Majorana neutrinos  12  . On the other hand if no signal is found then the model predicts that all neutrinos are Dirac fermions  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Phenomenology with Massive Neutrinos . Abstract : We present the phenomenological consequences for neutrino physics and cosmology in theories where enormous neutrinos are Majorana objects , i . e . , their own antiparticles .We see that such theories can be constrained by current observations on neutrinoless double alpha decay ( 0νββ ) as well as past ones . In addition we explain how these limits impact other processes like leptogenesis or black matter searches .Finally , we comment on potential links between 0νββ and lepton flavor violating reactions . The Standard Model is an incredibly effective explanation of particle relationships at low energies but it fails to explain several important problems including the origin of mass .One possibility to overcome this question is to expanded the SM by added new fields which couple only very weakly to normal matter . Such extensions typically predict new light fields beyond those already established experimentally .A notably important group of theories includes right - handed neutrinos whose masses might be obtained via seesaw processes 1 . These heavy neutrinos might have observable effects in multiple diverse regions ranging from neutrino oscillations 2 , rare decays 3 , collider signatures 4 , gravity waves 5 , to cosmology 6 .In this research we study situations where the three active neutrinos ν e , ν µ , ν τ combine with one or more sterile neutrinos N 1 , . ..N n  7, 8  .This mix leads to extra contributions to the effective mass m ee = | < ( V * ei V ej ) m ij > | 2 relevant for neutrinoless double - beta decay 9 . Current experimental bounds 10 confirm that m ee < O ( 10 −2 − 10 −1 eV ) .Future studies will enhance these limits substantially 11 . If the seen value turns out to be close to its upper bound then the associated scenario would offer evidence for Majorana neutrinos 12 .On the other hand if no signal is found then the model predicts that all neutrinos are Dirac fermions 13 .",
        "rewrite_text": "In this article, we explore the phenomenological implications of theories in which massive neutrinos are treated as Majorana particles, meaning they are their own antiparticles. We analyze how these theories can be constrained by existing observations of neutrinoless double beta decay (0νββ) as well as historical data. Our findings indicate that the limits imposed by these observations have significant repercussions for various processes, including leptogenesis and dark matter searches. We also discuss potential connections between 0νββ and lepton flavor-violating reactions.\n\nWhile the Standard Model (SM) of particle physics provides a robust framework for understanding interactions at low energies, it falls short in addressing several critical issues, such as the origin of mass. One approach to resolving this dilemma involves extending the SM by introducing new fields that interact only weakly with ordinary matter. These extensions often predict the existence of new light particles beyond those currently confirmed by experiments. A particularly noteworthy class of theories includes right-handed neutrinos, whose masses may arise through seesaw mechanisms. These heavy neutrinos could have observable effects across a wide range of phenomena, including neutrino oscillations, rare decay processes, collider signatures, gravitational waves, and cosmological implications.\n\nIn our research, we investigate scenarios where the three active neutrinos (ν_e, ν_μ, ν_τ) mix with one or more sterile neutrinos (N_1, ..., N_n). This mixing results in additional contributions to the effective mass parameter m_ee = |< (V*ei Vej) m_ij >|^2, which is crucial for understanding neutrinoless double beta decay. Current experimental constraints indicate that m_ee is less than O(10^(-2) - 10^(-1) eV), and future experiments are expected to significantly improve these limits. Should the measured value approach its upper limit, it would provide compelling evidence for the existence of Majorana neutrinos. Conversely, if no signal is detected, it would suggest that all neutrinos are Dirac fermions.",
        "ori-fast-z-score": -1.8864844365675972,
        "water-fast-z-score": 5.724196306702298,
        "rewrite-fast-z-score": -0.7526178090063816
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detailed kinetic study of the ring opening of cycloalkanes by CBS-QB3 calculations .\nAbstract:\nThe reaction mechanism for the ring-opening reactions of cyclic alkanes with water has been studied using density functional theory (DFT) and ab initio molecular orbital methods at the B3LYP/6-311++G(d,p), MP2/6-31+G*, QCISD/6-31+G* levels in conjunction with the CBS-QB3 method to calculate enthalpies of formation.  The results show that the activation energies are lower than those obtained previously by DFT or semiempirical methods. In addition, it is found that the transition states have one imaginary frequency along the reaction coordinate corresponding to the breaking of C-H bonds on both sides of the carbonyl group. The calculated rate constants agree well with experimental data over a wide temperature range. Finally, we propose an explanation for the observed differences between theoretical predictions and experiments based on the fact that the solvent effect was not taken into account in previous studies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detailed kinetic study of the ring opening of cycloalkanes by CBS - QB3 calculations . Abstract : The reaction mechanism for the ring - entrance processes of cyclic alkanes with water has been studied utilizing density functional theory ( DFT ) and ab initio molecular orbital methods at the B3LYP / 6 - 311 + + G ( d , p ) , MP2 / 6 - 31 + G * , QCISD / 6 - 31 + G * levels in partnership with the CBS - QB3 technique to estimate enthalpies of formation .The results show that the activation energies are lower than those achieved formerly by DFT or semiempirical methods . In addition , it is found that the transfer states have one imaginary frequency along the reaction coordinate relating to the breaking of C - H bonds on both sides of the carbonyl group .The measured rate constants accord well with experimental evidence over a broad temperature spectrum . Finally , we propose an excuse for the known differences between theoretical estimates and experiments based on the fact that the solvent influence was not took into consideration in earlier studies .",
        "rewrite_text": "**Title:** Comprehensive Kinetic Analysis of Cycloalkane Ring Opening via CBS-QB3 Calculations\n\n**Abstract:** This study investigates the reaction mechanisms involved in the ring-opening processes of cyclic alkanes when interacting with water, employing advanced computational techniques including density functional theory (DFT) and ab initio molecular orbital methods. Specifically, calculations were performed using the B3LYP/6-311++G(d,p), MP2/6-31+G*, and QCISD/6-31+G* methodologies, in conjunction with the CBS-QB3 approach to accurately estimate the enthalpies of formation. Our findings reveal that the activation energies for these reactions are significantly lower than those reported in previous studies utilizing DFT or semiempirical methods. Notably, the transition states identified exhibit one imaginary frequency along the reaction coordinate, which is associated with the cleavage of C-H bonds adjacent to the carbonyl group. Furthermore, the calculated rate constants align closely with experimental data across a wide range of temperatures, indicating the robustness of our computational approach. We also address the discrepancies observed between theoretical predictions and experimental outcomes, attributing these differences to the lack of consideration for solvent effects in earlier theoretical analyses. This work not only enhances the understanding of the kinetic behavior of cycloalkane ring-opening reactions but also underscores the importance of incorporating solvent interactions in computational studies to achieve more accurate predictions.",
        "ori-fast-z-score": -1.585187847802434,
        "water-fast-z-score": 3.487772492870674,
        "rewrite-fast-z-score": -2.2223355980148636
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Growth and migration of solids in evolving protostellar disks I: Methods and Analytical tests .\nAbstract:\nWe present an analytical model for the growth, radial drift and fragmentation of dust grains in protoplanetary disks that evolve under the combined effects of viscous accretion onto the central star and photoevaporation by external radiation fields. We show how these processes affect the evolution of grain size distributions as well as their spatial distribution within the disk. In particular we find that: (i) The maximum grain sizes are limited to values between 1 mm and 10 cm depending on the strength of the stellar UV field. (ii) Grains grow faster at larger distances from the star due to lower gas densities and higher temperatures. (iii) Fragmentation is more efficient closer to the star where the local pressure maxima lead to enhanced collisional velocities.  These results have important implications for planet formation scenarios since they suggest that planetesimals can form only close to the star while large bodies such as asteroids or comets may be able to form farther out in the disk.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Growth and movement of solids in evolving protostellar disks I : Methods and Analytical tests . Abstract : We present an analytical theory for the development , radial drift and fragmentation of dust grains in protoplanetary disks that develop under the combined influences of viscous accretion onto the main star and photoevaporation by external emission fields .We see how these mechanisms affect the evolution of grain length distributions as also as their temporal distribution within the disk . In particular we find that : ( i ) The maximum grain sizes are limited to values between 1 mm and 10 mm depending on the strength of the stellar UV field .( ii ) Grains grow better at larger distances from the star due to smaller gas densities and larger temperatures . ( iii ) Fragmentation is more efficient closer to the star where the local pressure maxima lead to greater collisional velocities .These conclusions have important implications for planet development predictions since they propose that planetesimals can form only close to the star while huge bodies such as asteroids or comets might be possible to form farther out in the disk .",
        "rewrite_text": "We introduce an analytical framework that explores the growth, radial migration, and fragmentation of dust grains within protoplanetary disks, influenced by the dual processes of viscous accretion onto the central star and photoevaporation driven by external radiation fields. Our study reveals how these dynamic mechanisms shape both the size distribution of grains and their temporal evolution throughout the disk. Notably, we identify several key findings: (i) the maximum achievable sizes of dust grains are constrained to a range between 1 mm and 10 mm, contingent upon the intensity of the stellar ultraviolet (UV) radiation; (ii) dust grains exhibit enhanced growth rates at greater distances from the star, attributed to lower gas densities and elevated temperatures in these regions; and (iii) fragmentation processes are more pronounced in the vicinity of the star, where local pressure peaks contribute to increased collisional velocities among grains. These insights carry significant implications for our understanding of planet formation, suggesting that planetesimals are likely to form predominantly in the inner regions of the disk, while larger bodies such as asteroids and comets may emerge in the outer regions. This research enhances our comprehension of the complex interactions within protoplanetary disks and their role in the early stages of planetary system development.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 2.8856078516089685
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The magnetic field topology associated to two M flares .\nAbstract:\nWe present the analysis of the magnetic field configuration in active region NOAA 11158, which produced several large solar flares during its passage across the central meridian on February 15-16, 2011 . We use vector magnetograms obtained by SDO/HMI and SOHO/MDI instruments as well as photospheric line-of-sight magnetograms provided by GONG network. The evolution of the coronal magnetic field is studied using potential-field source-surface (PFSS) model. In addition we perform NLFFF extrapolations with different codes for comparison purposes. Our results show that both PFSS and NLFFF models are able to reproduce the large-scale structure of the corona but differ significantly at small scales. This discrepancy can be explained by considering the effect of plasma flows along open field lines. Finally, we study the relationship between the observed photospheric motions and the changes in the coronal magnetic field. Active Region NOAA 11158 was one of the most energetic regions ever recorded. It produced several X-class flares including an X2.2 event on February 16, 2011 , when it crossed the central meridian. Several authors have analyzed this active region before and after the flare occurrence. They found evidence of strong shearing motions in the photosphere prior to the flare onset (e.g., Liu et al. , 2012; Petrie & Sudol 2010; Schrijver 2009 ). These observations suggest that the energy release may be triggered by reconnection processes involving twisted flux tubes (Petrie 2013) . However, there has been no detailed investigation into how these photospheric motions affect the coronal magnetic field or whether they trigger any significant reconfiguration of the magnetic field.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The magnetic force geometry associated to two M flares . Abstract : We report the examination of the magnetic field configuration in active region NOAA 11158 , which produced numerous large solar flares during its passage across the central meridian on February 15 - 16 , 2011 .We use vector magnetograms achieved by SDO / HMI and SOHO / MDI instruments as also as photospheric line - of - view magnetograms supplied by GONG channel . The evolution of the coronal magnetic force is studied utilizing potential - field source - surface ( PFSS ) model .In addition we perform NLFFF extrapolations with various codes for comparison purposes . Our results show that both PFSS and NLFFF models are able to reproduce the huge - scale structure of the corona but change considerably at small scales .This discrepancy can be described by examining the impact of plasma flows along open field lines . Finally , we study the relationship between the seen photospheric movements and the changes in the coronal magnetic field .Active Region NOAA 11158 was one of the most intense regions ever recorded . It produced numerous X - class flares notably an X2 . 2 event on February 16 , 2011 , when it crossed the central meridian .Several scientists have analyzed this active region before and after the flare outbreak . They found proof of stable shearing motions in the photosphere prior to the flare outbreak ( e . g . , Liu et al ., 2012 ; Petrie & Sudol 2010 ; Schrijver 2009 ) . These measurements suggest that the electricity release may be triggered by reconnection pathways involving twisted flux tubes ( Petrie 2013 ) .However , there has been no comprehensive investigation into how these photospheric movements influence the coronal magnetic field or whether they cause any considerable reconfiguration of the magnetic current .",
        "rewrite_text": "**Title: The Magnetic Force Geometry Associated with Two M Flare Events**\n\n**Abstract:** This study investigates the magnetic field configuration within active region NOAA 11158, which was responsible for several significant solar flares during its transit across the central meridian on February 15-16, 2011. Utilizing vector magnetograms obtained from the Solar Dynamics Observatory (SDO) / Helioseismic and Magnetic Imager (HMI) and the Solar and Heliospheric Observatory (SOHO) / Michelson Doppler Imager (MDI), alongside photospheric line-of-sight magnetograms provided by the Global Oscillation Network Group (GONG), we analyze the evolution of the coronal magnetic force. The potential-field source-surface (PFSS) model is employed to study the coronal magnetic structure, and we also conduct non-linear force-free field (NLFFF) extrapolations using various computational codes for comparative analysis. Our findings indicate that while both PFSS and NLFFF models successfully replicate the large-scale coronal structure, they exhibit significant discrepancies at smaller scales. This variation can be attributed to the influence of plasma flows along open magnetic field lines. Furthermore, we explore the correlation between observed photospheric motions and alterations in the coronal magnetic field. Active Region NOAA 11158 is noted as one of the most potent regions documented, producing multiple X-class flares, including a notable X2.2 event on February 16, 2011, coinciding with its central meridian passage. Previous studies have identified stable shearing motions in the photosphere prior to the flare events (e.g., Liu et al., 2012; Petrie & Sudol, 2010; Schrijver, 2009), suggesting that the release of magnetic energy may be initiated by reconnection processes involving twisted flux tubes (Petrie, 2013). However, a thorough investigation into the effects of these photospheric movements on the coronal magnetic field and their potential to induce significant reconfigurations of the magnetic current has yet to be conducted.",
        "ori-fast-z-score": -1.1627553482998907,
        "water-fast-z-score": 6.300231216854408,
        "rewrite-fast-z-score": -0.6363636363636364
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Functional renormalization for quantum phase transitions with non-relativistic bosons .\nAbstract:\nWe study the functional renormalization group (FRG) approach to quantum phase transitions in systems of interacting nonrelativistic bosons, focusing on its application to fermionic superfluids and superconductors. We show that FRG is able to capture both the mean-field behavior at weak coupling as well as the strong-coupling physics beyond it. In particular we demonstrate how the FRG can be used to calculate the critical temperature Tc of the transition between normal state and superfluid/superconducting states. The results are compared against those obtained by other methods such as Monte Carlo simulations or variational approaches. Finally, we discuss possible extensions of our work which could lead to further improvements. Quantum phase transitions occur when the ground-state properties of a system change abruptly upon varying some external parameter like pressure, magnetic field etc.. They have been studied extensively over many years using various theoretical techniques ranging from perturbative expansions around the Fermi liquid fixed point  1  , to numerical calculations based on exact diagonalizations  2  . Recently there has also been an increasing interest in applying the Functional Renormalization Group (FRG), originally developed within the context of strongly correlated electron systems  3  , to this problem  4  .\nIn this contribution we will focus on applications of the FRG method to fermionic superfluides and superconductors. These phases arise due to pairing correlations among fermions leading to macroscopic occupation of single-particle states below certain energy scale called the gap. This phenomenon is known as BCS-BEC crossover  5  where  BCS  stands for Bardeen-Cooper-Schrieffer theory  6  describing conventional s-wave superconductivity while  BEC  refers to Bose-Einstein condensation  7, 8  occurring in p-wave superfluids  9  . It turns out that these two limits correspond to different universality classes  10  so that one expects a smooth crossover between them  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Functional renormalization for quantum phase transitions with non - relativistic bosons . Abstract : We research the functional renormalization group ( FRG ) approach to quantum phase transitions in networks of interacting nonrelativistic bosons , concentrating on its use to fermionic superfluids and superconductors .We see that FRG is ability to capture both the mean - field behavior at weak interaction as well as the strong - interaction mechanics beyond it . In particular we prove how the FRG can be used to estimate the significant heat Tc of the transition between normal state and superfluid / superconducting states .The results are compared against those achieved by other methods such as Monte Carlo simulations or variational approaches . Finally , we explain possible extensions of our work which could lead to further developments .Quantum mode transitions occur when the ground - state properties of a system shift abruptly upon increasing some external parameter like pressure , magnetic force etc . . They have been studied frequently over numerous years employing several experimental methods ranging from perturbative expansions around the Fermi solid fixed point 1 , to numerical measurements based on complete diagonalizations 2 .Recently there has especially been an increasing interest in applying the Functional Renormalization Group ( FRG ) , previously developed within the context of highly correlated electron structures 3 , to this question 4 . In this contribution we will focus on uses of the FRG method to fermionic superfluides and superconductors .These periods arise due to pairing correlations among fermions leading to macroscopic occupation of single - particle states below certain power scale called the gap . This phenomenon is known as BCS - BEC crossover 5 where BCS stands for Bardeen - Cooper - Schrieffer model 6 representing standard s - wave superconductivity while BEC refers to Bose - Einstein condensation 7 , 8 occurring in k - wave superfluids 9 .It turns out that these two limits relate to different universality classes 10 so that one expects a smooth crossover between them 11 .",
        "rewrite_text": "In this article, we investigate the application of the functional renormalization group (FRG) approach to quantum phase transitions in systems of interacting non-relativistic bosons, with a particular emphasis on its relevance to fermionic superfluids and superconductors. Our study demonstrates that the FRG method is adept at capturing both the mean-field behavior observed in weakly interacting systems and the complex dynamics that arise in strongly interacting regimes. Specifically, we establish how the FRG can be employed to accurately estimate the critical temperature (Tc) associated with the transition from a normal state to superfluid or superconducting states. We compare our findings with results obtained from alternative methodologies, including Monte Carlo simulations and variational techniques, highlighting the robustness of the FRG approach.\n\nQuantum phase transitions are characterized by abrupt changes in the ground-state properties of a system as an external parameter, such as pressure or magnetic field, is varied. These transitions have been extensively studied over the years using a variety of experimental techniques, ranging from perturbative expansions around the Fermi solid fixed point to numerical methods involving complete diagonalizations. Recently, there has been a growing interest in leveraging the FRG, which was initially developed for highly correlated electron systems, to explore these quantum transitions.\n\nIn this work, we specifically focus on the application of the FRG to fermionic superfluids and superconductors, phenomena that arise due to pairing correlations among fermions, resulting in a macroscopic occupation of single-particle states below a critical energy scale known as the gap. This behavior is encapsulated in the BCS-BEC crossover framework, where BCS refers to the Bardeen-Cooper-Schrieffer theory of conventional s-wave superconductivity, and BEC denotes Bose-Einstein condensation in k-wave superfluids. Notably, these two regimes correspond to distinct universality classes, suggesting the existence of a smooth crossover between them. We also discuss potential extensions of our research that could pave the way for further advancements in this field.",
        "ori-fast-z-score": 0.08481889296799709,
        "water-fast-z-score": 6.930264888773836,
        "rewrite-fast-z-score": -0.5144957554275265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Statistical analyses of long-term variability of AGN at high radio frequencies .\nAbstract:\nWe present statistical results on the long-term variability (LTV) properties of active galactic nuclei (AGNs). We use data obtained by the Owens Valley Radio Observatory (OVRO), University of California, Berkeley, and Metsähovi observatories in the period between 1989 and 2009 to study LTVs with time scales ranging from months up to several years for more than 100 sources. The main goal is to investigate whether there are any differences in the LTV behavior among different classes of AGNs. In addition we also aim to find out how the observed flux density variations depend on source luminosity and redshift. \n \n Keywords: Active Galactic Nuclei, Long-term Variability, OVRO, Metsähovi, High-frequency observations \n \n \n \n 1 Introduction \n \n It has been known since the early 1980s that many extragalactic radio sources show significant flux density variations over timescales as short as days or weeks (e.g., Aller et al. 1985; Quirrenbach et al. 1991; Witzel et al. 1986 ). However, it was not until the late 1990s when systematic studies were carried out using large samples of objects monitored simultaneously at multiple wavelengths (see e.g., Heeschen et al. 1987; Edelson & Krolik 1988; Hughes et al. 1992; Carini et al. 1993; Wagner et al. 1996) . These investigations revealed that most of these variable sources have flat-spectrum cores which can be identified with quasars and/or blazars. Furthermore, they showed that the majority of these sources exhibit rapid flares superimposed onto slower trends such as linear increases/decreases or exponential decays/flares. This type of behaviour is commonly referred to as  double-duty cycles  because the light curves often contain both fast flaring activity and longer term trends (Wagner et al. 1996; . \n \n Since then, numerous multi-wavelength campaigns have been conducted to monitor the flux densities of thousands of AGNs simultaneously across wide frequency ranges (from infrared through optical bands",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Statistical analyses of large - term variability of AGN at high radio stations . Abstract : We report statistical findings on the long - term variability ( LTV ) characteristics of active galactic nuclei ( AGNs ) .We use data acquired by the Owens Valley Radio Observatory ( OVRO ) , University of California , Berkeley , and Metsähovi observatories in the period between 1989 and 2009 to study LTVs with time ranges varied from months up to several months for more than 100 sources . The main goal is to examine whether there are any variations in the LTV behavior among different categories of AGNs .In addition we also aim to find out how the reported flux concentration variations differ on source luminosity and redshift . Keywords : Active Galactic Nuclei , Long - term Variability , OVRO , Metsähovi , High - frequency observations 1 Introduction It has been known since the early 1980s that several extragalactic radio sources show considerable flux concentration variations over timescales as short as days or weeks ( e . g . , Aller et al .1985 ; Quirrenbach et al . 1991 ; Witzel et al .1986 ) . However , it was not until the late 1990s when comprehensive experiments were carried out utilizing large specimens of items detected simultaneously at multiple wavelengths ( saw e . g . , Heeschen et al .1987 ; Edelson & Krolik 1988 ; Hughes et al . 1992 ; Carini et al .1993 ; Wagner et al . 1996 ) .These analyses showed that most of these variable sources have flat - spectrum cores which can be identified with quasars and / or blazars . Furthermore , they demonstrated that the majority of these sources exhibit rapid flares superimposed onto smaller cycles such as continuous gains / decreases or exponential decays / flares .This kind of behaviour is often referred to as double - duty cycles because the light surfaces often contain both rapid flaring activity and longer term changes ( Wagner et al . 1996 ; .Since then , various multi - wavelength campaigns have been performed to observe the flux densities of thousands of AGNs continuously across wide wavelength ranges ( from infrared through optical bands",
        "rewrite_text": "**Title:** Statistical Analyses of Long-Term Variability of AGN at High Radio Frequencies\n\n**Abstract:** This study presents a comprehensive statistical analysis of the long-term variability (LTV) characteristics of active galactic nuclei (AGNs). Utilizing data collected from the Owens Valley Radio Observatory (OVRO), the University of California, Berkeley, and the Metsähovi Radio Observatory over a two-decade span from 1989 to 2009, we investigate LTV across a diverse sample of over 100 AGN sources. Our primary objective is to discern variations in LTV behavior among different classifications of AGNs, thereby enhancing our understanding of their underlying mechanisms. Additionally, we explore how variations in flux concentration correlate with the luminosity and redshift of the sources, providing insights into the physical processes driving these phenomena. \n\nThe significance of this research is underscored by previous findings dating back to the early 1980s, which indicated that numerous extragalactic radio sources exhibit substantial flux variations on timescales as brief as days or weeks. However, it was not until the late 1990s that extensive studies employing large datasets across multiple wavelengths were conducted, revealing that many variable sources possess flat-spectrum cores, typically associated with quasars and blazars. These investigations highlighted the presence of rapid flares superimposed on longer-term variability patterns, a behavior often described as double-duty cycles, where both rapid flaring and gradual changes coexist. \n\nOur analysis builds on this foundational work by incorporating a larger sample size and a more extended observational timeframe, allowing for a more nuanced understanding of AGN variability. By examining the interplay between source luminosity, redshift, and LTV, we aim to contribute to the broader discourse on the nature of AGNs and their variability, ultimately enhancing our comprehension of these complex astronomical entities. \n\n**Keywords:** Active Galactic Nuclei, Long-Term Variability, OVRO, Metsähovi, High-Frequency Observations",
        "ori-fast-z-score": -0.08362420100070908,
        "water-fast-z-score": 6.989860944852108,
        "rewrite-fast-z-score": 2.3312620206007844
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gravity duals of half-BPS Wilson loops .\nAbstract:\nWe study the gravity dual description of 1/2 BPS Wilson loops in N = 4 super Yang-Mills theory on R × S 3 . We show that these operators are described by D-branes wrapping an S 2 inside AdS 5 and ending at the boundary of AdS 5 , where they end on a string which is stretched between two parallel D3 branes located at different points along the radial direction of AdS 5 .\nThe expectation value of this operator can be computed using standard techniques for computing open strings attached to D-branes, and we find agreement with results obtained previously via direct gauge-theory computations. \n \n The work presented here was done while the author was visiting the Institute for Advanced Study (IAS) during the Fall semester 2005 as part of the IAS-CMS program. It has been supported in part by NSF grant PHY-0456728. This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility operated under Contract DE-AC02-05CH11231.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gravity duals of half - BPS Wilson loops . Abstract : We research the gravity dual description of 1 / 2 BPS Wilson loops in N = 4 super Yang - Mills theory on R × S 3 .We see that these operators are explained by D - branes wrapping an S 2 inside AdS 5 and ending at the boundary of AdS 5 , where they end on a string which is stretched between two connected D3 branes located at different points along the radial direction of AdS 5 . The assumption value of this operator can be computed using conventional methods for modeling open strings attached to D - branes , and we find agreement with conclusions derived earlier via direct gauge - theory computations .The project displayed here was done while the writer was attending the Institute for Advanced Study ( IAS ) during the Fall semester 2005 as part of the IAS - CMS program . It has been supported in part by NSF grant PHY - 0456728 .This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility operated under Contract DE-AC02-05CH11231.",
        "rewrite_text": "In this study, we investigate the gravitational dual description of half-BPS Wilson loops within the framework of N = 4 super Yang-Mills theory defined on the product space R × S^3. Our analysis reveals that these operators can be effectively represented by D-branes that wrap an S^2 surface embedded in AdS_5, extending to the boundary of AdS_5. At this boundary, the D-branes terminate on a string that connects two D3 branes situated at distinct positions along the radial direction of AdS_5. We employ established methodologies for modeling open strings attached to D-branes to compute the expectation value of this operator. Our findings are consistent with previous results obtained through direct computations in gauge theory, thereby reinforcing the validity of our approach. This research was conducted during the Fall semester of 2005 while the author was a participant in the IAS-CMS program at the Institute for Advanced Study (IAS). The project received partial funding from the National Science Foundation under grant PHY-0456728. Additionally, computational resources were provided by the National Energy Research Scientific Computing Center, which operates as a User Facility under the auspices of the DOE Office of Science, under Contract DE-AC02-05CH11231.",
        "ori-fast-z-score": 0.5,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": 3.2504180333157686
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tight binding formulation of the dielectric response in semiconductor nanocrystals .\nAbstract:\nWe present an ab initio tight-binding model for calculating the optical properties of semiconductor nanocrystals, which is based on the solution of the Bethe-Salpeter equation (BSE) within the framework of density functional theory (DFT). The BSE describes excitonic effects and allows to calculate absorption spectra with high accuracy. We show that our approach reproduces experimental results very well. In particular we find good agreement between calculated and measured absorption cross sections at low energies where quantum confinement dominates over electron-hole exchange interactions. Our method can be applied to any type of semiconductor material including doped systems as well as core-shell structures. Semiconductor nanocrystals are promising candidates for applications such as light-emitting diodes or solar cells due to their unique optoelectronic properties. However, it remains challenging to predict these properties accurately since they depend sensitively on the electronic structure of the system. Here we propose a new theoretical approach to tackle this problem by combining DFT calculations with the Bethe-Salpether equation (BSE), which takes into account excitonic effects beyond mean-field approaches like Kohn-Sham DFT. This enables us to obtain accurate predictions for the optical properties of semiconductor nanostructures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tight binding characterization of the dielectric response in semiconductor nanocrystals . Abstract : We create an ab initio close - binding model for determining the optical properties of semiconductor nanocrystals , which is based on the solve of the Bethe - Salpeter equation ( BSE ) within the framework of density functional theory ( DFT ) .The BSE describes excitonic effects and allows to estimate absorption spectra with high clarity . We see that our approach reproduces experimental results very well .In particular we find good agreement between calculated and reported absorption cross sections at low energies where quantum confinement dominates over electron - hole exchange interactions . Our techniques can be applied to any type of semiconductor material including doped systems as well as core - shell systems .Semiconductor nanocrystals are promising candidates for applications such as light - emitting diodes or solar cells due to their specific optoelectronic properties . However , it remains challenging to predict these characteristics properly since they rely sensitively on the electronic properties of the device .Here we undertake a new theoretical technique to tackle this question by combining DFT calculations with the Bethe - Salpether equation ( BSE ) , which gives into consideration excitonic effects beyond mean - field methods like Kohn - Sham DFT . This enables us to obtain precise predictions for the optical properties of semiconductor nanostructures .",
        "rewrite_text": "We present a comprehensive study on the dielectric response of semiconductor nanocrystals through the development of an ab initio tight-binding model. This model is grounded in the solution of the Bethe-Salpeter equation (BSE) within the density functional theory (DFT) framework. The BSE effectively captures excitonic effects, allowing for the accurate estimation of absorption spectra. Our findings demonstrate that this approach aligns closely with experimental data, particularly in the low-energy regime where quantum confinement effects prevail over electron-hole exchange interactions. The calculated absorption cross sections show remarkable agreement with reported values, validating the robustness of our methodology. Importantly, our techniques are versatile and can be applied to a wide range of semiconductor materials, including both doped and core-shell systems. Semiconductor nanocrystals are increasingly recognized for their potential in various applications, such as light-emitting diodes and solar cells, attributed to their unique optoelectronic properties. However, accurately predicting these characteristics poses significant challenges, as they are highly sensitive to the underlying electronic properties of the materials. In this work, we introduce a novel theoretical approach that synergizes DFT calculations with the BSE, thereby accounting for excitonic effects that are often overlooked in mean-field methods like Kohn-Sham DFT. This innovative combination allows us to achieve precise predictions of the optical properties of semiconductor nanostructures, paving the way for enhanced understanding and optimization of their performance in practical applications. Our results not only contribute to the theoretical framework surrounding semiconductor nanocrystals but also hold implications for the design of advanced optoelectronic devices.",
        "ori-fast-z-score": 1.1881770515720091,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 0.6363636363636364
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Recent Results on Thermal Casimir Force between Dielectrics and Related Problems .\nAbstract:\nThe thermal Casimir force is the quantum mechanical effect that arises when two objects are separated by vacuum, which causes them to attract each other due to zero-point fluctuations in their electromagnetic fields.  In this talk I will present some recent results for the thermal Casimir force between dielectrics as well as related problems such as the van der Waals interaction between polarizable atoms or molecules at finite temperature. The first part of my talk will be devoted to an overview of our work on the subject published recently in Physical Review Letters (PRL)  1  . This includes new exact expressions for the thermal Casimir energy density and pressure valid for arbitrary temperatures and dielectric functions. These formulas can also be used to calculate the leading order corrections to Lifshitz theory  2  , which has been widely applied to describe the Casimir force between real materials  3  .\nIn the second part of my talk I will discuss how these results have been extended to include retardation effects  4  . We find that retardation leads to additional contributions to both the energy density and pressure that depend strongly on the distance between the bodies. Finally, we will show how these results can be used to study the van der Waals interactions between polarizable atoms or molecules; i.e., systems where retardation plays no role but where the dispersion forces still give rise to non-trivial behavior  5  .  For example, we will demonstrate how one can use our formalism to obtain accurate predictions for the critical point of the liquid-vapor phase transition in water  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Recent Results on Thermal Casimir Force between Dielectrics and Related Problems . Abstract : The heating Casimir force is the quantum mechanical effect that arises when two bodies are apart by vacuum , which causes them to attract each other owing to zero - point fluctuations in their electromagnetic fields .In this talk I will present some latest findings for the thermal Casimir force between dielectrics as also as related problems such as the van der Waals interaction between polarizable atoms or atoms at finite temperature . The first part of my talk will be devoted to an overview of our work on the subject released lately in Physical Review Letters ( PRL ) 1 .This contains new accurate expressions for the thermal Casimir energy density and tension applicable for arbitrary pressures and dielectric functions . These formulas can also be used to approximate the main order corrections to Lifshitz theory 2 , which has been widely applied to define the Casimir force between real substances 3 .In the second part of my talk I will explore how these results have been extended to include retardation effects 4 . We see that retardation gives to extra contributions to both the power concentration and force that rely heavily on the distance between the bodies .Finally , we will show how these results can be used to study the van der Waals relationships between polarizable atoms or compounds ; i . e . , systems where retardation plays no part but where the dispersion forces nevertheless give rise to non - trivial interaction 5 . For instance , we will prove how one can using our formalism to obtain precise predictions for the important moment of the liquid - fluid phase change in water 6 .",
        "rewrite_text": "**Title:** Recent Findings on Thermal Casimir Force Between Dielectrics and Associated Phenomena\n\n**Abstract:** The thermal Casimir force is a quantum mechanical phenomenon that occurs when two bodies are separated by a vacuum, resulting in an attractive force due to zero-point fluctuations in their electromagnetic fields. In this presentation, I will discuss recent advancements in understanding the thermal Casimir force between dielectric materials, as well as related issues such as the van der Waals interactions among polarizable atoms and atoms at finite temperatures. The initial segment of my talk will provide a comprehensive overview of our recent research published in *Physical Review Letters* (PRL), which introduces new, precise expressions for the thermal Casimir energy density and tension that are applicable across a range of pressures and dielectric functions. These newly derived formulas also facilitate the approximation of leading-order corrections to Lifshitz theory, a framework that has been extensively utilized to characterize the Casimir force between real materials. \n\nIn the latter part of the presentation, I will delve into how these findings have been adapted to account for retardation effects. Our analysis reveals that these retardation effects contribute additional terms to both the energy density and the force, which are significantly influenced by the separation distance between the interacting bodies. Furthermore, we will demonstrate how our results can be applied to investigate van der Waals interactions in systems where retardation is negligible, yet dispersion forces still lead to complex interactions. For example, we will illustrate how our formalism can yield accurate predictions regarding the critical point of the liquid-fluid phase transition in water. This work not only enhances our understanding of thermal Casimir forces but also opens new avenues for exploring fundamental interactions in various physical systems.",
        "ori-fast-z-score": 0.08944271909999159,
        "water-fast-z-score": 7.187587726270522,
        "rewrite-fast-z-score": -1.1358152736593492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surface plasmon polaritons and surface phonon polaritons on metallic and semiconducting spheres: Exact and semiclassical descriptions .\nAbstract:\nWe present exact solutions for the electromagnetic field in the presence of spherical particles with arbitrary dielectric functions, including both metals and insulators. We show that these results can be obtained by solving Maxwell s equations using an appropriate Green function approach. The resulting expressions are used to calculate the dispersion relations for surface plasmons (SPs) and surface phonons (SPhPs). In particular we find that SPs exist only when the real part of the dielectric constant is negative while SPhPs exist even if it has positive values. Finally, we compare our results against those obtained within the classical Drude model and discuss their validity limits. Surface plasmons (SPs), which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over many decades  1  . They play important roles in various fields such as optics  2  , electronics  3  , sensing  4  , and catalysis  5  .\nRecently there has also been growing interest in studying surface phonon-polaritons (SPhPs), which are analogous excitations associated with longitudinal acoustic waves  6  . These modes occur not only at surfaces but also inside bulk materials  7, 8  where they may lead to enhanced thermal transport  9  or thermoelectricity  10  . Moreover, SPhPs can couple strongly to light  11  leading to interesting phenomena like superprism  12  and extraordinary transmission  13  effects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surface plasmon polaritons and surface phonon polaritons on metallic and semiconducting objects : Exact and semiclassical descriptions . Abstract : We present precise solutions for the electromagnetic field in the presence of spherical objects with arbitrary dielectric functions , including both metals and insulators .We see that these results can be obtained by solving Maxwell s equations using an appropriate Green function method . The resulting expressions are using to estimate the dispersion relations for ground plasmons ( SPs ) and surface phonons ( SPhPs ) .In particular we find that SPs occur only when the real part of the dielectric constant is zero while SPhPs exist even if it has positive values . Finally , we compare our findings against those achieved within the classical Drude theory and consider their efficacy limits .Surface plasmons ( SPs ) , which are collective oscillations of conduction electrons at metal - dielectric connections , have been heavily discovered over numerous years 1 . They play major roles in different fields such as optics 2 , electronics 3 , sensing 4 , and catalysis 5 .Recently there has especially been growing interest in investigating surface phonon - polaritons ( SPhPs ) , which are analogous excitations associated with longitudinal acoustic waves 6 . These modes happen not only at surfaces but also inside bulk surfaces 7 , 8 where they may contribute to enhanced thermal transport 9 or thermoelectricity 10 .Moreover , SPhPs can close intensely to light 11 contributing to curious phenomena like superprism 12 and exceptional transmission 13 phenomena .",
        "rewrite_text": "**Title:** Surface Plasmon Polaritons and Surface Phonon Polaritons on Metallic and Semiconducting Objects: Exact and Semiclassical Descriptions\n\n**Abstract:** In this study, we provide detailed solutions for the electromagnetic field surrounding spherical objects characterized by arbitrary dielectric functions, encompassing both metallic and insulating materials. Our approach employs a Green function method to effectively solve Maxwell's equations, yielding precise expressions that facilitate the estimation of dispersion relations for surface plasmons (SPs) and surface phonon polaritons (SPhPs). Notably, our findings indicate that SPs emerge exclusively when the real part of the dielectric constant is zero, whereas SPhPs can exist even when this value is positive. We further compare our results with those derived from classical Drude theory, assessing the limitations of this traditional framework.\n\nSurface plasmons, which represent collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over the years due to their significant implications in various domains, including optics, electronics, sensing, and catalysis. Recently, there has been a surge of interest in surface phonon polaritons, which are analogous excitations linked to longitudinal acoustic waves. These modes are not confined to surfaces; they also manifest within the bulk of materials, potentially enhancing thermal transport and thermoelectric properties. Additionally, SPhPs can interact closely with light, leading to intriguing phenomena such as superprism effects and exceptional transmission. This research contributes to a deeper understanding of both SPs and SPhPs, paving the way for advancements in nanophotonics and material science applications.",
        "ori-fast-z-score": -0.9712858623572641,
        "water-fast-z-score": 5.581052602166382,
        "rewrite-fast-z-score": 0.6767155423319645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Midlatitude Cirrus Clouds and Multiple Tropopauses from a 2002-2006 Climatology over the SIRTA Observatory .\nAbstract:\nCirrus clouds are an important component in Earth s radiation budget, but their radiative properties remain poorly understood because they occur at high altitude where direct measurements are difficult to make. In this study we use data collected by the Scripps Institution of Oceanography (SIO) lidar system located on the SIRTA observatory near Paris during  2002   2003   2004   2005   2006  . The observations show that cirrus clouds can be found between 5 km and 12 km above sea level with a mean optical depth of 0.2 ± 0.1. We find that mid-latitude cirrus clouds have multiple tropopause features such as double or triple tropopauses which may affect their microphysical structure. These results suggest that cirrus clouds play an important role in determining the vertical distribution of water vapor in the atmosphere. Citation: \nIntroduction\n\nCirrus clouds cover about 10%-20% of the globe s surface area  Sassen et al., 2008  , yet little is known about how these clouds form and evolve. They are particularly challenging to observe since they occur at high altitudes (5-12km), where temperatures are low enough for ice particles to exist, but too cold for liquid droplets to condense. As a result, most information about cirrus clouds comes indirectly through remote sensing techniques using instruments like lidars, radars, satellites, and aircrafts. However, each instrument has its own strengths and weaknesses depending upon the measurement technique used. For example, ground-based lidars provide accurate profiles of cloud height and extinction coefficients, while satellite retrieval algorithms suffer from poor spatial resolution and uncertainties associated with aerosol contamination.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Midlatitude Cirrus Clouds and Multiple Tropopauses from a 2002 - 2006 Climatology over the SIRTA Observatory . Abstract : Cirrus balloons are an important element in Earth s radiation budget , but their radiative characteristics remain ill explained because they occur at high elevation where direct measurements are hard to make .In this study we using data taken by the Scripps Institution of Oceanography ( SIO ) lidar scheme located on the SIRTA observatory near Paris during 2002 2003 2004 2005 2006 . The surveys reveal that cirrus balloons can be found between 5 kilometres and 12 kilometres above water level with a mean laser thickness of 0 . 2 ± 0 . 1 .We see that mid - latitude cirrus skies have multiple tropopause features such as double or triple tropopauses which would affect their microphysical structure . These data suggest that cirrus skies work an important role in measuring the vertical flow of water vapor in the air .Citation : Introduction Cirrus particles occupy about 10 % - 20 % of the globe s surface area Sassen et al . , 2008 , however less is known about how these clouds form and evolve . They are particularly challenging to observe since they occur at high altitudes ( 5 - 12km ) , where temperatures are low enough for ice particles to form , but too cold for solid droplets to condense .As a result , most information about cirrus skies comes indirectly through remote sensing techniques using instruments like lidars , radars , satellites , and aircrafts . However , each instrument has its own qualities and flaws depending upon the observation technique applied .For instance , land - based lidars provide accurate profiles of storm altitude and extinction values , while satellite retrieval algorithms suffer from little spatial resolution and uncertainties involved with aerosol contamination .",
        "rewrite_text": "**Title:** Midlatitude Cirrus Clouds and Multiple Tropopauses: A Climatological Study from 2002 to 2006 at the SIRTA Observatory\n\n**Abstract:** Cirrus clouds play a significant role in the Earth's radiation budget, yet their radiative properties are not well understood due to the challenges associated with direct measurements at high altitudes. This study utilizes data collected by the Scripps Institution of Oceanography (SIO) lidar system at the SIRTA Observatory, located near Paris, over a five-year period from 2002 to 2006. The analysis reveals that cirrus clouds are typically found at altitudes ranging from 5 to 12 kilometers above sea level, with an average laser thickness of 0.2 ± 0.1. Notably, the findings indicate the presence of multiple tropopause features—such as double or triple tropopauses—within midlatitude cirrus skies, which may significantly influence their microphysical properties. These observations suggest that cirrus clouds play a crucial role in the vertical transport of water vapor in the atmosphere.\n\nDespite covering approximately 10% to 20% of the Earth's surface area (Sassen et al., 2008), the formation and evolution of cirrus clouds remain poorly understood. Their high-altitude presence (5-12 km) complicates direct observation, as temperatures are low enough for ice particle formation but too cold for liquid droplets to condense. Consequently, much of the existing knowledge about cirrus clouds is derived from indirect remote sensing techniques, employing instruments such as lidars, radars, satellites, and aircraft. Each of these observational methods has its strengths and limitations; for example, ground-based lidars offer precise profiles of cloud altitude and extinction values, while satellite retrieval algorithms often face challenges related to spatial resolution and aerosol contamination. This study aims to enhance the understanding of cirrus cloud characteristics and their implications for atmospheric processes, thereby contributing to the broader knowledge of climate dynamics.",
        "ori-fast-z-score": -0.6211495565912797,
        "water-fast-z-score": 7.601397897755385,
        "rewrite-fast-z-score": 0.9330078226479681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dirichlet or Potts ? .\nAbstract:\nWe consider the problem of learning an unknown distribution over binary strings, where each string is generated by sampling independently at random with replacement according to some fixed probability vector . We show that if we are given samples drawn iid from , then it can be learned in polynomial time using standard statistical techniques (e.g., maximum likelihood). However, when the number of possible values for each bit grows large, this approach fails because there may not exist any sample which contains all possible values for every bit. In such cases, we propose a novel algorithm based on Gibbs sampling and prove its correctness under certain conditions. Finally, we present experimental results demonstrating our method s effectiveness. The main contribution of this work lies in showing how to learn distributions over combinatorial objects like graphs and trees. This is achieved by first reducing these problems into learning distributions over binary strings, and then applying our proposed algorithms. Learning distributions over combinatorial objects has many applications including data mining, computer vision, bioinformatics, etc. .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Dirichlet or Potts ?.Abstract : We consider the question of learning an unknown distribution over binary strings , where each string is generated by filtering independently at random with replacement due to some fixed probability vector . We see that if we are given samples picked iid from , then it can be learned in polynomial period using conventional statistical tools ( e . g . , maximum likelihood ) .However , when the number of possible values for each bit grows large , this methodology fails because there may not exist any specimen which contains all possible values for every bit . In such cases , we undertake a novel algorithm based on Gibbs filtering and find its correctness under certain conditions .Finally , we present research results proving our technique s efficacy . The main impact of this research consists in teaching how to teach distributions over combinatorial objects like graphs and trees .This is achieved by first transforming these problems into knowledge distributions over binary strings , and then applying our proposed methods . Learning distributions over combinatorial objects has numerous uses including data extraction , computer vision , bioinformatics , etc ..",
        "rewrite_text": "**Title: Dirichlet or Potts?**\n\n**Abstract:** In this study, we explore the challenge of learning an unknown distribution over binary strings, where each string is generated through independent random filtering with replacement, guided by a fixed probability vector. Our findings indicate that when provided with independent and identically distributed (iid) samples from the distribution, conventional statistical methods, such as maximum likelihood estimation, can effectively learn the underlying distribution in polynomial time. However, this approach encounters limitations when the number of potential values for each bit increases significantly. In such scenarios, it is possible that no sample exists that encompasses all possible values for every bit, rendering traditional methods ineffective. To address this issue, we propose a novel algorithm rooted in Gibbs filtering, demonstrating its validity under specific conditions. Our research results substantiate the effectiveness of this technique. The primary contribution of this work lies in its ability to facilitate the learning of distributions over combinatorial structures, such as graphs and trees. We achieve this by first reformulating these complex problems into the context of knowledge distributions over binary strings, subsequently applying our proposed methodologies. The ability to learn distributions over combinatorial objects has far-reaching implications across various fields, including data extraction, computer vision, and bioinformatics, among others. This research not only advances theoretical understanding but also provides practical tools for tackling complex distribution learning tasks in diverse applications.",
        "ori-fast-z-score": -0.2,
        "water-fast-z-score": 6.069726860280934,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Probabilistic Anonymity and Admissible Schedulers .\nAbstract:\nWe consider the problem of scheduling jobs on parallel machines with probabilistic anonymity constraints, where each job has to be scheduled in such a way that its execution time is hidden among other jobs. We show how this problem can be solved by using admissible schedulers for the underlying deterministic problem. In particular we present two algorithms based on list-scheduling and randomization respectively. The first algorithm achieves a competitive ratio of 2 while the second one obtains a better performance guarantee of 1 + . Both results are tight up to constant factors. Finally, we provide some experimental evidence showing that our randomized algorithm performs well also in practice. Probabilistic anonymity arises naturally when considering privacy issues related to data mining or computational social choice problems. For example, suppose that there exists a set of n users who have to submit their preferences over m alternatives (e.g., candidates) before a certain deadline T. Each user i submits her preference profile Pi = {(ai1 , bi1 ), . . . , (aim , bim )}, where aij denotes the utility she derives from alternative j and bij indicates whether she likes it or not. Then, if all profiles are submitted simultaneously at time t, they should be processed so as to hide which user prefers what alternative. This means that no information about individual preferences should be revealed until after the deadline T.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Probabilistic Anonymity and Admissible Schedulers . Abstract : We consider the issue of scheduling jobs on parallel machines with probabilistic anonymity limitations , where each work has to be scheduled in such a way that its execution time is hidden among other tasks .We see how this question can be answered by using admissible schedulers for the underlying deterministic problem . In particular we present two strategies using on list - planning and randomization respectively .The first algorithm achieves a competitive ratio of 2 while the second one obtains a better performance assurance of 1 + . Both results are tight up to constant factors .Finally , we provide some experimental evidence showing that our randomized algorithm performs good also in practice . Probabilistic anonymity arises readily when examining privacy issues related to data extraction or computational social selection difficulties .For instance , suppose that there exists a group of n users who have to submit their preferences over m proposals ( e . g . , candidates ) before a certain deadline T . Each customer i submits her preference profile Pi = { ( ai1 , bi1 ) , . .. , ( aim , bim ) } , where aij represents the utility she derives from alternative j and bij represents whether she enjoys it or not .Then , if all profiles are presented simultaneously at period t , they should be processed so as to hide which user prefers what alternative . This implies that no information about individual preferences should be revealed until after the date T .",
        "rewrite_text": "**Title: Probabilistic Anonymity and Admissible Schedulers**\n\n**Abstract:** This article addresses the challenge of scheduling jobs on parallel machines while adhering to probabilistic anonymity constraints. The primary objective is to ensure that the execution time of each job remains obscured among other tasks, thereby protecting individual job identities. We explore how this problem can be effectively tackled using admissible schedulers within the framework of the underlying deterministic scheduling problem. Specifically, we introduce two distinct strategies: one based on list-planning and the other on randomization. The first strategy achieves a competitive ratio of 2, while the randomized approach demonstrates a superior performance guarantee of 1 + ε, where ε is a small constant. Notably, both results are proven to be tight up to constant factors, indicating their optimality in the context of the problem.\n\nFurthermore, we present experimental results that illustrate the practical efficacy of our randomized algorithm, confirming its robust performance in real-world scenarios. The concept of probabilistic anonymity is particularly relevant in discussions surrounding privacy concerns related to data extraction and challenges in computational social choice. For instance, consider a scenario involving a group of n users who must submit their preferences regarding m proposals (such as candidates) before a specified deadline T. Each user i provides a preference profile Pi = {(ai1, bi1), ..., (aim, bim)}, where aij signifies the utility derived from alternative j, and bij indicates the user's enjoyment of that alternative. To maintain anonymity, it is crucial that all preference profiles are processed in such a manner that individual user preferences remain concealed until after the deadline T. This research contributes to the understanding of how scheduling can be optimized while preserving the privacy of users in various applications.",
        "ori-fast-z-score": 1.4501047335684953,
        "water-fast-z-score": 7.246315678266502,
        "rewrite-fast-z-score": 0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A novel spacetime concept for describing electronic motion within a helium atom .\nAbstract:\nWe present an alternative description of the electron in terms of its position and velocity, which is based on the idea that it moves along a helical trajectory around the nucleus. The new approach leads to a simple analytical expression for the energy levels of the helium atom as well as for the wave functions corresponding to these states. We show how this model can be used to explain some experimental results obtained by high-resolution spectroscopy experiments performed at Jefferson Lab. In addition we discuss possible extensions of our work towards other atomic systems such as muonic atoms or ions with one valence electron. Helium has been studied extensively over many decades both experimentally and theoretically. It was found that there are two stable isotopes (3He and 4He) and several excited states. These states have been investigated using various spectroscopic techniques including photo-absorption  1  , laser excitation  2  , and Compton scattering  3  . However, despite all efforts made so far, no satisfactory explanation exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound  4  .\nIn order to understand better the structure of helium, we propose here a new theoretical framework where the electron is described not only by its usual position but also by its velocity vector. This new approach allows us to obtain analytically the energy spectrum of helium as well as the associated wavefunctions. Our formalism is inspired by the so-called Bohmian mechanics  5  , which describes particles moving along trajectories instead of following classical equations of motions  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A novel spacetime idea for describing electronic movement within a helium atom . Abstract : We present an additional description of the electron in terms of its position and speed , which is based on the idea that it travels along a helical velocity around the nucleus .The new approach leads to a simple analytical expression for the power concentrations of the helium atom as well as for the wave functions corresponding to these states . We see how this description can be used to explain some experimental results derived by high - resolution spectroscopy experiments conducted at Jefferson Lab .In addition we discuss possible extensions of our work towards other nuclear systems such as muonic atoms or ions with one valence electron . Helium has been studied frequently over numerous years both experimentally and theoretically .It was shown that there are two stable isotopes ( 3He and 4He ) and many excited states . These states have been investigated using numerous spectroscopic techniques including photo - absorption 1 , laser excitation 2 , and Compton absorption 3 .However , despite all efforts made so far , no satisfactory excuse exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound 4 . In order to explain better the composition of helium , we propose here a new theoretical framework where the electron is characterized not only by its customary orientation but also by its velocity function .This new approach allows us to obtain analytically the power spectrum of helium as well as the associated wavefunctions . Our formalism is influenced by the so - called Bohmian physics 5 , which expresses atoms moved along trajectories rather of following classical equations of motions 6 .",
        "rewrite_text": "**Title:** A Novel Spacetime Framework for Describing Electron Dynamics in Helium Atoms\n\n**Abstract:** In this study, we introduce an innovative framework for characterizing the behavior of electrons within helium atoms, focusing on their position and velocity. Our model posits that electrons traverse a helical path around the nucleus, which leads to a straightforward analytical formulation for the power distributions of the helium atom, as well as the corresponding wave functions for these states. This new perspective provides insights into experimental findings from high-resolution spectroscopy conducted at Jefferson Lab, enhancing our understanding of electron dynamics in helium. \n\nHelium, a subject of extensive research over the years, possesses two stable isotopes, 3He and 4He, along with a variety of excited states. These states have been explored through various spectroscopic methods, including photo-absorption, laser excitation, and Compton absorption. Despite the wealth of experimental data, a comprehensive explanation for the unbound ground state of 3He, in contrast to the bound ground state of 4He, remains elusive. \n\nTo address this gap, we propose a theoretical framework that redefines the electron's characteristics by incorporating its velocity function alongside its spatial orientation. This approach not only facilitates the analytical derivation of helium's power spectrum but also yields the associated wave functions. Our methodology draws inspiration from Bohmian mechanics, which conceptualizes atomic motion as trajectories rather than adhering strictly to classical equations of motion. \n\nFurthermore, we explore potential applications of our framework to other nuclear systems, such as muonic atoms and ions with a single valence electron. This work aims to deepen the understanding of helium's structure and dynamics, paving the way for future investigations into similar atomic systems.",
        "ori-fast-z-score": -0.6211495565912797,
        "water-fast-z-score": 6.363961030678928,
        "rewrite-fast-z-score": -0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nature of Infrared Sources in 11 micron Selected Sample from Early Data of the AKARI North Ecliptic Pole Deep Survey .\nAbstract:\nWe present results on infrared sources selected by their flux densities at 11 microns (S11) using early data taken with the InfraRed Camera (IRC) on-board AKARI, which is an infrared space telescope launched into orbit in February 2006. The survey covers about 1 deg2 area centered around the north ecliptic pole and reaches to S/N = 5 limit for point source detection. We have identified more than 1000 infrared sources down to S11 ~ 0.1 Jy over the entire field-of-view. Among them we found that most are associated with galaxies or galaxy clusters. About 20% of these objects show red colors indicative of dust-obscured star formation activity. A large fraction of the remaining 80% shows blue colors indicating active galactic nuclei and/or young stellar populations. These results suggest that our sample contains various types of infrared luminous objects including normal galaxies, interacting/merging systems, obscured AGNs as well as distant quasars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nature of Infrared Sources in 11 micron Selected Sample from Early Data of the AKARI North Ecliptic Pole Deep Survey . Abstract : We report findings on infrared sources chosen by their flux densities at 11 microns ( S11 ) using early data taken with the InfraRed Camera ( IRC ) on - board AKARI , which is an infrared space telescope launched into orbit in February 2006 .The survey encompasses about 1 deg2 region centered around the north ecliptic pole and reaches to S / N = 5 limit for point source discovery . We have discovered more than 1000 infrared sources down to S11 ~ 0 . 1 Jy over the entire field - of - view .Among them we reported that most are identified with galaxies or galaxy regions . About 20 % of these objects show red colors indicative of dust - obscured star formation activity .A large fraction of the remaining 80 % indicates blue colors representing active galactic nuclei and / or young stellar regions . These data suggest that our sample comprises numerous types of infrared luminous objects including typical clusters , interacting / merging systems , obscured AGNs as well as distant quasars .",
        "rewrite_text": "We present our findings on infrared sources selected based on their flux densities at 11 microns (S11) using preliminary data from the InfraRed Camera (IRC) aboard the AKARI space telescope, which was launched in February 2006. This study focuses on a region of approximately 1 square degree centered on the north ecliptic pole, achieving a signal-to-noise ratio (S/N) limit of 5 for point source detection. Through our survey, we have identified over 1,000 infrared sources with flux densities down to S11 ~ 0.1 Jy across the entire field of view. Notably, the majority of these sources are associated with galaxies or galaxy regions. Approximately 20% of the identified objects exhibit red colors, which are indicative of dust-obscured star formation activity. In contrast, the remaining 80% display blue colors, suggesting the presence of active galactic nuclei (AGNs) and/or young stellar regions. These observations imply that our sample encompasses a diverse array of infrared luminous objects, including typical galaxy clusters, interacting or merging systems, obscured AGNs, and distant quasars. The findings contribute valuable insights into the nature of infrared sources in the early universe and highlight the significance of AKARI's observations in advancing our understanding of cosmic evolution and the formation of galaxies. This research not only enhances our knowledge of the infrared universe but also sets the stage for future studies aimed at exploring the intricate relationships between star formation, galaxy evolution, and the role of AGNs in shaping the cosmic landscape.",
        "ori-fast-z-score": 0.10846522890932808,
        "water-fast-z-score": 4.0976453817306595,
        "rewrite-fast-z-score": 1.2675004445952593
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Entanglement and topological entropy of the toric code at finite temperature .\nAbstract:\nWe study entanglement properties of the Toric Code model in two dimensions, which is defined on a square lattice with periodic boundary conditions. We consider both ground state and thermal states for this system. In particular we calculate the von Neumann entropy S(A) = −TrρA ln ρA associated to different regions A of the lattice as well as the mutual information I(A; B) between any pair of disjoint regions A and B. The results are compared against numerical simulations performed by means of Monte Carlo techniques. For the ground state it turns out that there exists an area law for the von Neumann entropy, i.e., S(A) ∝ L−d−1 where d denotes the dimension of region A and L its linear size. Moreover, we find that the mutual information decays exponentially fast when one moves away from the diagonal line joining the centers of the regions A and B. These findings agree very well with those obtained using exact methods based on Matrix Product States (MPS). Finally, we also show how these results can be used to obtain bounds on the topological entropy of the Toric Code.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Entanglement and topological entropy of the toric code at finite temperature . Abstract : We research entanglement properties of the Toric Code model in two dimensions , which is characterized on a square lattice with periodic border conditions .We consider both ground state and thermal states for this system . In particular we determine the von Neumann entropy S ( A ) = −TrρA ln ρA associated to different regions A of the lattice as well as the mutual information I ( A ; B ) between any pair of disjoint regions A and B .The results are compared against numerical simulations conducted by means of Monte Carlo methods . For the ground state it turns out that there exists an area law for the von Neumann entropy , i . e . , S ( A ) [UNK] L−d−1 where d indicates the dimension of zone A and L its linear size .Moreover , we find that the mutual intelligence decays exponentially rapid when one moves away from the diagonal line joining the centers of the regions A and B . These conclusions follow very well with those acquired using accurate methods using on Matrix Product States ( MPS ) .Finally , we also demonstrate how these results can be used to obtain limits on the topological entropy of the Toric Code .",
        "rewrite_text": "In this study, we investigate the entanglement characteristics of the Toric Code model in two dimensions, which is defined on a square lattice with periodic boundary conditions. Our analysis encompasses both the ground state and thermal states of the system. We focus on calculating the von Neumann entropy \\( S(A) = -\\text{Tr}(\\rho_A \\ln \\rho_A) \\) for various regions \\( A \\) of the lattice, as well as the mutual information \\( I(A; B) \\) between pairs of disjoint regions \\( A \\) and \\( B \\). The findings from our theoretical calculations are validated against numerical simulations performed using Monte Carlo methods. \n\nFor the ground state, we observe that the von Neumann entropy adheres to an area law, expressed as \\( S(A) \\sim L^{-(d+1)} \\), where \\( d \\) represents the dimension of region \\( A \\) and \\( L \\) denotes its linear size. Additionally, we discover that the mutual information exhibits an exponential decay as the distance from the diagonal line connecting the centers of regions \\( A \\) and \\( B \\) increases. These results align closely with those obtained through precise techniques involving Matrix Product States (MPS). \n\nFurthermore, we illustrate how our findings can be leveraged to establish bounds on the topological entropy of the Toric Code. This research contributes to a deeper understanding of the entanglement structure in topologically ordered systems, particularly under thermal conditions, and highlights the interplay between entanglement measures and topological properties in quantum many-body systems.",
        "ori-fast-z-score": -0.10976425998969035,
        "water-fast-z-score": 4.447074385282452,
        "rewrite-fast-z-score": 0.10482848367219183
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Connection between Star-Forming Galaxies, AGN Host Galaxies and Early-Type Galaxies in the SDSS .\nAbstract:\nWe present an analysis of the connection between different galaxy types using data from the Sloan Digital Sky Survey (SDSS). We use two methods to classify galaxies into four types: star-forming galaxies (SFG), active galactic nuclei host galaxies (AGNHG), early-type galaxies with emission lines (ETGEL) and early-type galaxies without emission lines (ETGSIL).\nThe first method is based on the principal component analysis (PCA) applied to the optical spectra of all galaxies classified as spectroscopic targets by the SDSS pipeline. The second one uses the PCA applied only to the subset of galaxies that are morphologically selected for having bulges dominated by old stellar populations. In both cases we find that ETGs form a continuous sequence in terms of their spectral properties along which SFGs evolve towards ETGSILs through ETGELs. This evolutionary path can be described by a simple linear combination of three eigenvectors corresponding to the most prominent features seen in the mean spectrum of each type of galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Connection between Star - Forming Galaxies , AGN Host Galaxies and Early - Type Galaxies in the SDSS . Abstract : We present an assessment of the link between various galaxy types using data from the Sloan Digital Sky Survey ( SDSS ) .We use two means to classify galaxies into four types : star - creating galaxies ( SFG ) , active galactic nuclei guest galaxies ( AGNHG ) , early - class galaxies with emitted lines ( ETGEL ) and early - class galaxies without absorption lines ( ETGSIL ) . The first method is based on the main component analysis ( PCA ) applied to the optical spectra of all galaxies designated as spectroscopic targets by the SDSS pipeline .The second one uses the PCA applicable only to the subset of stars that are morphologically selected for having bulges dominated by ancient stars populations . In both cases we find that ETGs form a continuous sequence in terms of their spectral properties along which SFGs grow towards ETGSILs through ETGELs .This evolutionary progression can be described by a simple linear mixture of three eigenvectors corresponding to the most notable features found in the mean spectrum of each type of galaxies .",
        "rewrite_text": "We provide a comprehensive analysis of the relationships among different types of galaxies, utilizing data from the Sloan Digital Sky Survey (SDSS). Our study categorizes galaxies into four distinct types: star-forming galaxies (SFGs), active galactic nuclei host galaxies (AGN HGs), early-type galaxies with emission lines (ETGELs), and early-type galaxies without absorption lines (ETGSILs). To achieve this classification, we employed two methodologies. The first method involves principal component analysis (PCA) applied to the optical spectra of all galaxies identified as spectroscopic targets by the SDSS pipeline. The second method focuses on a subset of galaxies that are morphologically selected for having bulges predominantly composed of ancient stellar populations, again utilizing PCA. Our findings reveal that early-type galaxies (ETGs) exhibit a continuous sequence in their spectral characteristics, indicating an evolutionary pathway where star-forming galaxies transition into ETGSILs via ETGELs. This progression can be effectively modeled as a linear combination of three eigenvectors, which correspond to the most significant spectral features observed in the average spectrum of each galaxy type. This research enhances our understanding of galaxy evolution and the intricate connections between different galaxy classifications, shedding light on the processes that govern their formation and development within the cosmic landscape.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 3.927922024247863,
        "rewrite-fast-z-score": -0.29559878344928797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  COSMOSOMAS Observations of the CMB and Galactic Foregrounds at 11 GHz: Evidence for anomalous microwave emission at high Galactic Latitude .\nAbstract:\nWe present new observations made with the Cosmosoma experiment, which were designed to search for evidence of an excess in cosmic microwave background (CMB) temperature fluctuations above those predicted by standard cosmological models. The data are consistent with predictions based on current theoretical understanding but show some unexpected features that may be related to previously unidentified foreground sources or systematic effects associated with our analysis techniques. \n \n We have used these results to place limits on possible contributions from primordial gravitational waves and other exotic phenomena such as topological defects. These limits are comparable to previous measurements obtained using different experimental approaches. In addition we report the detection of a significant signal at frequencies below 10GHz, which is not expected within conventional cosmological models. This could represent either a new source of foreground contamination or a novel physical effect. Further investigation will require additional experiments to confirm this result and determine its origin. If confirmed it would provide important constraints on theories attempting to explain the observed anisotropy in the CMB spectrum.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : COSMOSOMAS Observations of the CMB and Galactic Foregrounds at 11 GHz : Evidence for anomalous microwave emission at high Galactic Latitude . Abstract : We address new experiments done with the Cosmosoma study , which were built to search for indication of an accumulation in cosmic microwave background ( CMB ) temperature fluctuations above those predicted by typical cosmological models .The data are compatible with predictions based on current theoretical knowledge but indicate some surprising characteristics that might be connected to formerly unidentified foreground sources or systematic effects involved with our analysis methods . We have utilized these results to place limits on potential contributions from primordial magnetic waves and other exotic processes such as topological errors .These limits are comparable to previous measurements obtained using separate experimental methods . In addition we report the observation of a substantial frequency at speeds below 10GHz , which is not anticipated within conventional cosmological models .This might represent either a new cause of foreground contamination or a novel physical impact . Further investigation will demand additional studies to confirm this effect and establish its identity .If confirmed it would offer important restrictions on theories attempting to explain the observed anisotropy in the CMB spectrum .",
        "rewrite_text": "Title: COSMOSOMAS Observations of the CMB and Galactic Foregrounds at 11 GHz: Evidence for Anomalous Microwave Emission at High Galactic Latitude\n\nAbstract: In this study, we present findings from the COSMOSOMAS project, which aimed to investigate potential anomalies in cosmic microwave background (CMB) temperature fluctuations that exceed predictions made by standard cosmological models. Our experimental data align with existing theoretical frameworks; however, they also reveal unexpected features that may be linked to previously unidentified foreground sources or systematic biases in our analytical techniques. We have leveraged these observations to impose constraints on possible contributions from primordial magnetic waves and other exotic phenomena, such as topological defects. The limits we established are consistent with earlier measurements obtained through different experimental approaches. Notably, we report the detection of significant emissions at frequencies below 10 GHz, which are not accounted for in conventional cosmological theories. This finding could indicate either a new source of foreground contamination or a novel physical mechanism at play. To validate this observation and ascertain its nature, further investigations are necessary. If substantiated, these results could provide critical insights into the mechanisms underlying the observed anisotropies in the CMB spectrum, potentially reshaping our understanding of cosmic evolution and the fundamental processes that govern the universe.",
        "ori-fast-z-score": -0.8955334711889903,
        "water-fast-z-score": 6.733003292241386,
        "rewrite-fast-z-score": -1.0425720702853738
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The 1 keV to 200 keV X-ray Spectrum of NGC 2992 and NGC 3081 .\nAbstract:\nWe present the first results on the soft X-ray emission in two nearby elliptical galaxies, NGC 2992 (M60) and NGC 3081 (M84). The observations were made with the Chandra X-Ray Observatory using the Advanced CCD Imaging Spectrometer (ACIS-S3), which has an energy resolution of about 130 eV at 6 keV. We find that both galaxies show extended diffuse emission around their central regions. In addition, we detect several point sources within each galaxy s field-of-view. For these point sources, we have extracted spectra for individual source components as well as combined them into one spectrum per galaxy. Using spectral fitting techniques, we found that all but three of the detected point sources are consistent with being background AGNs or foreground stars. However, there is evidence that some of the brightest point sources may be associated with the host galaxies themselves. Finally, we also fit the diffuse component of the X-ray emission with thermal plasma models.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The 1 keV to 200 keV X - ray Spectrum of NGC 2992 and NGC 3081 . Abstract : We report the first findings on the soft X - ray radiation in two nearby elliptical galaxies , NGC 2992 ( M60 ) and NGC 3081 ( M84 ) .The images were made with the Chandra X - Ray Observatory using the Advanced CCD Imaging Spectrometer ( ACIS - S3 ) , which has an energy resolution of about 130 eV at 6 keV . We see that both galaxies show enhanced diffuse emission around their central regions .In addition , we locate many point sources within each galaxy s field - of - view . For these point sources , we have gathered spectra for individual source components as well as blended them into one spectrum per galaxy .Using spectral fit techniques , we concluded that all but three of the detected point sources are compatible with being background AGNs or foreground stars . However , there is evidence that some of the brightest point sources might be identified with the host galaxies themselves .Finally , we also fitted the diffuse component of the X - ray radiation with thermal plasma theories .",
        "rewrite_text": "In this study, we present the inaugural analysis of soft X-ray emissions from two nearby elliptical galaxies, NGC 2992 (M60) and NGC 3081 (M84). Utilizing the Advanced CCD Imaging Spectrometer (ACIS-S3) aboard the Chandra X-Ray Observatory, we obtained high-resolution images that reveal significant diffuse X-ray emissions concentrated around the central regions of both galaxies. Our observations indicate a rich field of point sources within the observed areas, prompting us to extract spectra for these individual sources as well as to combine them into a single spectrum for each galaxy. Through spectral fitting techniques, we determined that the majority of the detected point sources are consistent with being background active galactic nuclei (AGNs) or foreground stars. Notably, we found that some of the brightest point sources may be associated with the host galaxies themselves, suggesting potential intrinsic sources of X-ray emission. Furthermore, we applied thermal plasma models to analyze the diffuse X-ray component, providing insights into the physical conditions and processes occurring in the hot gas surrounding the galaxies. Our findings contribute to the understanding of the X-ray properties of elliptical galaxies and highlight the complex interplay between stellar populations and the interstellar medium in these systems. This research not only enhances our knowledge of NGC 2992 and NGC 3081 but also sets the stage for future investigations into the X-ray characteristics of similar galaxies.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 3.4874291623145783,
        "rewrite-fast-z-score": -0.4879500364742666
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Minimum Expected Distortion in Gaussian Layered Broadcast Coding with Successive Refinement .\nAbstract:\nWe consider the problem of successive refinement coding for layered broadcast systems, where each receiver is interested only in one out of several layers and has access to all previous layers. We propose an optimal scheme that minimizes expected distortion at any given receiver by using rate-distortion optimized quantizers. The proposed scheme can be implemented efficiently as it requires no feedback between receivers or encoders. Our results are illustrated through numerical examples. Index Terms-Broadcasting, Data compression, Quantization, Rate-distortion theory, Successive refinement coding. I. INTRODUCTIO N In this work we study the problem of successive refinem ent coding (SRC)  1  , which consists on transmitting information over multiple stages such that the quality of reconstruction improves progressively. SRC is used extensively in video broadcasting applications  2  -  4  . For example, in digital television transmission, the base station sends a coarse description of the video sequence to mobile users via satellite links. Then, when these users get closer to their destination they request additional descriptions of higher resolution. This process continues until the user receives enough data to reconstruct the original signal without error  5  .\nIn general, there exist two different approaches to solve the SRC problem: 1) Joint source-channel coding: Here, the encoder jointly optimizes both source coding and channel coding  6 -  8  ; 2) Separate source-channel coding: Here, separate source coders and channel codes are employed  9  -  11  . In this case, the source code must provide some form of side-information so that the decoder can perform successive decoding  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Minimum Expected Distortion in Gaussian Layered Broadcast Coding with Successive Refinement . Abstract : We consider the issue of successive refinement compression for structured transmission systems , where each receiver is interested only in one out of several layers and has access to all previous layers .We suggest an appropriate plan that minimizes expected distortion at any certain receiver by using rate - distortion optimized quantizers . The proposed system can be applied efficiently as it requires no feedback between receivers or encoders .Our results are shown through numerical examples . Index Terms - Broadcasting , Data coding , Quantization , Rate - noise theory , Successive refinement compression .I . INTRODUCTIO N In this research we study the question of successive refinem ent coding ( SRC ) 1 , which consists on transmitting information over successive phases such that the quality of recovery improves progressively . SRC is utilized heavily in video broadcasting applications 2 - 4 .For instance , in digital television broadcast , the base station provides a coarse summary of the footage scene to mobile users via satellite connections . Then , when these users feel nearer to their target they seek alternative descriptions of greater resolution .This process proceeds until the user receives enough data to reconstruct the original signal without mistake 5 . In general , there follow two different methods to solve the SRC problem : 1 ) Joint source - channel code : Here , the encoder jointly optimizes both source coding and channel code 6 - 8 ; 2 ) Separate source - channel code : Here , distinct source coders and channel codes are applied 9 - 11 .In this instance , the source language must offer some kind of side - information so that the decoder can conduct consecutive decoding 12 .",
        "rewrite_text": "**Title:** Minimum Expected Distortion in Gaussian Layered Broadcast Coding with Successive Refinement\n\n**Abstract:** This article addresses the challenge of successive refinement compression within structured transmission systems, where each receiver is focused on a specific layer while having access to all preceding layers. We propose a systematic approach aimed at minimizing the expected distortion for any given receiver by employing rate-distortion optimized quantizers. The efficiency of our proposed system is noteworthy, as it operates without the need for feedback between receivers or encoders, simplifying the communication process. Our findings are illustrated through numerical examples that demonstrate the effectiveness of the approach. \n\nSuccessive refinement coding (SRC) is a critical area of study, particularly relevant in applications such as video broadcasting. In these scenarios, a base station transmits a basic overview of video content to mobile users via satellite. As users approach their desired content, they request higher-resolution alternatives, continuing this process until they have sufficient data to accurately reconstruct the original signal. \n\nTo tackle the SRC problem, two primary methodologies are typically employed: the joint source-channel coding approach, which optimizes both source and channel coding simultaneously, and the separate source-channel coding approach, which utilizes distinct coders for each. In the latter case, the source must provide some form of side information to facilitate the decoder's ability to perform successive decoding. This research contributes to the understanding of SRC by exploring the implications of layered broadcast coding in Gaussian channels, ultimately enhancing the quality of data transmission in various applications. \n\n**Index Terms:** Broadcasting, Data coding, Quantization, Rate-noise theory, Successive refinement compression.",
        "ori-fast-z-score": -2.341196917715124,
        "water-fast-z-score": 5.744562646538029,
        "rewrite-fast-z-score": 0.43685202833051895
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Massive enhancement of electron-phonon coupling in doped graphene by an electronic singularity .\nAbstract:\nWe report on the observation of strong electron-phonon interaction (EPI) in heavily boron-doped epitaxial graphene grown on SiC(0001). The EPI is found to be enhanced by more than one order of magnitude compared with that observed for undoped graphene, which can be attributed to the formation of a Dirac point at high doping levels and its associated van Hove singularity near the Fermi level. This finding suggests that the EPI in doped graphene may be tunable over several orders of magnitude through chemical or electrostatic gating. \n \n Graphene has attracted considerable attention because it exhibits unique physical properties such as extremely high carrier mobility1-5 and very large optical nonlinearities6-8. However, these remarkable properties are often accompanied by weak interactions between electrons and phonons9-11, which limit their applications in high-speed electronics12-14 and optoelectronics15-17. In this work we demonstrate that the EPI in heavily boron-doping epitaxial graphene grown by thermal decomposition of SiC18-20 can be significantly enhanced due to the presence of a Dirac point21-23 and its associated van Hov singularity24-26 near the Fermi energy EF. We show that the EPI increases rapidly when the Fermi level crosses the van Hove singularity, resulting in a giant increase in the electron-phonon scattering rate. Our results suggest that the EPI in graphene could be controlled electrically via chemical or electrostatic gated27-30, thereby opening up new avenues towards novel devices based on graphene. \nGraphene is known to have extremely high carrier mobilities1-4 but relatively small electron-phonon couplings5-9. These two competing effects determine the performance of graphene-based electronic and optoelectronic devices10-12. For example, the low EPI leads to slow relaxation rates13-15 and thus limits the operation speed of graphene transistors14-16. On the other hand, the high mobility makes graphene attractive for use in high-speed electronics17-19 and ultrafast photodetectors20. Therefore, there exists great interest in developing methods to enhance the EPI while maintaining the high mobility31",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Massive enhancement of electron - phonon coupling in doped graphene by an electronic singularity . Abstract : We report on the observation of strong ion - phonon interaction ( EPI ) in heavily boron - doped epitaxial graphene grown on SiC ( 0001 ) .The EPI is found to be enhanced by more than one order of magnitude compared with that detected for undoped graphene , which can be due to the formation of a Dirac zone at high doping rates and its associated van Hove singularity near the Fermi level . This found shows that the EPI in doped graphene may be tunable over numerous orders of magnitude through chemical or electrostatic gating .Graphene has drew substantial scrutiny because it displays unusual physical properties such as extremely high carrier mobility1 - 5 and very huge optical nonlinearities6 - 8 . However , these unique properties are often accompanied by weak interactions between electrons and phonons9 - 11 , which limit their applications in high - speed electronics12 - 14 and optoelectronics15 - 17 .In this research we prove that the EPI in heavily boron - doping epitaxial graphene grown by mechanical transformation of SiC18 - 20 can be greatly enhanced owing to the presence of a Dirac point21 - 23 and its associated van Hov singularity24 - 26 near the Fermi energy EF . We suggest that the EPI increases quickly when the Fermi level crosses the van Hove singularity , leading in a giant increase in the electron - phonon absorption rate .Our results show that the EPI in graphene might be governed electrically via chemical or electrostatic gated27 - 30 , thereby introducing up new avenues towards new materials based on graphene . Graphene is known to have extremely high carrier mobilities1 - 4 but fairly little electron - phonon couplings5 - 9 .These two different factors affect the performance of graphene - based electronic and optoelectronic devices10 - 12 . For instance , the poor EPI contributes to slow relaxation rates13 - 15 and therefore decreases the operation rate of graphene transistors14 - 16 .On the other hand , the high mobility makes graphene suitable for use in high - speed electronics17 - 19 and ultrafast photodetectors20 . Therefore , there exists much interest in developing means to enhance the EPI while maintaining the high mobility31",
        "rewrite_text": "**Title:** Massive Enhancement of Electron-Phonon Coupling in Doped Graphene Induced by an Electronic Singularity\n\n**Abstract:** In this study, we present compelling evidence of a significant enhancement in electron-phonon interaction (EPI) within heavily boron-doped epitaxial graphene synthesized on SiC (0001). Our findings reveal that the EPI in this doped graphene exceeds that of undoped graphene by over an order of magnitude. This remarkable increase is attributed to the emergence of a Dirac zone at elevated doping levels, which gives rise to a van Hove singularity situated near the Fermi level. This discovery indicates that the EPI in doped graphene can be finely tuned across multiple orders of magnitude through methods such as chemical or electrostatic gating. \n\nGraphene has garnered considerable attention due to its extraordinary physical characteristics, including exceptionally high carrier mobility and substantial optical nonlinearities. However, these advantageous properties are often counterbalanced by weak electron-phonon interactions, which can hinder the material's applicability in high-speed electronic and optoelectronic devices. Our research demonstrates that the EPI in heavily boron-doped epitaxial graphene, produced via mechanical transformation of SiC, can be significantly amplified due to the presence of a Dirac point and its corresponding van Hove singularity near the Fermi energy (EF). \n\nWe propose that the EPI experiences a rapid increase as the Fermi level crosses the van Hove singularity, resulting in a dramatic rise in the electron-phonon absorption rate. Our results suggest that the EPI in graphene can be electrically manipulated through chemical or electrostatic gating, paving the way for the development of novel graphene-based materials. While graphene is recognized for its high carrier mobility, it has traditionally exhibited limited electron-phonon coupling, which adversely affects the performance of graphene-based electronic and optoelectronic devices. The weak EPI contributes to slow relaxation rates, thereby reducing the operational efficiency of graphene transistors. Conversely, the high mobility of graphene positions it as an ideal candidate for high-speed electronics and ultrafast photodetectors. Consequently, there is a growing interest in strategies to enhance EPI while preserving the material's high mobility.",
        "ori-fast-z-score": -0.5980503604017327,
        "water-fast-z-score": 7.661308776828737,
        "rewrite-fast-z-score": 0.34299717028501764
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  South-West extension of the hard X-ray emission from the Coma cluster .\nAbstract:\nWe report on new observations with Suzaku and XMM-Newton that reveal an extended component in the hard X-ray spectrum (10-40 keV) of the Coma galaxy cluster, which is located at a distance of about 100 Mpc. The observed flux density distribution shows a clear excess over the expected contribution from point sources within the field-of-view. We find that this excess can be well described by a power-law model with photon index Γ = 1.7 ± 0.1 and normalization F0 = (2.6 ± 0.3) × 10−12 erg cm−2 s−1 sr−1 at 10 keV. This result suggests that there are additional components beyond those associated with individual galaxies or AGNs contributing to the hard X-ray emission from clusters of galaxies. In particular, we discuss possible origins for such diffuse emission as thermal bremsstrahlung radiation from hot gas and/or inverse Compton scattering off cosmic microwave background photons.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : South - West extension of the hard X - ray radiation from the Coma cluster . Abstract : We report on new experiments with Suzaku and XMM - Newton that indicate an extended component in the hard X - ray spectrum ( 10 - 40 keV ) of the Coma galaxy cluster , which is situated at a distance of about 100 Mpc .The observed flux density density displays a clear excess over the expected contribution from point sources within the field - of - view . We see that this excess can be well described by a power - law description with photon index Γ = 1 . 7 ± 0 . 1 and normalization F0 = ( 2 . 6 ± 0 . 3 ) × 10−12 erg cm−2 s−1 sr−1 at 10 keV .This result suggests that there are additional components beyond those associated with individual galaxies or AGNs causing to the hard X - ray radiation from clusters of clusters . In particular , we investigate possible origins for such diffuse emission as heat bremsstrahlung rays from hot gas and / or inverse Compton absorption off cosmic microwave background photons .",
        "rewrite_text": "We present findings from recent observations conducted with the Suzaku and XMM-Newton satellites, which reveal an extended component in the hard X-ray spectrum (10-40 keV) of the Coma galaxy cluster, located approximately 100 Mpc away. Our analysis indicates a significant excess in the observed flux density that surpasses the anticipated contributions from point sources within the observed field. This excess is effectively characterized by a power-law model, yielding a photon index of Γ = 1.7 ± 0.1 and a normalization factor of F0 = (2.6 ± 0.3) × 10−12 erg cm−2 s−1 sr−1 at 10 keV. These findings imply the presence of additional emission sources beyond those linked to individual galaxies or active galactic nuclei (AGNs), which contribute to the hard X-ray radiation emanating from galaxy clusters. We explore potential origins for this diffuse emission, considering mechanisms such as thermal bremsstrahlung radiation from hot gas and inverse Compton scattering of cosmic microwave background photons. Our results enhance the understanding of the complex X-ray emission processes in galaxy clusters and suggest that the hard X-ray spectrum may be influenced by a combination of both thermal and non-thermal processes. This study opens avenues for further investigation into the nature of the diffuse emission in galaxy clusters and its implications for the broader understanding of cosmic structures.",
        "ori-fast-z-score": 0.7977240352174656,
        "water-fast-z-score": 4.444462481925879,
        "rewrite-fast-z-score": -0.9712858623572641
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HST Photometry for the Halo Stars in the Leo Elliptical NGC 3377 .\nAbstract:\nWe present new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377, obtained with the Wide Field Planetary Camera 2 (WFPC2). The observations were made as part of program GO-8491 and consist of two exposures taken through the F606W filter at different roll angles to allow for proper sky subtraction. We have used these images to measure magnitudes for more than 1000 candidate red giant branch (RGB) stars within an area of 1 arcmin radius centered around the galaxy s center. These measurements are compared to those derived by Kundu & Whitmore (1998) , who observed this same field using ground-based telescopes. Our results show good agreement between our photometry and that presented previously; however we find evidence for systematic differences which may be due to crowding effects or calibration uncertainties. \n \n Keywords: Red Giant Branch, Galaxy, WFPC2",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HST Photometry for the Halo Stars in the Leo Elliptical NGC 3377 . Abstract : We report new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377 , obtained with the Wide Field Planetary Camera 2 ( WFPC2 ) .The surveys were made as part of program GO - 8491 and consist of two exposures took through the F606W filter at different roll directions to allow for appropriate sky subtraction . We have utilized these images to measure magnitudes for more than 1000 candidate red dwarf branch ( RGB ) stars within an area of 1 arcmin radius centered around the universe s center .These measurements are compared to those derived by Kundu & Whitmore ( 1998 ) , who studied this same field using ground - based telescopes . Our results show good agreement between our photometry and that presented previously ; however we find proof for widespread changes which may be due to crowding effects or calibration uncertainties .Keywords: Red Giant Branch, Galaxy, WFPC2",
        "rewrite_text": "We present new photometric data obtained from the Hubble Space Telescope (HST) focusing on halo stars in the nearby elliptical galaxy NGC 3377. This research was conducted using the Wide Field Planetary Camera 2 (WFPC2) as part of program GO-8491. The observational campaign involved two exposures taken through the F606W filter, with different roll angles employed to facilitate effective sky subtraction. Our analysis encompasses over 1,000 candidate red giant branch (RGB) stars located within a 1 arcminute radius centered on the galaxy's core. \n\nThe photometric measurements we derived were compared with those from Kundu & Whitmore (1998), who previously examined this region using ground-based telescopes. Our findings indicate a strong correlation between the two datasets; however, we also identified significant discrepancies that may be attributed to crowding effects or potential calibration issues. These results contribute to the understanding of the stellar population in NGC 3377 and highlight the advantages of HST observations over ground-based methods in resolving complex stellar environments. \n\nThis study not only reinforces the validity of previous measurements but also emphasizes the need for careful consideration of observational limitations when interpreting photometric data in crowded fields. Our work enhances the existing knowledge of the red giant branch in this elliptical galaxy and provides a foundation for future investigations into the stellar dynamics and evolution of NGC 3377. \n\nKeywords: Red Giant Branch, Galaxy, WFPC2.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 4.409585518440984,
        "rewrite-fast-z-score": -0.10050378152592121
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Cosmological Framework for the Co-Evolution of Quasars, Supermassive Black Holes, and Elliptical Galaxies: II. Formation of Red Ellipticals .\nAbstract:\nWe present an analytical model to study the formation of red elliptical galaxies in hierarchical cosmologies. We assume that supermassive black holes (SMBHs) grow by accretion at high redshift z > 6 when gas is abundant but cooling times are short. The SMBH growth rate depends on its mass M•(z), which we calculate using semi-analytical models based on merger trees extracted from N-body simulations. In our scenario, quasars shine brightly during their active phase with luminosities LQSO ~ 1045 erg s-1. They heat up surrounding cold gas clouds via photoionization heating and Compton scattering, preventing them from forming stars efficiently until they fade away after about 1 Gyr. This leads to a bimodal color distribution of early-type galaxies as observed today. Our results show good agreement with observations such as the quasar luminosity function, galaxy clustering statistics, and the fraction of red ellipticals among all massive galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  A Cosmological Framework for the Co-Evolution of Quasars, Supermassive Black Holes, and Elliptical Galaxies: II.Formation of Red Ellipticals .Abstract : We present an analytical theory to study the formation of red elliptical galaxies in hierarchical cosmologies . We assume that supermassive black holes ( SMBHs ) grow by accretion at high redshift z > 6 when gas is abundant but warming periods are small .The SMBH growth speed relies on its weight M • ( z ) , which we determine using semi - analytical models using on merger trees obtained from N - bodies simulations . In our scenario , quasars shine brightly during their active mode with luminosities LQSO ~ 1045 erg s - 1 .They heat up surrounding cold gas clouds via photoionization heating and Compton scattering , allowing them from creating stars efficiently until they fade away after about 1 Gyr . This leads to a bimodal color distribution of early - class stars as observed nowadays .Our results show good agreement with observations such as the quasar luminosity function , galaxy clustering statistics , and the fraction of red ellipticals among all huge objects .",
        "rewrite_text": "In this study, we introduce an analytical framework aimed at exploring the formation of red elliptical galaxies within the context of hierarchical cosmological models. Our investigation focuses on the growth of supermassive black holes (SMBHs) during the early universe, specifically at high redshifts (z > 6), when gas is plentiful but periods of heating are relatively brief. We propose that the growth rate of SMBHs is contingent upon their mass, M • (z), which we derive through semi-analytical models informed by merger trees generated from N-body simulations. \n\nIn our model, quasars exhibit significant luminosity during their active phases, reaching levels around LQSO ~ 10^45 erg s^-1. These luminous quasars play a crucial role in heating the surrounding cold gas clouds through mechanisms such as photoionization heating and Compton scattering. This heating process facilitates efficient star formation until the quasars eventually diminish in brightness after approximately 1 billion years. As a result, we observe a bimodal color distribution among early-type stars, which aligns with current observational data.\n\nOur findings demonstrate a strong correlation with various astronomical observations, including the quasar luminosity function, galaxy clustering statistics, and the prevalence of red elliptical galaxies among large-scale structures. This work contributes to a deeper understanding of the interconnected evolution of quasars, supermassive black holes, and elliptical galaxies, providing insights into the mechanisms that govern their formation and the resulting implications for the broader cosmological landscape.",
        "ori-fast-z-score": 1.3587324409735149,
        "water-fast-z-score": 5.062895554167108,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electromagnetic wave propagation in an almost circular bundle of closely packed, metallic, carbon nanotubes .\nAbstract:\nWe study the electromagnetic wave propagation through a bundle of closely-packed metallic single-walled carbon nanotubes (SWCNTs). The SWCNTs  diameter is assumed to be 1 nm and their length is taken as 10 microns. We use the finite element method for solving Maxwell s equations with appropriate boundary conditions at the ends of each tube. Our results show that there are two different regimes depending on the frequency range considered. In one regime, we find that the transmission coefficient decreases rapidly when increasing the number density of tubes. This behavior can be explained by considering the effect of multiple scattering between neighboring tubes. On the other hand, in another regime where the wavelength is much larger than the tube radius, the transmission coefficient increases slowly with respect to the number density of tubes due to constructive interference effects among scattered waves inside individual tubes. Finally, our numerical results indicate that the presence of defects such as vacancies or impurities may significantly affect the overall transmission properties of the system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electromagnetic signal propagation in an almost circular bundle of closely packed , metallic , carbon nanotubes . Abstract : We research the electromagnetic wave propagation through a bundle of closely - packed metallic single - walled carbon nanotubes ( SWCNTs ) .The SWCNTs diameter is expected to be 1 nm and their height is taken as 10 microns . We use the finite element method for solving Maxwell s coefficients with suitable boundary constraints at the ends of each tube .Our results show that there are two different regimes based on the frequency spectrum considered . In one regime , we find that the propagation coefficient drops rapidly when increasing the number density of tubes .This phenomenon can be described by using the impact of multiple scattering between neighboring tubes . On the other hand , in another regime where the frequency is much larger than the tube diameter , the propagation coefficient increases slowly with regard to the number density of tubes due to constructive interference effects among scattered waves inside individual tubes .Finally , our numerical findings show that the presence of flaws such as vacancies or impurities may significantly affect the overall transmission properties of the system .",
        "rewrite_text": "In this study, we investigate the propagation of electromagnetic waves through a tightly packed bundle of metallic single-walled carbon nanotubes (SWCNTs), characterized by a diameter of approximately 1 nm and a height of 10 microns. Utilizing the finite element method, we solve Maxwell's equations under appropriate boundary conditions applied at the ends of each nanotube. Our analysis reveals two distinct regimes of wave propagation that depend on the frequency spectrum. In the first regime, we observe a rapid decline in the propagation coefficient as the number density of the nanotubes increases. This behavior can be attributed to the effects of multiple scattering occurring between adjacent tubes, which disrupts the coherent transmission of electromagnetic signals. Conversely, in the second regime, where the frequency significantly exceeds the tube diameter, the propagation coefficient exhibits a gradual increase with rising tube density. This enhancement is primarily due to constructive interference among the scattered waves within the individual nanotubes, leading to improved signal transmission. Additionally, our numerical simulations indicate that the presence of defects, such as vacancies or impurities within the nanotube structure, can considerably influence the overall transmission characteristics of the bundle. These findings provide valuable insights into the complex interactions governing electromagnetic signal propagation in nanostructured materials and highlight the importance of structural integrity in optimizing the performance of carbon nanotube-based devices.",
        "ori-fast-z-score": 0.19802950859533489,
        "water-fast-z-score": 4.902903378454601,
        "rewrite-fast-z-score": 1.873171623163388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coronal ion-cyclotron beam instabilities within the multi-fluid description .\nAbstract:\nWe study the linear stability properties of coronal beams in the presence of background plasma and magnetic field fluctuations, using a multi-fluid model for ions and electrons. We find that the growth rates are strongly dependent on the angle between the wavevector k and the mean magnetic field B 0 . In particular, we show that there is an instability at oblique angles with respect to B 0 , which has been previously overlooked by previous studies based on single-fluid models. The new mode arises due to the coupling between the Alfvénic modes associated with each species (ions and electrons). This mode can be excited even when the electron temperature anisotropy T e? /T ez < 1, where ? denotes directions perpendicular to B 0 .\nThe results presented here may have important implications for understanding the origin of solar radio bursts observed during solar flares. Introduction: Coronal mass ejections (CMEs) are large-scale expulsions of magnetized plasma from the Sun s corona into interplanetary space. They play an essential role in driving geomagnetic storms and are believed to be responsible for many other phenomena such as solar energetic particles  e.g., Reames et al. (1998) , Kahler & Ragot (2007)  , solar radio bursts  e.g., Aschwanden (2004)  , and white-light flares  e.g., Benz (2008)  . CME initiation involves the destabilization of a current sheet formed below the erupting flux rope through reconnection processes  e.g., Forbes & Priest (1995) ; Lin & Forbes (2000); Aulanier et al. (2010)  . However, it remains unclear how this process leads to the acceleration of the bulk plasma outflow along open magnetic fields lines. Recent observations suggest that the initial phase of the eruption is characterized by the formation of a narrow jet-like structure called a  flare loop  or  sheath   e.g., Liu et al. (2009a Liu et al. ( , 2009b ; Cheng et al. (2011); Jiang et al. (2012",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coronal electron - cyclotron beam instabilities within the multi - fluid model . Abstract : We research the linear stability properties of coronal beams in the presence of background plasma and magnetic field fluctuations , using a multi - fluid model for ions and electrons .We see that the development rates are strongly dependent on the angle between the wavevector k and the mean magnetic force B 0 . In particular , we prove that there is an instability at oblique angles with regard to B 0 , which has been previously overlooked by earlier methods using on single - fluid models .The new mode occurs due to the interaction between the Alfvénic mechanisms associated with each species ( atoms and electrons ) . This mode can be excited even when the electron thermal anisotropy T e ?/T ez < 1, where ?denotes directions perpendicular to B 0 .The results presented here possibly have important implications for studying the origin of solar radio pulses seen during thermal flares . Introduction : Coronal mass ejections ( CMEs ) are big - scale expulsions of magnetized liquid from the Sun s corona into interplanetary space .They play an essential part in causing geomagnetic winds and are considered to be responsible for numerous other effects such as solar energetic particles e . g . , Reames et al . ( 1998 ) , Kahler & Ragot ( 2007 ) , solar radio flashes e . g . , Aschwanden ( 2004 ) , and green - light flares e . g . , Benz ( 2008 ) .CME initiation consists the destabilization of a current sheet formed below the erupting flux rope through reconnection pathways g . g . , Forbes & Priest ( 1995 ) ; Lin & Forbes ( 2000 ) ; Aulanier et al . ( 2010 ) .However , it remains unsure how this process results to the acceleration of the bulk plasma outflow along open magnetic fields lines . Recent measurements suggest that the early stage of the volcano is characterized by the formation of a thin plane - like structure named a flare loop or pouch e . g . , Liu et al .( 2009a Liu et al . ( , 2009b ; Cheng et al .( 2011 ) ; Jiang et al . ( 2012",
        "rewrite_text": "**Title:** Coronal Electron-Cyclotron Beam Instabilities within the Multi-Fluid Model\n\n**Abstract:** This study investigates the linear stability characteristics of coronal electron beams in the context of background plasma and magnetic field fluctuations, employing a multi-fluid model that accounts for both ions and electrons. Our findings reveal that the growth rates of instabilities are significantly influenced by the angle between the wavevector \\( k \\) and the mean magnetic field \\( B_0 \\). Notably, we demonstrate the existence of an instability at oblique angles relative to \\( B_0 \\), a phenomenon that has been largely overlooked in previous analyses utilizing single-fluid models. This newly identified mode arises from the interaction of Alfvénic mechanisms associated with different species, specifically ions and electrons. Importantly, this mode can be excited even when the electron thermal anisotropy \\( T_e^\\perp / T_e^z < 1 \\), where \\( \\perp \\) indicates directions perpendicular to \\( B_0 \\). The implications of our results are significant, particularly in the context of understanding the origins of solar radio emissions observed during thermal flares.\n\n**Introduction:** Coronal mass ejections (CMEs) represent large-scale expulsions of magnetized plasma from the Sun's corona into the surrounding interplanetary space. These events play a crucial role in driving geomagnetic winds and are implicated in a variety of phenomena, including solar energetic particles (as discussed by Reames et al. 1998 and Kahler & Ragot 2007), solar radio bursts (noted by Aschwanden 2004), and green-light flares (Benz 2008). The initiation of CMEs involves the destabilization of a current sheet formed beneath the erupting flux rope, facilitated by magnetic reconnection processes (Forbes & Priest 1995; Lin & Forbes 2000; Aulanier et al. 2010). However, the mechanisms by which this destabilization leads to the acceleration of bulk plasma outflows along open magnetic field lines remain poorly understood. Recent observations indicate that the initial phase of a CME is characterized by the development of a thin, planar structure known as a flare loop or pouch (Liu et al. 2009a; Liu et al. 2009b; Cheng et al. 2011; Jiang et al. 2012).",
        "ori-fast-z-score": -1.281025230440697,
        "water-fast-z-score": 6.164414002968977,
        "rewrite-fast-z-score": 2.048079801297601
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An unusually brilliant transient in the galaxy Messier 85 .\nAbstract:\nWe report on an unusual bright optical transient (OT) discovered by the Palomar Transient Factory (PTF). The OT was detected at R = 16.7 mag and peaked at R = 14.6 mag, with a rise time of about 1 day. It is located near the center of M85, one of the nearest galaxies to our own Milky Way Galaxy. We find that this event has many properties similar to those observed for supernovae Ia but it lacks spectroscopic signatures typical of these events. This suggests that we are witnessing another type of explosion which may be related to some other types of transients such as tidal disruption flares or superluminous supernovae. \n \n Keywords: Supernova, Optical transient, PTF, Tidal disruption flare, Brightest cluster galaxy \n \n Introduction \n \n In recent years there have been several discoveries of extremely luminous optical transients associated with nearby galaxies. These include the famous outbursts of Eta Carinae (Davidson & Humphreys 1997; Smith et al. 1998), SN 2005ap (Gal-Yam et al. 2005; Foley et al. 2007), ASASSN-14li (Holoien et al. 2014a), ATLAS14aaq (Dong et al. 2015), PS1-10jh (Gezari et al. 2012), iPTF16axa (Kasliwal et al. 2016), and ASASSN-15oi (Shappee et al. 2016). Many of them were found to be associated with supermassive black holes residing in galactic nuclei. However, their exact nature remains unclear. Some authors suggested that they could be caused by tidal disruptions of stars by massive black holes (TDE) (Komossa 2002; Gezari et al. 2009a; Bloom et al. 2011; Holoien et al. 2013b; Arcavi et al. 2014; Brown et al. 2017), while others argued that they might represent new classes of thermonuclear explosions (SNe Ia-like) (Valenti et al. 2009; Kas",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An exceptionally brilliant transient in the universe Messier 85 . Abstract : We report on an strange bright optical transient ( OT ) discovered by the Palomar Transient Factory ( PTF ) .The OT was detected at R = 16 . 7 mag and peaked at R = 14 . 6 mag , with a rise time of about 1 day . It is situated near the center of M85 , one of the nearest galaxies to our own Milky Way Galaxy .We see that this event has numerous characteristics similar to those observed for supernovae Ia but it lacks spectroscopic signatures common of these events . This implies that we are witnessing another type of explosion which perhaps be connected to some other types of transients such as tidal disruption flares or superluminous supernovae .Keywords : Supernova , Optical transient , PTF , Tidal disruption flare , Brightest cluster galaxy Introduction In recent history there have been numerous discoveries of incredibly luminous optical transients linked with nearby galaxies . These include the famous outbursts of Eta Carinae ( Davidson & Humphreys 1997 ; Smith et al .1998 ) , SN 2005ap ( Gal - Yam et al . 2005 ; Foley et al .2007 ) , ASASSN - 14li ( Holoien et al . 2014a ) , ATLAS14aaq ( Dong et al .2015 ) , PS1 - 10jh ( Gezari et al . 2012 ) , iPTF16axa ( Kasliwal et al .2016 ) , and ASASSN - 15oi ( Shappee et al . 2016 ) .Many of them were found to be identified with supermassive black holes residing in galactic nuclei . However , their exact status remains obscure .Some authors proposed that they may be caused by tidal disruptions of stars by massive black holes ( TDE ) ( Komossa 2002 ; Gezari et al . 2009a ; Bloom et al .2011 ; Holoien et al . 2013b ; Arcavi et al .2014 ; Brown et al . 2017 ) , while others argued that they may represent new classes of thermonuclear explosions ( SNe Ia - like ) ( Valenti et al .2009; Kas",
        "rewrite_text": "**Title:** An Exceptionally Brilliant Transient in the Universe Messier 85\n\n**Abstract:** In this study, we present the discovery of an unusual and exceptionally bright optical transient (OT) identified by the Palomar Transient Factory (PTF). The transient was initially detected at a magnitude of R = 16.7 and reached a peak brightness of R = 14.6, exhibiting a rapid rise time of approximately one day. This event is located near the center of Messier 85 (M85), one of the closest galaxies to our Milky Way. While the characteristics of this transient share similarities with those of Type Ia supernovae, it notably lacks the spectroscopic signatures typically associated with such events. This absence suggests that we may be observing a different type of explosive phenomenon, potentially related to other transient events, such as tidal disruption flares or superluminous supernovae. \n\nRecent years have seen a surge in the identification of highly luminous optical transients linked to nearby galaxies, including notable cases like the outbursts of Eta Carinae, SN 2005ap, ASASSN-14li, ATLAS14aaq, PS1-10jh, iPTF16axa, and ASASSN-15oi. Many of these events have been associated with supermassive black holes located in the nuclei of galaxies, yet their precise nature remains uncertain. Some researchers have suggested that these transients could be the result of tidal disruptions of stars by massive black holes (TDEs), while others have posited that they may represent new classes of thermonuclear explosions akin to Type Ia supernovae. This paper aims to further investigate the implications of the discovered transient in M85 and its potential connections to these intriguing astrophysical phenomena. \n\n**Keywords:** Supernova, Optical transient, PTF, Tidal disruption flare, Brightest cluster galaxy.",
        "ori-fast-z-score": -1.4925557853149838,
        "water-fast-z-score": 2.85745490667645,
        "rewrite-fast-z-score": 1.7822655773580138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlinear Dynamics of the Phonon Stimulated Emission in Microwave Solid-State Resonator of the Nonautonomous Phaser Generator .\nAbstract:\nThe nonlinear dynamics of phonon stimulated emission (PSE) is investigated for microwave solid-state resonators with nonautonomous phase-locked loop generators. The PSE process is described by means of an extended version of the Lang-Kobayashi model, which takes into account the influence of the external driving field on the gain medium and includes additional terms describing the effect of spontaneous emission noise. It has been shown that the presence of these new effects leads to significant changes in the behavior of the system under consideration as compared to autonomous systems. In particular, it was found that the generation of chaotic regimes becomes possible even at relatively small values of the pumping parameter. This fact can be used to develop novel types of chaos-based devices based on microwave solid-state oscillators. \n \n Keywords: Nonlinear dynamics; Chaos; Phase synchronization; Phonons; Microwave solid state oscillator. PACS: 42.65.Tt; 42.65.Pq; 42.65.Re",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlinear Dynamics of the Phonon Stimulated Emission in Microwave Solid - State Resonator of the Nonautonomous Phaser Generator . Abstract : The nonlinear dynamics of phonon stimulated emission ( PSE ) is investigated for microwave solid - state resonators with nonautonomous phase - locked loop components .The PSE transition is characterized by means of an extended version of the Lang - Kobayashi model , which assumes into consideration the impact of the external driving field on the gain medium and incorporates additional terms describing the impact of induced emission interference . It has been shown that the presence of these new effects leads to significant improvements in the activity of the process under consideration as compared to autonomous systems .In particular , it was shown that the generation of turbulent regimes appears possible even at fairly little values of the pumping function . This fact can be used to develop new types of chaos - based devices derived on microwave solid - state oscillators .Keywords : Nonlinear dynamics ; Chaos ; Phase synchronization ; Phonons ; Microwave liquid state oscillator . PACS : 42 . 65 . Tt ; 42 . 65 . Pq ; 42 . 65 . Re",
        "rewrite_text": "Title: Nonlinear Dynamics of Phonon Stimulated Emission in Microwave Solid-State Resonators of the Nonautonomous Phaser Generator\n\nAbstract: This study explores the nonlinear dynamics of phonon stimulated emission (PSE) within microwave solid-state resonators that incorporate nonautonomous phase-locked loop components. Utilizing an enhanced version of the Lang-Kobayashi model, we analyze the PSE transition by considering the effects of an external driving field on the gain medium. This model also includes additional terms that account for the interference of induced emissions. Our findings reveal that these newly introduced factors significantly enhance the dynamics of the PSE process when compared to traditional autonomous systems. Notably, we demonstrate that turbulent regimes can be generated even with relatively low values of the pumping function. This discovery opens avenues for the development of innovative chaos-based devices utilizing microwave solid-state oscillators. The implications of this research extend to various applications in nonlinear dynamics, chaos theory, phase synchronization, and phonon interactions within microwave liquid state oscillators. The results underscore the potential for harnessing these nonlinear dynamics to create advanced technological solutions in the field of microwave engineering. \n\nKeywords: Nonlinear dynamics; Chaos; Phase synchronization; Phonons; Microwave solid-state oscillator. \n\nPACS: 42.65.Tt; 42.65.Pq; 42.65.Re",
        "ori-fast-z-score": -0.7875615306482168,
        "water-fast-z-score": 4.47213595499958,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ensemble Learning for Free with Evolutionary Algorithms ? .\nAbstract:\nIn this work, we propose an evolutionary algorithm to evolve ensemble classifiers in the context of multi-class classification problems. The proposed approach is based on the combination of two well-known techniques: genetic algorithms and bagging. We have conducted experiments using several datasets taken from UCI Machine Learning Repository. Our results show that our method outperforms other state-of-the-art methods such as Bagging or Random Forests. In addition, it has been shown that the use of ensembles can improve the performance of single models evolved by Genetic Programming (GP). This fact suggests that GP could be used not only to evolve individual solutions but also to evolve whole ensembles of solutions. Keywords: Ensemble learning, Multi-Class Classification, Genetic Algorithm, Bagging. 1 Introduction Ensembles are collections of multiple base learners whose outputs are combined into one final prediction  1  . They are widely used because they often provide better accuracy than any of their constituent members  2  .\nThe most popular approaches to combine predictions include voting schemes  3  , stacking  4  , boosting  5  , and blending  6  . However, these approaches require some knowledge about how to combine the output of each member of the ensemble  7?   . For example, if there are three classes, then the simplest way would be to assign equal weights to all the classifiers; however, this may lead to poor performance when dealing with imbalanced data  8  . Other more sophisticated ways involve assigning different weights according to the confidence level of each classifier  9  ; however, finding optimal values for those parameters requires additional effort  10  .\nRecently, researchers have started exploring new ways to automatically generate ensembles without requiring prior information  11  . One of them involves combining genetic algorithms  12  and bagging  13  . These two techniques were first applied separately  14, 15  , but later they were integrated together  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ensemble Learning for Free with Evolutionary Algorithms ? .Abstract : In this research , we attempt an phylogenetic algorithm to evolve ensemble classifiers in the context of multi - class classification difficulties . The proposed approach is based on the combination of two well - famous strategies : molecular techniques and bagging .We have done research use multiple datasets taken from UCI Machine Learning Repository . Our results show that our technique outperforms other state - of - the - art methods such as Bagging or Random Forests .In addition , it has been shown that the using of ensembles can boost the performance of single models adapted by Genetic Programming ( GP ) . This fact suggests that GP may be used not only to evolve individual solutions but also to evolve entire ensembles of solutions .Keywords : Ensemble learning , Multi - Class Classification , Genetic Algorithm , Bagging . 1 Introduction Ensembles are collections of multiple base learners whose outputs are united into one finished prediction 1 .They are widely using because they frequently provide better precision than any of their constituent groups 2 . The most popular methods to mix predictions include voting strategies 3 , stacking 4 , boosting 5 , and blending 6 .However , these perspectives need some understanding about how to mix the output of each member of the group 7 ? .For instance , if there are three categories , then the simplest manner might be to give equivalent weights to all the classifiers ; however , this might lead to bad performance when dealing with imbalanced data 8 . Other more sophisticated ways include assigning multiple weights based to the confidence rate of each classifier 9 ; however , finding ideal values for those variables requires added effort 10 .Recently , researchers have started researching new ways to automatically generate ensembles without using prior information 11 . One of them requires merging genetic methods 12 and bagging 13 .These two procedures were first applied separately 14 , 15 , but later they were integrated together 16 .",
        "rewrite_text": "**Title:** Ensemble Learning for Free with Evolutionary Algorithms\n\n**Abstract:** This study explores the application of a phylogenetic algorithm to evolve ensemble classifiers specifically tailored for multi-class classification challenges. Our innovative approach synergizes two well-established methodologies: molecular techniques and bagging. We conducted extensive experiments utilizing various datasets sourced from the UCI Machine Learning Repository. The findings indicate that our proposed technique significantly outperforms leading state-of-the-art methods, including Bagging and Random Forests. Furthermore, our research demonstrates that employing ensemble methods can enhance the performance of individual models optimized through Genetic Programming (GP). This observation implies that GP can be effectively utilized not only for evolving singular solutions but also for developing comprehensive ensembles of solutions.\n\nEnsemble learning involves aggregating the predictions of multiple base learners to produce a final output, which often results in improved accuracy compared to any individual model. Common strategies for combining predictions include voting, stacking, boosting, and blending. However, these methods typically require a nuanced understanding of how to appropriately weigh the contributions of each classifier. For example, in a scenario with three classes, a straightforward approach might assign equal weights to all classifiers; yet, this can lead to suboptimal performance, particularly in cases of imbalanced datasets. More advanced techniques involve assigning varying weights based on the confidence levels of each classifier, though determining the optimal values for these weights can be labor-intensive.\n\nRecently, there has been a growing interest in developing methods to automatically generate ensembles without relying on prior knowledge. One promising avenue involves the integration of genetic algorithms with bagging techniques. While these methods were initially applied independently, recent advancements have led to their successful combination, paving the way for more efficient ensemble learning strategies. This research contributes to the ongoing discourse on enhancing ensemble methods through evolutionary algorithms, offering a novel perspective on their application in multi-class classification tasks. \n\n**Keywords:** Ensemble learning, Multi-Class Classification, Genetic Algorithm, Bagging.",
        "ori-fast-z-score": -0.6620847108818944,
        "water-fast-z-score": 8.384348352573221,
        "rewrite-fast-z-score": 1.0182385849843445
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stochastic interacting particle systems out of equilibrium .\nAbstract:\nWe consider the dynamics of stochastic interacting particle systems in which particles evolve according to an overdamped Langevin equation with multiplicative noise and interact via pair potentials that decay exponentially fast at large distances.  We prove that, under suitable conditions on the interaction potential, these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator. These results provide new insights into the statistical mechanics of such systems far away from thermal equilibrium. Stochastic interacting particle systems have been widely used as simple models for describing physical phenomena ranging from traffic flow  1  , granular media  2  , colloidal suspensions  3  , and biological transport  4  . In this work we focus our attention on one-dimensional models where each particle evolves according to an overdamped Brownian motion driven by white Gaussian noise and interacts with its neighbors through a pairwise potential that decays exponentially fast at infinity (see Figure 1 ). The resulting system is described by the following set of Itô SDE s:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stochastic interacting particle structures out of equilibrium . Abstract : We consider the dynamics of stochastic interacting particle structures in which particles evolve according to an overdamped Langevin equation with multiplicative interference and interact via pair potentials that decay exponentially rapidly at large distances .We prove that , under suitable conditions on the interaction potential , these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator . These data provide fresh insights into the statistical mechanics of such systems farther far from temperature equilibrium .Stochastic interacting particle networks have been widely using as simple theories for describing physical phenomena ranging from street circulation 1 , granular material 2 , colloidal suspensions 3 , and biological transport 4 . In this research we focus our focus on one - dimensional theories where each particle evolves according to an overdamped Brownian movement generated by white Gaussian interference and interacts with its neighbors through a pairwise potential that decays exponentially rapidly at infinity ( see Figure 1 ) .The resulting system is characterized by the following setting of Itô SDE s :",
        "rewrite_text": "In this article, we investigate the dynamics of stochastic interacting particle systems that are out of equilibrium, specifically focusing on how particles evolve according to an overdamped Langevin equation influenced by multiplicative noise. The interactions between particles are modeled through pair potentials that exhibit rapid exponential decay at large distances. We establish that, under certain conditions pertaining to the interaction potential, these systems possess unique stationary states. The density profiles of these states are determined by solutions to nonlinear integral equations that incorporate fractional powers of the Laplacian operator. This finding contributes valuable insights into the statistical mechanics of systems that are significantly removed from thermal equilibrium.\n\nStochastic interacting particle networks serve as effective frameworks for modeling a variety of physical phenomena, including urban traffic dynamics, granular materials, colloidal suspensions, and biological transport mechanisms. Our study concentrates on one-dimensional systems, where each particle's motion is governed by overdamped Brownian dynamics, driven by white Gaussian noise. The interactions among neighboring particles are mediated through a pairwise potential that diminishes rapidly as the distance increases. This setup leads to a well-defined system characterized by Itô stochastic differential equations (SDEs).\n\nThe implications of our results extend to understanding the behavior of complex systems in non-equilibrium states, providing a mathematical foundation for analyzing the interplay between stochastic processes and particle interactions. Through our exploration, we aim to enhance the comprehension of how these systems evolve over time and the nature of their stationary distributions, thereby contributing to the broader field of statistical mechanics and its applications in various scientific domains.",
        "ori-fast-z-score": -0.9332565252573828,
        "water-fast-z-score": 4.538253483538691,
        "rewrite-fast-z-score": 1.4631270419005797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  U B V R I Photometry of Stellar Structures throughout the Disk of the Barred Galaxy NGC 3367 .\nAbstract:\nWe present new U BVRI photometric observations for the barred galaxy NGC 3367, obtained with the 1 m telescope at Cerro Tololo Inter-American Observatory (CTIO). The main goal is to study the stellar structures in this galaxy and their relation to its nuclear activity. We find that there are two bright knots along the major axis of the galaxy which may be associated with star formation regions. These knots have colors similar to those found in HII regions. In addition we detect several other faint knots on both sides of the nucleus. Their color indices suggest that they could also be related to recent star formation events. Finally, we identify an extended structure towards south-east direction whose nature remains unclear. This work was supported by CONACyT grant 36586-E. We thank J. M. Alloin for his help during our observing run at CTIO. Keywords: Starburst galaxies; Nuclear activity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : U B V R I Photometry of Stellar Structures throughout the Disk of the Barred Galaxy NGC 3367 . Abstract : We report new U BVRI photometric surveys for the barred star NGC 3367 , obtained with the 1 m observatory at Cerro Tololo Inter - American Observatory ( CTIO ) .The main goal is to study the stars formations in this galaxy and their connection to its nuclear activity . We see that there are two faint knots along the main axis of the galaxy which perhaps be identified with star formation regions .These threads have colors identical to those present in HII regions . In addition we find several other faint knots on both sides of the nucleus .Their color indices indicate that they may also be connected to recent star formation changes . Finally , we identify an extended structure towards south - eastward direction whose nature remains unsure .This project was supported by CONACyT grant 36586 - E . We praise J . M . Alloin for his help during our observing run at CTIO . Keywords : Starburst galaxies ; Nuclear activity",
        "rewrite_text": "We present new U BVRI photometric surveys of the barred galaxy NGC 3367, conducted using the 1-meter telescope at the Cerro Tololo Inter-American Observatory (CTIO). The primary objective of this research is to investigate the star formation processes within this galaxy and their potential relationship with its nuclear activity. Our observations reveal the presence of two faint knots aligned along the main axis of NGC 3367, which may correspond to regions of active star formation. Notably, these features exhibit color characteristics similar to those found in HII regions, suggesting ongoing stellar activity. Additionally, we have identified several other faint knots located on either side of the galaxy's nucleus, with their color indices indicating a possible connection to recent changes in star formation. Furthermore, we have detected an extended structure extending towards the southeast, although its exact nature remains uncertain at this time. This research was made possible through the support of CONACyT grant 36586-E, and we extend our gratitude to J. M. Alloin for his assistance during our observational campaign at CTIO. Our findings contribute to the understanding of starburst galaxies and their nuclear dynamics, highlighting the intricate relationship between star formation and galactic activity. Keywords: Starburst galaxies; Nuclear activity.",
        "ori-fast-z-score": -0.7977240352174656,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": 0.6060915267313265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Beyond the semi-classical description of black hole evaporation .\nAbstract:\nWe present an analysis of Hawking radiation in terms of Bogoliubov coefficients, which are calculated using WKB wavefunctions for scalar fields on Schwarzschild backgrounds. We show that these results agree with those obtained by other methods when the backreaction is neglected and we also find agreement between our method and previous calculations including backreaction effects at leading order in perturbation theory. In addition to this perturbative check, we perform numerical checks of our results against exact solutions of the Klein-Gordon equation in Schwarzschild spacetime. Finally, we discuss how our approach can be used to calculate corrections beyond the semiclassical approximation. The evaporation of black holes has been studied extensively over many years (see e.g. ), but there remain some open questions about its detailed behaviour. One such question concerns the precise form of the spectrum of emitted particles; it was shown recently  that the standard semi-classical treatment leads to a thermal distribution of particle energies, but it remains unclear whether or not this result holds true once quantum gravity effects become important.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Beyond the semi - classical description of black hole evaporation . Abstract : We present an assessment of Hawking radiation in terms of Bogoliubov coefficients , which are measured using WKB wavefunctions for scalar fields on Schwarzschild backgrounds .We see that these results agree with those achieved by other methods when the backreaction is neglected and we also find agreement between our technique and previous calculations including backreaction effects at leading order in perturbation theory . In addition to this perturbative check , we perform numerical checks of our findings against exact solutions of the Klein - Gordon equation in Schwarzschild spacetime .Finally , we explain how our approach can be used to estimate corrections beyond the semiclassical approximation . The evaporation of black holes has been studied frequently over numerous years ( saw e . g .) , but there remain some open questions about its precise behaviour . One such issue concerns the exact form of the spectrum of emitted particles ; it was shown lately that the standard semi - classical treatment leads to a heat distribution of particle energies , but it remains unsure whether or not this consequence holds true once quantum gravitational changes become crucial .",
        "rewrite_text": "In this article, we delve into the intricacies of Hawking radiation by employing Bogoliubov coefficients, utilizing WKB wavefunctions for scalar fields within Schwarzschild backgrounds. Our findings reveal a strong alignment with results derived from alternative methodologies, particularly in scenarios where backreaction effects are disregarded. Furthermore, we establish consistency between our approach and prior calculations that incorporate backreaction effects, specifically at the leading order of perturbation theory. Beyond these perturbative validations, we conduct numerical analyses to compare our results with exact solutions of the Klein-Gordon equation in the context of Schwarzschild spacetime. This comprehensive examination allows us to elucidate how our methodology can be leveraged to estimate corrections that extend beyond the semiclassical framework.\n\nThe phenomenon of black hole evaporation has been a focal point of research for many years, yet several critical questions remain unresolved regarding its precise dynamics. A significant area of inquiry pertains to the specific characteristics of the emitted particle spectrum. Recent studies have indicated that the conventional semiclassical approach yields a thermal distribution of particle energies. However, the validity of this conclusion is still uncertain, particularly when quantum gravitational effects become significant. Our work aims to contribute to this ongoing discourse by providing a more nuanced understanding of Hawking radiation and its implications for black hole evaporation, ultimately seeking to bridge the gap between semiclassical predictions and the complexities introduced by quantum gravity.",
        "ori-fast-z-score": 0.5184758473652127,
        "water-fast-z-score": 5.363390480545726,
        "rewrite-fast-z-score": 0.4833682445228318
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  WHAM: A WENO-based general relativistic numerical scheme I: Hydrodynamics .\nAbstract:\nWe present the first version of our new code, WHAM (Weno-Hybrid Arbitrary Mesh), which is based on the recently developed weighted essentially non-oscillatory (WENO) schemes for solving hyperbolic conservation laws in one dimension and multi-dimensions.  The main idea behind this method is to use high-order spatial reconstruction with an adaptive mesh refinement technique to achieve high accuracy at low computational cost. We have implemented several different versions of the WENO algorithm into our code including the fifth-order WENO-Z scheme as well as the third- and fifth-order WENO-JS schemes. In addition we also implement the fourth-order Runge-Kutta time integration scheme along with the Harten-Lax-van Leer contact discontinuity capturing scheme to handle discontinuities arising during hydrodynamic evolution. Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : WHAM : A WENO - based general relativistic numerical system I : Hydrodynamics . Abstract : We introduce the first version of our new code , WHAM ( Weno - Hybrid Arbitrary Mesh ) , which is based on the recently established weighted essentially non - oscillatory ( WENO ) schemes for solving hyperbolic conservation laws in one dimension and multi - dimensions .The main idea behind this algorithm is to use large - order spatial reconstruction with an adaptive mesh refinement technique to achieve high sensitivity at low computational efficiency . We have integrated various different versions of the WENO algorithm into our code including the fifth - order WENO - Z plan as well as the third - and fifth - order WENO - JS schemes .In addition we also incorporate the fourth - order Runge - Kutta time integration scheme along with the Harten - Lax - van Leer contact discontinuity capturing scheme to manage discontinuities resulting during hydrodynamic evolution . Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions .",
        "rewrite_text": "We present the initial version of our innovative computational code, WHAM (Weno-Hybrid Arbitrary Mesh), designed for general relativistic hydrodynamics. This code leverages the recently developed weighted essentially non-oscillatory (WENO) schemes, which are effective for solving hyperbolic conservation laws across both one-dimensional and multi-dimensional frameworks. The core concept of WHAM revolves around employing high-order spatial reconstruction techniques in conjunction with adaptive mesh refinement, allowing for enhanced sensitivity while maintaining computational efficiency. \n\nIn the development of WHAM, we have integrated multiple variants of the WENO algorithm, including the fifth-order WENO-Z scheme and both third- and fifth-order WENO-JS schemes. Furthermore, we have implemented a fourth-order Runge-Kutta method for time integration, complemented by the Harten-Lax-van Leer (HLL) scheme to effectively capture contact discontinuities that may arise during the hydrodynamic evolution process. \n\nOur extensive testing demonstrates that the algorithms incorporated within WHAM yield highly accurate solutions when benchmarked against exact or reference solutions. This capability is crucial for advancing the field of numerical relativity and hydrodynamics, as it allows for precise modeling of complex astrophysical phenomena. Overall, WHAM represents a significant step forward in the development of robust numerical tools for the simulation of relativistic hydrodynamic systems.",
        "ori-fast-z-score": 1.9188064472004938,
        "water-fast-z-score": 4.9819900360298925,
        "rewrite-fast-z-score": 0.09853292781642932
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optical BVI Imaging and HI Synthesis Observations of the Dwarf Irregular Galaxy ESO 364-G 029 .\nAbstract:\nWe present optical BVRI imaging, near-infrared JHKs photometry, and radio continuum observations at 1.4 GHz for the dwarf irregular galaxy ESO 364-G 029 (UGC 6456). The new data are combined with existing Hα spectroscopy to study its star formation history over the past few hundred million years. We find that this galaxy has experienced several bursts of intense star formation in recent times, which have produced large amounts of ionized gas visible as bright knots of emission across most of the face-on disk. These knots appear to be associated with young massive stars formed during each episode of star formation. In addition, we detect an extended component of diffuse ionized gas surrounding these knots. This is likely due to photoionization by hot evolved stars or supernovae remnants. Using our deepest images taken under good seeing conditions, we measure a total stellar mass of M = 2.1 × 10^7 M_sol within a radius of 5 kpc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optical BVI Imaging and HI Synthesis Observations of the Dwarf Irregular Galaxy ESO 364 - G 029 . Abstract : We report optical BVRI imaging , near - infrared JHKs photometry , and radio continuum measurements at 1 . 4 GHz for the dwarf irregular universe ESO 364 - G 029 ( UGC 6456 ) .The revised data are coupled with existing Hα spectroscopy to study its galaxy formation history over the previous few hundred million years . We see that this galaxy has undergone several bursts of aggressive star formation in recent periods , which have created vast quantities of ionized gas evident as bright knots of emission across most of the face - on disk .These knots appear to be identified with young massive stars formed during each season of star formation . In addition , we find an extended component of diffuse ionized gas covering these knots .This is probably due to photoionization by hot evolved galaxies or supernovae fragments . Using our deepest images took under good see conditions , we measure a total stellar mass of M = 2 . 1 × 10 ^ 7 M _ sol within a diameter of 5 kpc .",
        "rewrite_text": "We present a comprehensive study of the dwarf irregular galaxy ESO 364-G 029 (UGC 6456), utilizing optical BVRI imaging, near-infrared JHKs photometry, and radio continuum observations at 1.4 GHz. This research integrates our newly acquired data with existing Hα spectroscopy to investigate the galaxy's formation history over the last few hundred million years. Our findings indicate that ESO 364-G 029 has experienced multiple episodes of vigorous star formation in recent times, resulting in the production of significant amounts of ionized gas. This ionized gas is prominently displayed as bright emission knots scattered across the galaxy's face-on disk, which we associate with young, massive stars that have formed during these star formation events. Furthermore, we observe an extended region of diffuse ionized gas enveloping these emission knots, likely a consequence of photoionization from hot evolved stars or remnants from supernova explosions. By analyzing our deepest images captured under optimal seeing conditions, we estimate the total stellar mass of the galaxy to be approximately M = 2.1 × 10^7 M_sol within a radius of 5 kpc. This study enhances our understanding of the star formation processes and the overall evolutionary dynamics of dwarf irregular galaxies like ESO 364-G 029, contributing valuable insights into the mechanisms driving their development and the role they play in the broader context of galaxy formation.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 5.597977259474208,
        "rewrite-fast-z-score": 0.8700628401410971
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  MHD simulations of the magnetorotational instability in a shearing box with zero net flux. I. The issue of convergence .\nAbstract:\nWe present results for MHD simulations of the magneto-rotational instability (MRI) in a stratified, Keplerian disk threaded by an initially vertical magnetic field using the ZEUS-2D code. We consider both isothermal and adiabatic equations of state to examine how the MRI depends on the thermodynamics of the gas. In all cases we find that the growth rate of the fastest growing mode agrees well with linear theory predictions when normalized appropriately. However, there are significant differences between our runs depending upon whether or not they have reached steady-state equilibrium. For example, the saturated level of stress achieved at late times varies significantly among different models. This suggests that it may be difficult to accurately predict the saturation amplitude of the MRI unless one can perform very high resolution calculations which evolve over many orbital periods. Finally, we show that the inclusion of radiative cooling has little effect on the properties of the turbulence generated by the MRI.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : MHD simulations of the magnetorotational instability in a shearing box with zero net flux . I .The question of convergence . Abstract : We present results for MHD simulations of the magneto - rotational instability ( MRI ) in a stratified , Keplerian disk threaded by an initially vertical magnetic force using the ZEUS - 2D coding .We consider both isothermal and adiabatic equations of state to examine how the MRI depends on the thermodynamics of the gas . In all situations we find that the development frequency of the fastest growing mode agrees well with continuous theory expectations when normalized appropriately .However , there are significant variations between our runs depending upon whether or not they have achieved steady - state balance . For instance , the saturated amount of stress attained at late times changes significantly among different models .This implies that it could be impossible to correctly calculate the saturation amplitude of the MRI unless one can conduct very high resolution measurements which evolve over numerous orbital periods . Finally , we prove that the introduction of radiative cooling has little impact on the properties of the turbulence generated by the MRI .",
        "rewrite_text": "We present findings from our MHD simulations investigating the magnetorotational instability (MRI) within a stratified, Keplerian disk that is initially influenced by a vertical magnetic field, utilizing the ZEUS-2D code. Our study explores both isothermal and adiabatic equations of state to assess how the thermodynamic properties of the gas affect the MRI's behavior. Throughout our simulations, we observe that the frequency at which the fastest-growing mode develops aligns closely with theoretical predictions when appropriately normalized. However, we note significant discrepancies in the outcomes of different simulations, particularly regarding whether a steady-state balance has been achieved. For example, the level of stress that stabilizes at later stages varies considerably across different models. This variation suggests that accurately determining the saturation amplitude of the MRI may be challenging without conducting high-resolution simulations that span multiple orbital periods. Additionally, our results indicate that incorporating radiative cooling into the simulations has minimal effect on the turbulence characteristics induced by the MRI. Overall, our work highlights the complexities involved in studying the MRI and underscores the necessity for high-resolution data to fully understand its saturation behavior in astrophysical contexts.",
        "ori-fast-z-score": -1.7457431218879391,
        "water-fast-z-score": 4.528976474544414,
        "rewrite-fast-z-score": -0.32539568672798425
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Uncovering the Near-IR Dwarf Galaxy Population of the Coma Cluster with Spitzer IRAC .\nAbstract:\nWe present new near-infrared (NIR) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope, which allow us to study the dwarf galaxy population in this rich environment for the first time at wavelengths longer than 1 micron. We identify and classify all galaxies detected within an area of 0.5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag. The majority of these sources are faint red galaxies that have been missed by previous optical surveys due to their low surface brightnesses. Using photometric redshift estimates we find that most of them lie between z=0.1 and z=1.0. By comparing our sample to existing spectroscopic data sets we show that our NIR selection is complete up to M* ~ - 17 + 5 log h70. This corresponds roughly to L*(z=0), but it should be noted that there may still exist some fainter dwarfs below our detection limit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Uncovering the Near - IR Dwarf Galaxy Population of the Coma Cluster with Spitzer IRAC . Abstract : We report new near - infrared ( NIR ) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope , which allow us to study the dwarf galaxy community in this rich environment for the first time at wavelengths greater than 1 micron .We recognize and classify all galaxies found within an area of 0 . 5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag . The majority of these sources are faint red clusters that have been missed by earlier optical searches due to their low exterior brightnesses .Using photometric redshift estimates we find that most of them lie between z = 0 . 1 and z = 1 . 0 . By comparing our sample to existing spectroscopic data sets we determine that our NIR selection is complete up to M * ~ - 17 + 5 log h70 .This equals roughly to L * ( z = 0 ) , but it should be mentioned that there may still exist some fainter dwarfs below our detection limit .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the Coma cluster, conducted with the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. This study marks the first comprehensive examination of the dwarf galaxy population in this densely populated cluster at wavelengths exceeding 1 micron. Our research encompasses a region of 0.5 square degrees centered on the Coma cluster, where we identify and classify all galaxies down to a limiting magnitude of Ks = 18 mag. A significant portion of the detected sources comprises faint red galaxies that previous optical surveys overlooked due to their low surface brightness. Utilizing photometric redshift estimates, we ascertain that the majority of these galaxies are situated within the redshift range of z = 0.1 to z = 1.0. To validate our findings, we compare our NIR-selected sample with existing spectroscopic datasets, concluding that our selection is complete for galaxies with absolute magnitudes M* ~ -17 + 5 log h70. This threshold corresponds approximately to L* at z = 0, although it is important to note that there may still be undetected fainter dwarf galaxies below our current detection limit. Our findings contribute to a deeper understanding of the dwarf galaxy population in the Coma cluster, providing insights into their characteristics and distribution in this rich astronomical environment.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 1.4757295747452437
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for Heavy Neutral MSSM Higgs Bosons with CMS: Reach and Higgs-Mass Precision .\nAbstract:\nThe search is performed in the context of the Minimal Supersymmetric Standard Model (MSSM) using data collected by the Compact Muon Solenoid experiment at sqrt(s) = 7 TeV, corresponding to an integrated luminosity of 5 fb-1 . The results are interpreted as limits on the production cross section times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance. In addition, upper bounds on the mass difference between the lightest CP-even Higgs boson and its heavier CP-even or CP-odd partner are derived. These results improve upon previous searches conducted by the ATLAS collaboration. \n \n A summary of this work has been presented at: \n \n \n \n \n \n This document contains additional information that may be useful to readers interested in reproducing our analysis or applying it to other datasets. It also includes details about how we have validated our results against those obtained independently by the ATLAS collaboration. \n \nIntroduction\n\nThe discovery of a new particle consistent with the Standard Model (SM) Higgs boson  1–3  has opened up a new era in particle physics. However, many open questions remain regarding the properties of this newly discovered state  4  , including whether it is part of a larger multiplet  5  .\nIn supersymmetry  6  , each SM field has a superpartner differing only in spin statistics  7, 8  . If R-parity  9  is conserved, then all superpartners must be produced in pairs  10  . One consequence of this scenario is that there can exist more than one Higgs doublet  11  . In particular, if the lighter scalar Higgs boson observed at the LHC  12–18  corresponds to the lightest CP-eigenstate h0 of such a model  19, 20  , then the next-to-lightest CP-eigenstates H0 and A0 could both couple strongly to fermions  21  . Such scenarios would lead to enhanced rates for decays of these states into final states containing photons  22  . \n \n In order to explore possible deviations from the SM predictions  23  , precise measurements of the masses and couplings of the Higgs bosons predicted by",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for Heavy Neutral MSSM Higgs Bosons with CMS : Reach and Higgs - Mass Precision . Abstract : The hunt is conducted in the context of the Minimal Supersymmetric Standard Model ( MSSM ) using data received by the Compact Muon Solenoid research at sqrt ( s ) = 7 TeV , equivalent to an integrated luminosity of 5 fb - 1 .The results are understood as limits on the production cross area times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance . In addition , upper limits on the mass ratio between the lightest CP - even Higgs boson and its lighter CP - even or CP - even partner are derived .These conclusions improve upon recent searches undertaken by the ATLAS collaboration . A description of this research has been presented at : This report contains additional information that might be valuable to readers interested in reproducing our analysis or applying it to other datasets .It also contains details about how we have validated our findings against those acquired independently by the ATLAS collaboration . Introduction The discovery of a new particle compatible with the Standard Model ( SM ) Higgs boson 1 – 3 has opened up a new decade in particle science .However , many open questions remain regarding the properties of this newly discovered state 4 , particularly whether it is part of a greater multiplet 5 . In supersymmetry 6 , each SM field has a superpartner varying only in spin statistics 7 , 8 .If R - parity 9 is conserved , then all superpartners must be made in pairs 10 . One result of this situation is that there can exist more than one Higgs doublet 11 .In particular , if the lighter scalar Higgs boson seen at the LHC 12 – 18 corresponds to the lightest CP - eigenstate h0 of such a theory 19 , 20 , then the second - to - lightest CP - eigenstates H0 and A0 could both bond heavily to fermions 21 . Such scenarios would result to accelerated rates for decays of these states into last states carrying photons 22 .In order to examine possible deviations from the SM predictions 23 , detailed observations of the masses and couplings of the Higgs bosons predicted by",
        "rewrite_text": "Title: Search for Heavy Neutral MSSM Higgs Bosons with CMS: Reach and Higgs-Mass Precision\n\nAbstract: This study investigates the search for heavy neutral Higgs bosons within the framework of the Minimal Supersymmetric Standard Model (MSSM), utilizing data collected by the Compact Muon Solenoid (CMS) at a center-of-mass energy of √s = 7 TeV, corresponding to an integrated luminosity of 5 fb⁻¹. The analysis focuses on establishing limits on the production cross-section multiplied by the branching fraction into two photons for neutral Higgs bosons that decay within the detector's acceptance. Additionally, we derive upper limits on the mass ratio between the lightest CP-even Higgs boson and its lighter CP-even or CP-odd counterparts. These findings enhance the constraints set by recent searches conducted by the ATLAS collaboration. The report includes supplementary information that may assist readers in replicating our analysis or applying it to alternative datasets. Furthermore, we provide details on how our results have been cross-validated against independent findings from the ATLAS collaboration.\n\nIntroduction: The recent discovery of a particle consistent with the Standard Model (SM) Higgs boson has ushered in a new era in particle physics. However, numerous questions persist regarding the characteristics of this newly identified particle, particularly its potential association with a larger multiplet structure. In the context of supersymmetry, each SM field is paired with a superpartner that differs only in spin statistics. If R-parity is conserved, these superpartners must exist in pairs, leading to the possibility of multiple Higgs doublets. Specifically, if the lighter scalar Higgs boson observed at the Large Hadron Collider (LHC) corresponds to the lightest CP-eigenstate (h₀) in this theoretical framework, then the second-lightest CP-eigenstates (H₀ and A₀) could couple significantly to fermions. Such interactions would likely result in increased decay rates of these states into final states that include photons. To explore potential deviations from SM predictions, it is crucial to conduct detailed measurements of the masses and couplings of the Higgs bosons anticipated by the MSSM.",
        "ori-fast-z-score": -3.1057477829563984,
        "water-fast-z-score": 5.1946603473211805,
        "rewrite-fast-z-score": -1.0690449676496976
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An Exceptional VHE Gamma-Ray Flare of PKS 2155-304 .\nAbstract:\nWe report on the detection by HESS of an exceptional flaring activity in the very-high-energy (VHE) gamma-ray band for the blazar PKS 2155-304, which was observed between September and November 2007 with a flux doubling time scale as short as ~1 day. The source reached its highest recorded state ever detected at TeV energies during this period. We present results based on data taken simultaneously with Swift-XRT and UVOT instruments that show no significant change in X-ray or optical emission properties over the course of the outburst. This suggests that the high-energy emission is produced via inverse Compton scattering off relativistic electrons located close to the central black hole rather than synchrotron radiation emitted by particles accelerated up to high Lorentz factors within the jet itself. These findings are consistent with theoretical models where particle acceleration takes place in shocks formed when jets collide with surrounding clouds. Keywords: Blazars, Very High Energy Emission",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An Exceptional VHE Gamma - Ray Flare of PKS 2155 - 304 . Abstract : We report on the discovery by HESS of an exceptional flaring activity in the very - large - energy ( VHE ) γ - ray band for the blazar PKS 2155 - 304 , which was seen between September and November 2007 with a flux doubling time scale as short as ~ 1 day .The source achieved its highest recorded state yet detected at TeV energies during this era . We report findings based on statistics taken concurrently with Swift - XRT and UVOT instruments that demonstrate no major shift in X - ray or imaging emission behavior over the course of the outburst .This implies that the high - energy emission is produced via inverse Compton absorption off relativistic electrons located close to the main white hole rather than synchrotron emission generated by particles driven up to low Lorentz factors within the jet itself . These conclusions are compatible with theoretical theories where particle acceleration comes place in shocks formed when jets collide with nearby clouds .Keywords : Blazars , Very High Energy Emission",
        "rewrite_text": "We present our findings on an extraordinary event of flaring activity in the very high-energy (VHE) gamma-ray spectrum associated with the blazar PKS 2155-304, as observed by the High Energy Stereoscopic System (HESS) between September and November 2007. During this period, the source exhibited a remarkable flux doubling time scale of approximately one day, marking the highest level of TeV (teraelectronvolt) emission ever recorded for this object. Our analysis incorporates data collected simultaneously with the Swift X-Ray Telescope (XRT) and the Ultraviolet/Optical Telescope (UVOT), revealing no significant changes in the X-ray or optical emission characteristics throughout the duration of the flare. This stability suggests that the high-energy gamma-ray emissions are likely produced through inverse Compton scattering involving relativistic electrons situated in close proximity to the central black hole, rather than being a result of synchrotron radiation from particles accelerated to lower Lorentz factors within the jet. These observations align with existing theoretical models that propose particle acceleration occurs in shock waves generated when the jets interact with surrounding clouds. Our findings contribute to a deeper understanding of the mechanisms driving high-energy emissions in blazars and underscore the dynamic nature of PKS 2155-304 during this exceptional flaring episode. The implications of this research extend to the broader field of astrophysics, particularly in the study of active galactic nuclei and their energetic processes. \n\nKeywords: Blazars, Very High Energy Emission.",
        "ori-fast-z-score": -0.6625891564490792,
        "water-fast-z-score": 5.963302408041713,
        "rewrite-fast-z-score": -0.9534625892455924
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Numerical propagation of high energy cosmic rays in the Galaxy I: technical issues .\nAbstract:\nWe present here an overview of our numerical method for propagating cosmic ray particles through the Galactic magnetic field and interstellar medium, including all relevant physical processes such as diffusion, convection, adiabatic deceleration/acceleration, nuclear fragmentation and radioactive decay. We discuss how we treat the various sources of uncertainty associated with these processes (e.g., uncertainties in the strength and structure of the Galactic magnetic field) by performing several different calculations using different assumptions about each process. Finally, we describe some preliminary results obtained from this code which demonstrate that it is capable of reproducing many observed features of the diffuse gamma-ray emission produced by cosmic ray interactions with gas throughout the Milky Way. The main goal of this work is to develop a new technique for studying the origin and propagation of cosmic rays on cosmological scales. \n \n High-energy cosmic rays are believed to be accelerated at astrophysical shocks driven by supernova remnants or active galactic nuclei. These energetic particles then propagate diffusively through space until they interact with matter or radiation fields along their trajectories. In doing so, they produce secondary photons and neutrinos via hadronuclear reactions and inverse Compton scattering respectively. Cosmic rays also contribute significantly to the total pressure support within galaxies and may play an important role in regulating star formation rates therein. However, despite decades of theoretical study, there remain significant uncertainties regarding both the acceleration mechanisms responsible for producing cosmic rays and the transport properties of those same cosmic rays once they have been accelerated. This situation has led to considerable debate over whether cosmic rays can account for the bulk of the pressure required to maintain the regular rotation curves of spiral galaxies without violating observational constraints imposed by the non-detection of dark matter halos around most nearby galaxies.  \n \n To address these questions, we have developed a new computational tool called GALPROP (Galaxy Propagator), which solves numerically the time-dependent transport equation describing the evolution of cosmic ray distributions in three spatial dimensions under the influence of large-scale magnetic fields and turbulent motions. Our approach involves discretizing the distribution function into a large number of",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Numerical spreading of high energy cosmic rays in the Galaxy I : technical problems . Abstract : We present here an overview of our numerical technique for propagating cosmic ray dust through the Galactic magnetic field and interstellar medium , covering all relevant physical processes such as diffusion , convection , adiabatic deceleration / velocity , nuclear fragmentation and radioactive decay .We discuss how we treat the various sources of uncertainty associated with these mechanisms ( e . g . , uncertainties in the strength and shape of the Galactic magnetic force ) by performing numerous separate measurements involving varying predictions about each process . Finally , we explain some preliminary results acquired from this code which demonstrate that it is capable of reproducing many observed features of the diffuse γ - ray radiation created by cosmic ray interactions with gas throughout the Milky Way .The main goal of this project is to develop a new technique for studying the origin and propagation of cosmic rays on cosmological scales . High - energy cosmic rays are said to be enhanced at astrophysical shocks driven by supernova remnants or active galactic nuclei .These energetic particles then propagate diffusively through space until they interact with matter or emission fields along their trajectories . In doing so , they produce secondary photons and neutrinos via hadronuclear reactions and inverse Compton absorption respectively .Cosmic rays additionally help substantially to the total stress support within stars and may play an important role in controlling galaxy formation rates therein . However , despite decades of theoretical investigation , there remain considerable uncertainties regarding both the acceleration mechanisms involved for producing cosmic rays and the travel properties of those same cosmic rays once they have been accelerated .This problem has led to considerable debate over whether cosmic rays can provide for the majority of the pressure required to keep the standard rotation curves of spiral galaxies without violating observational restrictions imposed by the non - observation of bright matter halos around most nearby galaxies . To address these problems , we have developed a new computational tool called GALPROP ( Galaxy Propagator ) , which solves numerically the time - dependent transport equation explaining the evolution of cosmic ray distributions in three spatial dimensions under the impact of large - scale magnetic fields and turbulent movements .Our solution involves discretizing the distribution function into a large number of",
        "rewrite_text": "**Title:** Numerical Spreading of High-Energy Cosmic Rays in the Galaxy I: Technical Challenges\n\n**Abstract:** This article provides a comprehensive overview of our numerical methodology for simulating the propagation of cosmic ray particles through the Galactic magnetic field and the interstellar medium. Our approach encompasses all significant physical processes, including diffusion, convection, adiabatic deceleration, nuclear fragmentation, and radioactive decay. We address the various uncertainties associated with these processes, such as the variability in the strength and configuration of the Galactic magnetic field, by conducting multiple simulations that explore a range of predictions for each mechanism. Preliminary results obtained from our computational framework indicate its effectiveness in replicating many observed characteristics of the diffuse gamma-ray radiation produced by cosmic ray interactions with interstellar gas throughout the Milky Way.\n\nThe primary objective of this research is to establish a novel technique for investigating the origins and propagation of cosmic rays on cosmological scales. High-energy cosmic rays are believed to be amplified at astrophysical shocks generated by phenomena such as supernova remnants and active galactic nuclei. These energetic particles subsequently diffuse through space until they encounter matter or emission fields along their paths, resulting in the production of secondary photons and neutrinos through hadronuclear interactions and inverse Compton scattering, respectively. Furthermore, cosmic rays contribute significantly to the overall pressure support within stars and may play a crucial role in regulating galaxy formation rates.\n\nDespite extensive theoretical studies over the past few decades, substantial uncertainties persist regarding both the mechanisms responsible for cosmic ray acceleration and their propagation characteristics post-acceleration. This uncertainty has sparked considerable debate about the potential of cosmic rays to account for the majority of the pressure necessary to maintain the standard rotation curves of spiral galaxies, particularly in light of observational constraints that reveal a lack of bright matter halos surrounding many nearby galaxies. To tackle these challenges, we have developed a new computational tool named GALPROP (Galaxy Propagator), which numerically solves the time-dependent transport equation governing the evolution of cosmic ray distributions in three-dimensional space, influenced by large-scale magnetic fields and turbulent motions. Our methodology involves discretizing the distribution function into a substantial number of elements, allowing for a detailed analysis of cosmic ray dynamics.",
        "ori-fast-z-score": 0.9949366763261821,
        "water-fast-z-score": 7.285714285714286,
        "rewrite-fast-z-score": -0.5038710255240862
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Epitaxial thin films of multiferroic Bi2FeCrO6 with B-site cationic order .\nAbstract:\nEpitaxial thin films of the multiferroic compound Bi2FeCrO 6 were grown on (001)-oriented SrTiO3 substrates by pulsed laser deposition at 750 °C in an oxygen partial pressure of 0.1 mbar and annealed for 30 min under vacuum conditions to induce ferroelectricity.  The structural properties of these epitaxial films are investigated using X-ray diffraction, transmission electron microscopy, scanning probe techniques as well as Raman spectroscopy. It is found that the films grow coherently strained along  001  direction with a tetragonal structure. A strong in-plane anisotropy between the out-of-plane lattice parameters c and a was observed which can be explained by different ionic radii of Fe 3+ , Cr 3+ and Ti 4+ . In addition, it could be shown that the films exhibit a rhombohedral-like distortion due to the presence of antiphase boundaries.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Epitaxial thin films of multiferroic Bi2FeCrO6 with B - location cationic order . Abstract : Epitaxial thin films of the multiferroic compound Bi2FeCrO 6 were grown on ( 001 ) - oriented SrTiO3 substrates by pulsed infrared deposition at 750 °C in an oxygen partial pressure of 0 . 1 mbar and annealed for 30 min under vacuum environments to create ferroelectricity .The structural structure of these epitaxial films are examined utilizing X - ray diffraction , transmission electron microscopy , scanning probe methods as well as Raman spectroscopy . It is found that the films grow coherently strained along 001 direction with a tetragonal shape .A strong in - plane anisotropy between the out - of - plane lattice parameters c and a was seen which can be described by varying ionic radii of Fe 3 + , Cr 3 + and Ti 4 + . In addition , it could be shown that the films show a rhombohedral - like degradation due to the presence of antiphase borders .",
        "rewrite_text": "**Title:** Epitaxial Thin Films of Multiferroic Bi2FeCrO6 with B-Location Cationic Order\n\n**Abstract:** This study presents the growth and characterization of epitaxial thin films of the multiferroic compound Bi2FeCrO6, achieved on (001)-oriented SrTiO3 substrates through pulsed infrared deposition at a temperature of 750 °C under an oxygen partial pressure of 0.1 mbar. Following the deposition, the films underwent a 30-minute annealing process in a vacuum environment to enhance their ferroelectric properties. The structural characteristics of the resulting epitaxial films were thoroughly investigated using a combination of advanced techniques, including X-ray diffraction, transmission electron microscopy, scanning probe microscopy, and Raman spectroscopy. \n\nThe analysis revealed that the films exhibit coherent strain along the 001 direction, adopting a tetragonal crystal structure. Notably, a pronounced in-plane anisotropy was observed between the out-of-plane lattice parameters, c and a, which can be attributed to the differing ionic radii of the constituent cations, specifically Fe3+, Cr3+, and Ti4+. Furthermore, the presence of antiphase boundaries was identified, leading to a rhombohedral-like degradation in the films' structure. These findings contribute to a deeper understanding of the structural and electronic properties of Bi2FeCrO6 thin films, highlighting their potential applications in multiferroic devices. The results underscore the significance of cationic ordering and strain engineering in optimizing the functional properties of multiferroic materials.",
        "ori-fast-z-score": -2.3566599571949607,
        "water-fast-z-score": 1.9694638556693236,
        "rewrite-fast-z-score": -1.3337718577107005
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evidence for the Evolution of Young Early-Type Galaxies in the GOODS/CDF-S Field .\nAbstract:\nWe present new spectroscopic observations of galaxies at z ~ 1.5-2.0 selected by their UVJ colors and optical morphologies, obtained with VLT/VIMOS on the Very Large Telescope (VLT). We find that these objects are mostly early-type galaxies showing signs of recent star formation activity. The observed properties suggest that they may be progenitors of local massive elliptical galaxies. These results provide further evidence supporting the scenario where most massive galaxies grow through mergers between gas-rich disk systems during the first half of cosmic time. This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2.0, which permits unrestricted use, distribution, and reproduction in any medium provided the original work is properly cited. \n \n Keywords: galaxy evolution; merger remnants; young ellipticals; CDF-S field \n \n Massive galaxies evolve rapidly over cosmic time as a result of merging processes involving smaller companions. In particular, it has been suggested that many of today s brightest cluster galaxies were formed via major mergers of two or more gas-rich disks at redshifts around one to three  1  . However, direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift  2  .\n \nIn order to study the physical mechanisms driving galaxy growth we have carried out deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph  3  . Our sample consists of about 100 galaxies selected based on their ultraviolet J (UVJ) color  4  , morphological type  5  , and apparent magnitude  6  . Most of them show strong emission lines characteristic of active star-forming regions  7, 8  . Their stellar masses range from 10^10 M_sol to 10^11 M_sol  9  . \n\n\nThe main goal of our project was to identify possible candidates for progenitor populations of local massive elliptical/S0 galaxies  10  . To do so, we used several selection criteria designed to select galaxies with similar characteristics to those found among nearby massive spheroids  11  : \n\n\n1. Morphological type: all targets must",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evidence for the Evolution of Young Early - Type Galaxies in the GOODS / CDF - S Field . Abstract : We report new spectroscopic observations of galaxies at z ~ 1 . 5 - 2 . 0 selected by their UVJ colors and imaging morphologies , obtained with VLT / VIMOS on the Very Large Telescope ( VLT ) .We see that these objects are mostly early - class stars displaying signs of recent star formation activity . The observed properties suggest that they may be progenitors of local powerful elliptical galaxies .These data provide further evidence supporting the scenario where most gigantic galaxies grow through mergers between gas - rich disk systems during the first half of cosmic time . This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2 . 0 , which allows unrestricted use , distribution , and reproduction in any medium provided the original book is properly cited .Keywords : universe progression ; collision remnants ; young ellipticals ; CDF - S field Massive stars develop rapidly over cosmic time as a outcome of combining processes involving smaller companions . In particular , it has been proposed that several of today s brightest cluster clusters were created via large mergers of two or more gas - rich disks at redshifts around one to three 1 .However , direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift 2 . In order to study the physical mechanisms governing star development we have carried out deep spectroscopy of clusters at intermediate redshifts using the VLT - VIMOS spectrograph 3 .Our specimen consists of about 100 galaxies chose based on their ultraviolet J ( UVJ ) color 4 , morphological class 5 , and apparent magnitude 6 . Most of them show strong absorption patterns characteristic of active star - creating areas 7 , 8 .Their stellar masses range from 10 ^ 10 M _ sol to 10 ^ 11 M _ sol 9 . The main goal of our work was to identify possible candidates for progenitor populations of local heavy elliptical / S0 galaxies 10 .To do so , we using numerous selection categories modified to select galaxies with similar characteristics to those detected among neighboring massive spheroids 11 : 1 . Morphological type : all targets must",
        "rewrite_text": "**Title:** Evidence for the Evolution of Young Early-Type Galaxies in the GOODS/CDF-S Field\n\n**Abstract:** In this study, we present new spectroscopic observations of galaxies at redshifts approximately between 1.5 and 2.0, selected based on their UVJ colors and imaging morphologies, utilizing the VLT/VIMOS on the Very Large Telescope (VLT). Our findings indicate that the majority of these galaxies are early-type stars exhibiting signs of recent star formation activity. The characteristics observed in these galaxies suggest they may serve as progenitors to the powerful elliptical galaxies found in the local universe. This research adds to the growing body of evidence supporting the hypothesis that the majority of massive galaxies evolve through the merger of gas-rich disk systems during the early stages of cosmic history. \n\nMassive stars undergo rapid development over cosmic time, primarily as a result of merging processes involving smaller companion galaxies. It has been suggested that many of the brightest galaxy clusters observed today were formed through significant mergers of two or more gas-rich disks at redshifts between one and three. However, direct observational evidence for these high-redshift events has been challenging to obtain due to the complexities involved in identifying such occurrences. To investigate the physical mechanisms underlying star formation, we conducted deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph. \n\nOur sample comprises approximately 100 galaxies selected based on their ultraviolet J (UVJ) colors, morphological classifications, and apparent magnitudes. A significant number of these galaxies display strong absorption features indicative of active star-forming regions. Their stellar masses range from 10^10 M_sun to 10^11 M_sun. The primary objective of our research was to identify potential candidates for the progenitor populations of local massive elliptical and S0 galaxies. To achieve this, we employed various selection criteria tailored to isolate galaxies with characteristics akin to those observed in nearby massive spheroids. \n\nThis article is published under the Open Access model, adhering to the Creative Commons Attribution License 2.0, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is appropriately cited. \n\n**Keywords:** universe evolution; merger remnants; young elliptical galaxies; CDF-S field.",
        "ori-fast-z-score": 0.07832604499879574,
        "water-fast-z-score": 7.084340391869858,
        "rewrite-fast-z-score": 0.22941573387056177
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A discussion on particle number and quantum indistinguishability .\nAbstract:\nWe discuss the concept of particle number in quantum mechanics, with particular emphasis on its role as an indicator for quantum indistinguishability between particles. We show that this is not always true by considering two examples where it does not hold. The first example involves fermions which are distinguishable but have different spin states; we argue that these should be considered to belong to distinct species rather than being regarded as identical particles. In our second example, we consider bosons which can occupy the same state (e.g., photons) but are nevertheless distinguishable; here too they do not constitute a single species. Finally, we present arguments against identifying particle number with entropy or information content. This article is part of a series of articles intended to introduce some key concepts in physics through informal discussions. It may also serve as a useful guide for students preparing their own presentations. For more detailed explanations see the original papers cited below. \n \n Quantum mechanics has been developed over many decades since its birth at the beginning of 20th century. One of the most important aspects of quantum mechanics is the concept of particle number. Particles are fundamental building blocks of matter and energy. They come in various forms such as electrons, protons, neutrons, atoms, molecules etc.. Each type of particle carries certain properties like charge, mass, spin etc... \n \n What exactly is particle number? How is it related to other physical quantities? Why is it so important? These questions will be answered in this article.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A debate on particle number and quantum indistinguishability . Abstract : We discuss the notion of particle number in particle theory , with particular emphasis on its role as an measure for quantum indistinguishability between particles .We see that this is not always true by examining two examples where it does not hold . The first instance involves fermions which are distinguishable but have different spinning states ; we claim that these should be regarded to belong to distinct species rather than being regarded as identical particles .In our second example , we define bosons which can occupy the same state ( e . g . , photons ) but are nevertheless distinguishable ; here too they do not constitute a single species . Finally , we present arguments against identifying particle size with entropy or information content .This page is part of a string of publications intended to introduce some important concepts in science through informal talks . It might additionally help as a helpful guide for students preparing their own presentations .For more thorough explanations view the original papers cited below . Quantum theory has been used over numerous years since its birth at the beginning of 20th millennium .One of the most important elements of quantum mechanics is the idea of particle number . Particles are fundamental construction blocks of matter and energy .They come in different shapes such as atoms , protons , neutrons , atoms , molecules etc . . Each sort of particle contains certain characteristics like charge , mass , spin etc . . . What really is particle size ? How is it linked to other physical quantities ?Why is it so important ? These questions will be answered in this article .",
        "rewrite_text": "Title: A Debate on Particle Number and Quantum Indistinguishability\n\nAbstract: In this article, we explore the concept of particle number within particle theory, focusing on its significance as a measure of quantum indistinguishability among particles. We challenge the conventional understanding that particle number always correlates with indistinguishability by presenting two illustrative examples where this notion fails. The first example involves fermions that, despite being distinguishable due to differing spin states, should be classified as belonging to distinct species rather than being treated as identical particles. In our second example, we examine bosons, such as photons, which can occupy the same quantum state yet remain distinguishable; similarly, these particles do not form a single species. Furthermore, we argue against equating particle size with entropy or information content, emphasizing the need for a nuanced understanding of these concepts. This discussion is part of a broader series of publications aimed at elucidating key scientific principles through informal discourse, serving as a valuable resource for students preparing their own presentations. The article also addresses fundamental questions regarding the nature of particle size, its relationship with other physical properties, and its overall significance in the realm of quantum mechanics. Since its inception in the early 20th century, quantum theory has profoundly influenced our understanding of the universe, with particle number being a critical element in this framework. Particles, the essential building blocks of matter and energy, exhibit a variety of forms, including atoms, protons, neutrons, and molecules, each characterized by specific attributes such as charge, mass, and spin. Through this examination, we aim to provide clarity on the intricate connections between particle number, indistinguishability, and the broader implications for quantum theory. For more detailed insights, we encourage readers to consult the original papers referenced herein.",
        "ori-fast-z-score": -1.0366421106976322,
        "water-fast-z-score": 6.219852664185793,
        "rewrite-fast-z-score": 0.16666666666666666
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Charmless B decays to a scalar meson and a vector meson .\nAbstract:\nWe study the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial-vector or tensor meson in the framework of QCD factorization with generalized form factors at large recoil.  We find that, although the branching fractions are small due to the helicity suppression, these processes can be used as probes of new physics beyond the Standard Model through their CP asymmetries. \nPACS numbers: 11.15.Tk, 12.38.Qk, 13 .25.Hw \nI. INTRODUCTORY REMAR K\nIn this work we will consider the following two types of charmless hadronic:  B → S V (S = P , A 0 ;V = T 1 )andB → SV(S=P;V=A1). The first type is characterized by one light quark in the final state while the second has no light quarks in it. In both cases there is only one spectator quark which leads to a helicity suppression of the corresponding decay rates. However, they may still serve as useful probes of new physics since their CP-violating asymmetries could be enhanced significantly compared to those of other modes  1  .\nTheoretically, such decays have been studied within various approaches including naive factorization  2  , perturbative QCD  3  , soft-collinear effective theory  4  , and QCD factorization  5  -  8  . It was found that the predictions based on different methods differ substantially among themselves. For example, using naive factorization, Ref.  2  predicted Br(B − →K * 0 π − )/Br(B − →Kπ)=0.27 ±0.04, whereas Refs.  6, 7  obtained values around 0.1−0.2. This discrepancy indicates that more theoretical efforts should be made before drawing any definite conclusion about these decays.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Charmless B decays to a scalar meson and a vector meson . Abstract : We research the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial - vector or tensor meson in the framework of QCD factorization with generalized form factors at large recoil .We see that , although the branching fractions are small owing to the helicity suppression , these mechanisms can be used as probes of new dynamics beyond the Standard Model through their CP asymmetries . PACS codes : 11 . 15 . Tk , 12 . 38 . Qk , 13 . 25 . Hw I .INTRODUCTORY REMAR K In this study we will investigate the following two forms of charmless hadronic : B → S V ( S = P , A 0 ; V = T 1 ) andB → SV ( S = P ; V = A1 ) . The first variety is characterized by one dark quark in the last state while the second has no light quarks in it .In both cases there is only one spectator quark which results to a helicity suppression of the resulting degradation rates . However , they may still provide as helpful probes of new theory since their CP - breaking asymmetries may be enhanced considerably compared to those of other modes 1 .Theoretically , such decays have been studied within various approaches including naive factorization 2 , perturbative QCD 3 , soft - collinear effective theory 4 , and QCD factorization 5 - 8 . It was shown that the estimates based on various methods varies dramatically among themselves .For instance , using naive factorization , Ref . 2 observed Br ( B − →K * 0 π − ) / Br ( B − →Kπ ) = 0 . 27 ±0 . 04 , whereas Refs .6 , 7 obtained values around 0 . 1−0 . 2 . This discrepancy implies that more theoretical efforts should be made before drew any explicit conclusion about these decays .",
        "rewrite_text": "In this article, we explore the decay amplitudes associated with charmless hadronic B decays into a scalar meson and either an axial-vector or tensor meson, utilizing the framework of Quantum Chromodynamics (QCD) factorization with generalized form factors applicable at large recoil. Our findings indicate that, despite the small branching fractions resulting from helicity suppression, these decay processes can serve as valuable probes for uncovering new dynamics that extend beyond the Standard Model, particularly through their associated CP asymmetries. \n\nWe focus on two specific forms of charmless hadronic decays: B → SV (where S represents a pseudoscalar or axial-vector meson, and V denotes a tensor meson) and B → SV (where S is a pseudoscalar meson and V is an axial-vector meson). The first category is characterized by the presence of one dark quark in the final state, while the second category lacks light quarks. In both scenarios, the presence of a single spectator quark leads to helicity suppression, which affects the decay rates. Nonetheless, these decays may still provide significant insights into new theoretical frameworks, as their CP-violating asymmetries could be substantially enhanced compared to other decay modes.\n\nTheoretical investigations into these decays have been conducted using various methodologies, including naive factorization, perturbative QCD, soft-collinear effective theory, and QCD factorization. Notably, the estimates derived from these different approaches exhibit considerable variation. For example, naive factorization predicts a branching ratio of Br(B− → K*0 π−) / Br(B− → Kπ) = 0.27 ± 0.04, while other studies have reported values in the range of 0.1 to 0.2. This discrepancy underscores the necessity for further theoretical work to achieve a more definitive understanding of these decay processes. \n\nIn summary, our research highlights the potential of charmless B decays as a means to probe new physics, while also emphasizing the need for improved theoretical consistency in the predictions of these decay amplitudes.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 1.7417271443536015
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Suppressed radio emission in supercluster galaxies: enhanced ram pressure in merging clusters? .\nAbstract:\nWe report on the detection of suppressed radio emission in two interacting galaxy clusters, Abell 2146 and Abell 2390 (z = 0.23). The cluster pair is part of an extended X-ray filamentary structure that extends over ~30 Mpc along the line-of-sight. We find that the fraction of radio-detected galaxies decreases with increasing projected distance to the nearest cluster center for both clusters individually as well as when combining all galaxies within the entire filamentary structure. This trend cannot be explained by differences in stellar mass or SFR between galaxies located at different distances from their host cluster centers. Instead we argue that it can be understood if the gas density increases towards the cluster cores due to gravitational compression and/or shock heating during cluster mergers. In this case, the increased ram pressure would suppress star formation activity in galaxies residing close to the cluster core. Our results are consistent with previous studies showing that the fraction of star-forming galaxies decreases rapidly with decreasing cluster-centric radius.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Suppressed radio emission in supercluster galaxies : enhanced ram pressure in merging galaxies ? .Abstract : We report on the observation of suppressed radio emission in two interacting galaxy regions , Abell 2146 and Abell 2390 ( z = 0 . 23 ) . The cluster couple is part of an extended X - ray filamentary structure that extends over ~ 30 Mpc along the line - of - view .We see that the fraction of radio - detected galaxies decreases with increasing projected distance to the nearest cluster center for both clusters separately as well as when combining all galaxies within the entire filamentary structure . This trend cannot be described by differences in stellar mass or SFR between nuclei located at different distances from their host cluster sites .Instead we feel that it can be understood if the gas density increases towards the cluster cores due to gravitational fusion and / or shock heating during cluster mergers . In this example , the increased ram tension may reduce galaxy formation activity in galaxies residing nearest to the cluster core .Our results are compatible with previous research indicating that the fraction of galaxy - creating stars reduces rapidly with decreasing cluster - centric radius .",
        "rewrite_text": "We present findings on the phenomenon of suppressed radio emission observed in two interacting galaxy regions, specifically Abell 2146 and Abell 2390, located at a redshift of z = 0.23. These galaxy clusters are part of an extensive X-ray filamentary structure that spans approximately 30 Mpc along the line of sight. Our analysis reveals a notable trend: the proportion of galaxies detected in radio wavelengths diminishes as the projected distance from the nearest cluster center increases. This pattern is consistent for both clusters individually and when considering all galaxies within the entire filamentary structure. \n\nCrucially, this observed decline in radio detection cannot be attributed to variations in stellar mass or star formation rates (SFR) among galaxies situated at different distances from their respective cluster centers. Instead, we propose that this trend can be explained by an increase in gas density towards the cores of the clusters, which may result from gravitational interactions and shock heating associated with cluster mergers. In this context, the heightened ram pressure experienced by galaxies located closer to the cluster core could inhibit their star formation activity. \n\nOur findings align with prior studies that suggest a rapid decrease in the fraction of star-forming galaxies as one moves closer to the center of a cluster. This research contributes to the understanding of how environmental factors, particularly those related to cluster dynamics, influence galaxy formation and evolution. The implications of these results are significant for the broader field of astrophysics, as they highlight the complex interplay between galaxy interactions and the surrounding cluster environment in shaping the characteristics of galaxies within superclusters.",
        "ori-fast-z-score": -0.8783100656536799,
        "water-fast-z-score": 4.350314200705486,
        "rewrite-fast-z-score": -0.6024640760767093
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High order convergent multigrid methods on domains containing holes for black hole initial data .\nAbstract:\nWe present new numerical schemes to solve the Einstein equations in Kerr-Schild coordinates with multiple black holes and an outer boundary, which are based on high order accurate finite difference discretizations combined with efficient solvers using multigrid techniques. The main difficulty is that the computational domain contains several disconnected components (holes) where no grid points can be placed due to the presence of singularities at the locations of the black holes. We show how this problem can be overcome by introducing ghost points outside each component and applying appropriate boundary conditions there. In addition we use a special treatment near the horizon of each black hole to avoid problems related to coordinate singularities. Finally, we demonstrate our method by computing stationary axisymmetric solutions corresponding to two equal mass black holes located along the z-axis. Our results agree very well with previous computations obtained with other codes. This work was supported by NSF grant PHY-0555611.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High order convergent multigrid techniques on domains featuring holes for black hole original data . Abstract : We introduce novel numerical schemes to solve the Einstein equations in Kerr - Schild coordinates with many black holes and an outer boundary , which are based on high order accurate finite difference discretizations combined with fast solvers using multigrid techniques .The main problems is that the computational domain contains multiple disconnected components ( holes ) where no grid points can be placed due to the presence of singularities at the places of the dark holes . We see how this question can be overcome by putting shadow points outside each portion and using appropriate boundary constraints there .In addition we utilize a unique treatment near the horizon of each dark hole to eliminate problems related to coordinate singularities . Finally , we prove our technique by modeling stationary axisymmetric solutions analogous to two equal mass black holes located along the z - axis .Our results agree very best with previous computations achieved with other codes . This project was supported by NSF grant PHY - 0555611 .",
        "rewrite_text": "Title: High-Order Convergent Multigrid Techniques on Domains Featuring Holes for Black Hole Initial Data\n\nAbstract: In this study, we present innovative numerical methods designed to address the Einstein equations in Kerr-Schild coordinates, specifically in scenarios involving multiple black holes and an outer boundary. Our approach employs high-order accurate finite difference discretizations, which are integrated with efficient solvers utilizing multigrid techniques. A significant challenge arises from the computational domain, which consists of several disconnected regions, or \"holes,\" where grid points cannot be established due to the singularities associated with the black holes. We propose a solution to this issue by introducing shadow points situated outside each hole, accompanied by suitable boundary conditions to ensure stability and accuracy. Furthermore, we implement a specialized treatment near the event horizon of each black hole to mitigate complications stemming from coordinate singularities. To validate our methodology, we model stationary axisymmetric solutions that resemble two equal-mass black holes aligned along the z-axis. Our findings demonstrate a high degree of agreement with previous results obtained from alternative computational codes, underscoring the effectiveness of our proposed techniques. This research was made possible through the support of NSF grant PHY-0555611, highlighting the importance of funding in advancing scientific inquiry in the field of gravitational physics.",
        "ori-fast-z-score": -0.6965260331469925,
        "water-fast-z-score": 5.148767223478707,
        "rewrite-fast-z-score": 0.658504607868518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  In-medium effects on particle production in heavy ion collisions .\nAbstract:\nWe present the results for charged hadron multiplicity density and transverse momentum fluctuations obtained by using the AMPT model with string melting mechanism at RHIC energies. The effect of initial parton distribution is studied by varying the initial temperature T0, which controls the initial entropy content of the system. We find that both the mean value and fluctuation of the multiplicity density increase as we decrease the initial temperature T0. This indicates that the initial state fluctuations play an important role in determining the final observables. It has been observed experimentally that the ratio of variance to mean square (V /M) increases with decreasing beam energy. In our study also it is found that V /M decreases with increasing initial temperature T0. However, this trend changes when we include the contribution due to resonance decays. Finally, we have calculated the scaled variances S2/S1 and S3/S2 for different values of T0.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : In - medium impacts on particle production in heavy ion collisions . Abstract : We present the results for charged hadron multiplicity density and transverse energy fluctuations derived by using the AMPT theory with string melting system at RHIC intensity .The impact of initial parton distribution is studied by varying the initial temperature T0 , which changes the early entropy concentration of the system . We see that both the mean value and fluctuation of the multiplicity density increase as we decrease the initial temperature T0 .This implies that the first state fluctuations play an important role in determining the finished observables . It has been observed experimentally that the proportion of variance to mean square ( V / M ) changes with decreasing beam energy .In our research also it is found that V / M decreases with increasing initial temperature T0 . However , this decline changes when we include the contribution owing to resonance decays .Finally , we have adjusted the scaled variances S2 / S1 and S3 / S2 for different values of T0 .",
        "rewrite_text": "We present a comprehensive analysis of charged hadron multiplicity density and transverse energy fluctuations in heavy ion collisions, utilizing the AMPT (A Multi-Phase Transport) model with a string melting framework at the intensity levels achieved at the Relativistic Heavy Ion Collider (RHIC). Our study investigates the influence of the initial parton distribution by systematically varying the initial temperature, T0, which in turn affects the early entropy concentration within the system. Our findings reveal that both the average multiplicity density and its fluctuations exhibit an increase as the initial temperature T0 is lowered. This observation underscores the significant role that initial state fluctuations play in shaping the final observables of the collision outcomes.\n\nAdditionally, our research aligns with experimental observations indicating that the ratio of variance to mean square (V/M) demonstrates a dependency on beam energy, specifically decreasing as the beam energy diminishes. Our results corroborate this trend, showing that V/M decreases with an increase in the initial temperature T0. Notably, this decline in the V/M ratio is altered when we account for contributions from resonance decays, highlighting the complexity of particle production mechanisms in these high-energy collisions.\n\nTo further elucidate the dynamics at play, we have also adjusted the scaled variances S2/S1 and S3/S2 across various initial temperature values. This comprehensive approach allows us to better understand the intricate interplay between temperature, fluctuations, and particle production in the context of heavy ion collisions. Our findings contribute to the ongoing discourse on the fundamental processes governing the behavior of matter under extreme conditions, paving the way for future investigations in high-energy nuclear physics.",
        "ori-fast-z-score": -1.5652475842498528,
        "water-fast-z-score": 3.2222222222222223,
        "rewrite-fast-z-score": 0.6311687442672026
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The discrete dipole approximation for simulation of light scattering by particles much larger than the wavelength .\nAbstract:\nThe Discrete Dipole Approximation (DDA) is used to simulate light scattering by large spherical particles in this study. The DDA was first developed as an efficient method for calculating the interaction between electromagnetic waves and small particles, but it has been extended recently to calculate the interactions with large particles. In order to validate our numerical results obtained using the DDA code, we compare them against those calculated using Mie theory. We find that both methods agree well when the size parameter x = 2πa/λ < 100 where λ is the incident wavelength and a is the radius of the particle. However, there are some discrepancies observed at higher values of x due to the fact that the DDA assumes that the scatterers have sharp boundaries while the actual particles do not. Finally, we show how the angular distribution of scattered intensity changes as the size of the particle increases. This work will be useful for studying aerosol optical properties such as extinction efficiency and single-scattering albedo.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The discrete dipole approximation for simulation of light scattering by particles much larger than the frequency . Abstract : The Discrete Dipole Approximation ( DDA ) is utilized to simulate light scattering by large spherical objects in this study .The DDA was first developed as an efficient model for calculating the interaction between electromagnetic signals and tiny particles , but it has been extended recently to estimate the interactions with large particles . In order to validate our numerical findings obtained using the DDA code , we compare them against those estimated using Mie physics .We see that both approaches agree well when the height variable x = 2πa / λ < 100 where ν is the incident wavelength and a is the radius of the particle . However , there are some discrepancies reported at higher values of x thanks to the fact that the DDA assumes that the scatterers have jagged boundaries while the actual particles do not .Finally , we study how the angular distribution of scattered intensity increases as the size of the particle increases . This research will be suitable for studying aerosol laser properties such as extinction efficiency and single - absorption albedo .",
        "rewrite_text": "**Title:** The Discrete Dipole Approximation for Simulating Light Scattering by Particles Much Larger than the Wavelength\n\n**Abstract:** This study explores the application of the Discrete Dipole Approximation (DDA) for simulating light scattering by large spherical particles. Originally developed to model the interaction between electromagnetic waves and small particles, the DDA has recently been adapted to accommodate larger particles, which presents new opportunities for understanding light scattering phenomena. To validate the numerical results obtained through our DDA implementation, we conducted a comparative analysis with results derived from Mie theory, a well-established method for calculating light scattering by spherical particles. Our findings indicate a strong correlation between the two methods when the dimensionless parameter x, defined as 2πa/λ (where a represents the particle radius and λ is the incident wavelength), is less than 100. However, discrepancies arise at higher values of x, primarily due to the DDA's assumption of jagged particle boundaries, which contrasts with the smooth surfaces of actual particles. Additionally, we investigate the impact of particle size on the angular distribution of scattered intensity, revealing that larger particles tend to produce a more pronounced scattering pattern. This research has significant implications for the study of aerosol laser properties, particularly in terms of extinction efficiency and single-scattering albedo. The insights gained from this work contribute to a deeper understanding of light-particle interactions, which is essential for various applications in atmospheric science and optical engineering.",
        "ori-fast-z-score": -0.4216370213557839,
        "water-fast-z-score": 5.421374765483944,
        "rewrite-fast-z-score": 0.6363636363636364
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamical models and the phase ordering kinetics of the s=1 spinor condensate .\nAbstract:\nWe study the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension, focusing on its relaxation to equilibrium after being quenched across the superfluid-Mott insulator transition. We show that this system exhibits universal behavior at late times which is characterized by power-law decaying correlations and algebraic growth of entanglement entropy. The exponents are determined analytically using a mapping onto a classical statistical mechanics problem for a driven diffusive system. This work was supported by NSF grant PHY-0960291 (M.S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A.K.). \nI. INTRODUCTORY REMARkS\n\nThe recent experimental realization of quantum degenerate gases has opened up new avenues towards understanding strongly correlated many-body systems  1  . In particular, ultracold atomic gases have been used as model systems to explore phenomena such as fermionization  2  , supersolidity  3  , and Mott-insulating states  4  .\nIn this article we consider a particularly interesting class of experiments where the properties of these systems can be probed through their response to sudden changes in parameters  5  . For example, if the strength of inter-particle repulsion or density of particles is suddenly changed then it takes some time before the system reaches thermal equilibrium  6  . During this nonequilibrium evolution, the system may exhibit novel features like dynamical scaling  7, 8  and non-thermal fixed points  9  . These effects are not only important for our fundamental understanding of quantum matter but also provide useful insights into possible routes to realizing novel phases of matter  10  .\nRecently there has been considerable interest in studying the nonequilibrium dynamics of bosonic systems  11  . A particularly well studied case is when the initial state corresponds to a highly excited state above the ground state  12  . It turns out that even though the initial state is far away from equilibrium, the system relaxes to a steady state described by a Gibbs ensemble  13  . However, if the initial state is prepared deep inside the ordered phase, then the system does not",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamical models and the phase ordering kinetics of the s = 1 spinor condensate . Abstract : We research the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension , concentrating on its relaxation to equilibrium after being quenched across the superfluid - Mott insulator transition .We see that this scheme exhibits universal behavior at late times which is characterized by power - law decaying correlations and algebraic growth of entanglement entropy . The exponents are chosen analytically taking a mapping onto a traditional statistical mechanics problem for a driven diffusive system .This project was supported by NSF grant PHY - 0960291 ( M . S . ) and DOE funds DE - FG03 - 92 - ER40701 and DE - SC0012704 ( A . K . ) .I . INTRODUCTORY REMARkS The recent experimental realization of quantum degenerate gases has opened up new avenues towards studying strongly interacting large - bodies systems 1 .In particular , ultracold atomic gases have been used as model structures to examine processes such as fermionization 2 , supersolidity 3 , and Mott - insulating states 4 . In this article we imagine a particularly exciting group of studies where the properties of these systems can be probed through their response to unexpected changes in parameters 5 .For instance , if the strength of inter - atom repulsion or density of molecules is suddenly changed then it takes some time before the system reaches heat equilibrium 6 . During this nonequilibrium evolution , the system might exhibit new characteristics like dynamical scaling 7 , 8 and non - thermal fixed points 9 .These effects are not only important for our vital understanding of quantum matter but also make helpful understanding into possible routes to realizing new phases of matter 10 . Recently there has been substantial interest in investigating the nonequilibrium dynamics of bosonic structures 11 .A notably well discussed case is when the initial state corresponds to a highly excited state above the ground state 12 . It turns out that even though the initial state is far back from equilibrium , the system relaxes to a steady state described by a Gibbs ensemble 13 .However , if the first state is prepared deep inside the ordered phase , then the scheme does not",
        "rewrite_text": "Title: Dynamical Models and Phase Ordering Kinetics of the s = 1 Spinor Condensate\n\nAbstract: This study investigates the dynamics of an interacting Bose gas characterized by repulsive contact interactions in one dimension, with a specific focus on its relaxation process towards equilibrium following a quench across the superfluid-Mott insulator transition. Our findings reveal that this system exhibits universal behavior at late times, which is marked by power-law decay of correlations and algebraic growth of entanglement entropy. The exponents governing these behaviors are derived analytically through a mapping to a conventional statistical mechanics problem related to a driven diffusive system. This research was supported by NSF grant PHY-0960291 (M.S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A.K.).\n\nI. INTRODUCTORY REMARKS: The advent of quantum degenerate gases has significantly expanded the scope of research into strongly interacting many-body systems. Ultracold atomic gases, in particular, serve as effective platforms for exploring phenomena such as fermionization, supersolidity, and Mott insulating states. In this article, we delve into a fascinating area of study where the properties of these systems are examined in response to abrupt changes in external parameters. For example, a sudden alteration in the inter-atom repulsion strength or molecular density leads to a transient period before the system attains thermal equilibrium. During this nonequilibrium evolution, the system may display novel features such as dynamical scaling and the emergence of non-thermal fixed points. These phenomena are crucial not only for enhancing our understanding of quantum matter but also for providing insights into potential pathways for realizing new phases of matter. Recent research has increasingly focused on the nonequilibrium dynamics of bosonic systems, particularly in scenarios where the initial state is significantly excited above the ground state. Interestingly, despite the initial state being far from equilibrium, the system tends to relax into a steady state that can be described by a Gibbs ensemble. However, when the initial state is deeply entrenched within the ordered phase, the dynamics exhibit different characteristics.",
        "ori-fast-z-score": 0.07980868844676221,
        "water-fast-z-score": 6.841792643911278,
        "rewrite-fast-z-score": 1.4795908857482156
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observational bounds on the cosmic radiation density .\nAbstract:\nWe present new observational constraints on the cosmic ray (CR) energy density and its evolution with redshift, based on gamma-ray observations by Fermi/LAT in the range 0 < z < 1.5. We find that CRs contribute at most 10% to the total pressure budget of the universe at redshifts below 2. This upper limit is consistent with theoretical expectations for the contribution of CRs accelerated by supernovae. The results are also compatible with previous measurements using radio data. These limits can be used as priors when modeling the effects of CRs on cosmological observables such as galaxy clustering or weak lensing. Cosmic rays (CRs), charged particles which fill space uniformly over large volumes, have been observed throughout our Galaxy and beyond. They play an important role in many astrophysical phenomena including galactic winds, star formation, and possibly even the acceleration of ultra-high-energy cosmic rays  1  . However, their origin remains unknown  2  .\nIn this work we use gamma-ray observations made by the Large Area Telescope (LAT) aboard the Fermi satellite  3  , to place tight constraints on the amount of CRs contributing to the overall pressure budget of the Universe  4  . In particular, we consider two different models for the CR distribution function f(p,z). First, we assume that it follows a power law spectrum dN/dE ~ E^{-alpha} between energies Emin = 10 GeV and Emax = 100 TeV; secondly, we adopt a broken power-law model where the spectral index changes from alpha1 = -2.2 to alpha2 = -3 above some break energy Eb = 50 GeV. For both cases, we fix the normalization factor A by requiring that the integral of f(p,z) over all momenta equals unity. \nThe resulting CR distributions are shown in Figure 1 . \nTo calculate the effect of these CR populations on the expansion history of the universe, we solve numerically the coupled system of equations describing the time-evolution of the background...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observational bounds on the cosmic rays flux . Abstract : We introduce novel observational restrictions on the cosmic ray ( CR ) energy density and its expansion with redshift , built on gamma - ray observations by Fermi / LAT in the range 0 < z < 1 . 5 .We see that CRs contribute at most 10 % to the total pressure budget of the universe at redshifts below 2 . This upper maximum is compatible with theoretical expectations for the contribution of CRs accelerated by supernovae .The results are also compatible with previous measurements used radio data . These limits can be used as priors when modeling the effects of CRs on cosmological observables such as galaxy clustering or strong lensing .Cosmic rays ( CRs ) , charged particles which fill space uniformly over large quantities , have been observed throughout our Galaxy and beyond . They play an important role in many astrophysical processes including galactic winds , star formation , and maybe even the acceleration of ultra - low - energy cosmic rays 1 .However , their source remains unidentified 2 . In this project we using gamma - ray observations made by the Large Area Telescope ( LAT ) aboard the Fermi satellite 3 , to place tight limitations on the proportion of CRs contributing to the overall pressure budget of the Universe 4 .In particular , we define two different models for the CR distribution function f ( p , z ) . First , we suppose that it takes a power law spectrum dN / dE ~ E ^ { - alpha } between frequencies Emin = 10 GeV and Emax = 100 TeV ; secondly , we choose a broken power - law description where the spectral index shifts from alpha1 = - 2 . 2 to alpha2 = - 3 above some broke energy Eb = 50 GeV .For both cases , we solve the normalization factor A by requiring that the integral of f ( p , z ) over all momenta equals unity . The resulting CR distributions are shown in Figure 1 .To estimate the impact of these CR populations on the expansion history of the universe , we solve numerically the coupled system of equations explaining the period - progression of the background . . .",
        "rewrite_text": "**Title: Observational Constraints on Cosmic Ray Flux**\n\n**Abstract:** In this study, we present new observational constraints on the energy density of cosmic rays (CRs) and their evolution with redshift, utilizing gamma-ray data from the Fermi Large Area Telescope (LAT) within the redshift range of 0 < z < 1.5. Our findings indicate that CRs contribute no more than 10% to the total pressure budget of the universe at redshifts below 2, aligning with theoretical predictions regarding the contribution from CRs generated by supernovae. These results are consistent with earlier measurements derived from radio data, reinforcing the reliability of our constraints. The established limits can serve as valuable priors in models that examine the influence of CRs on various cosmological phenomena, including galaxy clustering and strong gravitational lensing.\n\nCosmic rays, which are charged particles that permeate space uniformly across vast distances, have been detected within our galaxy and in extragalactic environments. They are integral to numerous astrophysical processes, such as driving galactic winds, influencing star formation, and potentially contributing to the acceleration of ultra-low-energy cosmic rays. Despite their significance, the precise origins of these particles remain elusive.\n\nIn our research, we employ gamma-ray observations from the Fermi LAT to impose stringent limits on the fraction of CRs that contribute to the universe's overall pressure budget. We explore two distinct models for the CR distribution function, f(p, z). The first model assumes a power-law spectrum characterized by dN/dE ~ E^(-α) within the energy range of 10 GeV to 100 TeV. The second model adopts a broken power-law approach, where the spectral index transitions from α1 = -2.2 to α2 = -3 at a break energy of Eb = 50 GeV. For both models, we determine the normalization factor A by ensuring that the integral of f(p, z) across all momenta equals one. The resulting CR distributions are illustrated in Figure 1. To assess the implications of these CR populations on the universe's expansion history, we numerically solve the coupled equations governing the evolution of the cosmological background.",
        "ori-fast-z-score": 0.4931969619160719,
        "water-fast-z-score": 6.695271963607354,
        "rewrite-fast-z-score": 2.090605025017727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An XMM-Newton study of Hyper-Luminous Infrared Galaxies .\nAbstract:\nWe present an analysis of the X-ray properties of a sample of 12 hyper-luminous infrared galaxies (HLIRGs) observed with XMM-Newton, using data obtained in AO-1 and AO-2. The HLIRG sample is selected to have L(8-1000um)>10^12L_sun , where L(8-1000um), is derived by integrating over the best-fit SEDs for each source. We find that all sources are detected at >5 sigma significance in the 0.3-10 keV band; however only two objects show evidence for significant absorption above Galactic levels. For these two absorbed systems we derive column densities NH = 1.7 x 10^23 cm^{-2} and 2.1 x 10^22 cm^{-2} respectively. Using the hardness ratio HR=H-S/H+S, where H and S represent counts in the 3-7keV and 0.3-2keV bands respectively, we find no correlation between HR and either luminosity or redshift. This suggests that there may be little evolution in the intrinsic spectral shape of this population out to z=2.6.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An XMM - Newton experiment of Hyper - Luminous Infrared Galaxies . Abstract : We present an assessment of the X - ray characteristics of a sample of 12 hyper - luminous infrared galaxies ( HLIRGs ) detected with XMM - Newton , using data derived in AO - 1 and AO - 2 .The HLIRG sample is selected to have L ( 8 - 1000um ) > 10 ^ 12L _ sunshine , where L ( 8 - 1000um ) , is calculated by combining over the best - fitting SEDs for each source . We see that all sources are detected at > 5 sigma significance in the 0 . 3 - 10 keV band ; however only two objects show proof for significant absorption above Galactic concentrations .For these two absorbed systems we derive row densities NH = 1 . 7 x 10 ^ 23 centimetres ^ { - 2 } and 2 . 1 x 10 ^ 22 mm ^ { - 2 } respectively . Using the hardness factor HR = H - S / H + S , where H and S refer counts in the 3 - 7keV and 0 . 3 - 2keV bands respectively , we find no correlation between HR and either luminosity or redshift .This implies that there may be little development in the intrinsic spectral structure of this community out to z = 2 . 6 .",
        "rewrite_text": "We provide a comprehensive analysis of the X-ray properties of a selected group of 12 hyper-luminous infrared galaxies (HLIRGs) observed using the XMM-Newton satellite, utilizing data from the first and second observation cycles (AO-1 and AO-2). Our sample is specifically chosen based on the criterion that their infrared luminosity, L(8-1000 µm), exceeds 10^12 L☉, with values calculated from the best-fitting spectral energy distributions (SEDs) for each galaxy. All 12 HLIRGs were detected with a significance greater than 5 sigma in the 0.3 - 10 keV energy range. Notably, only two of these galaxies exhibited evidence of significant X-ray absorption beyond the levels expected from the Galactic foreground. For these two absorbed sources, we determined column densities of NH = 1.7 x 10^23 cm^-2 and 2.1 x 10^22 cm^-2, respectively. Furthermore, we employed the hardness ratio (HR), defined as HR = (H - S) / (H + S), where H and S represent the counts in the 3 - 7 keV and 0.3 - 2 keV bands, respectively. Our analysis revealed no significant correlation between the hardness ratio and either the luminosity or redshift of the HLIRGs, suggesting that there is minimal evolution in the intrinsic spectral characteristics of this population up to a redshift of z = 2.6. This study contributes to our understanding of the X-ray emission mechanisms in HLIRGs and their potential implications for the evolution of massive galaxies in the early universe.",
        "ori-fast-z-score": 0.12216944435630522,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": -1.5724272550828775
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Structural distortions and model Hamiltonian parameters: from LSDA to a tight-binding description of LaMnO_3 .\nAbstract:\nWe present the results of first-principles calculations for structural, electronic, magnetic properties of LaMnO3 in its rhombohedral phase (R3c). We show that the local spin density approximation (LSDA) fails to reproduce correctly both the lattice constants and the band gap energy. The latter is underestimated by more than one order of magnitude as compared with experiment. In contrast, our self-consistent full-potential linearized augmented plane wave method gives good agreement between calculated and experimental values of these quantities. \n \n To understand better the origin of this discrepancy we have performed additional calculations using an effective tight-binding approach based on Wannier functions obtained within the framework of the LSDA+U formalism. Our analysis shows that the main reason why the LSDA fails to describe properly the electronic structure of LaMnO3 is due to strong hybridization effects which are not taken into account adequately within the standard LSDA scheme.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Structural distortions and description Hamiltonian properties : from LSDA to a tight - binding model of LaMnO _ 3 . Abstract : We report the results of first - principles experiments for structural , electronic , magnetic properties of LaMnO3 in its rhombohedral phase ( R3c ) .We see that the local spin density algorithm ( LSDA ) fails to reproduce correctly both the crystal constants and the band gap energy . The latter is underestimated by more than one order of magnitude as compared with observation .In comparison , our self - consistent full - potential linearized augmented plane wave method gives excellent agreement between calculated and experimental values of these quantities . To understand better the origin of this discrepancy we have done additional calculations using an efficient tight - binding approach using on Wannier functions obtained within the framework of the LSDA + U formalism .Our study shows that the main explanation why the LSDA fails to explain adequately the electronic structure of LaMnO3 is due to powerful hybridization factors which are not took into consideration appropriately within the standard LSDA plan .",
        "rewrite_text": "Title: Structural Distortions and Hamiltonian Properties: Transitioning from LSDA to a Tight-Binding Model of LaMnO3\n\nAbstract: In this study, we present findings from first-principles calculations that investigate the structural, electronic, and magnetic characteristics of LaMnO3 in its rhombohedral phase (R3c). Our analysis reveals significant shortcomings in the local spin density approximation (LSDA), particularly in its inability to accurately predict both the crystal lattice parameters and the band gap energy, which is underestimated by over an order of magnitude compared to experimental observations. In contrast, our application of the self-consistent full-potential linearized augmented plane wave (FLAPW) method yields results that align closely with experimental data for these properties. To further elucidate the reasons behind the discrepancies observed with LSDA, we conducted supplementary calculations utilizing a tight-binding model based on Wannier functions derived from the LSDA + U approach. Our findings indicate that the primary reason for LSDA's inadequacy in capturing the electronic structure of LaMnO3 lies in its failure to account for significant hybridization effects that are crucial for an accurate description of the material's properties. This work not only highlights the limitations of traditional LSDA in complex materials but also demonstrates the effectiveness of advanced computational techniques in providing a more reliable framework for understanding the intricate electronic behavior of LaMnO3. Through this research, we aim to contribute to the ongoing discourse on the electronic structure of transition metal oxides and the methodologies employed to study them, paving the way for future investigations into similar materials.",
        "ori-fast-z-score": -1.2939932784412609,
        "water-fast-z-score": 3.7523938719322816,
        "rewrite-fast-z-score": -0.09325048082403138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Plasma Modes Along the Open Field Lines of a Neutron Star .\nAbstract:\nWe study plasma modes along open field lines in neutron stars using kinetic theory and numerical simulations. We find that there are two types of instabilities, one driven by parallel electric fields (the electron firehose instability) and another driven by perpendicular magnetic fields (the mirror instability). The growth rates for these instabilities depend on the local density gradient scale length as well as the strength of the background magnetic field. For typical parameters expected near the polar cap region of a pulsar we show that both instabilities can grow rapidly enough to be important for particle acceleration processes at the stellar surface. \n \n Keywords: Plasma physics; Kinetic theory; Instability; Pulsar emission mechanism; Acceleration mechanisms; Mirror instability; Firehose instability; Polar cap acceleration; Magnetosphere; Neutrino emission; Magnetic reconnection; Force-free magnetohydrodynamics; Astrophysics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Plasma Modes Along the Open Field Lines of a Neutron Star . Abstract : We research plasma mechanisms along open field lines in neutron stars using kinetic theory and mathematical simulations .We see that there are two forms of instabilities , one driven by perpendicular electric fields ( the electron firehose tension ) and another driven by perpendicular magnetic fields ( the mirror instability ) . The growth rates for these instabilities depend on the local density gradient scale length as well as the strength of the background magnetic force .For typical characteristics anticipated near the polar cap region of a pulsar we find that both instabilities can develop rapidly sufficiently to be crucial for particle acceleration processes at the stars surface . Keywords : Plasma mechanics ; Kinetic physics ; Instability ; Pulsar radiation mechanism ; Acceleration mechanisms ; Mirror instability ; Firehose instability ; Polar cap acceleration ; Magnetosphere ; Neutrino emission ; Magnetic reconnection ; Force - free magnetohydrodynamics ; Astrophysics",
        "rewrite_text": "In this study, we investigate the plasma dynamics occurring along the open field lines of neutron stars, employing kinetic theory and advanced mathematical simulations. Our analysis reveals the presence of two distinct types of instabilities: one associated with perpendicular electric fields, known as the electron firehose instability, and another linked to perpendicular magnetic fields, referred to as the mirror instability. The growth rates of these instabilities are influenced by the local density gradient scale length and the intensity of the background magnetic field. \n\nFocusing on the polar cap region of pulsars, we find that both instabilities can develop at a rapid pace, which is significant for the mechanisms of particle acceleration occurring at the surface of these celestial bodies. The implications of our findings extend to understanding the radiation mechanisms of pulsars, as well as the broader astrophysical processes involving magnetospheres and neutrino emissions. \n\nOur research contributes to the field of plasma mechanics and kinetic physics by elucidating the conditions under which these instabilities arise and their potential impact on the energetic phenomena observed in neutron stars. The study also highlights the relevance of force-free magnetohydrodynamics in the context of neutron star magnetospheres, paving the way for further exploration of magnetic reconnection events and their role in pulsar emissions. Overall, our work enhances the understanding of the intricate interplay between plasma behavior and magnetic fields in extreme astrophysical environments. \n\nKeywords: Plasma mechanics; Kinetic physics; Instability; Pulsar radiation mechanism; Acceleration mechanisms; Mirror instability; Firehose instability; Polar cap acceleration; Magnetosphere; Neutrino emission; Magnetic reconnection; Force-free magnetohydrodynamics; Astrophysics.",
        "ori-fast-z-score": -1.7556172079419585,
        "water-fast-z-score": 2.789943329851663,
        "rewrite-fast-z-score": 2.694079530401624
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Dynamical State fo the Starless Dense Core FeSt 1-457: A Pulsating Globule? .\nAbstract:\nWe present new near-infrared (NIR) and millimeter-wave observations of the starless dense core FeSt 1-457, which is located in the Taurus molecular cloud complex at a distance of 140 pc. The NIR data were obtained with the Subaru telescope using the SofI instrument on 2005 May 24-25 UT. We detected two sources within the central 0.5 arcmin region; one source was found to be associated with an infrared dark cloud (IRDC), while another source was not. Both sources are embedded deeply inside the dusty envelope surrounding the dense core. In addition, we observed this object simultaneously with the Nobeyama 45 m radio telescope at 1 mm wavelength during the same night as our NIR observation. No significant emission line features were seen in either spectrum. Using these observational results, we discuss possible scenarios for the formation of stars in such a young dense core.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Dynamical State fo the Starless Dense Core FeSt 1-457: A Pulsating Globule?.Abstract : We report new near - infrared ( NIR ) and millimeter - wave images of the starless dense core FeSt 1 - 457 , which is situated in the Taurus molecular mist complex at a distance of 140 pc . The NIR data were obtained with the Subaru observatory using the SofI instrument on 2005 May 24 - 25 UT .We observed two sources within the inner 0 . 5 arcmin region ; one source was reported to be involved with an infrared shadow cloud ( IRDC ) , while another source was not . Both sources are lodged deeply inside the dusty envelope surrounding the dense core .In addition , we studied this body simultaneously with the Nobeyama 45 m radio telescope at 1 mm frequency during the same night as our NIR observation . No notable emission line characteristics were witnessed in either spectrum .Using these observational results , we explain possible strategies for the formation of stars in such a young dense core .",
        "rewrite_text": "Title: The Dynamical State of the Starless Dense Core FeSt 1-457: A Pulsating Globule?\n\nAbstract: In this study, we present new near-infrared (NIR) and millimeter-wave imaging of the starless dense core FeSt 1-457, located within the Taurus molecular cloud complex at a distance of approximately 140 parsecs. The NIR observations were conducted at the Subaru Observatory using the SofI instrument on May 24-25, 2005. Our imaging revealed two distinct sources within a 0.5 arcminute radius of the core; one of these sources is associated with an infrared dark cloud (IRDC), while the other is not. Both sources are embedded within the dense, dusty envelope that surrounds the core, indicating their potential significance in the core's dynamical state. \n\nSimultaneously, we utilized the Nobeyama 45 m radio telescope to observe the core at a frequency of 1 mm during the same observational period. However, our spectral analysis did not reveal any significant emission line features, suggesting a lack of active star formation at this stage. The absence of notable emissions raises intriguing questions about the physical processes occurring within this young dense core. \n\nBased on our findings, we discuss potential mechanisms for star formation in such an environment, considering the implications of the observed structures and their interactions within the dense core. This research contributes to our understanding of the early stages of star formation and the dynamics of starless dense cores, particularly in relation to the role of pulsating globules in the star formation process. Our results provide a foundation for further investigations into the conditions that facilitate or hinder star formation in similar astrophysical settings.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 3.8247315498700596,
        "rewrite-fast-z-score": -0.2822162605150792
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The formation of spiral arms and rings in barred galaxies .\nAbstract:\nWe present an analysis of the distribution of gas, stars and dust in two nearby edge-on spirals with prominent bars (NGC 1365 and NGC 1530). We use high-resolution observations obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems. The main results are as follows:  - In both cases we find that the molecular hydrogen is concentrated on the leading edges of the bar, while atomic hydrogen follows closely the stellar light.  - The star formation rate peaks at the ends of the bar where the density of molecular hydrogen increases significantly. This suggests that the gravitational torques induced by the bar can trigger the collapse of dense clouds into new generations of young stars.  - The infrared emission associated with polycyclic aromatic hydrocarbons shows a clear correlation between the location of this component and the regions of active star formation. - The comparison of our data with hydrodynamical simulations indicates that the observed structure of the ISM may be explained if the bar potential has been able to drive significant amounts of cold gas towards its inner Lindblad resonance.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The formation of spiral arms and rings in barred clusters . Abstract : We report an assessment of the distribution of gas , stars and dust in two adjacent edge - on spirals with prominent bars ( NGC 1365 and NGC 1530 ) .We use large - resolution measurements obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems . The main results are as follows : - In both cases we find that the molecular hydrogen is confined on the led corners of the bar , while nuclear hydrogen takes closely the stellar radiation .- The star formation rate peaks at the ends of the bar where the density of molecular hydrogen rises considerably . This implies that the gravitational torques induced by the bar can cause the collapse of dense clouds into new generations of young stars .- The infrared absorption associated with polycyclic aromatic hydrocarbons reveals a clear correlation between the location of this constituent and the regions of active star formation . - The comparison of our information with hydrodynamical simulations indicates that the known composition of the ISM may be described if the bar potential has been able to drive considerable amounts of cold gas towards its outer Lindblad resonance .",
        "rewrite_text": "Title: The Formation of Spiral Arms and Rings in Barred Clusters\n\nAbstract: In this study, we present a detailed analysis of the distribution of gas, stars, and dust in two neighboring edge-on spiral galaxies characterized by prominent bars, namely NGC 1365 and NGC 1530. Utilizing high-resolution data obtained from the Herschel Space Observatory, we investigate the physical conditions of the interstellar medium (ISM) within these galaxies. Our findings reveal several significant insights: Firstly, we observe that molecular hydrogen is predominantly concentrated at the leading edges of the bars, while atomic hydrogen is closely associated with the stellar radiation emitted by the galactic core. Secondly, we identify that the star formation rate is maximized at the ends of the bars, where there is a notable increase in the density of molecular hydrogen. This observation suggests that the gravitational torques generated by the bar structure facilitate the collapse of dense molecular clouds, leading to the formation of new stellar populations. Additionally, our analysis of infrared absorption features linked to polycyclic aromatic hydrocarbons (PAHs) indicates a strong correlation between the distribution of these molecules and regions exhibiting active star formation. Lastly, by comparing our observational data with hydrodynamical simulations, we conclude that the existing composition of the ISM can be effectively explained if the gravitational potential of the bar is capable of channeling substantial amounts of cold gas toward its outer Lindblad resonance. This research enhances our understanding of the dynamic processes governing star formation in barred spiral galaxies and the role of the ISM in shaping their structural evolution.",
        "ori-fast-z-score": 1.2935483472729858,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 0.5477225575051661
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Enhanced Kondo Effect in an Electron System Dynamically Coupled with Local Optical Phonon .\nAbstract:\nWe study the effect of local optical phonons on the electronic transport properties of a quantum dot system by using the nonequilibrium Green s function method combined with the density functional theory (DFT). We find that the electron-phonon interaction can induce a strong enhancement to the Kondo resonance peak and lead to a significant reduction of the Kondo temperature TK, which is determined as the energy scale at which the conductance reaches its maximum value Gmax. The results show that the Kondo temperature decreases rapidly when increasing the strength of the electron-phonon coupling constant λ. In addition, we also investigate how the Kondo temperature depends on the size of the quantum dots for different values of λ. Our findings may be useful for understanding the physical mechanism behind some recent experiments. Introduction:-The Kondo effect has been studied extensively both theoretically  1 - 3 and experimentally  4  -  6  . It occurs due to the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level  7, 8  , leading to a sharp zero-bias anomaly in the differential conductance  9  . Recently, it was found that this phenomenon could occur even without any magnetic impurities  10  -  12  .\nIn fact, the Kondo effect has attracted much attention recently because of its potential applications in spintronics devices  13  -  16  . For example, the Kondo effect can be used to design novel spin transistors  17  or single-spin qubits  18  . However, there are still several open questions about the Kondo effect such as: How does the Kondo temperature depend on the size of the nanostructures? What happens if one introduces other degrees of freedom into the system?\nTo answer these questions, various theoretical methods have been developed  19  -  22  . Among them, the nonequilibrium Green functions technique  23  -  25  provides us with powerful tools to calculate the current through the systems under consideration  26  -  28  . This approach allows us not only to obtain the steady-state current but also to explore the time evolution of the current after switching on/off external fields  29  -  31  . Moreover, combining the nonequilibrium Green",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Enhanced Kondo Effect in an Electron System Dynamically Coupled with Local Optical Phonon . Abstract : We research the impact of local laser phonons on the electronic transport properties of a quantum dot network by using the nonequilibrium Green s function method combined with the density functional theory ( DFT ) .We see that the electron - phonon interaction can induce a powerful enhancement to the Kondo resonance peak and lead to a substantial lowering of the Kondo temperature TK , which is calculated as the power range at which the conductance reaches its highest value Gmax . The results show that the Kondo temperature reduces rapidly when increasing the strength of the electron - phonon coupling constant λ .In addition , we also investigate how the Kondo temperature varies on the size of the quantum dots for different values of λ . Our findings may be valuable for studying the physical process behind some latest studies .Introduction : - The Kondo phenomenon has been studied thoroughly both theoretically 1 - 3 and experimentally 4 - 6 . It happens due to the formation of a many - bodies singlet state between localized magnetic moments and conduction electrons near the Fermi level 7 , 8 , leading to a sharp zero - bias anomaly in the differential conductance 9 .Recently , it was shown that this phenomenon might arise even without any magnetic impurities 10 - 12 . In indeed , the Kondo phenomenon has garnered considerable scrutiny lately because of its potential applications in spintronics devices 13 - 16 .For instance , the Kondo phenomenon can be used to model novel spin transistors 17 or single - spinning qubits 18 . However , there are still several open questions about the Kondo phenomenon such as : How does the Kondo temperature depend on the size of the nanostructures ?What happens if one introduces other degrees of liberty into the process ? To answer these problems , various theoretical methods have been constructed 19 - 22 .Among them , the nonequilibrium Green functions method 23 - 25 offers us with powerful tools to estimate the charge through the systems under consideration 26 - 28 . This method enables us not only to obtain the stable - state current but also to examine the time progression of the current after switching on / off external fields 29 - 31 .Moreover , merging the nonequilibrium Green",
        "rewrite_text": "**Title:** Enhanced Kondo Effect in an Electron System Dynamically Coupled with Local Optical Phonons\n\n**Abstract:** This study investigates the influence of local optical phonons on the electronic transport characteristics of a quantum dot network, employing the nonequilibrium Green's function method in conjunction with density functional theory (DFT). Our findings reveal that the interaction between electrons and phonons significantly enhances the Kondo resonance peak, resulting in a notable decrease in the Kondo temperature (TK). This temperature is determined as the power range at which the conductance achieves its maximum value (Gmax). Notably, we observe a rapid decline in Kondo temperature with increasing strength of the electron-phonon coupling constant (λ). Furthermore, we explore the dependence of Kondo temperature on the size of the quantum dots for various values of λ. These insights could prove instrumental in understanding the underlying physical mechanisms highlighted in recent research. \n\nThe Kondo effect has been extensively examined both theoretically and experimentally, characterized by the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level, which leads to a pronounced zero-bias anomaly in differential conductance. Recent studies indicate that the Kondo effect can manifest even in the absence of magnetic impurities, drawing significant attention due to its potential applications in spintronic devices. For example, it can be utilized to model innovative spin transistors or single-spin qubits. However, several questions remain unresolved regarding the Kondo phenomenon, such as the dependence of Kondo temperature on nanostructure size and the effects of introducing additional degrees of freedom into the system. To address these inquiries, various theoretical approaches have been developed, with the nonequilibrium Green's function method providing robust tools for analyzing charge transport in these systems. This method not only allows for the calculation of steady-state current but also facilitates the examination of current dynamics in response to external field variations.",
        "ori-fast-z-score": -0.23942606534028665,
        "water-fast-z-score": 6.957010852370434,
        "rewrite-fast-z-score": -0.08247860988423225
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rotation Measures of Extragalactic Sources Behind the Southern Galactic Plane: New Insights into the Large-Scale Magnetic Field of the Inner Milky Way .\nAbstract:\nWe present rotation measures (RMs) for more than 1000 extragalactic radio sources behind the southern galactic plane, obtained with the Australia Telescope Compact Array and Parkes Radio Telescope at 1.4 GHz. The RMs are used to probe the large-scale magnetic field in the inner Galaxy on scales ranging from 0.1 kpc to 10 kpc. We find that the RM distribution is consistent with an axisymmetric model consisting of two components: one component associated with the local spiral arm structure near the Sun; another component tracing the global magnetic field of the entire Galaxy. This latter component has a strength of about 3 microgauss within 2 kpc of the solar position, which decreases rapidly beyond this distance. It also shows significant deviations from axial symmetry around the Galactic center. These results provide new insights into the origin of cosmic rays in our Galaxy. They can be found here . Rotation measure (RM) surveys have been widely used to study the largescale magnetic fields in nearby galaxies as well as in distant clusters of galaxies. However, such studies are difficult to carry out towards the central regions of the Galaxy due to strong foreground emission from ionized gas along the line-of-sight. In this work we report a survey of the large-scale magnetic field in front of the Galactic Center using rotation measures of extragalactic radio sources observed by the Australia Telescope Compact Array (ATCA). Our sample consists of 1253 radio sources selected from the AT20G catalog based on their angular distances from the Galactic Center. Their rotation measures were derived from multi-frequency observations carried out between 2007 and 2010. The resulting rotation measure map reveals a clear pattern of alternating positive and negative values across the sky. By fitting these data points with a simple axisymmetric model, we obtain a best-fit value of B = 3 μG for the mean magnetic field strength inside a radius of 2 kpc centered on the Galactic Center. Beyond this region, the magnetic field strength drops quickly to less than 1 μG. Furthermore, there appears to exist a systematic deviation from axial symmetry around the GC.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rotation Measures of Extragalactic Sources Behind the Southern Galactic Plane : New Insights into the Large - Scale Magnetic Field of the Inner Milky Way . Abstract : We report rotation measures ( RMs ) for more than 1000 extragalactic radio sources behind the southern galactic plane , obtained with the Australia Telescope Compact Array and Parkes Radio Telescope at 1 . 4 GHz .The RMs are using to probe the huge - scale magnetic force in the inner Galaxy on scales extending from 0 . 1 kpc to 10 kpc . We see that the RM distribution is compatible with an axisymmetric model composed of two parts : one element associated with the local spiral arm structure near the Sun ; another component tracing the global magnetic force of the entire Galaxy .This latter component has a intensity of about 3 microgauss within 2 kpc of the sun position , which drops rapidly beyond this distance . It additionally shows significant deviations from axial symmetry around the Galactic center .These data provide fresh insights into the origin of cosmic rays in our Galaxy . They can be found here .Rotation measure ( RM ) observations have been widely using to study the largescale magnetic fields in nearby galaxies as also as in nearby clusters of galaxies . However , such studies are hard to carry out towards the central regions of the Galaxy due to powerful foreground emission from ionized gas along the line - of - view .In this project we publish a survey of the huge - scale magnetic force in front of the Galactic Center involving rotation measures of extragalactic radio sources observed by the Australia Telescope Compact Array ( ATCA ) . Our specimen consists of 1253 radio sources chosen from the AT20G database based on their angular distances from the Galactic Center .Their rotation measures were obtained from multi - frequency observations carried out between 2007 and 2010 . The resulting rotation measure map presents a clear sequence of alternating positive and negative values across the sky .By fitting these information points with a simple axisymmetric model , we obtain a better - fitting value of B = 3 μG for the mean magnetic force speed inside a diameter of 2 kpc centered on the Galactic Center . Beyond this area , the magnetic force force drops quickly to smaller than 1 μG .Furthermore , there seems to exist a systematic deviation from axial symmetry around the GC .",
        "rewrite_text": "**Title:** Rotation Measures of Extragalactic Sources Behind the Southern Galactic Plane: New Insights into the Large-Scale Magnetic Field of the Inner Milky Way\n\n**Abstract:** In this study, we present rotation measures (RMs) for over 1,000 extragalactic radio sources located behind the southern galactic plane, utilizing data collected from the Australia Telescope Compact Array and the Parkes Radio Telescope at a frequency of 1.4 GHz. These rotation measures serve as a tool to investigate the large-scale magnetic field within the inner Milky Way, covering distances from 0.1 kpc to 10 kpc. Our analysis reveals that the distribution of RMs aligns with an axisymmetric model that comprises two distinct components: one associated with the local spiral arm structure in proximity to the Sun, and another that reflects the global magnetic field of the Milky Way. The latter component exhibits an intensity of approximately 3 microgauss within 2 kpc of the Sun, with a rapid decline beyond this range. Notably, we observe significant deviations from axial symmetry in the magnetic field around the Galactic center. These findings contribute valuable insights into the origins of cosmic rays within our Galaxy.\n\nRotation measure observations have been extensively employed to explore large-scale magnetic fields in nearby galaxies and galaxy clusters. However, conducting such studies in the central regions of the Milky Way poses challenges due to the intense foreground emissions from ionized gas along the line of sight. In this project, we present a comprehensive survey of the large-scale magnetic field in the vicinity of the Galactic Center, based on rotation measures of extragalactic radio sources observed by the Australia Telescope Compact Array (ATCA). Our sample comprises 1,253 radio sources selected from the AT20G database according to their angular distances from the Galactic Center. The RMs were derived from multi-frequency observations conducted between 2007 and 2010. The resulting RM map reveals a distinct pattern of alternating positive and negative values across the sky. By fitting these data points to a simple axisymmetric model, we determine a mean magnetic field strength of B = 3 μG within a 2 kpc diameter centered on the Galactic Center, with a rapid decline to below 1 μG beyond this region. Additionally, we identify a systematic deviation from axial symmetry surrounding the Galactic Center.",
        "ori-fast-z-score": -2.3144519649561044,
        "water-fast-z-score": 6.324555320336758,
        "rewrite-fast-z-score": -0.16329931618554522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Testing General Metric Theories of Gravity with Bursting Neutron Stars .\nAbstract:\nWe present the results of an analysis of gravitational wave data collected by the LIGO and Virgo detectors during the first observing run (O1) in 2015, which includes two candidate events for binary neutron star mergers. We use these observations to test general relativity against alternative theories of gravity that predict deviations from GR at high curvature regimes such as those encountered near black holes or neutron stars. In particular we consider scalar-tensor theories where the coupling between matter fields and the metric is mediated by a light scalar field. These theories are motivated by string theory and have been studied extensively over many decades. \n \n For each event, we perform Bayesian model selection using simulated signals generated from both GR and several representative scalartensor theories. Our results show no evidence for deviations from GR within current uncertainties. However, this does not rule out all possible deviations from GR; it only rules out certain classes of deviations predicted by specific models.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Testing General Metric Theories of Gravity with Bursting Neutron Stars . Abstract : We report the results of an assessment of gravitational wave information collected by the LIGO and Virgo detectors during the first observing run ( O1 ) in 2015 , which includes two candidate events for binary neutron galaxy mergers .We use these observations to test general relativity against alternative theories of gravitational that forecast deviations from GR at high curvature regimes such as those observed near black holes or neutron stars . In particular we investigate scalar - vector models where the interaction between matter fields and the metric is mediated by a light scalar field .These concepts are motivated by string theory and have been studied frequently over numerous years . For each event , we perform Bayesian model selection utilizing simulated waves generated from both GR and many representative scalartensor theories .Our results show no evidence for deviations from GR within current uncertainties . However , this does not order out all possible deviations from GR ; it only rules out specific grades of deviations expected by specific models .",
        "rewrite_text": "In this study, we present an analysis of gravitational wave data obtained from the LIGO and Virgo detectors during their inaugural observing run (O1) in 2015, focusing on two potential events associated with binary neutron star mergers. Our primary objective is to evaluate the validity of general relativity (GR) in comparison to alternative gravitational theories that predict deviations from GR in high-curvature environments, such as those found in the vicinity of black holes and neutron stars. Specifically, we explore scalar-vector models, which propose that the interaction between matter fields and the spacetime metric is influenced by a light scalar field. These theoretical frameworks are inspired by string theory and have been the subject of extensive research over the years.\n\nTo conduct our analysis, we employ Bayesian model selection techniques, utilizing simulated gravitational waveforms generated from both GR and a variety of representative scalar-tensor theories. Our findings indicate that, within the current uncertainties, there is no significant evidence to suggest deviations from general relativity. However, it is important to note that our results do not eliminate the possibility of all potential deviations from GR; rather, they specifically exclude certain types of deviations predicted by the models we examined. This work contributes to the ongoing discourse regarding the validity of general relativity and the exploration of alternative gravitational theories, particularly in the context of extreme astrophysical events such as neutron star mergers. The implications of our findings are significant for future gravitational wave observations and the theoretical understanding of gravity in extreme conditions.",
        "ori-fast-z-score": -0.5555555555555556,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 0.45454545454545453
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Likely Cause of the EGRET GeV Anomaly and its Implications .\nAbstract:\nWe present an explanation for the excess in gamma-ray emission observed by the Energetic Gamma Ray Experiment Telescope (EGRET) at energies above 10 GeV, which is known as the  GeV anomaly . We show that this excess can be explained if there are two populations of pulsars with different magnetic field strengths. The first population consists of young pulsars whose fields decay rapidly due to their rapid spin-downs. These pulsars produce most of the high-energy photons detected by EGRET. The second population consists of older pulsars whose fields have decayed more slowly because they rotate slower than younger pulsars on average. This second population produces less high-energy radiation but contributes significantly to the total number of pulsars. Our model predicts that Fermi should detect many new pulsar candidates not seen before. In addition, we predict that some of these newly discovered pulsars will exhibit very high luminosities compared to other pulsars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Likely Cause of the EGRET GeV Anomaly and its Implications . Abstract : We present an reason for the excess in gamma - ray radiation observed by the Energetic Gamma Ray Experiment Telescope ( EGRET ) at energies above 10 GeV , which is known as the GeV anomaly .We see that this excess can be understood if there are two communities of pulsars with varying magnetic force abilities . The first population contains of young pulsars whose fields collapse rapidly due to their fast spinning - downs .These pulsars produce most of the high - energy photons discovered by EGRET . The second population contains of older pulsars whose fields have decayed more slowly because they rotate slower than younger pulsars on average .This second population generates less large - energy rays but adds significantly to the total quantity of pulsars . Our model predicts that Fermi should detect many new pulsar candidates not seen before .In addition , we estimate that some of these newly discovered pulsars will exhibit very high luminosities relative to other pulsars .",
        "rewrite_text": "We provide an explanation for the observed excess in gamma-ray emissions detected by the Energetic Gamma Ray Experiment Telescope (EGRET) at energies exceeding 10 GeV, commonly referred to as the GeV anomaly. Our analysis suggests that this anomaly can be attributed to the existence of two distinct populations of pulsars, each characterized by different magnetic field strengths. The first group comprises young pulsars, which experience rapid magnetic field decay due to their swift rotational periods. These young pulsars are responsible for the majority of the high-energy photons recorded by EGRET. In contrast, the second group consists of older pulsars, which have slower rotation rates and, consequently, exhibit a more gradual decline in their magnetic fields. While this older population produces fewer high-energy gamma rays, it significantly contributes to the overall pulsar population. Our model anticipates that the Fermi Gamma-ray Space Telescope will identify numerous new pulsar candidates that have not yet been observed. Furthermore, we predict that some of these newly detected pulsars will demonstrate exceptionally high luminosities compared to their counterparts. This research not only sheds light on the nature of the GeV anomaly but also has broader implications for our understanding of pulsar populations and their contributions to gamma-ray emissions in the universe.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.266851623825876,
        "rewrite-fast-z-score": -0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Critical Casimir Effect in superfluid wetting films .\nAbstract:\nWe study the critical Casimir effect (CCE) between two parallel plates immersed into a liquid helium film at its superfluid transition temperature T_sf = 2.17 K, using Monte Carlo simulations based on the density functional theory for quantum fluids. We find that the CCE is strongly suppressed by the presence of the substrate and vanishes completely when the distance to it becomes smaller than about one molecular diameter. The results are compared with those obtained within the mean-field approximation which overestimates the magnitude of the effect considerably. In addition we show how the influence of the substrate can be taken into account in an approximate way. \nPACS numbers: 67.85.-j, 68.45.-k, 71.10.Fd \nI. INTRODUCTORY REMARK\nThe critical Casimir effect  1  , i.e., the force acting between macroscopic bodies due to fluctuations of the order parameter near their phase transitions, has been studied extensively during recent years both theoretically  2  -  4  and experimentally  5  . It was shown  6  that this effect may play important role in various physical phenomena such as capillary condensation  7, 8  or wetting  9  .\nIn particular, the critical Casimir effect plays crucial role in the physics of thin liquid helium films  10  where it leads to the appearance of additional forces  11  responsible for the formation of stable droplets  12  . These effects have been observed recently  13  in experiments performed on helium nanodroplets trapped inside magnetic traps  14  . However, most theoretical studies so far were restricted to idealized situations neglecting the influence of the substrate  15  -  17  . This simplification is justified only if the thickness of the film d is much larger than the range of interaction potential between atoms of the fluid and the surface  18  . For example, in case of 4 He films adsorbed on graphite substrates  19  the typical values of these parameters are  20  : r 0 ≈ 3Å, d ≈ 10 − 100 nm. Therefore, taking into account the substrate explicitly is necessary  21  especially close to the wetting transition  22  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Critical Casimir Effect in superfluid wetting movies . Abstract : We explore the critical Casimir effect ( CCE ) between two connected panels immersed into a liquid helium movie at its superfluid transition temperature T _ sf = 2 . 17 K , using Monte Carlo simulations based on the density functional theory for quantum fluids .We see that the CCE is strongly suppressed by the presence of the substrate and vanishes totally when the distance to it becomes lower than about one molecular size . The results are compared with those achieved within the mean - field approximation which overestimates the severity of the result considerably .In addition we show how the impact of the substrate can be taken into consideration in an approximate way . PACS scores : 67 . 85 . - j , 68 . 45 . - k , 71 . 10 . Fd I .INTRODUCTORY REMARK The essential Casimir effect 1 , i . e . , the force acting between macroscopic bodies owing to fluctuations of the order parameter near their phase transitions , has been studied thoroughly during recent seasons both theoretically 2 - 4 and experimentally 5 . It was shown 6 that this effect could play important role in different mechanical phenomena such as capillary condensation 7 , 8 or wetting 9 .In particular , the critical Casimir effect holds crucial role in the physics of thin liquid helium films 10 where it brings to the appearance of added forces 11 responsible for the formation of stable droplets 12 . These effects have been observed lately 13 in experiments conducted on helium nanodroplets caught inside magnetic traps 14 .However , most theoretical researchers so far were restricted to idealized scenarios neglecting the impact of the substrate 15 - 17 . This simplification is justified only if the length of the film d is much larger than the range of interaction potential between molecules of the liquid and the surface 18 .For instance , in case of 4 He films adsorbed on graphite compounds 19 the typical values of these parameters are 20 : l 0 ≈ 3Å , d ≈ 10 − 100 nm . Therefore , giving into consideration the substrate explicitly is required 21 especially close to the wetting transition 22 .",
        "rewrite_text": "**Title:** Critical Casimir Effect in Superfluid Wetting Films\n\n**Abstract:** In this study, we investigate the critical Casimir effect (CCE) occurring between two panels immersed in a superfluid helium film at its transition temperature of T_sf = 2.17 K. Utilizing Monte Carlo simulations grounded in the density functional theory for quantum fluids, we reveal that the presence of a substrate significantly suppresses the CCE. Notably, the effect diminishes entirely when the distance from the substrate falls below approximately one molecular size. Our findings are contrasted with results derived from mean-field approximations, which tend to overstate the intensity of the CCE. Furthermore, we propose a method to account for the substrate's influence in a more approximate manner. \n\nThe critical Casimir effect, which describes the force between macroscopic bodies due to fluctuations of the order parameter near phase transitions, has garnered considerable attention in recent years, both theoretically and experimentally. It has been established that this phenomenon can significantly influence various mechanical processes, including capillary condensation and wetting. Specifically, in the context of thin liquid helium films, the CCE plays a vital role by introducing additional forces that contribute to the formation of stable droplets. Recent experimental observations have confirmed these effects in helium nanodroplets trapped within magnetic fields. \n\nHowever, prior theoretical investigations have largely overlooked the substrate's impact, often relying on idealized models. Such simplifications are only valid when the film thickness is substantially greater than the interaction range between the liquid molecules and the substrate. For instance, in the case of ^4He films adsorbed on graphite, typical parameters indicate a molecular interaction range of approximately 3 Å and film thicknesses ranging from 10 to 100 nm. Therefore, it is essential to explicitly consider the substrate's effects, particularly in proximity to the wetting transition, to achieve a more accurate understanding of the CCE in superfluid systems. \n\n**PACS Numbers:** 67.85.-j, 68.45.-k, 71.10.Fd",
        "ori-fast-z-score": -0.8333333333333334,
        "water-fast-z-score": 6.289804753377997,
        "rewrite-fast-z-score": 0.34050261230349943
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Interaction of Supernova Ejecta with Nearby Protoplanetary Disks .\nAbstract:\nWe present the results of hydrodynamic simulations that show how supernova ejecta can interact with nearby protoplanetary disks and produce observable signatures in their infrared emission. We find that, depending on disk properties (mass, radius), the interaction may lead to an increase or decrease in the total luminosity emitted by the system at near-infrared wavelengths. The effect is strongest for massive disks around young stars; it decreases rapidly as the mass ratio between the star and its disk decreases. \n \n In addition, we find that the interaction leads to significant changes in the temperature distribution within the disk. These effects are most pronounced when the disk is relatively close to the supernova progenitor - less than 100 AU away. For more distant systems, the impact of the supernova blast wave becomes negligible. Finally, our models suggest that the observed excesses in mid-infrared flux detected towards some T Tauri stars could be due to such interactions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Interaction of Supernova Ejecta with Nearby Protoplanetary Disks . Abstract : We present the results of hydrodynamic simulations that demonstrate how supernova ejecta can interact with nearby protoplanetary disks and produce observable signatures in their infrared absorption .We see that , depending on disk properties ( mass , diameter ) , the interaction may contribute to an increase or decrease in the total luminosity generated by the system at near - infrared wavelengths . The phenomenon is greatest for huge disks around young galaxies ; it decreases quickly as the mass ratio between the star and its disk decreases .In addition , we find that the interaction results to significant variations in the temperature distribution within the disk . These effects are most pronounced when the disk is fairly nearby to the supernova progenitor - less than 100 AU away .For more distant systems , the impact of the supernova blast wave grows negligible . Finally , our calculations suggest that the seen excesses in middle - infrared flux detected towards some T Tauri stars could be due to such interactions .",
        "rewrite_text": "We present findings from hydrodynamic simulations that explore the interaction between supernova ejecta and nearby protoplanetary disks, revealing potential observable signatures in infrared absorption. Our study indicates that the nature of this interaction is heavily influenced by the properties of the protoplanetary disks, such as their mass and diameter. Specifically, we observe that the interaction can lead to either an increase or decrease in the total luminosity emitted by the system at near-infrared wavelengths. This effect is particularly pronounced in large disks surrounding young galaxies, while it diminishes rapidly as the mass ratio between the star and its disk decreases.\n\nMoreover, our simulations show that the interaction between supernova ejecta and protoplanetary disks causes significant alterations in the temperature distribution within the disks. These temperature variations are most evident when the disk is located within 100 astronomical units (AU) of the supernova progenitor. As the distance increases, the influence of the supernova blast wave becomes negligible, leading to a reduced impact on the disk's characteristics.\n\nOur results also suggest a potential explanation for the observed excesses in middle-infrared flux associated with certain T Tauri stars, which may be attributed to these interactions. Overall, this research enhances our understanding of the complex dynamics between supernova events and the formation of protoplanetary systems, providing insights into the processes that may influence star and planet formation in the universe.",
        "ori-fast-z-score": 0.22360679774997896,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": 0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spitzer observations of a 24 micron shadow: Bok Globule CB190 .\nAbstract:\nWe report the detection of an infrared dark cloud (IRDC) in the vicinity of the open cluster NGC 6334, using data obtained with Spitzer Space Telescope s Infrared Array Camera (IRAC). The IRDC is associated with the molecular cloud complex G327.3+0.6 and has been identified as Bok globule CB190 by Clemens & Barvainis (1988) . We find that this object exhibits a prominent 24 micron shadow which may be caused by absorption against bright mid-infrared emission from nearby protostars or young stellar objects. This feature suggests that the cloud contains dense cores at different evolutionary stages. Using near-infrared extinction mapping we identify two candidate starless cores within the cloud. These are located near the center of the cloud where the 24 micron shadow is most pronounced. Our analysis shows that these cores have masses between 0.5 Msun to 1 Msun and radii ranging from 1000 AU to 3000 AU .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spitzer observations of a 24 micron shadow : Bok Globule CB190 . Abstract : We report the observation of an infrared dark cloud ( IRDC ) in the vicinity of the open cluster NGC 6334 , using data acquired with Spitzer Space Telescope s Infrared Array Camera ( IRAC ) .The IRDC is associated with the molecular dust complex G327 . 3 + 0 . 6 and has been described as Bok globule CB190 by Clemens & Barvainis ( 1988 ) . We see that this body features a distinct 24 micron shadow which may be caused by absorption against bright mid - infrared absorption from nearby protostars or young stellar bodies .This characteristic demonstrates that the cloud contains thick cores at different evolutionary stages . Using near - infrared extinction mapping we identify two proposed starless cores within the cloud .These are situated near the center of the cloud where the 24 micron shadow is most pronounced . Our study shows that these cores have masses between 0 . 5 Msun to 1 Msun and radii ranging from 1000 AU to 3000 AU .",
        "rewrite_text": "We present our findings from the Spitzer Space Telescope's Infrared Array Camera (IRAC) regarding an infrared dark cloud (IRDC) located near the open cluster NGC 6334. This IRDC, identified as Bok globule CB190 by Clemens & Barvainis (1988), is associated with the molecular dust complex G327.3 + 0.6. Our observations reveal a prominent 24-micron shadow within the cloud, likely resulting from the absorption of light from nearby protostars or young stellar objects. This shadow indicates the presence of dense cores within the cloud, each at varying stages of evolution. Through near-infrared extinction mapping, we have pinpointed two potential starless cores situated at the cloud's center, where the 24-micron shadow is most evident. Our analysis suggests that these cores possess masses ranging from 0.5 to 1 solar mass and exhibit radii between 1000 AU and 3000 AU. These findings contribute to our understanding of the structure and evolutionary processes occurring within Bok globules and highlight the significance of infrared observations in studying star formation in dense molecular environments.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 0.601929265428846
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The effect of stellar-mass black holes on the structural evolution of massive star clusters .\nAbstract:\nWe study how the presence of a central supermassive black hole (SMBH) affects the dynamical evolution and mass loss rate of an open cluster with N = 10, 000 stars in virial equilibrium. We use direct N-body simulations to follow the long-term evolution of this system for up to 1 Gyr. The SMBH is modeled as a point particle that interacts gravitationally only with the most bound particles of each star cluster orbiting it. Our results show that the presence of the SMBH causes significant changes in the structure of the open cluster over time. In particular, we find that:  -The open cluster loses about half its initial total mass within 0.5 Gyr due to tidal stripping by the SMBH.  -The open cluster becomes more spherical after being tidally stripped by the SMBH. -The open cluster develops a core-halo structure where the density profile follows a power law at large radii but has a flat distribution near the center.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of stellar - mass black holes on the structural progression of large star clusters . Abstract : We research how the presence of a central supermassive black hole ( SMBH ) affects the dynamical development and mass loss rate of an open cluster with N = 10 , 000 stars in virial balance .We use direct N - bodies simulations to follow the long - term evolve of this system for up to 1 Gyr . The SMBH is modeled as a point object that interacts gravitationally only with the most bound particles of each star cluster orbiting it .Our results show that the presence of the SMBH causes significant variations in the composition of the open cluster over time . In particular , we find that : - The open cluster losing about half its initial total mass within 0 . 5 Gyr owing to tidal stripping by the SMBH .- The open cluster turns more spherical after being tidally stripped by the SMBH . - The open nucleus develops a core - halo structure where the density profile parallels a power law at large radii but has a flattened distribution near the center .",
        "rewrite_text": "**Title:** The Impact of Stellar-Mass Black Holes on the Structural Evolution of Large Star Clusters\n\n**Abstract:** This study investigates the influence of a central supermassive black hole (SMBH) on the dynamical evolution and mass loss rate of an open star cluster comprising N = 10,000 stars in virial equilibrium. Utilizing direct N-body simulations, we analyze the long-term evolution of this stellar system over a period of up to 1 billion years (Gyr). The SMBH is represented as a point mass that exerts gravitational influence primarily on the most tightly bound stars within the cluster's orbit. Our findings reveal that the presence of the SMBH leads to substantial changes in the cluster's composition over time. Notably, we observe that the open cluster experiences a mass reduction of approximately 50% of its initial total mass within the first 0.5 Gyr, primarily due to tidal stripping effects induced by the SMBH. Additionally, the cluster undergoes a transformation towards a more spherical shape as a result of this tidal interaction. Furthermore, the core of the cluster evolves into a core-halo structure, characterized by a density profile that follows a power law at larger radii, while exhibiting a flattened distribution in the central region. These results underscore the significant role that SMBHs play in shaping the structural dynamics of star clusters, highlighting the intricate interplay between gravitational forces and stellar evolution in such systems. This research contributes to our understanding of the long-term stability and structural changes in star clusters influenced by central massive black holes, with implications for the broader field of astrophysics and the evolution of galactic structures.",
        "ori-fast-z-score": -1.193117518002609,
        "water-fast-z-score": 3.0193176496962755,
        "rewrite-fast-z-score": 0.6363636363636364
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Finite Element framework for computation of protein normal modes and mechanical response .\nAbstract:\nWe present an efficient computational method to compute the elastic properties, vibrational frequencies and normal modes of proteins using finite element analysis (FEA). The proposed approach is based on representing each amino acid residue as a single node in a 3D tetrahedral mesh that covers the entire structure of interest. We show how this representation can be used within FEA to efficiently calculate the stiffness matrix associated with the system under study. In addition we demonstrate how the same representation allows us to obtain accurate results when computing the eigenfrequencies and corresponding mode shapes of the system by solving the generalized eigenvalue problem arising from the discretized form of the governing equations. Finally, we illustrate our methodology through several numerical examples involving different types of proteins. Our results indicate that the proposed approach provides highly accurate predictions compared to those obtained via other state-of-the-art methods such as molecular dynamics simulations or coarse-grained models. This work was supported by NSERC Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Finite Element framework for computation of gene proper modes and mechanical response . Abstract : We create an efficient numerical technique to compute the elastic properties , vibrational speeds and normal mechanisms of proteins using finite element assessment ( FEA ) .The proposed approach is based on depicting each amino residue compound as a single node in a 3D tetrahedral mesh that covers the entire structure of interest . We see how this representation can be used within FEA to easily assess the stiffness matrix associated with the process under research .In addition we prove how the same expression permits us to obtain precise conclusions when computing the eigenfrequencies and corresponding mode shapes of the system by addressing the generalized eigenvalue problem originating from the discretized form of the governing equations . Finally , we exhibit our technique through several mathematical examples involving varying kinds of proteins .Our results show that the suggested approach offers highly precise predictions compared to those achieved via other state - of - the - art methods such as molecular dynamics simulations or rough - grained estimates . This research was supported by NSERC Discovery Grants RGPIN - 2014 - 04567 and RGPIN - 2016 - 05189 .",
        "rewrite_text": "Title: A Finite Element Framework for Computing Gene Proper Modes and Mechanical Response\n\nAbstract: In this study, we present a novel and efficient numerical method for evaluating the elastic properties, vibrational frequencies, and normal modes of proteins through finite element analysis (FEA). Our approach involves modeling each amino acid residue as an individual node within a three-dimensional tetrahedral mesh that encompasses the entire protein structure of interest. This representation facilitates the straightforward computation of the stiffness matrix relevant to the mechanical processes under investigation. Furthermore, we demonstrate that this same framework allows for accurate determination of the eigenfrequencies and corresponding mode shapes of the protein system by solving the generalized eigenvalue problem derived from the discretized governing equations. To validate our methodology, we apply it to a series of mathematical examples involving various types of proteins. The results indicate that our proposed technique yields highly accurate predictions, significantly outperforming traditional methods such as molecular dynamics simulations and coarse-grained approximations. This research contributes to the field by providing a robust computational tool for the analysis of protein mechanics, which could have implications for understanding protein function and stability. The work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) through Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189.",
        "ori-fast-z-score": -1.212183053462653,
        "water-fast-z-score": 6.130730673081194,
        "rewrite-fast-z-score": -0.6123724356957946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Adiabatic quantum dynamics of a random Ising chain across its quantum critical point .\nAbstract:\nWe study the adiabatic evolution of an open-boundary spin-1/2 chain with nearest-neighbor interactions, which is driven by slowly varying external magnetic field and transverse fields. We show that this system undergoes a second-order phase transition at zero temperature when the longitudinal field changes sign. The ground state evolves smoothly through the critical point in the thermodynamic limit but exhibits singular behavior for finite systems. In particular, we find that the fidelity susceptibility diverges as $1/L$ near the critical point where $L$ denotes the number of spins. This result implies that the adiabatic process fails to be efficient if one tries to drive the system across the critical point using slow driving rates. Finally, we discuss how our results can be generalized to other models exhibiting similar features. Introduction:-Adiabatic quantum computation (AQC)  1  has been proposed as a promising approach towards solving hard computational problems  2  . It relies on the fact that it may be possible to solve certain optimization problems efficiently by evolving the initial ground state of some simple Hamiltonian into the ground state of another complicated problem Hamiltonian via a series of intermediate Hamiltonians  3  .\nIn AQC, the time-evolution operator corresponding to each step of the algorithm is obtained by applying a sequence of local unitary transformations to the identity matrix  4  , i.e., U = exp(−iHt/h), where H is the instantaneous Hamiltonian describing the physical system under consideration and t is the total duration of the algorithm. If the rate of change of the parameters characterizing the instantaneous Hamiltonians is sufficiently small compared to their characteristic energy scales then the final state will be close to the ground state of the target Hamiltonian  5  . However, there are several issues associated with implementing such algorithms experimentally  6  -  8  . For example, even though the adiabatic theorem guarantees that the final state will be very close to the ground state provided the evolution occurs over many orders of magnitude slower than the inverse gap between the ground and first excited states  9  , it does not provide any information about the speed required to achieve a given accuracy  10  . Moreover, since",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Adiabatic quantum mechanics of a random Ising chain across its quantum fundamental point . Abstract : We explore the adiabatic behavior of an open - boundary spin - 1 / 2 network with nearest - neighbor interactions , which is caused by slowly varying external magnetic force and longitudinal fields .We see that this system undergoes a second - order phase shift at zero temperature when the longitudinal field shifts sign . The ground state evolves continuously through the critical position in the thermodynamic limit but exhibits singular behavior for finite systems .In particular , we find that the fidelity susceptibility diverges as $ 1 / L $ near the critical position where $ L $ represents the number of spinning . This result means that the adiabatic process fails to be successful if one attempts to drive the process across the pivotal point using slow driving rates .Finally , we talk how our findings can be generalized to other models displaying comparable features . Introduction : - Adiabatic quantum computation ( AQC ) 1 has been proposed as a promising alternative towards solving hard computational problems 2 .It based on the fact that it could be possible to solve many optimization problems easily by expanding the first ground state of some simple Hamiltonian into the ground state of another complicated problem Hamiltonian via a sequence of intermediate Hamiltonians 3 . In AQC , the period - evolve operator corresponding to each step of the algorithm is found by using a sequence of local unitary transformations to the identity matrix 4 , i . e . , U = exp ( −iHt / h ) , where H is the instantaneous Hamiltonian describing the physical system under consideration and t is the total duration of the algorithm .If the rate of change of the variables characterizing the instantaneous Hamiltonians is sufficiently small relative to their characteristic energy scales then the finished state will be close to the ground state of the target Hamiltonian 5 . However , there are several questions associated with implementing such schemes experimentally 6 - 8 .For instance , even though the adiabatic theorem guarantees that the finished state will be very close to the ground state provided the evolution occurs over numerous orders of magnitude slower than the inverse gap between the ground and first excited states 9 , it does not offer any info about the speed required to achieve a given accuracy 10 . Moreover , since",
        "rewrite_text": "**Title:** Adiabatic Quantum Mechanics of a Random Ising Chain Across Its Quantum Fundamental Point\n\n**Abstract:** In this study, we investigate the adiabatic dynamics of an open-boundary spin-1/2 chain characterized by nearest-neighbor interactions, influenced by a slowly varying external magnetic field and longitudinal fields. Our findings reveal that the system experiences a second-order phase transition at zero temperature when the longitudinal field changes sign. Notably, while the ground state transitions smoothly through the critical point in the thermodynamic limit, it exhibits distinct singular behavior in finite systems. We specifically observe that the fidelity susceptibility diverges as \\(1/L\\) near the critical point, where \\(L\\) denotes the number of spins in the system. This divergence indicates that the adiabatic process becomes ineffective when attempting to navigate through the critical point at slow driving rates. Furthermore, we discuss the implications of our results and how they can be extended to other models exhibiting similar characteristics. \n\n**Introduction:** Adiabatic quantum computation (AQC) has emerged as a promising approach for addressing complex computational challenges. The fundamental principle behind AQC is the potential to efficiently solve various optimization problems by adiabatically transforming the ground state of a simple Hamiltonian into that of a more complex problem Hamiltonian through a series of intermediate Hamiltonians. In the context of AQC, the evolution operator for each algorithmic step is derived from a sequence of local unitary transformations applied to the identity matrix, expressed as \\(U = \\exp(-iHt/\\hbar)\\), where \\(H\\) represents the instantaneous Hamiltonian of the system and \\(t\\) is the total duration of the algorithm. If the rate of change of the parameters defining the instantaneous Hamiltonians is sufficiently slow compared to their characteristic energy scales, the final state is expected to closely approximate the ground state of the target Hamiltonian. However, several challenges remain in the experimental implementation of these schemes. Although the adiabatic theorem assures that the final state will be nearly indistinguishable from the ground state, provided the evolution occurs at a rate significantly slower than the inverse of the energy gap between the ground and first excited states, it does not specify the precise speed necessary to achieve a desired level of accuracy. Additionally, there are concerns regarding the practical limitations of achieving such slow evolution rates in real-world scenarios.",
        "ori-fast-z-score": -0.4423258684646914,
        "water-fast-z-score": 7.909667599213929,
        "rewrite-fast-z-score": 0.37371754637596794
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Concise theory of chiral lipid membranes .\nAbstract:\nThe authors present an overview of the current state-of-the-art in understanding how lipids form membranes and what determines their physical properties.  They then introduce a new theoretical framework for describing these phenomena, which they call the concise theory of chiral lipid membranes (CTCLM).  The CTCLM is based on three key concepts:  1) Lipid bilayers are composed of two interdigitated monolayers; 2) Each monolayer contains both enantiomeric forms of each lipid species; 3) Enantiomers have different molecular shapes that lead to differences in packing density within the membrane.  This model explains many experimental observations about the structure and dynamics of biological membranes without introducing any additional parameters or assumptions beyond those already used by existing models. It also provides a simple explanation for why certain types of lipids tend to be found at specific locations within cell membranes. Finally, it suggests several testable predictions that can help guide future experiments aimed at further refining our understanding of this important class of biomolecules.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Concise theory of chiral lipid membranes . Abstract : The authors present an overview of the present state - of - the - art in understanding how lipids form membranes and what determines their structural functions .They then introduce a new theoretical framework for describing these phenomena , which they term the concise theory of chiral lipid membranes ( CTCLM ) . The CTCLM is based on three key concepts : 1 ) Lipid bilayers are composed of two interdigitated monolayers ; 2 ) Each monolayer includes both enantiomeric types of each lipid species ; 3 ) Enantiomers have different molecular patterns that lead to differences in packing density within the membrane .This theory presents many experimental studies about the composition and dynamics of biological membranes without removing any additional parameters or assumptions beyond those already used by existing models . It additionally offers a simple explanation for why certain types of lipids tend to be found at different places within cell membranes .Finally , it makes several testable predictions that can help guide upcoming experiments intended at further refining our grasp of this vital class of biomolecules .",
        "rewrite_text": "The authors provide a comprehensive overview of the current understanding of lipid membrane formation and the factors influencing their structural functions in the scientific article titled \"Concise Theory of Chiral Lipid Membranes.\" They introduce an innovative theoretical framework known as the Concise Theory of Chiral Lipid Membranes (CTCLM), which seeks to elucidate these complex phenomena. The CTCLM is founded on three fundamental principles: first, lipid bilayers consist of two interdigitated monolayers; second, each monolayer contains both enantiomeric forms of each lipid species; and third, the distinct molecular arrangements of enantiomers result in variations in packing density within the membrane. \n\nThis new theory integrates a wealth of experimental data regarding the composition and dynamics of biological membranes without introducing additional parameters or assumptions beyond those utilized in existing models. Furthermore, it provides a straightforward rationale for the preferential localization of specific lipid types within cell membranes. The CTCLM not only enhances our understanding of lipid behavior but also generates several testable hypotheses that can inform future experimental investigations aimed at deepening our knowledge of this crucial category of biomolecules. Overall, the article contributes significantly to the field by offering a refined perspective on the intricate relationship between lipid structure and function in biological membranes.",
        "ori-fast-z-score": 2.0124611797498106,
        "water-fast-z-score": 6.846754616640485,
        "rewrite-fast-z-score": 1.5096588248481377
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lattice Boltzmann Approach to High-Speed Compressible Flows .\nAbstract:\nThe lattice Boltzmann method (LBM) is an alternative approach for solving the Navier-Stokes equations in fluid dynamics, which has been widely used due to its advantages over traditional numerical methods such as finite difference and finite element approaches.  In this work we present a new LBM scheme that can be applied to high-speed compressible flows with high Reynolds number by using multiple relaxation times (MRT). The MRT-LBM solves the discrete velocity model (DVM), where each distribution function represents one component of the macroscopic variables at different velocities on a regular grid. We use the D2Q9 DVM to solve the two-dimensional incompressible flow problems. To validate our proposed algorithm, several benchmark tests are performed including lid-driven cavity flow, Couette flow, Poiseuille flow, Taylor-Green vortex flow, and shock wave propagation through a channel. Our results show good agreement between the numerical solutions obtained by the MRT-LBM and those reported previously in literature.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lattice Boltzmann Approach to High - Speed Compressible Flows . Abstract : The lattice Boltzmann technique ( LBM ) is an alternative approach for solving the Navier - Stokes equations in flow dynamics , which has been widely useful due to its benefits over traditional numerical methods such as finite difference and finite element approaches .In this study we present a new LBM method that can be applied to large - speed compressible flows with high Reynolds number by using multiple relaxation times ( MRT ) . The MRT - LBM solves the discrete momentum model ( DVM ) , where each distribution function describes one element of the macroscopic parameters at different velocities on a regular grid .We use the D2Q9 DVM to solve the two - dimensional incompressible flow difficulties . To validate our proposed algorithm , various benchmark tests are performed including lid - driven cavity flow , Couette flow , Poiseuille flow , Taylor - Green vortex flow , and blast wave propagation through a channel .Our results show good agreement between the numerical solutions obtained by the MRT - LBM and those published previously in literature .",
        "rewrite_text": "**Title:** Lattice Boltzmann Approach to High-Speed Compressible Flows\n\n**Abstract:** The lattice Boltzmann method (LBM) offers a novel framework for addressing the Navier-Stokes equations that govern fluid dynamics, presenting significant advantages over conventional numerical techniques such as finite difference and finite element methods. This paper introduces an innovative LBM approach tailored for high-speed compressible flows characterized by elevated Reynolds numbers, utilizing a multiple relaxation time (MRT) scheme. The MRT-LBM framework operates on a discrete velocity model (DVM), where each distribution function corresponds to a specific macroscopic parameter at various velocities on a structured grid. In our investigation, we employ the D2Q9 DVM to tackle challenges associated with two-dimensional incompressible flows. To assess the efficacy of our proposed algorithm, we conduct a series of benchmark tests, including lid-driven cavity flow, Couette flow, Poiseuille flow, Taylor-Green vortex flow, and the propagation of blast waves through a channel. The results obtained from the MRT-LBM demonstrate a high level of consistency with previously published numerical solutions, affirming the reliability and accuracy of our method. This study not only enhances the understanding of high-speed compressible flow dynamics but also establishes the MRT-LBM as a robust tool for future research in fluid mechanics.",
        "ori-fast-z-score": 1.078327732034384,
        "water-fast-z-score": 4.610083899802517,
        "rewrite-fast-z-score": 0.6831300510639733
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlocal Phenomenology for anisotropic MHD turbulence .\nAbstract:\nWe present an analysis of the nonlocal phenomenology in magnetohydrodynamic (MHD) turbulence with strong magnetic field anisotropy, which is relevant to solar wind and space plasmas. We show that the energy transfer rate between different scales can be described by a simple formula based on the local nonlinear interactions only when the wavevector directions are aligned or anti-aligned with respect to the mean magnetic field direction. In other cases, we find that the nonlocal effects become important due to the presence of oblique waves. The results obtained here may provide useful insights into understanding the nature of turbulent transport processes in astrophysical plasma environments. Turbulence plays an essential role in many physical phenomena ranging from geophysics to fusion physics  1, 2  . It has been shown recently that there exist universal statistical properties shared among various types of turbulent flows  3  , such as Kolmogorov scaling  4  , intermittency  5  , and anomalous dissipation  6  .\nIn particular, it was found that the statistics of fully developed turbulence depend crucially on how fast the energy cascades down through the inertial range  7, 8  . This cascade process involves both linear and nonlinear interactions between different modes at different wavenumbers  9  . For example, in hydrodynamics, the energy flux Π(k) ≡< |δu k · δu * −k | 2 > / < u 2 k > depends not only on the magnitude of the wavenumber k but also its orientation relative to the large-scale flow  10  . Here, u k denotes the Fourier transform of velocity fluctuations at scale k −1 . When the angle θ = arccos (k·v 0 )/|k||v 0 |  between the wavevector k and the large-scale flow v 0 is small, i.e., θ ≪ 1, the energy flux Π ∝ k −2/3 sin 2/3 θ  11  . On the contrary, if θ becomes large, then Π decreases rapidly because of the cancellation effect  12  . Similar behaviors have been observed in magnetohydrodynamics (MHD), where the energy flux Π �",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlocal Phenomenology for anisotropic MHD turbulence . Abstract : We present an assessment of the nonlocal phenomenology in magnetohydrodynamic ( MHD ) turbulence with powerful magnetic force anisotropy , which is relevant to solar wind and space plasmas .We see that the power transfer frequency between various scales can be described by a simple equation based on the local nonlinear interactions only when the wavevector directions are aligned or anti - aligned with regard to the mean magnetic force direction . In other instances , we find that the nonlocal changes become crucial due to the presence of oblique waves .The results derived here perhaps offer useful insights into knowledge the nature of turbulent transport systems in astrophysical plasma settings . Turbulence plays an essential part in many natural phenomena ranging from geophysics to fusion science 1 , 2 .It has been shown lately that there remain universal empirical features common among various types of turbulent waves 3 , such as Kolmogorov scaling 4 , intermittency 5 , and anomalous dissipation 6 . In particular , it was shown that the statistics of fully developed turbulence depend crucially on how fast the electricity cascades down through the inertial range 7 , 8 .This cascade process involves both linear and nonlinear interactions between various modes at different wavenumbers 9 . For instance , in hydrodynamics , the power flux Π ( h ) ≡ < | δu k · δu * −k | 2 > / < u 2 k > depends not only on the magnitude of the wavenumber k but also its attitude relative to the small - scale stream 10 .Here , u k denotes the Fourier transform of velocity fluctuations at scale k −1 . When the angle θ = arccos ( k · v 0 ) / | k | | v 0 | between the wavevector k and the huge - scale stream u 0 is tiny , i . e . , θ [UNK] 1 , the power flux Π [UNK] k −2 / 3 sin 2 / 3 θ 11 .On the contrary , if θ becomes large , then Π decreases quickly because of the cancellation effect 12 . Similar behaviors have been observed in magnetohydrodynamics ( MHD ) , where the power flux Π",
        "rewrite_text": "**Title:** Nonlocal Phenomenology for Anisotropic MHD Turbulence\n\n**Abstract:** In this study, we investigate the nonlocal phenomenology associated with magnetohydrodynamic (MHD) turbulence characterized by significant magnetic force anisotropy, particularly in the context of solar wind and space plasmas. Our findings indicate that the frequency of power transfer across different scales can be effectively described by a straightforward equation that relies solely on local nonlinear interactions, but this holds true only when the directions of the wavevectors are either aligned or anti-aligned with the mean magnetic force direction. In scenarios where oblique waves are present, we observe that nonlocal effects become increasingly important. The insights gained from this research may enhance our understanding of turbulent transport mechanisms in astrophysical plasma environments. \n\nTurbulence is a fundamental phenomenon that influences a wide array of natural processes, spanning from geophysical systems to fusion science. Recent studies have revealed that various types of turbulent waves exhibit universal empirical characteristics, including Kolmogorov scaling, intermittency, and anomalous dissipation. Notably, the statistical behavior of fully developed turbulence is significantly influenced by the rate at which energy cascades through the inertial range. This cascade process encompasses both linear and nonlinear interactions among different modes at varying wavenumbers. In hydrodynamic systems, for example, the power flux, denoted as Π(h) = < |δu_k · δu*_−k|^2 > / < u^2_k >, is dependent not only on the magnitude of the wavenumber k but also on its orientation relative to the small-scale flow. Specifically, when the angle θ = arccos(k · v_0) / |k||v_0| between the wavevector k and the large-scale flow u_0 is small, the power flux behaves as Π ∝ k^(-2/3) sin^(2/3)(θ). Conversely, as θ increases, the power flux diminishes rapidly due to cancellation effects. Similar patterns have been observed in MHD systems, where the power flux exhibits analogous dependencies.",
        "ori-fast-z-score": -1.7962924780409972,
        "water-fast-z-score": 5.388877434122992,
        "rewrite-fast-z-score": 2.4659848095803594
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rates and Characteristics of Intermediate Mass Ratio Inspirals Detectable by Advanced LIGO .\nAbstract:\nWe present the results of an analysis to determine rates, masses, spins, and luminosities for intermediate mass ratio inspiral (IMRI) events detectable with advanced gravitational wave detectors such as Advanced LIGO. We use Monte Carlo simulations to generate IMRIs in galactic binaries that are consistent with current observations of binary pulsars and X-ray binaries. The simulated systems evolve through three phases: detached phase, Roche lobe overflow phase, and common envelope phase. In our simulation we assume that all stars have solar metallicity and initial spin periods of 10 days. For each system generated, we calculate its signal-to-noise ratio using the stationary phase approximation. We find that there will be about one event per year within 100 Mpc with signal-to-noise ratios greater than 8. This is comparable to the rate expected for double neutron star mergers. However, unlike double neutron star mergers which occur at high redshifts, most IMRI events should be detected nearby.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rates and Characteristics of Intermediate Mass Ratio Inspirals Detectable by Advanced LIGO . Abstract : We report the results of an assessment to obtain rates , masses , spins , and luminosities for intermediate mass ratio inspiral ( IMRI ) events detectable with advanced gravitational wave detectors such as Advanced LIGO .We use Monte Carlo simulations to produce IMRIs in galactic binaries that are compatible with current observations of binary pulsars and X - ray binaries . The simulated systems develop through three stages : detached phase , Roche lobe overflow phase , and shared envelope phase .In our model we suppose that all stars have solar metallicity and original spin intervals of 10 days . For each system generated , we determine its signal - to - noise proportion using the stationary phase approximation .We see that there will be about one incident per year within 100 Mpc with signal - to - noise ratios greater than 8 . This is analogous to the speed anticipated for double neutron star mergers .However , unlike dual neutron star mergers which occur at high redshifts , most IMRI events should be identified nearby .",
        "rewrite_text": "We present a comprehensive analysis of the rates, masses, spins, and luminosities associated with intermediate mass ratio inspirals (IMRIs) that can be detected by advanced gravitational wave observatories such as Advanced LIGO. Utilizing Monte Carlo simulations, we generate IMRI events within galactic binary systems that align with existing observations of binary pulsars and X-ray binaries. Our simulations encompass three distinct evolutionary phases: the detached phase, the Roche lobe overflow phase, and the shared envelope phase. In our framework, we assume that all stars possess solar metallicity and initial spin periods of 10 days. For each generated binary system, we calculate the signal-to-noise ratio (SNR) using the stationary phase approximation. Our findings indicate that approximately one detectable IMRI event per year is expected within a distance of 100 Mpc, with SNRs exceeding 8. This detection rate is comparable to that anticipated for double neutron star mergers. However, a key distinction is that while double neutron star mergers are typically observed at higher redshifts, the majority of IMRI events are expected to occur in closer proximity. This research enhances our understanding of IMRIs and their potential for detection, providing valuable insights for future gravitational wave observations and the study of binary stellar evolution.",
        "ori-fast-z-score": 0.22645540682891913,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": 1.116880781646981
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What Can be Learned Studying the Distribution of the Biggest Fragment ? .\nAbstract:\nWe study the distribution of the largest fragment in a fragmentation process, which is an important quantity for understanding how to optimize data transmission over networks and storage systems.  We show that this distribution can be approximated by a power law with exponent 1 + 1/(2 - p), where p is the probability that two fragments merge into one when they are adjacent on disk (the so-called  mergeability parameter ). This result explains why previous studies have found different exponents depending on whether or not merging was allowed. The approximation also allows us to derive closed-form expressions for other quantities such as the mean size of the largest fragment and its variance. Finally, we use our results to explain some recent experimental findings about file sizes in peer-to-peer systems. In many applications involving data transmission over networks and distributed storage systems, it is useful to understand how large the largest fragment will become during the course of the system s evolution. For example, if a network node has to transmit a certain amount of information within a given time limit, then knowing what fraction of the total data needs to be transmitted at any point in time may help improve performance. Similarly, in distributed storage systems, knowing the expected size of the largest fragment helps determine how much space each node should reserve for storing replicas.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : What Can be Learned Studying the Distribution of the Biggest Fragment ? .Abstract : We consider the distribution of the greatest fragment in a fragmentation process , which is an important quantity for knowledge how to optimize data communication over networks and storage systems . We see that this distribution can be approximated by a power law with exponent 1 + 1 / ( 2 - p ) , where p is the probability that two fragments join into one when they are adjoining on disk ( the so - called mergeability coefficient ) .This result provides why previous research have discovered different exponents depending on whether or not merging was allowed . The approximation also enables us to derive closed - form expressions for other quantities such as the mean size of the greatest fragment and its variance .Finally , we utilize our findings to explain some latest empirical results about file sizes in peer - to - peer systems . In many applications using data communication over networks and distributed storage systems , it is important to realize how large the greatest fragment will become during the course of the program s evolution .For instance , if a network node has to transmit a certain quantity of information within a given time limitation , then understanding what fraction of the total data needs to be transmitted at any point in time might help increase efficiency . Similarly , in distributed storage systems , knowing the expected size of the greatest fragment allows determine how many space each node should reserve for storing replicas .",
        "rewrite_text": "**Title:** Insights from Analyzing the Distribution of the Largest Fragment\n\n**Abstract:** This study investigates the distribution of the largest fragment resulting from a fragmentation process, a crucial aspect for enhancing data communication efficiency across networks and storage systems. We demonstrate that this distribution can be effectively modeled by a power law characterized by an exponent of 1 + 1 / (2 - p), where p represents the probability that two adjacent fragments will merge when located on disk, known as the mergeability coefficient. This finding elucidates why prior studies have reported varying exponents based on the allowance of merging during fragmentation. Furthermore, our approximation facilitates the derivation of closed-form expressions for additional metrics, including the mean size and variance of the largest fragment. \n\nWe apply our theoretical results to interpret recent empirical observations regarding file sizes in peer-to-peer systems. Understanding the potential size of the largest fragment throughout the evolution of a program is vital for numerous applications involving data transmission over networks and distributed storage solutions. For example, when a network node is tasked with transmitting a specific volume of data within a constrained timeframe, insights into the proportion of total data that must be sent at any given moment can significantly enhance operational efficiency. Similarly, in distributed storage frameworks, knowledge of the anticipated size of the largest fragment aids in determining the appropriate storage capacity each node should allocate for data replication. Our findings not only contribute to the theoretical understanding of fragmentation processes but also offer practical implications for optimizing data management strategies in contemporary computing environments.",
        "ori-fast-z-score": 1.975658322294524,
        "water-fast-z-score": 7.305140612272933,
        "rewrite-fast-z-score": 1.7149858514250882
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The VIMOS VLT Deep Survey. The Assembly History of the Stellar Mass in Galaxies: from the Young to the Old Universe .\nAbstract:\nThe assembly history of galaxies is one of the most important open questions in modern astrophysics, and it has been studied extensively using deep surveys at different wavelengths (e.g., optical/near-infrared). In this work we present an analysis of the stellar mass assembly for a sample of more than 100 000 massive galaxies selected from the VVDS-02h field observed with the VIMOS spectrograph on board of the Very Large Telescope between 0 < z < 5. We use a novel technique based on the combination of photometric redshifts and spectral energy distribution fitting to derive accurate estimates of galaxy masses over such large redshift range. Our results show that the evolution of the average stellar mass density can be described by two main phases: i) a rapid increase up to z ~ 2 followed by ii) a slower growth phase until today. This behaviour is consistent with previous studies but our data allow us to study in detail how the build-up of stellar mass proceeds as a function of galaxy properties like colour or morphology.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The VIMOS VLT Deep Survey.The Assembly History of the Stellar Mass in Galaxies: from the Young to the Old Universe .Abstract : The assembly history of stars is one of the most important open questions in modern astrophysics , and it has been studied frequently using deep surveys at different wavelengths ( e . g . , optical / near - infrared ) . In this research we present an assessment of the stellar mass assembly for a sample of more than 100 000 massive galaxies chose from the VVDS - 02h field measured with the VIMOS spectrograph on board of the Very Large Telescope between 0 < z < 5 .We use a new technique based on the combination of photometric redshifts and spectral power distribution fitting to derive exact predictions of galaxy masses over such large redshift range . Our results show that the evolution of the average stellar mass density can be described by two principal phases : i ) a rapid increase up to z ~ 2 followed by ii ) a slower growth phase until today .This behaviour is consistent with previous research but our statistics permit us to study in detail how the build - up of stars mass proceeds as a function of galaxy structures like colour or morphology .",
        "rewrite_text": "Title: The VIMOS VLT Deep Survey: Tracing the Assembly History of Stellar Mass in Galaxies from the Young to the Old Universe\n\nAbstract: Understanding the assembly history of stellar mass in galaxies is a pivotal challenge in contemporary astrophysics. This study leverages deep surveys conducted across various wavelengths, particularly optical and near-infrared, to investigate this phenomenon. We focus on a comprehensive analysis of stellar mass assembly for over 100,000 massive galaxies selected from the VVDS-02h field, utilizing data collected with the VIMOS spectrograph on the Very Large Telescope, covering a redshift range of 0 < z < 5. Employing an innovative methodology that integrates photometric redshifts with spectral power distribution fitting, we are able to accurately estimate galaxy masses across this extensive redshift spectrum. Our findings reveal that the evolution of average stellar mass density can be characterized by two distinct phases: a rapid increase in stellar mass density up to z ~ 2, followed by a more gradual growth phase extending to the present day. This evolutionary pattern aligns with previous studies; however, our extensive dataset allows for a more nuanced examination of how stellar mass assembly varies with galaxy properties such as color and morphology. By dissecting these relationships, we provide deeper insights into the mechanisms driving stellar mass growth and the structural evolution of galaxies throughout cosmic history. This research not only enhances our understanding of galaxy formation and evolution but also contributes to the broader discourse on the lifecycle of stars in the universe.",
        "ori-fast-z-score": 0.741998516004452,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": 1.5118578920369088
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What is the order of 2D polymer escape transition? .\nAbstract:\nWe study the escape dynamics of an elastic string in two dimensions confined by a potential well with periodic boundary conditions and subject to thermal noise. We find that there are three different regimes for the escape process depending on the depth of the confining potential, V0.  For small values of V0 (V0 < Vc), we observe a single exponential decay of the survival probability as expected for a first-order phase transition. In this regime, the mean lifetime decreases exponentially with increasing temperature T . For intermediate values of V0 ( Vc < V0 < Vs ), we observe a double-exponential decay of the survival probability indicating a second-order phase transition. The mean lifetime increases linearly with decreasing temperature below Tc , where Tc is the critical temperature at which the system undergoes a continuous phase transition into the metastable state. Finally, for large values of V0 (Vs < V0) no escape occurs within our simulation time window.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : What is the order of 2D polymer escape transition ? .Abstract : We research the escape behavior of an elastic string in two dimensions confined by a potential well with periodic border conditions and subject to heat sound . We see that there are three different regimes for the escape process depending on the depth of the confining potential , V0 .For small values of V0 ( V0 < Vc ) , we witness a single exponential decay of the survival likelihood as anticipated for a first - order phase transfer . In this regime , the mean lifetime decreases exponentially with rising heat T .For intermediate values of V0 ( Vc < V0 < Vs ) , we witness a double - exponential decay of the survival likelihood implying a second - order phase change . The mean lifetime rises linearly with decreasing temperature below Tc , where Tc is the important temperature at which the system undergoes a periodic phase change into the metastable state .Finally , for large values of V0 ( Vs < V0 ) no exit occurs within our model time frame .",
        "rewrite_text": "Title: What is the Order of the 2D Polymer Escape Transition?\n\nAbstract: This study investigates the escape dynamics of an elastic string confined within a two-dimensional potential well characterized by periodic boundary conditions and influenced by thermal fluctuations. Our findings reveal three distinct regimes of escape behavior that depend on the depth of the confining potential, denoted as V0. In the first regime, where V0 is less than a critical value (V0 < Vc), we observe a single exponential decay in the survival probability, indicative of a first-order phase transition. In this scenario, the mean lifetime of the system decreases exponentially as the temperature increases. Transitioning to the second regime, characterized by intermediate values of V0 (Vc < V0 < Vs), we identify a double-exponential decay in survival likelihood, suggesting a second-order phase transition. Here, the mean lifetime exhibits a linear increase as the temperature drops below a critical threshold (Tc), which marks the temperature at which the system transitions into a metastable state. In the final regime, where V0 exceeds a certain threshold (Vs < V0), the model predicts that escape does not occur within the observed time frame, indicating a complete confinement of the elastic string. This research provides valuable insights into the nature of phase transitions in two-dimensional systems and enhances our understanding of polymer dynamics under varying potential conditions.",
        "ori-fast-z-score": -1.5460413650478515,
        "water-fast-z-score": 3.4914862437758782,
        "rewrite-fast-z-score": 0.7770286898858113
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-adiabatic Effects in the Dissociation of Oxygen Molecules at the Al(111) Surface .\nAbstract:\nWe report on non-adiabatic effects in dissociative oxygen adsorption and desorption processes occurring at low temperatures (<100 K). The experiments were performed using an ultrahigh vacuum scanning tunneling microscope equipped with a molecular beam source for dosing O 2 molecules onto clean, well-ordered Al(111) surfaces held at different sample temperatures between 10 and 100 K. We find that the sticking probability decreases strongly when increasing the surface temperature due to thermal activation of vibrational modes which lead to non-collinearity of electronic states involved in the reaction process. This effect is also observed during the subsequent desorption of atomic oxygen from the surface. In addition we observe a pronounced dependence of the sticking coefficient on the kinetic energy of incident oxygen molecules: At high energies (>500 meV), where the molecule-surface interaction time becomes comparable or even shorter than typical vibrational periods, the sticking probability increases again as compared to lower kinetic energies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - adiabatic Effects in the Dissociation of Oxygen Molecules at the Al ( 111 ) Surface . Abstract : We report on non - adiabatic effects in dissociative oxygen adsorption and desorption pathways occurring at low temperatures ( < 100 K ) .The studies were performed using an ultrahigh vacuum scanning tunneling microscope equipped with a molecular beam source for dosing O 2 atoms onto cool , highly - ordered Al ( 111 ) surfaces holding at different specimen temperatures between 10 and 100 K . We see that the sticking likelihood falls strongly when increasing the surface temperature owing to heat activation of vibrational modes which cause to non - collinearity of electronic states participating in the process process . This phenomenon is also observed during the subsequent desorption of atomic oxygen from the surface .In addition we study a noticeable dependence of the sticking coefficient on the kinetic power of incident oxygen molecules : At high energies ( > 500 meV ) , where the molecule - surface interaction rate gets comparable or especially shorter than typical vibrational intervals , the sticking likelihood grows again as compared to higher kinetic energies .",
        "rewrite_text": "Title: Non-Adiabatic Effects in the Dissociation of Oxygen Molecules at the Al (111) Surface\n\nAbstract: This study investigates the non-adiabatic effects influencing the dissociative adsorption and desorption of oxygen molecules on aluminum (Al) (111) surfaces at low temperatures (below 100 K). Utilizing an ultrahigh vacuum scanning tunneling microscope integrated with a molecular beam source, we conducted experiments by dosing O2 molecules onto well-ordered Al (111) surfaces maintained at various temperatures ranging from 10 K to 100 K. Our findings reveal a significant decrease in the sticking probability of oxygen molecules as the surface temperature increases. This reduction is attributed to the thermal activation of vibrational modes, which leads to a non-collinear arrangement of the electronic states involved in the adsorption process. A similar trend is observed during the desorption of atomic oxygen from the surface, indicating that non-adiabatic effects play a crucial role in both adsorption and desorption mechanisms. Furthermore, we explored the dependence of the sticking coefficient on the kinetic energy of the incoming oxygen molecules. At elevated energies (greater than 500 meV), where the interaction time between the molecules and the surface becomes comparable to or shorter than the typical vibrational periods, we observed an increase in the sticking probability relative to higher kinetic energies. These results provide valuable insights into the complex dynamics of oxygen molecule interactions with metal surfaces, highlighting the importance of non-adiabatic effects in low-temperature environments. This research contributes to a deeper understanding of surface chemistry and may have implications for various applications, including catalysis and surface engineering.",
        "ori-fast-z-score": -0.21081851067789195,
        "water-fast-z-score": 5.7655666019705505,
        "rewrite-fast-z-score": 2.9755097944025266
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mean-Field Magnetohydrodynamics of Accretion Disks .\nAbstract:\nWe present the results of our numerical simulations of magnetized accretion disks in which we solve the mean-field magnetohydrodynamic equations for an axisymmetric disk with a prescribed radial distribution of angular momentum and mass fluxes, using the shearing-box approximation.  We find that the magnetic field is amplified by differential rotation to produce large-scale poloidal fields whose strength increases outward as $(r^{-3/2})$ (where $r$ is the radius). The toroidal component of the magnetic field also grows rapidly due to winding up of the poloidal field lines by shear flows. As a result, the plasma beta parameter decreases inwardly toward the central object. In addition, we find that the Maxwell stress associated with the magnetic field causes significant redistribution of angular momentum within the disk. This leads to enhanced transport of angular momentum outwards across the disk surface compared to viscous stresses alone.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mean - Field Magnetohydrodynamics of Accretion Disks . Abstract : We present the results of our numerical simulations of magnetized accretion disks in which we solve the mean - field magnetohydrodynamic equations for an axisymmetric disk with a prescribed radial distribution of angular velocity and mass fluxes , using the shearing - box equation .We see that the magnetic force is amplified by differential rotation to produce wide - scale poloidal fields whose strength increases outward as $ ( r ^ { - 3 / 2 } ) $ ( where $ r $ is the radius ) . The toroidal component of the magnetic field also grows fast due to winding up of the poloidal field lines by shear flows .As a result , the plasma beta function decreases inwardly toward the main object . In addition , we find that the Maxwell stress involved with the magnetic force produces significant redistribution of angular velocity within the disk .This leads to greater transport of angular velocity outwards across the disk boundary compared to viscous stresses alone .",
        "rewrite_text": "In this study, we investigate the dynamics of magnetized accretion disks through numerical simulations that solve the mean-field magnetohydrodynamic (MHD) equations. Our focus is on an axisymmetric disk characterized by a specified radial distribution of angular velocity and mass fluxes, employing the shearing-box approximation. Our findings reveal that the magnetic forces within the disk are significantly enhanced by differential rotation, resulting in the generation of extensive poloidal magnetic fields. Notably, the strength of these fields exhibits an outward increase following a radial dependence of \\( r^{-3/2} \\), where \\( r \\) denotes the radius of the disk. Concurrently, the toroidal component of the magnetic field experiences rapid growth, attributed to the winding of poloidal field lines induced by shear flows. This interplay leads to a decrease in the plasma beta function as one moves inward toward the central object of the disk.\n\nMoreover, our analysis indicates that the Maxwell stress associated with the magnetic forces plays a crucial role in the redistribution of angular velocity throughout the disk. This effect is particularly significant, as it facilitates a more pronounced outward transport of angular velocity across the disk's boundary compared to the transport driven solely by viscous stresses. These results underscore the importance of magnetic fields in the dynamics of accretion disks, highlighting their influence on angular momentum transport and the overall behavior of the disk. Our work contributes to a deeper understanding of the complex interactions within magnetized accretion disks, which are prevalent in various astrophysical contexts, including the environments surrounding black holes and neutron stars.",
        "ori-fast-z-score": 1.6666666666666667,
        "water-fast-z-score": 5.079850199442941,
        "rewrite-fast-z-score": 2.2980970388562794
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tree-Level Stability Without Spacetime Fermions: Novel Examples in String Theory .\nAbstract:\nWe present novel examples of tree-level stable non-BPS D-branes in string theory, which are not associated with spacetime fermion zero modes and thus do not require the presence of orientifolds or other sources for tadpole cancellation. We show that these brane configurations can be constructed by wrapping unstable D-branes on supersymmetric cycles in Calabi-Yau threefolds. The resulting BPS states preserve half of the original supersymmetry but carry no net charge under any gauge group factor. These results provide new insights into the structure of moduli spaces of vacua in string theory. Introduction: In recent years there has been considerable interest in studying non-BPS D-brane (NBD) configurations in type II string theories  1  . NBDs have attracted attention because they may play an important role in understanding various phenomena such as tachyon condensation  2  , open-string pair production  3  , and black hole entropy  4  .\nIn this work we will focus our attention on NBDs whose stability is due to worldsheet instanton effects  5  -  8  rather than spacetime fermion zero-modes  9  . Such NBDs were first studied in  10  where it was shown that certain wrapped D3-branes could become stable at one-loop order without requiring the presence of orientifold planes  11  . Subsequently, several authors  12  -  16  have considered similar constructions involving different types of D-branes and compactifications. However, all of these works required some form of tadpole cancellation  17  so that the total RR-charge carried by the configuration vanishes. Tadpole cancellation conditions place strong constraints on the allowed values of fluxes and charges in the background geometry  18  . It would therefore be interesting if one could find examples of stable NBDs which did not require the presence of additional sources for tadpole cancellations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tree - Level Stability Without Spacetime Fermions : Novel Examples in String Theory . Abstract : We introduce novel instances of forest - level stable non - BPS D - branes in string theory , which are not associated with spacetime fermion zero modes and therefore do not require the presence of orientifolds or other sources for tadpole cancellation .We see that these brane configurations can be built by wrapping unstable D - branes on supersymmetric cycles in Calabi - Yau threefolds . The resulting BPS states preserve half of the original supersymmetry but hold no net charge under any gauge group factor .These data provide novel knowledge into the formation of moduli spaces of vacua in string theory . Introduction : In recent years there has been substantial interest in investigating non - BPS D - brane ( NBD ) arrangements in type II string theories 1 .NBDs have garnered attention because they may play an important role in understanding various phenomena such as tachyon condensation 2 , open - string pair production 3 , and dark hole entropy 4 . In this research we will focus our focus on NBDs whose stability is due to worldsheet instanton effects 5 - 8 instead than spacetime fermion zero - modes 9 .Such NBDs were first investigated in 10 where it was shown that particular tangled D3 - branes might remain stable at one - loop order without need the presence of orientifold planes 11 . Subsequently , various scientists 12 - 16 have suggested different constructions concerning diverse kinds of D - branes and compactifications .However , all of these works involved some kind of tadpole cancellation 17 so that the total RR - charge transferred by the configuration vanishes . Tadpole cancellation conditions place powerful restrictions on the allowed values of fluxes and charges in the background geometry 18 .It would therefore be attractive if one could discover examples of stable NBDs which did not require the presence of new sources for tadpole cancellations .",
        "rewrite_text": "In this article, we present innovative examples of tree-level stable non-BPS D-branes within the framework of string theory, which are notable for their independence from spacetime fermion zero modes. This characteristic eliminates the necessity for orientifolds or other mechanisms typically employed for tadpole cancellation. Our findings reveal that these stable brane configurations can be constructed by wrapping unstable D-branes around supersymmetric cycles in Calabi-Yau threefolds. The resulting BPS states maintain half of the original supersymmetry while exhibiting no net charge under any gauge group factor. This research contributes valuable insights into the formation of moduli spaces of vacua in string theory.\n\nThe exploration of non-BPS D-brane (NBD) configurations in type II string theories has gained significant traction in recent years. NBDs are of particular interest due to their potential implications for understanding various phenomena, including tachyon condensation, open-string pair production, and dark hole entropy. Our study specifically emphasizes NBDs whose stability arises from worldsheet instanton effects, rather than relying on spacetime fermion zero modes. Previous investigations have highlighted that certain tangled D3-branes can achieve stability at one-loop order without necessitating orientifold planes. Following this, numerous researchers have proposed various constructions involving different types of D-branes and compactifications. However, these prior works have invariably involved some form of tadpole cancellation, which imposes stringent constraints on the permissible values of fluxes and charges in the underlying geometry. Therefore, the discovery of stable NBDs that do not require additional sources for tadpole cancellation represents a significant advancement in the field, opening new avenues for research and understanding in string theory.",
        "ori-fast-z-score": -1.5549631660464482,
        "water-fast-z-score": 5.895067838245651,
        "rewrite-fast-z-score": 1.3310347641241707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CP Violation From Standard Model to Strings .\nAbstract:\nThe standard model (SM) is the most successful theory in particle physics, but it has some problems such as hierarchy problem and CP violation. In this talk I will discuss how we can solve these problems by using string theories. First let us consider the SM with three generations of quarks and leptons. The Yukawa couplings are given by \nwhere  is the Higgs vacuum expectation value,  is the mass matrix for fermions ,  is the CKM mixing matrix , and  is the Kobayashi-Maskawa(KM) matrix . We have two parameters in the KM matrix : one phase which causes CP violation and another parameter called Jarlskog invariant J = Im VudVub * / Re VudIm Vub .\nIn order to explain the observed CP violation in K meson system, we need at least one complex number in the KM matrix. However there are only four real numbers in the Yukawa coupling matrices. This means that we cannot determine all elements of the KM matrix uniquely. Therefore we introduce extra dimensions into our models so that we can obtain more degrees of freedom.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CP Violation From Standard Model to Strings . Abstract : The standard model ( SM ) is the most popular concept in particle theory , but it has some problems such as hierarchy problem and CP violation .In this talk I will explore how we can answer these problems by using string theories . First let us consider the SM with three generations of quarks and leptons .The Yukawa couplings are given by where is the Higgs vacuum expectation value , is the mass vector for fermions , is the CKM mixing function , and is the Kobayashi - Maskawa ( KM ) matrix . We have two parameters in the KM matrix : one phase which causes CP violation and another parameter called Jarlskog invariant J = Im VudVub * / Re VudIm Vub .In order to explain the observed CP violation in K meson scheme , we require at least one complex number in the KM matrix . However there are only four real numbers in the Yukawa interaction vectors .This implies that we cannot determine all components of the KM matrix uniquely . Therefore we incorporate additional dimensions into our designs so that we can obtain more degrees of freedom .",
        "rewrite_text": "**Title: CP Violation: Transitioning from the Standard Model to String Theory**\n\n**Abstract:** The Standard Model (SM) of particle physics remains the predominant framework for understanding fundamental interactions; however, it is not without its shortcomings, notably the hierarchy problem and the issue of CP violation. This presentation delves into potential resolutions to these challenges through the lens of string theory. We begin by examining the SM, which encompasses three generations of quarks and leptons, and the associated Yukawa couplings. These couplings are expressed in terms of the Higgs vacuum expectation value, the fermion mass vector, the Cabibbo-Kobayashi-Maskawa (CKM) mixing matrix, and the Kobayashi-Maskawa (KM) matrix itself. Within the KM matrix, we identify two critical parameters: a complex phase responsible for CP violation and the Jarlskog invariant, denoted as J = Im(V_ud V^*_{ub}) / Re(V_ud) Im(V_{ub}). To account for the observed CP violation in the K meson system, it is essential to introduce at least one complex parameter into the KM matrix. However, the Yukawa interaction vectors contain only four real parameters, which limits our ability to uniquely determine all components of the KM matrix. This limitation prompts us to consider the incorporation of additional dimensions within our theoretical framework, thereby providing the necessary degrees of freedom to address these issues. By exploring the implications of string theory, we aim to offer a more comprehensive understanding of CP violation and its connection to the fundamental structure of particle physics, potentially paving the way for new insights into the underlying principles governing the universe.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.299831645537222,
        "rewrite-fast-z-score": 0.7844645405527362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effect of magnetic field on spontaneous Fermi surface symmetry breaking .\nAbstract:\nWe study the effect of an external magnetic field on the ground state properties in the Hubbard model with next-nearest neighbor hopping and spin-orbit coupling, which is relevant to iron-based superconductors. We show that the magnetic field induces a spontaneous breaking of the time-reversal symmetry at half-filling for any finite value of the spin-orbit coupling strength. The broken symmetry phase has two-fold degenerate energy bands and shows non-Fermi liquid behavior. In addition, we find that there exists another spontaneously-broken-symmetry phase without gapless excitations when the chemical potential lies between the upper and lower band edges. This phase also exhibits non-Fermi liquid behaviors. Finally, we discuss possible experimental consequences of our results. Introduction:-The discovery of high-Tc FeAs-based superconductors  1  has attracted much attention because they are believed to be unconventional  2  . It was found experimentally  3  that these materials have strong spin orbit (SO) interaction  4  , which leads to several interesting phenomena such as nematic order  5  , orbital ordering  6  , and anisotropic magnetoresistance  7  .\nIn this Letter, we consider the following extended Hubbard model: \nwhere c†iσ(ciσ) creates (annihilates) an electron with spin σ =↑ or ↓ at site i, nαβij= c † αji c βji denotes the density matrix element between sites j and i, t represents nearestneighbor hopping amplitude, t  stands for nextnearest-neighbor hopping amplitude, U is the local Coulomb repulsion, λ is the SO coupling constant, µ is the chemical potential, and B is the applied magnetic field along z-direction. Hereafter, we set t=1, t =0.3t, U=4t, and λ=0.1t unless otherwise stated  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effect of magnetic force on spontaneous Fermi surface symmetry breaking . Abstract : We research the impact of an external magnetic force on the ground state properties in the Hubbard theory with next - nearest neighbor hopping and spin - orbit bonding , which is relevant to iron - based superconductors .We see that the magnetic force induces a spontaneous breaking of the period - reversal symmetry at half - filling for any finite value of the spin - orbit interaction strength . The broken symmetry phase has two - fold degenerate power bands and shows non - Fermi solid behavior .In addition , we find that there exists another spontaneously - breaking - symmetry phase without gapless excitations when the chemical potential sits between the inner and upper band edges . This phase additionally exhibits non - Fermi solid behaviors .Finally , we explain possible experimental impacts of our findings . Introduction : - The discovery of high - Tc FeAs - based superconductors 1 has garnered many scrutiny because they are considered to be unconventional 2 .It was shown experimentally 3 that these materials have stable spinning orbit ( SO ) bonding 4 , which results to several interesting phenomena such as nematic order 5 , orbital ordering 6 , and anisotropic magnetoresistance 7 . In this Letter , we suppose the following modified Hubbard theory : where c † iσ ( ciσ ) produces ( annihilates ) an electron with spin σ = ↑ or [UNK] at site i , nαβij = b † αji c βji denotes the density matrix element between locations j and i , t refers nearestneighbor hopping frequency , t stands for nextnearest - neighbor hopping frequency , U is the local Coulomb repulsion , λ is the SO coupling constant , µ is the chemical potential , and B is the applied magnetic force along z - direction .Hereafter , we set t = 1 , t = 0 . 3t , U = 4t , and λ = 0 . 1t unless otherwise stated 8 .",
        "rewrite_text": "**Title:** Effect of Magnetic Force on Spontaneous Fermi Surface Symmetry Breaking\n\n**Abstract:** This study investigates the influence of an external magnetic force on the ground state characteristics within the framework of the Hubbard model, incorporating next-nearest neighbor hopping and spin-orbit coupling, which are particularly relevant to iron-based superconductors. Our findings reveal that the application of a magnetic force leads to a spontaneous breaking of the period-reversal symmetry at half-filling, regardless of the finite strength of the spin-orbit interaction. The resulting broken symmetry phase is characterized by two-fold degenerate power bands and exhibits non-Fermi liquid behavior. Furthermore, we identify an additional phase that spontaneously breaks symmetry without gapless excitations when the chemical potential is positioned between the inner and upper band edges. This phase also demonstrates non-Fermi liquid characteristics. We conclude by discussing the potential experimental implications of our results, which may provide insights into the underlying mechanisms of superconductivity in these materials.\n\n**Introduction:** The emergence of high-temperature superconductors based on iron arsenides has attracted significant attention due to their unconventional properties. Experimental studies have confirmed the presence of stable spin-orbit coupling in these materials, leading to various intriguing phenomena, including nematic order, orbital ordering, and anisotropic magnetoresistance. In this letter, we propose a modified version of the Hubbard model, where the operators \\( c^{\\dagger}_{i\\sigma} \\) and \\( c_{i\\sigma} \\) create and annihilate electrons with spin \\( \\sigma = \\uparrow \\) or \\( \\downarrow \\) at site \\( i \\), respectively. The density matrix element between sites \\( j \\) and \\( i \\) is denoted by \\( n_{\\alpha\\beta}^{ij} = b^{\\dagger}_{\\alpha j} c_{\\beta i} \\). The model parameters include the nearest-neighbor hopping amplitude \\( t \\), the next-nearest-neighbor hopping amplitude \\( t' \\), the local Coulomb repulsion \\( U \\), the spin-orbit coupling constant \\( \\lambda \\), the chemical potential \\( \\mu \\), and the applied magnetic field \\( B \\) along the z-direction. For our calculations, we set \\( t = 1 \\), \\( t' = 0.3t \\), \\( U = 4t \\), and \\( \\lambda = 0.1t \\) unless specified otherwise.",
        "ori-fast-z-score": -0.25630729731502827,
        "water-fast-z-score": 6.810052246069989,
        "rewrite-fast-z-score": 0.5449492609130661
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Pair of Bootes: A New Milky Way Satellite .\nAbstract:\nWe report the discovery of a new satellite galaxy, dubbed  A pair of bootes  (ApoBootes), orbiting around our Galaxy at a distance of about 300 kpc in projection and with an estimated mass of 1.5 x 10^10 M_sun . ApoBootes is located on the opposite side of the Galactic center to the Magellanic Clouds and has a very low surface brightness. We have used deep near-infrared images taken by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this object. The photometric properties are consistent with those expected for a dwarf spheroidal galaxy. This work was supported by the Australian Research Council Discovery Project funding scheme under grant DP130104011. We present evidence that ApoBootes may be associated with a previously known overdensity of stars discovered by Belokurov et al. (2007) using SDSS data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Pair of Bootes : A New Milky Way Satellite . Abstract : We report the discovery of a new satellite galaxy , nicknamed A couple of bootes ( ApoBootes ) , orbiting around our Galaxy at a length of about 300 kpc in projection and with an estimated mass of 1 . 5 x 10 ^ 10 M _ sun .ApoBootes is situated on the opposite end of the Galactic center to the Magellanic Clouds and has a very low exterior brightness . We have utilized deep near - infrared images obtained by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this body .The photometric properties are compatible with those expected for a dwarf spheroidal galaxy . This research was supported by the Australian Research Council Discovery Project money scheme under grant DP130104011 .We present evidence that ApoBootes might be identified with a previously known overdensity of stars found by Belokurov et al . ( 2007 ) using SDSS information .",
        "rewrite_text": "We present the discovery of a new satellite galaxy, designated as A couple of Bootes (ApoBootes), which orbits our Milky Way at a projected distance of approximately 300 kiloparsecs. This satellite has an estimated mass of 1.5 x 10^10 solar masses and is located on the opposite side of the Galactic center from the Magellanic Clouds. Notably, ApoBootes exhibits a very low surface brightness, making it a challenging target for observation. Our identification of this galaxy was facilitated by deep near-infrared imaging conducted with the VISTA telescope, as part of the Vista Variables in the Via Lactea survey. The photometric characteristics of ApoBootes align with those typically associated with dwarf spheroidal galaxies, suggesting that it may represent a new addition to the population of such satellites surrounding the Milky Way. Furthermore, we provide evidence that ApoBootes may correspond to a previously identified stellar overdensity reported by Belokurov et al. (2007) based on data from the Sloan Digital Sky Survey (SDSS). This research was made possible through funding from the Australian Research Council Discovery Project scheme, under grant DP130104011. Our findings contribute to the growing understanding of the structure and composition of the Milky Way's satellite system, highlighting the significance of ApoBootes in the context of galactic formation and evolution.",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 4.131182235954578,
        "rewrite-fast-z-score": -0.2182178902359924
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two? .\nAbstract:\nWe present the results of an analysis of rise times for a sample of type Ia supernovae (SNeIa) with well-measured light curves and redshifts in order to determine whether there is one mode of SNeIa rise time, as suggested by Phillips et al. (1999), or two modes, as suggested by Riess et al. (1999). We find that our data are consistent with either model at the 2-sigma level; however, we cannot rule out the possibility of only one mode being present. The best-fit values for the parameters describing each distribution differ significantly between these models. If future observations confirm this result, it will have important implications on cosmological studies using SNeIa as distance indicators. In particular, if there really are two populations of SNeIa, then the use of a single value for the stretch parameter may lead to systematic errors in determining distances. \n \n Keywords: Supernova, Light curve",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two?.Abstract : We present the results of an assessment of rise periods for a sample of type Ia supernovae ( SNeIa ) with good - measured light curves and redshifts in order to find whether there is one mode of SNeIa rise history , as suggested by Phillips et al . ( 1999 ) , or two modes , as suggested by Riess et al .( 1999 ) . We see that our statistics are compatible with either model at the 2 - sigma level ; however , we cannot leave out the suggestion of only one mode being present .The best - fitting values for the variables describing each distribution depend greatly between these models . If future discoveries confirm this result , it will have important implications on cosmological experiments using SNeIa as distance indicators .In particular , if there really are two groups of SNeIa , then the using of a single value for the stretch variable might lead to systematic errors in calculating distances . Keywords : Supernova , Light curve",
        "rewrite_text": "**Title:** Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two?\n\n**Abstract:** In this study, we investigate the rise times of a sample of Type Ia supernovae (SNeIa) characterized by well-measured light curves and redshift data. Our objective is to determine whether the rise history of SNeIa can be accurately described by a single mode, as proposed by Phillips et al. (1999), or if it is better represented by two distinct modes, as suggested by Riess et al. (1999). Our analysis reveals that the statistical evidence supports both models at a 2-sigma confidence level, indicating that we cannot definitively exclude the possibility of a single mode existing. Notably, the parameters that best fit each distribution exhibit significant variability depending on the model chosen. This finding underscores the complexity of SNeIa rise times and suggests that further investigation is warranted. Should future observations corroborate our results, it would have profound implications for cosmological studies that utilize SNeIa as distance indicators. Specifically, the existence of two separate groups of SNeIa could introduce systematic errors in distance calculations if a uniform stretch value is applied across the board. This research highlights the necessity for a more nuanced understanding of SNeIa behavior, which is critical for refining cosmological measurements and enhancing the accuracy of our models of the universe's expansion. \n\n**Keywords:** Supernova, Light curve",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": 0.30779350562554625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Curvature Inspired Cosmological Scenario .\nAbstract:\nWe propose an alternative scenario for the evolution of our universe, which is based on the idea that the expansion rate of the universe may be driven by its curvature rather than dark energy. We show how this can lead to a viable cosmology with no need for dark energy and without any fine tuning problems associated with other models in the literature. In particular we find that:  The model has a number of interesting features including:  This work was supported by the Australian Research Council (ARC) Discovery Project DP0877481. Any opinions expressed are those of the authors only. 1 Introduction.\nThe discovery of accelerated cosmic expansion  1, 2  , as well as the recent detection of gravitational waves  3  have led to renewed interest in understanding the nature of gravity at large scales  4  . A possible explanation for these phenomena could lie within the framework of modified theories of gravity  5  .\nIn order to explain the observed acceleration of the universe it seems necessary to introduce some form of  dark energy   6  into Einstein s field equations  7, 8  . However, there appears to be little agreement amongst theorists about what exactly constitutes dark energy  9  or whether it should even exist  10  . Furthermore, if one assumes that dark energy exists then it must be extremely finely tuned  11  so that it behaves like a cosmological constant  12  over many orders of magnitude  13  . It also remains unclear why such a small value of vacuum energy density would arise naturally  14  .\nAnother possibility is that the apparent accelerating behaviour of the universe arises due to quantum effects  15  . For example, loop quantum gravity  16  predicts that space-time becomes discrete  17  leading to corrections to the Friedmann equation  18  . These corrections become significant when the scale factor reaches values close to the Planck length  19  . Other approaches include string theory  20  where the extra dimensions of spacetime  21  provide another source of potential modifications  22  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Curvature Inspired Cosmological Scenario . Abstract : We suggest an alternative scenario for the evolution of our universe , which is based on the idea that the development frequency of the universe might be motivated by its curvature instead than dark energy .We see how this can lead to a viable cosmology with no want for black energy and without any coarse tuning problems related with other models in the books . In particular we find that : The model has a number of interesting features including : This research was supported by the Australian Research Council ( ARC ) Discovery Project DP0877481 .Any views stated are those of the writers only . 1 Introduction .The observation of rapid cosmic expansion 1 , 2 , as also as the recent discovery of gravitational waves 3 have led to renewed concern in understanding the nature of gravitational at large scales 4 . A potential explanation for these phenomena could lay within the framework of revised theories of gravitational 5 .In order to explain the observed acceleration of the universe it appears necessary to introduce some kind of dark energy 6 into Einstein s field equations 7 , 8 . However , there seems to be little accord amongst theorists about what actually constitutes bright energy 9 or whether it should even exist 10 .Furthermore , if one assumes that dark energy occurs then it must be extremely finely tuned 11 so that it behaves like a cosmological constant 12 over numerous orders of magnitude 13 . It additionally seems unclear why such a small value of vacuum energy density would occur naturally 14 .Another possibility is that the apparent accelerating behaviour of the universe occurs due to quantum effects 15 . For instance , loop quantum gravitational 16 predicts that space - time remains discrete 17 leading to corrections to the Friedmann equation 18 .These corrections prove substantial when the scale factor reaches values close to the Planck size 19 . Other approaches involve string theory 20 where the extra dimensions of spacetime 21 offer another source of potential alterations 22 .",
        "rewrite_text": "**Title: Curvature Inspired Cosmological Scenario**\n\n**Abstract:** In this paper, we propose a novel framework for understanding the evolution of the universe, positing that its developmental frequency may be influenced more by its curvature than by the presence of dark energy. This perspective allows for the formulation of a cosmological model that does not rely on dark energy and circumvents the fine-tuning issues that plague many existing theories. Our findings reveal several intriguing characteristics of this curvature-driven model. \n\nRecent observations of accelerated cosmic expansion and the detection of gravitational waves have intensified the quest to comprehend gravitational dynamics at cosmological scales. Traditionally, the acceleration of the universe has been attributed to dark energy, necessitating its incorporation into Einstein's field equations. However, there is significant debate among theorists regarding the true nature of dark energy and its existence. The requirement for dark energy to be finely tuned to mimic a cosmological constant across vast scales raises further questions about its natural occurrence and the underlying mechanisms that could give rise to such a small vacuum energy density.\n\nAn alternative perspective suggests that the observed acceleration might stem from quantum effects. For example, loop quantum gravity posits that spacetime is fundamentally discrete, leading to modifications in the Friedmann equations that become significant at scales approaching the Planck length. Additionally, string theory introduces the concept of extra dimensions, which could provide further avenues for understanding the universe's dynamics.\n\nOur research, supported by the Australian Research Council (ARC) Discovery Project DP0877481, emphasizes the potential of curvature as a driving force in cosmological evolution. The views expressed herein are solely those of the authors. This work contributes to the ongoing discourse on the nature of cosmic expansion and the fundamental principles governing the universe.",
        "ori-fast-z-score": 0.07980868844676221,
        "water-fast-z-score": 8.273795755427592,
        "rewrite-fast-z-score": 0.3310423554409472
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Is there Evidence for a Hubble bubble? The Nature of Type Ia Supernova Colors and Dust in External Galaxies .\nAbstract:\nWe present new results on the nature of dust extinction in external galaxies, based on observations with HST/ACS of type Ia supernovae (SNe) at redshifts z=0.1-0.7. We find that SNe Ia are systematically bluer than expected if they were standard candles, but this effect is consistent with being due to dust extinction by interstellar material along their line-of-sight. In addition we find evidence for an additional component of dust extinction which increases rapidly towards higher redshift. This extra extinction may be associated with the intergalactic medium surrounding galaxy clusters, or it could arise within individual galaxies as a result of recent star formation activity. These findings have important implications for cosmological studies using distant SNe Ia as distance indicators. \n \n Keywords: Galaxy cluster, Interstellar matter, Intergalactic medium, Redshift evolution, Supernova, Supernova Ia",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Is there Evidence for a Hubble bubble ? The Nature of Type Ia Supernova Colors and Dust in External Galaxies .Abstract : We report new data on the nature of dust extinction in external galaxies , using on observations with HST / ACS of type Ia supernovae ( SNe ) at redshifts z = 0 . 1 - 0 . 7 . We see that SNe Ia are systematically bluer than expected if they were standard candles , but this effect is consistent with being owing to dust disappearance by interstellar material along their line - of - view .In addition we find proof for an additional element of dust extinction which increases quickly towards higher redshift . This extra extinction may be identified with the intergalactic medium surrounding galaxy galaxies , or it could occur within individual stars as a product of recent star formation activity .These conclusions have important implications for cosmological experiments using distant SNe Ia as distance indicators . Keywords : Galaxy cluster , Interstellar matter , Intergalactic medium , Redshift formation , Supernova , Supernova Ia",
        "rewrite_text": "Title: Is There Evidence for a Hubble Bubble? The Nature of Type Ia Supernova Colors and Dust in External Galaxies\n\nAbstract: In this study, we present new findings regarding the characteristics of dust extinction in external galaxies, derived from observations of Type Ia supernovae (SNe Ia) using the Hubble Space Telescope's Advanced Camera for Surveys (HST/ACS) at redshifts ranging from z = 0.1 to z = 0.7. Our analysis reveals that SNe Ia exhibit a systematic blue shift that deviates from the expected behavior of standard candles. This observed blueness can be attributed to the reduction of dust along the line of sight due to interstellar material. Furthermore, we have identified a significant component of dust extinction that intensifies with increasing redshift. This additional extinction may be associated with the intergalactic medium surrounding galaxies or could originate from dust produced within individual stars as a result of recent star formation activities. These findings carry substantial implications for cosmological studies that utilize distant SNe Ia as reliable distance indicators. Understanding the nature of dust extinction is crucial for refining measurements of cosmic distances and, consequently, for enhancing our comprehension of the universe's expansion and the potential existence of phenomena such as the Hubble bubble. Our results underscore the necessity of considering both interstellar and intergalactic dust effects when interpreting observations of Type Ia supernovae in various cosmic environments. \n\nKeywords: Galaxy cluster, Interstellar matter, Intergalactic medium, Redshift formation, Supernova, Supernova Ia.",
        "ori-fast-z-score": -0.46499055497527714,
        "water-fast-z-score": 4.50333209967908,
        "rewrite-fast-z-score": 0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Common Envelope Evolution Redux .\nAbstract:\nWe present the results of our study on common envelope evolution (CEE) in binary systems with helium white dwarfs and red giant secondaries, using detailed stellar evolutionary models for both components. We find that CEE is very efficient at removing orbital angular momentum when the secondary star fills its Roche lobe during core helium burning or later phases. The efficiency increases as the mass ratio decreases because more energy can be extracted by tidal friction if the primary has less mass than the secondary. \n \n In addition to this effect, we also found that the amount of orbital angular momentum removed depends strongly on whether the system undergoes stable or unstable mass transfer after the onset of RLOF. If the system experiences unstable mass transfer, then it will lose much more orbital angular momentum compared to cases where the mass transfer proceeds stably. This occurs mainly due to the fact that the former case leads to stronger tidal interaction between the two stars which causes them to spiral inward faster.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Common Envelope Evolution Redux . Abstract : We present the conclusion of our research on common envelope development ( CEE ) in binary systems with helium white dwarfs and red dwarf secondaries , using detailed stars evolutionary estimates for both components .We see that CEE is very efficient at removing orbital angular velocity when the secondary star occupies its Roche lobe during core helium burning or later periods . The efficiency increases as the mass ratio falls because more power can be extracted by tidal vibration if the primary has less mass than the secondary .In addition to this effect , we also discovered that the quantity of orbital angular energy expelled depends strongly on whether the system undergoes stable or unstable mass transfer after the beginning of RLOF . If the system encounters unstable mass transfer , then it will lose much more orbital angular velocity compared to cases where the mass transfer continues stably .This occurs mostly owing to the fact that the former situation leads to greater tidal association between the two stars which makes them to spiral inward faster .",
        "rewrite_text": "In this article, we present the findings of our investigation into the dynamics of common envelope evolution (CEE) in binary systems featuring helium white dwarfs and red dwarf companions. Utilizing comprehensive stellar evolutionary models for both stellar components, we have elucidated the mechanisms by which CEE effectively diminishes orbital angular momentum. Our results indicate that CEE is particularly efficient in removing angular momentum when the secondary star is within its Roche lobe during the core helium burning phase or subsequent stages of evolution. Notably, we found that the efficiency of angular momentum removal is inversely related to the mass ratio of the binary system; as the mass of the primary decreases relative to the secondary, the potential for tidal interactions increases, allowing for greater energy extraction through tidal forces. \n\nFurthermore, our research highlights a critical distinction in the behavior of orbital angular momentum loss depending on the stability of mass transfer during the onset of Roche lobe overflow (RLOF). In scenarios characterized by unstable mass transfer, the system experiences a significantly greater loss of orbital angular momentum compared to those undergoing stable mass transfer. This phenomenon can be attributed to the enhanced tidal coupling that occurs in unstable cases, which accelerates the inward spiral of the two stars. Our findings contribute to a deeper understanding of the intricate processes governing binary evolution and the role of common envelope phases in shaping the final outcomes of binary star systems. This work not only advances theoretical models of binary evolution but also has implications for the formation of various astrophysical phenomena, including gravitational wave sources and the evolution of compact objects.",
        "ori-fast-z-score": -0.40406101782088427,
        "water-fast-z-score": 4.924685294770139,
        "rewrite-fast-z-score": 1.3151918984428583
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-cooperative games for spreading code optimization, power control and receiver design in wireless data networks .\nAbstract:\nIn this thesis we study the problem of optimizing resource allocation in wireless networks by using non-cooperative game theory. We consider three different problems: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. In each case, we formulate an optimization problem as a noncooperative game between users competing to maximize their own utility functions. Then, we propose distributed algorithms that converge to Nash equilibria of these games. Finally, we evaluate our proposed schemes through extensive simulations on both static and mobile scenarios. \n \n Keywords: Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium. 1 Introduction \n \n The rapid growth of wireless communication has led to increased demand for high quality services such as voice over IP (VoIP), video streaming, online gaming etc., which require efficient use of limited resources available at base stations or access points. To meet this growing demand, researchers have been working towards developing new techniques to improve the performance of existing wireless systems while maintaining low cost and energy consumption  1  . One promising approach is to optimize resource allocations among users in order to increase overall system throughput  2  , reduce interference  3  , minimize transmission delay  4  , and/or enhance fairness  5  .\n \nThe main challenge faced when designing resource allocation strategies lies in the fact that there are usually multiple conflicting objectives  6  . For example, maximizing total user satisfaction may lead to unfair distribution of resources across users  7 ; increasing spectral efficiency can cause severe inter-user interference  8  ; minimizing transmission delays may result in poor channel utilization  9  . Therefore, it becomes necessary to develop novel approaches that strike a balance between various conflicting goals  10  . \n \n This work was supported in part by NSF under Grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - cooperative games for spreading code optimization , power control and receiver planning in wireless data systems . Abstract : In this dissertation we study the question of optimizing resource sharing in wireless networks by using non - cooperative play principles .We consider three different problems : ( 1 ) Code Optimization , ( 2 ) Power Control , and ( 3 ) Receiver Design . In each situation , we formulate an optimization problem as a noncooperative contest between users battling to maximize their own utility functions .Then , we develop dispersed techniques that converge to Nash equilibria of these games . Finally , we assess our proposed methods through extensive simulations on both static and mobile situations .Keywords : Non - Cooperative Game Theory ; Wireless Networks ; Resource Allocation ; Distributed Algorithms ; Nash Equilibrium . 1 Introduction The rapid increase of mobile communication has led to expanded availability for high quality services such as voice over IP ( VoIP ) , television viewing , internet gaming etc . , which require efficient application of restricted resources accessible at base stations or entry points .To address this increasing demand , researchers have been pushing towards developing innovative techniques to upgrade the performance of older wireless technologies while maintaining low cost and energy consumption 1 . One promising solution is to optimize resource allocations among consumers in order to expand overall network throughput 2 , avoid noise 3 , avoid transmission delay 4 , and / or enhance fairness 5 .The main challenge facing when designing asset distribution techniques comes in the fact that there are typically many conflicting aims 6 . For instance , maximizing gross user loyalty may contribute to inappropriate distribution of assets across users 7 ; increasing spectral capacity can cause profound cross - customer interference 8 ; minimizing broadcast delays may bring in poor channel utilization 9 .Therefore , it becomes necessary to develop new approaches that strike a balance between various differing aims 10 . This project was supported in part by NSF under Grants CNS - 0721440 , CCF - 0729260 , ECCS - 0801571 , and CNS - 0916275 .",
        "rewrite_text": "**Title:** Non-Cooperative Games for Spreading Code Optimization, Power Control, and Receiver Planning in Wireless Data Systems\n\n**Abstract:** This dissertation explores the optimization of resource sharing in wireless networks through the lens of non-cooperative game theory. We investigate three primary challenges: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. In each scenario, we frame the optimization problem as a non-cooperative game where users compete to maximize their individual utility functions. We propose distributed algorithms that converge to Nash equilibria, providing a stable solution to these competitive interactions. Our methodologies are rigorously evaluated through extensive simulations in both static and dynamic environments, demonstrating their effectiveness in real-world applications.\n\nThe surge in mobile communication has significantly increased the demand for high-quality services such as voice over IP (VoIP), streaming television, and online gaming. These services necessitate the efficient utilization of limited resources available at base stations or access points. To meet this growing demand, researchers are focused on developing innovative strategies to enhance the performance of existing wireless technologies while keeping costs and energy consumption low. A key approach involves optimizing resource allocation among users to improve overall network throughput, minimize noise, reduce transmission delays, and promote fairness.\n\nHowever, the design of effective resource allocation techniques is complicated by the presence of conflicting objectives. For example, prioritizing user satisfaction may lead to suboptimal resource distribution, while efforts to increase spectral efficiency can result in significant interference among users. Additionally, minimizing broadcast delays might compromise channel utilization. Thus, it is crucial to devise new strategies that effectively balance these competing goals. This research is partially funded by the National Science Foundation under Grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.\n\n**Keywords:** Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium.",
        "ori-fast-z-score": -2.101947149236112,
        "water-fast-z-score": 8.332380897952962,
        "rewrite-fast-z-score": -1.655211777204736
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measurement of the top-quark mass using missing $E_T$+jets events with secondary vertex $b$-tagging at CDF II .\nAbstract:\nWe present measurements of the top quark mass in final states containing one or two jets and large missing transverse energy, based on data collected by the Collider Detector at Fermilab during Run I (1992-1996). The analysis uses an iterative likelihood fit to extract the signal yield as a function of the assumed top quark mass for each jet multiplicity bin. We find mtop = 174.3 +/- 1.7 GeV/c2(stat) +/-3.1(syst), where the systematic uncertainty is dominated by uncertainties associated with modeling of initial state radiation and gluon splitting.  These results are consistent with previous measurements performed by D0 and CDF experiments. This measurement supersedes our previously published result. \nThe full text can be found at: http://arxiv.org/abs/0907.4189v1  # Measurement of the top-quarks mass using missing E_T+jets events with secondary-vertex b-tagging at CDF-II",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measurement of the top - quark mass using missing $ E _ T $ + jets events with secondary vertex $ b $ - tagging at CDF II . Abstract : We report observations of the top quark mass in finished states involving one or two jets and large missing radial energy , using on evidence generated by the Collider Detector at Fermilab during Run I ( 1992 - 1996 ) .The calculation utilizes an iterative probability fit to extract the signal yield as a function of the expected top quark mass for each jet multiplicity bin . We get mtop = 174 . 3 + / - 1 . 7 GeV / c2 ( stat ) + / - 3 . 1 ( syst ) , where the systematic uncertainty is dominated by uncertainties involved with modeling of initial state radiation and gluon splitting .These conclusions are compatible with previous measurements accomplished by D0 and CDF experiments . This measurement supersedes our previously reported result .The full text can be found at : www : / / arxiv . org / abs / 0907 . 4189v1 # Measurement of the top - quarks weight using missing E _ T + jets events with secondary - vertex b - tagging at CDF - II",
        "rewrite_text": "We present a comprehensive analysis of the top quark mass derived from events characterized by missing transverse energy and jets, utilizing secondary vertex b-tagging techniques at the Collider Detector at Fermilab (CDF II) during Run I (1992-1996). This study focuses on final states that include either one or two jets alongside significant missing radial energy. To determine the top quark mass, we employed an iterative probability fitting method that allows us to extract the signal yield as a function of the anticipated top quark mass for each bin of jet multiplicity. Our findings yield a top quark mass of m_top = 174.3 ± 1.7 GeV/c² (statistical uncertainty) and ± 3.1 GeV/c² (systematic uncertainty). Notably, the systematic uncertainty is primarily influenced by the complexities associated with modeling initial state radiation and gluon splitting processes. The results obtained are consistent with previous measurements conducted by both the D0 and CDF collaborations, reinforcing the reliability of our findings. This measurement represents an advancement over our earlier reported results, providing a more precise determination of the top quark mass. The full details of this research can be accessed at www.arxiv.org/abs/0907.4189v1.",
        "ori-fast-z-score": -1.7320508075688772,
        "water-fast-z-score": 3.670651741928988,
        "rewrite-fast-z-score": 0.5241424183609592
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Resolving the Formation of Protogalaxies. I. Virialization .\nAbstract:\nWe present an analysis of the virialization process in protogalactic halos, using high-resolution N-body simulations with gas dynamics and radiative cooling. We find that the density profiles of dark matter halos are well described by the Navarro-Frenk-White (NFW) profile at all redshifts z < 10. The NFW concentration parameter c(z), which characterizes how concentrated is the halo mass distribution relative to its mean density, increases rapidly as redshift decreases below z = 5. This rapid increase can be explained by the fact that the central regions of the halos become denser due to adiabatic contraction caused by infalling baryons. However, we also find that this effect alone cannot explain the observed evolution of c(z). In order to reproduce the results obtained from our numerical experiments, it is necessary to assume that the initial conditions for the formation of these halos were set up such that they had already undergone some degree of previrialization before their collapse into galactic-sized objects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Resolving the Formation of Protogalaxies.I. Virialization .Abstract : We present an assessment of the virialization system in protogalactic halos , using high - resolution N - bodies simulations with gas mechanics and radiative cooling . We see that the density characteristics of deep material halos are better represented by the Navarro - Frenk - White ( NFW ) profile at all redshifts z < 10 .The NFW concentration function c ( z ) , which characterizes how concentrated is the halo mass distribution relative to its average density , increases quickly as redshift decreases below z = 5 . This rapid increase can be described by the fact that the central regions of the halos become denser thanks to adiabatic contraction caused by infalling baryons .However , we also find that this effect alone cannot explain the observed evolution of c ( z ) . In order to reproduce the results derived from our numerical studies , it is important to assume that the early conditions for the formation of these halos were setting up such that they had already undergone some degree of previrialization before their collapse into galactic - sized bodies .",
        "rewrite_text": "In this study, we investigate the virialization processes occurring within protogalactic halos through high-resolution N-body simulations that incorporate gas dynamics and radiative cooling effects. Our findings indicate that the density profiles of these deep material halos are consistently well-described by the Navarro-Frenk-White (NFW) profile across all redshifts below z = 10. Notably, we observe a significant increase in the NFW concentration parameter, c(z), as redshift decreases past z = 5. This trend can be attributed to the adiabatic contraction of the halos, which is driven by the influx of baryonic matter, leading to denser central regions within the halos. However, our analysis reveals that this mechanism alone does not fully account for the observed evolution of c(z). To accurately replicate the results from our simulations, it is crucial to consider that the initial conditions for halo formation likely involved a degree of previrialization prior to their collapse into structures resembling galaxies. This previrialization phase suggests that the halos were not entirely in a relaxed state at the onset of their evolution, which has significant implications for our understanding of galaxy formation processes. Overall, our research provides valuable insights into the dynamics of protogalactic halos and highlights the importance of considering both baryonic effects and initial conditions in the study of cosmic structure formation.",
        "ori-fast-z-score": -1.1322770341445956,
        "water-fast-z-score": 4.024922359499621,
        "rewrite-fast-z-score": 0.10259783520851541
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray Timing of PSR J1852+0040 in Kesteven 79: Evidence of Neutron Stars Weakly Magnetized at Birth .\nAbstract:\nWe report on the discovery and timing analysis of pulsar PSR J1852 + 0040, which is associated with supernova remnant (SNR) Kes 79. The pulsar has spin period P = 1.56 ms and characteristic age τc = 3 kyrs. We find that its surface magnetic field strength Bs = 2 × 10^10 G, assuming an inclination angle i = 60 degrees between the rotation axis and line-of-sight to Earth. This value is consistent with theoretical predictions for neutron stars born weakly magnetized. In addition we have detected X-ray pulsations from this source using Chandra observations taken during 2009-2011. These results are presented here along with our timing solution obtained over a span of eight years. \n \n Keywords: Pulsar, Supernova Remnant, X-Ray Pulsars, Chandra Observatory, Radio Pulsar Timing \n \n Introduction \n \n A number of young radio pulsars show very low values of their surface dipole magnetic fields inferred from their spin-down rates. Such objects include Geminga, B1951+32, B1620-26, B1509-58, B0531+21, B1757-24, B1800-21, B1853+01, B1857+09, B1913+16, B1957+50, B2224+65, B2303+46, B2334+61, B0826-34, B1133+16, B1237+25, B1929+10, B1930+42, B1932+29, B1933+16, B1944+43, B1946+35, B1947+36, B1953+50, B1954+28, B1956+54, B1959+20, B1960+03, B1962+14, B1963+27, B1968+18, B1969+22, B1970+38, B1971+02, B1973+51, B1974+14, B1975+28, B1976+44, B1977+47, B1980+12, B1981+24, B1983",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray Timing of PSR J1852 + 0040 in Kesteven 79 : Evidence of Neutron Stars Weakly Magnetized at Birth . Abstract : We report on the discovery and timed study of pulsar PSR J1852 + 0040 , which is associated with supernova remnant ( SNR ) Kes 79 .The pulsar has spin length P = 1 . 56 ms and typical age τc = 3 kyrs . We see that its surface magnetic force force Bs = 2 × 10 ^ 10 G , assuming an inclination angle i = 60 degrees between the rotation axis and line - of - view to Earth .This value is compatible with theoretical estimates for neutron galaxies born mildly magnetized . In addition we have discovered X - ray pulsations from this source using Chandra measurements taken during 2009 - 2011 .These conclusions are presented here along with our timing solution acquired over a spanning of eight years . Keywords : Pulsar , Supernova Remnant , X - Ray Pulsars , Chandra Observatory , Radio Pulsar Timing Introduction A several of young radio pulsars show very low values of their surface dipole magnetic fields inferred from their spin - down rates .Such objects include Geminga , B1951 + 32 , B1620 - 26 , B1509 - 58 , B0531 + 21 , B1757 - 24 , B1800 - 21 , B1853 + 01 , B1857 + 09 , B1913 + 16 , B1957 + 50 , B2224 + 65 , B2303 + 46 , B2334 + 61 , B0826 - 34 , B1133 + 16 , B1237 + 25 , B1929 + 10 , B1930 + 42 , B1932 + 29 , B1933 + 16 , B1944 + 43 , B1946 + 35 , B1947 + 36 , B1953 + 50 , B1954 + 28 , B1956 + 54 , B1959 + 20 , B1960 + 03 , B1962 + 14 , B1963 + 27 , B1968 + 18 , B1969 + 22 , B1970 + 38 , B1971 + 02 , B1973 + 51 , B1974 + 14 , B1975 + 28 , B1976 + 44 , B1977 + 47 , B1980 + 12 , B1981 + 24 , B1983",
        "rewrite_text": "**Title:** X-ray Timing of PSR J1852+0040 in Kesteven 79: Evidence of Neutron Stars Weakly Magnetized at Birth\n\n**Abstract:** In this study, we present the discovery and detailed timing analysis of the pulsar PSR J1852+0040, which is associated with the supernova remnant Kes 79. The pulsar exhibits a spin period of P = 1.56 ms and is estimated to have an age of approximately τc = 3 kyrs. Our calculations indicate a surface magnetic field strength of Bs = 2 × 10^10 G, based on an inclination angle of i = 60 degrees between the pulsar's rotation axis and the line of sight to Earth. This magnetic field strength aligns with theoretical predictions suggesting that neutron stars can be born with relatively weak magnetic fields. Furthermore, we report the detection of X-ray pulsations from PSR J1852+0040, utilizing data collected from the Chandra X-ray Observatory between 2009 and 2011. Our findings, along with a comprehensive timing solution developed over an eight-year period, are discussed in detail. This research contributes to the growing body of evidence regarding the characteristics of young pulsars and their magnetic properties, particularly those that appear to be weakly magnetized at birth. The implications of these findings are significant for our understanding of neutron star formation and evolution, as they challenge traditional models that assume a strong magnetic field is a universal characteristic of newly formed neutron stars. Keywords: Pulsar, Supernova Remnant, X-ray Pulsars, Chandra Observatory, Radio Pulsar Timing. \n\nThis abstract encapsulates the key findings and significance of our research, highlighting the unique properties of PSR J1852+0040 and its implications for the study of neutron stars.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": -0.19069251784911848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  GRB 061121: Broadband spectral evolution through the prompt and afterglow phases of a bright burst .\nAbstract:\nWe present broadband (radio to X-ray) observations of GRB 061121, one of the most energetic gamma-ray bursts ever detected by Swift/BAT with an isotropic equivalent energy release of 1.8 x 10^54 erg in the 15-350 keV band. The temporal behavior of this event was complex; it consisted of multiple pulses that were superimposed on top of each other during both the prompt emission phase as well as the early part of its afterglow. We find evidence for two distinct components in the optical light curve -one which decays rapidly at first but then flattens out later-on timescales ranging between 0.1-10 days post-burst. This flattening could be due either to continued activity of the central engine or to refreshed shocks. In addition we detect significant radio emission upto 100 days post-burst. Our results are consistent with those obtained using data from the VLA and ATCA telescopes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : GRB 061121 : Broadband spectral evolution through the prompt and afterglow stages of a bright burst . Abstract : We report broadband ( radio to X - ray ) observations of GRB 061121 , one of the most intense gamma - ray bursts ever observed by Swift / BAT with an isotropic equivalent energy release of 1 . 8 x 10 ^ 54 erg in the 15 - 350 keV band .The temporal response of this event was difficult ; it consisted of multiple pulses that were superimposed on top of each other during both the prompt emission stage as well as the early portion of its afterglow . We see evidence for two different components in the optical light curve - one which decays slowly at first but then flattens out eventually - on timescales ranging between 0 . 1 - 10 weeks post - burst .This flattening may be due either to continued action of the main motor or to refreshed shocks . In addition we find considerable radio emission upto 100 months post - burst .Our results are compatible with those acquired using data from the VLA and ATCA telescopes .",
        "rewrite_text": "We present a comprehensive analysis of the broadband spectral evolution of GRB 061121, one of the most powerful gamma-ray bursts detected by the Swift/BAT, releasing an isotropic equivalent energy of approximately 1.8 x 10^54 erg in the 15-350 keV range. Our observations span from radio to X-ray wavelengths, capturing the intricate temporal dynamics of this event, which exhibited a complex structure characterized by multiple overlapping pulses during both the prompt emission phase and the initial stages of the afterglow. \n\nIn our study, we identify two distinct components in the optical light curve. The first component demonstrates a slow decay initially, followed by a notable flattening over a timescale of 0.1 to 10 weeks after the burst. This flattening behavior may be attributed to either the sustained activity of the central engine or the influence of refreshed shocks propagating through the surrounding medium. Furthermore, we observe significant radio emission persisting for up to 100 months following the burst, indicating ongoing processes in the aftermath of the explosion.\n\nOur findings align well with data obtained from the Very Large Array (VLA) and the Australia Telescope Compact Array (ATCA), reinforcing the robustness of our results. This study contributes to the understanding of the physical mechanisms at play during and after gamma-ray bursts, shedding light on the complexities of their emission processes and the evolution of their afterglow across a broad range of wavelengths. The implications of these observations are significant for the broader field of high-energy astrophysics, providing insights into the nature of gamma-ray bursts and their interactions with the surrounding environment.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.04145188432738,
        "rewrite-fast-z-score": 1.1239029738980328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmological applications of a wavelet analysis on the sphere .\nAbstract:\nWe present an algorithm for computing spherical wavelets and their associated scaling functions, which are used to analyze data defined over the unit sphere in three dimensions. The method is based on a decomposition into spherical harmonics and can be applied to any function that has been expanded as such. We show how this approach allows one to perform fast calculations of convolutions between two spherical signals or between a signal and its Fourier transform. As examples we apply our technique to calculate correlation functions of CMB temperature fluctuations and to compute power spectra of simulated galaxy surveys. Finally, we discuss possible extensions of these methods to higher-dimensional spaces. Wavelets have become popular tools for analyzing various types of data sets ranging from images to time series. In cosmology they were first introduced by Bond & Efstathiou (1987) who showed how they could be used to efficiently calculate angular correlations of cosmic microwave background radiation (CMB). Since then many authors have employed wavelets to study different aspects of large-scale structure formation including the evolution of dark matter haloes (e.g., Colombi et al. (1998) ), gravitational lensing effects (e.g., Jain et al. (2000)), weak gravitational lensing statistics (e.g., Schneider et al. (2002)) , and the clustering properties of galaxies (e.g., Percival et al. (2003)). However, all previous studies focused exclusively on flat space where it was straightforward to define wavelets using translations and dilations of mother wavelets. This situation changes dramatically when considering three-dimensional data sets like those obtained with modern astronomical instruments. Here, the concept of translation becomes ambiguous because there exists no unique way to identify corresponding points at different locations within the sample volume. Moreover, the notion of scale loses its meaning since distances cannot be measured directly but only inferred indirectly through redshift distortions caused by peculiar velocities.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmological applications of a wavelet analysis on the sphere . Abstract : We present an algorithm for modeling spherical wavelets and their accompanying scaling functions , which are using to analyze information defined over the unit sphere in three dimensions .The method is based on a transformation into spherical harmonics and can be applied to any function that has been expanded as such . We see how this methodology allows one to conduct fast calculations of convolutions between two spherical waves or between a signal and its Fourier shift .As instance we apply our technique to estimate correlation functions of CMB heat fluctuations and to compute power spectra of virtual galaxy surveys . Finally , we explain possible extend of these algorithms to higher - dimensional spaces .Wavelets have developed popular tools for searching various types of statistics sets ranging from images to time series . In cosmology they were first developed by Bond & Efstathiou ( 1987 ) who demonstrated how they could be used to easily predict angular correlations of cosmic microwave background radiation ( CMB ) .Since then many writers have utilized wavelets to study various details of large - scale system formation including the evolution of dark matter haloes ( e . g . , Colombi et al . ( 1998 ) ) , gravity lensing effects ( e . g . , Jain et al .( 2000 ) ) , weak gravitational lensing statistics ( e . g . , Schneider et al . ( 2002 ) ) , and the clustering behavior of galaxies ( e . g . , Percival et al .( 2003 ) ) . However , all previous research focused exclusively on straight space where it was straightforward to define wavelets using translations and dilations of parent wavelets .This condition shifts significantly when assessing three - dimensional data sets like those acquired with modern astronomical equipment . Here , the notion of translation becomes obscure because there exists no unique way to identify corresponding points at different places within the sample volume .Moreover , the notion of scale loses its significance since distances never be measured immediately but only inferred indirectly through redshift distortions caused by unique velocities .",
        "rewrite_text": "We introduce a novel algorithm designed for modeling spherical wavelets and their associated scaling functions, aimed at analyzing data defined over the unit sphere in three-dimensional space. This approach utilizes a transformation into spherical harmonics, making it applicable to any function that can be expressed in this form. Our methodology facilitates rapid computations of convolutions between two spherical wavelets or between a signal and its Fourier shift. As a demonstration of our technique, we apply it to estimate correlation functions of cosmic microwave background (CMB) temperature fluctuations and to compute power spectra for simulated galaxy surveys. Additionally, we discuss the potential for extending these algorithms to higher-dimensional spaces.\n\nWavelets have emerged as valuable tools for analyzing various statistical datasets, including images and time series. In the field of cosmology, their application began with Bond and Efstathiou (1987), who illustrated their effectiveness in predicting angular correlations of CMB radiation. Since then, numerous researchers have employed wavelets to investigate various aspects of large-scale structure formation, such as the evolution of dark matter halos (e.g., Colombi et al. 1998), gravitational lensing effects (e.g., Jain et al. 2000), weak gravitational lensing statistics (e.g., Schneider et al. 2002), and the clustering behavior of galaxies (e.g., Percival et al. 2003). \n\nHowever, prior studies have predominantly concentrated on flat space, where defining wavelets through translations and dilations of parent wavelets is straightforward. This simplicity is challenged when dealing with three-dimensional datasets obtained from contemporary astronomical instruments. In this context, the concept of translation becomes ambiguous, as there is no unique method to identify corresponding points across different locations within the sample volume. Furthermore, the notion of scale becomes less meaningful, as distances cannot be directly measured but must be inferred through redshift distortions resulting from peculiar velocities.",
        "ori-fast-z-score": -1.0120486274099798,
        "water-fast-z-score": 7.652514332541697,
        "rewrite-fast-z-score": 0.5813183589761798
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytical evaluation of the X-ray scattering contribution to imaging degradation in grazing-incidence X-ray telescopes .\nAbstract:\nWe present an analytical model for evaluating the effect of X-ray scattering on image quality in grazing incidence X-ray telescopes (GIXTs). The model is based on ray tracing and takes into account both surface roughness and bulk density fluctuations as sources of scattering. We show that, depending on the telescope parameters, either one or both effects can dominate over the other. In addition we demonstrate how our results compare with those obtained by Monte Carlo simulations. Finally, using this model, we evaluate the impact of different levels of surface roughness on the performance of the proposed GIXT mission Athena+. Our analysis shows that even relatively small values of surface roughness may significantly degrade the angular resolution of the instrument. This work was supported by NASA under grant NNX10AD65G issued through the Astrophysics Theory Program. Keywords: X-ray optics, Ray tracing, Surface roughness, Scattering",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytical evaluation of the X - ray scattering contribution to visual degradation in grazing - incidence X - ray telescopes . Abstract : We present an analytical model for evaluating the impact of X - ray scattering on image quality in grazing incidence X - ray telescopes ( GIXTs ) .The model is based on ray tracing and takes into consideration both exterior roughness and bulk density fluctuations as sources of scattering . We see that , depending on the telescope parameters , either one or both impacts can dominate over the other .In addition we prove how our findings compare with those achieved by Monte Carlo simulations . Finally , using this simulation , we assess the impact of different amounts of terrain roughness on the performance of the suggested GIXT mission Athena + .Our study shows that even surprisingly low measures of exterior roughness might considerably degrade the angular resolution of the instrument . This effort was supported by NASA under grant NNX10AD65G granted through the Astrophysics Theory Program .Keywords : X - ray optics , Ray tracing , Surface roughness , Scattering",
        "rewrite_text": "Title: Analytical Evaluation of the X-ray Scattering Contribution to Visual Degradation in Grazing-Incidence X-ray Telescopes\n\nAbstract: In this study, we introduce a comprehensive analytical model designed to assess the influence of X-ray scattering on the image quality of grazing incidence X-ray telescopes (GIXTs). Our model employs ray tracing techniques and incorporates both surface roughness and bulk density variations as potential sources of scattering. The analysis reveals that the dominance of either factor—surface roughness or bulk density fluctuations—depends on the specific parameters of the telescope. Furthermore, we validate our analytical findings by comparing them with results obtained from Monte Carlo simulations, demonstrating a strong correlation between the two approaches. To further illustrate the practical implications of our model, we evaluate the effects of varying levels of terrain roughness on the performance of the proposed GIXT mission, Athena+. Our results indicate that even minimal levels of surface roughness can significantly impair the angular resolution of the telescope, highlighting the importance of considering these factors in the design and implementation of GIXTs. This research was conducted with the support of NASA under grant NNX10AD65G, awarded through the Astrophysics Theory Program. \n\nKeywords: X-ray optics, ray tracing, surface roughness, scattering.",
        "ori-fast-z-score": -0.562543950463012,
        "water-fast-z-score": 4.024922359499621,
        "rewrite-fast-z-score": 0.41702882811414954
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diameters in preferential attachment models .\nAbstract:\nWe study the diameter and clustering coefficient for several classes of random graphs with power law degree distribution, including preferential attachment (PA) networks.  We show that PA networks have small diameters if their growth is slow enough; this result holds even when we allow multiple edges between nodes. For fast growing PA networks, however, there are examples where the diameter grows exponentially as a function of n. In addition to studying the diameter, we also consider the clustering coefficient of these networks. The clustering coefficient measures how well connected neighbors of a node are among themselves. We prove upper bounds on the clustering coefficients of PA networks which hold regardless of the rate at which new nodes join the network. Finally, we present some numerical results showing that our theoretical predictions agree very well with simulations. Keywords: Preferential Attachment Networks; Diameter; Clustering Coefficient. 1 Introduction Random graphs with power-law degree distributions arise naturally in many applications such as social networks  1  , Internet topology  2  , citation networks  3  , etc.. These types of networks can be generated by using various mechanisms known as  network growth models ; see  4  for an overview. One popular model is called  preferential attachment  or  Power-Law Graphs   5  . This class of networks has been studied extensively over the past decade  6  -  8  .\nIn this work, we focus on two important properties of these graphs: the diameter and the clustering coefficient  9  . The diameter D(G) of a graph G = (V, E) is defined as max{d(u, v) | u, v ∈ V }, i.e., it is the maximum distance between any pair of vertices in G. The clustering coefficient C(v) of a vertex v is defined as the fraction of pairs of neighbors of v that are adjacent to each other  10  . It is easy to verify that both quantities depend only on the degrees of the nodes in the graph  11  ; hence they do not change under edge rewirings  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diameters in preferential attachment models . Abstract : We research the diameter and clustering density for numerous classes of random graphs with power law degree distribution , including preferential attachment ( PA ) networks .We see that PA systems have short diameters if their development is slow enough ; this consequence holds even when we allow multiple edges between nodes . For quick expanding PA systems , however , there are examples where the radius grows exponentially as a function of n . In addition to modeling the radius , we also consider the clustering coefficient of these networks .The clustering parameter measures how well connected neighbors of a node are among themselves . We derive upper limits on the clustering coefficients of PA systems which hold regardless of the rate at which additional nodes join the organization .Finally , we present some numerical findings indicating that our theory estimates agree very best with simulations . Keywords : Preferential Attachment Networks ; Diameter ; Clustering Coefficient .1 Introduction Random graphs with power - law degree distributions arise naturally in different users such as community systems 1 , Internet topology 2 , citation connections 3 , etc . . These sorts of networks can be derived by using numerous mechanisms referred as network growth models ; look 4 for an overview .One popular approach is known preferential attachment or Power - Law Graphs 5 . This group of networks has been studied thoroughly over the previous decade 6 - 8 .In this research , we focus on two essential properties of these graphs : the radius and the clustering coefficient 9 . The diameter D ( G ) of a graph G = ( V , E ) is calculated as max { d ( u , v ) | v , v ∈ V } , i . e . , it is the maximum length between any pair of vertices in G . The clustering coefficient C ( v ) of a node v is calculated as the fraction of pairs of neighbors of v that are adjoining to each other 10 .It is easy to confirm that both quantities rely only on the degrees of the nodes in the graph 11 ; thus they do not change under path rewirings 12 .",
        "rewrite_text": "**Title:** Diameters in Preferential Attachment Models\n\n**Abstract:** This study investigates the diameter and clustering density of various classes of random graphs characterized by power law degree distributions, specifically focusing on preferential attachment (PA) networks. Our findings reveal that PA networks exhibit short diameters when their growth is sufficiently gradual; this observation remains valid even in scenarios where multiple edges can exist between nodes. Conversely, in rapidly expanding PA networks, we identify instances where the radius can increase exponentially with respect to the number of nodes, n. Alongside our analysis of the radius, we also examine the clustering coefficient of these networks, which quantifies the degree of interconnectedness among a node's neighbors. We establish upper bounds on the clustering coefficients for PA networks, demonstrating that these limits are applicable irrespective of the rate at which new nodes are integrated into the network. Furthermore, we present numerical results that suggest a strong alignment between our theoretical predictions and simulation outcomes, reinforcing the validity of our models. This research contributes to the understanding of the structural properties of networks that emerge through preferential attachment, which are prevalent in various real-world systems such as social networks, the Internet, and citation networks. By focusing on the diameter and clustering coefficient, we provide insights into the connectivity and clustering behavior of these complex systems, which are essential for comprehending their dynamics and functionality. \n\n**Keywords:** Preferential Attachment Networks; Diameter; Clustering Coefficient.",
        "ori-fast-z-score": -1.8594397919452197,
        "water-fast-z-score": 7.091242083423347,
        "rewrite-fast-z-score": -0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pixels simultaneous detection probabilities and spatial resolution determination of pixelized detectors by means of correlation measurements .\nAbstract:\nThe authors present an experimental method for determining the probability that two photons simultaneously hit pixels in a detector, as well as its spatial resolution. The method is based on measuring correlations between pairs of photons emitted by a source with known angular distribution. It can be used to characterize any type of photon-counting detector (CCD cameras, photomultipliers etc.) without requiring knowledge about their internal structure or electronics. This information may then be used to improve the performance of imaging systems such as telescopes. The results are presented for a silicon-strip detector. They show good agreement with Monte Carlo simulations. DOI: 10.1088/1742-6596/aa5e20\nSpatial resolution and coincidence resolving time measurement of Si strip detectors using single-photon counting technique \nI. INTRODUCTIO N\nIn many applications it is important to know how accurately one can determine the position where a photon hits a detector. For example this information is needed when designing optical instruments like telescopes  1  . In order to measure the spatial resolution of a detector we need to have some reference point against which we compare our measured data  2  .\nOne way to obtain this reference point is to use a light source emitting photons at a well-defined angle relative to the normal direction  3  , see Fig.  1(a) . If the detector has no intrinsic spatial resolution, all detected photons will come from a small area around the center of the detector surface. By scanning the detector over different angles θ, we can find out what fraction of the total number of counts comes from each part of the detector  4  . We call these fractions the response function R(θ) of the detector  5  . Knowing the shape of the response function allows us to calculate the spatial resolution of the detector  6  . However, if there is more than one pixel per unit solid angle, the situation becomes complicated because now several pixels could detect a given photon  7, 8  . To solve this problem we introduce here a new concept -the joint probability P ij that i-th and j-th pixels detect a photon simultaneously  9  . Using this concept together with the response function we",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pixels concurrent discovery probabilities and spatial resolution determination of pixelized detectors by means of correlation observations . Abstract : The authors present an research technique for determining the probability that two photons simultaneously impact pixels in a detector , as well as its spatial resolution .The method is based on measuring correlations between pairs of photons generated by a source with known angular distribution . It can be used to characterize any type of photon - tracking detector ( CCD cameras , photomultipliers etc . )without need understanding about their internal structure or electronics . This knowledge might then be used to upgrade the performance of optical units such as telescopes .The results are presented for a silicon - strip detector . They show good agreement with Monte Carlo simulations .DOI : 10 . 1088 / 1742 - 6596 / aa5e20 Spatial resolution and coincidence resolving time measurement of Si strip detectors using single - photon counting technique I . INTRODUCTIO N In many applications it is important to know how accurately one can determine the position where a photon hits a detector . For example this information is needed when designing optical instruments like telescopes 1 .In order to measure the spatial resolution of a detector we require to have some reference location against which we compare our measured data 2 . One method to obtain this reference point is to use a light source emitting photons at a well - defined angle relative to the normal direction 3 , see Fig .1 ( a ) . If the sensor has no intrinsic spatial resolution , all detected photons will coming from a small area around the center of the sensor surface .By scanning the sensor over different angles θ , we can find out what fraction of the total number of counts coming from each portion of the detector 4 . We call these fractions the response function R ( θ ) of the detector 5 .Knowing the shape of the response function allows us to estimate the spatial resolution of the sensor 6 . However , if there is more than one pixel per unit solid angle , the situation grows difficult because now multiple pixels might detect a given photon 7 , 8 .To solve this question we give here a new notion - the joint probability P ij that i - th and j - th pixels detect a photon simultaneously 9 . Using this concept together with the response vector we",
        "rewrite_text": "Title: Concurrent Discovery Probabilities of Pixels and Spatial Resolution Assessment in Pixelized Detectors through Correlation Observations\n\nAbstract: In this study, the authors introduce a novel methodology for assessing the likelihood that two photons strike pixels in a detector simultaneously, alongside determining the spatial resolution of the detector. This approach relies on analyzing correlations between pairs of photons emitted from a source with a known angular distribution. The technique is versatile and can be applied to various photon-tracking detectors, such as CCD cameras and photomultipliers, without necessitating an in-depth understanding of their internal mechanisms or electronic configurations. The insights gained from this research can significantly enhance the performance of optical instruments, including telescopes. The findings are illustrated through experiments conducted on a silicon-strip detector, demonstrating a strong correlation with Monte Carlo simulation results. \n\nThe introduction emphasizes the critical importance of accurately determining the photon impact location on a detector, particularly in the design of optical devices like telescopes. To evaluate the spatial resolution of a detector, a reference point is essential for comparison with the measured data. One effective method for establishing this reference is by utilizing a light source that emits photons at a precisely defined angle relative to the detector's normal direction. In scenarios where the sensor lacks intrinsic spatial resolution, all detected photons will originate from a confined area around the center of the sensor's surface. By systematically scanning the sensor across various angles, the fraction of total counts from each section of the detector can be quantified, leading to the formulation of the detector's response function, R(θ). Understanding the characteristics of this response function is crucial for estimating the sensor's spatial resolution. However, complications arise when multiple pixels correspond to a single unit solid angle, as this can result in multiple pixels detecting the same photon. To address this challenge, the authors introduce the concept of joint probability, P_ij, which quantifies the likelihood that the i-th and j-th pixels detect a photon concurrently. This innovative framework, combined with the response vector, provides a comprehensive understanding of the spatial resolution and coincidence resolving time in silicon strip detectors using single-photon counting techniques.",
        "ori-fast-z-score": 2.343379732657209,
        "water-fast-z-score": 7.387027942155209,
        "rewrite-fast-z-score": 0.9203579866168444
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometrically frustrated magnetic behavior of Sr3NiRhO6 and Sr3NiPtO6 .\nAbstract:\nThe geometric frustration in the spin-1/2 triangular lattice is studied by means of neutron powder diffraction, magnetization measurements, specific heat data, and first-principles calculations for two new compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 . The results show that both compounds are antiferromagnetic insulators with Néel temperatures TN = 5 K (Sr3NiRhO6) and T N = 7 K (Sr3NiPtO6). In addition to the expected collinear antiferromagnetism, we find evidence for noncollinear ordering in Sr3NiRhO6: First, there is an additional weak reflection at Q = 1.5 Å -1 , which can be explained as superlattice peak due to a small rhombohedral distortion; secondly, the temperature dependence of the ordered moment shows a kink around 2 K indicating a change of the order parameter below this temperature.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometrically frustrated magnetic behavior of Sr3NiRhO6 and Sr3NiPtO6 . Abstract : The mathematical frustration in the spin - 1 / 2 triangular lattice is studied by means of neutron dust diffraction , magnetization calculations , specific heat statistics , and first - principles measurements for two proposed compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 .The results show that both compounds are antiferromagnetic insulators with Néel pressures TN = 5 K ( Sr3NiRhO6 ) and T N = 7 K ( Sr3NiPtO6 ) . In addition to the expected collinear antiferromagnetism , we find proof for noncollinear ordering in Sr3NiRhO6 : First , there is an additional strong reflection at Q = 1 . 5 Å - 1 , which can be understood as superlattice peak thanks to a small rhombohedral distortion ; secondly , the temperature dependence of the ordered moment displays a kink around 2 K indicating a change of the order parameter below this heat .",
        "rewrite_text": "Title: Geometrically Frustrated Magnetic Behavior of Sr3NiRhO6 and Sr3NiPtO6\n\nAbstract: This study investigates the phenomenon of mathematical frustration in spin-1/2 triangular lattices through a comprehensive analysis of two proposed compounds, Sr3NiRhO6 and Sr3NiPtO6. Utilizing techniques such as neutron dust diffraction, magnetization measurements, specific heat analysis, and first-principles calculations, we reveal that both compounds exhibit characteristics of antiferromagnetic insulators, with Néel temperatures (TN) of 5 K for Sr3NiRhO6 and 7 K for Sr3NiPtO6. While the presence of collinear antiferromagnetism aligns with theoretical expectations, our findings also indicate the emergence of noncollinear magnetic ordering in Sr3NiRhO6. Notably, we observe a significant reflection at Q = 1.5 Å^-1, which can be interpreted as a superlattice peak resulting from a slight rhombohedral distortion in the crystal structure. Furthermore, the temperature dependence of the ordered magnetic moment reveals a distinct kink around 2 K, suggesting a transition in the order parameter at this temperature. These results contribute to the understanding of geometrically frustrated systems and highlight the complex magnetic interactions present in these materials, paving the way for future research into their unique properties and potential applications in spintronics and quantum computing.",
        "ori-fast-z-score": 0.2626128657194451,
        "water-fast-z-score": 3.7754784184438925,
        "rewrite-fast-z-score": -0.3418817293789138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Linked by Loops: Network Structure and Switch Integration in Complex Dynamical Systems .\nAbstract:\nWe study the relationship between network structure, dynamics, and function using an example system that is both complex and experimentally accessible -- the brain s neuronal circuitry. We show how to use statistical mechanics methods to analyze the functional properties of this system as it evolves over time through its underlying structural connectivity. In particular we focus on the role played by switches which are nodes with high degree centrality whose removal disconnects large portions of the network. Our results suggest that these switches play important roles in maintaining global dynamical stability while allowing for local flexibility. The manuscript is organized into four sections. Section I introduces our model and describes some basic features of the resulting networks. Section II presents numerical simulations showing how switching affects the evolution of the system. Section III analyzes the effect of removing switches on the networks  ability to maintain synchrony under perturbations. Finally section IV discusses possible extensions of our work. This article has been published in final form at http://arxiv.org/abs/1306.5995",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Linked by Loops : Network Structure and Switch Integration in Complex Dynamical Systems . Abstract : We research the relationship between network structure , dynamics , and function using an instance structure that is both complex and experimentally visible - - the brain s neuronal circuitry .We see how to use statistical mechanics algorithms to analyze the functional characteristics of this network as it evolves over time through its underlying structural connectivity . In particular we focus on the part played by switches which are networks with high degree centrality whose departure disconnects huge segments of the organization .Our results show that these switches act essential roles in maintaining international dynamical stability while providing for local flexibility . The manuscript is organized into four sections .Section I outlines our model and explains some fundamental characteristics of the resulting systems . Section II offers numerical simulations demonstrating how switching affects the evolution of the system .Section III analyzes the impact of eliminating switches on the systems ability to keep synchrony under perturbations . Finally section IV examines possible extensions of our work .This section has been publication in final form at http : / / arxiv . org / abs / 1306 . 5995",
        "rewrite_text": "Title: Linked by Loops: Network Structure and Switch Integration in Complex Dynamical Systems\n\nAbstract: This study investigates the intricate interplay between network structure, dynamics, and functionality, using the brain's neuronal circuitry as a complex and experimentally accessible model. We employ statistical mechanics algorithms to analyze the evolving functional characteristics of this network, which is fundamentally shaped by its structural connectivity. A particular focus is placed on the role of switches—nodes within the network that exhibit high degree centrality. The removal of these switches can lead to the disconnection of substantial portions of the network, highlighting their critical importance. Our findings indicate that switches are vital for maintaining global dynamical stability while simultaneously allowing for local adaptability within the system. The manuscript is structured into four distinct sections. Section I introduces our model and delineates key characteristics of the systems we study. In Section II, we present numerical simulations that illustrate the effects of switching on the system's evolution. Section III delves into the consequences of switch removal, specifically examining how it impacts the network's ability to sustain synchrony amidst external perturbations. Finally, Section IV explores potential extensions of our research, suggesting avenues for future inquiry. This work has been published in its final form and can be accessed at http://arxiv.org/abs/1306.5995.",
        "ori-fast-z-score": -1.1406468642034677,
        "water-fast-z-score": 6.807380225308036,
        "rewrite-fast-z-score": 1.507556722888818
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Near-Infrared and X-ray Observations of the Enigmatic G70.7+1.2 .\nAbstract:\nWe report on near-infrared (NIR) photometry, optical spectroscopy, and X-ray observations of the enigmatic radio source G70.7+1. \n \n The NIR data show that this object is an extremely reddened star with A V = 25 mag located at a distance of about 5 kpc in front of the supernova remnant W28. We find no evidence for any infrared excess emission indicative of dust formation around the central star. Optical spectra reveal strong P-Cygni profiles indicating mass loss rates as high as 10^-5 M_sol/yr. In addition to these features we detect narrow absorption lines which are likely due to interstellar material along our line-of-sight towards the star. Finally, we present Chandra ACIS-S X-ray images showing extended diffuse emission surrounding the radio source. This emission may be associated with shocked gas produced by stellar winds or outflows from the central star.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Near - Infrared and X - ray Observations of the Enigmatic G70 . 7 + 1 . 2 . Abstract : We report on near - infrared ( NIR ) photometry , optical spectroscopy , and X - ray observations of the enigmatic radio source G70 . 7 + 1 .The NIR data reveal that this body is an incredibly reddened star with A V = 25 mag found at a distance of about 5 kpc in top of the supernova remnant W28 . We see no evidence for any infrared excess emission indicative of dust form around the primary star .Optical spectra indicate strong P - Cygni profiles suggesting mass loss rates as great as 10 ^ - 5 M _ sol / yr . In addition to these characteristics we perceive narrow absorption patterns which are likely due to interstellar material along our line - of - view towards the star .Finally , we present Chandra ACIS - S X - ray pictures showing extended diffuse emission surrounding the radio source . This absorption may be correlated with shocked energy produced by stellar rains or outflows from the central star .",
        "rewrite_text": "We present a comprehensive study of the intriguing radio source G70.7+1.2, incorporating near-infrared (NIR) photometry, optical spectroscopy, and X-ray observations. Our NIR analysis indicates that G70.7+1.2 is an exceptionally reddened star, exhibiting an extinction of A_V = 25 mag, and is located approximately 5 kpc away, situated within the supernova remnant W28. Notably, we find no signs of infrared excess emission that would suggest the presence of dust surrounding the primary star. The optical spectra reveal pronounced P-Cygni profiles, which imply significant mass loss rates, potentially reaching up to 10^-5 M_sol/yr. Additionally, we observe narrow absorption features in the spectra, likely attributable to interstellar material along the line of sight to the star. Our analysis is further complemented by X-ray observations obtained from the Chandra ACIS-S, which reveal extended diffuse emission in the vicinity of the radio source. This observed absorption may be linked to shocked energy resulting from stellar winds or outflows emanating from the central star. Collectively, these findings enhance our understanding of G70.7+1.2 and its environment, shedding light on the complex interactions between the star and the surrounding interstellar medium.",
        "ori-fast-z-score": -0.7071067811865476,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": -0.10846522890932808
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vacancy localization in the square dimer model .\nAbstract:\nWe study the ground state properties of the spin-1/2 square lattice with nearest-neighbor antiferromagnetic interactions and vacancies, using exact diagonalization (ED) on finite clusters up to 12x12 sites. We find that for low concentrations of vacancies there is no significant change in the magnetic order parameter or the energy gap between singlet and triplet excitations as compared to the pure system. However, we observe an increase in the density of states at zero energy when increasing the concentration of vacancies. This effect can be explained by considering the formation of bound pairs of vacancies which are localized around each other due to their mutual interaction. The binding energies of these pairs depend strongly on the distance between them but only weakly on the size of the cluster considered. In addition, we show how this behavior changes if one considers next-nearest neighbor interactions instead of nearest-neighbor ones. Finally, we discuss possible experimental realizations of our results.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vacancy localization in the square dimer model . Abstract : We explore the ground state properties of the spin - 1 / 2 square lattice with nearest - neighbor antiferromagnetic interactions and vacancies , using accurate diagonalization ( ED ) on finite clusters up to 12x12 places .We see that for low levels of vacancies there is no major shift in the magnetic order parameter or the power gap between singlet and triplet excitations as compared to the pure system . However , we study an increase in the density of states at zero energy when increasing the concentration of vacancies .This phenomenon can be described by using the formation of bound pairs of vacancies which are localized around each other owing to their mutual interaction . The activation energies of these couples differ highly on the distance between them but only weakly on the length of the cluster considered .In addition , we explain how this behavior changes if one considers next - nearest neighbor interactions instead of nearest - neighbor ones . Finally , we explain possible experimental realizations of our findings .",
        "rewrite_text": "In this study, we investigate the ground state characteristics of a spin-1/2 square lattice exhibiting nearest-neighbor antiferromagnetic interactions in the presence of vacancies. Utilizing precise diagonalization (ED) techniques on finite clusters of up to 12x12 sites, we analyze the effects of varying vacancy concentrations on the system's magnetic properties. Our findings indicate that at low vacancy levels, there is no significant alteration in the magnetic order parameter or the energy gap between singlet and triplet excitations when compared to the pristine lattice. However, we observe a notable increase in the density of states at zero energy as the vacancy concentration rises. This increase can be attributed to the formation of bound pairs of vacancies, which become localized due to their mutual interactions. The activation energies associated with these vacancy pairs exhibit a strong dependence on their separation distance, while their variation is only weakly influenced by the size of the cluster under consideration. Furthermore, we explore how the introduction of next-nearest neighbor interactions modifies this behavior, providing insights into the interplay between vacancy dynamics and magnetic ordering. Lastly, we discuss potential experimental implementations of our results, highlighting the relevance of our findings in the context of real-world materials and their magnetic properties. This work contributes to a deeper understanding of vacancy-induced phenomena in quantum spin systems and opens avenues for future research in this area.",
        "ori-fast-z-score": 0.8834522085987723,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": 0.6527533657682196
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High Precision CTE-Measurement of SiC-100 for Cryogenic Space-Telescopes .\nAbstract:\nThe development of large space telescopes requires the use of lightweight materials to reduce launch costs and increase telescope performance in orbit. Silicon Carbide (SiC) is an excellent candidate material due to its high strength, low density, and radiation hardness at cryogenic temperatures. However, it has been shown that SiC exhibits significant changes in thermal expansion with temperature which can lead to dimensional instability during cool-downs or warm-ups. This work presents results on the measurement of the coefficient of thermal expansion (CTE) of SiC-100 over a wide range of temperatures using a novel technique based on laser interferometry. The measured values are compared against literature data as well as theoretical predictions obtained by ab initio calculations. It was found that the experimental measurements agree very well with theory within the uncertainty limits. These results will be used to improve the design of future space missions such as JWST and WFIRST-AFTA.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High Precision CTE - Measurement of SiC - 100 for Cryogenic Space - Telescopes . Abstract : The construction of large space telescopes demands the using of lightweight materials to reduce rocket expenses and increase telescope performance in orbit .Silicon Carbide ( SiC ) is an excellent candidate material owing to its high strength , low density , and radiation hardness at cryogenic temperatures . However , it has been shown that SiC exhibits significant variations in heat expansion with temperature which can lead to dimensional instability during cool - downs or cold - ups .This research provides findings on the observation of the coefficient of thermal expansion ( CTE ) of SiC - 100 over a broad variety of temperatures using a novel technique focused on laser interferometry . The measured values are compared against literature statistics as well as conceptual predictions produced by ab initio methods .It was shown that the empirical observations coincide very best with principle within the uncertainty restrictions . These data will be used to improve the development of later space missions such as JWST and WFIRST - AFTA .",
        "rewrite_text": "Title: High Precision CTE Measurement of SiC-100 for Cryogenic Space Telescopes\n\nAbstract: The design and construction of large-scale space telescopes necessitate the utilization of lightweight materials to minimize launch costs and enhance operational efficiency in orbit. Silicon Carbide (SiC) emerges as a highly suitable material due to its exceptional strength, low density, and resistance to radiation at cryogenic temperatures. However, it has been observed that SiC experiences considerable fluctuations in its coefficient of thermal expansion (CTE) with temperature changes, which can result in dimensional instability during thermal cycling. This study presents comprehensive findings on the CTE of SiC-100 across a wide temperature range, employing an innovative laser interferometry technique for precise measurement. The results obtained from this method are rigorously compared with existing literature values and theoretical predictions derived from ab initio calculations. The empirical data align closely with theoretical expectations within the established uncertainty limits, confirming the reliability of the measurements. These findings are crucial for advancing the design and development of future space missions, including the James Webb Space Telescope (JWST) and the Wide Field Infrared Survey Telescope - Astrophysics Focused Telescope Assets (WFIRST-AFTA). By enhancing our understanding of SiC's thermal properties, this research contributes significantly to the optimization of materials used in cryogenic environments, ultimately improving the performance and stability of next-generation space telescopes.",
        "ori-fast-z-score": -1.3937462952891333,
        "water-fast-z-score": 5.253351420705195,
        "rewrite-fast-z-score": -0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pair Production of Doubly-Charged Scalars: Neutrino Mass Constraints and Signals at the LHC .\nAbstract:\nWe study pair production of doubly-charged scalars in association with two jets, which can be induced by both weak gauge bosons (W or Z) and photons via loops involving heavy fermions such as top quarks. We derive constraints on the masses of these particles using current experimental data for W+jets and Z+jets processes collected by ATLAS and CMS experiments at the Large Hadron Collider (LHC). In addition to the standard model backgrounds, we also consider contributions from other new physics models that may have similar signatures. The results are presented in terms of exclusion limits on the mass parameters of various new physics scenarios. Finally, we discuss possible signals of this process at future runs of the LHC. PACS numbers: 12.60.Jv, 13 .85.Rm, 14.80.Ly \nI. INTRODUCTIO N\nThe discovery of neutrinos has opened up an exciting possibility of probing beyond Standard Model (SM), especially its Majorana nature  1  , through their lepton number violating interactions  2  . One interesting scenario is the seesaw mechanism  3  where SM singlet right-handed neutrinos acquire large Majorana masses after electroweak symmetry breaking  4  .\nIn order to test whether the observed light neutrinos are indeed Majorana particles, one needs to look for lepton-number-violating processes mediated by virtual heavy neutrinos  5  . These include neutrinoless double beta decay  6  , tritium beta decay  7  , and charged-current quasielastic scattering  8  . However, it turns out that all these processes suffer from severe astrophysical and/or nuclear matrix element uncertainties  9  . On the other hand, colliders provide clean environments to probe lepton number violation directly  10  . For example, searches for same-sign dileptons  11  and trileptons  12  at hadronic colliders could lead to important information about Majorana neutrinos  13  . Another promising channel is the production of doubly-charge scalar particles  14  , which can occur either through s-channel exchange of neutral gauge bosons  15  or t-channel exchange of heavy ferm",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pair Production of Doubly - Charged Scalars : Neutrino Mass Constraints and Signals at the LHC . Abstract : We research pair production of doubly - charged scalars in association with two jets , which can be induced by both weak gauge bosons ( W or Z ) and photons via loops involving heavy fermions such as top quarks .We derive restrictions on the masses of these objects utilizing current experimental evidence for W + jets and Z + jets effects obtained by ATLAS and CMS tests at the Large Hadron Collider ( LHC ) . In addition to the standard theory backgrounds , we also consider contributions from other recent physics models that might have related signatures .The results are presented in terms of exclusion limits on the mass parameters of several novel physics scenarios . Finally , we explain possible signals of this process at next ran of the LHC .PACS numbers : 12 . 60 . Jv , 13 . 85 . Rm , 14 . 80 . Ly I . INTRODUCTIO N The observation of neutrinos has opened up an exciting possibility of probing beyond Standard Model ( SM ) , particularly its Majorana nature 1 , through their lepton size violating interactions 2 . One interesting scenario is the seesaw mechanism 3 where SM singlet right - handed neutrinos gain big Majorana masses after electroweak symmetry breaking 4 .In order to test whether the seen light neutrinos are indeed Majorana ions , one needs to search for lepton - number - violating reactions mediated by virtual heavy neutrinos 5 . These include neutrinoless double alpha emission 6 , tritium alpha emission 7 , and charged - current quasielastic emission 8 .However , it turns out that all these mechanisms suffer from severe astrophysical and / or atomic matrix element uncertainties 9 . On the other hand , colliders provide clean environments to probe lepton number violation directly 10 .For instance , searches for same - sign dileptons 11 and trileptons 12 at hadronic colliders may bring to key information about Majorana neutrinos 13 . Another promising channel is the production of doubly - charge scalar particles 14 , which can occur either through s - channel exchange of neutral gauge bosons 15 or t - channel exchange of light ferm",
        "rewrite_text": "**Title:** Pair Production of Doubly-Charged Scalars: Neutrino Mass Constraints and Signals at the LHC\n\n**Abstract:** This study investigates the pair production of doubly-charged scalar particles in conjunction with two jets, a process that can be triggered by weak gauge bosons (W or Z) and photons through loop interactions involving heavy fermions, such as top quarks. We establish constraints on the masses of these doubly-charged scalars by analyzing current experimental data from the ATLAS and CMS collaborations at the Large Hadron Collider (LHC), specifically focusing on W + jets and Z + jets events. In addition to the standard model backgrounds, we also take into account contributions from various recent physics models that may exhibit similar signatures. Our findings are articulated in terms of exclusion limits on the mass parameters associated with several new physics scenarios. Furthermore, we discuss potential signals of this production process that could be observed in the upcoming runs of the LHC.\n\nThe discovery of neutrinos has opened avenues for exploring phenomena beyond the Standard Model (SM), particularly concerning their Majorana nature through lepton number violating interactions. A notable framework is the seesaw mechanism, where SM singlet right-handed neutrinos acquire significant Majorana masses following electroweak symmetry breaking. To ascertain whether the observed light neutrinos are indeed Majorana particles, it is essential to investigate lepton number violating processes mediated by virtual heavy neutrinos. These processes include neutrinoless double beta decay, tritium beta decay, and charged-current quasielastic interactions. However, these mechanisms face substantial uncertainties due to astrophysical and atomic matrix elements. In contrast, collider experiments offer a more controlled environment to directly probe lepton number violation. For example, searches for same-sign dileptons and trileptons at hadronic colliders can yield critical insights into the nature of Majorana neutrinos. Another promising avenue is the production of doubly-charged scalar particles, which can occur through the s-channel exchange of neutral gauge bosons or t-channel exchange involving light fermions. This research aims to enhance our understanding of these processes and their implications for neutrino mass and new physics.",
        "ori-fast-z-score": 0.8391813582966892,
        "water-fast-z-score": 6.333333333333333,
        "rewrite-fast-z-score": 0.159111456835146
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum-like Representation of Macroscopic Configurations .\nAbstract:\nWe present an approach to the description of macroscopic configurations in terms of quantum states, which is based on the concept of entanglement entropy and its generalization for mixed states. We show that this representation allows one to obtain exact results for some statistical properties of systems with many degrees of freedom. In particular, we consider the problem of calculating the partition function of classical spin models at high temperatures. The proposed method can be used as a basis for constructing approximate algorithms for solving problems related to the calculation of thermodynamic characteristics of complex systems. Introduction.\nThe main goal of statistical mechanics is to describe the behavior of macroscopic objects (for example, gases) by using microscopic information about their constituents (atoms). This task becomes especially difficult when dealing with large systems consisting of many particles or spins. For such cases, it is necessary to use approximations, since direct calculations are impossible due to the exponential growth of the number of possible microstates with increasing system size N . One of these approaches is the so-called mean-field approximation  1  , according to which each particle interacts only with all other particles simultaneously; i.e., the interaction between different pairs of particles is neglected. However, even within this simplified model, the calculation of the partition function Z = Tr exp(−βH) (1) remains extremely complicated  2  .\nIn recent years, there has been growing interest in developing new methods for describing macroscopic configurations in terms similar to those used in quantum physics  3  -  8  . These studies were inspired by the fact that both classical and quantum descriptions have certain common features  9  : they are formulated in terms of wave functions ψ(x), where x denotes either positions of particles or spins, respectively. Moreover, the evolution of these wave functions obeys the same Schrödinger equation ih∂ t |ψ(t) = H|ψ(t) , where H is the corresponding Hamiltonian operator. It should also be noted that the density matrix ρ = |ψ(t) ψ(t)| plays the role of a probability distribution in both theories  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum - like Representation of Macroscopic Configurations . Abstract : We present an view to the description of macroscopic configurations in terms of quantum states , which is based on the idea of entanglement entropy and its generalization for mixed states .We see that this representation enables one to obtain exact findings for some statistical characteristics of networks with many degrees of liberty . In particular , we investigate the question of calculating the partition function of classical spin machines at high temperatures .The proposed concept can be used as a framework for constructing approximate schemes for solving problems related to the determination of thermodynamic qualities of complex systems . Introduction .The main goal of statistical mechanics is to explain the dynamics of macroscopic objects ( for example , atoms ) by using microscopic information about their constituents ( atoms ) . This job seems particularly challenging when dealing with large systems composed of several particles or spins .For such cases , it is important to use approximations , since direct calculations are unable due to the exponential growth of the number of possible microstates with increasing system width N . One of these method is the so - called mean - field analogy 1 , according to which each particle interacts only with all other particles concurrently ; i . e . , the interaction between different pairs of atoms is neglected .However , even within this simplified theory , the determination of the partition function Z = Tr exp ( −βH ) ( 1 ) stays extremely difficult 2 . In recent years , there has been growing interest in developing novel techniques for describing macroscopic configurations in terms comparable to those utilized in particle mathematics 3 - 8 .These studies were inspired by the fact that both classical and quantum descriptions have many common features 9 : they are formulated in terms of wave functions φ ( x ) , where h indicates either positions of atoms or spins , respectively . Moreover , the evolution of these wave functions obeys the same Schrödinger equation ih∂ t | ψ ( t ) = H | ψ ( t ) , where H is the analogous Hamiltonian operator .It should additionally be mentioned that the density function ρ = | ψ ( t ) ψ ( t ) | serves the importance of a probability distribution in both theories 10 .",
        "rewrite_text": "**Title:** Quantum-like Representation of Macroscopic Configurations\n\n**Abstract:** In this article, we propose a novel framework for describing macroscopic configurations through the lens of quantum states, leveraging the concepts of entanglement entropy and its extension to mixed states. This quantum-like representation facilitates the derivation of precise statistical characteristics for complex networks with numerous degrees of freedom. Specifically, we focus on the calculation of the partition function for classical spin systems at elevated temperatures, a task that poses significant challenges due to the intricate nature of these systems. Our approach provides a systematic method for approximating solutions to problems associated with the thermodynamic properties of complex systems.\n\nThe overarching aim of statistical mechanics is to elucidate the behavior of macroscopic entities, such as atoms, by utilizing microscopic data about their constituents. This endeavor becomes increasingly complex when addressing large systems composed of multiple particles or spins, where direct calculations are often infeasible due to the exponential increase in the number of potential microstates as the system size grows. Traditional methods, such as the mean-field approximation, simplify interactions by assuming that each particle interacts uniformly with all others, thereby neglecting the interactions between distinct pairs. Despite this simplification, calculating the partition function remains a formidable challenge.\n\nRecent advancements have sparked interest in innovative techniques that draw parallels between macroscopic configurations and those found in particle mathematics. These investigations reveal that classical and quantum frameworks share numerous similarities, particularly in their formulation through wave functions, which describe either atomic positions or spin states. Furthermore, the evolution of these wave functions adheres to the same Schrödinger equation, underscoring the analogous nature of the Hamiltonian operator in both contexts. The density function, serving as a probability distribution in both classical and quantum theories, further emphasizes the interconnectedness of these two domains. Our findings suggest that the proposed quantum-like representation could significantly enhance the understanding and analysis of macroscopic systems, paving the way for future research in statistical mechanics and related fields.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.23076923076923,
        "rewrite-fast-z-score": 1.2649110640673518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diversity-Multiplexing Tradeoff via Asymptotic Analysis of Large MIMO Systems .\nAbstract:\nIn this work, we study the diversity-multiplexing tradeoff (DMT) for large multiple-input and multiple-output systems with independent Rayleigh fading channels. We first derive an exact expression for DMT by using the asymptotic analysis technique proposed in  1  . Then, based on our derived results, we propose two low-complexity suboptimal schemes to achieve near-optimum performance at low SNR region. Finally, simulation results are provided to verify the effectiveness of these proposed schemes. The main contributions of this paper can be summarized as follows:  1) An exact expression is obtained for the DMT of large MIMO systems; 2) Two low-complexity suboptimum schemes are proposed; 3) Simulation results show that both proposed schemes have good performances compared with existing ones. In recent years, there has been growing interest in studying large-scale antenna arrays due to their potential advantages over conventional small-scale antenna arrays  2  -  4  , such as higher data rates, lower transmit power consumption, reduced inter-cell interference, etc.. However, it should also be noted that increasing the number of antennas will lead to increased hardware cost and energy consumption  5  .\nTo fully exploit the benefits brought about by massive MIMO technology while keeping its disadvantages under control, many researchers have studied how to optimize the design parameters  6 -  8  or develop new transmission techniques  9  -  11  . Among them, one important issue is to investigate the fundamental limits of largescale antenna array systems  12  -  14  . For example, Zheng et al.  15  investigated the ergodic capacity scaling law of multi-cell multiuser massive MIMO networks. Liu et al.  16  analyzed the outage probability of downlink massive MIMO systems. Wang et al.  17  studied the achievable rate of uplink massive MIMO systems with imperfect channel state information (CSI). Moreover, some other works  18  -  20  focused on analyzing the error performance of large-scale antenna array systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diversity - Multiplexing Tradeoff via Asymptotic Analysis of Large MIMO Systems . Abstract : In this research , we study the diversity - multiplexing tradeoff ( DMT ) for large single - input and multiple - output devices with independent Rayleigh fading sources .We first derive an precise representation for DMT by using the asymptotic analysis technique proposed in 1 . Then , using on our derived results , we propose two low - complexity suboptimal techniques to achieve near - optimum performance at low SNR region .Finally , simulation data are provided to confirm the effectiveness of these suggested schemes . The main contributions of this paper can be summarized as follows : 1 ) An exact representation is found for the DMT of large MIMO systems ; 2 ) Two low - complexity suboptimum systems are proposed ; 3 ) Simulation data demonstrate that both proposed methods have better performances relative with existing ones .In recent years , there has been growing interest in experimental huge - scale array arrays due to their potential advantages over traditional small - scale array arrays 2 - 4 , such as higher data costs , superior transmit electricity usage , reduced inter - cell interference , etc . . However , it should also be mentioned that expanding the number of antennas will result to greater hardware expense and energy consumption 5 .To fully utilize the advantages brought about by massive MIMO technology while maintaining its disadvantages under control , many scientists have researched how to optimize the project characteristics 6 - 8 or develop new transmission strategies 9 - 11 . Among them , one key issue is to examine the fundamental limits of largescale antenna array systems 12 - 14 .For instance , Zheng et al . 15 explored the ergodic capacity scaling principle of multi - cell multiuser large MIMO connections .Liu et al . 16 analyzed the outage likelihood of downlink large MIMO systems .Wang et al . 17 studied the achievable rate of uplink large MIMO systems with imperfect channel state information ( CSI ) .Moreover , some other works 18 - 20 centered on examining the error behaviour of large - scale array antenna networks .",
        "rewrite_text": "**Title:** Diversity-Multiplexing Tradeoff via Asymptotic Analysis of Large MIMO Systems\n\n**Abstract:** This study investigates the diversity-multiplexing tradeoff (DMT) in large single-input multiple-output (SIMO) systems characterized by independent Rayleigh fading channels. We begin by establishing a precise formulation for the DMT through the application of asymptotic analysis techniques. Building on our findings, we introduce two low-complexity suboptimal strategies designed to achieve near-optimal performance in low signal-to-noise ratio (SNR) environments. To validate the efficacy of these proposed methods, we present simulation results that demonstrate their superior performance compared to existing approaches. The key contributions of this paper are threefold: first, we derive an exact representation of the DMT for large MIMO systems; second, we propose two innovative low-complexity suboptimal systems; and third, we provide simulation data that illustrate the enhanced performance of our methods relative to current solutions. \n\nIn recent years, there has been an increasing interest in large-scale antenna arrays due to their potential benefits over traditional small-scale systems, including higher data rates, improved energy efficiency, and reduced inter-cell interference. However, it is important to note that increasing the number of antennas can lead to higher hardware costs and greater energy consumption. To maximize the advantages of massive MIMO technology while mitigating its drawbacks, researchers have focused on optimizing system characteristics and developing new transmission strategies. A critical area of exploration involves understanding the fundamental limits of large-scale antenna array systems. For example, Zheng et al. examined the scaling laws of ergodic capacity in multi-cell multiuser large MIMO networks, while Liu et al. analyzed the outage probability in downlink scenarios. Additionally, Wang et al. investigated the achievable rates in uplink large MIMO systems with imperfect channel state information. Other studies have focused on the error performance of large-scale antenna networks, contributing to a comprehensive understanding of the capabilities and limitations of these advanced communication systems.",
        "ori-fast-z-score": -0.6246950475544243,
        "water-fast-z-score": 8.692885887910927,
        "rewrite-fast-z-score": 2.1602468994692865
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neutral Networks of Sequence to Shape Maps .\nAbstract:\nWe introduce the concept of neutral networks, which are graphs that represent relationships between sequences and shapes in an unsupervised manner. We show how these can be used for shape recognition by training them on synthetic data generated using computer graphics techniques. The resulting network is able to recognize shapes with high accuracy even when they have been transformed or distorted. Finally we demonstrate our method on real world images showing its ability to generalize well beyond the training set. In this work we present a novel approach to learning about shapes through their relationship to sequences. Our main contribution is introducing the concept of neutral networks as a way to learn such relationships without supervision. Neutral networks are graphs whose nodes correspond to sequences (e.g., strings) and edges connect similar sequences. They provide a natural representation for capturing relationships between different types of information. For example, one could use a neutral network to capture relationships between sentences and documents, or between music notes and melodies. Here we focus on applying neutral networks to the problem of recognizing shapes based solely on their sequence of points sampled along their boundary. This task has many applications including medical image analysis where it may not always be possible to obtain ground truth labels due to privacy concerns.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neutral Networks of Sequence to Shape Maps . Abstract : We introduce the idea of neutral networks , which are graphs that represent connections between sequences and shapes in an unsupervised manner .We see how these can be used for shape perception by testing them on synthetic information generated using computer graphics techniques . The resulting network is ability to identify designs with high clarity even when they have been transformed or distorted .Finally we prove our technique on real time images showing its capacity to generalize far beyond the training set . In this project we present a novel method to knowledge about forms through their connection to sequences .Our main achievement is pioneering the idea of neutral networks as a way to teach such interactions without supervision . Neutral networks are graphs whose nodes relate to strings ( e . g . , strings ) and edges connect identical sequences .They offer a natural representation for capturing relationships between various types of documents . For instance , one might using a neutral network to capture relationships between sentences and documents , or between music notes and melodies .Here we focus on introducing neutral networks to the issue of recognizing shapes based primarily on their sequence of points analyzed along their boundary . This job has numerous uses including medical visual evaluation where it would not always be possible to obtain ground truth labels due to privacy issues .",
        "rewrite_text": "We present a comprehensive exploration of neutral networks, a novel concept that serves as a framework for understanding the relationships between sequences and shapes in an unsupervised manner. Neutral networks are structured as graphs, where nodes represent sequences (such as strings) and edges connect identical sequences, thereby facilitating the capture of intricate relationships across various types of data. Our research demonstrates the application of these networks in shape perception, utilizing synthetic data generated through advanced computer graphics techniques. The experiments reveal that the neutral network can effectively identify and classify designs with remarkable clarity, even when subjected to transformations or distortions.\n\nA significant aspect of our findings is the ability of the neutral network to generalize beyond the training dataset, as evidenced by its performance on real-time images. This capability underscores the potential of neutral networks to enhance our understanding of forms through their connections to sequences, paving the way for innovative applications in fields such as medical imaging. In scenarios where obtaining ground truth labels is challenging due to privacy concerns, our method offers a promising alternative for visual evaluation.\n\nBy pioneering the concept of neutral networks, we provide a new approach to teaching interactions between sequences and shapes without the need for supervision. This research not only contributes to the theoretical understanding of shape recognition but also opens avenues for practical applications in various domains, including document analysis and music composition. Overall, our work highlights the versatility and effectiveness of neutral networks in addressing complex recognition tasks, establishing a foundation for future studies in this area.",
        "ori-fast-z-score": -1.1445861782233109,
        "water-fast-z-score": 7.137183894183611,
        "rewrite-fast-z-score": 0.9135002783911397
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Functional Methods in the Generalized Dicke Model .\nAbstract:\nWe study the generalized Dicke model with an arbitrary number N of two-level atoms interacting with one-mode radiation field, and show that it can be mapped to a spin-1/2 system by using the Holstein-Primakoff transformation. We then use the exact diagonalization method to calculate its ground state energy spectrum for different values of the coupling constant g and the number N . The results are compared with those obtained by other methods such as perturbation theory and numerical integration. It is found that our results agree well with previous ones when the coupling strength is small but deviate significantly from them if the coupling becomes strong. Finally we discuss some possible applications of this work. PACS: 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv \nI. INTRODUCTIO N\nThe Dicke model  1  describes how many identical two-level atoms interact collectively with a single mode of electromagnetic field. In recent years there has been renewed interest in studying this model because of its potential application in quantum information processing  2  , quantum optics  3  , condensed matter physics  4  , etc.. For example, the collective spontaneous emission rate of the atomic ensemble depends on the total angular momentum J = N /2 (N being the number of atoms)  5  .\nIn fact, the Dicke model was originally proposed more than half century ago  6  . Since then various theoretical approaches have been developed to solve it  7 -10  . Among these approaches, the most successful one is probably the so-called HolsteinPrimakoff transformation  11  which maps the original problem into a spin-1/2 system  12  . This approach works very well at weak-coupling regime where the interaction between atom-field is relatively small. However, it fails completely at large-coupling limit since the mapping procedure breaks down due to the appearance of unphysical states  13  . Recently, several authors  14 -19  have tried to overcome this difficulty by introducing new transformations or approximations. Nevertheless, their solutions still suffer from certain drawbacks  20, 21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Functional Methods in the Generalized Dicke Model . Abstract : We explore the generalized Dicke model with an arbitrary number N of two - level atoms interacting with one - mode radiation field , and find that it can be mapped to a spin - 1 / 2 system by using the Holstein - Primakoff transformation .We then use the exact diagonalization technique to estimate its ground state energy spectrum for different values of the interaction function g and the number N . The results are compared with those achieved by other methods such as perturbation theory and mathematical integration .It is found that our findings agree well with previous ones when the interaction strength is tiny but deviate drastically from them if the interaction gets powerful . Finally we explain some possible users of this study .PACS : 03 . 65 . Ud , 05 . 45 . Mt , 11 . 10 . Gh , 12 . 20 . Dc , 13 . 25 . Gv I . INTRODUCTIO N The Dicke model 1 explains how many identical two - level atoms behave collectively with a single mode of electromagnetic field . In recent months there has been continued interest in understanding this model because of its potential application in quantum information processing 2 , quantum optics 3 , condensed matter science 4 , etc . . For instance , the collective spontaneous emission speed of the atomic ensemble depends on the total angular velocity J = N / 2 ( N being the number of atoms ) 5 .In reality , the Dicke approach was originally proposed more than quarter century ago 6 . Since then various theoretical methods have been constructed to solve it 7 - 10 .Among these method , the most popular one is probably the so - called HolsteinPrimakoff transformation 11 which maps the original problem into a spin - 1 / 2 system 12 . This method works very best at weak - interaction regime where the interaction between particle - field is fairly little .However , it fails totally at large - coupling limit since the mapping method splits down due to the appearance of unphysical states 13 . Recently , various literature 14 - 19 have tried to overcome this challenge by using new transformations or approximations .Nevertheless, their solutions still suffer from certain drawbacks  20, 21  .",
        "rewrite_text": "**Title:** Functional Methods in the Generalized Dicke Model\n\n**Abstract:** In this study, we investigate the generalized Dicke model, which encompasses an arbitrary number \\( N \\) of two-level atoms interacting with a single mode of radiation. By employing the Holstein-Primakoff transformation, we successfully map this model to a spin-\\( \\frac{1}{2} \\) system. Utilizing the exact diagonalization technique, we compute the ground state energy spectrum for varying interaction strengths \\( g \\) and atom numbers \\( N \\). Our findings are juxtaposed with results obtained through alternative methodologies, including perturbation theory and mathematical integration. We observe that our results align closely with previous studies in the regime of weak interactions; however, significant discrepancies arise as the interaction strength increases. This divergence highlights the limitations of traditional approaches in the strong coupling regime. Furthermore, we discuss the implications of our findings for potential applications in fields such as quantum information processing, quantum optics, and condensed matter physics. The Dicke model, which describes the collective behavior of identical two-level atoms in conjunction with a single electromagnetic mode, has garnered renewed interest due to its relevance in these areas. The collective spontaneous emission rate of the atomic ensemble is notably influenced by the total angular momentum \\( J = \\frac{N}{2} \\), where \\( N \\) represents the number of atoms. Although the Dicke model was initially proposed over twenty-five years ago, numerous theoretical approaches have since been developed to address its complexities. Among these, the Holstein-Primakoff transformation remains a prominent technique, particularly effective in the weak interaction regime. However, it encounters significant challenges in the strong coupling limit, leading to the emergence of unphysical states. Recent literature has sought to address these challenges through novel transformations and approximations, yet many of these solutions still exhibit inherent limitations. This work aims to contribute to the ongoing discourse surrounding the generalized Dicke model and its applications in modern physics.",
        "ori-fast-z-score": -0.5551361100027009,
        "water-fast-z-score": 7.171804696775816,
        "rewrite-fast-z-score": -0.5551361100027009
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Long-Term Evolution of Massive Black Hole Binaries. III. Binary Evolution in Collisional Nuclei .\nAbstract:\nWe present the results of long-term numerical simulations of binary black hole (BBH) evolution, including gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption. We focus on binaries with total mass M = 100-1000M⊙ that evolve through collisional nuclear environments at high redshifts z > 10. Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds. In particular we investigate whether these systems can reach masses above 1000M⊙ before merging within a Hubble time. The initial conditions for our models were obtained using Monte Carlo sampling of the distribution function of isolated BBHs constructed by Belczynski et al. (2010) . For each model we performed several runs starting from different orbital configurations. All calculations were carried out assuming circular orbits. We find that most of the massive binaries merge within a few hundred million years after formation due to emission of gravitational waves. However, some of them survive until today if they form in regions where the density of surrounding gas exceeds $10^{9}$ cm$^{-3}$. These binaries may be detectable by future space-based gravitational wave observatories like LISA or DECIGO/BBO.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Long - Term Evolution of Massive Black Hole Binaries . III .Binary Evolution in Collisional Nuclei . Abstract : We present the conclusion of long - term numerical simulations of binary dark hole ( BBH ) development , notably gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption .We focus on binaries with total mass M = 100 - [UNK] that develop through collisional nuclear habitats at high redshifts z > 10 . Our main goal is to study how BBHs can develop by accretion during their early stages of evolved when they are surrounded by dense gas clouds .In particular we investigate whether these systems can reach masses above [UNK] before merging within a Hubble time . The initial conditions for our models were obtained using Monte Carlo sampling of the distribution function of isolated BBHs constructed by Belczynski et al .( 2010 ) . For each model we performed numerous runs beginning from varying orbital configurations .All calculations were carried out assuming circular orbits . We see that most of the huge binaries unite within a few hundred million years after formed owing to emission of gravitational waves .However , some of them remain until today if they appear in areas where the density of neighbouring gas approaches $ 10 ^ { 9 } $ cm $ ^ { - 3 } $ . These binaries may be detectable by future space - based gravity wave observatories like LISA or DECIGO / BBO .",
        "rewrite_text": "We present the findings from our extensive numerical simulations focused on the long-term evolution of binary black hole (BBH) systems, particularly examining the influences of gravitational radiation and general relativistic phenomena, including frame dragging and tidal disruption. Our study centers on BBHs with a total mass in the range of 100 solar masses, which evolve within collisional nuclear environments at high redshifts (z > 10). The primary objective of this research is to explore the mechanisms through which BBHs can grow via accretion during their formative stages, particularly when they are enveloped by dense gas clouds. We specifically assess the potential for these systems to achieve significant masses prior to merging within a Hubble time.\n\nTo establish the initial conditions for our simulations, we employed Monte Carlo sampling techniques based on the distribution function of isolated BBHs as outlined by Belczynski et al. (2010). Each simulation was initiated from a variety of orbital configurations, with all calculations assuming circular orbits. Our results indicate that the majority of these massive binaries coalesce within a few hundred million years post-formation, primarily due to the emission of gravitational waves. However, we also find that certain binaries can persist to the present day, particularly in regions where the surrounding gas density approaches \\(10^{9} \\, \\text{cm}^{-3}\\). These enduring binaries hold the potential for detection by upcoming space-based gravitational wave observatories, such as LISA or DECIGO/BBO, thereby offering exciting prospects for future astrophysical research and observation.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.902903378454601,
        "rewrite-fast-z-score": 0.1841149235796647
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  General Relativistic Flux Modulations from Disk Instabilities in Sagittarius A* .\nAbstract:\nWe present the first detection of relativistically modulated X-ray fluxes from the Galactic Center black hole candidate SgrA*, using data obtained with Chandra and XMM-Newton observatories over an eight-year period (2000-2007). The observed light curves show clear periodic dips on time scales ranging between 20 minutes to several hours that are consistent with being caused by general relativistic effects near the event horizon of this supermassive black hole. We find no evidence for long-term variability or flaring activity during these observations. These results provide strong support for theoretical models where the emission is produced close to the last stable orbit around the central black hole via accretion disk instabilities. This work was supported by NASA grants NAG5-10842, NNG06GH50G, NNX07AH41G, and NSF grant AST-0707765. Subject headings: Black holes -accretion disks -X-rays",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : General Relativistic Flux Modulations from Disk Instabilities in Sagittarius A * . Abstract : We report the first detection of relativistically modulated X - ray fluxes from the Galactic Center black hole member SgrA * , using data acquired with Chandra and XMM - Newton observatories over an eight - year period ( 2000 - 2007 ) .The observed light curves show consistent continuous dips on time ranges ranging between 20 seconds to several days that are compatible with being created by general relativistic effects near the event horizon of this supermassive black hole . We see no evidence for short - term variability or flaring activity during these observations .These data provide broad support for theoretical theories where the emission is produced nearly to the last steady orbit around the central black hole via accretion disk instabilities . This research was supported by NASA grants NAG5 - 10842 , NNG06GH50G , NNX07AH41G , and NSF grant AST - 0707765 .Subject headings : Black holes - accretion disks - X - rays",
        "rewrite_text": "**Title:** General Relativistic Flux Modulations from Disk Instabilities in Sagittarius A*\n\n**Abstract:** In this study, we present the groundbreaking detection of relativistically modulated X-ray fluxes emanating from Sagittarius A* (SgrA*), the supermassive black hole located at the center of our galaxy. Utilizing data collected over an extensive eight-year period (2000-2007) from the Chandra and XMM-Newton observatories, we have identified distinct light curves characterized by continuous dips in flux. These fluctuations occur over time scales ranging from 20 seconds to several days, suggesting a strong correlation with general relativistic phenomena occurring in the vicinity of the black hole's event horizon. Notably, our observations reveal an absence of short-term variability or flaring activity, which further supports the hypothesis that the observed emissions are primarily influenced by the dynamics of the accretion disk surrounding SgrA*. \n\nThe findings align with theoretical models positing that the X-ray emissions are generated close to the last stable orbit around the black hole, driven by instabilities within the accretion disk. This research not only enhances our understanding of the behavior of matter in extreme gravitational fields but also provides compelling evidence for the role of general relativity in the modulation of X-ray emissions from supermassive black holes. The implications of these results extend to the broader field of astrophysics, particularly in the study of black hole accretion processes and their associated emissions. This work was made possible through the support of NASA grants NAG5-10842, NNG06GH50G, NNX07AH41G, and NSF grant AST-0707765. \n\n**Subject Headings:** Black holes, Accretion disks, X-rays.",
        "ori-fast-z-score": -1.4552137502179978,
        "water-fast-z-score": 3.491189739487307,
        "rewrite-fast-z-score": -0.2873478855663454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exploring Infrared Properties of Giant Low Surface Brightness Galaxies .\nAbstract:\nWe present new infrared photometry for the sample of giant low surface brightness galaxies (LSBGs) studied by van Dokkum et al. (2015) . We use these data to explore their stellar populations and dust content, as well as investigate whether they are consistent with being massive analogues of local dwarf spheroidal galaxies. The LSBGs have been observed in J-(1.25 um), H-(1.65um), Ks-(2.17um), 3.6um, 4.5um, 5.8um & 8.0um bands using Spitzer/IRAC and MIPS cameras on board the Spitzer Space Telescope. Our results show that all but one galaxy has an old age population dominated by red giants at least 1 Gyr older than those found in typical early-type galaxies. This is supported by our analysis which shows that most of them have very little cold dust emission compared to other types of galaxies. These findings suggest that the majority of the LSBGs may be undergoing passive evolution without significant star formation activity over the past few billion years.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exploring Infrared Properties of Giant Low Surface Brightness Galaxies . Abstract : We bring novel infrared photometry for the sample of giant low surface brightness galaxies ( LSBGs ) studied by van Dokkum et al .( 2015 ) . We use these information to examine their stellar environments and dust content , as well as investigate whether they are compatible with being huge analogues of local dwarf spheroidal galaxies .The LSBGs have been observed in J - ( 1 . 25 um ) , H - ( 1 . 65um ) , Ks - ( 2 . 17um ) , 3 . 6um , 4 . 5um , 5 . 8um & 8 . 0um bands using Spitzer / IRAC and MIPS cameras on board the Spitzer Space Telescope . Our results show that all but one galaxy has an old age population dominated by red giants at least 1 Gyr older than those observed in standard early - class objects .This is backed by our analysis which reveals that most of them have very less chill wind emitted compared to other types of stars . These conclusions show that the majority of the LSBGs might be experiencing passive evolution without significant star formation activity over the previous few billion decades .",
        "rewrite_text": "In this study, we present new infrared photometric data for a sample of giant low surface brightness galaxies (LSBGs) as investigated by van Dokkum et al. (2015). Our research aims to analyze the stellar environments and dust content of these galaxies, while also assessing their potential as large counterparts to local dwarf spheroidal galaxies. We conducted observations across various infrared bands, including J (1.25 µm), H (1.65 µm), Ks (2.17 µm), as well as 3.6 µm, 4.5 µm, 5.8 µm, and 8.0 µm, utilizing the Spitzer Space Telescope's IRAC and MIPS instruments. \n\nThe findings indicate that nearly all of the LSBGs in our sample exhibit an old stellar population, predominantly composed of red giants that are at least 1 billion years older than those found in typical early-type galaxies. Our analysis further suggests that these galaxies emit significantly less infrared radiation compared to other stellar types, implying a low level of star formation activity. This evidence supports the notion that the majority of LSBGs are undergoing a phase of passive evolution, characterized by minimal star formation over the past several billion years. \n\nThese insights contribute to our understanding of the evolutionary pathways of LSBGs and their relationship to other galaxy types, particularly in the context of galaxy formation and evolution theories. The implications of our findings may also shed light on the broader role of LSBGs in the cosmic landscape, particularly regarding their formation histories and the processes that govern their current states.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.512930714537517,
        "rewrite-fast-z-score": 1.0864289525102224
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The cold gaseous halo of NGC 891 .\nAbstract:\nWe present new observations of the molecular gas in the central region of the nearby galaxy NGC 891, obtained with the IRAM 30m telescope at 1mm and 3mm wavelengths. The data reveal an extended distribution of dense (n(H2) ~ 104 cm-3), warm (T~50K) molecular gas that is associated with the optical disk of this edge-on spiral galaxy. We find evidence for two distinct components to the molecular gas distribution; one component follows closely the dust lane seen in visible light images while another component extends out into the surrounding intergalactic medium. This latter component has been detected previously by other authors but our higher resolution data allow us to resolve it into individual clouds. In addition we detect several compact sources within the galactic plane which are likely to be young star forming regions. These results suggest that there may exist a significant reservoir of molecular material outside the main body of galaxies such as NGC 891.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The cold gaseous halo of NGC 891 . Abstract : We report new images of the molecular gas in the central region of the nearby galaxy NGC 891 , obtained with the IRAM 30m telescope at 1mm and 3mm wavelengths .The data reveal an extended distribution of dense ( n ( H2 ) ~ 104 mm - 3 ) , warm ( T ~ 50K ) molecular gas that is associated with the optical disk of this edge - on spiral galaxy . We see evidence for two different components to the molecular gas distribution ; one element follows tightly the dust track seen in bright light photographs while another component extends out into the nearby intergalactic medium .This latter component has been detected earlier by other researchers but our higher resolution data enable us to separate it into multiple clouds . In addition we locate many compact sources within the galactic plane which are likely to be young galaxy producing regions .These data suggest that there may contain a substantial pool of molecular matter outside the main bodies of galaxies such as NGC 891 .",
        "rewrite_text": "We present new observational data regarding the molecular gas in the central region of the nearby galaxy NGC 891, acquired using the IRAM 30m telescope at both 1mm and 3mm wavelengths. Our findings reveal a significant and extended distribution of dense molecular gas, characterized by a density of approximately \\( n(H_2) \\sim 10^4 \\, \\text{mm}^{-3} \\) and a temperature around \\( T \\sim 50 \\, \\text{K} \\). This molecular gas is closely associated with the optical disk of this edge-on spiral galaxy. Notably, our analysis indicates the presence of two distinct components within the molecular gas distribution. One component closely follows the dust lane observed in high-resolution optical images, while the second component extends into the surrounding intergalactic medium. Although this latter component has been previously identified by other studies, our high-resolution observations allow us to resolve it into multiple distinct clouds. Furthermore, we have identified numerous compact sources within the galactic plane, which are likely regions of active star formation. These observations imply that there exists a considerable reservoir of molecular material beyond the primary structures of galaxies like NGC 891. This research enhances our understanding of the molecular gas dynamics in edge-on spiral galaxies and suggests that significant amounts of molecular gas may be present in the intergalactic medium, potentially influencing star formation processes and the overall evolution of galaxies.",
        "ori-fast-z-score": -2.013995972012084,
        "water-fast-z-score": 4.9819900360298925,
        "rewrite-fast-z-score": 2.0426487199475707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The habitability of super-Earths in Gliese 581 .\nAbstract:\nWe present the results of our study on the possible existence and stability of terrestrial planets around the star Gliese 581, which is located at about 20 light-years away from Earth. We have performed numerical simulations for different orbital configurations of three hypothetical terrestrial planets with masses ranging between 1 to 10 times that of Earth s mass (1-10 M⊕). Our calculations show that all these systems are dynamically stable over time scales longer than 100 Myr. The most massive planet has an eccentric orbit with e=0.2 and its periastron distance ranges between 0.05 AU and 0.15 AU depending on the initial conditions used. This planet can be considered as a hot Jupiter-like planet because it orbits very close to its host star. However, we find that there exists another region where two or more terrestrial planets may exist stably. In this region, one of them could be a super-Earth-type planet with a mass larger than 5M⊕ but smaller than 8M⊕.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The habitability of super - Earths in Gliese 581 . Abstract : We present the conclusion of our research on the possible existence and stability of terrestrial worlds around the star Gliese 581 , which is situated at about 20 light - years far from Earth .We have done mathematical simulations for different orbital arrangements of three hypothetical terrestrial worlds with masses ranging between 1 to 10 twice that of Earth s mass ( 1 - 10 M⊕ ) . Our calculations show that all these systems are dynamically stable over time ranges longer than 100 Myr .The most large planet has an eccentric orbit with e = 0 . 2 and its periastron speed ranges between 0 . 05 AU and 0 . 15 AU depending on the first conditions utilized . This planet can be regarded as a bright Jupiter - like planet because it orbits very close to its host star .However , we find that there exists another region where two or more terrestrial worlds may arise stably . In this area , one of them could be a super - Earth - class planet with a mass greater than 5M⊕ but smaller than 8M⊕ .",
        "rewrite_text": "In this study, we explore the potential habitability and stability of terrestrial planets orbiting the star Gliese 581, located approximately 20 light-years from Earth. Our research involved conducting mathematical simulations to assess various orbital configurations for three hypothetical terrestrial planets, each with masses ranging from 1 to 10 times that of Earth (1 - 10 M⊕). The results of our simulations indicate that these planetary systems exhibit dynamic stability over time spans exceeding 100 million years. Notably, the largest of the simulated planets possesses an eccentric orbit characterized by an eccentricity of e = 0.2, with its periastron distance fluctuating between 0.05 AU and 0.15 AU, contingent upon the initial conditions applied in our models. This planet can be likened to a bright Jupiter-like entity due to its proximity to its host star. Furthermore, our findings reveal the existence of an additional region within the orbital dynamics where two or more terrestrial planets could coexist stably. Within this zone, we identify the potential for a super-Earth-class planet, with a mass exceeding 5M⊕ but less than 8M⊕, to form. This research contributes to our understanding of the conditions under which super-Earths may develop and thrive in the vicinity of Gliese 581, offering insights into the broader implications for planetary habitability in similar stellar systems.",
        "ori-fast-z-score": -1.62746694241347,
        "water-fast-z-score": 3.4412360080584263,
        "rewrite-fast-z-score": 0.30779350562554625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic turbulence production by streaming cosmic rays upstream of SNR shocks .\nAbstract:\nWe study the magnetic field amplification in supernova remnants (SNRs) due to cosmic ray streaming instability, which is driven by anisotropic diffusion of energetic particles across the mean magnetic field lines. We show that this process can be responsible for the observed level of magnetic fields in young SNRs and may explain their origin. The growth rate of the instability depends on the ratio between the gyrofrequency of relativistic protons and the frequency of plasma waves excited by them. This ratio decreases with time as the number density of accelerated particles increases downstream of the shock front. As a result, the instability saturates at some distance behind the shock front where the magnetic energy density becomes comparable to the kinetic energy density of the flow. In order to estimate the saturation scale we use an analytical model developed recently by Bell et al. (2013) . It allows us to calculate the spectrum of amplified magnetic fluctuations produced by cosmic ray streaming instability.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic turbulence production by streaming cosmic rays upstream of SNR shocks . Abstract : We research the magnetic force amplification in supernova remnants ( SNRs ) resulting to cosmic ray streaming instability , which is caused by anisotropic absorption of excited particles across the mean magnetic field lines .We see that this process can be responsible for the observed level of magnetic fields in young SNRs and may reason their source . The growth speed of the instability depends on the proportion between the gyrofrequency of relativistic protons and the frequency of plasma beams excited by them .This ratio falls with time as the number density of advanced substances rises downstream of the shock back . As a result , the instability saturates at some distance behind the shock back where the magnetic energy density becomes comparable to the kinetic power concentration of the flow .In order to estimate the saturation level we utilize an analytical method developed recently by Bell et al . ( 2013 ) .It enables us to estimate the spectrum of amplified magnetic fluctuations produced by cosmic ray streaming instability .",
        "rewrite_text": "Title: Magnetic Turbulence Generation by Streaming Cosmic Rays Upstream of Supernova Remnant Shocks\n\nAbstract: This study investigates the amplification of magnetic fields within supernova remnants (SNRs) driven by cosmic ray streaming instability, which arises from the anisotropic absorption of energetic particles along the mean magnetic field lines. Our findings suggest that this mechanism could account for the observed magnetic field strengths in young SNRs and may elucidate their origins. The growth rate of the instability is influenced by the ratio of the gyrofrequency of relativistic protons to the frequency of the plasma beams they generate. Notably, this ratio decreases over time as the density of the advanced materials increases downstream of the shock front. Consequently, the instability reaches a saturation point at a certain distance behind the shock, where the density of magnetic energy becomes comparable to the kinetic energy density of the flow. To assess the saturation level, we employ a recently developed analytical approach by Bell et al. (2013), which allows us to estimate the spectrum of magnetic fluctuations amplified by cosmic ray streaming instability. Our results provide insights into the dynamics of magnetic turbulence in SNRs and its implications for cosmic ray propagation and acceleration processes. This research contributes to a deeper understanding of the interplay between cosmic rays and magnetic fields in the context of astrophysical phenomena, highlighting the significance of streaming instability in shaping the magnetic landscape of young supernova remnants.",
        "ori-fast-z-score": -0.6255432421712244,
        "water-fast-z-score": 4.458892287340829,
        "rewrite-fast-z-score": 3.2716515254078793
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolution of Magnetic Fields in Stars Across the Upper Main Sequence: II. Observed distribution of the magnetic field geometry .\nAbstract:\nWe present an analysis of the observed distribution of the magnetic field geometries for stars across the upper main sequence (UMS). We use data on the projected surface magnetic fields and rotation periods, as well as stellar parameters derived by spectroscopic observations, to determine the fraction of oblique rotators among UMS stars with different masses and ages. The results are compared with predictions based on dynamo models that include effects of differential rotation. \n \n In our sample we find that the fraction of obliquely rotating stars increases towards lower masses: it is about 50% for F-type dwarfs but only 20% for G-type giants. This trend can be explained if the large-scale magnetic fields generated by dynamos operating at the base of convective envelopes become more complex during evolution along the red giant branch. Our findings also suggest that the fraction of obliquely-rotating stars decreases with age. For example, this fraction is higher than 80% for young open clusters such as NGC 2516 or Pleiades, while it drops below 40% for old open clusters like M67.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolution of Magnetic Fields in Stars Across the Upper Main Sequence : II . Observed distribution of the magnetic field structures .Abstract : We present an assessment of the seen distribution of the magnetic field geometries for stars across the upper primary sequence ( UMS ) . We use data on the projected surface magnetic fields and rotation periods , as also as stellar characteristics obtained by spectroscopic observations , to estimate the fraction of oblique rotators among UMS stars with various masses and ages .The results are compared with predictions based on dynamo models that include effects of differential rotation . In our sample we find that the fraction of obliquely rotating stars increases towards lesser masses : it is about 50 % for F - class dwarfs but only 20 % for G - class giants .This trend can be understood if the huge - scale magnetic waves generated by dynamos working at the base of convective envelopes get more sophisticated during progression along the red giant branch . Our findings also suggest that the fraction of obliquely - spinning stellar reduces with age .For instance , this amount is higher than 80 % for young open nuclei such as NGC 2516 or Pleiades , while it drops below 40 % for old close galaxies like M67 .",
        "rewrite_text": "We provide a comprehensive analysis of the observed distribution of magnetic field structures in stars located along the upper main sequence (UMS). Utilizing data on projected surface magnetic fields and rotation periods, along with stellar characteristics derived from spectroscopic observations, we aim to quantify the prevalence of oblique rotators among UMS stars of varying masses and ages. Our findings are juxtaposed with theoretical predictions from dynamo models that account for the influence of differential rotation. Our analysis reveals a notable trend: the proportion of obliquely rotating stars tends to increase with decreasing mass. Specifically, we observe that approximately 50% of F-type dwarfs exhibit oblique rotation, in contrast to only about 20% of G-type giants. This observed pattern can be interpreted through the lens of large-scale magnetic waves generated by dynamos operating at the base of convective envelopes, which appear to become more complex as stars evolve along the red giant branch. Additionally, our results indicate a decline in the fraction of obliquely rotating stars with age. For instance, in younger open clusters such as NGC 2516 and the Pleiades, the fraction exceeds 80%, whereas it falls below 40% in older, more evolved clusters like M67. These findings contribute to our understanding of the evolution of magnetic fields in stars and highlight the intricate relationship between stellar age, mass, and magnetic field geometry.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 4.444671196029727,
        "rewrite-fast-z-score": -0.0949157995752499
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dalitz plot analysis of the D+ to K-pi+pi+ decay in the FOCUS experiment .\nAbstract:\nThe Dalitz plot distribution for the decay D+ ->K-pi+pi+ is measured using data collected by the FOCUS experiment at Fermilab, corresponding to an integrated luminosity of 1 fb-1 . The measurement uses a sample of about 2 million events with one charged track and two neutral clusters reconstructed in the central drift chamber (CDC) and electromagnetic calorimeter (EMC). A maximum likelihood fit is performed on this sample to extract the branching fraction B(D+ ->K-pi+pipi+), which is found to be  _ = (1.55 +/- 0.10 ) x 10-3 , where the uncertainty includes both statistical and systematic contributions.  This result agrees well with previous measurements but has improved precision due to the larger number of signal events used here compared to earlier results. It also improves upon the most recent theoretical prediction based on lattice QCD calculations. The ratio Rc/D between the Cabibbo-suppressed and Cabibbo-favored decays into three pions is determined as Rc/D=(0.84+0.11-0.12)x10-2.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dalitz plot analysis of the D + to K - pi + pi + decay in the FOCUS experiment . Abstract : The Dalitz plot distribution for the decay D + - > K - pi + pi + is measured using data taken by the FOCUS experiment at Fermilab , equivalent to an integrated luminosity of 1 fb - 1 .The measurement involves a sample of about 2 million events with one charged track and two neutral clusters preserved in the main drift chamber ( CDC ) and electromagnetic calorimeter ( EMC ) . A maximum likelihood fit is conducted on this specimen to extract the branching fraction B ( D + - > K - pi + pipi + ) , which is found to be _ = ( 1 . 55 + / - 0 . 10 ) x 10 - 3 , where the uncertainty includes both statistical and systematic contributions .This result agrees well with previous measurements but has improved precision thanks to the bigger quantity of signal events employed here relative to earlier findings . It additionally improves upon the most current theoretical estimate based on lattice QCD calculations .The ratio Rc / D between the Cabibbo - subdued and Cabibbo - preferred decays into three pions is calculated as Rc / D = ( 0 . 84 + 0 . 11 - 0 . 12 ) x10 - 2 .",
        "rewrite_text": "Title: Dalitz Plot Analysis of the D+ to K-π+π+ Decay in the FOCUS Experiment\n\nAbstract: This study presents a comprehensive analysis of the Dalitz plot distribution for the decay process D+ → K-π+π+, utilizing data collected by the FOCUS experiment at Fermilab, which corresponds to an integrated luminosity of 1 fb^-1. The analysis is based on a substantial dataset comprising approximately 2 million events characterized by one charged track and two neutral clusters, recorded in the main drift chamber (CDC) and the electromagnetic calorimeter (EMC). To extract the branching fraction B(D+ → K-π+π+), a maximum likelihood fit is performed on the collected data, yielding a result of B(D+ → K-π+π+) = (1.55 ± 0.10) x 10^-3. This measurement incorporates both statistical and systematic uncertainties and demonstrates strong agreement with previous results, while offering enhanced precision due to the larger number of signal events analyzed compared to earlier studies. Furthermore, this result provides an improved estimate over the latest theoretical predictions derived from lattice QCD calculations. Additionally, the study calculates the ratio Rc/D, which compares the rates of Cabibbo-suppressed and Cabibbo-favored decays into three pions, resulting in Rc/D = (0.84 +0.11 -0.12) x 10^-2. This work not only contributes valuable data to the understanding of D meson decays but also reinforces the consistency between experimental findings and theoretical models in particle physics.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 4.900304787764432,
        "rewrite-fast-z-score": -0.8
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Cooperative Scheduling of Multiple Bag-of-Task Applications .\nAbstract:\nWe consider the problem of scheduling multiple bag-of-work applications on parallel machines with non-cooperative tasks, where each task has its own deadline and budget constraints. We propose an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets. The proposed algorithm uses dynamic programming to find the optimal schedule for these intervals. Finally, we show how this approach can be extended to handle more general cases by using bin-packing techniques. Our experimental results demonstrate significant performance improvements over existing algorithms. In particular, our algorithm outperforms state-of-the-art approaches when there are many small tasks and/or tight deadlines/budgets. Keywords: Parallel computing, Computational complexity theory, Computational geometry, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Cooperative Scheduling of Multiple Bag - of - Task Applications . Abstract : We consider the issue of scheduling multiple bag - of - work applications on parallel machines with non - cooperative tasks , where each work has its own deadline and funding limitations .We suggest an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets . The proposed algorithm employs dynamic programming to find the ideal schedule for these periods .Finally , we prove how this methodology can be improved to treat more general instances by using bin - packing methods . Our research results show considerable performance improvements over existing algorithms .In particular , our algorithm outperforms state - of - the - art methods when there are many small tasks and / or tight deadlines / budgets . Keywords : Parallel computing , Computational complexity analysis , Computational topology , Data mining , Bioinformatics , High - performance computing , Grid computing , Cloud computing , Big data analysis , Distributed systems",
        "rewrite_text": "Title: Non-Cooperative Scheduling of Multiple Bag-of-Task Applications\n\nAbstract: This study addresses the challenge of scheduling multiple bag-of-task applications on parallel machines, particularly in scenarios involving non-cooperative tasks. Each task is characterized by its own specific deadline and funding constraints, complicating the scheduling process. To tackle this issue, we propose a novel algorithm that partitions time into discrete intervals, allowing for the simultaneous scheduling of all tasks within a given interval without breaching their respective deadlines or budgetary limits. Our approach leverages dynamic programming techniques to optimize the scheduling within these defined periods. Furthermore, we demonstrate how our methodology can be enhanced to accommodate a broader range of scenarios by integrating bin-packing strategies. The results of our research indicate significant performance gains compared to existing scheduling algorithms. Notably, our algorithm excels in situations characterized by a high volume of small tasks and/or stringent deadlines and budget constraints, outperforming current state-of-the-art methods. This work contributes to the fields of parallel computing and computational complexity analysis, with implications for various applications including data mining, bioinformatics, high-performance computing, grid and cloud computing, big data analysis, and distributed systems. Our findings underscore the potential for improved efficiency in scheduling non-cooperative tasks, paving the way for advancements in the management of complex computational workloads. \n\nKeywords: Parallel computing, Computational complexity analysis, Computational topology, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 5.019011475427825,
        "rewrite-fast-z-score": 0.3481553119113957
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deviations from Tri-bimaximal Neutrino Mixing in Type-II Seesaw and Leptogenesis .\nAbstract:\nWe study deviations from tri-bimaximal neutrino mixing in the type-II seesaw model with an additional U(1) gauge symmetry, which is broken at high energy scales by two Higgs doublets. We show that this scenario can be realized within the framework of supersymmetric grand unified theories (SUSY GUTs). In particular we consider SO(10) SUSY GUT models where the right-handed Majorana mass matrix for the light neutrinos arises from the vacuum expectation values of three pairs of Higgs fields transforming as 10 ⊕ 126 ⊕ 120 under SU(5), while the Dirac mass matrices are generated through Yukawa couplings to 16-plet fermions. The resulting effective neutrino mass matrix has off-diagonal entries proportional to the ratio between the masses of the heavy and light neutrinos. This leads to corrections to the tribimaximal pattern of neutrino mixings. Furthermore, we discuss leptogenesis in our model and find that it can account for the observed baryon asymmetry of the universe if the CP violating phases satisfy certain conditions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deviations from Tri - bimaximal Neutrino Mixing in Type - II Seesaw and Leptogenesis . Abstract : We research deviations from tri - bimaximal neutrino mixes in the class - II seesaw model with an additional U ( 1 ) gauge symmetry , which is shattered at high energy scales by two Higgs doublets .We see that this situation can be realized within the framework of supersymmetric grand unified theories ( SUSY GUTs ) . In particular we define SO ( 10 ) SUSY GUT models where the right - handed Majorana mass vector for the light neutrinos emerges from the vacuum expectation values of three sets of Higgs fields transforming as 10 ⊕ 126 ⊕ 120 under SU ( 5 ) , while the Dirac mass matrices are produced through Yukawa couplings to 16 - plet fermions .The resulting effective neutrino mass vector has off - diagonal entries proportional to the proportion between the masses of the heavy and light neutrinos . This leads to corrections to the tribimaximal pattern of neutrino mixings .Furthermore , we investigate leptogenesis in our model and find that it can account for the seen baryon asymmetry of the universe if the CP violating stages fulfill certain conditions .",
        "rewrite_text": "In this study, we explore the deviations from tri-bimaximal neutrino mixing within the context of the Type-II seesaw mechanism, augmented by an additional U(1) gauge symmetry that is broken at high energy scales through the involvement of two Higgs doublets. Our analysis is situated within the framework of supersymmetric grand unified theories (SUSY GUTs), specifically focusing on SO(10) SUSY GUT models. In these models, the right-handed Majorana mass vector for light neutrinos is derived from the vacuum expectation values of three distinct sets of Higgs fields, which transform as 10, 126, and 120 representations under SU(5). Meanwhile, the Dirac mass matrices are generated via Yukawa interactions with 16-plet fermions.\n\nThe effective neutrino mass matrix we obtain exhibits off-diagonal elements that are proportional to the mass ratio of heavy to light neutrinos, resulting in modifications to the standard tri-bimaximal mixing pattern. This deviation is significant as it provides a mechanism to account for the observed mixing angles in neutrino oscillation experiments. Additionally, we delve into the implications of our model for leptogenesis, a process that could explain the observed baryon asymmetry of the universe. We demonstrate that our framework can successfully generate the necessary conditions for CP violation, which is crucial for leptogenesis to occur. The findings suggest that the interplay between neutrino mass generation and baryogenesis can lead to a deeper understanding of the fundamental symmetries and dynamics governing particle physics and cosmology. Overall, our research contributes to the ongoing discourse on neutrino physics and its implications for the early universe, highlighting the potential of SUSY GUTs in addressing these profound questions.",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": 1.3348476249438292
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Curious Case of NGC6908 .\nAbstract:\nThe Hubble Space Telescope has revealed the most detailed view yet of an active galactic nucleus (AGN) in the galaxy NGC6908, located about 300 million light years away.  The AGN is powered by supermassive black holes that are surrounded by bright clouds of gas and dust called torii.   This image shows how these torii appear when they are illuminated by powerful radiation coming out of the central engine of the AGN.   ... Full text here . \n \n Image credits: NASA, ESA, STScI, A. Simionescu et al. (University of Leicester), DSS2, Digitized Sky Survey 2.0, Aladin sky atlas developed at CDS, Strasbourg Observatory; NRAO/AUI/NSF, National Radio Astronomy Observatory, which is operated by Associated Universities Inc., under cooperative agreement with the National Science Foundation. This work was supported by NASA grant NNX10AD65G to University of Leicester.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Curious Case of NGC6908 . Abstract : The Hubble Space Telescope has confirmed the most detailed view ever of an active galactic nucleus ( AGN ) in the galaxy NGC6908 , located about 300 million light years far .The AGN is powered by supermassive black holes that are surrounded by intense layers of gas and dust called torii . This image shows how these torii appear when they are illuminated by massive radiation coming out of the main motor of the AGN .. . . Full text here . Image credits : NASA , ESA , STScI , A . Simionescu et al .( University of Leicester ) , DSS2 , Digitized Sky Survey 2 . 0 , Aladin sky atlas constructed at CDS , Strasbourg Observatory ; NRAO / AUI / NSF , National Radio Astronomy Observatory , which is controlled by Associated Universities Inc . , under partnership agreement with the National Science Foundation . This project was supported by NASA gift NNX10AD65G to University of Leicester .",
        "rewrite_text": "Title: The Intriguing Case of NGC6908\n\nAbstract: The Hubble Space Telescope has provided an unprecedented and detailed observation of an active galactic nucleus (AGN) within the galaxy NGC6908, situated approximately 300 million light-years from Earth. This AGN is fueled by a supermassive black hole, which is enveloped by dense layers of gas and dust known as torii. The newly captured imagery reveals the striking appearance of these torii when illuminated by the intense radiation emitted from the AGN's central engine. The findings enhance our understanding of the complex interactions between supermassive black holes and their surrounding environments, shedding light on the processes that govern AGN activity. The research team, including A. Simionescu from the University of Leicester, utilized advanced imaging techniques to analyze the structure and composition of the torii, offering insights into their role in the AGN's dynamics. This study not only contributes to the broader field of astrophysics but also emphasizes the capabilities of the Hubble Space Telescope in exploring distant cosmic phenomena. The project received support from NASA grant NNX10AD65G awarded to the University of Leicester. The accompanying image credits include contributions from NASA, ESA, STScI, and various astronomical institutions, highlighting the collaborative effort in advancing our knowledge of the universe. Full text available here.",
        "ori-fast-z-score": 0.2886751345948129,
        "water-fast-z-score": 4.714285714285714,
        "rewrite-fast-z-score": 1.5724272550828775
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A survey of debris trails from short-period comets .\nAbstract:\nWe present the results of an analysis of all available data on cometary dust tails, including those observed by spacecraft and ground-based telescopes in recent years. We find that most of these objects are associated with Jupiter family comets (JFCs), which have orbital periods less than 20 yr. The JFCs produce dust tails that can be traced for up to several thousand AU along their orbits. These tails appear as narrow streams of material extending outward at high speed from the parent bodies. In some cases they show evidence of being disrupted into multiple fragments or branches. Most of the tail structures we observe are consistent with models where particles are released continuously over time scales ranging from months to thousands of years. However, there is growing observational evidence suggesting that many of these tails may also contain significant amounts of freshly produced dust grains ejected during more recent outburst events. This suggests that the production rate of dust particles in these systems varies significantly both spatially and temporally. \n \n Keywords: Comet",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A review of debris trails from short - period comets . Abstract : We report the conclusion of an assessment of all available data on cometary dust tails , particularly those observed by satellites and land - based telescopes in recent seasons .We see that most of these objects are identified with Jupiter class comets ( JFCs ) , which have orbital periods fewer than 20 yr . The JFCs produce dust tails that can be traced for up to several thousand AU along their orbits .These tails occur as short streams of debris extending outward at high velocity from the parent bodies . In some cases they show proof of being disrupted into multiple pieces or limbs .Most of the tail structures we study are compatible with models where objects are released constantly over time scales extending from months to thousands of years . However , there is growing observational evidence indicating that several of these tails might additionally include significant amounts of newly released dust grains ejected during more recent outburst events .This implies that the production frequency of dust particles in these systems vary significantly both spatially and temporally . Keywords : Comet",
        "rewrite_text": "Title: A Review of Debris Trails from Short-Period Comets\n\nAbstract: This article presents a comprehensive evaluation of the existing data concerning cometary dust tails, with a particular focus on observations made by both satellite and ground-based telescopes in recent years. Our findings indicate that the majority of the examined objects are associated with Jupiter Family Comets (JFCs), which are characterized by their orbital periods of less than 20 years. These JFCs generate dust tails that can extend for several thousand astronomical units (AU) along their trajectories. The dust tails are typically observed as short streams of debris that are ejected at high velocities from the parent comets. Notably, some tails exhibit signs of fragmentation, appearing as multiple pieces or limbs. \n\nThe tail structures analyzed in this review align with theoretical models suggesting that dust particles are released continuously over time scales ranging from months to thousands of years. However, there is an increasing body of observational evidence that suggests several of these tails may also contain substantial quantities of newly released dust grains, which are expelled during more recent outburst events. This observation points to a significant variability in the production frequency of dust particles within these cometary systems, both in terms of spatial distribution and temporal occurrence. The implications of these findings enhance our understanding of the dynamics of cometary dust production and the complex processes governing the evolution of cometary debris trails. \n\nKeywords: Comet, Jupiter Family Comets, dust tails, debris trails, observational data.",
        "ori-fast-z-score": -0.10153461651336192,
        "water-fast-z-score": 7.273098320775917,
        "rewrite-fast-z-score": 2.475085941976171
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Discrete Choices under Social Influence: Generic Properties .\nAbstract:\nWe study the generic properties of discrete choice models with social influence, where agents make decisions based on their own private information and the choices made by others in their neighborhood.  We show that if there is no correlation between individuals  private signals then any equilibrium must be symmetric across individuals (i.e., all equilibria are identical up to relabeling). If instead there exists some positive correlation among individuals  private signals, we prove that there always exist asymmetric equilibria. Finally, for intermediate levels of correlation, we provide examples showing both types of equilibria can co-exist. These results hold regardless of whether or not agents have common knowledge about each other s payoffs. \nOur analysis relies heavily on recent advances in the theory of global games. The main idea behind our approach is to view an agent s decision problem as a two-stage game played against nature. At stage one, Nature chooses her signal realization; at stage two, she makes her choice conditional on this realized signal.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Discrete Choices under Social Influence : Generic Properties . Abstract : We research the generic properties of discrete choosing models with social impact , where agents create choices based on their own private information and the decisions created by others in their neighborhood .We show that if there is no correlation between individuals private signals then any equilibrium must be symmetric across individuals ( i . e . , all equilibria are identical up to relabeling ) . If instead there exists some positive correlation among individuals private signals , we prove that there always exist asymmetric equilibria .Finally , for intermediate levels of correlation , we provide examples demonstrating both types of equilibria can co - arise . These data hold regardless of whether or not agents have common knowledge about each other s payoffs .Our study depends heavily on current developments in the notion of international matches . The main idea behind our approach is to see an agent s decision problem as a two - phase play played against nature .At phase one , Nature chooses her signal realization ; at stage two , she gives her decision conditional on this realized signal .",
        "rewrite_text": "Title: Discrete Choices under Social Influence: Generic Properties\n\nAbstract: This study investigates the fundamental characteristics of discrete choice models influenced by social factors, where individuals make decisions based on their private information as well as the choices of others within their social network. We establish that in scenarios where there is no correlation among the private signals of individuals, all equilibria must exhibit symmetry, meaning that they are indistinguishable apart from the relabeling of individuals. Conversely, when a positive correlation exists among the private signals, we demonstrate that asymmetric equilibria are guaranteed to emerge. Furthermore, in cases of intermediate correlation levels, we present examples that illustrate the coexistence of both symmetric and asymmetric equilibria. Notably, these findings remain valid irrespective of whether agents possess common knowledge regarding each other's payoffs. Our research builds upon recent advancements in the understanding of international matches, framing the decision-making process of an agent as a two-phase interaction with nature. In the first phase, nature determines the realization of her signal, while in the second phase, the agent makes a decision based on this realized signal. This dual-phase approach allows for a comprehensive analysis of how social influence shapes individual choices in discrete settings, contributing to the broader discourse on decision-making under uncertainty and social interaction.",
        "ori-fast-z-score": -1.3627702877384937,
        "water-fast-z-score": 4.088310863215481,
        "rewrite-fast-z-score": 1.9425717247145282
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Enhanced Half-Metallicity in Edge-Oxidized Zigzag Graphene Nanoribbons .\nAbstract:\nWe report on the electronic structure and magnetic properties of zigzag graphene nanoribbons (ZGNRs) with different edge structures, including hydrogenated ZGNR (H-ZGNR), fluorinated ZGNR (F-ZGNR), oxygenated ZGNR (O-ZGNR), and nitrogen-doped O-ZGNR (N-ZGNR). We find that all these ZGNRs are half-metals except for H-ZGNR which is metallic. The band gaps of F-ZGNR and N-ZGNR increase as compared to those of pristine ZGNR due to the electronegativity difference between carbon atoms at edges and their neighbors. In contrast, the band gap decreases slightly when oxygen replaces one or two carbon atoms at each edge because of charge transfer from oxygen to neighboring carbon atom(s). Our results show that the spin polarization can be enhanced by introducing oxygen into the edges of ZGNRs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Enhanced Half - Metallicity in Edge - Oxidized Zigzag Graphene Nanoribbons . Abstract : We report on the electronic structure and magnetic properties of zigzag graphene nanoribbons ( ZGNRs ) with various edge structures , notably hydrogenated ZGNR ( H - ZGNR ) , fluorinated ZGNR ( F - ZGNR ) , oxygenated ZGNR ( O - ZGNR ) , and nitrogen - doped O - ZGNR ( N - ZGNR ) .We see that all these ZGNRs are half - metals except for H - ZGNR which is metallic . The band holes of F - ZGNR and N - ZGNR increase as compared to those of pristine ZGNR due to the electronegativity difference between carbon atoms at corners and their relatives .In contrast , the band gap falls slightly when oxygen replaces one or two carbon atoms at each edge because of charge transfer from oxygen to neighboring carbon atom ( s ) . Our results show that the spin polarization can be enhanced by bringing oxygen into the edges of ZGNRs .",
        "rewrite_text": "We present a comprehensive study on the electronic and magnetic characteristics of zigzag graphene nanoribbons (ZGNRs) featuring various edge modifications, including hydrogenation (H-ZGNR), fluorination (F-ZGNR), oxidation (O-ZGNR), and nitrogen doping (N-ZGNR). Our findings reveal that all modified ZGNRs exhibit half-metallic behavior, with the exception of H-ZGNR, which behaves as a metallic conductor. Notably, we observe that the band gaps of F-ZGNR and N-ZGNR are significantly larger compared to those of pristine ZGNR. This enhancement is attributed to the differences in electronegativity between the carbon atoms located at the edges and their adjacent counterparts. Conversely, the introduction of oxygen at the edges results in a slight reduction of the band gap when one or two carbon atoms are replaced, a phenomenon linked to charge transfer from the oxygen atoms to the neighboring carbon atoms. Our results indicate that incorporating oxygen into the edges of ZGNRs can significantly enhance spin polarization, making these materials promising candidates for spintronic applications. This work not only deepens our understanding of the electronic properties of edge-modified ZGNRs but also opens avenues for the design of advanced materials with tailored magnetic and electronic functionalities.",
        "ori-fast-z-score": 0.13736056394868904,
        "water-fast-z-score": 2.721655269759087,
        "rewrite-fast-z-score": -0.2182178902359924
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Role of the rho meson in the description of pion electroproduction experiments at JLab .\nAbstract:\nWe present results on the role played by the rho meson in describing pion electroproduction data obtained with the CLAS detector at Jefferson Lab (JLab). The analysis is performed within an effective field theory approach, where we use chiral perturbation theory to describe the interaction between pions and nucleons up to next-to-leading order. We then introduce vector-meson degrees of freedom through the hidden gauge formalism. In particular, we consider contributions coming from one-loop diagrams involving rho mesons as well as tree-level processes mediated by rho mesons. Our theoretical framework allows us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single-pion production off protons. Using this model, we are able to reproduce experimental data for all these observables simultaneously. Finally, we discuss how our findings can be used to extract information about the properties of the rho meson. This work was supported by the U.S. \n\n\nDepartment of Energy under Contract No. DE-SC0012704.\n\nPACS numbers: 12.38.Mh",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Role of the rho meson in the description of pion electroproduction research at JLab . Abstract : We report findings on the part played by the rho meson in understanding pion electroproduction data received with the CLAS detector at Jefferson Lab ( JLab ) .The investigation is conducted within an efficient field theory approach , where we using chiral perturbation theory to explain the interaction between pions and nucleons up to next - to - leading order . We then introduce vector - meson degrees of liberty through the hidden gauge formalism .In particular , we treat contributions come from one - loop diagrams featuring rho mesons as well as tree - level processes induced by rho mesons . Our conceptual formulation enables us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single - pion production off protons .Using this model , we are able to publish empirical data for all these observables simultaneously . Finally , we explain how our findings can be used to extract information about the properties of the rho meson .This project was supported by the U . S . Department of Energy under Contract No . DE - SC0012704 .PACS numbers : 12 . 38 . Mh",
        "rewrite_text": "In this article, we present our research on the significance of the rho meson in the context of pion electroproduction, utilizing data collected by the CLAS detector at Jefferson Lab (JLab). Our study employs an effective field theory framework, specifically leveraging chiral perturbation theory to analyze the interactions between pions and nucleons up to next-to-leading order. To enhance our understanding, we incorporate vector-meson degrees of freedom through the hidden gauge formalism. This approach allows us to systematically account for contributions arising from one-loop diagrams that include rho mesons, as well as tree-level processes that are mediated by these mesons.\n\nOur theoretical framework facilitates the examination of both neutral current processes, such as elastic electron-proton scattering, and charged current processes, including single-pion production from protons. By applying this model, we are able to simultaneously analyze and publish empirical data for a range of observables related to these reactions. Furthermore, we discuss how our results can be utilized to extract valuable information regarding the properties of the rho meson, thereby enhancing our understanding of its role in the dynamics of pion electroproduction.\n\nThis research was conducted with the support of the U.S. Department of Energy under Contract No. DE-SC0012704. The findings contribute to the broader field of particle physics, particularly in the study of meson interactions and their implications for nuclear structure and reactions. The PACS number associated with this work is 12.38.Mh, indicating its relevance to the field of quantum chromodynamics and effective field theories.",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 3.9378076532410837,
        "rewrite-fast-z-score": 0.19245008972987526
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Room temperature spin polarized magnetic semiconductor .\nAbstract:\nWe report on the room-temperature ferromagnetism in Mn-doped ZnO thin films grown by pulsed laser deposition (PLD). The Curie temperatures are found to be around 300 K for all samples with different doping levels, which is much higher than that reported previously. We also find that the magnetization increases linearly as the applied field decreases and shows hysteresis loops at low fields. These results indicate that the observed ferromagnetic behavior may originate from exchange coupling between localized spins rather than intrinsic ferromagnetism. \n \n In recent years, there has been growing interest in developing new materials for spintronic applications such as nonvolatile memory devices or logic circuits based on the manipulation of electron spins instead of charge carriers1-5 . Among these materials, diluted magnetic semiconductors have attracted considerable attention because they can combine both electronic and magnetic functionalities into one material6-8 .\n \n\n\nZnO-based DMSs have been extensively studied due to their wide band gap energy (3.37 eV), large exciton binding energy (60 meV)9 , high transparency10-12 , and good chemical stability13-15 . However, it remains challenging to achieve room-temperature ferromagnetically ordered states in ZnO-based DMSs16-18 . Although several groups have recently demonstrated room-temperature ferromagnetic ordering in various types of ZnO-based DMS systems19-24 , most of them show relatively small saturation magnetizations25-27 . \n \n Here we report on the observation of room-temperature ferromagnetisms in Mn-doped ZnObased DMSs prepared using pulsed laser deposition28-30 . Our experimental data clearly demonstrate that the dopant concentration plays an important role in determining the Curie temperature31-33 . For example, our sample with x = 0.5% exhibits a Curie temperature of about 300 K while those with lower concentrations exhibit smaller values ranging from 150-250 K34-36 . Moreover, we observe that the magnetization increases almost linearly when decreasing the external magnetic field below 1 T and displays hysteretic behaviors at very low fields. This indicates that the observed ferr",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Room temperature spin polarized magnetic semiconductor . Abstract : We report on the room - temperature ferromagnetism in Mn - doped ZnO thin films developed by pulsed laser deposition ( PLD ) .The Curie temperatures are found to be around 300 K for all specimens with varying doping rates , which is much higher than that confirmed previously . We additionally find that the magnetization increases linearly as the applied field decreases and shows hysteresis loops at low areas .These data indicate that the studied ferromagnetic activity may originate from exchange interactions between scattered spinning rather than intrinsic ferromagnetism . In past decades , there has been growing interest in building new materials for spintronic use such as nonvolatile memory devices or logic devices based on the manipulation of electron spins rather of charge carriers1 - 5 .Among these materials , diluted magnetic semiconductors have garnered considerable scrutiny because they can mix both electronic and magnetic functionalities into one material6 - 8 . ZnO - based DMSs have been heavily explored thanks to their wide band gap energy ( 3 . 37 eV ) , large exciton binding energy ( 60 meV ) 9 , large transparency10 - 12 , and good molecular stability13 - 15 .However , it remains challenging to achieve room - temperature ferromagnetically ordered states in ZnO - based DMSs16 - 18 . Although several teams have recently shown room - temperature ferromagnetic ordering in different kinds of ZnO - based DMS systems19 - 24 , most of them show relatively small saturation magnetizations25 - 27 .Here we publish on the observation of room - temperature ferromagnetisms in Mn - doped ZnObased DMSs assembled utilizing pulsed laser deposition28 - 30 . Our research data distinctly show that the dopant concentration acts an important role in establishing the Curie temperature31 - 33 .For instance , our sample with x = 0 . 5 % experiences a Curie temperature of about 300 K while those with lesser levels demonstrate lower values ranging from 150 - 250 K34 - 36 . Moreover , we find that the magnetization increases almost linearly when decreasing the external magnetic force below 1 T and displays hysteretic behaviors at very low fields .This implies that the observed ferr",
        "rewrite_text": "**Title: Room Temperature Spin-Polarized Magnetic Semiconductor**\n\n**Abstract:** This study presents findings on the occurrence of room-temperature ferromagnetism in manganese (Mn)-doped zinc oxide (ZnO) thin films synthesized through pulsed laser deposition (PLD). Our results indicate that all samples exhibit Curie temperatures around 300 K, significantly surpassing previously reported values. Notably, we observe a linear increase in magnetization as the applied magnetic field decreases, alongside the emergence of hysteresis loops at low magnetic fields. These observations suggest that the ferromagnetic behavior may stem from exchange interactions among scattered spins rather than from intrinsic ferromagnetism. \n\nIn recent years, there has been a heightened interest in the development of novel materials for spintronic applications, such as non-volatile memory and logic devices that leverage the manipulation of electron spins instead of charge carriers. Diluted magnetic semiconductors (DMS) have attracted considerable attention due to their ability to integrate electronic and magnetic properties within a single material. Among these, ZnO-based DMSs have been extensively investigated owing to their advantageous characteristics, including a wide band gap energy of 3.37 eV, a large exciton binding energy of 60 meV, high optical transparency, and robust molecular stability. \n\nDespite these promising attributes, achieving room-temperature ferromagnetic ordering in ZnO-based DMSs has proven to be a significant challenge. While several research groups have recently reported room-temperature ferromagnetism in various ZnO-based DMS systems, many of these studies have been limited by relatively low saturation magnetizations. In this work, we detail our observations of room-temperature ferromagnetism in Mn-doped ZnO DMSs fabricated via PLD. Our findings reveal that the concentration of the dopant plays a crucial role in determining the Curie temperature; for instance, a sample with a doping level of 0.5% exhibits a Curie temperature of approximately 300 K, whereas samples with lower doping concentrations show Curie temperatures ranging from 150 K to 250 K. Furthermore, we note that magnetization increases almost linearly as the external magnetic field is reduced below 1 T, with hysteretic behavior observed at very low fields, indicating the complex nature of the ferromagnetic interactions in these materials.",
        "ori-fast-z-score": -0.08084520834544433,
        "water-fast-z-score": 7.526023228839096,
        "rewrite-fast-z-score": -0.5416762627738958
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prompt GeV-TeV Emission of Gamma-Ray Bursts Due to High-Energy Protons, Muons and Electron-Positron Pairs .\nAbstract:\nWe propose that the prompt emission of gamma-ray bursts (GRBs) is due to high-energy protons, muons and electron-positron pairs produced by ultra-relativistic shocks in GRB jets. The observed MeV-GeV spectrum can be explained as synchrotron radiation emitted by these particles accelerated at the shock front. We show that this model naturally explains why the peak energy of the observed spectrum decreases with time during the prompt phase. In addition, we find that our model predicts an anti-correlation between the duration of the prompt phase and the luminosity of the afterglow for short-hard GRBs. This prediction could be tested using future observations made by Fermi/LAT and Swift/BAT. Introduction -Gamma-ray bursts are brief flashes of high energy photons lasting only milliseconds or less  1  . They have been detected out to redshifts z = 8.2  2  , which implies their total energy output may exceed 10^53 erg  3  . Despite decades of research into the origin of GRBs there remains no consensus on how they work  4  .\nThe most popular models involve either black holes or neutron stars collapsing into a black hole  5  . However, it has recently become clear that many GRBs do not fit neatly into one category  6  . For example, some GRBs appear to contain two separate pulses  7, 8  while others exhibit extended periods of activity  9  . Furthermore, some GRBs seem to occur when two galaxies merge  10  . These complexities suggest that more than one mechanism might operate simultaneously  11  .\nIn recent years several authors  12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59   have proposed that GRBs are powered by relativistic jets e",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prompt GeV - TeV Emission of Gamma - Ray Bursts Due to High - Energy Protons , Muons and Electron - Positron Pairs . Abstract : We suggest that the prompt emission of gamma - ray bursts ( GRBs ) is due to large - energy protons , muons and electron - positron couples created by ultra - relativistic shocks in GRB jets .The observed MeV - GeV spectrum can be understood as synchrotron emission generated by these objects accelerated at the shock front . We see that this description readily explains why the maximum energy of the seen spectrum drops with time during the prompt phase .In addition , we find that our model predicts an counter - correlation between the duration of the prompt phase and the luminosity of the afterglow for short - hard GRBs . This prediction might be evaluated using later observations made by Fermi / LAT and Swift / BAT .Introduction - Gamma - ray bursts are mild flashes of high energy photons lasting only milliseconds or less 1 . They have been detected out to redshifts z = 8 . 2 2 , which implies their total energy output may exceed 10 ^ 53 erg 3 .Despite decades of research into the origin of GRBs there exists no discussion on how they work 4 . The most popular theories include either black holes or neutron galaxies exploding into a black hole 5 .However , it has recently become clear that several GRBs do not fit neatly into one category 6 . For instance , some GRBs occur to contain two separate pulses 7 , 8 while several display extended phases of intensity 9 .Furthermore , some GRBs appeared to arise when two galaxies merge 10 . These complexities indicate that more than one process may operate simultaneously 11 .In recent years many writers 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 have proposed that GRBs are powered by relativistic jets e",
        "rewrite_text": "**Title:** Prompt GeV-TeV Emission of Gamma-Ray Bursts Due to High-Energy Protons, Muons, and Electron-Positron Pairs\n\n**Abstract:** In this study, we propose that the prompt emission observed in gamma-ray bursts (GRBs) is primarily driven by high-energy protons, muons, and electron-positron pairs generated through ultra-relativistic shocks within GRB jets. The resulting MeV-GeV spectrum can be effectively interpreted as synchrotron radiation produced by these particles as they are accelerated at the shock front. Our analysis reveals that this framework accounts for the observed phenomenon where the maximum energy of the emitted spectrum diminishes over time during the prompt phase. Furthermore, our model predicts a counter-correlation between the duration of the prompt phase and the luminosity of the afterglow specifically for short-hard GRBs. This intriguing prediction can be tested against future observations from instruments such as Fermi/LAT and Swift/BAT.\n\nGamma-ray bursts are brief yet intense emissions of high-energy photons, typically lasting only milliseconds or less, and have been detected at redshifts as high as z = 8. This suggests that their total energy output may surpass 10^53 erg. Despite extensive research spanning several decades, the underlying mechanisms driving GRBs remain poorly understood. The prevailing theories often attribute these bursts to the collapse of black holes or the explosion of neutron stars into black holes. However, recent findings indicate that many GRBs do not conform to these singular models. For example, some bursts exhibit dual pulses, while others display prolonged phases of intensity. Additionally, certain GRBs appear to be associated with the merging of galaxies. These complexities suggest that multiple processes may be at play simultaneously. In light of these observations, our work contributes to the ongoing discourse by suggesting that relativistic jets are a significant factor in the generation of GRBs, thereby enhancing our understanding of these enigmatic cosmic events.",
        "ori-fast-z-score": -0.4583492485141057,
        "water-fast-z-score": 7.061788191316445,
        "rewrite-fast-z-score": 0.16783627165933782
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Un Resultat Gravimetrique pour la Renaissance de la Theorie Corpusculaire  An Experimental Gravimetric Result for the Revival of Corpuscular Theory  .\nAbstract:\nLa gravite est une force fondamentale qui agit sur tous les corps materiels, et dont l action se manifeste par l attraction entre ces derniers. La gravitation newtonienne ne peut expliquer que le phenomene de la chute des corps vers un point unique au centre du systeme solaire. Les theories relativistes d Einstein ont permis d expliquer ce phenomene en introduisant la notion d espace-temps courbe. Cependant, il existe d autres phenomenes physiques tels que l effet Casimir ou encore celui de la pression de radiation electromagnetique qui necessitent l introduction d une nouvelle dimension dans l espace temps. Dans cette these nous proposons d introduire une nouvelle dimension spatiale afin d etendre la geometrie euclidienne classique. Nous montrons alors qu il est possible d interpreter la gravite comme une manifestation macroscopique d un effet microscopique lie aux fluctuations quantiques de l espacetemps. Cette interpretation permet ainsi de retrouver les predictions theoriques de la physique actuelle concernant la dynamique des astres tout en proposant une explication rationnelle a certains phenomenes physiques non expliques jusqu a present.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Un Resultat Gravimetrique pour la Renaissance de la Theorie Corpusculaire An Experimental Gravimetric Result for the Revival of Corpuscular Theory . Abstract : La gravite est le power fondamentale qui agit à tous les corps materiels , et dont l action se manifeste par l attraction entre ces derniers .La gravitation newtonienne ne peut expliquer que le phenomene de la chute des corps vers un place unique au centre le systeme solaire . Les theories relativistes d Einstein ont permis d expliquer ce phenomene en introduisant la conception d espace - temps courbe .Cependant , el existe d autres phenomenes physiques tels se l effet Casimir ou encore celui de la pression de energy electromagnetique qui necessitent l introduction d la nouvelle dimension sur l espace temps . Dans cette these nous proposons d introduire une nouvelle dimension spatiale afin d etendre la geometrie euclidienne classique .Nous montrons alors qu il est possible d interpreter la gravite comme une manifestation macroscopique d un effet microscopique lie à fluctuations quantiques de l espacetemps . Cette interpretation permet ainsi de retrouver les predictions theoriques de la physique actuelle concernant le dynamique du astres tout en proposant une explication rationnelle a certains phenomenes physiques non expliques jusqu a present .",
        "rewrite_text": "**Title:** An Experimental Gravimetric Result for the Revival of Corpuscular Theory\n\n**Abstract:** Gravity is the fundamental force that acts on all material bodies, manifesting through the attraction between them. While Newtonian gravitation adequately describes the phenomenon of objects falling toward a singular point at the center of the solar system, it falls short in addressing other complex physical phenomena. Einstein's relativistic theories have advanced our understanding by introducing the concept of curved spacetime, yet there remain unresolved issues, such as the Casimir effect and electromagnetic energy pressure, which suggest the need for an additional dimension in spacetime. In this paper, we propose the introduction of a new spatial dimension to extend classical Euclidean geometry. We demonstrate that gravity can be interpreted as a macroscopic manifestation of microscopic effects related to quantum fluctuations in spacetime. This interpretation not only aligns with current theoretical predictions regarding the dynamics of celestial bodies but also offers a rational explanation for several previously unexplained physical phenomena. By exploring this novel perspective, we aim to bridge gaps in our understanding of gravitational interactions and provide insights that could lead to a revival of corpuscular theory in the context of modern physics. Our findings suggest that incorporating an additional dimension may yield significant implications for both theoretical and experimental physics, potentially reshaping our comprehension of gravitational forces and their underlying mechanisms.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 2.8977276264040754,
        "rewrite-fast-z-score": -0.29002094671369905
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multivariable generalizations of the Schur class: positive kernel characterization and transfer function realization .\nAbstract:\nWe present new characterizations for multivariable functions in the Schur class, which are defined as those analytic on the open unit disk that satisfy certain growth conditions at infinity.  We show how to construct such functions by using an operator-theoretic approach based on the concept of a positive kernel. In particular, we prove that every function in the Schur class can be realized as the transfer function of some stable system with state space dimension equal to its McMillan degree (the number of poles inside the closed unit disk). This result is obtained via a generalization of the classical theorem due to Kalman-Yakubovich-Popov. Finally, we also provide necessary and sufficient conditions under which two given functions belong to the same Schur class. These results have applications in control theory and signal processing. The research leading to these results has received funding from the European Research Council under the Seventh Framework Programme (FP7/2007-2013) / ERC Grant Agreement n",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multivariable generalizations of the Schur class : positive kernel description and transfer function realization . Abstract : We introduce novel characterizations for multivariable functions in the Schur class , which are specified as those analytic on the open unit disk that fulfill certain growth conditions at infinity .We see how to build such functions by using an operator - theoretic approach based on the idea of a positive kernel . In particular , we prove that every function in the Schur class can be realized as the transfer function of some stable system with state space dimension equal to its McMillan degree ( the number of poles inside the closed unit disk ) .This result is achieved via a generalization of the classical principle attributed to Kalman - Yakubovich - Popov . Finally , we also obtain necessary and sufficient situations under which two given functions belong to the same Schur class .These data have applications in control theory and signal systems . The studies leading to these results has obtained support from the European Research Council under the Seventh Framework Programme ( FP7 / 2007 - 2013 ) / ERC Grant Agreement n",
        "rewrite_text": "Title: Multivariable Generalizations of the Schur Class: Positive Kernel Description and Transfer Function Realization\n\nAbstract: In this article, we present innovative characterizations of multivariable functions that belong to the Schur class, defined as those functions that are analytic within the open unit disk and satisfy specific growth conditions at infinity. Our approach leverages operator theory, particularly the concept of positive kernels, to construct these functions. A significant finding of our research is that every function within the Schur class can be represented as the transfer function of a stable system, where the state space dimension corresponds to its McMillan degree, which is determined by the number of poles located within the closed unit disk. This result is derived from a generalization of the well-known Kalman-Yakubovich-Popov theorem. Additionally, we establish necessary and sufficient conditions for determining when two functions are members of the same Schur class. The implications of our findings extend to various applications in control theory and signal processing systems. Our research has been supported by the European Research Council under the Seventh Framework Programme (FP7/2007-2013) through ERC Grant Agreement number [insert number].",
        "ori-fast-z-score": 0.22360679774997896,
        "water-fast-z-score": 4.555555555555555,
        "rewrite-fast-z-score": -0.6974858324629157
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High resolution radio continuum survey of M33: I. The radio maps .\nAbstract:\nWe present new high-resolution (0.5 arcsec) radio continuum observations at 1.4 GHz and 4.8 GHz made with the Australia Telescope Compact Array (ATCA). These data are combined to produce images in Stokes parameters I, Q, U and V which allow us to investigate both total intensity emission as well as linear polarization properties across the face of this nearby spiral galaxy. We find that the polarized emission is concentrated along the brightest regions of the disk where it reaches values up to ~8%. In addition we detect significant circularly polarized emission associated with two supernova remnants located near the center of the galaxy. This work represents an important step towards understanding magnetic field structure on kiloparsec scales within galaxies. It also provides valuable information for future studies aimed at investigating cosmic ray transport processes through galactic disks. \n \n Keywords: Radio astronomy, Galaxy evolution, Magnetic fields, Polarization",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High resolution signals continuum study of M33 : I . The radio mapping .Abstract : We report new high - resolution ( 0 . 5 arcsec ) radio continuum measurements at 1 . 4 GHz and 4 . 8 GHz made with the Australia Telescope Compact Array ( ATCA ) . These data are coupled to produce images in Stokes variables I , Q , U and V which allow us to examine both maximum frequency emission as well as linear polarization properties across the face of this adjacent spiral galaxy .We see that the polarized emission is confined along the brightest parts of the disk where it hits values up to ~ 8 % . In addition we find considerable circularly polarized emission associated with two supernova remnants situated near the center of the galaxy .This research constitutes an important milestone towards studying magnetic field stability on kiloparsec scales within stars . It additionally offers important information for future research targeted at studying cosmic ray flow processes through galactic disks .Keywords : Radio astronomy , Galaxy evolution , Magnetic fields , Polarization",
        "rewrite_text": "Title: High-Resolution Continuum Study of M33: I. Radio Mapping\n\nAbstract: In this study, we present new high-resolution radio continuum measurements at frequencies of 1.4 GHz and 4.8 GHz, achieved using the Australia Telescope Compact Array (ATCA). Our observations, with a resolution of 0.5 arcseconds, enable us to generate detailed images in Stokes parameters I, Q, U, and V. This comprehensive dataset allows for an in-depth analysis of both the maximum frequency emissions and the linear polarization characteristics across the surface of the nearby spiral galaxy M33. Our findings reveal that the polarized emission is predominantly concentrated in the brightest regions of the galaxy's disk, reaching levels of approximately 8%. Furthermore, we observe significant circularly polarized emissions linked to two supernova remnants located near the galactic center. This research marks a significant advancement in our understanding of magnetic field stability on kiloparsec scales within stellar environments. Additionally, it provides crucial insights for future investigations focused on the dynamics of cosmic ray propagation through galactic disks. The implications of our work extend to the broader fields of radio astronomy, galaxy evolution, and the study of magnetic fields and polarization phenomena. \n\nKeywords: Radio astronomy, Galaxy evolution, Magnetic fields, Polarization.",
        "ori-fast-z-score": -2.23606797749979,
        "water-fast-z-score": 4.555555555555555,
        "rewrite-fast-z-score": 1.4924050144892729
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Halo-model signatures from 380,000 SDSS Luminous Red Galaxies with photometric redshifts .\nAbstract:\nWe present the results of an analysis of the clustering properties of luminous red galaxies (LRGs) in the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use a sample of 380,000 LRGs selected to have 0.4 < zphot < 1.0 and Mr < --21.5 + 5logh. The angular correlation function is measured for this sample using the Landy & Szalay estimator on scales between 10  and 100 . To account for redshift space distortions we measure the projected cross-correlation functions wp(rp), where rp = Dproj/ H(z)/H0 , H(z) is the Hubble parameter at redshift z, and H0 is its value today. These measurements are made over a range of transverse separations corresponding to physical scales ranging from 2 h-1 Mpc to 20 h-1 Mpc. In addition, we also measure the real-space two-point correlation function by applying the method developed by Eisenstein et al. (2007) . This measurement is performed only out to a maximum separation of 60 h-1 Mpc due to the limited number density of our galaxy sample.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Halo - model signatures from 380 , 000 SDSS Luminous Red Galaxies with photometric redshifts . Abstract : We release the conclusion of an assessment of the clustering behavior of luminous red clusters ( LRGs ) in the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) .We use a sample of 380 , 000 LRGs chosen to have 0 . 4 < zphot < 1 . 0 and Mr < - - 21 . 5 + 5logh . The angular correlation function is measured for this sample using the Landy & Szalay estimator on scales between 10 and 100 .To account for redshift space distortions we measure the projected cross - correlation functions wp ( rp ) , where rp = Dproj / H ( z ) / H0 , H ( z ) is the Hubble parameter at redshift k , and H0 is its value today . These measurements are achieved over a range of transverse separations corresponding to physical scales ranging from 2 h - 1 Mpc to 20 h - 1 Mpc .In addition , we also measure the real - space two - point correlation function by using the method developed by Eisenstein et al . ( 2007 ) .This measurement is conducted only out to a maximum separation of 60 h - 1 Mpc owing to the limited number density of our galaxy sample .",
        "rewrite_text": "We present the findings of a comprehensive analysis focused on the clustering characteristics of luminous red galaxies (LRGs) utilizing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our study encompasses a substantial sample of 380,000 LRGs selected based on specific criteria: photometric redshifts in the range of 0.4 < zphot < 1.0 and absolute magnitudes of Mr < -21.5 + 5logh. To investigate the angular clustering of these galaxies, we employ the Landy & Szalay estimator, analyzing scales that span from 10 to 100 arcminutes. \n\nTo address the effects of redshift space distortions, we compute the projected cross-correlation functions, denoted as wp(rp), where rp is defined as Dproj / H(z) / H0. Here, H(z) represents the Hubble parameter at the given redshift, while H0 denotes its present-day value. Our analysis covers a range of transverse separations that correspond to physical scales between 2 h^-1 Mpc and 20 h^-1 Mpc. Furthermore, we also derive the real-space two-point correlation function utilizing the methodology established by Eisenstein et al. (2007). This particular measurement is limited to a maximum separation of 60 h^-1 Mpc due to the constraints imposed by the galaxy sample's number density.\n\nThe results of this study provide valuable insights into the large-scale structure of the universe as inferred from the clustering of LRGs. By examining the correlation functions in both angular and real space, we aim to enhance our understanding of the underlying cosmological principles governing galaxy formation and distribution. This research contributes to the broader field of astrophysics by offering empirical data that can be used to test and refine existing cosmological models.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 2.5927248643506746,
        "rewrite-fast-z-score": -0.3746343246326776
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HST/ACS Coronagraphic Observations of the Dust Surrounding HD 100546 .\nAbstract:\nWe present new Hubble Space Telescope (HST) Advanced Camera for Survey (ACS)\ncoronagraphic observations in visible light and near-infrared wavelengths that reveal an extended dusty disk surrounding the Herbig Ae star HD 100546, which is known to harbor a protoplanetary disk with spiral arms. The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the central star while allowing us to detect scattered light from circumstellar material located farther away. We find evidence for two bright rings of emission separated by ~0.5′′ along the major axis of the disk. These features are most likely due to scattering off large grains or planetesimals orbiting close to their parent stars. \n \n In addition, we have detected several dark gaps within these bright rings as well as fainter structures extending outward into the outer regions of the disk. Our results suggest that this system may be undergoing planet formation through gravitational interactions between larger bodies such as planets and/or planetesimals.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HST / ACS Coronagraphic Observations of the Dust Surrounding HD 100546 . Abstract : We report new Hubble Space Telescope ( HST ) Advanced Camera for Survey ( ACS ) coronagraphic observations in visible light and near - infrared wavelengths that expose an extended dusty disk surrounding the Herbig Ae star HD 100546 , which is known to harbor a protoplanetary disk with spiral arms .The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the main star while providing us to identify scattered radiation from circumstellar material located farther distant . We see evidence for two faint rings of emission separated by ~ 0 . 5 ′ ′ along the main axis of the disk .These features are most likely due to scattering off large grains or planetesimals orbiting close to their father planets . In addition , we have discovered numerous dark gaps within these dark rings as well as fainter forms extending outward into the inner regions of the disk .Our results propose that this system might be experiencing planet development through gravity interactions between big bodies such as planets and / or planetesimals .",
        "rewrite_text": "We present new coronagraphic observations conducted with the Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) that reveal an extensive dusty disk surrounding the Herbig Ae star HD 100546. This star is recognized for its protoplanetary disk, which features spiral arms. Utilizing the ACS coronagraph, we effectively blocked direct stellar light at small angular separations, allowing us to detect scattered light from circumstellar material located at greater distances. Our observations reveal two faint emission rings, approximately 0.5″ apart, aligned along the primary axis of the disk. These rings are likely the result of scattering from large grains or planetesimals that are in close orbit around their parent star. Furthermore, we identified several dark gaps within these emission rings, along with fainter structures extending outward into the inner regions of the disk. The presence of these features suggests that the HD 100546 system may be undergoing planet formation, driven by gravitational interactions among substantial bodies such as planets and planetesimals. Our findings contribute to the understanding of disk dynamics and the processes involved in planet formation in protoplanetary systems.",
        "ori-fast-z-score": 0.10721125348377948,
        "water-fast-z-score": 5.5432186252458715,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Elongated Structure of the Hercules dSph from Deep LBT Imaging .\nAbstract:\nWe present deep imaging data for the nearby dwarf spheroidal galaxy, Hercules (dSph), obtained with the Large Binocular Telescope (LBT). The new observations are used to study the structure and stellar populations in this system. We find that the surface brightness profile is well described by an exponential function over most of its extent but shows evidence for a break at about 30 arcsec radius. This feature may be associated with tidal disruption or stripping due to interactions between Hercules and other galaxies. Using colour-magnitude diagrams we show that there exists two distinct components within Hercules; one which has been stripped off and another which appears to have remained intact. These results suggest that Hercules was once more extended than it currently is today. Finally, using our photometric catalogue we measure the line-of-sight velocity dispersion as a function of projected distance from the centre of Hercules. Our measurements indicate that the central region of Hercules exhibits higher values compared to those measured further out.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Elongated Structure of the Hercules dSph from Deep LBT Imaging . Abstract : We report deep scanning data for the nearby dwarf spheroidal galaxy , Hercules ( dSph ) , obtained with the Large Binocular Telescope ( LBT ) .The newest observations are using to study the composition and stellar groups in this system . We see that the surface brightness profile is well described by an exponential function over most of its extent but shows proof for a break at about 30 arcsec radius .This structure could be involved with tidal disruption or stripping due to interactions between Hercules and other stars . Using colour - magnitude diagrams we find that there exists two different components within Hercules ; one which has been stripped off and another which appears to have remained intact .These data suggest that Hercules was once more extended than it currently is presently . Finally , using our photometric catalogue we measure the line - of - view velocity dispersion as a function of projected distance from the centre of Hercules .Our measurements indicate that the central region of Hercules exhibits higher values compared to those observed further out .",
        "rewrite_text": "We present a comprehensive analysis of deep imaging data for the nearby dwarf spheroidal galaxy, Hercules (dSph), acquired using the Large Binocular Telescope (LBT). This study aims to enhance our understanding of the galaxy's structural composition and the distribution of its stellar populations. Our findings reveal that the surface brightness profile of Hercules can be accurately modeled by an exponential function across the majority of its extent; however, we observe a notable deviation at approximately 30 arcseconds from the center, indicating a potential structural break. This anomaly may be indicative of tidal interactions or stripping events resulting from gravitational encounters with neighboring stellar systems. \n\nThrough the application of color-magnitude diagrams, we identify two distinct stellar components within Hercules: one that appears to have been stripped away and another that remains largely intact. These observations imply that Hercules may have originally possessed a more extensive structure than what is currently observed. Furthermore, utilizing our photometric catalog, we assess the line-of-sight velocity dispersion as a function of projected distance from the galaxy's center. Our results demonstrate that the central region of Hercules exhibits significantly higher velocity dispersion values compared to those measured in the outer regions. This gradient in velocity dispersion may provide insights into the dynamical state of the galaxy and its evolutionary history. Overall, our study contributes to the growing body of knowledge regarding the formation and evolution of dwarf spheroidal galaxies, particularly in the context of their interactions with the surrounding cosmic environment.",
        "ori-fast-z-score": -0.7875615306482168,
        "water-fast-z-score": 4.387842813611494,
        "rewrite-fast-z-score": 1.0776318121606494
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The First Stellar Cluster .\nAbstract:\nThe first stars in the universe were born out of primordial gas clouds, which collapsed under their own gravity to form dense cores that ignited nuclear fusion and became hot white dwarfs.  The most massive of these early stars are now known as Population III (PopIII) stars.   In this work we present results for PopIII star formation using cosmological hydrodynamic simulations with radiative transfer calculations performed on an adaptive mesh refinement grid.  We find that PopIII stars can be formed by direct collapse of metal-free gas clouds at redshifts z > 20.  These stars have masses M * = 100 - 1000M⊙ and lifetimes t < 10Myr.  They evolve into black holes or pair-instability supernovae after consuming all available fuel within their convective envelopes.  Our results show that PopIII stars may contribute significantly to reionization of the intergalactic medium around redshift z ~ 15.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The First Stellar Cluster . Abstract : The first stars in the universe were born out of primordial plasma storms , which collapsed under their own gravity to form dense cores that sparked nuclear fusion and became hot white dwarfs .The most large of these older stars are now referred as Population III ( PopIII ) stars . In this research we present results for PopIII star formation combining cosmological hydrodynamic simulations with radiative transfer calculations performed on an adaptive mesh refinement grid .We see that PopIII stars can be formed by direct collapse of metal - loose gas clouds at redshifts z > 20 . These stars have masses M * = 100 - [UNK] and lifetimes t < 10Myr .They evolve into black holes or pair - instability supernovae after consuming all available fuel within their convective envelopes . Our results show that PopIII stars likely contribute greatly to reionization of the intergalactic medium around redshift z ~ 15 .",
        "rewrite_text": "**Title: The First Stellar Cluster**\n\n**Abstract:** The formation of the universe's first stars emerged from primordial plasma storms, which collapsed under their own gravitational forces, leading to the creation of dense cores that ignited nuclear fusion, ultimately resulting in the formation of hot white dwarfs. The largest of these ancient stars are classified as Population III (PopIII) stars. In this study, we present findings on PopIII star formation by integrating cosmological hydrodynamic simulations with radiative transfer calculations conducted on an adaptive mesh refinement grid. Our simulations indicate that PopIII stars can form through the direct collapse of metal-poor gas clouds at redshifts greater than 20. These stars exhibit masses ranging from M* = 100 to an undetermined upper limit, with lifetimes of less than 10 million years. Following the depletion of their available fuel within their convective envelopes, these stars evolve into either black holes or undergo pair-instability supernovae. Our results suggest that PopIII stars played a significant role in the reionization of the intergalactic medium, particularly around redshift z ~ 15. This research enhances our understanding of the early universe's stellar population and its impact on cosmic evolution, highlighting the importance of these primordial stars in shaping the structure and composition of the universe as we know it today.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 4.213504858001922,
        "rewrite-fast-z-score": 1.970208219987808
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SBF: multi-wavelength data and models .\nAbstract:\nThe Spitzer Bright Field (SBF) is an all-sky survey at 3.6, 4.5, 5.8 & 8 microns carried out by the Infrared Array Camera on board the Spitzer Space Telescope.  The SBF was designed to provide deep infrared photometry for extragalactic studies in order to complement existing optical surveys such as the Sloan Digital Sky Survey.   This dataset contains images taken with IRAC channel 1 (3.6 micron), channel 2 (4.5 micron), channel 3 (5.8 micron) and channel 4 (8 micron). Each image has been processed using the MOPEX software package developed by the Spitzer Science Center. These images are available through the NASA/IPAC Extragalactic Database (NED).  For more information about this project please see http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SBF : multi - wavelength information and models . Abstract : The Spitzer Bright Field ( SBF ) is an all - skies study at 3 . 6 , 4 . 5 , 5 . 8 & 8 microns taken out by the Infrared Array Camera on board the Spitzer Space Telescope .The SBF was built to provide deep infrared photometry for extragalactic studies in order to complement existing imaging observations such as the Sloan Digital Sky Survey . This dataset contains images took with IRAC channel 1 ( 3 . 6 micron ) , channel 2 ( 4 . 5 micron ) , channel 3 ( 5 . 8 micron ) and channel 4 ( 8 micron ) .Each image has been processed utilizing the MOPEX software suite created by the Spitzer Science Center . These photographs are available through the NASA / IPAC Extragalactic Database ( NED ) .For more information about this project please view http : / / irsa . ipac . caltech . edu / data / SPITZER / docs / sbf / sbf - intro . html",
        "rewrite_text": "Title: SBF: Multi-Wavelength Information and Models\n\nAbstract: The Spitzer Bright Field (SBF) represents a comprehensive all-sky survey conducted at wavelengths of 3.6, 4.5, 5.8, and 8 microns, utilizing the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. This initiative was designed to deliver deep infrared photometry that enhances extragalactic research, serving as a valuable complement to existing optical imaging datasets, such as those provided by the Sloan Digital Sky Survey. The SBF dataset comprises images captured across all four IRAC channels: channel 1 at 3.6 microns, channel 2 at 4.5 microns, channel 3 at 5.8 microns, and channel 4 at 8 microns. Each image has undergone meticulous processing using the MOPEX software suite, developed by the Spitzer Science Center, ensuring high-quality data suitable for scientific analysis. The processed images are accessible through the NASA/IPAC Extragalactic Database (NED), providing researchers with a rich resource for studying various extragalactic phenomena. This dataset not only facilitates the exploration of distant galaxies and cosmic structures but also aids in the development of models that can predict the behavior and characteristics of celestial objects across different wavelengths. For further details regarding the SBF project and its implications for extragalactic studies, interested parties are encouraged to visit the dedicated project page at http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html. This comprehensive dataset is poised to significantly advance our understanding of the universe by offering multi-wavelength insights into the nature of galaxies and other astronomical entities.",
        "ori-fast-z-score": -0.30151134457776363,
        "water-fast-z-score": 4.024922359499621,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetism in the spiral galaxy NGC 6946: magnetic arms, depolarization rings, dynamo modes and helical fields .\nAbstract:\nWe present new observations at 1.4 GHz with the VLA of polarized emission from the nearby (7 Mpc) grand-design spiral galaxy NGC 6946. The data reveal several interesting features that are not seen in previous radio continuum studies of this galaxy. We find that:  -The total intensity distribution is dominated by two bright nuclear components separated by about 2 kpc along an axis perpendicular to the main galactic disk.  -There is no evidence for large-scale ordered fields on kiloparsec scales as previously reported.   -The polarization vectors show a clear pattern of alternating directions across the central region of the galaxy which we interpret as a signature of a global magnetic field reversal between the two nuclei.  -The rotation measure map shows a ring-like structure around each nucleus where the RM changes sign indicating a change in direction of the line-of-sight component of the magnetic field. This feature may be related to the so-called depolarization rings observed in other galaxies but it could also result from beam smearing effects or from intrinsic Faraday dispersion within the source itself.  -The polarized intensity distribution reveals a number of extended structures including a prominent southern arm extending over more than 10 kpc towards the south-east.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetism in the spiral galaxy NGC 6946 : magnetic arms , depolarization belts , dynamo modes and helical fields . Abstract : We report new images at 1 . 4 GHz with the VLA of polarized emission from the nearby ( 7 Mpc ) great - design spiral galaxy NGC 6946 .The data reveal several interesting features that are not seen in earlier radio continuum experiments of this galaxy . We see that : - The total frequency distribution is dominated by two faint nuclear elements divided by about 2 kpc along an axis perpendicular to the main galactic disk .- There is no evidence for large - scale ordered fields on kiloparsec scales as previously reported . - The polarization coefficients show a clear sequence of alternating directions across the central region of the galaxy which we perceive as a signature of a global magnetic force reversal between the two nuclei .- The rotation measure map displays a ring - like structure around each core where the RM changes sign indicating a change in direction of the line - of - view component of the magnetic force . This characteristic could be connected to the so - called depolarization loops observed in other stars but it could also occur from beam smearing effects or from intrinsic Faraday dispersion within the source itself .- The polarized intensity distribution reveals a number of extended features including a large southern arm reaching over more than 10 kpc towards the south - east .",
        "rewrite_text": "We present new 1.4 GHz polarized emission images obtained with the Very Large Array (VLA) of the nearby spiral galaxy NGC 6946, located approximately 7 Mpc away. Our findings unveil several intriguing characteristics that were not captured in previous radio continuum studies of this galaxy. Notably, the total frequency distribution is primarily influenced by two faint nuclear components, which are separated by roughly 2 kpc along an axis that is perpendicular to the main galactic disk. Contrary to earlier reports, we find no evidence of large-scale ordered magnetic fields on kiloparsec scales. Instead, the polarization coefficients exhibit a distinct pattern of alternating directions within the central region, suggesting a global magnetic field reversal between the two nuclei. Additionally, our rotation measure (RM) map reveals a ring-like structure surrounding each core, where the RM sign changes, indicating a shift in the direction of the line-of-sight component of the magnetic field. This phenomenon may be associated with the depolarization loops observed in other stellar systems, although it could also arise from beam smearing effects or intrinsic Faraday dispersion within the galaxy itself. Furthermore, the distribution of polarized intensity highlights several extended features, including a significant southern arm that extends over 10 kpc towards the southeast. These observations contribute to our understanding of the magnetic properties and dynamics of NGC 6946, providing insights into the complex interplay between magnetic fields and galactic structures.",
        "ori-fast-z-score": 0.1873171623163388,
        "water-fast-z-score": 5.874780291913977,
        "rewrite-fast-z-score": 1.671258043593467
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Improved Quantum Hard-Sphere Ground-State Equations of State .\nAbstract:\nWe present an improved quantum hard-sphere ground-state equation-of-state (EOS) for the description of dense matter in astrophysics and nuclear physics, which is based on the exact solution to the Schrödinger equation with a repulsive delta-function potential. The EOS has been derived by solving numerically the corresponding integral equations using the method of successive iterations. We have also obtained analytical expressions for the pressure and energy density as functions of the number density at zero temperature. Our results are compared with those calculated previously within various approximations such as the virial expansion up to second order, the Carnahan-Starling approximation, and the Percus-Yevick approximation. It turns out that our new EOS agrees well with these previous calculations over wide ranges of densities and temperatures. In particular, it reproduces very accurately the low-density limit where the ideal gas law holds exactly. \n \n Keywords: Equation of state",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Improved Quantum Hard - Sphere Ground - State Equations of State . Abstract : We create an updated quantum hard - sphere ground - state equation - of - state ( EOS ) for the description of dense materials in astrophysics and nuclear science , which is based on the exact solution to the Schrödinger equation with a repulsive delta - function potential .The EOS has been constructed by solving numerically the equivalent integral equations using the method of successive iterations . We have also achieved analytical expressions for the pressure and energy density as functions of the number density at zero temperature .Our results are compared with those estimated previously within various approximations such as the virial expansion up to second order , the Carnahan - Starling approximation , and the Percus - Yevick approximation . It turns out that our new EOS follows well with these previous analyses over broad ranges of densities and altitudes .In particular , it reproduces very correctly the small - density maximum where the ideal gas law stands exactly . Keywords : Equation of state",
        "rewrite_text": "Title: Enhanced Quantum Hard-Sphere Ground-State Equations of State\n\nAbstract: In this study, we present a refined quantum hard-sphere ground-state equation of state (EOS) tailored for the analysis of dense materials relevant to astrophysics and nuclear science. Our approach is grounded in the exact solution of the Schrödinger equation, incorporating a repulsive delta-function potential. To derive the EOS, we employed numerical methods to solve the corresponding integral equations through successive iterations. Additionally, we have derived analytical expressions for both pressure and energy density as functions of number density at absolute zero temperature. Our findings are juxtaposed with previous estimates obtained through various approximations, including the second-order virial expansion, the Carnahan-Starling approximation, and the Percus-Yevick approximation. Notably, our new EOS aligns closely with these earlier analyses across a wide spectrum of densities and conditions. Specifically, it accurately captures the small-density maximum where the ideal gas law is precisely valid. This work not only enhances the understanding of the thermodynamic properties of dense systems but also provides a robust framework for future investigations in related fields. \n\nKeywords: Equation of state, quantum mechanics, hard spheres, astrophysics, nuclear science.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 4.444462481925879,
        "rewrite-fast-z-score": 3.5909242322980397
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On-Line Condition Monitoring using Computational Intelligence .\nAbstract:\nThe aim of this book is to provide an overview on the state-of-the-art in on-line condition monitoring and fault diagnosis for industrial processes, with special emphasis on computational intelligence techniques such as neural networks (NNs), fuzzy logic systems (FLS) or evolutionary algorithms (EAs). The book covers both theoretical aspects and practical applications of these methods. It also discusses some recent advances in intelligent sensor technologies that are essential for successful implementation of on-line condition monitoring schemes. This book will be useful not only for researchers but also for engineers who want to apply computational intelligence techniques into their own research work. Contents include:  Chapter 1: Introduction to On-line Condition Monitoring.  Chapter 2: Intelligent Sensors for On-line Condition Monitoring.  Chapters 3-7: Neural Networks for Fault Diagnosis.  Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.   Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.   ...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On - Line Condition Monitoring using Computational Intelligence . Abstract : The goal of this book is to provide an overview on the state - of - the - art in on - line condition monitoring and failure detection for industrial systems , with special emphasis on mathematical intelligence techniques such as neural systems ( NNs ) , fuzzy logic systems ( FLS ) or genetic algorithms ( EAs ) .The text encompasses both theoretical components and useful use of these procedures . It additionally outlines some latest advances in intelligent detector technologies that are essential for successful implementation of on - line condition monitoring schemes .This book will be valuable not only for researchers but also for designers who desire to apply computational intelligence techniques into their own research effort . Contents include : Chapter 1 : Introduction to On - line Condition Monitoring .Chapter 2 : Intelligent Sensors for On - line Condition Monitoring . Chapters 3 - 7 : Neural Networks for Fault Diagnosis .Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis....",
        "rewrite_text": "**Title: On-Line Condition Monitoring using Computational Intelligence**\n\n**Abstract:** This book aims to present a comprehensive overview of the latest advancements in on-line condition monitoring and failure detection within industrial systems, focusing particularly on the application of mathematical intelligence techniques. These techniques include neural networks (NNs), fuzzy logic systems (FLS), and evolutionary algorithms (EAs). The text not only delves into the theoretical foundations of these methodologies but also emphasizes their practical applications, providing insights into how they can be effectively utilized in real-world scenarios. Furthermore, the book highlights recent developments in intelligent detection technologies that are crucial for the successful implementation of on-line condition monitoring systems. This resource is designed to be beneficial for both researchers and practitioners who are interested in integrating computational intelligence techniques into their work. The structure of the book is organized into several chapters, beginning with an introduction to on-line condition monitoring, followed by a detailed examination of intelligent sensors tailored for this purpose. Subsequent chapters focus on the application of neural networks for fault diagnosis, with a dedicated exploration spanning five chapters. This is followed by an in-depth discussion of fuzzy logic systems and their role in fault diagnosis, covered over three chapters. The book concludes with a thorough analysis of evolutionary algorithms and their application in diagnosing faults, also presented across three chapters. Overall, this work serves as a vital resource for those looking to enhance their understanding and application of computational intelligence in the field of condition monitoring.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 2.151657414559676
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SW Sextantis stars: the dominant population of CVs with orbital periods between 3-4 hours .\nAbstract:\nWe present an analysis of all known short-period cataclysmic variables (CVs) in which we find that most systems have orbital periods longer than 3 hrs and are dominated by SW Sex stars, while those with shorter periods tend to be AM Her binaries. We show that this dichotomy is consistent with theoretical predictions for the evolution of CVs driven by angular momentum loss via gravitational radiation. The observed distribution of orbital periods can also be explained if there exists a minimum period below which no CVs exist due to magnetic braking. This result has important implications on our understanding of how CVs evolve towards shorter orbital periods. Cataclysmic Variables (CVs), interacting binary star systems consisting of a white dwarf primary accreting matter from its low-mass companion through Roche lobe overflow, are among the best studied classes of close binary stars. They provide unique opportunities to study many aspects of astrophysics such as stellar structure and evolution, mass transfer processes, nuclear burning at high temperatures, and relativistic effects near compact objects. In particular, they offer insights into the formation mechanisms of both single and double degenerate white dwarfs, the progenitors of Type Ia supernovae.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SW Sextantis stars : the dominant population of CVs with orbital periods between 3 - 4 hours . Abstract : We present an assessment of all known short - period cataclysmic variables ( CVs ) in which we find that most systems have orbital periods longer than 3 hrs and are dominated by SW Sex stars , while those with shorter cycles seem to be AM Her binaries .We see that this dichotomy is compatible with theoretical expectations for the evolution of CVs caused by angular velocity loss via gravitational radiation . The observed distribution of orbital periods can also be understood if there exists a minimum period below which no CVs occur due to magnetic braking .This result has crucial consequences on our knowing of how CVs develop towards shorter orbital periods . Cataclysmic Variables ( CVs ) , interacting binary star systems composed of a white dwarf secondary accreting matter from its high - weight companion through Roche lobe overflow , are among the best researched groups of close binary galaxies .They offer special opportunities to study many aspects of astrophysics such as stellar formation and evolution , mass transfer mechanisms , nuclear burning at high temperatures , and relativistic effects near compact objects . In particular , they give insights into the formation patterns of both single and double degenerate white dwarfs , the progenitors of Type Ia supernovae .",
        "rewrite_text": "Title: SW Sextantis Stars: The Predominant Population of Cataclysmic Variables with Orbital Periods Ranging from 3 to 4 Hours\n\nAbstract: In this study, we conduct a comprehensive evaluation of all known short-period cataclysmic variables (CVs), revealing that the majority of these systems exhibit orbital periods exceeding 3 hours, predominantly characterized by SW Sextantis (SW Sex) stars. In contrast, systems with shorter orbital periods appear to be primarily AM Herculis (AM Her) binaries. This observed distinction aligns with theoretical predictions regarding the evolutionary pathways of CVs, particularly those influenced by angular momentum loss through gravitational radiation. Furthermore, our findings suggest the existence of a minimum orbital period threshold, below which CVs are unlikely to form, a phenomenon attributed to magnetic braking effects. This insight has significant implications for our understanding of the evolutionary processes that drive CVs toward shorter orbital periods.\n\nCataclysmic variables, which are interacting binary star systems consisting of a white dwarf that accretes material from a more massive companion via Roche lobe overflow, represent one of the most extensively studied categories of close binary systems. They provide unique opportunities to investigate various astrophysical phenomena, including stellar formation and evolution, mass transfer dynamics, high-temperature nuclear burning, and relativistic effects in the vicinity of compact objects. Notably, CVs offer valuable perspectives on the formation mechanisms of both single and double degenerate white dwarfs, which are the progenitors of Type Ia supernovae. Through our analysis, we aim to enhance the understanding of the evolutionary trajectories of CVs and their role in the broader context of stellar evolution and cosmic events.",
        "ori-fast-z-score": 0.4833682445228318,
        "water-fast-z-score": 5.388602512436507,
        "rewrite-fast-z-score": 1.4368424162141993
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Origin of the 60K plateau in YBa_2Cu_3O_6+x .\nAbstract:\nWe have studied the origin of the 60 K plateau in YBa2Cu3Ox by measuring the specific heat and magnetic susceptibility as functions of temperature, field strength, oxygen content x, and doping level p. The results show that the 60 K anomaly is due to an antiferromagnetic transition at TAF = 56 K for x= 0.6 but disappears with increasing x or decreasing p. We also find that the low-temperature upturns observed in both C/T and χ(T) are caused by impurity phases which appear when x > 0.65.  These results suggest that the 60 K anomality may be related to the appearance of these impurities. In addition we found that the high-field magnetization shows a sharp increase below 50 K indicating that there exists another phase transition near this temperature. This new phase has been identified as a charge density wave state (CDW). Finally, our data indicate that the CDW state appears only if the sample contains some amount of impurities.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Origin of the 60K plateau in YBa _ 2Cu _ 3O _ 6 + x . Abstract : We have researched the origin of the 60 K plateau in YBa2Cu3Ox by measuring the specific heat and magnetic susceptibility as functions of temperature , field intensity , hydrogen content x , and doping grade p . The results show that the 60 K anomaly is due to an antiferromagnetic transition at TAF = 56 K for x = 0 . 6 but disappears with rising x or decreasing p . We additionally find that the high - temperature upturns observed in both C / T and χ ( T ) are created by impurity phases which appear when x > 0 . 65 .These data suggest that the 60 K anomality may be connected to the appearance of these impurities . In addition we concluded that the high - field magnetization shows a sharp increase below 50 K indicating that there exists another phase shift near this heat .This new phase has been described as a charge density wave state ( CDW ) . Finally , our statistics indicate that the CDW state emerges only if the sample comprises some number of impurities .",
        "rewrite_text": "Title: Origin of the 60K Plateau in YBa2Cu3O6+x\n\nAbstract: In this study, we investigate the underlying mechanisms responsible for the 60 K plateau observed in YBa2Cu3O6+x by conducting a series of measurements on specific heat and magnetic susceptibility across varying temperatures, magnetic field strengths, hydrogen content (x), and doping levels (p). Our findings reveal that the 60 K anomaly is linked to an antiferromagnetic transition occurring at TAF = 56 K when x is set at 0.6. Notably, this anomaly diminishes with increasing values of x or decreasing levels of p. Furthermore, we observe that the high-temperature upturns in both the specific heat divided by temperature (C/T) and the magnetic susceptibility (χ(T)) can be attributed to the presence of impurity phases that emerge when x exceeds 0.65. This correlation suggests that the 60 K anomaly may be influenced by these impurity phases. Additionally, our results indicate a pronounced increase in high-field magnetization below 50 K, which points to the existence of another phase transition occurring at this temperature. We characterize this new phase as a charge density wave (CDW) state. Importantly, our statistical analysis indicates that the emergence of the CDW state is contingent upon the presence of impurities within the sample. These findings contribute to a deeper understanding of the complex interactions in YBa2Cu3O6+x and highlight the significant role of impurities in influencing the material's magnetic and thermal properties.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 4.2723919920032305,
        "rewrite-fast-z-score": 0.8164965809277261
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Testing anthropic predictions for Lambda and the CMB temperature .\nAbstract:\nWe test whether the observed value of lambda is consistent with the prediction that it should be equal to one third of the square root of the number density of galaxies in the universe today, as suggested by Tegmark et al. (2006) . We find no evidence against this hypothesis using data on galaxy luminosity functions at redshifts z = 0.1, 1.0 and 3.5 taken from the Sloan Digital Sky Survey (SDSS). The predicted values are obtained assuming that the dark energy equation-of-state parameter w is constant over time. This assumption may not hold if there exists an interaction between dark matter and dark energy. However, we show that even allowing w to vary significantly does not affect our results. \n \n In addition, we use the WMAP 5-year cosmological parameters to calculate the expected temperature anisotropy power spectrum of the cosmic microwave background radiation (CMB) given the current best-fit model. We compare these theoretical predictions with measurements made by the Wilkinson Microwave Anisotropy Probe (WMAP), finding good agreement across all multipole moments up to lmax = 1000.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Testing anthropic predictions for Lambda and the CMB heat . Abstract : We test whether the seen value of lambda is compatible with the observation that it should be equal to one third of the square root of the number density of galaxies in the universe today , as suggested by Tegmark et al .( 2006 ) . We see no evidence against this hypothesis utilizing information on star luminosity functions at redshifts z = 0 . 1 , 1 . 0 and 3 . 5 taken from the Sloan Digital Sky Survey ( SDSS ) .The predicted values are derived assuming that the dark energy equation - of - state variable w is constant over time . This assumption would not hold if there exists an interaction between dark matter and dark energy .However , we prove that even allowing w to vary significantly does not alter our findings . In addition , we utilize the WMAP 5 - month cosmological factors to estimate the expected cooling anisotropy energy spectrum of the cosmic microwave background radiation ( CMB ) given the present better - fitting model .We match these theoretical estimates with observations made by the Wilkinson Microwave Anisotropy Probe ( WMAP ) , finding high agreement across all multipole minutes up to lmax = 1000 .",
        "rewrite_text": "**Title:** Testing Anthropic Predictions for Lambda and the CMB Heat\n\n**Abstract:** In this study, we investigate the compatibility of the observed value of lambda (Λ) with the anthropic prediction that it should equal one-third of the square root of the current galaxy number density in the universe, as proposed by Tegmark et al. (2006). Our analysis utilizes data on star luminosity functions at redshifts z = 0.1, 1.0, and 3.5, sourced from the Sloan Digital Sky Survey (SDSS), and we find no evidence contradicting this hypothesis. The predicted values are derived under the assumption that the dark energy equation of state parameter, w, remains constant over time. However, this assumption may not hold true if there is an interaction between dark matter and dark energy. Despite this potential variability, our results demonstrate that allowing w to change significantly does not impact our conclusions. Furthermore, we employ the five-month data from the Wilkinson Microwave Anisotropy Probe (WMAP) to estimate the expected cooling anisotropy energy spectrum of the cosmic microwave background (CMB) radiation based on our improved fitting model. Our theoretical predictions are then compared with the observational data from WMAP, revealing a high degree of agreement across all multipole moments up to lmax = 1000. This consistency reinforces the validity of the anthropic predictions regarding lambda and provides further insight into the interplay between dark energy and the structure of the universe. Overall, our findings contribute to the ongoing discourse on the nature of dark energy and its implications for cosmological models.",
        "ori-fast-z-score": -0.21081851067789195,
        "water-fast-z-score": 5.838403593598094,
        "rewrite-fast-z-score": 0.9805806756909202
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Stueckelberg Extension and Milli Weak and Milli Charge Dark Matter .\nAbstract:\nWe propose that the dark matter in our universe is composed of milli-charged particles, which are neutral under electromagnetism but carry an electric charge on the order of 10^(-6)e (electrons). We show how this scenario can be realized within the context of the Standard Model by introducing a new gauge boson with mass mX ~ 1TeV/c2 into the theory through the Stueckelberg extension to the Standard Model. The introduction of such a massive vector particle leads to modifications to the usual Feynman rules for charged fermions interacting via photons or gluons. In particular, we find that the cross section for scattering between two milli-charged particles mediated by a photon is suppressed compared to the case where there were no additional massive vector boson present. This suppression results in a reduction of the number density of milli-charged dark matter particles at late times as they annihilate more slowly than their un-massive counterparts.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Stueckelberg Extension and Milli Weak and Milli Charge Dark Matter . Abstract : We suggest that the dark matter in our universe is composed of milli - charged particles , which are neutral under electromagnetism but hold an electric current on the order of 10 ^ ( - 6 ) e ( atoms ) .We see how this situation can be realized within the context of the Standard Model by bringing a new gauge boson with mass mX ~ 1TeV / c2 into the model through the Stueckelberg extension to the Standard Model . The advent of such a huge vector beam leads to modifications to the usual Feynman restrictions for charged fermions interacting via photons or gluons .In particular , we find that the cross section for propagation between two milli - charged particles mediated by a photon is suppressed compared to the case where there were no additional massive vector boson present . This suppression results in a reduction of the number density of milli - charged dark matter molecules at late times as they annihilate more slowly than their un - massive counterparts .",
        "rewrite_text": "Title: The Stueckelberg Extension and Milli Weak and Milli Charge Dark Matter\n\nAbstract: In this study, we propose that the dark matter present in our universe consists of milli-charged particles, which, while neutral under conventional electromagnetic interactions, carry an electric charge on the order of 10^(-6)e (where e represents the charge of an electron). We explore how this phenomenon can be integrated into the framework of the Standard Model by introducing a new gauge boson with a mass of approximately 1 TeV/c² through the Stueckelberg extension. The introduction of such a massive vector boson significantly alters the standard Feynman rules governing the interactions of charged fermions via photons or gluons. Notably, we demonstrate that the cross-section for the interaction between two milli-charged particles mediated by a photon is notably suppressed in the presence of this additional massive vector boson. This suppression leads to a decrease in the number density of milli-charged dark matter particles over time, as their annihilation processes occur at a slower rate compared to their massless counterparts. Our findings suggest that the dynamics of milli-charged dark matter could provide valuable insights into the nature of dark matter and its interactions, potentially offering a new avenue for understanding the composition of the universe. This research not only enhances our comprehension of dark matter but also emphasizes the significance of incorporating new gauge bosons into existing theoretical frameworks to explain observed phenomena in cosmology and particle physics.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 4.242640687119286,
        "rewrite-fast-z-score": 0.8700628401410971
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relaxation dynamics in fluids of platelike colloidal particles .\nAbstract:\nWe study the relaxation dynamics of suspensions of platelike colloids by means of Brownian Dynamics simulations and experiments on polystyrene platelets suspended in water. We find that the decay of the intermediate scattering function is nonexponential, with an initial fast decay followed by a slower one. The slowest mode has been identified as the collective diffusion of the suspension. By comparing our results to those obtained for spherical colloids we show how the shape anisotropy affects the relaxation process. In particular, we observe that the presence of flat surfaces enhances the effect of hydrodynamic interactions between neighboring particles leading to faster relaxation times than expected based on simple scaling arguments. Finally, we discuss possible applications of these systems as model soft matter systems for studying glass transitions. Colloidal dispersions are widely used as model systems for understanding phenomena such as phase separation or gel formation  1  . However, most studies have focused on spherical particles  2  , while only few works have considered non-spherical shapes  3  .\nIn this work we investigate the relaxation dynamics of suspentions of platelike colloidals using both computer simulation techniques and experimental measurements. Platelike colloids can be realized experimentally by suspending polystyrene-platelet-like particles  4  into water (see Fig.  1 ). These systems exhibit interesting properties which make them suitable candidates for investigating fundamental physical processes like glass transition  5  . For example, they display enhanced viscosity  6  compared to their spherical counterparts  7, 8  due to the increased friction arising from the interaction of the particle s flat surface with its neighbors  9  . Moreover, it was recently shown  10  that platelike colloids undergo a structural arrest at higher volume fractions than spherical ones  11  . This behavior is reminiscent of what observed in hard-sphere glasses  12  but also in other types of amorphous solids  13  where the local structure displays icosahedral order  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relaxation mechanics in fluids of platelike colloidal particles . Abstract : We research the relaxation behavior of suspensions of platelike colloids by means of Brownian Dynamics simulations and experiments on polystyrene platelets suspended in water .We see that the decay of the intermediate scattering function is nonexponential , with an initial quick decay preceded by a slower one . The slowest mechanism has been described as the collective diffusion of the suspension .By matching our findings to those achieved for spherical colloids we determine how the form anisotropy affects the relaxation process . In particular , we find that the presence of smooth surfaces enhances the impact of hydrodynamic interactions between neighboring objects resulting to faster relaxation times than expected based on simple scaling statements .Finally , we explain possible users of these systems as model soft matter structures for studying glass transitions . Colloidal dispersions are widely using as model models for explaining events such as phase splitting or gel formation 1 .However , most studies have concentrated on spherical objects 2 , while only few papers have explored non - cylindrical forms 3 . In this research we investigate the relaxation behavior of suspentions of platelike colloidals using both computer simulation tools and theoretical measurements .Platelike colloids can be realized experimentally by suspending polystyrene - platelet - like ions 4 into water ( see Fig . 1 ) .These systems exhibit unusual characteristics which make them useful candidates for investigating fundamental physical processes like glass process 5 . For instance , they show enhanced viscosity 6 compared to their spherical counterparts 7 , 8 due to the increased friction arose from the interaction of the particle s flat surface with its relatives 9 .Moreover , it was recently shown 10 that platelike colloids undergo a structural arrest at higher volume fractions than cylindrical ones 11 . This phenomenon is characteristic of what seen in hard - sphere glasses 12 but also in other types of amorphous solids 13 where the local form shows icosahedral order 14 .",
        "rewrite_text": "**Title:** Relaxation Mechanics in Fluids of Platelike Colloidal Particles\n\n**Abstract:** This study investigates the relaxation dynamics of suspensions composed of platelike colloidal particles through a combination of Brownian Dynamics simulations and experimental observations involving polystyrene platelets dispersed in water. Our findings reveal that the decay of the intermediate scattering function is characterized by a nonexponential behavior, featuring an initial rapid decay phase followed by a more gradual decline. The slower relaxation mechanism is attributed to the collective diffusion of the suspension. By comparing our results with those obtained for spherical colloids, we elucidate the influence of shape anisotropy on the relaxation processes. Notably, we observe that the presence of smooth surfaces significantly enhances hydrodynamic interactions among neighboring particles, leading to relaxation times that are shorter than what would be predicted by basic scaling arguments. \n\nFurthermore, we discuss the potential applications of these platelike colloidal systems as model soft matter structures for the exploration of glass transitions. Colloidal dispersions have been extensively utilized as model systems to elucidate phenomena such as phase separation and gelation. However, the majority of existing research has predominantly focused on spherical particles, with only a limited number of studies addressing non-cylindrical geometries. In our work, we delve into the relaxation characteristics of platelike colloids, which can be experimentally realized by suspending polystyrene platelets in water. These systems exhibit unique properties that render them valuable for probing fundamental physical processes, including glass formation. For instance, platelike colloids demonstrate increased viscosity compared to their spherical counterparts, a consequence of the heightened friction arising from the interactions between the flat surfaces of the particles. Additionally, recent findings indicate that platelike colloids experience structural arrest at higher volume fractions than cylindrical particles, a behavior reminiscent of that observed in hard-sphere glasses and other amorphous solids characterized by local icosahedral order.",
        "ori-fast-z-score": -1.3834403799109711,
        "water-fast-z-score": 6.98800816145174,
        "rewrite-fast-z-score": 0.8333333333333334
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetization and specific heat of TbFe3(BO3)4: Experiment and crystal field calculations .\nAbstract:\nThe magnetization, susceptibility, and specific heat measurements were performed on the single crystals of TbFe3( BO3 )4 . The magnetic properties are analyzed in terms of the crystal-field splitting scheme for Tb3+ ions. It is found that the ground state doublet has an Ising-like anisotropy along c-axis with gz = 8.0 ± 0.1 , which leads to the large spontaneous polarization ( Ps ~ 1μC/cm2 ). The calculated results reproduce well the experimental data except for the low-temperature part of the specific-heat curve below 2 K. This discrepancy may be attributed to the presence of impurities or defects in our samples. \n \n Keywords: Magnetism; Crystal field theory; Specific heat measurement; Susceptibility measurement; Single-crystal growth; Anisotropic magnetoresistance effects; Polarized neutron scattering \n \n \n \n INTRODUCTION : \nTbFe 3 (BO 3 ) 4 belongs to the family of rare-earth iron borates RFe 3 (BO 3 ) (R = Y, Yb, Lu). These compounds have attracted much attention because they exhibit various interesting physical phenomena such as ferroelectricity  1  , multiferroicity  2  , colossal magnetoresistance  3  , and quantum critical behavior  4  .\nIn particular, TbFe 3 (BO 3 )\n4 exhibits a giant spontaneous polarization P s ~ 1 μ C / cm 2 at room temperature  5  due to its unique crystal structure  6  . In this compound, Fe atoms form a three-dimensional network of corner-sharing tetrahedra by sharing their apical oxygen atoms  7   . On the other hand, Tb atoms occupy two different sites, i.e., one site surrounded by eight O atoms forming a square antiprismatic coordination polyhedron  8  and another site surrounded by six O atoms forming a trigonal prismatic coordination polyhedron  9  . As shown in Figs. 1 (a) and (b), these two types of polyhedra share common faces perpendicularly to the c -axis  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetization and particular heat of TbFe3 ( BO3 ) 4 : Experiment and crystal field calculations . Abstract : The magnetization , susceptibility , and particular heat measurements were performed on the single crystals of TbFe3 ( BO3 ) 4 .The magnetic properties are examined in terms of the crystal - field separation scheme for Tb3 + ions . It is found that the ground state doublet has an Ising - like anisotropy along c - axis with gz = 8 . 0 ± 0 . 1 , which results to the huge spontaneous polarization ( Ps ~ 1μC / cm2 ) .The measured data reproduce well the laboratory information except for the small - temperature half of the specific - temperature curve below 2 K . This discrepancy may be due to the presence of impurities or defects in our specimens . Keywords : Magnetism ; Crystal field model ; Specific temperature measurement ; Susceptibility measurement ; Single - crystal growth ; Anisotropic magnetoresistance effects ; Polarized neutron scattering INTRODUCTION : TbFe 3 ( BO 3 ) 4 belongs to the group of rare - earth iron borates RFe 3 ( BO 3 ) ( R = Y , Yb , Lu ) .These compounds have garnered great popularity because they show numerous interesting physical phenomena such as ferroelectricity 1 , multiferroicity 2 , colossal magnetoresistance 3 , and quantum important interaction 4 . In particular , TbFe 3 ( BO 3 ) 4 displays a huge spontaneous polarization P s ~ 1 μ C / cm 2 at room temperature 5 due to its unique crystal composition 6 .In this compound , Fe molecules form a three - dimensional network of spot - sharing tetrahedra by sharing their apical oxygen atoms 7 . On the other hand , Tb molecules occupy two different places , i . e . , one site surrounded by eight O atoms forming a square antiprismatic coordination polyhedron 8 and another site surrounded by six O atoms forming a trigonal prismatic coordination polyhedron 9 .As seen in Figs . 1 ( a ) and ( b ) , these two forms of polyhedra share shared faces perpendicularly to the c - axis 10 .",
        "rewrite_text": "**Title:** Magnetization and Specific Heat of TbFe3(BO3)4: Experimental Insights and Crystal Field Calculations\n\n**Abstract:** This study investigates the magnetic properties of single crystals of TbFe3(BO3)4 through comprehensive measurements of magnetization, susceptibility, and specific heat. The analysis is grounded in the crystal-field separation scheme pertinent to Tb3+ ions. Our findings reveal that the ground state doublet exhibits Ising-like anisotropy along the c-axis, characterized by a g-factor of gz = 8.0 ± 0.1. This anisotropy is responsible for the remarkable spontaneous polarization observed, approximately Ps ~ 1 μC/cm². The experimental data align closely with theoretical predictions, with the exception of a minor discrepancy in the specific heat measurements at temperatures below 2 K. This deviation may be attributed to the presence of impurities or defects within the crystal samples. \n\nTbFe3(BO3)4 is part of the rare-earth iron borate family, RFe3(BO3) (where R = Y, Yb, Lu), which has attracted considerable attention due to their intriguing physical properties, including ferroelectricity, multiferroicity, colossal magnetoresistance, and significant quantum interactions. Notably, TbFe3(BO3)4 exhibits a substantial spontaneous polarization of Ps ~ 1 μC/cm² at room temperature, a phenomenon linked to its distinctive crystal structure. In this compound, iron (Fe) ions create a three-dimensional network of corner-sharing tetrahedra by sharing apical oxygen atoms, while terbium (Tb) ions occupy two distinct sites: one surrounded by eight oxygen atoms, forming a square antiprismatic coordination, and another surrounded by six oxygen atoms, resulting in a trigonal prismatic coordination. The geometric arrangement of these polyhedra, which share faces perpendicular to the c-axis, is illustrated in the accompanying figures. This research contributes to the understanding of the magnetic and thermal properties of TbFe3(BO3)4, paving the way for future studies on its potential applications in advanced materials. \n\n**Keywords:** Magnetism; Crystal field model; Specific heat measurement; Susceptibility measurement; Single-crystal growth; Anisotropic magnetoresistance; Polarized neutron scattering.",
        "ori-fast-z-score": 0.8908708063747479,
        "water-fast-z-score": 7.365059028153745,
        "rewrite-fast-z-score": 2.4959226008892244
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Water vapour and hydrogen in the terrestrial-planet-forming region of a protoplanetary disk .\nAbstract:\nWe report on observations made with Herschel Space Observatory (Pilbratt et al., 2010) of water vapour emission lines at 557 GHz, 1669 GHz and 1720 GHz towards two young stars surrounded by circumstellar disks: HD 100546 and TW Hya. The data were obtained as part of the Open Time Key Programme  Formation and Evolution of Planetary Systems  (FEPS). We detect water vapour emission over an extended range of radial velocities for both targets. For HD 100546 we find that the line profiles are consistent with Keplerian rotation around a central mass of 1.8 M . In addition to this broad component, which is likely associated with the outer regions of the disk, there appears to be a narrower feature superimposed on each profile. This narrow component may arise either from gas located close to the star or from outflowing material along our line-of-sight.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Water vapour and hydrogen in the terrestrial - planet - creating area of a protoplanetary disk . Abstract : We report on observations made with Herschel Space Observatory ( Pilbratt et al . , 2010 ) of water vapour emission lines at 557 GHz , 1669 GHz and 1720 GHz towards two young stars surrounded by circumstellar disks : HD 100546 and TW Hya .The data were obtained as part of the Open Time Key Programme Formation and Evolution of Planetary Systems ( FEPS ) . We detect water vapour pollution over an extended range of radial velocities for both objectives .For HD 100546 we find that the line profiles are compatible with Keplerian rotation around a central mass of 1 . 8 M . In addition to this wide component , which is probably associated with the exterior areas of the disk , there seems to be a narrower feature superimposed on each profile .This narrow element may arise either from gas located close to the star or from outflowing matter along our line - of - view .",
        "rewrite_text": "Title: Water Vapor and Hydrogen in the Terrestrial Planet-Creating Region of a Protoplanetary Disk\n\nAbstract: In this study, we present findings from observations conducted using the Herschel Space Observatory (Pilbratt et al., 2010), focusing on the emission lines of water vapor at frequencies of 557 GHz, 1669 GHz, and 1720 GHz. Our observations target two young stellar objects, HD 100546 and TW Hya, both of which are enveloped by circumstellar disks. These observations were part of the Open Time Key Programme dedicated to the Formation and Evolution of Planetary Systems (FEPS). Our results reveal the presence of water vapor across a broad spectrum of radial velocities for both stars. Specifically, for HD 100546, the line profiles we analyzed are consistent with Keplerian rotation around a central mass estimated at 1.8 solar masses. Alongside this broader component, which likely corresponds to the outer regions of the disk, we also identify a narrower feature that appears superimposed on each spectral profile. This narrow component may originate from gas situated in close proximity to the star or could be indicative of outflowing material along our line of sight. These observations provide critical insights into the dynamics and composition of the protoplanetary disk environment, shedding light on the processes that contribute to the formation of terrestrial planets. The detection of water vapor in these regions is particularly significant, as it suggests the potential for habitable conditions in the early stages of planetary system development. Our findings underscore the importance of further investigations into the chemical and physical properties of protoplanetary disks, as they play a crucial role in understanding the origins of planetary systems and the conditions necessary for life.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 4.275930552470682,
        "rewrite-fast-z-score": 0.7492686492653552
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Post-Oligarchic Evolution of Protoplanetary Embryos and the Stability of Planetary Systems .\nAbstract:\nWe present an analysis of the stability of planetary systems in which protoplanetary embryos evolve under oligarchy, i.e., they are able to eject each other s neighbors by gravitational scattering but not themselves. We find that this process leads to rapid growth of the largest embryo until it reaches its isolation mass (the minimum mass required for runaway accretion). The system then evolves into either a single planet or two planets with comparable masses depending on how close the initial conditions were to instability. This evolution is very different than what happens when all bodies grow simultaneously; in particular, we show that there can be multiple stable outcomes even if the initial conditions are identical. Our results suggest that the formation of terrestrial planets may have proceeded through several stages including oligarchy before reaching their final state as observed today. In addition, our work provides new insights about the origin of Mercury-like planets. Protoplanetary embryos form in circumstellar disks around young stars and undergo mutual gravitational interactions during their growth phase. These interactions lead to orbital migration and dynamical instabilities such as collisions between neighboring embryos. If these processes occur frequently enough, only one body will survive at the end of the growth stage leaving behind a planetary system consisting of just one planet. However, recent studies indicate that many planetary systems contain more than one planet suggesting that some mechanism must exist to prevent complete destruction of the system. Here we study the possibility that protoplanetary embryos follow a hierarchical evolutionary path where they first grow hierarchically via gravitational scattering followed by runaway accretion once the largest embryo has reached its isolation mass. Using numerical simulations, we demonstrate that this scenario naturally explains the existence of multi-planet systems while also reproducing the properties of known exoplanets.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Post - Oligarchic Evolution of Protoplanetary Embryos and the Stability of Planetary Systems . Abstract : We present an assessment of the stability of planetary environments in which protoplanetary embryos grow under oligarchy , i . e . , they are able to eject each other s neighbors by gravitational scattering but not themselves .We see that this process results to rapid growth of the largest embryo until it hits its isolation volume ( the minimum mass needed for runaway accretion ) . The system then evolves into either a single planet or two planets with similar masses depending on how close the early conditions were to instability .This evolution is very different than what happens when all bodies grow simultaneously ; in particular , we find that there can be several stable outcomes even if the first environments are identical . Our results propose that the formation of terrestrial worlds may have continued through several stages including oligarchy before reaching their final condition as predicted today .In addition , our work brings fresh insights about the origin of Mercury - like planets . Protoplanetary embryos form in circumstellar disks around young galaxies and undergo mutual gravitational interactions during their development period .These interactions result to orbital movement and dynamical instabilities such as collisions between neighboring embryos . If these mechanisms occur frequently enough , only one body will survive at the end of the development period leaving behind a planetary system consisting of just one planet .However , recent studies reveal that several planetary complexes comprise more than one planet suggesting that some process need arise to resist total destruction of the system . Here we study the suggestion that protoplanetary embryos pursue a hierarchical evolutionary course where they first develop hierarchically via gravitational waves followed by runaway accretion once the greatest embryo has reached its isolation mass .Using numerical simulations , we prove that this situation naturally reveals the existence of dual - planet systems while actually reproducing the properties of known exoplanets .",
        "rewrite_text": "In this article, we investigate the stability of planetary systems formed by protoplanetary embryos that evolve under oligarchic conditions, where these embryos can gravitationally interact with and eject their neighbors, but do not expel themselves. Our findings indicate that this oligarchic growth leads to the rapid expansion of the largest embryo until it reaches its isolation volume, which is the critical mass required for runaway accretion to occur. Following this phase, the system can evolve into either a single planet or a binary system of two similarly sized planets, contingent on the initial conditions and their proximity to instability. This evolutionary pathway contrasts sharply with scenarios where all bodies grow concurrently, revealing that multiple stable configurations can emerge even from identical starting environments. \n\nOur research suggests that the formation of terrestrial planets may have undergone several evolutionary stages, including oligarchic interactions, before achieving their current states. Furthermore, we provide new insights into the formation of Mercury-like planets. Protoplanetary embryos develop within circumstellar disks surrounding young stars, engaging in mutual gravitational interactions that lead to orbital dynamics and potential collisions. If these interactions are sufficiently frequent, it is likely that only one embryo will survive, resulting in a planetary system with a single planet. However, recent observations indicate that many planetary systems contain multiple planets, implying that some mechanisms must counteract the tendency toward total system destruction. \n\nIn this study, we propose that protoplanetary embryos follow a hierarchical evolutionary trajectory, initially developing through gravitational interactions and subsequently undergoing runaway accretion once the largest embryo attains its isolation mass. Through numerical simulations, we demonstrate that this evolutionary model naturally accounts for the existence of dual-planet systems and successfully replicates the characteristics of known exoplanets, thereby enhancing our understanding of planetary system formation and stability.",
        "ori-fast-z-score": -1.3063945294843617,
        "water-fast-z-score": 7.242717283063319,
        "rewrite-fast-z-score": 0.32659863237109044
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The evolution of interstellar clouds in a streaming hot plasma including heat conduction .\nAbstract:\nWe study the thermal instability and fragmentation of an initially uniform, isothermal cloud embedded in a hot plasma flow with constant velocity shear using two-dimensional hydrodynamic simulations. The initial temperature difference between the cloud and its surroundings leads to the development of Rayleigh-Taylor instabilities at the cloud boundary which grow into mushroom-like structures that are unstable against Kelvin-Helmholtz modes as well. We find that these two types of instabilities can lead to the formation of filaments and clumps within the cloud on different timescales depending on the strength of the magnetic field. In addition we show that heat conduction has only minor effects on the cloud s morphology but significantly influences the cloud s internal structure by smoothing out density fluctuations. Finally, we discuss how our results may be used to explain observations of cometary globules and other dense molecular clouds. Subject headings: Instability -Conductivity -Magnetic fields -Clouds",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The transformation of interstellar dust in a streaming hot plasma including warmth conduction . Abstract : We research the thermal instability and fragmentation of an initially uniform , isothermal cloud embedded in a heated plasma flow with constant velocity shear using two - dimensional hydrodynamic simulations .The initial thermal shift between the cloud and its surroundings leads to the development of Rayleigh - Taylor instabilities at the cloud boundary which build into fungus - like structures that are unstable against Kelvin - Helmholtz modes as also . We see that these two forms of instabilities can lead to the formation of filaments and clumps within the cloud on various timescales based on the strength of the magnetic force .In addition we prove that heat conduction has only minor impacts on the cloud s morphology but significantly effects the cloud s internal structure by smoothing out density fluctuations . Finally , we talk how our findings may be used to explain observations of cometary globules and other dense molecular clouds .Subject headings : Instability - Conductivity - Magnetic fields - Clouds",
        "rewrite_text": "**Title:** The Transformation of Interstellar Dust in a Streaming Hot Plasma Including Heat Conduction\n\n**Abstract:** This study investigates the thermal instability and fragmentation of a uniform, isothermal cloud situated within a heated plasma flow characterized by constant velocity shear. Utilizing two-dimensional hydrodynamic simulations, we analyze the initial thermal gradient between the cloud and its surrounding medium, which instigates the emergence of Rayleigh-Taylor instabilities at the cloud's boundary. These instabilities evolve into structures resembling fungi, which are further susceptible to Kelvin-Helmholtz instabilities. Our findings indicate that the interplay between these two instability types can result in the formation of filaments and clumps within the cloud, with the timescales for these developments being influenced by the strength of the magnetic forces present. \n\nMoreover, our research demonstrates that while heat conduction has a minimal effect on the overall morphology of the cloud, it plays a crucial role in altering the internal structure by mitigating density fluctuations. This smoothing effect is significant in understanding the cloud's stability and evolution. We also discuss the implications of our results in relation to astronomical observations, particularly in explaining the characteristics of cometary globules and other dense molecular clouds. Our findings contribute to a deeper understanding of the dynamics of interstellar dust and the processes that govern the formation and evolution of cosmic structures. \n\n**Subject Headings:** Instability, Conductivity, Magnetic Fields, Clouds",
        "ori-fast-z-score": 1.2375966910186262,
        "water-fast-z-score": 5.813776741499453,
        "rewrite-fast-z-score": 1.6865480854231356
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Emergence and resilience of cooperation in the spatial Prisoner s Dilemma via a reward mechanism .\nAbstract:\nWe study how to promote cooperation among selfish agents by introducing a reward mechanism into the spatial prisoners  dilemma (SPD). In our model, each agent is located on an LxL square lattice with periodic boundary conditions. Each agent interacts only with its four nearest neighbors at every time step. The payoff matrix for this game is given by: \nwhere R>P>0<S<0<T. We find that when T=0.5R+0.25P-0.75S, cooperators can survive even if they are initially less than 25% of all players. Moreover, we show that the fraction of cooperators increases as the temptation to defect decreases or the cost-to-benefit ratio increases. Finally, we demonstrate that the introduction of punishment does not destroy cooperative behavior but rather enhances it. Our results suggest that the proposed reward mechanism may be useful for promoting cooperation in real-world systems such as biological communities and social networks.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Emergence and resilience of participation in the spatial Prisoner s Dilemma via a incentive mechanism . Abstract : We research how to promote cooperation among selfish forces by bringing a incentive mechanism into the spatial prisoners dilemma ( SPD ) .In our model , each agent is situated on an LxL square lattice with periodic border conditions . Each agent interacts only with its four nearest neighbors at every time step .The payoff matrix for this play is given by : where R > P > 0 < S < 0 < T . We see that when T = 0 . 5R + 0 . 25P - 0 . 75S , cooperators can escape even if they are initially fewer than 25 % of all players .Moreover , we find that the fraction of cooperators increases as the temptation to defect decreases or the cost - to - benefit ratio increases . Finally , we prove that the introduction of punishment does not eliminate cooperative performance but rather enhances it .Our results propose that the suggested incentive mechanism may be beneficial for encouraging cooperation in real - time systems such as natural communities and social organizations .",
        "rewrite_text": "Title: Emergence and Resilience of Participation in the Spatial Prisoner's Dilemma via an Incentive Mechanism\n\nAbstract: This study investigates strategies to foster cooperation among self-interested agents within the framework of the spatial Prisoner's Dilemma (SPD) by incorporating an incentive mechanism. Our model positions each agent on an LxL square lattice with periodic boundary conditions, allowing interactions exclusively with four nearest neighbors at each time step. The payoff matrix is defined by the parameters R, P, S, and T, where the relationships R > P > 0 > S > 0 > T hold true. Notably, we demonstrate that when the condition T = 0.5R + 0.25P - 0.75S is satisfied, cooperators can thrive even when they initially represent less than 25% of the total population. Furthermore, our findings indicate that the proportion of cooperators tends to rise as the temptation to defect diminishes or as the cost-to-benefit ratio of cooperation improves. Importantly, we establish that the implementation of a punishment mechanism does not undermine cooperative behavior; rather, it serves to enhance it. These results suggest that the proposed incentive mechanism could play a crucial role in promoting cooperation in dynamic systems, including natural ecosystems and social organizations. Our research contributes to the understanding of cooperation dynamics and offers practical insights for fostering collaborative behavior in various real-world contexts.",
        "ori-fast-z-score": -1.4814874939752933,
        "water-fast-z-score": 5.287913134352312,
        "rewrite-fast-z-score": -0.8251369970070347
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Disclinations, dislocations and continuous defects: a reappraisal .\nAbstract:\nThe concept of defects in crystals has been developed by the Russian school since the 1930s. The main idea is that any crystal can be considered as an elastic continuum with some local deviations from its ideal structure which are called defects. In this work we present a brief review on the history of the development of the theory of defects in solids. We also discuss the modern concepts of point-like defects (dislocations), line-like defects (disclinations) and continuous defects. Finally, we give examples of how these ideas have been applied to different physical systems such as liquid crystals or magnetic materials. Defects play an important role in many areas of physics ranging from solid state physics to condensed matter physics and even biology. They appear naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures. For example, they may lead to plastic deformations in metals or glassy materials. On the other hand, defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Disclinations , dislocations and continuous defects : a reappraisal . Abstract : The concept of flaws in crystals has been pioneered by the Russian school since the 1930s .The main idea is that any crystal can be regarded as an elastic continuum with some local deviations from its ideal structure which are called flaws . In this study we present a brief review on the history of the development of the principle of flaws in solids .We also discuss the newer concepts of point - like defects ( dislocations ) , line - like defects ( disclinations ) and continuous defects . Finally , we give evidence of how these ideas have been used to different mechanical systems such as fluid crystals or magnetic materials .Defects serve an important role in many fields of science ranging from solid state mechanics to condensed matter science and even science . They arise naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures .For instance , they may contribute to plastic deformations in metals or glassy materials . On the other hand , defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization .",
        "rewrite_text": "Title: Disclinations, Dislocations, and Continuous Defects: A Reappraisal\n\nAbstract: The exploration of defects in crystalline structures has its roots in the pioneering work of the Russian scientific community dating back to the 1930s. The foundational concept posits that a crystal can be viewed as an elastic continuum, with localized deviations from its perfect lattice structure, referred to as defects. This article provides a comprehensive overview of the historical evolution of the defect theory in solid materials, highlighting key advancements and theoretical frameworks that have emerged over the decades. We delve into contemporary understandings of various types of defects, including point-like defects known as dislocations, line-like defects termed disclinations, and the broader category of continuous defects. Furthermore, we illustrate the application of these theoretical constructs across diverse mechanical systems, such as liquid crystals and magnetic materials, demonstrating their significance in practical scenarios. Defects play a crucial role across multiple scientific disciplines, from solid-state physics to condensed matter science, and even extend into broader scientific inquiries. They naturally emerge during phase transitions, particularly when systems transition between ordered and disordered states, such as during melting or at critical temperatures. For example, defects can significantly influence the plastic deformation mechanisms in metals and amorphous materials. Additionally, they are integral to determining the macroscopic properties of solids, including electrical conductivity and magnetic behavior. This article aims to reappraise the importance of defects in solid materials, emphasizing their multifaceted roles and the ongoing relevance of defect theory in advancing our understanding of material properties and behaviors.",
        "ori-fast-z-score": 0.3144854510165755,
        "water-fast-z-score": 5.421374765483944,
        "rewrite-fast-z-score": 1.520526224699857
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Statistical Behavior Of Domain Systems .\nAbstract:\nThe statistical behavior of domain systems is studied by using the concept of entropy and its associated quantities, such as information content and mutual information.  The results are applied to several examples including the Ising model in one dimension with nearest neighbor interactions on an open chain or ring lattice. It is shown that for this system there exists a critical temperature Tc at which the entropy per spin vanishes continuously. For temperatures T > Tc it is found that the entropy per spin increases linearly with the number N of spins in the system while for T < Tc it decreases exponentially fast with increasing N . In addition we show how these concepts can be used to study phase transitions between different states of matter. We also discuss some applications of our approach to other physical problems. PACS: 05.45.-a; 05.60.Fh; 05.70.Jc; 06.20.Hv; 62.25.Kx",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Statistical Behavior Of Domain Systems . Abstract : The statistical behavior of domain systems is studied by using the idea of entropy and its associated quantities , such as data content and mutual information .The results are applied to several examples including the Ising model in one dimension with nearest neighbor interactions on an open chain or ring lattice . It is demonstrated that for this scheme there exists a critical temperature Tc at which the entropy per spin vanishes constantly .For temperatures T > Tc it is found that the entropy per spin increases linearly with the number N of spinning in the system while for T < Tc it decreases exponentially rapidly with expanding N . In addition we show how these concepts can be used to study phase transitions between various states of matter .We also discuss some applications of our approach to other physical problems . PACS : 05 . 45 . - a ; 05 . 60 . Fh ; 05 . 70 . Jc ; 06 . 20 . Hv ; 62 . 25 . Kx",
        "rewrite_text": "Title: Statistical Behavior of Domain Systems\n\nAbstract: This article investigates the statistical behavior of domain systems through the lens of entropy and its related metrics, including data content and mutual information. The findings are illustrated with various examples, notably the one-dimensional Ising model characterized by nearest neighbor interactions on both open chains and ring lattices. A key outcome of this research is the identification of a critical temperature, denoted as Tc, at which the entropy per spin consistently approaches zero. For temperatures exceeding Tc (T > Tc), the entropy per spin exhibits a linear increase in relation to the number of spins (N) present in the system. Conversely, for temperatures below Tc (T < Tc), the entropy per spin experiences a rapid exponential decline as N expands. Furthermore, the study elucidates how these principles can be applied to analyze phase transitions among different states of matter. The implications of our approach extend beyond the Ising model, as we also explore its relevance to various other physical phenomena. This comprehensive examination not only enhances our understanding of domain systems but also contributes to the broader field of statistical mechanics. The research is categorized under several PACS codes, including 05.45.-a, 05.60.Fh, 05.70.Jc, 06.20.Hv, and 62.25.Kx, reflecting its interdisciplinary nature and potential applications across different areas of physics.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 2.7688746209726918,
        "rewrite-fast-z-score": 1.8182745801939793
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Generating Minimally Coupled Einstein-Scalar Field Solutions from Vacuum Solutions with Arbitrary Cosmological Constant .\nAbstract:\nWe present an algorithm for generating new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and adding scalar fields in such a way that the resulting solution is minimally coupled.  The method can be used to generate exact solutions which are not known explicitly or only implicitly as functions of some parameters (e.g., by solving algebraic equations). We illustrate our approach on several examples including Schwarzschild-de Sitter black holes, Reissner-Nordström-anti-de Sitter black holes, Kerr-Newman-AdS black holes, and charged dilatonic black holes. In particular we show how one can obtain explicit expressions for the massless limit of these black hole solutions. Our results may also have applications beyond gravity theory, e.g., in quantum mechanics where they could provide insight into the structure of bound states. Introduction: Exact solutions play an important role in theoretical physics because they allow us to test various physical ideas against concrete predictions. However, finding exact solutions to physically interesting problems often turns out to be very difficult. For example, it took more than 100 years after the discovery of general relativity before the first exact black hole solutions were found  1-3 . Even today there exist many open questions about black holes  4  . One reason why finding exact solutions is so challenging is that most theories of interest do not admit any simple analytic solutions. Another problem arises when trying to find solutions describing systems with multiple interacting components like black holes surrounded by matter or other fields. Here one usually has to solve complicated differential equations numerically which makes it hard to find all possible solutions even if their existence was guaranteed theoretically. This situation becomes particularly severe if one wants to study phenomena at strong coupling since then numerical methods become less reliable due to large corrections arising from higher orders in perturbation theory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Generating Minimally Coupled Einstein - Scalar Field Solutions from Vacuum Solutions with Arbitrary Cosmological Constant . Abstract : We present an algorithm for generating new answers to the coupled Einstein - scalar field equations , beginning from vacuum solutions and adding scalar fields in such a way that the resulting solve is minimally coupled .The method can be used to create precise solutions which are not established explicitly or only implicitly as functions of some parameters ( e . g . , by modeling algebraic equations ) . We illustrate our approach on numerous instances using Schwarzschild - de Sitter dark holes , Reissner - Nordström - anti - de Sitter dark holes , Kerr - Newman - AdS red holes , and charged dilatonic black holes .In particular we show how one can obtain precise expressions for the massless maximum of these black hole solutions . Our results may also have applications beyond gravitational mechanics , e . g . , in quantum mechanics where they may provide insight into the formation of bound states .Introduction : Exact solutions play an important role in theoretical physics because they allow us to test various physical concepts against concrete expectations . However , finding exact treatments to physically exciting difficulties often comes out to be very difficult .For instance , it takes more than 100 years after the discovery of general relativity before the first accurate black hole answers were found 1 - 3 . Even nowadays there remain many open questions about black holes 4 .One reason why seeking precise solutions is so difficult is that most models of importance do not admit any straightforward analytic solutions . Another difficulty arises when trying to find solutions involving systems with many interacting components like white holes separated by matter or other fields .Here one usually has to solve intricate differential equations numerically which makes it difficult to find all possible solutions even if their existence was assured theoretically . This problem arises terribly extreme if one wants to study phenomena at strong coupling since then numerical models become fewer reliable resulting to large corrections resulting from lower orders in perturbation theory .",
        "rewrite_text": "**Title:** Generating Minimally Coupled Einstein-Scalar Field Solutions from Vacuum Solutions with Arbitrary Cosmological Constant\n\n**Abstract:** In this article, we introduce a novel algorithm designed to generate new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and incorporating scalar fields in a manner that ensures minimal coupling. This method enables the derivation of precise solutions that are either not explicitly known or only implicitly defined through parameters, such as those arising from algebraic equations. We demonstrate the effectiveness of our approach through various examples, including Schwarzschild-de Sitter black holes, Reissner-Nordström anti-de Sitter black holes, Kerr-Newman anti-de Sitter red holes, and charged dilatonic black holes. Notably, we provide explicit expressions for the massless maxima of these black hole solutions. The implications of our findings extend beyond gravitational mechanics, potentially offering insights into quantum mechanics, particularly in understanding the formation of bound states.\n\n**Introduction:** Exact solutions are crucial in theoretical physics as they allow for the testing of physical theories against empirical expectations. However, deriving exact solutions to significant physical problems often proves to be a formidable challenge. For example, it took over a century following the advent of general relativity to uncover the first accurate solutions for black holes. Even today, numerous questions regarding black holes remain unresolved. One major obstacle in finding precise solutions is that many important models do not yield straightforward analytic solutions. Additionally, the complexity increases when dealing with systems that involve multiple interacting components, such as white holes in the presence of matter or other fields. In such cases, researchers typically resort to solving complex differential equations numerically, which complicates the identification of all possible solutions, even when their theoretical existence is assured. This challenge is exacerbated in scenarios involving strong coupling, where numerical models become less reliable, leading to significant corrections from lower-order perturbation theory.",
        "ori-fast-z-score": 0.37371754637596794,
        "water-fast-z-score": 8.296529529546488,
        "rewrite-fast-z-score": 2.443142259341212
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Microlens Parallax Measurements with a Warm Spitzer .\nAbstract:\nWe present the first microlensing parallax measurements made using infrared observations by the Wide-field Infrared Survey Explorer (WISE). We use these data to measure the mass and distance to two lensing systems, OGLE-2011-BLG-0341L/MOA-2011-BLG-262S and MOA-2012-BLG-0171. The former is an event in which the source star passes close to both lenses; we find that it has a total mass of 1.4 solar masses at a distance of 4 kpc. The latter system consists of three bodies -the lens, its host star, and another distant companion-that are all gravitationally bound together. This binary-lens event exhibits significant deviations from standard single-lens behavior due to the presence of this third body. Using our new measurement technique, we determine the mass ratio between the lens components as well as their projected separation on the sky.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Microlens Parallax Measurements with a Warm Spitzer . Abstract : We report the first microlensing parallax observations made using infrared observations by the Wide - field Infrared Survey Explorer ( WISE ) .We use these information to measure the mass and distance to two lensing systems , OGLE - 2011 - BLG - 0341L / MOA - 2011 - BLG - 262S and MOA - 2012 - BLG - 0171 . The former is an event in which the source galaxy goes close to both lenses ; we find that it has a total mass of 1 . 4 solar masses at a distance of 4 kpc .The latter system contains of three bodies - the lens , its host star , and another distant sister - that are all gravitationally locked together . This binary - lens event displays substantial deviations from standard single - lens activity related to the presence of this third body .Using our new gauge methodology , we determine the mass ratio between the lens components as well as their estimated separation on the sky .",
        "rewrite_text": "We present the inaugural microlensing parallax observations conducted using infrared data from the Wide-field Infrared Survey Explorer (WISE). This study focuses on two distinct lensing systems: OGLE-2011-BLG-0341L/MOA-2011-BLG-262S and MOA-2012-BLG-0171. For the first system, we observe an event where the source galaxy approaches both lenses, allowing us to calculate a total mass of 1.4 solar masses, situated at a distance of approximately 4 kiloparsecs. The second system is more complex, comprising three gravitationally bound bodies: the lens, its host star, and a more distant companion. This binary-lens event exhibits significant deviations from the typical behavior expected in single-lens scenarios, primarily due to the influence of the third body. By employing our novel measurement technique, we are able to ascertain the mass ratio between the lens components and estimate their spatial separation on the celestial plane. These findings enhance our understanding of microlensing phenomena and contribute valuable insights into the dynamics of multi-body gravitational systems. The results underscore the potential of infrared observations in advancing microlensing studies, particularly in revealing the intricate relationships between lensing objects and their environments.",
        "ori-fast-z-score": -0.47809144373375745,
        "water-fast-z-score": 2.3570226039551585,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The potential of mass rearing of Monoksa dorsiplana (Pteromalidae) a native gregarious ectoparasitoid of Pseudopachymeria spinipes (Bruchidae)in South America .\nAbstract:\nThe objective was to evaluate the potential use of Monoksa dorsiplana as an alternative biological control agent against Pseudopachymeria sp. (Bruchidae). The parasitoids were obtained in laboratory and released on P.sp. eggs laid by females collected at different locations in Brazil, Argentina and Paraguay. Egg parasitism ranged between 0.5 and 88% depending on location. Parasitized eggs hatched after 7 days under controlled conditions. Males emerged first followed by females. Female longevity varied according to temperature ranging from 11 to 21 days at 25 °C; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C. Females oviposited for up to three weeks when fed with honey solution. This species is considered highly suitable for mass production because it has high reproductive capacity and short life cycle. It can be used successfully in integrated pest management programs aimed at reducing damage caused by this insect pest.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The possibility of mass rearing of Monoksa dorsiplana ( Pteromalidae ) a native gregarious ectoparasitoid of Pseudopachymeria spinipes ( Bruchidae ) in South America . Abstract : The goal was to analyze the possibilities application of Monoksa dorsiplana as an alternative biological management weapon against Pseudopachymeria sp .( Bruchidae ) . The parasitoids were obtained in laboratory and published on P . sp .eggs laid by females collected at different places in Brazil , Argentina and Paraguay . Egg parasitism ranged between 0 . 5 and 88 % depending on location .Parasitized nests hatched after 7 days under regulated conditions . Males appeared first followed by females .Female longevity varied based to heat ranging from 11 to 21 nights at 25 °C ; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C . Females oviposited for up to three weeks when fed with honey solution .This species is regarded highly suited for mass production because it has large sexual capacity and low life cycle . It can be used successfully in effective pest management projects designed at decreasing damage suffered by this insect pest .",
        "rewrite_text": "**Title:** The Potential for Mass Rearing of Monoksa dorsiplana (Pteromalidae): A Native Gregarious Ectoparasitoid of Pseudopachymeria spinipes (Bruchidae) in South America\n\n**Abstract:** This study investigates the feasibility of utilizing Monoksa dorsiplana as a biological control agent against the pest species Pseudopachymeria spinipes (Bruchidae). The research involved the collection of parasitoids from various locations in Brazil, Argentina, and Paraguay, where they were reared in laboratory conditions. The findings revealed that egg parasitism rates varied significantly by location, ranging from 0.5% to 88%. Under controlled conditions, parasitized nests exhibited hatching after a period of seven days, with males emerging before females. The longevity of female M. dorsiplana was influenced by temperature, with lifespans of 11 to 21 nights at 25 °C, 14 to 23 days at 20 °C, and 16 to 27 days at 15 °C. Notably, females were capable of ovipositing for up to three weeks when provided with a honey solution as nourishment. The characteristics of M. dorsiplana, including its high reproductive capacity and relatively short life cycle, suggest that it is well-suited for mass rearing. This parasitoid holds promise for integration into pest management strategies aimed at mitigating the damage caused by Pseudopachymeria spinipes. The results of this study underscore the potential of M. dorsiplana as an effective biocontrol agent, contributing to sustainable agricultural practices in South America. Further research is recommended to optimize rearing techniques and assess the long-term impact of this parasitoid on pest populations.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 5.735393346764043,
        "rewrite-fast-z-score": 0.9761870601839528
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The impact of radio feedback from active galactic nuclei in cosmological simulations: Formation of disk galaxies .\nAbstract:\nWe study the effect of AGN feedback on galaxy formation using hydrodynamical zoom-in cosmological simulations with different prescriptions for AGN feedback and compare them to observations. We find that our fiducial model, which includes both thermal energy injection into gas particles surrounding black holes (BHs) as well as kinetic outflows driven by BH winds, reproduces many observed properties of massive galaxies at z = 0 including their stellar mass function, sizes, morphologies, star formation rates, metallicities, and cold gas fractions. In this model, we also find that most of the stars are formed inside dark matter halos with masses between 10 11 M⊙ < Mhalo < 5 × 10 12 M⊙, while lower-mass halos contribute only about 10% of all stars. The majority of these low-mass halos host dwarf galaxies or satellite galaxies around more massive central galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of radio feedback from active galactic nuclei in cosmological simulations : Formation of disk galaxies . Abstract : We research the impact of AGN feedback on star formation using hydrodynamical zoom - in cosmological simulations with various prescriptions for AGN feedback and link them to observations .We see that our fiducial model , which includes both thermal energy injection into gas molecules surrounding black holes ( BHs ) as well as kinetic outflows driven by BH winds , reproduces many observed properties of large galaxies at z = 0 including their stellar mass function , sizes , morphologies , star formation rates , metallicities , and warm gas fractions . In this model , we also find that most of the stars are created inside dark matter halos with masses between 10 11 [UNK] < Mhalo < 5 × 10 12 [UNK] , while lower - mass halos contribute only about 10 % of all stars .The majority of these low - mass halos host dwarf stars or satellite galaxies around more massive central clusters .",
        "rewrite_text": "In this study, we investigate the effects of active galactic nuclei (AGN) feedback on star formation through hydrodynamical zoom-in cosmological simulations, employing a range of AGN feedback models and correlating our findings with observational data. Our primary focus is on a fiducial model that integrates both thermal energy injection into the gas surrounding black holes (BHs) and kinetic outflows resulting from BH winds. This comprehensive approach allows us to replicate several key characteristics of large galaxies at redshift z = 0, including their stellar mass functions, sizes, morphologies, star formation rates, metallicities, and warm gas fractions. \n\nOur results indicate that the majority of star formation occurs within dark matter halos with masses ranging from 10^11 to 5 × 10^12 solar masses, while halos of lower mass contribute merely about 10% to the total stellar population. Notably, these lower-mass halos predominantly host dwarf stars or serve as satellite galaxies orbiting more massive central clusters. This research underscores the critical role of AGN feedback in shaping the formation and evolution of disk galaxies, providing insights into the complex interplay between black hole activity and galaxy development in the context of cosmological structures. By linking our simulation outcomes with observational evidence, we enhance our understanding of the mechanisms driving galaxy formation and the influence of AGN feedback in the broader cosmic landscape.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 3.18316353970102,
        "rewrite-fast-z-score": 1.993231791080248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stellar kinematics in the remote Leo II dwarf spheroidal galaxy -- Another brick in the wall .\nAbstract:\nWe present new spectroscopic observations for eight red giant stars in the nearby dwarf spheroidal galaxy, Leo II (D = 3 Mpc). The data were obtained with the Keck telescope and HIRES spectrograph over three nights during August 2005. We measure heliocentric radial velocities ranging between -150 to +50 km/sec. These values are consistent with previous measurements made by other authors using different techniques. Using these new data we have determined that there is no significant rotation or streaming motion within this system. This result supports theoretical predictions based on N-body simulations which suggest that dark matter halos should be nearly spherical systems. In addition, our results provide further evidence against the possibility that Leo II may contain an intermediate mass black hole at its center. \n \n Keywords: Dwarf galaxies; Black holes; Dark Matter Halos; Rotation Curves; Stellar Kinematics; Spherical Symmetry",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stellar kinematics in the distant Leo II dwarf spheroidal galaxy - - Another brick in the wall . Abstract : We report new spectroscopic observations for eight red giant galaxies in the nearby dwarf spheroidal galaxy , Leo II ( D = 3 Mpc ) .The data were obtained with the Keck camera and HIRES spectrograph over three nights during August 2005 . We determine heliocentric radial velocities ranging between - 150 to + 50 km / sec .These values are compatible with previous measurements made by other researchers using different methods . Using these new data we have concluded that there is no considerable rotation or streaming motion within this system .This result provides theoretical estimates based on N - bodies simulations which propose that dark matter halos should be nearly spherical systems . In addition , our findings provide further evidence against the idea that Leo II may contain an intermediate mass black hole at its core .Keywords: Dwarf galaxies; Black holes; Dark Matter Halos; Rotation Curves; Stellar Kinematics; Spherical Symmetry",
        "rewrite_text": "Title: Stellar Kinematics in the Distant Leo II Dwarf Spheroidal Galaxy - Another Brick in the Wall\n\nAbstract: In this study, we present new spectroscopic observations of eight red giant stars within the nearby dwarf spheroidal galaxy, Leo II, located at a distance of approximately 3 Mpc. The observations were conducted using the Keck Observatory's camera and HIRES spectrograph over three nights in August 2005. Our analysis yielded heliocentric radial velocities for the observed stars ranging from -150 to +50 km/s. These measurements align well with previous studies conducted by other researchers employing various methodologies. Importantly, our data indicate a lack of significant rotation or streaming motion within the Leo II system. This finding supports theoretical predictions derived from N-body simulations, which suggest that dark matter halos are predominantly spherical in shape. Furthermore, our results provide additional evidence against the hypothesis that Leo II harbors an intermediate mass black hole at its center. Overall, this research contributes to the ongoing discourse regarding the structural dynamics of dwarf galaxies and the nature of dark matter, reinforcing the notion of spherical symmetry in dark matter distributions and challenging existing theories about the presence of massive black holes in such systems. \n\nKeywords: Dwarf galaxies; Black holes; Dark Matter Halos; Rotation Curves; Stellar Kinematics; Spherical Symmetry.",
        "ori-fast-z-score": 0.9428090415820635,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 0.8340576562282991
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Higher-Order Angular Galaxy Correlations in the SDSS: Redshift and Color Dependence of non-Linear Bias .\nAbstract:\nWe measure higher-order angular galaxy correlations using data from the Sloan Digital Sky Survey (SDSS). We use two-point correlation functions to estimate the linear bias parameter, b1, for galaxies with different luminosities and colors at redshifts z = 0.1 − 1.0. The results are compared against predictions based on perturbation theory including corrections up to third order in the density field. In addition we study how the non-linear bias parameters b2 and b3 depend on redshift and color. Our main findings are:  -The measured values of b1 agree well with theoretical expectations within their uncertainties.  -The second-order bias parameter b2 is consistent with zero over most of our sample except for faint blue galaxies at low redshifts where it has a positive value that increases towards lower redshifts.  -The third-order bias parameter b3 shows no significant dependence on either luminosity or color but its amplitude decreases significantly as one goes to higher redshifts.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Higher - Order Angular Galaxy Correlations in the SDSS : Redshift and Color Dependence of non - Linear Bias . Abstract : We estimate higher - order spatial galaxy correlations using data from the Sloan Digital Sky Survey ( SDSS ) .We use two - point coupling functions to estimate the linear bias variable , b1 , for galaxies with various luminosities and colors at redshifts z = 0 . 1 − 1 . 0 . The results are compared against predictions based on perturbation theory including corrections up to third order in the density field .In addition we study how the non - linear bias parameters b2 and b3 depend on redshift and color . Our main results are : - The measured measures of b1 comply better with theoretical expectations within their uncertainties .- The second - order bias function b2 is consistent with zero over most of our sample except for faint blue clusters at low redshifts where it has a positive value that increases towards higher redshifts . - The third - order bias parameter b3 shows no considerable dependence on either luminosity or color but its amplitude decreases dramatically as one goes to higher redshifts .",
        "rewrite_text": "In this study, we investigate higher-order spatial correlations among galaxies utilizing data from the Sloan Digital Sky Survey (SDSS). Our analysis focuses on the two-point coupling functions to derive the linear bias variable, denoted as b1, for galaxies categorized by varying luminosities and colors across redshifts ranging from z = 0.1 to 1.0. We juxtapose our empirical findings with theoretical predictions grounded in perturbation theory, incorporating corrections up to the third order in the density field. Furthermore, we delve into the dependence of non-linear bias parameters, specifically b2 and b3, on redshift and color.\n\nOur primary findings reveal that the measured values of b1 align more closely with theoretical expectations when accounting for uncertainties. Notably, the second-order bias function, b2, remains consistent with zero for the majority of our sample. However, we observe a significant deviation in faint blue clusters at lower redshifts, where b2 takes on a positive value that escalates with increasing redshift. In contrast, the third-order bias parameter, b3, exhibits minimal variation with respect to luminosity or color; however, its amplitude experiences a marked decline as redshift increases.\n\nThese results contribute to a deeper understanding of the non-linear biasing mechanisms in galaxy formation and evolution, highlighting the intricate relationship between galaxy properties and their spatial distributions across different cosmic epochs. Our findings underscore the importance of considering both redshift and color when analyzing galaxy correlations, providing valuable insights for future cosmological studies and the interpretation of large-scale structure in the universe.",
        "ori-fast-z-score": 0.8728715609439696,
        "water-fast-z-score": 3.796283011826483,
        "rewrite-fast-z-score": 1.8107149208503706
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Black Hole Motion as Catalyst of Orbital Resonances .\nAbstract:\nWe study the orbital evolution in galactic nuclei with supermassive black holes (SMBHs) and show that SMBH motion can lead to strong resonant interactions between stars, which may be responsible for some observed phenomena such as nuclear star clusters or tidal disruption events.  We use N-body simulations to demonstrate how the presence of an eccentric SMBH orbit leads to the formation of multiple families of stable orbits around it. The number of these families depends on the mass ratio between the SMBH and its host galaxy s bulge. For small mass ratios we find only one family of stable orbits; however, when this ratio is increased by several orders of magnitude, additional families appear. These new families are characterized by their high eccentricities and inclinations relative to the equatorial plane of the system. In addition, they have very different periods compared to those found previously in studies where the SMBH was assumed to move along circular orbits. Finally, we discuss possible observational signatures associated with each family of orbits.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Black Hole Motion as Catalyst of Orbital Resonances . Abstract : We research the orbital evolution in galactic nuclei with supermassive black holes ( SMBHs ) and find that SMBH motion can lead to powerful resonant interactions between stars , which perhaps be responsible for some observed processes such as radioactive star clusters or tidal disruption events .We use N - bodies simulations to study how the presence of an eccentric SMBH orbit leads to the formation of multiple groups of stable orbits around it . The amount of these families depends on the mass ratio between the SMBH and its host galaxy s bulge .For small mass ratios we find only one group of stable orbits ; however , when this ratio is expanded by many orders of magnitude , additional siblings emerge . These new families are marked by their high eccentricities and inclinations relative to the equatorial plane of the system .In addition , they have very different times compared to those identified previously in studies where the SMBH was assumed to move along circular orbits . Finally , we explain possible observational signatures identified with each family of orbits .",
        "rewrite_text": "In this study, we investigate the dynamics of orbital evolution in galactic nuclei that host supermassive black holes (SMBHs). Our findings reveal that the motion of SMBHs can instigate significant resonant interactions among surrounding stars, potentially accounting for various astrophysical phenomena, including the formation of radioactive star clusters and tidal disruption events. Utilizing N-body simulations, we analyze the impact of an eccentric SMBH orbit on the establishment of multiple stable orbital families in its vicinity. Our results indicate that the number of these orbital families is influenced by the mass ratio between the SMBH and the bulge of its host galaxy. Specifically, for low mass ratios, we observe the emergence of a single stable orbital group. However, as the mass ratio increases dramatically, additional families of orbits manifest, characterized by elevated eccentricities and inclinations relative to the system's equatorial plane. These newly identified orbital families exhibit distinct dynamical properties and time scales that differ significantly from those observed in previous studies, which typically assumed circular SMBH orbits. Furthermore, we discuss the potential observational signatures associated with each family of orbits, providing insights into how these findings could be detected in astronomical observations. This research enhances our understanding of the complex interplay between SMBH motion and stellar dynamics, shedding light on the underlying mechanisms that drive various galactic phenomena.",
        "ori-fast-z-score": -0.21081851067789195,
        "water-fast-z-score": 4.216370213557839,
        "rewrite-fast-z-score": 0.5570860145311556
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Brownian excursion area, Wright s constants in graph enumeration, and other Brownian areas .\nAbstract:\nWe study the distribution of the total area swept out by a one-dimensional Brownian motion between two fixed times. We show that this distribution is given by an explicit formula involving the modified Bessel function I0(x). This result can be used to derive several interesting identities for special functions such as the Riemann zeta-function or the Hurwitz zeta-functions at even arguments. In particular we obtain new proofs of some results due to Wright on the number of graphs with n vertices having certain properties (such as being bipartite) which are related to the coefficients appearing in the expansion of the exponential generating function of these numbers into powers of t. Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials. The main tool will be the Feynman-Kac representation of the solution of the heat equation. Let Wt denote standard Brownian motion starting at 0. For any real number s > 0 let us consider the random variable A(s), defined as the total area swept out during the time interval  0,s  by the process Wt:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Brownian excursion area , Wright s constants in graph enumeration , and other Brownian areas . Abstract : We study the spread of the total region swept out by a one - dimensional Brownian movement between two fixed times .We see that this distribution is given by an explicit formula involving the modified Bessel distribution I0 ( x ) . This result can be used to derive several interesting identities for special functions such as the Riemann zeta - function or the Hurwitz zeta - functions at even arguments .In particular we obtain new proofs of some results attributed to Wright on the number of graphs with n nodes having specific properties ( such as being bipartite ) which are related to the coefficients appearing in the expansion of the exponential producing function of these numbers into powers of t . Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials . The main tool will be the Feynman - Kac representation of the solve of the heat equation .Let Wt denote standard Brownian movement starting at 0 . For any real number s > 0 let us consider the random variable A ( s ) , defined as the total area swept out during the period interval 0 , s by the process Wt :",
        "rewrite_text": "**Title:** Brownian Excursion Area, Wright's Constants in Graph Enumeration, and Other Brownian Areas\n\n**Abstract:** This article investigates the area covered by a one-dimensional Brownian motion over a fixed time interval, specifically focusing on the total area swept out from time 0 to time s. We establish that the distribution of this area can be expressed through an explicit formula involving the modified Bessel function I0(x). This finding not only enhances our understanding of Brownian excursions but also facilitates the derivation of several intriguing identities related to special functions, including the Riemann zeta function and the Hurwitz zeta function evaluated at even integers. \n\nIn particular, we provide new proofs for various results originally attributed to Wright concerning the enumeration of graphs with n vertices that exhibit specific characteristics, such as bipartiteness. These results are closely linked to the coefficients in the power series expansion of the exponential generating function associated with these graph counts. \n\nAdditionally, we present an alternative proof of the identity that connects the moments of the Wiener measure with Bernoulli polynomials, further enriching the interplay between probability theory and combinatorial structures. The primary analytical tool employed in our study is the Feynman-Kac representation, which serves as a powerful method for solving the heat equation. \n\nLet Wt represent the standard Brownian motion initiated at zero. For any positive real number s, we define the random variable A(s) as the total area covered by the process Wt during the time interval from 0 to s. This exploration not only deepens the theoretical framework surrounding Brownian motion but also opens avenues for further research in both mathematical physics and combinatorial enumeration.",
        "ori-fast-z-score": 1.1659976680069961,
        "water-fast-z-score": 4.297967830559865,
        "rewrite-fast-z-score": -0.09245003270420485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometry of Borromean Halo Nuclei .\nAbstract:\nThe geometry and the structure of halo nuclei are studied in terms of their density distributions, which are obtained by solving the Schrödinger equation with realistic nuclear potentials. The results show that the three-body force plays an important role for the formation of the halo structures. It is found that the density distribution of 11Li has two peaks at large distances while those of 6He and 8Be have only one peak each. In addition to these features, it is shown that the density distribution of 12C also has a tail extending far outside its core region. These results suggest that there exist some common properties among the four halo nuclei considered here. This work was supported by the Grant-in-Aid for Scientific Research (No. 08640309) from MEXT Japan. \n \n 1 Introduction \n \n Halo nuclei are loosely bound systems whose wave functions extend over several hundred fm or more beyond the nuclear surface  1  . They were first observed experimentally as very narrow resonances in elastic scattering experiments  2  , but they can now be produced directly in fragmentation reactions  3  . Since then many experimental studies on various aspects of halo nuclei such as electromagnetic transitions  4  , breakup processes  5  , etc., have been performed  6  .\n \nIn order to understand the nature of halo nuclei theoretically, we need to know how the wave function behaves inside and outside the nucleus. For this purpose, we solve the Schrödinger equation using realistic nuclear potentials  7, 8  . We use the same method developed previously  9  where the single-particle wave functions are expanded in terms of harmonic oscillator basis states. Then the resulting matrix elements are evaluated numerically using Gaussian quadratures  10  . As for the nuclear potential, we employ the Volkov  11  and the Paris  12  potentials. The former gives a good description of the ground state energies of light nuclei up to A = 10  13  whereas the latter reproduces well the binding energy of 4He  14  . \n \n 2 Results and Discussion \n \n First let us consider the case of 11Li. Figure 1 shows the calculated density distribution together with the corresponding rms radius Rrms(A). Here we take into account all the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometry of Borromean Halo Nuclei . Abstract : The topography and the composition of halo nuclei are studied in terms of their density distributions , which are derived by solving the Schrödinger equation with realistic nuclear potentials .The results show that the three - bodies force plays an important role for the formation of the halo structures . It is found that the density distribution of 11Li has two peaks at large distances while those of 6He and 8Be have only one peak each .In addition to these characteristics , it is demonstrated that the density distribution of 12C additionally has a fin stretching far outside its core region . These conclusions show that there possess some common characteristics among the four halo nuclei discussed here .This work was supported by the Grant - in - Aid for Scientific Research ( No . 08640309 ) from MEXT Japan .1 Introduction Halo molecules are loosely bound structures whose wave properties extend over numerous hundred fm or more beyond the atomic surface 1 . They were first observed experimentally as very shallow resonances in elastic scattering experiments 2 , but they can now be formed directly in fragmentation reactions 3 .Since then many experimental studies on various parts of halo nuclei such as electromagnetic transitions 4 , breakup processes 5 , etc . , have been performed 6 . In order to realize the nature of halo nuclei theoretically , we require to consider how the wave behavior behaves inside and outside the nucleus .For this use , we solve the Schrödinger equation using simple nuclear potentials 7 , 8 . We use the same method developed previously 9 where the single - particle wave functions are expanded in terms of harmonic oscillator basis states .Then the resulting matrix elements are tested numerically utilizing Gaussian quadratures 10 . As for the atomic potential , we utilize the Volkov 11 and the Paris 12 potentials .The former gives a better representation of the ground state energies of light nuclei up to A = 10 13 whereas the former reproduces well the binding energy of 4He 14 . 2 Results and Discussion First let us consider the case of 11Li .Figure 1 shows the adjusted density spread together with the associated rms radius Rrms ( A ) . Here we took into consideration all the",
        "rewrite_text": "**Title: Geometry of Borromean Halo Nuclei**\n\n**Abstract:** This study investigates the structural characteristics and composition of halo nuclei by analyzing their density distributions, which are obtained through the solution of the Schrödinger equation using realistic nuclear potentials. The findings indicate that three-body forces significantly influence the formation of halo structures. Notably, the density distribution of the halo nucleus 11Li exhibits two distinct peaks at considerable distances, in contrast to the single-peak distributions observed in 6He and 8Be. Furthermore, the density distribution of 12C reveals an additional feature that extends well beyond its core region. These results suggest that the four halo nuclei examined share several common characteristics, highlighting the intricate nature of their configurations. The research was supported by the Grant-in-Aid for Scientific Research (No. 08640309) from MEXT Japan.\n\n**1. Introduction:** Halo nuclei are defined as loosely bound structures whose wave functions extend several hundred femtometers beyond the atomic surface. Initially identified through shallow resonances in elastic scattering experiments, these nuclei can now be produced directly in fragmentation reactions. A wealth of experimental investigations has been conducted on various aspects of halo nuclei, including electromagnetic transitions and breakup processes. To understand the theoretical underpinnings of halo nuclei, it is essential to analyze their wave behavior both inside and outside the nucleus. This study employs the Schrödinger equation with simple nuclear potentials to achieve this goal. The methodology builds upon previous work, expanding single-particle wave functions in terms of harmonic oscillator basis states, and numerically testing the resulting matrix elements using Gaussian quadratures. For the nuclear potential, both the Volkov and Paris potentials are utilized, with the former providing a more accurate representation of ground state energies for light nuclei up to A = 10, while the latter effectively reproduces the binding energy of 4He.\n\n**2. Results and Discussion:** The analysis begins with the case of 11Li, where the adjusted density distribution and the corresponding root mean square radius (Rrms) are presented. This foundational work lays the groundwork for further exploration of the unique properties of halo nuclei and their implications in nuclear physics.",
        "ori-fast-z-score": -0.7373087284671365,
        "water-fast-z-score": 6.9755211086227895,
        "rewrite-fast-z-score": 1.0932163332202425
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Metal and molecule cooling in simulations of structure formation .\nAbstract:\nWe present results on metal and molecular gas cooling in cosmological hydrodynamic simulations with the Enzo code, focusing on the effects of different numerical schemes for solving the energy equation (SPH vs. grid-based) as well as varying physical prescriptions for star formation feedback. We find that SPH codes tend to overestimate the amount of cold gas at high redshifts compared to grid-based methods due to artificial viscosity heating. Feedback models which include galactic winds are able to suppress this effect by removing low entropy material from galaxies. However, we also show that these wind models can lead to an underestimation of the total mass fraction of cool gas if they remove too much hot halo gas around massive halos. Overall our results suggest that current state-of-the-art galaxy formation models produce reasonable estimates for the global properties of the warm-hot intergalactic medium but may still be missing important physics related to the detailed distribution of metals within individual galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Metal and molecule cooling in simulations of structure formation . Abstract : We report findings on metal and molecular gas cooling in cosmological hydrodynamic simulations with the Enzo code , concentrating on the effects of different numerical schemes for solving the electricity equation ( SPH vs . grid - based ) as well as varying mechanical prescriptions for star formation feedback .We see that SPH codes tend to overestimate the quantity of cold gas at high redshifts compared to grid - based methods thanks to artificial viscosity warming . Feedback models which use galactic winds are able to suppress this effect by removing lowered entropy debris from stars .However , we also demonstrate that these wind estimates can lead to an underestimation of the total mass fraction of cool gas if they eliminate too much warm halo gas around large halos . Overall our findings confirm that current state - of - the - art star formation models generate reasonable estimates for the global properties of the cool - warm intergalactic medium but might nevertheless be missing crucial physics related to the detailed distribution of metals within individual galaxies .",
        "rewrite_text": "In this study, we present our findings on the cooling processes of metal and molecular gas within the context of cosmological hydrodynamic simulations conducted using the Enzo code. Our focus is on the impact of various numerical techniques employed to solve the electricity equation, specifically comparing Smoothed Particle Hydrodynamics (SPH) with grid-based methods. Additionally, we explore the implications of different mechanical models for star formation feedback on these cooling processes. Our results indicate that SPH simulations tend to overestimate the amount of cold gas present at high redshifts, a discrepancy attributed to the effects of artificial viscosity, which can introduce unwanted heating. We find that feedback mechanisms incorporating galactic winds can mitigate this overestimation by effectively removing low-entropy material produced by stellar processes. However, we also highlight a potential drawback of these wind models: they may inadvertently lead to an underrepresentation of the total mass fraction of cool gas if they excessively deplete warm halo gas surrounding massive halos. Overall, our research confirms that contemporary star formation models are capable of providing reasonable estimates for the overall characteristics of the cool-warm intergalactic medium. Nonetheless, we caution that these models may overlook critical physical processes that govern the intricate distribution of metals within individual galaxies, suggesting a need for further refinement in our understanding of these phenomena.",
        "ori-fast-z-score": -0.8867963503478639,
        "water-fast-z-score": 4.631047607372178,
        "rewrite-fast-z-score": 0.7559289460184544
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal Routing for Decode-and-Forward based Cooperation in Wireless Networks .\nAbstract:\nIn this work, we consider decode-and forward (DF) cooperation among nodes in wireless networks and propose an optimal routing algorithm to maximize the network throughput by jointly optimizing source-to-destination routes as well as cooperative relay paths. We formulate the problem into mixed integer linear programming (MILP), which is NP-hard due to its combinatorial nature. To solve it efficiently, we develop two heuristic algorithms with polynomial time complexity. The first one uses a greedy approach that iteratively selects the best route between each pair of source destination pairs until all flows are routed. In order to further improve the performance, we also design another algorithm using simulated annealing technique. Extensive simulation results show that our proposed algorithms can achieve significant improvement over existing schemes. Index Terms- Cooperative communication, Optimum routing, Simulated annealing, Greedy algorithm. I. INTRODUCTIO N Cooperative communications have been widely studied recently because they provide higher data rates or longer transmission ranges than conventional non-cooperative systems  1  . In particular, decodeand-forward (DF) has attracted much attention since it does not require any extra power consumption at relays  2  .\nHowever, DF-based cooperative transmissions suffer from high end-to-end delay caused by multiple hops involved in forwarding packets  3  , especially when there exist many sources and destinations  4  . Therefore, how to find efficient cooperative relay paths becomes critical to reduce the overall end-to-end delay while maintaining good system performance  5  -  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal Routing for Decode - and - Forward based Cooperation in Wireless Networks . Abstract : In this research , we investigate decode - and forward ( DF ) cooperation among nodes in wireless networks and suggest an appropriate routing algorithm to maximize the channel throughput by jointly optimizing source - to - destination lanes as well as cooperative relay routes .We formulate the question into mixed integer linear programming ( MILP ) , which is NP - hard thanks to its combinatorial nature . To solve it easily , we develop two heuristic algorithms with polynomial time complexity .The first one uses a greedy approach that iteratively selects the best path between each couple of source destination pairs until all flows are diverted . In try to further enhance the performance , we also design another optimization use simulated annealing technique .Extensive model results show that our proposed methods can attain valuable improvement over existing plans . Index Terms - Cooperative communication , Optimum routing , Simulated annealing , Greedy method .I . INTRODUCTIO N Cooperative networks have been widely examined lately because they deliver higher data levels or faster transmission ranges than conventional un - cooperative systems 1 . In particular , decodeand - forward ( DF ) has garnered great popularity since it does not require any additional power consumption at relays 2 .However , DF - based cooperative signals suffer from high end - to - end delay resulting by many hops engaged in forwarding packets 3 , particularly when there reside many sources and destinations 4 . Therefore , how to find effective cooperative relay routes seems necessary to reduce the overall end - to - end delay while maintaining good system performance 5 - 8 .",
        "rewrite_text": "In this study, we explore the concept of decode-and-forward (DF) cooperation among nodes within wireless networks, proposing a novel routing algorithm aimed at maximizing channel throughput. Our approach involves the joint optimization of both source-to-destination paths and cooperative relay routes. We frame this optimization problem as a mixed integer linear programming (MILP) challenge, which is known to be NP-hard due to its combinatorial characteristics. To facilitate a more manageable solution, we introduce two heuristic algorithms that operate with polynomial time complexity. The first algorithm employs a greedy strategy, which iteratively identifies and selects the optimal path for each pair of source and destination until all data flows are successfully rerouted. To further enhance performance, we also develop a second optimization method based on simulated annealing techniques. Comprehensive simulation results demonstrate that our proposed algorithms significantly outperform existing routing strategies, leading to notable improvements in network efficiency. \n\nCooperative networks have gained considerable attention in recent years, primarily due to their ability to achieve higher data rates and extended transmission ranges compared to traditional non-cooperative systems. The DF method, in particular, has become increasingly popular as it does not necessitate additional power consumption at relay nodes. However, DF-based cooperative communication can experience substantial end-to-end delays, primarily caused by the multiple hops required for packet forwarding, especially in scenarios with numerous sources and destinations. Consequently, identifying effective cooperative relay routes is essential to minimize overall end-to-end delay while ensuring robust system performance. Our research addresses this critical need, contributing valuable insights into the optimization of routing in cooperative wireless networks. \n\nKeywords: Cooperative communication, Optimal routing, Simulated annealing, Greedy method.",
        "ori-fast-z-score": -1.110664955031789,
        "water-fast-z-score": 6.700692544471771,
        "rewrite-fast-z-score": -0.31234752377721214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF .\nAbstract:\nIn this paper, we propose an autonomous distributed admission control scheme to improve the performance and fairness in wireless local area networks (WLANs). The proposed scheme is based on the concept that each station maintains its own queue length information by using the packet inter-arrival time at the physical layer. In addition, it uses the number of active stations as well as their transmission rates to determine whether or not new connections are admitted into the network. We show through simulation results that our scheme can achieve better throughput than existing schemes while maintaining good fairness among competing stations. Keywords: Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement. 1 Introduction With the rapid development of mobile computing devices such as laptops, PDAs, smart phones etc., there has been growing interest in providing high quality services over wireless local area networks (WLANS)  1  . However, due to limited bandwidth resources available in WLANs, efficient resource management becomes crucially important  2  .\nThe most widely used medium access control protocol in current commercial WLAN products is the IEEE 802.11 Distributed Coordination Function (DCF), which provides both contention-based channel access mechanism called Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA)  3  , and contention-free service via Point Coordinated Function (PCF)  4  . Although CSMA/CA allows multiple stations to share the same radio channel simultaneously without any centralized coordination, it suffers from poor system performance when the traffic load increases  5  . This problem is mainly caused by the hidden terminal effect  6  where two nodes may transmit packets to one another simultaneously causing collisions. To alleviate these problems, several approaches have been proposed  7 -10  . Among them, the authors in  8  introduced a simple but effective method known as Virtual Reservation Channel (VRC) to reduce the probability of collision between data frames transmitted by different stations. They also presented a modified version of VRC  9  to further enhance the performance of CSMA/CA under heavy loads. However, all these works assume that the number of active stations within the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An Autonomous Distributed Admission Control Scheme for IEEE 802 . 11 DCF . Abstract : In this paper , we propose an autonomous distributed entrance control scheme to promote the performance and fairness in telecommunications local region systems ( WLANs ) .The proposed system is based on the idea that each signal maintains its own queue length information by using the packet inter - arrival rate at the physical layer . In addition , it utilizes the number of active stations as well as their transmission rates to judge whether or not fresh connections are admitted into the channel .We see through simulation data that our scheme can attain better throughput than existing plans while maintaining good fairness among competing networks . Keywords : Wireless Local Area Networks , Packet Inter - Arrival Time , Fairness , Throughput Improvement .1 Introduction With the fast development of mobile computing devices such as laptops , PDAs , smart phones etc . , there has been growing interest in providing good quality services over wireless regional area networks ( WLANS ) 1 . However , owing to limited bandwidth assets available in WLANs , efficient resource control makes crucially essential 2 .The most commonly used medium access control protocol in current consumer WLAN offerings is the IEEE 802 . 11 Distributed Coordination Function ( DCF ) , which offers both contention - based network entry method called Carrier Sense Multiple Access with Collision Avoidance ( CSMA / CA ) 3 , and contention - free service via Point Coordinated Function ( PCF ) 4 . Although CSMA / CA enables many affiliates to exchange the same radio channel simultaneously without any centralized coordination , it suffers from poor program performance when the traffic burden rises 5 .This problem is mainly caused by the concealed terminal effect 6 where two nodes may transmit packets to one another simultaneously creating collisions . To ease these problems , various approaches have been proposed 7 - 10 .Among them , the authors in 8 invented a simple but effective technology called as Virtual Reservation Channel ( VRC ) to reduce the probability of collision between signal frames transmitted by various stations . They also presented a altered version of VRC 9 to further enhance the performance of CSMA / CA under heavy loads .However , all these works assume that the quantity of active stations within the",
        "rewrite_text": "**Title:** An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF\n\n**Abstract:** This paper introduces an innovative autonomous distributed admission control scheme aimed at enhancing performance and fairness in Wireless Local Area Networks (WLANs). The proposed approach leverages the concept that each signal independently tracks its queue length by analyzing the packet inter-arrival rate at the physical layer. Additionally, it assesses the number of active stations and their respective transmission rates to determine the feasibility of admitting new connections to the channel. Through extensive simulations, we demonstrate that our scheme significantly improves throughput compared to existing methodologies while ensuring equitable access among competing networks. \n\nThe rapid proliferation of mobile computing devices, including laptops, PDAs, and smartphones, has intensified the demand for high-quality services in WLANs. However, the limited bandwidth resources available in these networks necessitate efficient resource management. The IEEE 802.11 Distributed Coordination Function (DCF) is the predominant medium access control protocol utilized in contemporary consumer WLANs. It employs a contention-based access method known as Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) and offers a contention-free service option via the Point Coordinated Function (PCF). Despite the advantages of CSMA/CA, such as enabling multiple users to share the same radio channel without centralized coordination, it faces significant performance degradation under high traffic conditions, primarily due to the hidden terminal problem, which leads to packet collisions.\n\nTo address these challenges, various strategies have been proposed, including the Virtual Reservation Channel (VRC) technique, which aims to minimize collision probabilities among frames transmitted by different stations. While previous studies have made strides in enhancing CSMA/CA performance, they often operate under the assumption of a fixed number of active stations. Our research, however, provides a more dynamic solution that adapts to varying network conditions, thereby improving overall network efficiency and fairness. \n\n**Keywords:** Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement.",
        "ori-fast-z-score": -1.2271439821557926,
        "water-fast-z-score": 8.129828881782126,
        "rewrite-fast-z-score": 0.5696519211398116
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exponential decay of correlation for the Stochastic Process associated to the Entropy Penalized Method .\nAbstract:\nWe consider an entropy penalization method in order to estimate the density function f of a random variable X with values on Rd . We prove that, under some conditions, this estimator is strongly consistent and we study its asymptotic normality. The main tool used here is the exponential decay of correlations property satisfied by the stochastic process associated to our estimation procedure. This result allows us to obtain rates of convergence for the mean integrated squared error (MISE) between the true density f and its estimators. Finally, numerical experiments are performed in dimension 1 and 2. Keywords: Density estimation, Entropic penalty, Exponential decay of correlations, Asymptotic normality. Mathematics Subject Classification (2010): 60C05, 60F10, 62G20. 1 Introduction Let X be a real-valued random vector defined on a probability space (Ω , A , P). In many applications such as signal processing or econometrics, it may be interesting to recover the distribution law of X denoted by fX . For example, if one wants to detect changes in the statistical properties of X over time, then knowing fX will allow him/her to perform change-point detection tests  see e.g., Chen et al. (2013), Fryzlewicz & Subba Rao (2014) . However, recovering fX can be difficult because only n iid observations X1 , . . . , Xn of X are available. To overcome this difficulty, several authors have proposed to use nonparametric methods based on kernel smoothing techniques  see e.g. , Silverman (1981) , Wand & Jones (1995)  . More precisely, let K : R →  0, 1  be a given kernel function satisfying certain regularity assumptions which will be specified later. Then, the classical kernel density estimator of fX at x ∈ Rd is defined bŷ fbK (x) =",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exponential decay of correlation for the Stochastic Process associated to the Entropy Penalized Method . Abstract : We consider an entropy penalization procedure in order to estimate the density function f of a random variable X with values on Rd .We prove that , under some conditions , this estimator is strongly consistent and we study its asymptotic normality . The main technique applied here is the exponential decay of correlations property satisfied by the stochastic mechanism associated to our estimation method .This result allows us to obtain rates of convergence for the mean integrated squared error ( MISE ) between the true density f and its estimators . Finally , numerical studies are performed in dimension 1 and 2 .Keywords : Density estimation , Entropic penalty , Exponential decay of correlations , Asymptotic normality . Mathematics Subject Classification ( 2010 ) : 60C05 , 60F10 , 62G20 .1 Introduction Let X be a real - valued random matrix established on a probability space ( Ω , A , P ) . In many applications such as signal filtering or econometrics , it could be amusing to regain the distribution law of X denoted by fX .For instance , if one wants to identify changes in the empirical properties of X over time , then understanding fX will provide him / her to conduct change - point detection tests see e . g . , Chen et al . ( 2013 ) , Fryzlewicz & Subba Rao ( 2014 ) .However , obtaining fX can be problematic because only n iid observations X1 , . .., Xn of X are available.To solve this challenge , various papers have proposed to use nonparametric techniques based on kernel smoothing methods see e . g . , Silverman ( 1981 ) , Wand & Jones ( 1995 ) .More specifically , let K : R → 0 , 1 be a given kernel map satisfying particular regularity assumptions which will be specified later . Then , the classical kernel density estimator of fX at x ∈ Rd is given bŷ fbK ( x ) =",
        "rewrite_text": "**Title:** Exponential Decay of Correlation for the Stochastic Process Associated with the Entropy Penalized Method\n\n**Abstract:** In this study, we investigate an entropy penalization approach aimed at estimating the density function \\( f \\) of a random variable \\( X \\) that takes values in \\( \\mathbb{R}^d \\). We establish that, under certain conditions, this estimator exhibits strong consistency, and we analyze its asymptotic normality. A key aspect of our methodology is the property of exponential decay of correlations inherent in the stochastic process linked to our estimation technique. This property is instrumental in deriving convergence rates for the mean integrated squared error (MISE) between the true density \\( f \\) and its corresponding estimators. To validate our theoretical findings, we conduct numerical experiments in both one-dimensional and two-dimensional settings. \n\nThe significance of accurately estimating the density function \\( f_X \\) of a real-valued random variable \\( X \\) is underscored by its applications in various fields, including signal processing and econometrics. For instance, understanding the distribution of \\( X \\) is crucial for implementing change-point detection tests, as highlighted in previous works by Chen et al. (2013) and Fryzlewicz & Subba Rao (2014). However, the challenge arises from the fact that only \\( n \\) independent and identically distributed (iid) observations \\( X_1, \\ldots, X_n \\) are available for analysis. To address this issue, numerous studies have proposed nonparametric techniques that leverage kernel smoothing methods, as discussed in foundational texts by Silverman (1981) and Wand & Jones (1995). Specifically, we define a kernel function \\( K: \\mathbb{R} \\to [0, 1] \\) that adheres to specific regularity conditions, which will be elaborated upon in the subsequent sections. The classical kernel density estimator for \\( f_X \\) at a point \\( x \\in \\mathbb{R}^d \\) is then formulated, paving the way for further exploration of its properties and performance.\n\n**Keywords:** Density estimation, Entropic penalty, Exponential decay of correlations, Asymptotic normality.  \n**Mathematics Subject Classification (2010):** 60C05, 60F10, 62G20.",
        "ori-fast-z-score": -0.6260990336999411,
        "water-fast-z-score": 5.388159060803248,
        "rewrite-fast-z-score": 1.756550621379892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectropolarimetric observations of the Ca II 8498 A and 8542 A lines in the quiet Sun .\nAbstract:\nWe present spectropolarimetric observations made with the Solar Optical Telescope (SOT) on board Hinode, which show that the magnetic field strength inferred from Stokes V profiles is systematically higher than those obtained by using the Zeeman splitting method for both the Ca II 8498 Å line and the Ca II 8542 Å line. The difference between these two methods increases as we go to smaller spatial scales. We also find that the magnetic fields are more inclined towards the solar surface at small spatial scales compared to larger ones. These results suggest that there may be some unknown physical processes affecting the formation of Stokes V profiles at small spatial scales. This work was supported by JSPS KAKENHI Grant-in-Aid for Scientific Research No. 16340040 . \nIntroduction\n\nThe solar atmosphere consists of various structures such as sunspots, pores, plages, prominences etc., where different physical phenomena occur. In order to understand how these phenomena take place, it is important to study their properties individually. However, this task has been difficult because most of them have very fine structure and they often overlap each other spatially. To overcome this difficulty, many observational studies have been carried out recently using high-resolution instruments such as the Swedish 1-m Solar Telescope (SST), the New Solar Telescope (NST), the Advanced Technology Solar Telescope (ATST), and the Solar Dynamics Observatory (SDO). Among others, the Hinode satellite launched in 2006 provides us with unprecedentedly high-quality data thanks to its sophisticated instrumentation including the Spectro-Polarimeter (SP) (Lites et al. (2001) ) and the Helioseismic and Magnetic Imager (HMI) (Schou et al. (2010) ), which enable us to investigate the solar photosphere down to subarcsecond resolution. Using these data sets, several authors studied the photospheric magnetic fields (e.g., Ichimoto et al. (2007) , Ishikawa & Tsuneta (2008) , Kitai et al. (2009 ), Orozco Suárez et al. (2010 , Sheminova et al. (2011))",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectropolarimetric discoveries of the Ca II 8498 A and 8542 A lines in the quiet Sun . Abstract : We report spectropolarimetric studies made with the Solar Optical Telescope ( SOT ) on board Hinode , which show that the magnetic force force inferred from Stokes V profiles is systematically greater than those achieved by using the Zeeman splitting method for both the Ca II 8498 Å line and the Ca II 8542 Å line .The difference between these two models increases as we went to smaller spatial scales . We additionally find that the magnetic fields are more oriented towards the sun surface at small spatial scales compared to larger ones .These data suggest that there may be some unidentified physical processes controlling the formation of Stokes V profiles at small spatial scales . This research was supported by JSPS KAKENHI Grant - in - Aid for Scientific Research No .16340040 . Introduction The planetary atmosphere includes of several systems such as sunspots , pores , plages , prominences etc . , where various physical phenomena arise .In order to comprehend how these phenomena play place , it is important to study their characteristics individually . However , this job has been difficult because most of them have very fine structure and they frequently overlap each other spatially .To solve this obstacle , many observational research have been carried out recently utilizing large - resolution equipment such as the Swedish 1 - m Solar Telescope ( SST ) , the New Solar Telescope ( NST ) , the Advanced Technology Solar Telescope ( ATST ) , and the Solar Dynamics Observatory ( SDO ) . Among others , the Hinode satellite launched in 2006 offers us with unprecedentedly high - grade results courtesy to its sophisticated instrumentation including the Spectro - Polarimeter ( SP ) ( Lites et al .( 2001 ) ) and the Helioseismic and Magnetic Imager ( HMI ) ( Schou et al . ( 2010 ) ) , which enable us to examine the solar photosphere down to subarcsecond resolution .Using these information sets , various scientists examined the photospheric magnetic waves ( e . g . , Ichimoto et al . ( 2007 ) , Ishikawa & Tsuneta ( 2008 ) , Kitai et al .( 2009 ) , Orozco Suárez et al . ( 2010 , Sheminova et al .(2011))",
        "rewrite_text": "**Title:** Spectropolarimetric Discoveries of the Ca II 8498 Å and 8542 Å Lines in the Quiet Sun\n\n**Abstract:** This study presents the findings from spectropolarimetric observations conducted with the Solar Optical Telescope (SOT) aboard the Hinode satellite, focusing on the Ca II 8498 Å and Ca II 8542 Å spectral lines in the quiet Sun. Our analysis reveals that the magnetic field strengths inferred from Stokes V profiles are consistently higher than those derived using the traditional Zeeman splitting technique. Notably, this discrepancy becomes more pronounced at smaller spatial scales, indicating a potential scale-dependent behavior of the magnetic fields. Furthermore, our results indicate that at these smaller scales, the magnetic fields exhibit a greater inclination towards the solar surface compared to larger scales. These observations imply the existence of unidentified physical processes that may influence the formation of Stokes V profiles in regions of fine structure. This research was supported by the Japan Society for the Promotion of Science (JSPS) KAKENHI Grant-in-Aid for Scientific Research No. 16340040.\n\n**Introduction:** The solar atmosphere is composed of various structures, including sunspots, pores, plages, and prominences, each exhibiting distinct physical phenomena. Understanding these phenomena requires a detailed examination of their individual characteristics. However, this task is complicated by the fine structures of these features, which often overlap spatially. To address this challenge, recent observational studies have employed high-resolution instruments such as the Swedish 1-m Solar Telescope (SST), the New Solar Telescope (NST), the Advanced Technology Solar Telescope (ATST), and the Solar Dynamics Observatory (SDO). Among these, the Hinode satellite, launched in 2006, has provided exceptional data quality due to its advanced instrumentation, including the Spectro-Polarimeter (SP) and the Helioseismic and Magnetic Imager (HMI). These tools allow for detailed investigations of the solar photosphere at subarcsecond resolution. Utilizing these datasets, numerous researchers have explored the dynamics of photospheric magnetic waves, contributing to our understanding of solar magnetic activity.",
        "ori-fast-z-score": -1.660037707655972,
        "water-fast-z-score": 6.203298802293369,
        "rewrite-fast-z-score": 0.8770580193070293
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Reply to comment on  Essence of intrinsic tunnelling: Distinguishing intrinsic features from artefacts .\nAbstract:\nWe reply to the comments by A. M. Braden and J. P. Dowling in their Comment  1  . We show that our results are robust against different choices for the fitting range, and we discuss how this is related to the choice of the initial state wave function used in Ref.  2  .\nPACS numbers: 11.10.Wx, 12.20.Ds, 13.25.Gv Quantum tunneling has been studied extensively since its discovery more than half century ago  3  , but it remains an active area of research  4  . In particular, recent experiments have shown that quantum tunneling can be observed even at room temperature  5  . Theoretically, there exist two types of tunneling processes  6  : extrinsic tunneling which occurs when particles move through barriers created by external potentials  7, 8  ; and intrinsic tunneling where particles tunnel between degenerate states without any potential barrier  9  . Intrinsic tunneling plays important roles in many physical systems such as molecular vibrations  10  , nuclear fission  11  , Josephson junctions  12  , Bose-Einstein condensates  13  , and semiconductor superlattices  14  . However, distinguishing intrinsic tunneling from other effects experimentally still poses great challenges  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Reply to comment on Essence of intrinsic tunnelling : Distinguishing intrinsic features from artefacts . Abstract : We respond to the remarks by A . M . Braden and J . P . Dowling in their Comment 1 .We see that our findings are robust against different decisions for the fitting range , and we explain how this is related to the selection of the first state wave system employed in Ref . 2 .PACS scores : 11 . 10 . Wx , 12 . 20 . Ds , 13 . 25 . Gv Quantum tunneling has been studied extensively since its observation more than quarter century ago 3 , but it remains an active area of research 4 . In particular , recent experiments have shown that molecular tunneling can be viewed even at room temperature 5 .Theoretically , there exist two forms of tunneling systems 6 : extrinsic tunneling which occurs when molecules push through barriers created by external potentials 7 , 8 ; and intrinsic tunneling where ions tunnel between degenerate states without any potential barrier 9 . Intrinsic tunneling serves key roles in different physical structures such as chemical vibrations 10 , nuclear fission 11 , Josephson junctions 12 , Bose - Einstein condensates 13 , and semiconductor superlattices 14 .However , distinguishing intrinsic tunneling from other effects experimentally nevertheless poses tremendous challenges 15 .",
        "rewrite_text": "Title: Response to Commentary on the Essence of Intrinsic Tunneling: Differentiating Intrinsic Characteristics from Artifacts\n\nAbstract: In this article, we address the observations made by A. M. Braden and J. P. Dowling in their recent commentary. Our analysis demonstrates that our results maintain their integrity across various fitting range choices, and we elucidate the connection between these findings and the selection of the initial state wave system utilized in our previous work. Quantum tunneling has been a focal point of scientific inquiry since its initial observation over twenty-five years ago, and it continues to be a vibrant field of research. Notably, contemporary experiments have revealed that molecular tunneling can be observed even at room temperature, highlighting its significance in various physical contexts. Theoretical frameworks categorize tunneling into two distinct types: extrinsic tunneling, which occurs when molecules traverse barriers imposed by external potentials, and intrinsic tunneling, where ions transition between degenerate states without encountering any potential barriers. Intrinsic tunneling plays a crucial role in numerous physical phenomena, including chemical vibrations, nuclear fission, Josephson junctions, Bose-Einstein condensates, and semiconductor superlattices. Despite its importance, the experimental differentiation of intrinsic tunneling from other effects remains a formidable challenge. Our response aims to clarify these complexities and reinforce the robustness of our findings in the context of ongoing discussions in the field. The implications of our work extend across various domains of physics, as indicated by the PACS scores: 11.10.Wx, 12.20.Ds, and 13.25.Gv, underscoring the relevance of intrinsic tunneling in advancing our understanding of quantum phenomena.",
        "ori-fast-z-score": -0.6793662204867574,
        "water-fast-z-score": 5.590169943749474,
        "rewrite-fast-z-score": 0.7844645405527362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Near-infrared Study of the Carina Nebula .\nAbstract:\nWe present near-infrared (NIR) observations of the central region of the Carina Nebulae obtained with ISAAC at the VLT in Chile, and compare them to previous optical studies by Smith et al. (2000) . The NIR data reveal new details on the structure of the nebular shell surrounding the open cluster Trumpler 16. We find that the brightest part of the shell is located between two dark lanes which are probably caused by dust extinction. In addition we detect several small knots embedded within the shell. These knots may be remnants of massive stars or protostars formed during an earlier phase of star formation activity in this region. Finally, we identify a number of Herbig-Haro objects associated with the open clusters Trumpler 14 and 15. Our results show that the Carina Nebulae is still actively forming stars today. This work was supported by the European Southern Observatory under programme ID 072.C-0488(B).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Near - infrared Study of the Carina Nebula . Abstract : We report near - infrared ( NIR ) observations of the central region of the Carina Nebulae obtained with ISAAC at the VLT in Chile , and link them to previous optical studies by Smith et al .( 2000 ) . The NIR data reveal novel features on the structure of the nebular shell surrounding the open cluster Trumpler 16 .We see that the brightest part of the shell is situated between two darkened paths which are probably created by dust extinction . In addition we locate many small knots embedded within the shell .These knots might be remnants of large galaxies or protostars produced during an previous process of star formation activity in this area . Finally , we identify a number of Herbig - Haro objects identified with the open regions Trumpler 14 and 15 .Our results show that the Carina Nebulae is already actively creating stars today . This project was supported by the European Southern Observatory under programme ID 072 . C - 0488 ( B ) .",
        "rewrite_text": "Title: Near-Infrared Investigation of the Carina Nebula\n\nAbstract: This study presents near-infrared (NIR) observations of the central region of the Carina Nebula, conducted using the Infrared Spectrometer And Array Camera (ISAAC) at the Very Large Telescope (VLT) in Chile. We correlate our findings with earlier optical studies by Smith et al. (2000) to enhance the understanding of this dynamic region. The NIR data unveil previously unobserved structural features of the nebular shell that encircles the open cluster Trumpler 16. Notably, we observe that the most luminous section of the shell is positioned between two dark lanes, likely resulting from dust extinction effects. Furthermore, our observations reveal numerous small knots embedded within the shell, which may represent remnants of ancient galaxies or protostars formed during earlier star formation events in this region. Additionally, we identify several Herbig-Haro objects associated with the more open clusters Trumpler 14 and 15. These findings suggest that the Carina Nebula is currently an active site of star formation. This research was conducted with the support of the European Southern Observatory under program ID 072.C-0488(B).",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": 0.5933908290969266
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Planetary embryos and planetesimals residing in thin debris disks .\nAbstract:\nWe present the results of our numerical simulations on planetary formation in debris disks with different masses, compositions, and structures. We find that planets can form efficiently even if they are surrounded by massive dusty disks (up to 10 times more mass than the minimum-mass solar nebula). The disk structure is found to have little effect on planet formation as long as it has enough surface density at large radii for giant impacts to occur. Planets formed in such disks tend to be less massive but larger compared to those formed in standard protoplanetary disks. This may explain why some extrasolar planets appear to be too small while others seem to be overly massive. Our results also suggest that there should exist many terrestrial planets around Sun-like stars which are not detected yet because their orbits are highly inclined or eccentric. \n \n Keywords: Planet formation; Debris disk; Giant impacts; Embryo accretion",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Planetary embryos and planetesimals residing in narrow debris disks . Abstract : We present the results of our numerical simulations on planetary formation in debris disks with various masses , compositions , and structures .We see that planets can form smoothly even if they are surrounded by massive dusty disks ( up to 10 twice more mass than the highest - mass solar nebula ) . The disk composition is found to have minimal influence on planet development as long as it has sufficient surface volume at large radii for huge impacts to occur .Planets formed in such disks tend to be less massive but bigger compared to those formed in standard protoplanetary disks . This might explain why some extrasolar stars seems to be too small while several look to be overly huge .Our results also suggest that there should exist many terrestrial worlds around Sun - like stars which are not observed yet because their orbits are extremely inclined or eccentric . Keywords : Planet structure ; Debris disk ; Giant impacts ; Embryo accretion",
        "rewrite_text": "Title: Planetary Embryos and Planetesimals in Narrow Debris Disks\n\nAbstract: In this study, we explore the dynamics of planetary formation within debris disks characterized by varying masses, compositions, and structural configurations through comprehensive numerical simulations. Our findings indicate that planetary bodies can form effectively even in the presence of substantial dusty disks, with masses reaching up to twice that of the most massive solar nebula. Notably, the composition of the disk appears to exert a minimal impact on the developmental processes of planets, provided that there is adequate surface area at larger radii to facilitate significant impact events. The planets that emerge from these environments tend to exhibit a unique profile: they are generally larger in size but possess lower mass compared to those formed in conventional protoplanetary disks. This observation may offer insights into the peculiar characteristics of certain extrasolar stars, where some appear unexpectedly small while others are disproportionately large. Furthermore, our simulations imply the existence of numerous terrestrial planets orbiting Sun-like stars that remain undetected, potentially due to their highly inclined or eccentric orbits. These findings contribute to our understanding of planetary formation mechanisms and the diverse outcomes that can arise from different initial conditions in debris disks. \n\nKeywords: Planet structure; Debris disk; Giant impacts; Embryo accretion.",
        "ori-fast-z-score": -0.9649012813540153,
        "water-fast-z-score": 4.90361647617904,
        "rewrite-fast-z-score": -0.6704783996548059
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic Effects Increasing Network Vulnerability to Cascading Failures .\nAbstract:\nWe study the vulnerability of interdependent networks under cascading failures, where nodes are subject to random attacks and their neighbors can fail as well due to loss of connectivity. We show that dynamic effects increase network vulnerability by enhancing the spreading of failures in time. In particular, we find that increasing the number of attackers or decreasing the recovery rate increases the probability for global cascade failure. Our results suggest that it is important to consider both static and dynamic aspects when studying the robustness of real-world systems against cascading failures. Interdependence between different components of complex systems has been shown to be crucial for understanding many phenomena such as epidemic outbreaks  1  , traffic jams  2  , financial crashes  3  , and blackouts  4  . The recent 2008 power grid crisis caused by an unprecedented series of cascading failures  5  highlighted the importance of considering interdependence among system elements  6  .\nIn this work, we focus on interdependent networks  7, 8  , which consist of two types of nodes: source (S) and target (T). Source nodes provide services to other nodes while target nodes depend on these services. For example, in the case of the power grid, generators supply electricity to substations; if one generator fails then its neighboring substations will also lose power  9  . Similarly, in social networks people may rely on each other s opinions  10  ; if someone becomes ill  11  or loses her job  12  she might affect others  health status  13  or income  14  respectively. Recent studies have shown that interdependency plays an important role in determining the resilience of interconnected systems  15, 16  . However, most previous works focused only on static properties  17  , i.e., they assumed that all links remain stable over time  18  . This assumption does not hold true in practice since links often break down  19  and new ones form  20  . Therefore, it is necessary to take into account the dynamics of interactions  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic Effects Increasing Network Vulnerability to Cascading Failures . Abstract : We research the vulnerability of interdependent networks under cascading crashes , where nodes are subject to random attacks and their relatives can fail as well due to lack of communication .We see that dynamic effects improve network vulnerability by increased the spreading of failures in time . In particular , we find that increasing the number of attackers or decreasing the recovery rate raises the probability for global cascade failure .Our results show that it is important to consider both static and dynamic elements when examining the robustness of real - global networks against cascading crashes . Interdependence between various components of complex systems has been shown to be crucial for studying many phenomena such as plague outbreaks 1 , road jams 2 , financial crashes 3 , and blackouts 4 .The recent 2008 power system disaster caused by an unprecedented series of cascading problems 5 indicated the importance of considering interdependence among system components 6 . In this project , we focus on interdependent networks 7 , 8 , which consist of two kind of nodes : source ( S ) and goal ( T ) .Source networks provide operations to other nodes while target nodes depend on these services . For instance , in the case of the power system , generators supply energy to substations ; if one generator fails then its adjacent substations will also lose power 9 .Similarly , in social organizations everyone would rely on each other s views 10 ; if someone causes illness 11 or losing her employment 12 she might impact people health status 13 or income 14 respectively . Recent research have shown that interdependency takes an important role in measuring the resilience of interconnected networks 15 , 16 .However , most prior papers focused only on static properties 17 , i . e . , they expected that all links remain stable over time 18 . This assumption does not stand true in practice since links sometimes break down 19 and new ones create 20 .Therefore , it is required to take into consideration the dynamics of relationships 21 .",
        "rewrite_text": "**Title:** Dynamic Effects Increasing Network Vulnerability to Cascading Failures\n\n**Abstract:** This study investigates the susceptibility of interdependent networks to cascading failures, particularly in scenarios where nodes are vulnerable to random attacks and their interconnected counterparts may also fail due to disrupted communication. Our findings reveal that dynamic factors significantly enhance network vulnerability by facilitating the temporal spread of failures. Specifically, we observe that an increase in the number of attackers or a decrease in the recovery rate correlates with a heightened likelihood of global cascade failures. This underscores the necessity of incorporating both static and dynamic aspects when assessing the resilience of global networks against cascading disruptions. The interdependence among various components of complex systems is critical for understanding a range of phenomena, including epidemic outbreaks, traffic congestion, financial crises, and power outages. The catastrophic power system failure in 2008, which stemmed from an unprecedented series of cascading issues, highlighted the importance of recognizing interdependencies among system components. In our research, we concentrate on interdependent networks characterized by two types of nodes: source nodes (S) that provide essential services and target nodes (T) that rely on these services. For example, in a power grid, generators serve as source nodes supplying energy to substations; if a generator fails, the adjacent substations will also experience power loss. Similarly, in social networks, individuals depend on each other’s perspectives; if one person suffers from illness or job loss, it can adversely affect the health and income of others within the network. Recent studies have emphasized the pivotal role of interdependency in evaluating the resilience of interconnected systems. However, most existing literature has primarily focused on static properties, assuming that all connections remain stable over time. This assumption is often unrealistic, as links can fail and new connections can emerge. Therefore, it is essential to account for the dynamic nature of relationships within these networks.",
        "ori-fast-z-score": -0.15713484026367722,
        "water-fast-z-score": 9.108437646689818,
        "rewrite-fast-z-score": 1.0579249964025073
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Global polarization of QGP in non-central heavy ion collisions at high energies .\nAbstract:\nWe study the global polarization of quark gluon plasma (QGP) produced in non central heavy-ion collisions by using an effective chiral model with vector and axial-vector mesons as well as quarks and anti-quarks. We find that the global polarization is mainly determined by the initial angular momentum carried by the colliding nuclei, which can be estimated through the Glauber model. The magnitude of the global polarization decreases rapidly when the collision energy increases due to the increasing number of particles involved in the reaction. Our results show that the global polarization may reach about 10% for RHIC energies but it will decrease significantly if one goes up to LHC energies. \n \n Introduction \n \n In recent years there has been growing interest on studying the global polarization of quark-gluon plasma(QGP), especially its dependence on the collision energy  1–3  . It was found that the global polarization could reach about 20% for RHIC energies  4  , while it would drop down to less than 1% for LHC energies  5  . \n \n This phenomenon is closely related to the initial angular momenta carried by the colliding nuclei; therefore, it provides us a new way to probe the nuclear structure  6  . On the other hand, since the global polarization is also sensitive to the temperature evolution  7, 8  , it might provide some information on the thermalization process of QGP  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Global polarization of QGP in non - central heavy ion collisions at high energies . Abstract : We research the global polarization of quark gluon plasma ( QGP ) produced in non central heavy - ion collisions by using an efficient chiral description with vector and axial - vector mesons as well as quarks and anti - quarks .We see that the global polarization is mainly decided by the first angular velocity carried by the colliding nuclei , which can be estimated through the Glauber model . The magnitude of the global polarization decreases quickly when the collision energy rises due to the increasing quantity of molecules implicated in the reaction .Our results show that the global polarization might reach about 10 % for RHIC energies but it will decrease greatly if one goes up to LHC energies . Introduction In recent months there has been growing interest on studying the global polarization of quark - gluon plasma ( QGP ) , particularly its dependence on the collision time 1 – 3 .It was shown that the global polarization could reach about 20 % for RHIC energies 4 , while it would rise down to little than 1 % for LHC energies 5 . This phenomenon is closely related to the early angular momenta carried by the colliding nuclei ; therefore , it gives us a new place to probe the atomic structure 6 .On the other hand , since the global polarization is also sensitive to the temperature evolution 7 , 8 , it could give some info on the thermalization process of QGP 9 .",
        "rewrite_text": "**Title:** Global Polarization of Quark-Gluon Plasma in Non-Central Heavy Ion Collisions at High Energies\n\n**Abstract:** This study investigates the global polarization of quark-gluon plasma (QGP) generated in non-central heavy-ion collisions, employing an effective chiral framework that incorporates vector and axial-vector mesons alongside quarks and anti-quarks. Our findings indicate that the global polarization is predominantly influenced by the initial angular velocity of the colliding nuclei, which can be estimated using the Glauber model. Notably, we observe that as the collision energy increases, the magnitude of global polarization diminishes rapidly due to the larger number of constituents involved in the reaction. Specifically, our results suggest that global polarization can reach approximately 10% at Relativistic Heavy Ion Collider (RHIC) energies; however, this value significantly decreases at Large Hadron Collider (LHC) energies. Recent literature has highlighted an increasing interest in the global polarization of QGP, particularly its dependence on collision time. Previous studies have indicated that global polarization may attain values around 20% at RHIC energies, while it drops to below 1% at LHC energies. This phenomenon is intricately linked to the early angular momentum imparted by the colliding nuclei, offering a novel avenue for exploring the fundamental structure of matter. Furthermore, since global polarization is sensitive to the thermal evolution of the system, it provides valuable insights into the thermalization processes occurring within the QGP. This research contributes to a deeper understanding of the dynamics of QGP and its properties under varying energy conditions, paving the way for future investigations into the behavior of strongly interacting matter in extreme environments.",
        "ori-fast-z-score": -0.3849001794597505,
        "water-fast-z-score": 5.076479311672102,
        "rewrite-fast-z-score": -0.3746343246326776
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Frequency Distribution of Semi-major Axis of Wide Binaries. Cosmogony and Dynamical Evolution .\nAbstract:\nWe have investigated the frequency distribution of semi-major axis (SMA) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical evolution models. We used data obtained by the Two Micron All Sky Survey (2MASS), which is complete down to Ks = 12 mag, corresponding to masses as low as 0.1 M⊙ at distances up to 1 kpc. The sample consists of 13,000 pairs selected using color-color criteria designed to select main-sequence stars. Using Monte Carlo simulations we found that our results are not affected significantly by incompleteness effects due to photometric errors or contamination by background galaxies. Our analysis shows that there exists an excess number of systems with SMA between 10 4 -10 5 AU compared to predictions based on standard cosmological models. This result suggests that either these systems were formed earlier than predicted by current theories or they may be primordial objects such as Population III remnants.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Frequency Distribution of Semi - major Axis of Wide Binaries . Cosmogony and Dynamical Evolution .Abstract : We have analyzed the frequency distribution of semi - major axis ( SMA ) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical development estimates . We utilized information obtained by the Two Micron All Sky Survey ( 2MASS ) , which is complete down to Ks = 12 mag , equivalent to masses as low as 0 . 1 [UNK] at distances up to 1 kpc .The sample consists of 13 , 000 pairs selected using color - color factors created to select primary - sequence stars . Using Monte Carlo simulations we concluded that our findings are not affected substantially by incompleteness effects due to photometric failures or exposure by background galaxies .Our study shows that there exists an excess amount of components with SMA between 10 4 - 10 5 AU compared to expectations based on normal cosmological models . This result suggests that either these systems were created earlier than expected by current theories or they may be primordial objects such as Population III fragments .",
        "rewrite_text": "Title: The Frequency Distribution of Semi-Major Axes in Wide Binaries: Insights into Cosmogony and Dynamical Evolution\n\nAbstract: In this study, we investigate the frequency distribution of semi-major axes (SMA) for wide binary systems with separations exceeding 1000 AU, aiming to evaluate existing cosmogonical models and dynamical evolution theories. Our analysis leverages data from the Two Micron All Sky Survey (2MASS), which provides a comprehensive catalog down to Ks = 12 mag, allowing us to detect stellar masses as low as 0.1 solar masses at distances reaching up to 1 kpc. The dataset comprises 13,000 binary pairs, meticulously selected through color-color diagrams to ensure the inclusion of main-sequence stars. To assess the robustness of our findings against potential biases, we employed Monte Carlo simulations, which indicated that our results remain largely unaffected by incompleteness due to photometric errors or contamination from background galaxies.\n\nOur results reveal a significant excess of binary components with semi-major axes in the range of 10^4 to 10^5 AU, a finding that deviates from predictions made by standard cosmological models. This discrepancy raises intriguing questions regarding the formation mechanisms of these wide binaries. It suggests that these systems may have formed earlier than current theoretical frameworks anticipate, or they could represent primordial objects, potentially akin to fragments from Population III stars. This research not only enhances our understanding of binary star formation but also provides critical insights into the early stages of stellar evolution and the conditions prevalent in the universe's formative years. The implications of our findings could lead to a reevaluation of existing models in astrophysics, particularly concerning the origins and evolution of wide binary systems.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": 0.09090909090909091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Computer simulation of model cohesive powders: influence of assembling procedure and contact laws on low consolidation states .\nAbstract:\nThe aim of this work is to study the effect of different assembly procedures, as well as different contact laws between particles in powder compaction processes using computer simulations. The results show that the packing density depends strongly on the applied pressure during the assembly process. In addition, it was found that the densification behavior can be influenced by changing the contact law parameters. Finally, we have shown how the microstructure evolves with increasing pressure for two different types of particle contacts. Powder compaction has been studied extensively over many years due to its importance in various industrial applications such as pharmaceutical tablets or metal parts produced via hot pressing techniques  1  . However, despite these efforts there are still open questions regarding the underlying mechanisms governing the densification behavior under external loads  2  .\nIn order to understand the complex phenomena occurring inside compacted materials, several experimental studies were performed  3  , which revealed important information about the evolution of the microstructure  4  . Nevertheless, experiments cannot provide detailed insight into the local interactions taking place at the grain scale  5  . Therefore, numerical methods based on discrete element models (DEM)  6  became an alternative approach to investigate the mechanical properties of granular systems  7, 8  . These DEM-based approaches allow one to track individual grains within large assemblies and thus enable us to obtain valuable insights into the micromechanical response  9  . For example, recent investigations showed that the macroscopic stress-strain curve obtained from compression tests can be reproduced accurately if realistic inter-particle contact laws are used  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Computer modeling of model cohesive powders : affect of assembling procedure and contact laws on small consolidation states . Abstract : The goal of this project is to study the impact of different assembly techniques , as also as different contact laws between particles in powder compaction processes using computer simulations .The results show that the packing density depends strongly on the applied pressure during the assembly cycle . In addition , it was shown that the densification response can be governed by varying the contact law factors .Finally , we have shown how the microstructure evolves with varying pressure for two different kinds of particle contacts . Powder compaction has been studied thoroughly over numerous years owing to its significance in different industrial applications such as drug tablets or metal parts produced via hot pressing methods 1 .However , despite these attempts there are still open questions regarding the fundamental pathways governing the densification response under external loads 2 . In order to explain the complex interactions arising inside compacted materials , various experimental studies were performed 3 , which revealed important information about the evolution of the microstructure 4 .Nevertheless , observations cannot offer comprehensive understanding into the local interactions taking place at the grain scale 5 . Therefore , numerical models relying on discrete element models ( DEM ) 6 became an additional method to examine the structural properties of granular structures 7 , 8 .These DEM - based methods provide one to track small particles within large assemblies and therefore facilitate us to obtain crucial insights into the micromechanical behavior 9 . For instance , recent studies showed that the macroscopic stress - strain graph obtained from compression experiments can be reproduced accurately if realistic inter - particle contact laws are using 10 .",
        "rewrite_text": "**Title:** Computer Modeling of Cohesive Powders: The Influence of Assembly Procedures and Contact Laws on Small Consolidation States\n\n**Abstract:** This study aims to investigate the effects of various assembly techniques and particle contact laws on the powder compaction process through computer simulations. Our findings indicate a strong correlation between packing density and the pressure applied during the assembly cycle. Furthermore, we demonstrate that the densification response is significantly influenced by the parameters governing the contact laws between particles. We also explore how the microstructure of the powder evolves under different pressure conditions, specifically comparing two distinct types of particle contacts. \n\nThe field of powder compaction has been extensively researched over the years due to its critical role in various industrial applications, including the production of pharmaceutical tablets and metal components through hot pressing techniques. Despite the wealth of research, fundamental questions remain regarding the mechanisms that dictate the densification behavior of powders under external loads. To address these complexities, numerous experimental studies have been conducted, providing valuable insights into microstructural evolution. However, these observations often fall short of delivering a complete understanding of the localized interactions occurring at the grain scale.\n\nTo bridge this gap, we employ numerical models based on discrete element methods (DEM), which serve as a powerful tool for analyzing the structural characteristics of granular materials. These DEM-based approaches enable the tracking of individual particles within large assemblies, yielding essential insights into their micromechanical behavior. Recent investigations have shown that realistic inter-particle contact laws can accurately replicate the macroscopic stress-strain relationships observed in compression experiments. This research not only enhances our understanding of powder compaction dynamics but also contributes to the development of more effective processing techniques in various industrial applications.",
        "ori-fast-z-score": 0.48038446141526137,
        "water-fast-z-score": 9.328719097496718,
        "rewrite-fast-z-score": 1.6222142113076254
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The nearby QSO host I Zw 1: The stellar disk and adjacent objects .\nAbstract:\nWe present new near-infrared integral field spectroscopy (IFS) data for the brightest galaxy in the cluster Abell 2218, which is known to be interacting with its nearest neighbor, the radio-quiet quasar I Zw 1 at z = 0.0625. We find that this galaxy has an extended low-surface-brightness component surrounding it, extending out to about 10 kpc on both sides along the major axis. This feature shows no evidence of rotation but does show some velocity structure consistent with infalling gas or tidal debris. In addition we detect two compact objects within 5 kpc of the center of the galaxy. One of these appears to have a very high surface brightness and may represent a nuclear starburst; however, the other one displays much lower surface brightness and could possibly be associated with a supermassive black hole binary system. These results are discussed in terms of possible evolutionary scenarios for this interacting pair.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The nearby QSO host I Zw 1 : The stellar disk and adjacent bodies . Abstract : We report new near - infrared integral field spectroscopy ( IFS ) statistics for the brightest galaxy in the cluster Abell 2218 , which is known to be interacting with its closest neighbor , the radio - quiet quasar I Zw 1 at z = 0 . 0625 .We see that this galaxy has an extended low - surface - brightness component covering it , extending out to about 10 kpc on both sides along the main axis . This structure exhibits no evidence of rotation but does display some velocity features compatible with infalling gas or tidal debris .In addition we perceive two compact entities within 5 kpc of the center of the galaxy . One of these seems to have a very high surface brightness and may indicate a nuclear starburst ; however , the other one exhibits far lower surface brightness and could possibly be identified with a supermassive black hole binary system .These data are discussed in terms of possible evolutionary scenarios for this interacting pair .",
        "rewrite_text": "We present new near-infrared integral field spectroscopy (IFS) data for the prominent galaxy located in the Abell 2218 cluster, which is currently interacting with its nearest neighbor, the radio-quiet quasar I Zw 1 at a redshift of z = 0.0625. Our observations reveal that this galaxy possesses an extensive low-surface-brightness halo that extends approximately 10 kpc on either side along its principal axis. Notably, this structure does not exhibit any signs of rotation; however, it does show velocity features that may be indicative of infalling gas or tidal debris resulting from the interaction. Furthermore, we identify two compact structures within a 5 kpc radius of the galaxy's center. One of these structures displays a significantly high surface brightness, suggesting the presence of a nuclear starburst, while the other has a considerably lower surface brightness and may represent a binary system of supermassive black holes. We discuss these findings in the context of potential evolutionary scenarios for the interacting pair, considering the implications of their dynamics and the influence of the quasar on the host galaxy's evolution. This study contributes to our understanding of the complex interactions between galaxies and their neighboring quasars, shedding light on the processes that govern their development and the role of environmental factors in shaping their characteristics.",
        "ori-fast-z-score": 0.10846522890932808,
        "water-fast-z-score": 4.74464202095129,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fusion process studied with preequilibrium giant dipole resonance in time dependent Hartree-Fock theory .\nAbstract:\nThe fusion cross sections for the reactions 16O+16O, 40Ca+40Ca and 48Ti+48Ti are calculated using the time-dependent Hartree-Fock (TDHF) method including the effects of the pre-equilibrium Giant Dipole Resonance (PDR). The PDR is found to play an important role in determining the fusion barrier heights as well as the fusion probabilities at sub-barrier energies. It is shown that the inclusion of the PDR leads to better agreement between theoretical predictions and experimental data than those obtained without it. \n \n In recent years there has been considerable interest in studying nuclear fusion processes by means of microscopic theories such as the Time-Dependent Hartree-Fock (TDDF) approach  1  . This is mainly due to its ability to provide detailed information on various aspects of the reaction dynamics which cannot be easily extracted experimentally  2  .\n \nIn this work we have investigated the effect of the Pre-Equilibrium Giant Dipole Resonances (PEGDRs), which appear during the initial stage of heavy ion collisions  3  , on the fusion cross section for three different systems namely 16 O + 16 O, 40 Ca + 40 Ca and 48 Ti + 48 Ti. These calculations were performed within the TDHF framework  4  where the ground state wave functions of these nuclei were generated using the Skyrme interaction Sly6  5  . \n \n We find that the PEGDR plays an important role in determining both the height and shape of the fusion barriers. Moreover, our results show that the inclusion of the pygmy resonances improves considerably the agreement between the theoretical predictions and available experimental data  6  -  8  compared to those obtained without them.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fusion process researched with preequilibrium giant dipole resonance in time dependent Hartree - Fock technique . Abstract : The fusion cross sections for the compounds 16O + 16O , 40Ca + 40Ca and 48Ti + 48Ti are measured using the time - dependent Hartree - Fock ( TDHF ) method including the effects of the pre - equilibrium Giant Dipole Resonance ( PDR ) .The PDR is found to take an important role in measuring the fusion barrier depths as well as the fusion probabilities at sub - barrier energies . It is demonstrated that the inclusion of the PDR leads to higher agreement between theoretical estimates and theoretical data than those achieved without it .In recent years there has been substantial interest in investigating nuclear fusion mechanisms by means of microscopic theories such as the Time - Dependent Hartree - Fock ( TDDF ) approach 1 . This is mainly owing to its able to provide comprehensive information on various parts of the reaction dynamics which cannot be easily retrieved experimentally 2 .In this research we have researched the impact of the Pre - Equilibrium Giant Dipole Resonances ( PEGDRs ) , which appear during the first phase of large particle collisions 3 , on the fusion cross section for three different systems namely 16 O + 16 O , 40 Ca + 40 Ca and 48 Ti + 48 Ti . These measurements were performed within the TDHF framework 4 where the ground state wave functions of these ions were generated utilizing the Skyrme coupling Sly6 5 .We see that the PEGDR plays an important role in measuring both the height and shape of the fusion barriers . Moreover , our findings show that the introduction of the pygmy resonances improves substantially the cooperation between the theoretical estimates and accessible empirical data 6 - 8 compared to those achieved without them .",
        "rewrite_text": "**Title:** Investigation of Fusion Processes via Pre-Equilibrium Giant Dipole Resonance in Time-Dependent Hartree-Fock Framework\n\n**Abstract:** This study explores the fusion cross sections for the reactions involving the isotopes 16O + 16O, 40Ca + 40Ca, and 48Ti + 48Ti, utilizing the Time-Dependent Hartree-Fock (TDHF) method while incorporating the effects of Pre-Equilibrium Giant Dipole Resonance (PDR). Our findings indicate that PDR significantly influences the determination of fusion barrier depths and enhances fusion probabilities at sub-barrier energies. The results demonstrate that integrating PDR into the TDHF framework yields a closer alignment between theoretical predictions and experimental data compared to analyses that exclude this resonance. Recent advancements in microscopic theories, particularly the TDHF approach, have sparked considerable interest in understanding nuclear fusion mechanisms, as these methods provide detailed insights into the reaction dynamics that are often challenging to capture through experimental means. In this research, we specifically examine the role of Pre-Equilibrium Giant Dipole Resonances (PEGDRs), which emerge during the initial stages of substantial particle collisions, on the fusion cross sections of the aforementioned systems. The calculations were conducted within the TDHF framework, utilizing the Skyrme interaction Sly6 to generate the ground state wave functions for the nuclei involved. Our analysis reveals that PEGDRs are crucial for accurately assessing both the height and shape of the fusion barriers. Furthermore, our results indicate that the inclusion of pygmy resonances significantly enhances the correlation between theoretical estimates and available empirical data, thereby improving the predictive power of the model. This work contributes to a deeper understanding of the fusion process and highlights the importance of accounting for resonant phenomena in theoretical frameworks.",
        "ori-fast-z-score": -2.3539293971054818,
        "water-fast-z-score": 5.975359238806223,
        "rewrite-fast-z-score": -0.18107149208503706
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmogenic neutrinos as a probe of the transition from Galactic to extragalactic cosmic rays .\nAbstract:\nWe present an analysis of cosmogenic neutrino fluxes expected in IceCube for different models of cosmic ray sources and propagation, including recent results on anisotropy measurements by PAMELA and AMS-02 experiments. We show that these data can be used to discriminate between various scenarios of cosmic ray origin and propagation. In particular we find that:  - The observed anisotropies are consistent with predictions based on conventional astrophysical acceleration mechanisms only if one assumes very hard injection spectra at source (with spectral index below -2). This is inconsistent with observations of other messengers such as gamma-rays or radio waves.  - If one allows for more exotic particle physics processes like decaying dark matter particles then it becomes possible to explain both the observed anisotropies and the spectrum of cosmic rays without violating any observational constraints. However this scenario requires fine-tuning of parameters which makes it less appealing than conventional astrophysics explanations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmogenic neutrinos as a investigation of the shift from Galactic to extragalactic cosmic rays . Abstract : We report an assessment of cosmogenic neutrino fluxes anticipated in IceCube for different models of gamma ray sources and propagation , notably recent results on anisotropy observations by PAMELA and AMS - 02 experiments .We see that these information can be used to discriminate between various circumstances of cosmic ray origin and propagation . In particular we find that : - The observed anisotropies are compatible with predictions based on conventional astrophysical acceleration mechanisms only if one takes very hard injection spectra at source ( with spectral index below - 2 ) .This is conflicting with observations of other messengers such as gamma - radiation or radio beams . - If one permits for more exotic electron physics processes like decaying heavy material particles then it becomes possible to explain both the known anisotropies and the spectrum of cosmic rays without violating any observational restrictions .However this situation requires fine - tuned of values which makes it less acceptable than conventional astrophysics explanations .",
        "rewrite_text": "Title: Cosmogenic Neutrinos as a Probe of the Transition from Galactic to Extragalactic Cosmic Rays\n\nAbstract: In this study, we evaluate the anticipated fluxes of cosmogenic neutrinos detectable by the IceCube observatory, focusing on various models of gamma-ray sources and their propagation. Our analysis incorporates recent findings from the PAMELA and AMS-02 experiments, which have provided significant insights into the anisotropy of cosmic rays. We demonstrate that these observations can effectively differentiate between different scenarios regarding the origins and propagation mechanisms of cosmic rays. Notably, our findings indicate that the observed anisotropies align with predictions derived from conventional astrophysical acceleration mechanisms only when assuming very steep injection spectra at the source, characterized by a spectral index lower than -2. This assumption, however, contradicts observations from other cosmic messengers, such as gamma rays and radio emissions. Alternatively, we explore the implications of allowing for more unconventional processes, such as the decay of heavy particles, which could potentially account for both the observed anisotropies and the cosmic ray spectrum without infringing upon existing observational constraints. Nevertheless, this scenario necessitates a precise tuning of parameters, which raises concerns about its viability compared to more traditional astrophysical explanations. Our results underscore the complexity of cosmic ray origins and highlight the need for further investigation into both conventional and exotic models to reconcile the discrepancies observed across different cosmic messenger types.",
        "ori-fast-z-score": -0.6625891564490792,
        "water-fast-z-score": 6.110100926607787,
        "rewrite-fast-z-score": -0.4975185951049946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exploring Continuous Tensegrities .\nAbstract:\nThe concept of tensegrity is used to describe the structural behavior of many biological systems, such as muscles and tendons.  In this work we explore how continuous tensegrities can be generated by using an evolutionary algorithm that optimizes their performance in terms of compliance with external loads while maintaining stability under gravity loading conditions.   The results show that it is possible to generate stable structures that are able to resist large deformations without collapsing or losing their integrity. This research has been funded by the European Commission through the Marie Curie Initial Training Network (ITN) program. The concept of tensegrity was first introduced by Buckminster Fuller more than 60 years ago  1  . It describes the structural behavior of many natural systems like muscles  2  , tendons  3  , bones  4  , and even living organisms  5  .\nIn recent decades there have been several attempts at applying the concept of tensegrity to engineering applications  6  -  8  . However, most of these works focus on discrete tensegrities which consist of rigid bars connected together by elastic struts  9  . These types of structures cannot easily adapt to changes in their environment since they do not allow for any deformation  10  . On the other hand, continuous tensegrities  11  are capable of changing shape continuously when subjected to external forces  12  . They also exhibit higher levels of robustness against damage  13  compared to conventional materials  14  . Despite all these advantages, very little attention has been paid so far to the design of continuous tensegrities  15  .\nThis lack of interest may be due to the fact that designing continuous tensegrities requires solving highly nonlinear optimization problems  16  . Moreover, finding solutions to these problems is extremely challenging because of the high number of local optima  17  . To overcome these difficulties, researchers usually use heuristic search algorithms  18  -  20  instead of exact methods  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exploring Continuous Tensegrities . Abstract : The concept of tensegrity is utilized to define the structural response of several biological units , such as muscles and tendons .In this research we investigate how continuous tensegrities can be derived by using an phylogenetic algorithm that optimizes their performance in terms of compliance with external loads while maintaining strength under gravity loading circumstances . The results show that it is possible to create stable objects that are able to resist big deformations without weakening or losing their stability .This research has been sponsored by the European Commission through the Marie Curie Initial Training Network ( ITN ) program . The concept of tensegrity was first applied by Buckminster Fuller more than 60 years early 1 .It details the structural response of several biological systems like nerves 2 , tendons 3 , ribs 4 , and even living organisms 5 . In past decades there have been numerous attempts at application the idea of tensegrity to engineering users 6 - 8 .However , most of these works concentrate on discrete tensegrities which consist of rigid bars connected together by elastic struts 9 . These sorts of structures cannot easily respond to changes in their environment since they do not enable for any deformation 10 .On the other hand , continuous tensegrities 11 are capable of changing shape consistently when exposed to external forces 12 . They addition exhibit greater levels of robustness against damage 13 relative to conventional materials 14 .Despite all these benefits , very less attention has been paid so far to the design of continuous tensegrities 15 . This lack of interest might be due to the fact that designing continuous tensegrities demands modeling highly nonlinear optimization problems 16 .Moreover , finding solutions to these problems is incredibly hard because of the high number of local optima 17 . To solve these problems , researchers normally use heuristic search methods 18 - 20 rather of precise methods 21 .",
        "rewrite_text": "**Title: Exploring Continuous Tensegrities**\n\n**Abstract:** The concept of tensegrity, which describes a structural system characterized by a balance of tension and compression, has significant implications for understanding the mechanical behavior of various biological structures, including muscles and tendons. This study delves into the derivation of continuous tensegrities through the application of a phylogenetic algorithm aimed at optimizing their performance under external loads while ensuring structural integrity against gravitational forces. Our findings demonstrate the feasibility of constructing stable continuous tensegrity structures that can endure substantial deformations without compromising their stability or strength. This research is supported by the European Commission via the Marie Curie Initial Training Network (ITN) program. The tensegrity concept, originally introduced by Buckminster Fuller over six decades ago, has been instrumental in elucidating the structural dynamics of numerous biological systems, such as nerves, tendons, ribs, and even entire organisms. In recent years, there has been a surge of interest in applying tensegrity principles to engineering applications; however, most studies have focused on discrete tensegrities, which are composed of rigid bars interconnected by elastic struts. These discrete structures often lack the adaptability required to respond effectively to environmental changes due to their limited capacity for deformation. In contrast, continuous tensegrities possess the ability to undergo shape transformations in response to external forces, offering enhanced resilience against damage compared to traditional materials. Despite their advantages, the design of continuous tensegrities has received relatively little attention, likely due to the complexities involved in modeling highly nonlinear optimization problems associated with their design. The challenge of navigating numerous local optima further complicates the search for optimal solutions, leading researchers to predominantly rely on heuristic search methods rather than precise optimization techniques. This study aims to bridge this gap by exploring innovative approaches to the design and optimization of continuous tensegrities, thereby contributing to the advancement of both theoretical understanding and practical applications in engineering and biology.",
        "ori-fast-z-score": -1.6666666666666667,
        "water-fast-z-score": 7.835467939002064,
        "rewrite-fast-z-score": 0.23942606534028665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exciting the Magnetosphere of the Magnetar CXOU J164710.2-455216 in Westerlund 1 .\nAbstract:\nWe report on observations made with Chandra and XMM-Newton that reveal an X-ray flare from the magnetar CXOU J16 47 10 . 2-45 52 16 (hereafter, J1647) located within the open cluster Westerlund 1. The flare was detected by both observatories during their respective slews to point at another target; it lasted for about one hour before fading below detectability. We find no evidence for any significant change in the spin-down rate or period derivative of this source following its outburst. \n \n This is the first time such a large event has been observed from a magnetar; we estimate that the total energy released in the flare was ~3 x 10^44 erg. Our analysis shows that the flare occurred when the star s magnetic field lines were nearly perpendicular to our line-of-sight. In addition, we detect pulsations from J1647 during the flare which are consistent with those seen prior to the flare. These results suggest that the flaring activity may be due to reconnection events occurring along the closed loops of the stellar magnetic field.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exciting the Magnetosphere of the Magnetar CXOU J164710 . 2 - 455216 in Westerlund 1 . Abstract : We report on observations made with Chandra and XMM - Newton that reveal an X - ray flare from the magnetar CXOU J16 47 10 .2 - 45 52 16 ( hereafter , J1647 ) located within the open cluster Westerlund 1 . The flare was noticed by both observatories during their separate slews to point at another target ; it persisted for about one hour before faded below detectability .We see no evidence for any large change in the spin - down frequency or time derivative of this source following its outburst . This is the first time such a large incident has been observed from a magnetar ; we estimate that the total energy released in the flare was ~ 3 x 10 ^ 44 erg .Our study shows that the flare originated when the star s magnetic field lines were roughly perpendicular to our line - of - view . In addition , we find pulsations from J1647 during the flare which are compatible with those observed earlier to the flare .These data suggest that the flaring activity may be due to reconnection events resulting along the shut rings of the stars magnetic force .",
        "rewrite_text": "We present findings from observations conducted with the Chandra and XMM-Newton space telescopes, which detected an X-ray flare from the magnetar CXOU J164710.2-455216 (referred to as J1647), situated within the open cluster Westerlund 1. This significant flare was identified by both observatories during their independent slews toward different targets and lasted for approximately one hour before diminishing to levels below detectability. Notably, our analysis reveals no substantial alterations in the spin-down frequency or its time derivative following the outburst, marking this event as the first instance of such a pronounced flare observed from a magnetar. We estimate that the total energy emitted during this flare was approximately 3 x 10^44 erg. \n\nOur investigation indicates that the flare occurred when the magnetic field lines of the star were oriented nearly perpendicular to our line of sight. Furthermore, we detected pulsations from J1647 during the flare, which align with previously recorded pulsations prior to the event. These observations imply that the flaring activity may be attributed to magnetic reconnection events occurring along the star's magnetic field lines. This study not only enhances our understanding of magnetar behavior but also provides critical insights into the mechanisms driving such energetic phenomena in these enigmatic celestial objects.",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": -0.21081851067789195
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Massive planet migration: Theoretical predictions and comparison with observations .\nAbstract:\nWe present the results of our theoretical study on massive planet migration in protoplanetary disks, focusing on its dependence on disk properties such as viscosity and surface density profiles. We find that the type I migration rate is strongly dependent on the disk s viscosity profile; it decreases for higher viscosities at small radii but increases again beyond a certain radius (typically 1 AU). This behavior can be explained by considering the balance between corotation torques and Lindblad torques. In addition to this effect, we also found that the mass accretion onto planets changes significantly depending on their orbital distance due to the change in gas pressure gradient across the gap opened up by the planet. Finally, we compare these theoretical predictions with recent observational data obtained using direct imaging techniques. Our results suggest that there are two possible scenarios for explaining the observed distribution of exoplanets: either they formed very close to their host stars or they underwent significant inward migration after formation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Massive planet migration : Theoretical predictions and contrast with observations . Abstract : We present the conclusion of our theory study on massive planet migration in protoplanetary disks , concentrating on its dependence on disk properties such as viscosity and surface velocity characteristics .We see that the class I migration rate is strongly dependent on the disk s viscosity profile ; it reduces for greater viscosities at small radii but grows again beyond a certain radius ( commonly 1 AU ) . This phenomenon can be understood by examining the balance between corotation torques and Lindblad torques .In addition to this effect , we also discovered that the mass accretion onto planets changes significantly depending on their orbital length owing to the shift in gas pressure gradient across the gap opened up by the planet . Finally , we compare these theoretical estimates with recent observational data acquired using direct imaging method .Our results propose that there are two possible possibilities for explaining the known distribution of exoplanets : either they formed very close to their host stars or they underwent considerable inward movement after formed .",
        "rewrite_text": "We present the findings of our theoretical investigation into the migration of massive planets within protoplanetary disks, with a particular focus on how various disk properties—such as viscosity and surface velocity—affect this process. Our analysis reveals that the migration rate for Class I planets is significantly influenced by the viscosity profile of the disk. Specifically, we observe that at smaller radii, increased viscosity leads to a reduction in migration rates; however, beyond a certain radius, typically around 1 AU, the migration rate begins to increase again. This behavior can be explained by the interplay between corotation torques and Lindblad torques, which dictate the dynamics of planet migration. Furthermore, we found that the mass accretion onto planets is notably affected by their orbital distances, primarily due to variations in the gas pressure gradient across the gap created by the planet's presence. To validate our theoretical predictions, we juxtaposed our findings with recent observational data obtained through direct imaging techniques. Our results suggest two plausible scenarios for the observed distribution of exoplanets: either these planets formed in close proximity to their host stars or they experienced significant inward migration after their formation. This study enhances our understanding of planet formation and migration processes, providing a framework for reconciling theoretical models with empirical observations in the field of exoplanet research.",
        "ori-fast-z-score": -0.6255432421712244,
        "water-fast-z-score": 5.910624659963424,
        "rewrite-fast-z-score": 0.19425717247145283
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytic result for the one-loop massless triangle Feynman diagram .\nAbstract:\nWe present an analytic expression for the one-loop mass-less triangle Feynman integral in terms of generalized hypergeometric functions. The results are obtained by using Mellin-Barnes representation and contour integration techniques. We also provide numerical values for some special cases which can be used to check our analytical expressions. This work is motivated by recent interest on the study of higher order corrections to various physical processes, such as Higgs decay into two photons or gluons at next-to-leading-order (NLO) accuracy. \nI. INTRODUCTORY REMARK\nThe calculation of loop diagrams plays an important role in theoretical physics. In particular, it has been shown that the inclusion of radiative corrections leads to significant changes in the predictions of many observables  1  . For example, the NLO QCD correction to the decay widths of heavy quarks  2  , top quark pair production  3  , Higgs boson decays  4  etc., have been calculated recently with great success. However, there still remain several open problems related to the evaluation of multi-loop integrals  5  .\nIn this letter we consider the following one-loop mass-less triangle Feyman integral  6  : \nwhere m 1 = m 2 ≡ m 3 ≡ m 4 ≡ 0 and s 12 = q 2 . It should be noted here that I(q 2 ) vanishes when any three masses become equal i.e. m 1 = m 2 = m 3 = m 4 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytic result for the one - loop massless triangle Feynman diagram . Abstract : We create an analytic definition for the one - loop mass - less triangle Feynman integral in terms of generalized hypergeometric functions .The results are derived by using Mellin - Barnes representation and contour integration methods . We additionally offer mathematical values for some special cases which can be used to test our analytical expressions .This research is prompted by recent interest on the study of greater order corrections to several physical processes , such as Higgs decay into two photons or gluons at next - to - leading - order ( NLO ) accuracy . I .INTRODUCTORY REMARK The calculation of loop diagrams takes an important role in theoretical physics . In particular , it has been shown that the introduction of radiative corrections leads to significant improvements in the estimates of several observables 1 .For instance , the NLO QCD correction to the decay widths of hard quarks 2 , top quark pair production 3 , Higgs boson decays 4 etc . , have been measured recently with great success . However , there still continue several open problems related to the evaluation of dual - loop integrals 5 .In this letter we imagine the following one - loop mass - less triangle Feyman integral 6 : where m 1 = m 2 ≡ m 3 ≡ m 4 ≡ 0 and s 12 = q 2 . It should be mentioned here that I ( q 2 ) vanishes when any three masses become equal i . e .m 1 = m 2 = m 3 = m 4 .",
        "rewrite_text": "**Title:** Analytic Result for the One-Loop Massless Triangle Feynman Diagram\n\n**Abstract:** In this study, we present an analytic formulation for the one-loop massless triangle Feynman integral, utilizing generalized hypergeometric functions. Our findings are derived through the application of Mellin-Barnes representation and contour integration techniques. Additionally, we provide specific mathematical values for select cases, which serve as benchmarks for validating our analytical expressions. This research is motivated by the growing interest in exploring higher-order corrections to various physical phenomena, including the next-to-leading-order (NLO) accuracy of Higgs boson decay into two photons or gluons. \n\nThe calculation of loop diagrams is a fundamental aspect of theoretical physics, particularly in the context of radiative corrections, which have been shown to enhance the precision of numerous observable quantities. For example, recent measurements of NLO QCD corrections have yielded significant insights into the decay widths of heavy quarks, top quark pair production, and Higgs boson decays. Despite these advancements, challenges remain in the evaluation of dual-loop integrals, which continue to pose open questions in the field.\n\nIn this letter, we focus on the specific case of the one-loop massless triangle Feynman integral, characterized by the condition where all masses are set to zero (m1 = m2 = m3 = m4 = 0) and the Mandelstam variable is defined as s12 = q². Notably, we observe that the integral I(q²) approaches zero when any three of the masses are equal, highlighting an important symmetry in the system. Our results contribute to the ongoing discourse on loop integrals and their implications for particle physics, paving the way for further exploration of higher-order corrections in various processes.",
        "ori-fast-z-score": -0.09853292781642932,
        "water-fast-z-score": 5.148767223478707,
        "rewrite-fast-z-score": 0.4745789978762495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ellipsoidal Oscillations Induced by Substellar Companions: A Prospect for the Kepler Mission .\nAbstract:\nWe present an analysis of ellipsoidal oscillations induced in main-sequence stars by their substellar companions, and discuss prospects for detecting such signals with the Kepler mission. We find that these effects are detectable over a wide range of orbital periods (from 1 to 1000 days) and companion masses (0.1-10 MJ). The amplitude of the signal is proportional to the square root of the mass ratio between the star and its companion; it can be as large as 0.1% of the total flux at optical wavelengths. For typical parameters expected among planet-hosting systems, we estimate that this effect should produce a peak-to-peak variation in brightness on the order of 10 mmag or less. This level of precision will require several months of continuous observations using the Kepler spacecraft s photometer. However, if detected, the presence of such a signal would provide strong evidence for the existence of a low-mass stellar or planetary companion orbiting the primary star. \n \n Keywords: Ellipsoidal variations, Planetary system",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ellipsoidal Oscillations Induced by Substellar Companions : A Prospect for the Kepler Mission . Abstract : We present an assessment of ellipsoidal oscillations induced in major - sequence stars by their substellar companions , and consider prospects for detecting such signals with the Kepler expedition .We see that these changes are detectable over a broad variety of orbital periods ( from 1 to 1000 days ) and companion masses ( 0 . 1 - 10 MJ ) . The amplitude of the signal is proportional to the square root of the mass ratio between the star and its companion ; it can be as big as 0 . 1 % of the total flux at imaging wavelengths .For common parameters predicted among planet - hosting environments , we estimate that this effect should produce a peak - to - peak change in intensity on the order of 10 mmag or smaller . This amount of precision will take many months of simultaneous measurements using the Kepler satellites s photometer .However , if detected , the presence of such a signal might give strong evidence for the existence of a small - weight stellar or planetary companion orbiting the primary star . Keywords : Ellipsoidal changes , Planetary system",
        "rewrite_text": "Title: Ellipsoidal Oscillations Induced by Substellar Companions: A Prospect for the Kepler Mission\n\nAbstract: This study evaluates the ellipsoidal oscillations that occur in main-sequence stars due to the gravitational influence of their substellar companions, with a focus on the potential for detecting these oscillations through the Kepler mission. Our analysis reveals that these oscillations can be observed across a wide range of orbital periods, spanning from 1 to 1000 days, and for companion masses ranging from 0.1 to 10 times the mass of Jupiter (MJ). The amplitude of the oscillation signal is found to be directly related to the square root of the mass ratio between the star and its companion, with potential amplitudes reaching up to 0.1% of the total flux at imaging wavelengths. For typical parameters expected in environments that host planets, we estimate that the resulting peak-to-peak variations in intensity would be approximately 10 millimagnitudes (mmag) or less. Achieving this level of precision will necessitate extensive simultaneous measurements over several months using the photometer aboard the Kepler satellites. Should these oscillations be detected, they would provide compelling evidence for the presence of a low-mass stellar or planetary companion orbiting the primary star. This research underscores the significance of ellipsoidal changes in understanding planetary systems and highlights the capabilities of the Kepler mission in uncovering subtle astrophysical phenomena. \n\nKeywords: Ellipsoidal oscillations, substellar companions, Kepler mission, planetary systems.",
        "ori-fast-z-score": -2.0124611797498106,
        "water-fast-z-score": 4.638124095143555,
        "rewrite-fast-z-score": -0.10259783520851541
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the nature of the phase transition in the three-dimensional random field Ising model .\nAbstract:\nWe study the critical behavior of the 3D RFIM with Gaussian distributed disorder by means of Monte Carlo simulations and finite-size scaling analysis. We find that the system undergoes a continuous phase transition at zero temperature, which is characterized by an infinite correlation length but no divergent susceptibility. The results are compared to those obtained for the pure 3D Ising model as well as other models with quenched disorder. In particular we show how our findings can be understood within the framework of the droplet picture. \nPACS numbers: 64.60.Cn, 64.60.J-, 64.60.Nz \nI. INTRODUCTORY REMARkS\nThe Random Field Ising Model (RFIM) has been introduced more than 50 years ago  1  . It describes a ferromagnetic material where each spin interacts only with its nearest neighbors via exchange interactions J ij , while it also feels an external magnetic field h i randomly oriented on different sites  2  .\nIn recent years there have been many studies devoted to this problem both experimentally  3  -  6  and theoretically  7  -  12  . This interest was triggered mainly by the fact that the RFIM shares some features with real systems such as diluted antiferromagnets or spin-glasses  13  -  15  . For example, the presence of quenched disorder leads to frustration effects  16  similar to those observed in spin-glass materials  17  . Moreover, the RFIM displays a rich variety of phases depending on the strength of the applied magnetic field  18  . At low fields one finds a paramagnetic phase, whereas above a certain threshold value H c = O(J), the spins align along the direction of the local magnetic field leading to a ferromagnetic state  19  . Finally, if the magnitude of the external field exceeds another threshold value H t > H c , the magnetization becomes discontinuous  20  . These three regimes are separated by two second-order transitions occurring at T c1 < 0 and T c2 > 0  21  . However, despite these analogies between the RFIM and experimental systems  22  , the exact nature of the phase diagram remains controversial  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the nature of the phase shift in the three - dimensional random field Ising model . Abstract : We research the important dynamics of the 3D RFIM with Gaussian distributed disorder by means of Monte Carlo simulations and finite - length scaling processing .We see that the system undergoes a continuous phase shift at zero temperature , which is characterized by an endless correlation length but no divergent susceptibility . The results are compared to those achieved for the pure 3D Ising model as well as other models with quenched instability .In particular we give how our findings can be understood within the framework of the droplet picture . PACS numbers : 64 . 60 . Cn , 64 . 60 . J - , 64 . 60 . Nz I .INTRODUCTORY REMARkS The Random Field Ising Model ( RFIM ) has been proposed more than 50 centuries earlier 1 . It describes a ferromagnetic material where each spin interacts only with its closest neighbors via transfer interactions J ij , while it also feels an external magnetic force h i randomly oriented on various places 2 .In recent years there have been many research devoted to this question both experimentally 3 - 6 and theoretically 7 - 12 . This concern was sparked mainly by the fact that the RFIM shares some features with real systems such as diluted antiferromagnets or spin - glasses 13 - 15 .For instance , the presence of quenched instability leads to frustration effects 16 comparable to those observed in spinning - glass materials 17 . Moreover , the RFIM displays a rich range of phases depending on the strength of the applied magnetic force 18 .At small fields one gets a paramagnetic phase , whereas above a certain threshold number H c = O ( J ) , the spins align along the direction of the local magnetic field leading to a ferromagnetic state 19 . Finally , if the magnitude of the external field exceeds another threshold quantity H t > H c , the magnetization makes discontinuous 20 .These three regimes are separated by two second - order transitions happening at T c1 < 0 and T c2 > 0 21 . However , despite these analogies between the RFIM and theoretical systems 22 , the exact nature of the phase diagram remains disputed 23 .",
        "rewrite_text": "**Title:** On the Nature of the Phase Shift in the Three-Dimensional Random Field Ising Model\n\n**Abstract:** This study investigates the dynamic behavior of the three-dimensional Random Field Ising Model (RFIM) characterized by Gaussian-distributed disorder, utilizing Monte Carlo simulations and finite-length scaling techniques. Our findings reveal that the system experiences a continuous phase transition at absolute zero temperature, marked by an infinite correlation length without a corresponding divergence in susceptibility. We compare these results with those obtained from the pure three-dimensional Ising model and other models exhibiting quenched disorder. Notably, we interpret our results within the droplet model framework, which provides insights into the underlying mechanisms of the phase transition. \n\nThe Random Field Ising Model, introduced over five decades ago, serves as a theoretical representation of ferromagnetic materials where each spin interacts with its nearest neighbors through exchange interactions while being influenced by a randomly oriented external magnetic field. Recent research has extensively explored the RFIM from both experimental and theoretical perspectives, driven by its relevance to real-world systems such as diluted antiferromagnets and spin glasses. The presence of quenched disorder in the RFIM leads to frustration phenomena akin to those observed in spin-glass materials, resulting in a complex phase structure that varies with the strength of the applied magnetic field. \n\nAt low external fields, the system exhibits a paramagnetic phase, transitioning to a ferromagnetic state when the field surpasses a critical threshold (H_c = O(J)). Beyond another threshold (H_t > H_c), the magnetization undergoes a discontinuous change. These distinct regimes are separated by two second-order phase transitions occurring at temperatures T_c1 < 0 and T_c2 > 0. Despite the similarities between the RFIM and other theoretical models, the precise nature of its phase diagram remains a topic of ongoing debate within the scientific community. Our research contributes to this discourse by elucidating the characteristics of the phase shift in the RFIM and its implications for understanding disordered magnetic systems. \n\n**PACS numbers:** 64.60.Cn, 64.60.J-, 64.60.Nz",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 6.4390634770545985,
        "rewrite-fast-z-score": 0.08362420100070908
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Atomic Electric Dipole Moments: The Schiff Theorem and Its Corrections .\nAbstract:\nThe electric dipole moment (EDM) is an important observable in atomic physics, which can be used to test fundamental symmetries such as parity violation.  In this work we present the results for the EDM of Yb+ ions produced by laser cooling techniques at the Paul trap facility at the University of Bern.   We have measured the EDM with two different methods using either circularly polarized light or magnetic field gradients.  Our measurement yields a value of dYb+=(0.6 ± 1.1 stat ± 0.7 sys ) × 10−29 e cm where the systematic uncertainty has been reduced compared to previous measurements performed on Yb+ ions.  This result agrees well within uncertainties with theoretical predictions based on Standard Model calculations including nuclear structure corrections. A new experiment will start soon aiming at improving our current precision by one order of magnitude. The electric dipole moment (EDMs) are observables that can be used to probe fundamental symmetries like parity violation  1  . They are predicted to arise from CP-violating interactions beyond the standard model  2  , but their values are extremely small  3  .\nIn recent years there has been considerable interest in measuring EDMs of atoms  4  -  8  because they provide clean tests of fundamental theories  9  . Measurements of the electron EDM  10  -  12  yield upper limits on possible contributions from new particles  13  . On the other hand, experiments searching for permanent EDMs of nuclei  14  -  16  may lead to direct evidence of new sources of CP violation  17  .  Currently, most experimental efforts focus on heavy polarizable systems  18  -  20  since these offer higher sensitivity  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Atomic Electric Dipole Moments : The Schiff Theorem and Its Corrections . Abstract : The electric dipole point ( EDM ) is an important observable in nuclear physics , which can be used to test fundamental symmetries such as parity violation .In this research we present the results for the EDM of Yb + ions produced by laser cooling technology at the Paul trap plant at the University of Bern . We have recorded the EDM with two different methods using either circularly polarized light or magnetic field gradients .Our measurement yields a value of dYb + = ( 0 . 6 ± 1 . 1 stat ± 0 . 7 sys ) × 10−29 e cm where the systematic uncertainty has been reduced compared to previous measurements accomplished on Yb + ions . This result agrees well within uncertainties with theoretical estimates based on Standard Model estimates including radioactive structure corrections .A second experiment will start soon aim at enhancing our current precision by one order of magnitude . The electric dipole moment ( EDMs ) are observables that can be used to probe intrinsic symmetries like parity violation 1 .They are expected to arise from CP - violating interactions beyond the standard theory 2 , but their values are extremely tiny 3 . In recent months there has been substantial interest in measuring EDMs of atoms 4 - 8 because they give clean tests of fundamental theories 9 .Measurements of the electron EDM 10 - 12 yield upper limits on potential contributions from additional particles 13 . On the other hand , experiments searching for permanent EDMs of nuclei 14 - 16 might lead to direct indication of new sources of CP violation 17 .Currently , most research efforts focus on heavy polarizable systems 18 - 20 since these allow larger sensitivity 21 .",
        "rewrite_text": "**Title:** Atomic Electric Dipole Moments: The Schiff Theorem and Its Corrections\n\n**Abstract:** The electric dipole moment (EDM) serves as a crucial observable in nuclear physics, providing a means to investigate fundamental symmetries, particularly parity violation. In this study, we present our findings on the EDM of Yb⁺ ions, which were generated using advanced laser cooling techniques at the Paul trap facility at the University of Bern. Our measurements were conducted using two distinct methodologies: one involving circularly polarized light and the other utilizing magnetic field gradients. The resulting measurement of the EDM is dYb⁺ = (0.6 ± 1.1 stat ± 0.7 sys) × 10⁻²⁹ e cm, showcasing a significant reduction in systematic uncertainty compared to prior measurements of Yb⁺ ions. This value aligns well with theoretical predictions derived from the Standard Model, which includes corrections for radioactive structure. \n\nLooking ahead, we are preparing to initiate a second experiment aimed at enhancing our measurement precision by an order of magnitude. Electric dipole moments are pivotal for probing intrinsic symmetries such as CP violation, which are anticipated to emerge from interactions beyond the Standard Model. Despite their expected minuscule values, recent months have seen a surge of interest in measuring EDMs of various atomic systems, as they provide clear tests of fundamental theories. Notably, measurements of the electron EDM have established upper limits on potential contributions from hypothetical additional particles. Conversely, experiments focused on the permanent EDMs of nuclei may offer direct evidence of new sources of CP violation. Presently, the majority of research is concentrated on heavy polarizable systems, as these offer enhanced sensitivity for detecting EDMs. This work not only contributes to our understanding of fundamental symmetries but also paves the way for future explorations into the nature of CP violation and its implications for particle physics.",
        "ori-fast-z-score": -0.18107149208503706,
        "water-fast-z-score": 5.139516917604364,
        "rewrite-fast-z-score": 0.26013299085723596
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Proper Motions in the Galactic Bulge: Plaut s Window .\nAbstract:\nWe present proper motions for stars with magnitudes between 8 and 16, obtained by combining data from two epochs of photographic plates taken at the Palomar Observatory (POSS-I) and one epoch of digital images taken with the Hubble Space Telescope (HST). The sample consists of about 1 million objects located within a region centered on the galactic center that is known as  Plaut s window .  We find that our results are consistent with previous measurements made using POSS-II plates combined with HST observations. However, we also find significant differences when compared to other recent studies based on similar datasets but different analysis techniques. These discrepancies may be due to systematic errors introduced during the reduction process or they could indicate real changes in the structure of the bulge over time. Our final catalogue will be available online through the CDS Vizier service. This work was supported by NASA grant NAG5-13523.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Proper Motions in the Galactic Bulge : Plaut s Window . Abstract : We present proper motions for stars with magnitudes between 8 and 16 , obtained by combining information from two epochs of photographic sheets taken at the Palomar Observatory ( POSS - I ) and one epoch of digital photographs took with the Hubble Space Telescope ( HST ) .The sample consists of about 1 million bodies located within a region focused on the galactic center that is known as Plaut s window . We see that our findings are compatible with previous measurements made use POSS - II sheets combined with HST observations .However , we also find considerable variations when compared to other recent studies relying on similar datasets but different analysis methods . These discrepancies may be due to systematic errors acquired during the reduction phase or they may indicate real improvements in the composition of the bulge over time .Our last catalogue will be available digital through the CDS Vizier network . This project was supported by NASA grant NAG5 - 13523 .",
        "rewrite_text": "In this study, we present an analysis of proper motions for stars with magnitudes ranging from 8 to 16, derived from a combination of data collected from two epochs of photographic plates from the Palomar Observatory (POSS-I) and one epoch of digital images captured by the Hubble Space Telescope (HST). Our sample encompasses approximately one million stars situated in a specific area around the galactic center, referred to as Plaut's Window. The results of our investigation align well with previous measurements obtained from POSS-II plates in conjunction with HST data. However, we also observe significant discrepancies when comparing our findings to other recent studies that utilized similar datasets but employed different analytical techniques. These variations may stem from systematic errors encountered during the data reduction process, or they could suggest genuine changes in the structure of the galactic bulge over time. Our comprehensive catalog of proper motions will be made available digitally through the CDS Vizier network, facilitating further research in this area. This project received funding support from NASA grant NAG5-13523, underscoring the importance of collaborative efforts in advancing our understanding of stellar dynamics within the Milky Way's bulge.",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": -0.9761870601839528
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Isospin asymmetry in the continuum of the A=14 mirror nuclei .\nAbstract:\nWe present results on isospin symmetry breaking effects for the ground states and excited states of the mirror nuclei 14Be, 14B, 14C, and 14N using the shell model with realistic interactions. We find that the calculated energy differences between the mirror pairs are consistent with experimental data within uncertainties except for the case of 14N where we predict an excitation energy which is about 1 MeV higher than experiment.  The predicted excitation energies of the first 2+ state in 14Be agree well with those obtained by other theoretical calculations but differ significantly from experiments. This discrepancy may be due to missing three-body forces or possibly because our calculation does not include any explicit treatment of the continuum. Our results show that the effect of Coulomb interaction plays only minor role in determining the properties of these nuclei. In addition, we have studied the dependence of the calculated results on different single-particle wave functions used as input into the shell-model diagonalization procedure.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Isospin asymmetry in the continuum of the A = 14 reflection clusters . Abstract : We report findings on isospin symmetry breaking effects for the ground states and excited states of the mirror nuclei 14Be , 14B , 14C , and 14N using the shell model with realistic interactions .We see that the determined power differences between the mirror pairs are compatible with experimental evidence within uncertainties except for the case of 14N where we estimate an excitation energy which is about 1 MeV higher than study . The predicted excitation energies of the first 2 + state in 14Be agree well with those achieved by other theoretical calculations but varies much from experiments .This discrepancy may be due to missing three - bodies forces or possibly because our analysis does not include any explicit treatment of the continuum . Our results show that the impact of Coulomb interaction plays only minor importance in determining the properties of these nuclei .In addition , we have researched the dependence of the derived results on various single - particle wave systems applied as input into the shell - model diagonalization procedure .",
        "rewrite_text": "In this study, we investigate the effects of isospin symmetry breaking in the ground and excited states of the mirror nuclei 14Be, 14B, 14C, and 14N, utilizing the shell model with realistic interactions. Our analysis reveals that the power differences observed between the mirror pairs align well with existing experimental data, within the margins of uncertainty, with the notable exception of 14N. For this nucleus, we estimate an excitation energy approximately 1 MeV higher than previously reported studies. Furthermore, the predicted excitation energies for the first 2+ state in 14Be are consistent with other theoretical models, yet they diverge significantly from experimental results. This inconsistency may stem from the omission of three-body forces in our calculations or the lack of an explicit treatment of the continuum in our analysis. Our findings indicate that the influence of Coulomb interactions is relatively minor in shaping the properties of these nuclei. Additionally, we explore how variations in the single-particle wave functions used as input for the shell-model diagonalization affect our results. Overall, this research contributes to a deeper understanding of isospin asymmetry in nuclear physics, highlighting the complexities involved in accurately modeling nuclear interactions and the need for further investigation into the role of continuum effects and three-body forces.",
        "ori-fast-z-score": 1.078327732034384,
        "water-fast-z-score": 5.253351420705195,
        "rewrite-fast-z-score": 2.1105794120443453
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Charge Ordering in Half-Doped Manganites: Weak Charge Disproportion and Leading Mechanisms .\nAbstract:\nCharge ordering (CO) is one of the most important phenomena in strongly correlated electron systems, which has been observed in many transition metal oxides such as manganese perovskite compounds. In this work we study charge disproportionation in half-doped manganites by using density functional theory with Hubbard U correction. We find that CO state can be stabilized at low temperatures due to strong Coulomb interaction between Mn3+ and Mn4+ ions. The energy gain for CO state over metallic state increases rapidly when temperature decreases below Tc. Our results show that CO state is more stable than other competing states including ferromagnetic insulator phase and antiferromagnetic insulating phase. \n \n Keywords: Charge-ordering, Density-functional-theory, Correlated electrons, Transition-metal-oxides, Manganites, Energy-band-structure, Insulators, Ferromagnetism, Antiferromagnetism",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Charge Ordering in Half - Doped Manganites : Weak Charge Disproportion and Leading Mechanisms . Abstract : Charge ordering ( CO ) is one of the most important phenomena in heavily correlated electron structures , which has been observed in many transition metal oxides such as manganese perovskite compounds .In this research we study charge disproportionation in half - doped manganites by using density functional theory with Hubbard U correction . We see that CO state can be stabilized at low temperatures due to strong Coulomb interaction between Mn3 + and Mn4 + ions .The energy gain for CO state over metallic state increases quickly when pressure drops below Tc . Our results show that CO state is more stable than other competing states including ferromagnetic insulator transition and antiferromagnetic insulating phase .Keywords : Charge - ordering , Density - functional - theory , Correlated atoms , Transition - iron - oxides , Manganites , Energy - band - structure , Insulators , Ferromagnetism , Antiferromagnetism",
        "rewrite_text": "**Title:** Charge Ordering in Half-Doped Manganites: Weak Charge Disproportion and Leading Mechanisms\n\n**Abstract:** Charge ordering (CO) represents a critical phenomenon in the realm of strongly correlated electron systems, prominently observed in various transition metal oxides, particularly manganese perovskite compounds. This study investigates the charge disproportionation occurring in half-doped manganites through the application of density functional theory (DFT) enhanced by Hubbard U corrections. Our findings indicate that the CO state can achieve stability at lower temperatures, primarily driven by the robust Coulomb interactions between Mn3+ and Mn4+ ions. Notably, we observe that the energy advantage of the CO state over the metallic phase escalates significantly as pressure decreases below the critical temperature (Tc). The results of our analysis reveal that the CO state exhibits greater stability compared to other competing phases, including the ferromagnetic insulator transition and the antiferromagnetic insulating phase. This research contributes to a deeper understanding of charge ordering mechanisms in half-doped manganites and highlights the intricate balance of interactions that govern their electronic properties. The implications of these findings extend to the broader field of correlated electron systems, offering insights into the fundamental processes that dictate charge dynamics in transition metal oxides. \n\n**Keywords:** Charge ordering, Density functional theory, Correlated electrons, Transition metal oxides, Manganites, Energy band structure, Insulators, Ferromagnetism, Antiferromagnetism.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 2.3566599571949607,
        "rewrite-fast-z-score": -0.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SuprimeCam Observation of Sporadic Meteors during Perseids 2004 .\nAbstract:\nWe report on the observation of sporadic meteors with Suprime-Cam attached to Subaru Telescope in August, 2004. The total number of detected meteor events is about 12000 and their distribution over sky agrees well with that expected for sporadic meteors. We also found some interesting features such as clustering around bright stars (probably due to fragmentation) and an excess of faint meteors near the ecliptic plane. These results will be useful for understanding the physical processes involved in the formation of meteoroid streams. \n \n Keywords: Meteor shower, Suprime-Cam, Subaru telescope, Spacecraft debris \n \n \n \n Received September 30, 2005 Accepted December 16, 2005 Published January 31, 2006 Online publication date: February 3, 2006 \n \n We have observed sporadic meteors using Suprime-Cam mounted at the Cassegrain focus of the 8-meter Subaru Telescope in August 2004 when the Perseid meteor shower was active. About 12 000 meteor events were detected by our program which automatically detects moving objects in images taken every 20 seconds. Their spatial distribution shows good agreement with that predicted theoretically for sporadic meteors based on orbital elements obtained from radar observations. Some interesting features are also seen; e.g., clustering around bright stars probably caused by fragmentation or an excess of faint meteoroids near the ecliptic.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SuprimeCam Observation of Sporadic Meteors during Perseids 2004 . Abstract : We report on the observation of sporadic meteors with Suprime - Cam attached to Subaru Telescope in August , 2004 .The total number of identified meteor events is about 12000 and their distribution over skies agrees well with that expected for sporadic meteors . We additionally found some interesting features such as clustering around bright stars ( probably due to fragmentation ) and an accumulation of bright meteors near the ecliptic plane .These conclusions will be valuable for studying the physical processes responsible in the formation of meteoroid streams . Keywords : Meteor shower , Suprime - Cam , Subaru observatory , Spacecraft debris Received September 30 , 2005 Accepted December 16 , 2005 Published January 31 , 2006 Online publication date : February 3 , 2006 We have discovered sporadic meteors using Suprime - Cam mounted at the Cassegrain scope of the 8 - meter Subaru Telescope in August 2004 when the Perseid meteor shower was active .About 12 000 meteor events were detected by our system which automatically detects moving objects in images took every 20 seconds . Their spatial distribution shows excellent compliance with that predicted theoretically for sporadic meteors based on orbital elements obtained from radar measurements .Some interesting features are also observed ; e . g . , clustering around bright stars probably due by fragmentation or an accumulation of bright meteoroids near the ecliptic .",
        "rewrite_text": "We present findings from our observations of sporadic meteors conducted with the Suprime-Cam attached to the Subaru Telescope in August 2004, coinciding with the active period of the Perseid meteor shower. Our study identified approximately 12,000 meteor events, and the spatial distribution of these events aligns closely with theoretical predictions for sporadic meteors, which are based on orbital elements derived from radar data. Notably, we observed intriguing patterns, including clusters of meteors around bright stars, likely attributed to fragmentation processes, as well as a notable concentration of bright meteors near the ecliptic plane. These observations provide valuable insights into the physical mechanisms underlying the formation of meteoroid streams. The data collected during this period enhances our understanding of meteor behavior and contributes to the broader field of meteor studies. Our findings underscore the capabilities of the Suprime-Cam in capturing and analyzing transient astronomical phenomena, and they pave the way for future research into the dynamics of meteoroid interactions in the Earth's atmosphere. This work was received on September 30, 2005, accepted on December 16, 2005, and published on January 31, 2006, with an online publication date of February 3, 2006. The keywords associated with this research include meteor shower, Suprime-Cam, Subaru Observatory, and spacecraft debris.",
        "ori-fast-z-score": 1.078327732034384,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 1.7457431218879391
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Anisotropic Locations of Satellite Galaxies: Clues to the Orientations of Galaxies within their Dark Matter Halos .\nAbstract:\nWe present an analysis of anisotropies in satellite galaxy locations around isolated galaxies, using data from the Sloan Digital Sky Survey (SDSS). We find that satellites are preferentially located along the major axes of their hosts and show no preference for being aligned with minor axes or random directions. This result is robust against variations in host luminosity, color, morphology, environment density, and redshift range. The observed alignment between satellites and major axes persists even when we restrict our sample to only those satellites which have been accreted most recently by their hosts. These results suggest that dark matter halos may be triaxial ellipsoids whose orientations align with the shapes of their central galaxies. In addition, we find evidence that this effect increases as one moves towards lower mass systems. Our findings provide new constraints on models of galaxy formation and evolution. Using data from the Sloan Digitial Sky Survey (SDSS), we study the distribution of satellite galaxies around isolated galaxies. We find that satellites are more likely to lie along the major axes of the hosts than they are to lie along either the minor axes or randomly oriented lines through space. This result holds true over a wide variety of host properties including luminosity, color, morphological type, local environmental density, and redshift range. \n \n Figure 1: An example of how we define the orientation of each host s halo relative to its position angle. Here, the blue line shows the projected major axis of the host while the red dashed line indicates the direction perpendicular to it.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Anisotropic Locations of Satellite Galaxies : Clues to the Orientations of Galaxies within their Dark Matter Halos . Abstract : We present an assessment of anisotropies in satellite galaxy locations around distant galaxies , using data from the Sloan Digital Sky Survey ( SDSS ) .We see that orbits are preferentially found along the main axes of their hosts and take no preference for being aligned with minor axes or random directions . This result is robust against variations in host luminosity , color , morphology , environment density , and redshift range .The observed orientation between satellites and major axes persists even when we limit our sample to only those satellites which have been accreted most recently by their hosts . These data suggest that dark matter halos may be triaxial ellipsoids whose orientations align with the shapes of their central galaxies .In addition , we find proof that this effect grows as one moves approaching lower mass systems . Our findings provide novel constraints on estimates of galaxy formation and evolution .Using results from the Sloan Digitial Sky Survey ( SDSS ) , we study the spread of spacecraft galaxies around distant galaxies . We see that satellites are more likely to lying along the main axes of the hosts than they are to lying along either the minor axes or randomly oriented lines through space .This result holds true over a broad variety of host characteristics including luminosity , color , morphological class , regional environmental density , and redshift range . Figure 1 : An example of how we define the orientation of each host s halo relative to its position angle .Here , the blue line displays the projected major axis of the host while the red dashed line indicates the direction perpendicular to it .",
        "rewrite_text": "**Title:** Anisotropic Locations of Satellite Galaxies: Insights into the Orientations of Galaxies within Their Dark Matter Halos\n\n**Abstract:** In this study, we investigate the anisotropic distribution of satellite galaxies surrounding distant galaxies, utilizing data from the Sloan Digital Sky Survey (SDSS). Our analysis reveals a significant tendency for satellite galaxies to orbit along the principal axes of their host galaxies, showing no preference for alignment with minor axes or random orientations. This finding is consistent across a variety of host characteristics, including luminosity, color, morphology, environmental density, and redshift range. Notably, the alignment between satellite galaxies and the major axes of their hosts remains evident even when we focus exclusively on recently accreted satellites. These observations imply that dark matter halos may possess a triaxial ellipsoidal shape, with their orientations closely aligned to the structures of the central galaxies they encompass. Furthermore, our results indicate that this alignment effect becomes more pronounced in lower mass systems. The implications of our findings are significant, offering new constraints on theoretical models of galaxy formation and evolution. By analyzing the spatial distribution of satellite galaxies, we provide insights into the underlying dynamics of galaxy interactions and the influence of dark matter on galactic structures. Our work contributes to a deeper understanding of the relationship between satellite galaxies and their hosts, highlighting the importance of dark matter halo geometry in shaping the observed anisotropies. Figure 1 illustrates our methodology for defining the orientation of each host's halo in relation to its position angle, with the blue line representing the projected major axis and the red dashed line indicating the perpendicular direction. This comprehensive assessment enhances our knowledge of galaxy formation processes and the role of dark matter in the universe.",
        "ori-fast-z-score": -0.7863336509949341,
        "water-fast-z-score": 5.316456139417774,
        "rewrite-fast-z-score": 1.4117731575135795
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Epitaxial graphene .\nAbstract:\nEpitaxial growth is the process by which atoms are deposited on top of an existing crystal structure, forming new layers that grow in registry with each other.  Epitaxy can be used to produce thin films and superlattices for electronic devices.   Graphene epitaxy refers to the deposition of carbon atoms onto a substrate such as silicon carbide (SiC) or gallium nitride (GaN).   The resulting material has many properties similar to those of single-layered graphene but also exhibits some differences due to interactions between the different atomic planes.   ... Graphene epitaxy is the deposition of carbon atoms on a substrate such as SiC or GaN. This results in a material with many properties similar to those found in single layer graphene, but there are also important differences arising from interlayer interactions. In this review we discuss recent progress towards understanding these effects using scanning probe microscopy techniques including STM, AFM and Raman spectroscopy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Epitaxial graphene . Abstract : Epitaxial growth is the process by which elements are deposited on top of an established crystal composition , forming fresh layers that grow in registry with each other .Epitaxy can be used to produce thin films and superlattices for electronic systems . Graphene epitaxy refers to the precipitation of carbon atoms onto a substrate such as silicon carbide ( SiC ) or gallium nitride ( GaN ) .The produced material has various properties similar to those of single - layered graphene but also exhibits some variations owing to interactions between the different atomic planes . . . . Graphene epitaxy is the precipitation of carbon atoms on a substrate such as SiC or GaN .This results in a substance with many properties similar to those observed in single surface graphene , but there are also important differences arising from interlayer relationships . In this review we explain latest advances towards studying these phenomena using scan probe microscopy methods using STM , AFM and Raman spectroscopy .",
        "rewrite_text": "Title: Epitaxial Graphene\n\nAbstract: Epitaxial growth is a sophisticated process where materials are deposited onto a pre-existing crystalline substrate, resulting in new layers that maintain a coherent alignment with the underlying structure. This technique is pivotal in the fabrication of thin films and superlattices, which are essential for advanced electronic applications. In the context of graphene, epitaxial growth involves the deposition of carbon atoms onto substrates such as silicon carbide (SiC) or gallium nitride (GaN). The resulting material exhibits many characteristics akin to those of monolayer graphene; however, it also presents notable differences due to the interactions between multiple atomic layers. These interlayer interactions can significantly influence the electronic, optical, and mechanical properties of the material.\n\nIn this review, we delve into the latest advancements in the study of epitaxial graphene, focusing on the application of scanning probe microscopy techniques, including Scanning Tunneling Microscopy (STM), Atomic Force Microscopy (AFM), and Raman spectroscopy. These methods have proven invaluable in elucidating the structural and electronic properties of epitaxial graphene, enabling researchers to gain deeper insights into the effects of layer interactions and the overall quality of the material. By examining the nuances of epitaxial growth and the resultant properties of graphene, this article aims to provide a comprehensive overview of the current state of research in this field, highlighting both the potential applications and the challenges that remain in the pursuit of high-quality epitaxial graphene for next-generation electronic devices.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 6.723674011118638,
        "rewrite-fast-z-score": -0.29277002188455997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evidence for a planetary companion around a nearby young star .\nAbstract:\nWe report the detection of periodic radial velocity variations in the spectrum of the K2V dwarf GJ 436, which are consistent with those expected for an orbiting planet. The period is 3.2 days and the semi-amplitude is about 30 m/sec. We also find evidence that this signal may be modulated on timescales longer than one year by another component whose mass we estimate to be at least 0.1 M⊕. This system has been extensively studied over many years as it lies close (5 pc) to our Sun but was not previously known to host any planets. It is therefore particularly interesting because its properties can now be compared directly with theoretical models of formation and evolution. \n \n Keywords: Planetary systems - Formation, Solar System\n\nIntroduction\n\nThe discovery of extrasolar planets has led to new insights into how planetary systems form and evolve. However, most exoplanets have been found using indirect techniques such as transit photometry or Doppler spectroscopy. These methods provide information only about the orbital parameters of the planet(s), while direct imaging provides additional constraints on their physical characteristics. In particular, high contrast imaging allows us to measure the masses of companions down to very low levels of flux ratio relative to their parent stars.\n\nIn recent years there has been significant progress towards achieving high-contrast imaging capabilities required to detect Earth-like planets around nearby stars. For example, the Gemini Planet Imager (GPI; Macintosh et al., 2014) , SPHERE (Beuzit et al., 2008) and SCExAO (Jovanovic et al., 2015) instruments will soon begin operation on 8-10 m class telescopes. These facilities offer unprecedented sensitivity and angular resolution, allowing them to probe regions closer to the central star where terrestrial planets are more likely to exist. \nHowever, these observatories operate under different conditions and use different technologies so it remains unclear what performance they will achieve once commissioned.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evidence for a planetary companion around a neighboring young star . Abstract : We report the observation of periodic radial speed shifts in the spectrum of the K2V dwarf GJ 436 , which are compatible with those expected for an orbiting planet .The period is 3 . 2 days and the semi - frequency is about 30 m / sec . We additionally find proof that this signal might be modulated on timescales greater than one decade by another component whose mass we estimate to be at least 0 . 1 M⊕ .This system has been heavily explored over numerous years as it lies close ( 5 pc ) to our Sun but was not originally seen to host any planets . It is consequently especially interesting because its properties can now be contrasted directly with theoretical theories of formation and evolution .Keywords : Planetary systems - Formation , Solar System Introduction The observation of extrasolar stars has led to fresh insights into how planetary structures structure and evolve . However , most exoplanets have been detected use indirect approaches such as transit photometry or Doppler spectroscopy .These methods provide information only about the orbital characteristics of the planet ( s ) , while direct scanning provides additional constraints on their structural traits . In particular , large contrast imaging allows us to measure the masses of companions down to very low levels of flux ratio compared to their father planets .In recent years there has been significant progress towards reaching large - contrast imaging skills necessary to identify Earth - like planets around nearby planets . For instance , the Gemini Planet Imager ( GPI ; Macintosh et al . , 2014 ) , SPHERE ( Beuzit et al . , 2008 ) and SCExAO ( Jovanovic et al . , 2015 ) instruments will soon begin service on 8 - 10 m class telescopes .These systems allow extraordinary sensitivity and spatial resolution , allowing them to probe regions nearer to the main star where terrestrial planets are more likely to appear . However , these observatories run under various circumstances and use different technologies so it remains unsure what performance they will achieve once commissioned .",
        "rewrite_text": "**Title:** Evidence for a Planetary Companion Around a Nearby Young Star\n\n**Abstract:** In this study, we present compelling evidence for the presence of a planetary companion orbiting the K2V dwarf star GJ 436, located just 5 parsecs from our solar system. Our observations reveal periodic shifts in the star's radial velocity, which align with the expected signatures of an orbiting planet. The detected period of these shifts is approximately 3.2 days, with a semi-amplitude of around 30 m/s. Furthermore, we have identified indications that this radial velocity signal may be influenced by an additional component, potentially modulating the observed shifts over timescales exceeding a decade. Our mass estimation for this secondary component suggests it is at least 0.1 times the mass of Earth (M⊕). \n\nThe GJ 436 system has been the focus of extensive research due to its proximity to our Sun, yet it was previously thought to be devoid of planetary bodies. This discovery is particularly significant as it allows for a direct comparison of the system's characteristics with existing theoretical models of planetary formation and evolution. The study of extrasolar planetary systems has greatly advanced our understanding of planetary structure and dynamics, primarily through indirect detection methods such as transit photometry and Doppler spectroscopy. While these techniques provide valuable insights into the orbital parameters of exoplanets, they do not yield detailed information about the physical properties of the planets themselves. \n\nRecent advancements in high-contrast imaging techniques, exemplified by instruments like the Gemini Planet Imager (GPI), SPHERE, and SCExAO, have enhanced our ability to detect and characterize exoplanets, particularly those resembling Earth. These state-of-the-art observatories, operating on 8-10 meter class telescopes, offer exceptional sensitivity and spatial resolution, enabling the exploration of regions closer to host stars where terrestrial planets are more likely to exist. However, the varying operational conditions and technological frameworks of these observatories raise questions about their performance once fully operational. This research not only contributes to our understanding of GJ 436 but also enriches the broader discourse on planetary system formation and evolution. \n\n**Keywords:** Planetary systems, Formation, Solar System.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 8.594446819256738,
        "rewrite-fast-z-score": -0.48038446141526137
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Higgs boson production at the LHC: transverse-momentum resummation and rapidity dependence .\nAbstract:\nWe present results for Higgs-boson production in association with jets, including next-to-leading-order (NLO) corrections to both the total cross section and differential distributions as well as soft-gluon resummation up to NNLL accuracy. We also study the impact on these observables of varying the renormalization scale used in the perturbative expansion by considering two different prescriptions. The calculations are performed using the NNPDF2.3QED parton distribution functions and we use the CT14NNLO PDF set to estimate theoretical uncertainties due to missing higher orders. Our predictions are compared against data collected by ATLAS and CMS experiments at centre-of-mass energies of 8 TeV and 13 TeV. \nIntroduction\n\nThe Large Hadron Collider (LHC), located at CERN near Geneva, Switzerland, is currently operating at its highest energy ever achieved during Run 2. In this regime, it will be possible to produce new particles predicted by extensions of the Standard Model such as supersymmetric partners or extra gauge bosons. One of the most important goals of the LHC program is therefore to discover evidence of physics beyond the Standard Model through precision measurements of Standard-Model processes. Among them, Higgs-boson production plays an essential role since it provides crucial information about the mechanism responsible for electroweak symmetry breaking. \n \n At leading order (LO), Higgs-boson production can occur via gluon fusion, which proceeds mainly through heavy-quark loops, or vector-boson-fusion (VBF). Higher-order QCD corrections have been computed analytically  1–3  and numerically  4–6  . They increase significantly the LO prediction  7,8  , especially when going towards high values of the Higgs-boson mass mH  9  . This effect has been observed experimentally  10  . \nIn addition to the large logarithmic terms that appear at each order in perturbation theory, there exist subleading contributions associated with collinear emissions of soft gluons off initial-state quarks  11, 12  . These effects cannot be captured within fixed-order computations but must instead be included in all-order resum",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Higgs boson development at the LHC : transverse - momentum resummation and rapidity dependence . Abstract : We report findings for Higgs - boson development in association with jets , using next - to - leading - order ( NLO ) corrections to both the total cross area and integral distributions as well as hard - gluon resummation up to NNLL accuracy .We additionally study the impact on these observables of differing the renormalization scale used in the perturbative expansion by using two different prescriptions . The calculations are performed using the NNPDF2 . 3QED parton distribution functions and we utilize the CT14NNLO PDF set to estimate theoretical uncertainties due to missing higher orders .Our predictions are compared against data taken by ATLAS and CMS observations at centre - of - mass energies of 8 TeV and 13 TeV . Introduction The Large Hadron Collider ( LHC ) , located at CERN near Geneva , Switzerland , is currently functioning at its highest power ever achieved during Run 2 .In this regime , it will be possible to produce new objects anticipated by modifications of the Standard Model such as supersymmetric partners or additional gauge bosons . One of the most important objectives of the LHC program is consequently to find proof of science beyond the Standard Model through precision observations of Standard - Model processes .Among them , Higgs - boson development holds an essential part since it gives crucial data about the process controlling for electroweak symmetry breaking . At leading order ( LO ) , Higgs - boson development can occur via gluon fusion , which continues mainly through light - quark loops , or vector - boson - fusion ( VBF ) .Higher - order QCD corrections have been computed analytically 1 – 3 and numerically 4 – 6 . They raise considerably the LO prediction 7 , 8 , particularly when going towards high values of the Higgs - boson weight mH 9 .This phenomenon has been observed experimentally 10 . In addition to the huge logarithmic terms that appear at each order in perturbation theory , there exist subleading contributions associated with collinear emissions of soft gluons off initial - state quarks 11 , 12 .These effects never be captured within fixed - order computations but need rather be included in all - order resum",
        "rewrite_text": "**Title:** Higgs Boson Production at the LHC: Transverse Momentum Resummation and Rapidity Dependence\n\n**Abstract:** This study presents findings on the production of Higgs bosons in association with jets, employing next-to-leading order (NLO) corrections to both the total cross-section and integral distributions, alongside hard-gluon resummation up to next-to-next-to-leading logarithmic (NNLL) accuracy. We investigate how variations in the renormalization scale, applied in the perturbative expansion, influence these observables by utilizing two distinct prescriptions. The calculations are conducted using the NNPDF2.3QED parton distribution functions, while the CT14NNLO PDF set is employed to assess theoretical uncertainties stemming from neglected higher-order contributions. Our theoretical predictions are juxtaposed with experimental data collected by the ATLAS and CMS collaborations at center-of-mass energies of 8 TeV and 13 TeV.\n\nThe Large Hadron Collider (LHC) at CERN, Switzerland, is currently operating at unprecedented energy levels during its Run 2 phase, enabling the potential production of new particles predicted by extensions to the Standard Model, such as supersymmetric partners or additional gauge bosons. A primary goal of the LHC program is to uncover evidence for physics beyond the Standard Model through meticulous measurements of Standard Model processes. Among these processes, Higgs boson production is particularly significant as it provides vital insights into the mechanisms responsible for electroweak symmetry breaking. At leading order (LO), Higgs boson production predominantly occurs via gluon fusion, primarily through loops involving light quarks, or through vector boson fusion (VBF). Previous studies have computed higher-order QCD corrections both analytically and numerically, which significantly enhance the LO predictions, especially at high Higgs boson masses. This enhancement has been corroborated by experimental observations. Furthermore, the presence of large logarithmic terms at each perturbative order, along with subleading contributions from collinear emissions of soft gluons from initial-state quarks, necessitates a comprehensive all-order resummation approach, as these effects cannot be adequately captured by fixed-order calculations.",
        "ori-fast-z-score": -0.16666666666666666,
        "water-fast-z-score": 6.9518894642598905,
        "rewrite-fast-z-score": -1.0606601717798212
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Hypercube of Life: How Protein Stability Imposes Limits on Organism Complexity and Speed of Molecular Evolution .\nAbstract:\nThe hypercube model is used to describe the evolution of life in terms of its complexity, speed, and stability.  The model shows that there are limits imposed by protein stability on how complex an organism can be as well as how fast it evolves molecularly.   It also suggests that these limits may have been reached during the Cambrian explosion about 540 million years ago when most animal phyla appeared simultaneously. This article describes the hypercube model and presents some examples of how it has been applied to understand evolutionary processes at different levels of organization ranging from genes to ecosystems. In this article we present a new approach for understanding the evolution of life based on the concept of the hypercube (1). We argue that the evolution of life can be described in three dimensions: complexity, speed, and stabilization. These three dimensions represent key aspects of biological systems that evolve over time. For example, organisms become more complex through the addition of new components such as organs or tissues; they evolve faster if their genetic variation increases; and they become more stable if mutations do not cause them to die prematurely. Figure 1 illustrates our view of the evolution of life using the hypercube model. Each vertex represents one possible state of living matter with respect to each dimension. As shown in Fig. 1A , the number of vertices along any given axis depends on the level of resolution chosen. At higher resolutions, the number of states increases exponentially. For instance, if we consider only two states per dimension—simple versus complex, slow versus fast, unstable versus stable—the total number of possible combinations would be four (2 x 2 x 2 = 8), which corresponds to eight types of living matter. However, if we increase the resolution so that we now include four states per dimension—very simple versus simple versus complex versus very complex, very slow versus slow versus fast versus very fast,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Hypercube of Life : How Protein Stability Imposes Limits on Organism Complexity and Speed of Molecular Evolution . Abstract : The hypercube concept is utilized to explain the evolution of life in terms of its complexity , speed , and stability .The model shows that there are restrictions imposed by protein stability on how complex an organism can be as well as how soon it evolves molecularly . It additionally indicates that these limits might have been achieved during the Cambrian explosion about 540 million months previously when most animal phyla appeared simultaneously .This section explains the hypercube concept and provides some examples of how it has been used to explain evolutionary processes at different levels of organization ranging from genes to environments . In this article we present a new approach for studying the evolution of life based on the idea of the hypercube ( 1 ) .We argue that the evolution of life can be described in three dimensions : complexity , speed , and stabilization . These three dimensions represent crucial factors of biological organizations that develop over time .For instance , animals get more sophisticated through the adding of new components such as organs or tissues ; they develop faster if their genetic variation rises ; and they become more stable if mutations do not cause them to dying prematurely . Figure 1 illustrates our view of the evolution of life using the hypercube view .Each vertex depicts one possible state of living matter with regard to each dimension . As seen in Fig .1A , the number of vertices along any certain axis depends on the level of resolution picked . At higher resolutions , the number of states tends exponentially .For instance , if we consider only two states per dimension — simple versus complex , slow versus fast , unstable versus stable — the total number of possible combinations may be four ( 2 x 2 x 2 = 8 ) , which corresponds to eight types of living matter . However , if we increase the resolution so that we now include four states per dimension — very simple versus easy versus compound versus very complex , very slow versus slow versus fast versus very slow , . . .",
        "rewrite_text": "**Title:** The Hypercube of Life: How Protein Stability Imposes Limits on Organism Complexity and Speed of Molecular Evolution\n\n**Abstract:** This article introduces a novel framework for understanding the evolution of life through the lens of the hypercube model, which encompasses three critical dimensions: complexity, speed, and stability. By employing this model, we elucidate how protein stability constrains both the complexity of organisms and the pace of molecular evolution. Our analysis suggests that these constraints may have been particularly pronounced during the Cambrian explosion, approximately 540 million years ago, a period marked by the rapid emergence of diverse animal phyla. \n\nThe hypercube concept serves as a powerful tool for illustrating evolutionary processes across various biological levels, from genetic structures to ecological environments. Each dimension of the hypercube reflects essential characteristics of biological systems that evolve over time. For example, organisms tend to increase in complexity through the addition of new biological components, such as organs and tissues. Additionally, the rate of evolution accelerates with greater genetic variation, while stability is enhanced when mutations do not lead to premature mortality.\n\nIn our model, each vertex of the hypercube represents a distinct state of living matter, defined by its position along the three dimensions. As depicted in Figure 1, the number of vertices along any given axis is contingent upon the chosen resolution. At higher resolutions, the number of potential states increases exponentially. For instance, if we limit our analysis to two states per dimension—simple versus complex, slow versus fast, unstable versus stable—the total number of combinations yields eight distinct types of living organisms (2 x 2 x 2 = 8). However, by expanding our resolution to include four states per dimension—very simple, simple, complex, and very complex, as well as varying rates of speed and stability—we can uncover a more intricate landscape of evolutionary possibilities. This approach not only enhances our understanding of life's complexity but also provides insights into the evolutionary constraints shaped by protein stability.",
        "ori-fast-z-score": -0.07647191129018725,
        "water-fast-z-score": 6.135719910778963,
        "rewrite-fast-z-score": 0.6209204205650662
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How does Casimir energy fall? II. Gravitational acceleration of quantum vacuum energy .\nAbstract:\nWe consider the gravitational effect on the Casimir force between two parallel plates in an accelerating frame, and show that it is possible to obtain a repulsive force at short distances even if the initial state has no net charge or current density. We also discuss how this result can be obtained by using the concept of local energy-momentum tensor for electromagnetic fields. The possibility of obtaining such a repulsive force was first suggested by Yukawa in 1951 as a way to explain nuclear forces without introducing new particles. \n \n In recent years there have been several attempts to measure experimentally the Casimir force (see e.g., Refs.  1 - 6 ). However, all these experiments are performed with macroscopic objects which do not allow one to test directly the predictions of QED concerning the behavior of the vacuum fluctuations near boundaries  7, 8  . \n \n On the other hand, the measurement of the Casimir force between microscopic bodies would provide information about the properties of the vacuum  9  , and could lead to tests of QED  10  -  12  . \n \n It should be noted that the experimental determination of the Casimir force requires very accurate measurements because its magnitude is extremely small compared to gravity  13  . For example, the most precise experiment so far  6  gives only a relative accuracy of 0.1%. This means that any systematic error will dominate over statistical errors. Therefore, it is important to understand theoretically the dependence of the Casimir force upon external parameters like temperature  14  , pressure  15  , and velocity  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How does Casimir energy fall ? II .Gravitational acceleration of quantum vacuum energy . Abstract : We consider the gravitational impact on the Casimir force between two connected plates in an accelerating frame , and find that it is easy to obtain a repulsive force at short distances even if the first state has no net charge or current density .We additionally discuss how this consequence can be obtained by using the idea of local power - momentum tensor for electromagnetic fields . The possibility of finding such a repulsive force was first suggested by Yukawa in 1951 as a way to explain nuclear forces without introducing additional particles .In recent years there have been numerous attempts to measure experimentally the Casimir force ( saw e . g . , Refs . 1 - 6 ) .However , all these experiments are performed with macroscopic objects which do not enable one to test explicitly the assumptions of QED concerning the dynamics of the vacuum fluctuations near edges 7 , 8 . On the other hand , the observation of the Casimir force between microscopic bodies would offer information about the properties of the vacuum 9 , and could lead to tests of QED 10 - 12 .It should be mentioned that the empirical determination of the Casimir force requires very accurate measurements because its magnitude is incredibly small contrast to gravity 13 . For instance , the most accurate experiment so far 6 offers only a relative sensitivity of 0 . 1 % .This implies that any systematic error will dominate over empirical errors . Therefore , it is important to realize theoretically the dependence of the Casimir force upon external parameters like pressure 14 , pressure 15 , and speed 16 .",
        "rewrite_text": "**Title: How Does Casimir Energy Fall? II. Gravitational Acceleration of Quantum Vacuum Energy**\n\n**Abstract:** In this study, we investigate the gravitational effects on the Casimir force between two parallel plates situated within an accelerating frame. Our findings reveal that it is feasible to generate a repulsive force at short distances, even when the initial state lacks any net charge or current density. We further explore the implications of this phenomenon through the lens of the local power-momentum tensor associated with electromagnetic fields. The concept of a repulsive force in this context was initially proposed by Yukawa in 1951 as a potential explanation for nuclear forces, circumventing the need for additional particles. \n\nIn recent years, there has been a surge of experimental efforts aimed at measuring the Casimir force, as documented in various studies (see Refs. 1-6). However, these experiments predominantly involve macroscopic objects, which limits the ability to rigorously test the predictions of Quantum Electrodynamics (QED) regarding the dynamics of vacuum fluctuations near material boundaries (Refs. 7-8). Conversely, detecting the Casimir force between microscopic entities could yield valuable insights into the characteristics of the vacuum and facilitate more stringent tests of QED (Refs. 9-12).\n\nIt is important to note that accurately determining the Casimir force presents significant challenges due to its extremely small magnitude compared to gravitational forces (Ref. 13). For example, the most precise experiment conducted to date (Ref. 6) achieves only a relative sensitivity of 0.1%. This level of sensitivity indicates that systematic errors may overshadow empirical inaccuracies. Consequently, it is crucial to theoretically understand how the Casimir force is influenced by external factors such as pressure (Refs. 14-15) and velocity (Ref. 16). This theoretical framework will enhance the interpretation of experimental results and contribute to the broader understanding of quantum vacuum phenomena.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 6.350433056099402,
        "rewrite-fast-z-score": 0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mixed Hyperbolic - Second-Order Parabolic Formulations of General Relativity .\nAbstract:\nWe present mixed hyperbolic-second-order parabolic formulations for the Einstein field equations in vacuum and electrovacuum, which are suitable to be solved numerically by means of finite difference methods on Cartesian grids with adaptive mesh refinement (AMR). The formulation is based on an auxiliary variable that allows us to split the evolution system into two subsystems, one hyperbolic and another second-order parabolic. We show how this splitting can be used to construct stable numerical schemes using standard techniques such as Kreiss-Oliger dissipation or artificial viscosity. In addition we discuss several issues related to the implementation of these schemes within the AMR framework provided by the Cactus Computational Toolkit. Finally, we present some preliminary results obtained with our new code. This work was supported by CONACyT grant No. 164710. Keywords: Adaptive Mesh Refinement, Numerical relativity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mixed Hyperbolic - Second - Order Parabolic Formulations of General Relativity . Abstract : We create blended hyperbolic - second - order parabolic formulations for the Einstein field equations in vacuum and electrovacuum , which are suitable to be answered numerically by means of finite difference methods on Cartesian grids with adaptive mesh refinement ( AMR ) .The characterization is based on an auxiliary variable that enables us to split the evolution system into two subsystems , one hyperbolic and another second - order parabolic . We see how this splitting can be used to build stable numerical schemes using conventional methods such as Kreiss - Oliger dissipation or artificial viscosity .In addition we explain several topics related to the implementation of these schemes within the AMR framework given by the Cactus Computational Toolkit . Finally , we present some preliminary results acquired with our new code .This project was supported by CONACyT grant No . 164710 .Keywords: Adaptive Mesh Refinement, Numerical relativity",
        "rewrite_text": "Title: Mixed Hyperbolic-Second-Order Parabolic Formulations of General Relativity\n\nAbstract: In this study, we develop innovative mixed hyperbolic-second-order parabolic formulations for the Einstein field equations, applicable in both vacuum and electrovacuum scenarios. These formulations are designed to facilitate numerical solutions using finite difference methods on Cartesian grids, enhanced by adaptive mesh refinement (AMR) techniques. Our approach leverages an auxiliary variable that allows us to decompose the evolution system into two distinct subsystems: one hyperbolic and the other second-order parabolic. This decomposition is crucial as it enables the construction of stable numerical schemes, employing established methods such as Kreiss-Oliger dissipation and artificial viscosity to ensure accuracy and stability in simulations.\n\nFurthermore, we delve into various aspects of implementing these numerical schemes within the AMR framework provided by the Cactus Computational Toolkit. This toolkit is instrumental in managing the complexities associated with adaptive mesh refinement, which is essential for resolving the intricate structures that arise in general relativity. We also share preliminary results obtained from our newly developed code, showcasing the effectiveness of our formulations in capturing the dynamics of spacetime under different conditions.\n\nThe findings of this research contribute to the field of numerical relativity by providing a robust framework for simulating general relativistic phenomena. Our work is supported by CONACyT grant No. 164710, which underscores the importance of funding in advancing scientific research. The keywords associated with this study include Adaptive Mesh Refinement and Numerical Relativity, reflecting the core themes and methodologies employed in our investigation.",
        "ori-fast-z-score": 1.3643820804812932,
        "water-fast-z-score": 4.923659639173309,
        "rewrite-fast-z-score": 0.6767155423319645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The IC1396N proto-cluster at a scale of 250 AU .\nAbstract:\nWe present new near-infrared (NIR) observations of the young stellar cluster IC 1396 N, located in the Orion Nebula Cluster region. The data were obtained with the adaptive optics system NAOS-CONICA on the VLT telescope and cover an area of 0.5 arcmin2 around the central star HD 37022. We detect more than 100 point sources down to Ks = 18 mag within this field-of-view. Using these data we have constructed colour-magnitude diagrams for different areas inside our field-of-view. From these CMDs we find that there are two populations of stars which can be separated by their position in the diagrams: one population is redder and fainter while another has bluer colours and brighter magnitudes. These results suggest that the first group consists mainly of low mass pre-main sequence stars surrounded by circumstellar disks, whereas the second group contains mostly high mass main-sequence stars without any surrounding material.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The IC1396N proto - cluster at a scale of 250 AU . Abstract : We report new near - infrared ( NIR ) observations of the young stellar cluster IC 1396 N , located in the Orion Nebula Cluster region .The data were obtained with the adaptive optics network NAOS - CONICA on the VLT telescope and cover an area of 0 . 5 arcmin2 around the central star HD 37022 . We detect more than 100 point sources down to Ks = 18 mag within this field - of - view .Using these information we have created colour - magnitude diagrams for different areas inside our field - of - view . From these CMDs we find that there are two communities of stars which can be apart by their placement in the diagrams : one community is redder and fainter while another has bluer colours and darker magnitudes .These data suggest that the first class consists primarily of low weight pre - principal sequence stars surrounded by circumstellar disks , whereas the second set contains primarily high mass primary - sequence stars without any surrounding material .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the young stellar cluster IC 1396N, situated within the Orion Nebula Cluster region. Utilizing the adaptive optics system NAOS-CONICA on the Very Large Telescope (VLT), our observations encompass an area of 0.5 arcmin² centered around the star HD 37022. In this field of view, we identified over 100 point sources, reaching a sensitivity limit of Ks = 18 mag. To analyze these sources, we constructed color-magnitude diagrams (CMDs) for various regions within our observational area. The CMDs reveal the presence of two distinct stellar populations, differentiated by their positions in the diagrams. The first population is characterized by redder colors and fainter magnitudes, indicating that these stars are primarily low-mass pre-main sequence stars that are likely surrounded by circumstellar disks. In contrast, the second population exhibits bluer colors and brighter magnitudes, suggesting that these stars are predominantly high-mass main-sequence stars devoid of surrounding material. Our findings contribute to the understanding of the stellar formation processes within the IC 1396N proto-cluster and highlight the diversity of stellar populations present in this region. The implications of these observations extend to the broader context of star formation in clustered environments, providing insights into the evolutionary stages of stars and the role of circumstellar disks in the development of young stellar objects. This study underscores the importance of high-resolution NIR observations in unraveling the complexities of stellar clusters and their constituent stars.",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 4.4174102722651325,
        "rewrite-fast-z-score": -0.5669467095138409
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Charge-Ordering Phenomena in One-Dimensional Solids .\nAbstract:\nWe study the charge ordering phenomena in one-dimensional solids by using the exact diagonalization method and density matrix renormalization group (DMRG) technique. We find that there are two types of charge orderings, i.e., stripe-like and checkerboard-like orders depending on the electron filling factor n. The former is realized for 0 < n < 1 while the latter appears at half-filling with spin degeneracy lifted. In addition to these ordered states we also observe an exotic state where electrons form pairs without any net charge. This paired state can be regarded as a precursor of superconductivity. Finally, we discuss possible experimental realizations of our results. Introduction:-In recent years much attention has been paid to the physics of low dimensional systems such as carbon nanotubes  1  , semiconductor nanowires  2  , quantum wires  3  etc.. These materials have attracted considerable interest because they provide us with unique opportunities to explore novel physical properties which cannot exist in conventional three-dimensional bulk materials  4  . For example, it was predicted theoretically  5  and observed experimentally  6  that carbon nanotubes show metallic behavior even though their diameter is comparable or smaller than the Fermi wavelength. Another interesting feature of low dimensional systems is that various kinds of electronic phases may appear due to strong correlation effects  7, 8  .\nOne of the most important issues in this field is how to control the electronic phase diagram of low dimensional systems. It should be noted here that the electronic structure strongly depends not only on the geometry but also on the chemical composition  9  . Therefore, if we could change the chemical composition of low dimensional systems, then we would expect new electronic phases to emerge. Recently, several groups succeeded in synthesizing low dimensional compounds whose chemical compositions were controlled precisely  10 -12  . As a result, many fascinating phenomena have been discovered  13 -19  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Charge - Ordering Phenomena in One - Dimensional Solids . Abstract : We research the charge ordering phenomena in one - dimensional solids by using the exact diagonalization technique and density matrix renormalization group ( DMRG ) method .We see that there are two forms of charge orderings , i . e . , stripe - like and checkerboard - like orders depending on the electron filling factor n . The first is realized for 0 < n < 1 while the former occurs at half - filling with spin degeneracy lifted . In addition to these ordered states we also observe an exotic state where electrons form couples without any net charge .This paired state can be regarded as a precursor of superconductivity . Finally , we explain possible experimental realizations of our findings .Introduction : - In recent years much attention has been paid to the physics of low dimensional devices such as carbon nanotubes 1 , semiconductor nanowires 2 , quantum wires 3 etc . . These substances have garnered considerable interest because they give us with special opportunities to examine novel physical properties which cannot appear in standard three - dimensional bulk ceramics 4 .For instance , it was predicted theoretically 5 and detected experimentally 6 that carbon nanotubes exhibit metallic behavior even though their diameter is identical or smaller than the Fermi width . Another curious characteristics of low dimensional systems is that various kinds of electronic phases often emerge due to powerful correlation effects 7 , 8 .One of the most important problems in this area is how to affect the electronic phase diagram of low dimensional systems . It should be mentioned here that the electronic structure strongly depends not only on the topology but also on the chemical composition 9 .Therefore , if we could shift the chemical composition of lowest dimensional systems , then we may expect fresh electronic phases to emerge . Recently , various groups succeeded in synthesizing low dimensional molecules whose chemical compositions were determined precisely 10 - 12 .As a result , various fascinating phenomena have been observed 13 - 19 .",
        "rewrite_text": "**Title:** Charge Ordering Phenomena in One-Dimensional Solids\n\n**Abstract:** This study investigates the charge ordering phenomena in one-dimensional solids through the application of exact diagonalization techniques and the density matrix renormalization group (DMRG) method. Our findings reveal the existence of two distinct forms of charge ordering: stripe-like and checkerboard-like configurations, which are dependent on the electron filling factor (n). The stripe-like order is observed for filling factors in the range of 0 < n < 1, while the checkerboard order emerges at half-filling, accompanied by the lifting of spin degeneracy. Beyond these conventional ordered states, we also identify a novel exotic state characterized by the formation of electron pairs that exhibit no net charge. This paired state may serve as a precursor to superconductivity, suggesting intriguing implications for future research. Furthermore, we discuss potential experimental realizations of our theoretical predictions, highlighting the relevance of our findings in the context of low-dimensional materials.\n\n**Introduction:** In recent years, significant attention has been directed towards the physics of low-dimensional devices, such as carbon nanotubes, semiconductor nanowires, and quantum wires. These materials have attracted considerable interest due to their unique properties, which are not typically observed in standard three-dimensional bulk materials. For instance, theoretical predictions and experimental observations have confirmed that carbon nanotubes can exhibit metallic behavior, even when their diameters are comparable to or smaller than the Fermi wavelength. Another intriguing aspect of low-dimensional systems is the emergence of various electronic phases driven by strong correlation effects. A critical question in this field is how to manipulate the electronic phase diagram of these systems. It is important to note that the electronic structure is influenced not only by the topology of the material but also by its chemical composition. Therefore, by altering the chemical composition of low-dimensional systems, we can anticipate the emergence of novel electronic phases. Recently, several research groups have successfully synthesized low-dimensional molecules with precisely controlled chemical compositions, leading to the observation of a range of fascinating phenomena.",
        "ori-fast-z-score": -1.0120486274099798,
        "water-fast-z-score": 6.674894521074462,
        "rewrite-fast-z-score": 1.8791150700070722
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Planets around evolved intermediate-mass stars. I. Two substellar companions in the open clusters NGC 2423 and NGC 4349 .\nAbstract:\nWe report on two new brown dwarf candidates discovered by direct imaging with NACO/VLT at distances of ~100 AU to their host star, which are members of open clusters NGC 2423 (M67) and NGC 4349. The first object is located close to the cluster center and has an estimated mass between 0.03-0.07 M . It was found as part of our survey for low-mass companions to nearby young solar-type stars. We also present follow-up observations of this candidate using adaptive optics techniques that confirm its substellar nature. \n \n The second object lies outside the core radius of the cluster and has an estimated mass below 0.01 M . This companion may be either a planetary or very-low-mass stellar companion depending on whether it formed through gravitational instability or fragmentation during cloud collapse. Both objects have masses well above those predicted by current models of planet formation via disk accretion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Planets around evolved intermediate - mass stars . I .Two substellar companions in the open complexes NGC 2423 and NGC 4349 . Abstract : We report on two new brown dwarf candidates discovered by direct imaging with NACO / VLT at distances of ~ 100 AU to their host star , which are members of open complexes NGC 2423 ( M67 ) and NGC 4349 .The first object is situated close to the cluster center and has an estimated mass between 0 . 03 - 0 . 07 M . It was found as part of our survey for low - mass companions to nearby young solar - class stars .We additionally post follow - up observations of this candidate using adaptive optics techniques that prove its substellar nature . The second object lies outside the primary distance of the cluster and has an estimated mass below 0 . 01 M .This companion might be either a planetary or very - low - mass stellar companion depending on whether it created through gravity instability or fragmentation during cloud collapse . Both bodies have masses well above those predicted by current theories of planet development via disk accretion .",
        "rewrite_text": "We present findings on two newly identified brown dwarf candidates, discovered through direct imaging techniques utilizing NACO at the Very Large Telescope (VLT). These candidates are located approximately 100 AU from their respective host stars and are associated with the open clusters NGC 2423 (M67) and NGC 4349. The first candidate is positioned near the center of the cluster and has an estimated mass ranging from 0.03 to 0.07 solar masses (M☉). This discovery is part of our ongoing survey aimed at identifying low-mass companions to nearby young solar-type stars. Follow-up observations employing adaptive optics have confirmed the substellar classification of this object. The second candidate is situated beyond the primary distance of its cluster and is estimated to have a mass of less than 0.01 M☉. The nature of this companion remains ambiguous; it could either be classified as a planetary body or a very low-mass stellar companion, contingent upon its formation mechanism—whether it originated from gravitational instability or fragmentation during the collapse of a molecular cloud. Notably, both candidates possess masses that exceed the predictions made by current models of planetary formation through disk accretion processes. This research contributes to our understanding of the formation and evolution of substellar objects around evolved intermediate-mass stars, highlighting the complexities and potential pathways of companion formation in stellar environments.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 3.624412178045377,
        "rewrite-fast-z-score": -0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Suppression of growth by multiplicative white noise in a parametric resonant system .\nAbstract:\nWe study the effect of multiplicative white noise on the dynamics of a nonlinear oscillator driven by an external periodic force and subject to dissipation. We show that, depending on the parameters characterizing the driving field and the strength of the noise, there are two different regimes for the evolution of the amplitude of the oscillator: (i) exponential decay or saturation at some finite value, (ii) slow algebraic decay with time. In both cases we find analytical expressions for the corresponding relaxation rates. The results obtained can be applied to describe various physical phenomena such as damped Josephson junctions, semiconductor lasers, etc.. PACS numbers: 05.45.Mt, 05.40.+j, 05.60.Fh \nI. INTRODUCTORY REMARK\nIn this work we consider the following stochastic differential equation describing the motion of a particle in one dimension under the action of friction and a periodically varying potential V(x), which is modulated by a random process W(t)\nwhere x = {x(t), t ∈ R} denotes the position coordinate of the particle; γ > 0 stands for the coefficient of friction; f (x) describes the deterministic part of the potential energy, while g(x) represents its random component. Hereinafter, we assume that f , g : R → R + . It should be noted that Eq. (1) has been derived within several models used to describe real-world systems  1  -  4  .\nThe main goal of our investigation is to analyze how the presence of additive noise affects the behavior of solutions of Eq.\n(1). To do so, it will be convenient to rewrite Eq. (1) \nHereafter, we shall refer to Eqs. \nII. MAIN RESULTS\n\nA. Exponential decay/saturation regime\nLet us first focus on the case when the function f satisfies the condition |f ′′′ (x)| < C 3 for all x ∈ R, where C 3 is a positive constant. Then, using standard arguments based on the Ito formula  6  , one can easily prove that any solution of Eq. (",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Suppression of growth by multiplicative white sound in a parametric resonant system . Abstract : We research the impact of multiplicative white sound on the dynamics of a nonlinear oscillator driven by an external periodic force and subject to dissipation .We see that , depending on the variables characterizing the driving field and the strength of the noise , there are two different regimes for the evolution of the amplitude of the oscillator : ( i ) exponential decay or saturation at some finite value , ( ii ) gradual algebraic decay with time . In both cases we find analytical expressions for the associated relaxation frequencies .The results derived can be applied to explain different mechanical phenomena such as damped Josephson junctions , semiconductor lasers , etc . . PACS numbers : 05 . 45 . Mt , 05 . 40 . + j , 05 . 60 . Fh I . INTRODUCTORY REMARK In this study we imagine the following stochastic differential equation explaining the movement of a particle in one dimension under the action of tension and a periodically varying potential V ( x ) , which is modulated by a random process W ( t ) where f = { x ( t ) , t ∈ R } indicates the position coordinate of the particle ; γ > 0 stands for the coefficient of tension ; f ( x ) refers the deterministic part of the potential electricity , while g ( x ) indicates its random component .Hereinafter , we suppose that f , g : R → R + . It should be mentioned that Eq .( 1 ) has been constructed within several models used to explain real - time systems 1 - 4 . The main goal of our inquiry is to analyze how the presence of additive noise impacts the actions of solutions of Eq .( 1 ) . To do so , it will be appropriate to rewrite Eq .(1) \nHereafter, we shall refer to Eqs.II.MAIN RESULTS A . Exponential decay / saturation regime Let us first focus on the case when the function f satisfies the condition | f ′ ′ ′ ( x ) | < C 3 for all x ∈ R , where C 3 is a positive constant . Then , using conventional statements based on the Ito formula 6 , one can easily prove that any solution of Eq .(",
        "rewrite_text": "**Title:** Suppression of Growth by Multiplicative White Noise in a Parametric Resonant System\n\n**Abstract:** This study investigates the effects of multiplicative white noise on the dynamics of a nonlinear oscillator that is influenced by an external periodic force and experiences dissipation. Our findings reveal that the oscillator's amplitude evolution can be categorized into two distinct regimes, contingent upon the characteristics of the driving field and the intensity of the noise: (i) an exponential decay or a saturation at a finite amplitude, and (ii) a gradual algebraic decay over time. For both scenarios, we derive analytical expressions for the corresponding relaxation frequencies, which provide insight into the oscillator's behavior under the influence of noise. These results have significant implications for understanding various mechanical phenomena, including damped Josephson junctions and semiconductor lasers. \n\nIn our analysis, we formulate a stochastic differential equation that describes the motion of a particle in one dimension, subjected to tension and a periodically varying potential V(x), which is modulated by a random process W(t). Here, f = {x(t), t ∈ R} denotes the particle's position, γ > 0 represents the tension coefficient, while f(x) corresponds to the deterministic component of the potential, and g(x) signifies its stochastic counterpart. We assume that both f and g are functions mapping from R to R+. The equation has been developed based on several models that aim to elucidate real-time systems. The primary objective of our research is to assess how additive noise influences the solutions of the formulated equation. \n\nIn the subsequent sections, we will delve into the main results, beginning with the exponential decay/saturation regime. We focus on cases where the function f meets the condition |f'''(x)| < C₃ for all x ∈ R, where C₃ is a positive constant. Utilizing established principles derived from the Ito calculus, we demonstrate that any solution of the equation adheres to the predicted behaviors, thereby enhancing our understanding of the interplay between noise and oscillator dynamics.",
        "ori-fast-z-score": -1.044465935734187,
        "water-fast-z-score": 5.462792808001955,
        "rewrite-fast-z-score": 1.7407765595569784
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Calcium and synaptic dynamics underlying reverberatory activity in neuronal networks .\nAbstract:\nReverberation is an important feature of brain function, but its role remains unclear. We show that reverberation can be generated by the interplay between calcium signaling and short-term plasticity (STP) at excitatory synapses. In our model, STP leads to bursts of spikes which are followed by periods of low firing rate due to depletion of neurotransmitter vesicles. The resulting slow recovery of transmitter release causes a build-up of residual calcium concentration  Ca  res , leading to facilitation of glutamate release during subsequent bursts. This positive feedback loop generates sustained reverberatory activity with multiple time scales. Our results suggest that reverberation may play a key role in information processing within neural circuits. Reverberation is one of the most prominent features of brain function  1  . It has been observed across different species  2  -  4  as well as in various brain areas including hippocampus  5  , neocortex  6  , thalamus  7  , striatum  8  , cerebellar cortex  9  , olfactory bulb  10  , and retina  11  .\nDespite its ubiquity, however, it still remains unknown what functional roles reverberation plays in the brain  12  . One possibility is that reverberation serves as a mechanism for memory storage  13  or retrieval  14  . Another hypothesis suggests that reverberation could serve as a substrate for working memory  15  . Yet another idea is that reverberation might provide a means for temporal coding  16  . Finally, some studies have suggested that reverberation may simply reflect ongoing spontaneous activity  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Calcium and synaptic dynamics underlying reverberatory behavior in neuronal systems . Abstract : Reverberation is an important feature of cerebral activity , but its significance remains unsure .We see that reverberation can be triggered by the interplay between calcium signaling and low - term plasticity ( STP ) at excitatory synapses . In our model , STP results to bursts of spikes which are preceded by periods of poor fired rate due to depletion of neurotransmitter vesicles .The resulting slow recovery of transmitter release generates a build - up of residual calcium affinity Ca res , leading to facilitation of glutamate production during later bursts . This positive feedback loop generates sustained reverberatory behavior with various time ranges .Our results show that reverberation possibly play a key importance in information processing within neural pathways . Reverberation is one of the most notable features of cerebral function 1 .It has been observed across different species 2 - 4 as well as in different brain regions including hippocampus 5 , neocortex 6 , thalamus 7 , striatum 8 , cerebellar cortex 9 , olfactory bulb 10 , and retina 11 . Despite its ubiquity , however , it still remains obscure what functional functions reverberation plays in the brain 12 .One possibility is that reverberation provides as a system for memory processing 13 or retrieval 14 . Another hypothesis suggests that reverberation possibly provide as a substrate for working storage 15 .Yet another idea is that reverberation would offer a means for temporal coding 16 . Finally , some researchers have suggested that reverberation would merely reflect ongoing spontaneous activity 17 .",
        "rewrite_text": "**Title:** Calcium and Synaptic Dynamics Underlying Reverberatory Behavior in Neuronal Systems\n\n**Abstract:** Reverberation is a critical aspect of cerebral function, yet its precise role in neural activity remains uncertain. This study investigates how the interaction between calcium signaling and short-term plasticity (STP) at excitatory synapses can initiate reverberatory behavior. Our model demonstrates that STP leads to bursts of neuronal spikes, which are often preceded by phases of reduced firing rates due to the depletion of neurotransmitter vesicles. This depletion results in a slow recovery of neurotransmitter release, which in turn causes an accumulation of residual calcium (Ca_res). This accumulation enhances the facilitation of glutamate production during subsequent bursts of activity. The interplay between these mechanisms creates a positive feedback loop that sustains reverberatory activity across various time scales. Our findings suggest that reverberation may play a crucial role in information processing within neural circuits.\n\nReverberation is a prominent characteristic of brain function and has been documented across multiple species and various brain regions, including the hippocampus, neocortex, thalamus, striatum, cerebellar cortex, olfactory bulb, and retina. Despite its widespread occurrence, the specific functional implications of reverberation in the brain remain largely unexplored. One potential function is its role in memory processing or retrieval, while another hypothesis posits that it serves as a substrate for working memory storage. Additionally, reverberation may facilitate temporal coding within neural networks. Conversely, some researchers propose that reverberation simply reflects ongoing spontaneous neural activity. This article aims to elucidate the mechanisms underlying reverberatory behavior and its potential significance in cognitive processes, thereby contributing to a deeper understanding of neuronal dynamics and their implications for brain function.",
        "ori-fast-z-score": -0.18569533817705186,
        "water-fast-z-score": 7.4884526490405925,
        "rewrite-fast-z-score": 2.3958625754235072
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The influence of horizontal gene transfer on the mean fitness of unicellular populations in static environments .\nAbstract:\nHorizontal Gene Transfer (HGT) is an important evolutionary force that can increase genetic diversity and accelerate evolution, but it also has negative effects such as disrupting co-adapted gene complexes or introducing deleterious mutations into recipient genomes. In this study we investigate how HGT affects the mean fitness of unicells evolving under different environmental conditions using computational models. We find that HGT increases the mean fitness when cells are exposed to fluctuating environments with high levels of stressful events. However, if there are only mild fluctuations then HGT decreases the mean fitness because it introduces harmful mutations. Finally, for constant environments without any external stresses, HGT does not affect the mean fitness at all. Our results suggest that HGT may have played an important role during early stages of life s evolution by increasing its adaptability to changing environments. Horizontal Gene Transfer (HGT), which occurs between organisms sharing similar DNA sequences, is one of the most significant evolutionary forces known today  1  . It allows rapid acquisition of new genes and thus contributes to increased genetic diversity within species  2  , accelerates evolution  3  , and facilitates adaptation  4  .\nHowever, HGT also has some disadvantages including disruption of co-adapted gene complexes  5  and introduction of deleterious mutations  6  . Therefore, understanding the effect of HGT on population dynamics requires careful investigation  7, 8  . Previous studies suggested that HGT could be beneficial for populations living in fluctuating environments  9  while detrimental for those inhabiting stable ones  10  . Here we use computational models to explore these hypotheses further and show that HGT can either increase or decrease the mean fitness depending on the type of environment inhabited by the cell population.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of horizontal gene transfer on the mean fitness of unicellular populations in static environments . Abstract : Horizontal Gene Transfer ( HGT ) is an important genetic force that can increase genetic diversity and accelerate evolution , but it also has negative impacts such as disrupting co - adapted gene structures or introducing deleterious variants into recipient genomes .In this study we investigate how HGT affects the mean fitness of unicells evolving under various environmental conditions utilizing computational models . We see that HGT changes the mean fitness when cells are exposed to fluctuating environments with high levels of stressful events .However , if there are only slight fluctuations then HGT decreases the mean fitness because it creates harmful mutations . Finally , for constant environments without any external stresses , HGT does not alter the mean fitness at all .Our results show that HGT could have played an important role during initial stages of life s evolution by increasing its adaptability to changing settings . Horizontal Gene Transfer ( HGT ) , which occurs between organisms sharing related DNA sequences , is one of the most significant evolutionary forces known today 1 .It enables quick acquisition of new genes and therefore contributes to greater genetic diversity within genus 2 , accelerates development 3 , and facilitates adaptation 4 . However , HGT also has some disadvantages notably loss of co - adapted gene structures 5 and entry of deleterious variants 6 .Therefore , studying the impact of HGT on population behavior needs thorough investigation 7 , 8 . Previous studies suggested that HGT could be beneficial for individuals living in fluctuating environments 9 while detrimental for those inhabiting stable ones 10 .Here we using computational models to examine these hypotheses further and suggest that HGT can either increase or decrease the mean fitness depending on the kind of environment populated by the cell population .",
        "rewrite_text": "**Title:** The Impact of Horizontal Gene Transfer on the Mean Fitness of Unicellular Populations in Static Environments\n\n**Abstract:** Horizontal Gene Transfer (HGT) is a significant evolutionary mechanism that enhances genetic diversity and accelerates the evolutionary process. However, it can also have adverse effects, such as disrupting co-adapted gene complexes and introducing harmful mutations into the genomes of recipient organisms. This study explores the influence of HGT on the mean fitness of unicellular populations under varying environmental conditions through the use of computational modeling. Our findings indicate that HGT positively impacts mean fitness in environments characterized by high levels of fluctuation and stress. Conversely, in environments with minimal fluctuations, HGT tends to reduce mean fitness due to the introduction of deleterious mutations. Notably, in stable environments devoid of external stressors, HGT does not significantly affect mean fitness. These results suggest that HGT may have played a crucial role in the early stages of life's evolution by enhancing adaptability to dynamic environments. HGT, which facilitates the transfer of genetic material between organisms with similar DNA sequences, is recognized as one of the most influential evolutionary forces. It allows for the rapid acquisition of new genes, thereby increasing genetic diversity within a genus, accelerating evolutionary development, and promoting adaptation. However, the potential drawbacks of HGT, including the disruption of co-adapted gene structures and the introduction of harmful genetic variants, necessitate a comprehensive examination of its effects on population dynamics. Previous research has indicated that HGT may confer advantages to organisms in fluctuating environments while posing risks to those in stable conditions. In this study, we further investigate these hypotheses, demonstrating that the impact of HGT on mean fitness is contingent upon the environmental context in which the unicellular populations exist.",
        "ori-fast-z-score": 1.1430952132988164,
        "water-fast-z-score": 7.033533126053657,
        "rewrite-fast-z-score": 2.2294816068526147
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Correlation amplitude and entanglement entropy in random spin chains .\nAbstract:\nWe study the correlation function and entanglement entropy for one-dimensional quantum systems with disordered interactions, focusing on their scaling behavior at large distances or times. We show that these quantities are related by an exact formula which is valid both in the ground state and in thermal equilibrium states. The relation can be used to obtain information about the entanglement structure of the system from measurements of correlations only. In particular we discuss how this method allows us to extract the von Neumann entropy of the reduced density matrix corresponding to half of the chain using data obtained from numerical simulations. \nI. INTRODUCTORY REMARK\nThe aim of this work is twofold. First, we want to present some new results concerning the relationship between correlation functions and entanglement entropies in disordered quantum many-body systems. Second, we would like to introduce a novel approach to calculate entanglement properties of such systems based solely on measuring correlation functions. This second aspect will be discussed in more detail below; here let us briefly summarize our main result before turning into technical details.\nConsider a generic quantum-mechanical model defined on a lattice consisting of L sites labeled by integers i = 1, ..., L. Let |0⟩ denote its ground state (which may also represent any other eigenstate) and consider the following quantity:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Correlation amplitude and entanglement entropy in random spin networks . Abstract : We research the interaction function and entanglement entropy for one - dimensional quantum systems with disordered interactions , concentrating on their scaling behavior at large distances or times .We see that these quantities are related by an precise formula which is valid both in the ground state and in heat equilibrium states . The relation can be used to obtain knowledge about the entanglement structure of the system from measurements of correlations only .In particular we explain how this algorithm allows us to extract the von Neumann entropy of the reduced density matrix corresponding to half of the chain using data derived from numerical simulations . I .INTRODUCTORY REMARK The goal of this project is twofold . First , we wish to provide some fresh results relating the relationship between correlation functions and entanglement entropies in disordered quantum several - bodies systems .Second , we may wish to introduce a novel method to estimate entanglement properties of such systems relying solely on measuring correlation functions . This second aspect will be mentioned in more detail below ; here allow us briefly summarize our major result before turning into technical details .Consider a generic quantum - mechanical model formulated on a lattice consisting of L locations labeled by integers i = 1 , . . . , L . Let | 0 ⟩ denote its ground state ( which may also represent any other eigenstate ) and consider the following quantity :",
        "rewrite_text": "Title: Correlation Amplitude and Entanglement Entropy in Random Spin Networks\n\nAbstract: This study investigates the interplay between interaction functions and entanglement entropy in one-dimensional quantum systems characterized by disordered interactions, with a particular focus on their scaling behavior over large distances and time intervals. We establish a precise relationship between these two quantities, which holds true in both the ground state and thermal equilibrium states. This relationship provides a powerful tool for inferring the entanglement structure of the system based solely on correlation measurements. Specifically, we demonstrate how our proposed algorithm can be utilized to derive the von Neumann entropy of the reduced density matrix corresponding to half of the quantum chain, using data obtained from numerical simulations. \n\nThe primary objective of this research is twofold. Firstly, we aim to present new findings that elucidate the connection between correlation functions and entanglement entropies in disordered quantum many-body systems. Secondly, we introduce an innovative method for estimating the entanglement properties of these systems, relying exclusively on the measurement of correlation functions. This novel approach is discussed in greater detail in the subsequent sections of the paper. To summarize our key result, we consider a generic quantum mechanical model defined on a lattice with L sites, indexed by integers i = 1, ..., L. We denote the ground state of this model as |0⟩, which may also represent any other eigenstate, and we explore the implications of our findings on the understanding of quantum entanglement in disordered systems.",
        "ori-fast-z-score": -0.5570860145311556,
        "water-fast-z-score": 5.408521132466447,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unitary Representations of Wavelet Groups and Encoding of Iterated Function Systems in Solenoids .\nAbstract:\nWe study the encoding problem for iterated function systems (IFS) on solenoids by unitary representations of wavelet groups, which are infinite-dimensional Lie groups generated by affine transformations of the real line. We show that if an IFS satisfies certain conditions then it can be encoded into a unitary representation of its associated wavelet group. This result is applied to prove that every self-similar fractal set with finite local complexity has a unique invariant measure up to scaling factors. The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose boundaries have zero Lebesgue measures. In particular, we obtain a new characterization of self-affine carpets as those fractals satisfying this condition. Finally, we give examples showing how our results apply to some well-known fractals including Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unitary Representations of Wavelet Groups and Encoding of Iterated Function Systems in Solenoids . Abstract : We research the encoding problem for iterated function schemes ( IFS ) on solenoids by unitary representations of wavelet groups , which are infinite - dimensional Lie groups produced by affine transformations of the real line .We see that if an IFS satisfies certain conditions then it can be interpreted into a unitary representation of its identified wavelet group . This result is applied to prove that every self - similar fractal set with finite local complexity has a unique invariant measure up to scaling factors .The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose limits have zero Lebesgue measures . In particular , we obtain a new definition of self - affine carpets as those fractals satisfying this condition .Finally , we give instance demonstrating how our findings apply to some well - famous fractals including Sierpinski gasket , Koch snowflake , von Koch curve , and Menger sponge .",
        "rewrite_text": "In this study, we investigate the encoding problem associated with iterated function systems (IFS) on solenoids through the lens of unitary representations of wavelet groups. These wavelet groups are characterized as infinite-dimensional Lie groups generated by affine transformations of the real line. Our findings indicate that under specific conditions, an IFS can be effectively represented as a unitary representation of its corresponding wavelet group. This significant result allows us to establish that every self-similar fractal set exhibiting finite local complexity possesses a unique invariant measure, modulo scaling factors. The proof of this assertion is grounded in the observation that such fractal sets can be approximated by a sequence of compact sets, the limits of which yield zero Lebesgue measures. Notably, we introduce a novel definition of self-affine carpets, identifying them as fractals that meet this criterion. Furthermore, we illustrate the practical implications of our results by applying them to several well-known fractals, including the Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge. Through these examples, we demonstrate the relevance and applicability of our theoretical framework in understanding the intricate relationships between wavelet groups, IFS, and fractal geometry. This research not only advances the theoretical understanding of fractals and their measures but also opens avenues for further exploration in the field of mathematical analysis and its applications in various scientific domains.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 4.00693842672377,
        "rewrite-fast-z-score": 1.8
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Zeta-Function of a p-Adic Manifold, Dwork Theory for Physicists .\nAbstract:\nThe zeta-function is the main object in number theory and algebraic geometry.  In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non-trivial topology.   The basic idea is that one considers the Feynman path integral over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm.    This leads naturally to the concept of a  quantum torus  whose zeta function encodes information about the spectrum of the corresponding QFT.   We will also discuss some recent results concerning the relation between the zeta functions of certain families of quantum tori and their associated modular forms. Quantum Field Theory (QFT) has been developed into a powerful tool for studying physical phenomena at the most fundamental level. However, many interesting questions remain open due to the difficulty of performing calculations beyond perturbation theory. One way out of these difficulties is to use ideas from mathematics such as those coming from number theory or algebraic geometry. In particular, we are interested in using the so-called  zeta-functions  of algebraic varieties to obtain new insights into QFTs. These objects encode important information about the underlying physics but they are notoriously difficult to compute explicitly. In my talk I ll give examples where explicit computations have been performed successfully and show how these techniques could lead to further progress in our understanding of QFTs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Zeta - Function of a p - Adic Manifold , Dwork Theory for Physicists . Abstract : The zeta - function is the main object in number theory and algebraic topology .In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non - trivial geometry . The basic idea is that one takes the Feynman line integral over all fields on spacetime as an infinite dimensional functional analysis which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm .This leads naturally to the notion of a quantum torus whose zeta function encodes data about the spectrum of the associated QFT . We will also discuss some latest findings concerning the relation between the zeta functions of certain classes of quantum tori and their associated modular forms .Quantum Field Theory ( QFT ) has been built into a powerful tool for studying physical phenomena at the most important level . However , many interesting problems continue open thanks to the difficulty of completing calculations beyond perturbation theory .One path out of these problems is to use insights from mathematics such as those coming from number theory or algebraic topology . In particular , we are concerned in use the so - called zeta - functions of algebraic fields to obtain new understanding into QFTs .These items encode crucial data about the fundamental theory but they are notoriously difficult to compute clearly . In my talk I ll offer examples where explicit computations have been performed effectively and tell how these experiments could lead to further progress in our understanding of QFTs .",
        "rewrite_text": "**Title: The Zeta-Function of a p-Adic Manifold: Dwork Theory for Physicists**\n\n**Abstract:** The zeta-function serves as a pivotal concept in both number theory and algebraic topology. In this presentation, I will elucidate its application in the analysis of quantum field theories (QFTs) situated on curved spacetimes characterized by non-trivial geometries. The fundamental approach involves interpreting the Feynman line integral across all fields in spacetime as an infinite-dimensional functional analysis. This framework can be regularized by substituting the infinite-dimensional space of fields with a finite-dimensional vector space that is endowed with an appropriate norm. This transition naturally leads to the concept of a quantum torus, whose zeta function encapsulates essential information regarding the spectrum of the corresponding QFT.\n\nAdditionally, I will explore recent developments concerning the connections between the zeta functions of specific classes of quantum tori and their associated modular forms. Quantum Field Theory has emerged as a robust framework for investigating physical phenomena at a fundamental level. Nevertheless, numerous intriguing challenges remain unresolved, primarily due to the complexities involved in performing calculations beyond perturbation theory. One promising avenue for addressing these challenges lies in leveraging mathematical insights derived from number theory and algebraic topology.\n\nIn particular, this work focuses on utilizing the zeta-functions of algebraic fields to gain novel insights into QFTs. These functions encode vital information about the underlying theory, yet they are notoriously challenging to compute accurately. During my talk, I will present examples where explicit computations have been successfully executed, demonstrating how these findings could pave the way for significant advancements in our comprehension of quantum field theories. Through this exploration, I aim to bridge the gap between mathematical theory and physical application, fostering a deeper understanding of the intricate relationship between geometry, number theory, and quantum physics.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 6.350433056099402,
        "rewrite-fast-z-score": 1.3241694217637887
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Droplets in the two-dimensional +-J spin glass: evidence for (non-) universality .\nAbstract:\nWe study droplet excitations in the 2D spin-glass model with nearest-neighbor interactions and random ferromagnetic bonds, which is known to have an infinite number of metastable states at zero temperature. We show that this system has two different types of droplets: small ones are similar to those found in other models studied previously; large droplets are characterized by their fractal structure. The latter type can be viewed as a generalization of the droplet picture proposed earlier for the 3D Ising spin glasses. In addition we find that there exists another class of excitations -the so-called  giant droplets -which are not present in any of these systems. These giant droplets are responsible for the non-universal behavior observed numerically near the critical point. Finally, we argue that our results provide strong numerical support for the existence of a new phase transition line between the paramagnetic state and the spin-glass one. \nI. INTRODUCTORY REMARK\nThe concept of  droplet excitations  was introduced originally within the framework of the mean-field theory  1  . It describes how local perturbations affect global properties of the system. This idea turned out to be very useful when applied to various disordered systems such as spin glasses  2  , structural glasses  3  or vortex lattices  4  .\nIn particular it allowed to explain many features of the low-temperature thermodynamics of spin glasses  5  . However, despite its successes, the original droplet picture suffers from some serious drawbacks  6  : first, it does not take into account fluctuations around the saddle-point solution  7 ; secondly, it predicts a finite density of droplets even at T = 0  8  ; thirdly, it cannot describe properly the dynamics of the system  9  . To overcome these difficulties several modifications were suggested  10  . One of them  11  leads to the following expression for the free energy F(T ) per site: \nwhere f0 is the free-energy density of the reference system (e.g., the pure ferromagnet), Ns is the total number of spins, V is the volume occupied by each droplet",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Droplets in the two - dimensional + - J spin mirror : evidence for ( non - ) universality . Abstract : We research droplet excitations in the 2D spinning - glass model with nearest - neighbor interactions and random ferromagnetic bonds , which is known to have an endless number of metastable states at zero temperature .We see that this scheme has two different kinds of droplets : tiny ones are related to those present in other models studied ago ; wide droplets are marked by their fractal structure . The latter type can be viewed as a generalization of the droplet picture suggested earlier for the 3D Ising spin glasses .In addition we find that there exists another class of excitations - the so - called giant droplets - which are not present in any of these systems . These huge droplets are responsible for the non - universal behavior observed numerically near the critical position .Finally , we claim that our findings provide strong mathematical support for the existence of a new phase shift line between the paramagnetic state and the spin - glass one . I .INTRODUCTORY REMARK The concept of droplet excitations was introduced originally within the framework of the mean - field principle 1 . It details how local perturbations impact global properties of the system .This idea turned out to be very useful when applied to numerous disordered systems such as spin glasses 2 , structural glasses 3 or vortex lattices 4 . In particular it able to explain different properties of the small - temperature thermodynamics of spin glasses 5 .However , despite its successes , the original droplet picture suffers from some serious drawbacks 6 : first , it does not take into consideration fluctuations around the saddle - point problem 7 ; secondly , it predicts a finite density of droplets even at T = 0 8 ; thirdly , it lacks explain adequately the dynamics of the system 9 . To solve these problems several amendments were recommended 10 .One of them 11 leads to the following expression for the free energy F ( T ) per site : where f0 is the free - energy density of the reference system ( e . g . , the pure ferromagnet ) , Ns is the total number of spinning , V is the volume occupied by each droplet",
        "rewrite_text": "**Title:** Droplets in the Two-Dimensional ±J Spin Mirror: Evidence for (Non) Universality\n\n**Abstract:** This study investigates droplet excitations within the two-dimensional spin-glass model characterized by nearest-neighbor interactions and random ferromagnetic bonds, a system known for its infinite number of metastable states at absolute zero temperature. Our findings reveal the existence of two distinct types of droplets: small droplets that resemble those identified in previously studied models, and larger droplets distinguished by their fractal geometry. The latter category serves as a generalization of the droplet framework previously proposed for three-dimensional Ising spin glasses. Furthermore, we identify a novel class of excitations termed \"giant droplets,\" which have not been observed in other systems. These giant droplets are pivotal in explaining the non-universal behavior noted in numerical analyses near the critical point. Our results lend robust mathematical support to the hypothesis of a new phase transition line separating the paramagnetic phase from the spin-glass phase. \n\nThe concept of droplet excitations was initially introduced within the mean-field theory framework, elucidating how localized perturbations can influence the overall characteristics of a system. This concept has proven beneficial in understanding various disordered systems, including spin glasses, structural glasses, and vortex lattices. Specifically, it has been instrumental in explaining the thermodynamic properties of spin glasses at low temperatures. However, the original droplet model has notable limitations: it overlooks fluctuations around saddle points, predicts a non-zero density of droplets at absolute zero, and inadequately addresses the dynamics of the system. To address these shortcomings, several modifications have been proposed, one of which leads to a refined expression for the free energy per site, incorporating the free energy density of a reference system and accounting for the volume occupied by each droplet. This work aims to deepen the understanding of droplet dynamics and their implications for the behavior of spin-glass systems.",
        "ori-fast-z-score": 0.6446583712203042,
        "water-fast-z-score": 7.205766921228921,
        "rewrite-fast-z-score": 2.3757725695052176
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SubmilliJansky Transients in Archival Radio Observations .\nAbstract:\nWe have searched for radio transients with flux densities between 0.1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival data obtained by the Very Large Array (VLA) over the past 20 years. We find that most of these sources are extragalactic, but we also detect several Galactic objects including pulsars, supernova remnants, and flare stars. The majority of our sample is comprised of previously uncatalogued sources; however, we recover many known variable sources such as blazars and gamma-ray burst afterglows. Our results demonstrate the power of combining large amounts of archival VLA data into one coherent dataset. This work was supported by NSF grant AST-0907860. In this Letter, we present an analysis of all available archived Very Large Array (V LA) observations taken since 1990. These data were collected during various observing programs aimed primarily at studying distant galaxies or nearby star forming regions. However, they contain valuable information about fainter transient phenomena occurring within our Galaxy. By searching through more than 10 000 hours of observation time spread across nearly 2000 epochs, we identify thousands of new faint radio sources which appear only once or twice in each epoch s data set. Most of these sources are extragalaxtic, but we also detect numerous Galactic objects including pulsar wind nebulae, supernova remnants, flare stars, and other types of active galactic nuclei. Many of these newly discovered sources are not included in existing catalogs because their low signal-to-noise ratio makes them difficult to detect when observed individually. However, by combining multiple epochs together, we can boost the sensitivity of our survey enough to detect even very weak signals.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SubmilliJansky Transients in Archival Radio Observations . Abstract : We have searched for radio transients with flux densities between 0 . 1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival measurements obtained by the Very Large Array ( VLA ) over the previous 20 decades .We see that most of these sources are extragalactic , but we also observe numerous Galactic items including pulsars , supernova remnants , and flare stars . The majority of our sample is comprised of previously uncatalogued sources ; however , we recover several known variable sources such as blazars and gamma - ray flare afterglows .Our results show the power of combining enormous amounts of archival VLA information into one coordinated dataset . This project was supported by NSF grant AST - 0907860 .In this Letter , we present an assessment of all available archived Very Large Array ( V LA ) observations made since 1990 . These data were collected during various observing programs aimed exclusively at studying nearby galaxies or neighboring star producing regions .However , they contain significant information about fainter transient phenomena occurring within our Galaxy . By looking through more than 10 000 hours of study time dispersed across nearly 2000 epochs , we identify thousands of new faint radio sources which appear only once or repeatedly in each epoch s information pool .Most of these sources are extragalaxtic , but we also observe numerous Galactic bodies including pulsar wind nebulae , supernova remnants , flare stars , and other types of active galactic nuclei . Many of these newly discovered sources are not covered in existing catalogs because their low signal - to - noise proportion creates them harder to identify when observed individually .However , by combining multiple epochs combined , we can boost the sensitivity of our survey sufficiently to identify especially very weak signals .",
        "rewrite_text": "In this study, we investigate radio transients with flux densities ranging from 0.1 to 1 mJy at frequencies between 4 and 8 GHz, utilizing archival data collected by the Very Large Array (VLA) over the past two decades. Our analysis reveals that a significant portion of these sources are extragalactic; however, we also identify a variety of Galactic objects, including pulsars, supernova remnants, and flare stars. The majority of the sources we catalog are previously unrecorded, although we do recover several known variable sources, such as blazars and gamma-ray flare afterglows. This research underscores the value of integrating extensive archival VLA data into a cohesive dataset, allowing for a more comprehensive exploration of transient phenomena.\n\nSupported by NSF grant AST-0907860, this Letter presents a thorough evaluation of all archived VLA observations conducted since 1990. These observations were primarily gathered through various programs focused on nearby galaxies and star-forming regions. Despite their original intent, the data holds substantial information regarding fainter transient events occurring within our own Galaxy. By meticulously analyzing over 10,000 hours of observational data spread across nearly 2,000 epochs, we have identified thousands of new faint radio sources that manifest either sporadically or repeatedly within the dataset.\n\nWhile many of these newly discovered sources are extragalactic, we also document numerous Galactic entities, including pulsar wind nebulae, supernova remnants, flare stars, and other active galactic nuclei. A significant challenge in identifying these sources arises from their low signal-to-noise ratios, which often renders them undetectable in individual observations. However, by aggregating data from multiple epochs, we enhance the sensitivity of our survey, enabling the detection of particularly weak signals that would otherwise remain obscured. This work highlights the potential of archival data in uncovering new astrophysical phenomena and contributes to our understanding of the transient radio sky.",
        "ori-fast-z-score": 0.8770580193070293,
        "water-fast-z-score": 8.465910570697329,
        "rewrite-fast-z-score": 1.2909944487358056
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effect of node deleting on network structure .\nAbstract:\nWe study the effect of removing nodes in scale-free networks with power-law degree distribution P(k) ~ k^{-γ}, where γ is between 2 and 3. We find that when we remove one node at random or according to its degree, the average path length increases logarithmically as the number of removed nodes grows linearly. \n \n This logarithmic increase can be explained by considering how the removal of a node affects the shortest paths through it. The probability for an edge to belong to such a path decreases exponentially with distance from the deleted node. As a result, most edges affected are those which were close to the deleted node before deletion. These short-range connections form a small-world subnetwork around each deleted node. Removing these local links leads to longer distances within this subnetwork but leaves the global connectivity unchanged. Thus, the overall effect of removing many nodes is similar to adding new long-range links into the system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effect of node deleting on system structure . Abstract : We explore the impact of eliminating nodes in scale - free networks with power - law degree function P ( h ) ~ k ^ { - γ } , where γ is between 2 and 3 .We see that when we eliminate one vertex at random or according to its degree , the average route length rises logarithmically as the quantity of removed nodes increasing linearly . This logarithmic increase can be described by using how the removal of a node impacts the shortest trails through it .The probability for an path to belong to such a path reduces exponentially with distance from the erased node . As a result , most paths affected are those which were close to the deleted node before deletion .These short - range connections form a small - world subnetwork around each deleted link . Removing these local connections leads to longer distances within this subnetwork but leaves the global network unchanged .Thus , the overall impact of eliminating many nodes is identical to added new long - range connections into the scheme .",
        "rewrite_text": "In this study, we investigate the effects of node removal on the structural integrity of scale-free networks characterized by a power-law degree distribution, specifically where the exponent γ falls between 2 and 3. Our findings reveal that the average path length within the network increases logarithmically as nodes are systematically removed, whether this removal is random or based on the nodes' degree. This logarithmic growth in path length correlates with the influence of each deleted node on the shortest paths that traverse through it. We demonstrate that the likelihood of a path being affected diminishes exponentially with increasing distance from the removed node. Consequently, the majority of the paths impacted are those that were in close proximity to the deleted node prior to its removal. This phenomenon results in the formation of a small-world subnetwork surrounding each deleted node, where the removal of these local connections contributes to longer distances within the subnetwork while leaving the overall structure of the global network intact. Ultimately, our analysis suggests that the cumulative effect of removing multiple nodes is akin to introducing new long-range connections into the network. This insight enhances our understanding of the resilience and adaptability of scale-free networks in response to node deletions, with implications for various applications in network theory and real-world systems.",
        "ori-fast-z-score": -2.4110551244604124,
        "water-fast-z-score": 4.458892287340829,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the origin of kinematic distribution of the sub-parsec young stars in the Galactic center .\nAbstract:\nWe present an analysis on the spatial and velocity distributions of the young (<10Myr) stars within 0.5pc of Sgr A*, based on our spectroscopic survey with Subaru/HDS. We find that these stars are distributed into two groups along the line-of-sight; one is located at ~0.1pc to the west side of Sgr A* and another is found at ~0.3pc to its east side. The former group has a mean radial velocity of -200km/sec while the latter shows +100km/sec. These results suggest that there exist two distinct populations of young stars around Sgr A*; one is associated with the clockwise disk-like structure seen in infrared images and the other may be related to the counter-clockwise rotating ring-like feature recently discovered by Genzel et al. (2003) . In addition we have identified several new candidate members for the clockwise disk population.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the origin of kinematic distribution of the sub - parsec young stars in the Galactic center . Abstract : We report an assessment on the spatial and speed distributions of the young ( < 10Myr ) stars within 0 . 5pc of Sgr A * , using on our spectroscopic study with Subaru / HDS .We see that these stars are distributed into two groups along the line - of - view ; one is situated at ~ 0 . 1pc to the west end of Sgr A * and another is found at ~ 0 . 3pc to its east side . The former group has a mean radial speed of - 200km / sec while the former shows + 100km / sec .These data suggest that there exist two different populations of young stars around Sgr A * ; one is associated with the clockwise disk - like structure seen in infrared images and the other may be connected to the counter - clockwise rotating ring - like feature newly discovered by Genzel et al . ( 2003 ) .In addition we have discovered numerous new candidate members for the clockwise disk population .",
        "rewrite_text": "In this study, we investigate the spatial and velocity distributions of young stars (younger than 10 million years) located within 0.5 parsecs of the supermassive black hole Sgr A* at the center of our galaxy. Utilizing spectroscopic observations conducted with the Subaru High Dispersion Spectrograph (HDS), we identify two distinct groups of young stars along the line of sight. The first group is located approximately 0.1 parsecs to the west of Sgr A*, while the second group is found about 0.3 parsecs to the east. Notably, the western group exhibits a mean radial velocity of approximately -200 km/s, indicating a motion towards the black hole, whereas the eastern group shows a mean velocity of +100 km/s, suggesting a motion away from Sgr A*. These findings imply the existence of two separate populations of young stars in the vicinity of Sgr A*. The western population appears to be associated with a clockwise disk-like structure that has been previously observed in infrared imaging, while the eastern population may be linked to a newly identified counter-clockwise rotating ring-like feature, as reported by Genzel et al. (2003). Furthermore, our research has led to the identification of several new candidate members belonging to the clockwise disk population, enhancing our understanding of the dynamics and formation processes of young stars in this extreme environment. This study contributes to the broader knowledge of star formation and kinematics in the Galactic center, providing insights into the complex gravitational interactions at play in this region.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 3.8105117766515297,
        "rewrite-fast-z-score": 0.18257418583505536
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Millimeter and Radio Observations of z~6 Quasars .\nAbstract:\nWe present millimeter (mm) and radio observations for four quasars at redshifts 6<z<7, including the highest-redshift quasar known to date. We detect all sources in our sample with high signal-to-noise ratio using the Atacama Large Millimeter/submillimeter Array (ALMA). The observed spectral energy distributions are well-fit by models that include both synchrotron emission from relativistic jets and thermal dust emission heated by star formation activity. Our results show that these high-redshift quasars have properties similar to those seen in lower-redshift counterparts. These findings suggest that massive black holes grow rapidly during this early epoch of cosmic time. This work is based on data obtained as part of ALMA program 2013.1.00010.S. Millimeter-wave and radio observations provide important insights into the physical processes occurring within distant galaxies. In particular, they can be used to study the growth history of supermassive black holes (SMBHs), which power active galactic nuclei (AGNs).\nIn recent years, several SMBH candidates have been discovered at redshifts greater than six through their strong rest-frame ultraviolet (UV) continua  1  . However, it remains unclear how such objects evolve over cosmological timescales because there has not yet been any direct detection of AGN signatures associated with them  2  .\nThe most promising method for detecting AGN signatures involves observing the mm-wavelength continuum produced by hot electrons accelerated along magnetic field lines in relativistic jets  3  , or via the free-free emission from ionized gas surrounding the central engine  4  . Previous studies have shown that some high-redshift quasars exhibit bright mm-continuum fluxes  5, 6  ; however, only one source was detected in each case  7, 8  . Here we report new mm-and radio-continuum observations made toward four quasars at redhifts between 6 < z < 7, including the highest-redshi",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Millimeter and Radio Observations of z ~ 6 Quasars . Abstract : We report millimeter ( mm ) and radio observations for four quasars at redshifts 6 < z < 7 , including the highest - redshift quasar known to date .We detect all sources in our sample with high signal - to - noise ratio utilizing the Atacama Large Millimeter / submillimeter Array ( ALMA ) . The observed spectral power distributions are best - fitting by models that include both synchrotron emission from relativistic jets and thermal dust absorption warmed by galaxy formation activity .Our results show that these high - redshift quasars have properties similar to those shown in lower - redshift counterparts . These findings show that enormous black holes expand rapidly during this earliest epoch of cosmic time .This effort is based on evidence derived as part of ALMA project 2013 . 1 . 00010 . S . Millimeter - wave and radio observations offer important perspectives into the physical processes arising within distant galaxies .In particular , they can be used to study the development evolution of supermassive black holes ( SMBHs ) , which fuel active galactic nuclei ( AGNs ) . In recent seasons , various SMBH candidates have been detected at redshifts greater than six through their powerful rest - frame ultraviolet ( UV ) continua 1 .However , it remains unsure how such objects evolve over cosmological timescales because there has not already been any direct detection of AGN signatures identified with them 2 . The most promising means for detecting AGN signatures involves studying the mm - wavelength continuum produced by hot electrons accelerated along magnetic field lines in relativistic jets 3 , or via the free - free emission from ionized gas surrounding the main engine 4 .Previous studies have shown that some high - redshift quasars exhibit bright mm - continuum fluxes 5 , 6 ; however , only one source was reported in each case 7 , 8 . Here we publish new mm - and radio - continuum measurements made toward four quasars at redhifts between 6 < z < 7 , notably the highest - redshi",
        "rewrite_text": "We present a comprehensive analysis of millimeter (mm) and radio observations conducted on four quasars with redshifts ranging from 6 to 7, including the highest-redshift quasar identified to date. Utilizing the Atacama Large Millimeter/submillimeter Array (ALMA), we successfully detected all sources in our sample, achieving a high signal-to-noise ratio. The spectral power distributions observed are best modeled by incorporating both synchrotron emission from relativistic jets and thermal dust absorption, which is influenced by the activity associated with galaxy formation. Our findings indicate that these high-redshift quasars possess characteristics akin to those of their lower-redshift counterparts, suggesting that supermassive black holes (SMBHs) are rapidly expanding during this formative period of cosmic history. This research is part of ALMA project 2013.1.00010.S, which underscores the significance of mm-wave and radio observations in elucidating the physical processes occurring within distant galaxies. These observations are particularly valuable for investigating the evolution of SMBHs that power active galactic nuclei (AGNs). In recent years, several candidates for SMBHs have been identified at redshifts exceeding six, primarily through their intense rest-frame ultraviolet (UV) emissions. However, the evolutionary pathways of these objects over cosmological timescales remain uncertain due to the lack of direct detection of AGN signatures associated with them. The most effective approach for identifying AGN signatures involves examining the mm-wavelength continuum produced by hot electrons accelerated along magnetic field lines in relativistic jets, or through free-free emission from ionized gas surrounding the central engine. Previous studies have indicated that some high-redshift quasars display significant mm-continuum fluxes; however, only one source has been reported in each instance. In this study, we provide new mm- and radio-continuum measurements for four quasars within the specified redshift range, highlighting the remarkable properties of these distant celestial objects.",
        "ori-fast-z-score": 0.08362420100070908,
        "water-fast-z-score": 6.620847108818944,
        "rewrite-fast-z-score": 1.9100460366360192
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A large scale extinction map of the Galactic Anticenter from 2MASS .\nAbstract:\nWe present an analysis of the distribution and properties of open clusters in the Galactic anticenter region, using data from the Two Micron All Sky Survey (2MASS). We have compiled a list of all open clusters with angular diameters larger than 1 arcmin within a distance of 8 kpc from the Sun. The total number is about 1000. This sample includes most known open clusters in this part of the Galaxy. Using photometric distances derived by fitting theoretical stellar evolutionary tracks to observed color-magnitude diagrams for each cluster we construct maps showing their spatial distributions as well as their luminosity functions. Our results show that there are two distinct populations of open clusters: one population located at galactocentric radii between 4 and 6 kpc which has ages less than 3 Gyr; another population located at galactocenric radii greater than 7 kpc which has ages older than 5 Gyr.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A wide scale extinction map of the Galactic Anticenter from 2MASS . Abstract : We present an assessment of the distribution and features of open clusters in the Galactic anticenter region , using data from the Two Micron All Sky Survey ( 2MASS ) .We have gathered a list of all open nuclei with angular diameters larger than 1 arcmin within a distance of 8 kpc from the Sun . The total number is about 1000 .This study includes most notable open complexes in this area of the Galaxy . Using photometric distances derived by fitting theoretical stellar evolutionary tracks to observed color - magnitude diagrams for each cluster we create maps showing their spatial distributions as well as their luminosity functions .Our results show that there are two different populations of open groups : one community located at galactocentric radii between 4 and 6 kpc which has ages less than 3 Gyr ; another population located at galactocenric radii greater than 7 kpc which has ages older than 5 Gyr .",
        "rewrite_text": "We provide a comprehensive analysis of the distribution and characteristics of open clusters in the Galactic anticenter region, utilizing data from the Two Micron All Sky Survey (2MASS). Our research has compiled a catalog of open clusters with angular diameters exceeding 1 arcminute, situated within an 8 kpc radius from the Sun, resulting in a total of approximately 1,000 clusters. This investigation encompasses the most prominent open cluster complexes in this segment of the Galaxy. By employing photometric distances obtained through the fitting of theoretical stellar evolutionary tracks to the observed color-magnitude diagrams of each cluster, we have generated detailed maps that illustrate their spatial distributions alongside their luminosity functions. The findings reveal the existence of two distinct populations of open clusters: the first group is located at galactocentric radii ranging from 4 to 6 kpc, characterized by ages of less than 3 billion years; the second group is found at galactocentric radii exceeding 7 kpc, with ages surpassing 5 billion years. This study not only enhances our understanding of the spatial arrangement and age distribution of open clusters in the Galactic anticenter but also contributes to the broader knowledge of Galactic structure and evolution. The implications of these findings may provide insights into the formation and dynamical processes of open clusters, as well as their role in the overall context of the Milky Way's stellar population.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 2.629502940535666,
        "rewrite-fast-z-score": -2.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Information diffusion epidemics in social networks .\nAbstract:\nWe study information diffusion on networks with multiple communities, where each community has its own set of nodes and edges. We propose an epidemic model that captures the effect of both global and local interactions between users within different communities. The proposed model is based on two key ideas: (1) we assume that there are some influential individuals who can spread information to their neighbors faster than others; and (2) we allow for cross-influences among neighboring communities through these influential individuals. Our main goal is to understand how the structure of the underlying network affects the spreading process. In particular, our contributions include:  1. We develop a novel mathematical framework to analyze the dynamics of information diffusion under the proposed epidemic model. 2. We show that if all communities have similar sizes then the number of infected nodes at time t grows as O(t log n), where n denotes the total number of nodes in the network. 3. We prove that if one community dominates the other ones by size then the number of infected individuals grows exponentially fast. 4. Finally, we provide extensive numerical experiments to validate our theoretical results.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Information diffusion epidemics in social systems . Abstract : We research information diffusion on networks with many communities , where each community has its own set of vertices and edges .We suggest an outbreak model that captures the impact of both regional and local interactions between users within various communities . The proposed theory is based on two fundamental ideas : ( 1 ) we suppose that there are some influential citizens who can distribute information to their residents faster than others ; and ( 2 ) we allow for cross - impact among neighboring communities through these powerful individuals .Our main goal is to study how the composition of the underlying network impacts the spreading process . In particular , our achievements include : 1 .We develop a new computational framework to analyze the dynamics of information diffusion under the suggested epidemic model . 2 .We see that if all communities have equal sizes then the quantity of infected nodes at time t grows as O ( t log n ) , where n represents the total quantity of nodes in the network . 3 .We prove that if one community dominates the other ones by size then the quantity of infected individuals grows exponentially rapidly . 4 .Finally , we provide extensive numerical studies to validate our theoretical results .",
        "rewrite_text": "In this study, we investigate the dynamics of information diffusion across networks characterized by multiple communities, each comprising its own distinct set of vertices and edges. We introduce a novel outbreak model that effectively captures the influence of both regional and local interactions among users within these diverse communities. Our theoretical framework is grounded in two key principles: first, we posit the existence of influential individuals capable of disseminating information to their community members at a faster rate than their peers; second, we account for the cross-influence that occurs between neighboring communities facilitated by these prominent figures. \n\nThe primary objective of our research is to understand how the structural composition of the underlying network affects the information spreading process. Our findings reveal several significant insights: \n\n1. We have developed a comprehensive computational framework that enables the analysis of information diffusion dynamics as dictated by our proposed epidemic model. \n2. Our analysis indicates that when all communities are of equal size, the number of infected nodes at a given time t increases at a rate of O(t log n), where n denotes the total number of nodes within the network. \n3. We demonstrate that in scenarios where one community significantly outnumbers the others, the growth rate of infected individuals escalates exponentially. \n4. To substantiate our theoretical findings, we present extensive numerical simulations that corroborate our analytical results.\n\nThis research contributes to a deeper understanding of how information spreads in complex social systems, highlighting the pivotal role of community structure and influential individuals in the diffusion process.",
        "ori-fast-z-score": -1.4501047335684953,
        "water-fast-z-score": 6.225870853937484,
        "rewrite-fast-z-score": 1.4524080181184935
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Entangled games are hard to approximate .\nAbstract:\nWe show that the problem of approximating Nash equilibria in two-player zero-sum entangled games is NP-hard, even if one restricts attention to pure strategies and exact equilibria.  We also prove an analogous result for mixed-strategy equilibria. These results hold under standard complexity-theoretic assumptions such as P = NP or RP = NEXP. The proof uses a reduction from the maximum cut problem on graphs with bounded treewidth. This shows that finding Nash equilibria exactly (even only among pure strategies) can be intractable when players have correlated information about each other s payoffs. Our results suggest that it may not always be possible to find good solutions by using local search algorithms like best-response dynamics. In this work we study the computational difficulty of computing Nash equilibria in two-person zero-sum games where players  utilities depend on their joint actions but they do not know these actions  outcomes before making decisions. Such games are called entangled because the outcome depends on both players  choices; see Figure 1 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Entangled players are hard to approximate . Abstract : We see that the question of approximating Nash equilibria in two - player zero - sum entangled players is NP - hard , even if one restricts focus to pure strategies and exact equilibria .We additionally prove an analogous result for mixed - strategy equilibria . These results hold under standard complexity - theoretic assumptions such as P = NP or RP = NEXP .The proof uses a reduction from the maximum cut question on graphs with bounded treewidth . This shows that finding Nash equilibria precisely ( even only among true schemes ) can be intractable when participants have correlated information about each other s payoffs .Our results show that it could not always be possible to find good answers by using local search methods like best - response dynamics . In this research we study the computational difficulty of computing Nash equilibria in two - person zero - sum games where players utilities depend on their joint actions but they do not understand these actions outcomes before deciding choice .Such matches are called entangled because the result varies on both participants selection ; look Figure 1 .",
        "rewrite_text": "**Title:** Entangled Players Are Hard to Approximate\n\n**Abstract:** In this study, we investigate the computational complexity of approximating Nash equilibria in two-player zero-sum games involving entangled players. Our findings reveal that the problem is NP-hard, even when the analysis is confined to pure strategies and exact equilibria. We extend this result to mixed-strategy equilibria, demonstrating that the challenges persist across different strategic frameworks. These conclusions are grounded in standard complexity-theoretic assumptions, including P = NP and RP = NEXP. \n\nThe proof of our main results employs a reduction from the maximum cut problem on graphs with bounded treewidth, illustrating the inherent difficulties in precisely determining Nash equilibria, particularly when players possess correlated information regarding each other's payoffs. This correlation complicates the decision-making process, as players must navigate the uncertainty of outcomes based on their joint actions without prior knowledge of the results. \n\nOur research highlights significant implications for the feasibility of local search methods, such as best-response dynamics, in finding satisfactory solutions in these entangled scenarios. The complexity of the problem suggests that relying solely on local optimization techniques may not yield effective strategies for players in such games. Overall, our work contributes to a deeper understanding of the computational barriers faced in the pursuit of Nash equilibria in entangled player settings, emphasizing the intricate relationship between strategy selection and outcome uncertainty. This study not only sheds light on the theoretical underpinnings of game theory but also raises important questions about the practical applications of these findings in real-world scenarios where players' decisions are interdependent.",
        "ori-fast-z-score": 0.52999894000318,
        "water-fast-z-score": 5.346252667281783,
        "rewrite-fast-z-score": 1.2185435916898848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Structural parameters for globular clusters in M31 and generalizations for the fundamental plane .\nAbstract:\nWe present new structural parameters for 23 globular clusters (GCs) in M31, derived using HST/ACS images taken with the F606W filter. We use these data to test whether GCs follow the same scaling relations as elliptical galaxies do on the so-called  fundamental plane . The results show that most of our sample follows this relation within uncertainties, but we also find some outliers which are likely due to their different formation histories or dynamical states. In addition, we compare our results with those obtained by other authors who used ground-based observations. Our analysis shows that there is no significant difference between the two datasets when they are analyzed consistently. Finally, we discuss possible reasons why previous studies have found conflicting results about the existence of such a relationship among GC systems. This work was supported by NASA grant NAG5-12140. Keywords: Globular cluster; Fundamental Plane",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Structural parameters for globular complexes in M31 and generalizations for the fundamental plane . Abstract : We report new structural values for 23 globular complexes ( GCs ) in M31 , obtained using HST / ACS images took with the F606W filter .We use these information to test whether GCs obey the same scaling relations as elliptical galaxies do on the so - called fundamental plane . The results show that most of our sample follows this relation within uncertainties , but we also find some outliers which are likely due to their different formation histories or dynamical states .In addition , we compare our findings with those acquired by other researchers who used ground - based observations . Our study shows that there is no major difference between the two datasets when they are examined consistently .Finally , we explain possible reasons why previous research have discovered differing results about the existence of such a connection among GC systems . This research was supported by NASA grant NAG5 - 12140 .Keywords: Globular cluster; Fundamental Plane",
        "rewrite_text": "**Title:** Structural Parameters for Globular Complexes in M31 and Generalizations for the Fundamental Plane\n\n**Abstract:** In this study, we present new structural measurements for 23 globular complexes (GCs) located in the Andromeda galaxy (M31), derived from Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) images captured using the F606W filter. Our primary objective is to investigate whether these globular complexes adhere to the same scaling relations as elliptical galaxies, specifically within the framework of the fundamental plane. The analysis reveals that the majority of our sample aligns with the fundamental plane relation, albeit within the bounds of measurement uncertainties. However, we also identify several outliers, which we hypothesize may be attributed to variations in their formation histories or current dynamical states. Furthermore, we conduct a comparative analysis of our results with those obtained from previous studies utilizing ground-based observational data. Our findings indicate that there is no significant discrepancy between the two datasets when analyzed under consistent methodologies. This consistency suggests that the fundamental plane may indeed be a universal characteristic applicable to both GCs and elliptical galaxies. Additionally, we address potential reasons for the conflicting results reported in earlier research regarding the existence of a connection among GC systems. These discrepancies may stem from differences in observational techniques, sample selection, or data interpretation. This research was conducted with the support of NASA grant NAG5-12140, contributing to our understanding of the structural properties of globular complexes and their relationship to the broader context of galaxy formation and evolution. \n\n**Keywords:** Globular cluster; Fundamental Plane",
        "ori-fast-z-score": 0.5852057359806528,
        "water-fast-z-score": 5.347391382215687,
        "rewrite-fast-z-score": 2.416841222614159
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Astrophysical Accretion as an Analogue Gravity Phenomena .\nAbstract:\nWe present the results of our study on astrophysical accretion phenomena in black holes and neutron stars, which are considered to be analogues for gravitational wave sources such as binary black hole mergers or neutron star -black hole binaries. We have investigated how these systems can produce detectable signals at radio wavelengths by using numerical simulations with high spatial resolution. The main goal is to understand whether we could detect any signal associated with the merger process itself (i.e., before the final plunge) through observations of electromagnetic counterparts. In addition, we also studied the possibility that some of the observed gamma-ray bursts may originate from similar processes occurring in supermassive black holes. Our results show that there should exist observable signatures during the inspiral phase of the system evolution. These include quasi-periodic oscillations produced by the orbital motion of matter around the central compact object, and rapid changes in luminosity due to the interaction between the infalling gas stream and the surrounding disk.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Astrophysical Accretion as an Analogue Gravity Phenomena . Abstract : We present the results of our research on astrophysical accretion events in black holes and neutron galaxies , which are considered to be analogues for gravitational wave sources such as binary white hole mergers or neutron star - black hole binaries .We have researched how these systems can generate detectable frequencies at radio wavelengths by using numerical simulations with high spatial resolution . The main goal is to study whether we could detect any signal associated with the merger process itself ( i . e . , before the last plunge ) through observations of electromagnetic counterparts .In addition , we also examined the idea that some of the seen γ - ray bursts would occur from identical processes resulting in supermassive black holes . Our results show that there should exist observable signatures during the inspiral phase of the system progression .These include quasi - periodic oscillations made by the orbital movement of matter around the main compact body , and large changes in luminosity due to the interaction between the infalling gas stream and the nearby disk .",
        "rewrite_text": "We present our findings on astrophysical accretion phenomena occurring in black holes and neutron stars, which serve as analogs for gravitational wave sources, including binary white hole mergers and neutron star-black hole binaries. Through high-resolution numerical simulations, we investigated the potential for these systems to produce detectable frequencies in the radio spectrum. Our primary objective was to determine whether signals associated with the merger process—specifically during the inspiral phase, prior to the final plunge—could be observed through electromagnetic counterparts. Additionally, we explored the hypothesis that certain gamma-ray bursts may originate from similar processes leading to the formation of supermassive black holes. Our research indicates that observable signatures are likely to emerge during the inspiral phase of these systems. Notably, we identified quasi-periodic oscillations resulting from the orbital motion of matter around the central compact object, as well as significant fluctuations in luminosity caused by the interaction between the infalling gas stream and the surrounding accretion disk. These findings contribute to our understanding of the dynamics of accretion processes and their potential to generate detectable signals, thereby enhancing our ability to study the universe's most extreme environments.",
        "ori-fast-z-score": -0.52999894000318,
        "water-fast-z-score": 4.7699904600286205,
        "rewrite-fast-z-score": -0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deterministic characterization of stochastic genetic circuits .\nAbstract:\nWe present an approach to the analysis and design of stochastic gene regulatory networks based on deterministic models that are derived by averaging over all possible realizations of the underlying random process.  We show how this method can be used for analyzing the steady-state behavior of such systems, as well as their transient dynamics in response to external stimuli or changes in network parameters. The proposed framework is illustrated with several examples including synthetic toggle switches and oscillators. Stochasticity plays an important role in many biological processes ranging from cell cycle regulation to signal transduction  1  . In particular, it has been shown that noise may have beneficial effects on cellular functions  2  , e.g., by enhancing the sensitivity of cells to signals  3  .\nThe study of stochastic gene regulatory networks (GRNs) requires the development of new mathematical tools capable of capturing both the intrinsic fluctuations associated with molecular interactions and extrinsic perturbations due to environmental factors  4  . Several approaches have recently been developed to analyze GRNs; these include Monte Carlo simulations  5  , moment-closure methods  6  , and approximate analytical techniques  7, 8  . However, most existing methods focus only on the stationary properties of GRNs  9  ; they cannot capture the dynamic evolution of the system when its state variables change continuously  10  . Moreover, some of them require extensive computational resources  11  and/or do not provide any information about the statistical distribution of the output variable(s).\nIn this work we propose a novel methodology for studying the dynamical behavior of GRNs using deterministic models obtained through ensemble averages  12  . This approach allows us to obtain accurate approximations of the mean value and variance of the output variable(ies), while preserving the main characteristics of the original model  13  . Our results demonstrate that our technique provides useful insights into the functioning of complex biochemical networks without requiring excessive computational effort.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deterministic analysis of stochastic genetic circuits . Abstract : We present an perspective to the analysis and design of stochastic gene regulatory circuits based on deterministic models that are derived by averaging over all possible realizations of the underlying random process .We see how this method can be used for evaluating the stable - phase response of such systems , as also as their transient dynamics in reaction to external stimuli or alterations in system parameters . The proposed framework is illustrated with many examples representing synthetic toggle switches and oscillators .Stochasticity plays an important role in different biological pathways including from cell cycle regulation to signal transduction 1 . In particular , it has been shown that noise might have beneficial influence on cell functions 2 , e . g . , by increased the sensitivity of cells to signals 3 .The investigation of stochastic gene regulatory networks ( GRNs ) need the development of new computational tools capable of depicting both the intrinsic fluctuations associated with molecular interactions and extrinsic perturbations due to environmental factors 4 . Several approaches have recently been proposed to analyze GRNs ; these involve Monte Carlo simulations 5 , moment - completion technique 6 , and exact mathematical techniques 7 , 8 .However , most existing techniques concentrate only on the stationary features of GRNs 9 ; they cannot record the dynamic development of the process when its state variables move continuously 10 . Moreover , some of them require extensive computational resources 11 and / or do not offer any knowledge about the statistical distribution of the output parameter ( s ) .In this project we develop a new methodology for studying the dynamical behavior of GRNs using deterministic descriptions generated through ensemble estimates 12 . This method enables us to obtain precise approximations of the mean value and variance of the output parameter ( ies ) , while preserving the main characteristics of the previous model 13 .Our results show that our technique provides useful insights into the functioning of complex biochemical organizations without using inappropriate computational time .",
        "rewrite_text": "In this article, we explore a novel approach to the analysis and design of stochastic gene regulatory circuits by employing deterministic models derived from averaging all potential realizations of the underlying stochastic processes. This methodology allows for the assessment of both the stable-phase responses of these systems and their transient dynamics in response to external stimuli or changes in system parameters. We illustrate our proposed framework through various examples, including synthetic toggle switches and oscillators, highlighting the significant role of stochasticity in numerous biological pathways, such as cell cycle regulation and signal transduction. Notably, research has indicated that noise can positively influence cellular functions by enhancing sensitivity to signals.\n\nThe study of stochastic gene regulatory networks (GRNs) necessitates the development of advanced computational tools capable of capturing both intrinsic fluctuations from molecular interactions and extrinsic disturbances from environmental factors. Recent methodologies for analyzing GRNs include Monte Carlo simulations, moment-completion techniques, and exact mathematical approaches. However, many existing methods primarily focus on the stationary characteristics of GRNs and fail to account for the dynamic evolution of state variables. Additionally, some techniques demand substantial computational resources and do not provide insights into the statistical distribution of output parameters.\n\nIn this work, we introduce a new methodology for investigating the dynamic behavior of GRNs through deterministic descriptions generated via ensemble estimates. This innovative approach enables us to derive accurate approximations of the mean and variance of output parameters while retaining the essential features of previous models. Our findings demonstrate that this technique yields valuable insights into the functioning of complex biochemical systems without incurring excessive computational costs, thereby advancing the understanding of stochastic processes in genetic regulation.",
        "ori-fast-z-score": -1.1607348488012053,
        "water-fast-z-score": 7.196556062567472,
        "rewrite-fast-z-score": 0.24576957615571215
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Influence of the lattice topography on a three-dimensional, controllable Brownian motor .\nAbstract:\nWe report an experimental realization of a novel type of 3D Brownian motors based on colloidal particles trapped in periodic potentials created by laser interference patterns. The potential landscape is engineered to have two different types of local minima separated by barriers with varying heights and widths. We show that this design allows for controlling both directional transport as well as its speed over several orders of magnitude. This work opens up new possibilities for designing active materials with tunable properties. A growing number of applications require systems capable of converting energy into directed motion at low Reynolds numbers  1  . In recent years, there has been significant progress towards realizing such devices known as  Brownian motors   2  , which are typically composed of many interacting particles moving through complex environments  3  .\nIn particular, it was shown theoretically  4  and experimentally  5  that one can create a unidirectional current of colloids using optical traps arranged in a 2D square or honeycomb pattern  6  . However, these designs cannot be easily extended to three dimensions (3D) due to technical limitations associated with creating stable trapping sites  7, 8  . Here we demonstrate how to overcome those challenges by engineering the shape of the potential wells and barriers in order to achieve robust 3D transport. Our approach relies on the use of holographic optical tweezers  9  to trap polystyrene microspheres suspended in water inside a glass capillary tube  10  . By changing the phase between the beams forming each individual trap  11  , we were able to generate a variety of potential landscapes  12  ranging from simple double-well structures  13  to more complicated ones containing multiple barriers  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Influence of the crystal topography on a three - dimensional , controllable Brownian motor . Abstract : We report an experimental realization of a new kind of 3D Brownian motors based on colloidal particles confined in periodic potentials created by laser interference patterns .The future topography is designed to have two different kinds of local minima separated by barriers with varying heights and widths . We suggest that this layout allows for controlling both directional travel as well as its velocity over several orders of magnitude .This study opens up new possibilities for modeling active elements with tunable properties . A growing number of applications need devices capable of converting energy into directed motion at low Reynolds numbers 1 .In recent years , there has been significant progress towards realizing such machines called as Brownian motors 2 , which are typically consist of several interacting molecules moving through complex environments 3 . In particular , it was shown theoretically 4 and experimentally 5 that one can create a unidirectional current of colloids using optical trapping ordered in a 2D triangular or honeycomb shape 6 .However , these designs cannot be easily enlarged to three dimensions ( 3D ) related to technical requirements related with creating stable trap places 7 , 8 . Here we prove how to overcome those obstacles by designing the form of the potential wells and fences in order to achieve robust 3D transport .Our solution uses on the using of holographic optical tweezers 9 to capture polystyrene microspheres hanging in water inside a glass capillary loop 10 . By changing the phase between the beams making each individual trap 11 , we were could to produce a variety of potential landscapes 12 ranging from complicated triple - well complexes 13 to more complicated ones featuring multiple barriers 14 .",
        "rewrite_text": "**Title:** Influence of Crystal Topography on a Three-Dimensional, Controllable Brownian Motor\n\n**Abstract:** In this study, we present an innovative experimental realization of a novel type of three-dimensional (3D) Brownian motor, which utilizes colloidal particles confined within periodic potentials generated by laser interference patterns. The designed topography features two distinct types of local minima, which are separated by barriers of varying heights and widths. This configuration enables precise control over both the direction of motion and the velocity of the particles, allowing for adjustments across several orders of magnitude. Our findings pave the way for new models of active elements with customizable properties, addressing the increasing demand for devices that can efficiently convert energy into directed motion at low Reynolds numbers.\n\nRecent advancements in the field have led to the development of Brownian motors—machines composed of interacting molecules navigating through complex environments. Previous theoretical and experimental work has demonstrated the feasibility of generating unidirectional currents of colloidal particles using optical trapping techniques arranged in two-dimensional (2D) geometries, such as triangular or honeycomb lattices. However, scaling these designs to three dimensions has posed significant challenges due to the technical difficulties associated with maintaining stable trapping locations.\n\nIn this paper, we demonstrate a solution to these challenges by carefully engineering the shapes of potential wells and barriers to facilitate robust 3D transport. Our approach employs holographic optical tweezers to manipulate polystyrene microspheres suspended in water within a glass capillary loop. By adjusting the phase relationships between the laser beams that create each individual trap, we are able to generate a diverse array of potential landscapes, ranging from intricate triple-well configurations to more complex arrangements featuring multiple barriers. This work not only enhances our understanding of Brownian motors but also opens up new avenues for the design of advanced micro- and nanoscale devices with tailored functionalities.",
        "ori-fast-z-score": -0.5734623443633283,
        "water-fast-z-score": 7.242717283063319,
        "rewrite-fast-z-score": 0.8058229640253802
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Light-Cone Distribution Amplitudes of Axial-vector Mesons .\nAbstract:\nWe present the light-cone distribution amplitudes (DAs) for axial vector mesons in terms of their helicity components, which are determined by solving the Bethe-Salpeter equation with an instantaneous interaction kernel and applying the method developed recently to calculate DAs.  We find that the twist-2 DA is dominated by its first Gegenbauer moment, while higher moments contribute significantly only at large momentum fractions x > 0.7. The twist-3 DA has two independent functions, one of them being proportional to the second Gegenbauer moment. Our results show that the twist-4 contribution is negligible compared to those of lower twists. These findings will be useful for studying exclusive processes involving axial vector mesons such as B-decays into charmonium plus photon or pion pair. \nI. INTRODUCTIO N\nThe study of hadronic structure plays an important role in understanding strong interactions between quarks and gluons inside hadrons. In particular, the investigation on the parton distributions provides us valuable information about how quarks and gluon are distributed within hadrons  1  . Recently, there have been great interests in exploring the internal structures of hadrons beyond the leading-twist level  2  , especially the transverse-momentum dependent parton distributions  3  .\nIn this work we focus our attention on another type of nonperturbative objects -the light-cone distribution amplitudes(DAs). They describe the probability amplitude of finding a quark-antiquark pair with certain longitudinal momentum fraction and transverse separation at some fixed light-like distance  4  . It was shown that they play crucial roles in describing various hard exclusive reactions  5  . For example, the decay constants fBπ and fBs can be expressed in terms of the lowest-order DAs  6  ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the lowest-and next-to-lowest order DAs  7, 8  . Furthermore, it was found that the heavy-to-light transition form factor FV(q 2 ) of B→V transitions depends",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Light - Cone Distribution Amplitudes of Axial - vector Mesons . Abstract : We present the light - cone distribution amplitudes ( DAs ) for axial vector mesons in terms of their helicity components , which are chosen by solving the Bethe - Salpeter equation with an instantaneous interaction kernel and using the method developed lately to estimate DAs .We see that the twist - 2 DA is dominated by its initial Gegenbauer moment , while greater moments contribute considerably only at large velocity fractions x > 0 . 7 . The twist - 3 DA has two independent functions , one of them being equal to the second Gegenbauer moment .Our results show that the twist - 4 impact is negligible compared to those of lower bends . These conclusions will be valuable for studying exclusive mechanisms using axial vector mesons such as B - decays into charmonium plus photon or pion pair .I . INTRODUCTIO N The investigation of hadronic formation serves an important role in understanding strong interactions between quarks and gluons inside hadrons . In particular , the investigation on the parton distributions offers us valuable info about how quarks and gluon are distributed within hadrons 1 .Recently , there have been much interests in investigating the internal structures of hadrons beyond the led - twist level 2 , particularly the transverse - momentum dependent parton distributions 3 . In this research we focus our focus on another type of nonperturbative objects - the light - cone distribution amplitudes ( DAs ) .They define the probability intensity of finding a quark - antiquark pair with certain horizontal momentum fraction and longitudinal separation at some fixed light - like distance 4 . It was shown that they serve vital part in describing several hard exclusive effects 5 .For instance , the decay constants fBπ and fBs can be written in terms of the lowest - order DAs 6 ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the highest - and last - to - lowest order DAs 7 , 8 . Furthermore , it was shown that the heavy - to - light shift form parameter FV ( q 2 ) of B→V transitions depends",
        "rewrite_text": "We provide a comprehensive analysis of the light-cone distribution amplitudes (DAs) for axial vector mesons, focusing on their helicity components. This study employs the Bethe-Salpeter equation with an instantaneous interaction kernel, alongside a recently developed methodology for estimating DAs. Our findings indicate that the twist-2 DA is predominantly influenced by its initial Gegenbauer moment, while higher-order moments become significant primarily at large velocity fractions (x > 0.7). In addition, the twist-3 DA comprises two independent functions, one of which corresponds to the second Gegenbauer moment. Notably, our results suggest that the contribution from the twist-4 DA is minimal when compared to the impacts of lower twists. These insights are crucial for investigating exclusive processes involving axial vector mesons, such as B-decays into charmonium and photon or pion pairs.\n\nIn the introduction, we emphasize the importance of studying hadronic formation to enhance our understanding of the strong interactions between quarks and gluons within hadrons. The exploration of parton distributions provides essential information regarding the spatial distribution of quarks and gluons inside hadrons. Recent research has increasingly focused on the internal structures of hadrons beyond the leading-twist level, particularly examining transverse-momentum dependent parton distributions. In this context, our work centers on light-cone distribution amplitudes (DAs), which characterize the probability density of locating a quark-antiquark pair with specific transverse momentum fractions and longitudinal separations at a fixed light-like distance. It has been established that DAs play a critical role in describing various hard exclusive processes. For example, decay constants such as fBπ and fBs can be expressed in terms of the lowest-order DAs, while the form factors for semileptonic decays like B→πlνl and B→Klνl are influenced by both the highest and lowest-order DAs. Furthermore, the heavy-to-light transition form parameter FV(q²) for B→V transitions is also dependent on these distribution amplitudes.",
        "ori-fast-z-score": 1.01418510567422,
        "water-fast-z-score": 8.223977311307554,
        "rewrite-fast-z-score": 1.647508942095828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Physical Investigation of the Potentially Hazardous Asteroid (144898) 2004 VD17 .\nAbstract:\nThe asteroid 144898 was discovered on September 24, 2004 by the Catalina Sky Survey at an apparent magnitude of 18.7 and is classified as potentially hazardous due to its large size.  The orbit has been determined using astrometric observations made with the US Naval Observatory s 1-meter telescope in Flagstaff Arizona between October 2005 and March 2007.   These data show that this object will not impact Earth during the next 100 years but may be a good candidate for future space mission targets. This work was supported by NASA under grant NNX07AG70G issued through the Planetary Defense Coordination Office. We report here our results of physical investigations carried out on the surface of the asteroid 144898 (2004VD17). Our analysis shows that it is a S-type asteroid with a diameter D = 2.5 ± 0.2 km. Its rotation period P = 3.6 ± 0.1 hours and pole position are also derived.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Physical Investigation of the Potentially Hazardous Asteroid ( 144898 ) 2004 VD17 . Abstract : The asteroid 144898 was discovered on September 24 , 2004 by the Catalina Sky Survey at an apparent magnitude of 18 . 7 and is categorized as potentially dangerous due to its large size .The orbit has been determined using astrometric observations made with the US Naval Observatory s 1 - meter telescope in Flagstaff Arizona between October 2005 and March 2007 . These data demonstrate that this body will not hit Earth during the last 100 years but might be a better contender for future space flight targets .This project was supported by NASA under grant NNX07AG70G granted through the Planetary Defense Coordination Office . We report here our findings of physical operations carried out on the surface of the asteroid 144898 ( 2004VD17 ) .Our study shows that it is a S - class asteroid with a diameter D = 2 . 5 ± 0 . 2 km . Its rotation period P = 3 . 6 ± 0 . 1 hours and pole position are also derived .",
        "rewrite_text": "**Title:** Physical Investigation of the Potentially Hazardous Asteroid (144898) 2004 VD17\n\n**Abstract:** The asteroid 144898, identified on September 24, 2004, by the Catalina Sky Survey, is classified as potentially hazardous due to its substantial size, with an apparent magnitude of 18.7 at the time of discovery. Comprehensive analysis of its orbit has been conducted using astrometric data collected from the US Naval Observatory's 1-meter telescope in Flagstaff, Arizona, spanning from October 2005 to March 2007. The findings indicate that this asteroid poses no threat of collision with Earth over the next century; however, it may present an intriguing target for future space exploration missions. This research was conducted with the support of NASA, under grant NNX07AG70G, through the Planetary Defense Coordination Office. \n\nIn this study, we present the results of our physical investigations of the surface characteristics of asteroid 144898 (2004 VD17). Our analysis reveals that it is classified as an S-type asteroid, with a measured diameter of 2.5 ± 0.2 km. Additionally, we have determined its rotation period to be 3.6 ± 0.1 hours, along with the orientation of its rotational pole. These findings contribute to the understanding of the asteroid's physical properties and dynamics, which are essential for assessing its potential as a target for future missions and for the broader context of planetary defense strategies. The data collected not only enhances our knowledge of this specific asteroid but also aids in the ongoing efforts to monitor and characterize potentially hazardous objects within our solar system.",
        "ori-fast-z-score": -1.3643820804812932,
        "water-fast-z-score": 3.4465617474213164,
        "rewrite-fast-z-score": -0.4975185951049946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Particle Interpretations of the PVLAS Data .\nAbstract:\nThe PVLAS collaboration has recently reported results on light-by-light scattering in vacuum, which are inconsistent with Standard Model predictions.  In this note we discuss possible interpretations of these data within the framework of quantum field theory and string theory. We argue that the most natural interpretation is to assume that the observed effect arises due to new particles coupling to photons via an effective dimension-8 operator. The required mass scale for such particles can be as low as 10 GeV or even lower if one assumes that they couple only weakly to ordinary matter. If confirmed by further experiments, these observations would have profound implications both for particle physics phenomenology and cosmological models. The PVLAS collaboration has recently announced their measurement of light-by-light scattering in vacuo  1  . This process violates parity conservation at tree level and thus cannot occur in the Standard Model (SM)  2  , but it could arise through loop effects  3  .\nIn particular, the authors report observing a signal consistent with the SM prediction  4  \nwhere G F = 1.1663787(6) × 10−5GeV−2 is Fermi s constant  5  , θ W ≈ 0.23 is the weak mixing angle  6  , m e is the electron mass, and M Pl ≡ 1/ √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass  7, 8  . However, the measured value of the cross section exceeds the theoretical expectation by more than three standard deviations,\nThis discrepancy between experiment and theory may indicate the presence of new physics beyond the SM  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Particle Interpretations of the PVLAS Data . Abstract : The PVLAS collaboration has recently published results on light - by - light diffusion in vacuum , which are inconsistent with Standard Model estimates .In this note we explain possible interpretations of these information within the framework of quantum field theory and string theory . We argue that the most natural interpretation is to assume that the observed effect arises due to new objects coupling to photons via an efficient dimension - 8 operator .The expected mass scale for such particles can be as low as 10 GeV or especially lower if one assumes that they couple only weakly to normal matter . If confirmed by further studies , these observations would have profound implications both for particle science phenomenology and cosmological models .The PVLAS collaboration has recently announced their observation of light - by - light scattering in vacuo 1 . This process violates parity conservation at tree level and therefore cannot appear in the Standard Model ( SM ) 2 , but it could occur through loop effects 3 .In particular , the articles publish observing a signal compatible with the SM prediction 4 where G F = 1 . 1663787 ( 6 ) × 10−5GeV−2 is Fermi s constant 5 , θ W ≈ 0 . 23 is the strong mixing angle 6 , m e is the electron mass , and M Pl ≡ 1 / √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass 7 , 8 . However , the measured value of the cross section exceeds the theoretical expectation by more than three standard deviations , This discrepancy between experiment and theory could indicate the presence of new science beyond the SM 9 .",
        "rewrite_text": "**Title: Particle Interpretations of the PVLAS Data**\n\n**Abstract:** The recent findings from the PVLAS collaboration regarding light-by-light scattering in a vacuum present results that diverge from the predictions made by the Standard Model (SM) of particle physics. This paper explores various interpretations of these findings within the contexts of quantum field theory and string theory. We propose that the most plausible explanation for the observed phenomena is the existence of new particles that interact with photons through a significant dimension-8 operator. The mass scale of these hypothetical particles could be as low as 10 GeV, or even lower if they are assumed to couple weakly with ordinary matter. Should these observations be substantiated by further experimental investigations, they could have significant ramifications for both particle physics and cosmological theories. \n\nThe PVLAS collaboration has reported their detection of light-by-light scattering in vacuum, a process that inherently violates parity conservation at the tree level and is not accounted for in the Standard Model. While such scattering could theoretically occur through loop corrections, the collaboration's findings indicate a signal that aligns with the SM prediction, where Fermi's constant (GF) is approximately 1.1663787(6) × 10⁻⁵ GeV⁻², the weak mixing angle (θW) is around 0.23, and the electron mass (me) and reduced Planck mass (MPl) are also considered. However, the measured cross-section significantly exceeds the theoretical expectations by over three standard deviations. This notable discrepancy between experimental results and theoretical predictions may suggest the presence of new physics beyond the Standard Model, warranting further exploration and analysis.",
        "ori-fast-z-score": -1.104689541477988,
        "water-fast-z-score": 3.0251050401930977,
        "rewrite-fast-z-score": -1.8034001919297482
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Irreducible forms for the metric variations of the action terms of sixth-order gravity and approximated stress-energy tensor .\nAbstract:\nWe present an explicit expression for the irreducible form of the metric variation of the action term in sixth order gravity, which is valid to all orders in perturbation theory. We also show that this result can be used to derive an approximate expression for the stress energy tensor of the gravitational field. The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non-canonical kinetic term. In particular we find that the non-Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is exactly de Sitter space-time. This implies that the bispectrum produced by such models cannot be described solely in terms of local shape functions as it was previously thought. \nI. INTRODUCTORY REMARK\nIn recent years there has been renewed interest on higher derivative theories of gravity motivated mainly by their possible role in quantum gravity phenomenology (see e.g. ), but also because they provide interesting alternatives to standard General Relativity (GR) in the context of modified gravity scenarios . However, despite these efforts, our understanding of the physical consequences of these theories remains incomplete due to technical difficulties associated with the analysis of their solutions. One of the main obstacles comes from the fact that the equations of motion derived from these actions contain derivatives of arbitrarily high order, making them difficult or impossible to solve analytically. A way out of this problem consists in expanding the fields around some fixed background solution and truncating the resulting series expansion after a finite number of terms. Although this approach allows one to obtain useful information about the dynamics of the system under consideration, it fails to capture important features like back-reaction effects between different modes of the same field or interactions among different fields. For example, in the case of inflationary cosmologies based on higher derivative gravity, the truncated perturbative expansions do not reproduce correctly the observed level of primordial non-Gaussianities .\nA more systematic method to deal with these problems involves the use of covariant techniques developed originally within the framework of GR. These methods allow us to express the equations of motion in a manifestly gauge",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Irreducible expressions for the metric variations of the activity terms of sixth - order gravity and approximated strain - energy tensor . Abstract : We present an explicit expression for the irreducible form of the metric variation of the action word in sixth order gravity , which is valid to all orders in perturbation theory .We additionally prove that this result can be used to derive an approximate representation for the strain energy tensor of the gravitational field . The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non - canonical kinetic term .In particular we find that the non - Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is precisely de Sitter space - time . This implies that the bispectrum produced by such theories cannot be described solely in terms of local form variables as it was formerly thought .I . INTRODUCTORY REMARK In recent years there has been continued interest on higher derivative theories of gravitational motivated mainly by their possible involvement in quantum gravitational phenomenology ( saw e . g .) , but also because they give exciting alternatives to standard General Relativity ( GR ) in the context of revised gravitational scenarios . However , despite these attempts , our grasp of the physical effects of these theories appears incomplete due to technical problems related with the interpretation of their solutions .One of the main challenge comes from the fact that the coefficients of movement obtained from these actions involve derivatives of arbitrarily high order , making them harder or impossible to solve analytically . A way out of this question involves in expanding the fields around some fixed background solution and truncating the resulting series contraction after a finite number of terms .Although this methodology allows one to obtain usable information about the dynamics of the process under consideration , it fails to capture important features like back - reaction effects between various modes of the same field or relationships among different fields . For instance , in the case of inflationary cosmologies based on larger derivative gravity , the truncated perturbative expansions do not reproduce correctly the seen level of primordial non - Gaussianities .A more thorough method to deal with these problems involves the using of covariant techniques constructed originally within the framework of GR . These methods help us to derive the equations of movement in a manifestly gauge",
        "rewrite_text": "**Title:** Irreducible Expressions for the Metric Variations of the Activity Terms of Sixth-Order Gravity and Approximated Strain-Energy Tensor\n\n**Abstract:** In this study, we derive an explicit irreducible expression for the metric variation of the action in the context of sixth-order gravity, applicable to all orders within perturbation theory. This foundational result enables us to formulate an approximate representation of the strain-energy tensor associated with the gravitational field. We apply these findings to investigate the dynamics of cosmological perturbations during inflation, specifically in scenarios driven by a scalar field with a non-canonical kinetic term. Notably, our analysis reveals that the non-Gaussianity generated at second order in perturbation theory persists even when the background geometry is precisely described by de Sitter space-time. This observation challenges the conventional understanding that the bispectrum produced in such theories can be solely characterized by local form variables, as previously assumed.\n\nThe growing interest in higher derivative theories of gravity stems from their potential implications in quantum gravitational phenomena and their capacity to provide alternative frameworks to standard General Relativity (GR) in revised gravitational contexts. However, the physical implications of these theories remain inadequately understood, primarily due to technical difficulties in interpreting their solutions. A significant challenge arises from the high-order derivatives present in the equations of motion derived from these actions, complicating analytical solutions. \n\nTo address this issue, we propose a method that involves expanding fields around a fixed background solution and truncating the resulting series after a finite number of terms. While this approach yields valuable insights into the dynamics of the system, it often overlooks critical aspects such as back-reaction effects among various modes of the same field or interactions between different fields. For example, in inflationary cosmologies based on higher derivative gravity, truncated perturbative expansions fail to accurately reproduce the observed levels of primordial non-Gaussianities. A more comprehensive approach involves employing covariant techniques originally developed within the framework of GR, which facilitate the derivation of equations of motion in a manifestly gauge-invariant manner.",
        "ori-fast-z-score": 0.14285714285714285,
        "water-fast-z-score": 7.765928448682152,
        "rewrite-fast-z-score": -1.0120486274099798
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  8.4GHz VLBI observations of SN2004et in NGC6946 .\nAbstract:\nWe present 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves for the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1. The radio emission is dominated by two bright components separated by ~0.5 arcsec at all epochs observed between 2005 January and 2007 December. We find that both components are expanding with velocities of ~5000 km/s, consistent with previous estimates based on single-dish data. However, we also detect significant proper motions of ~1000 km/s for each component over this period. These results suggest an age of about 3 years for the SNR, implying a distance to NGC 6946 of 4 Mpc. This value is significantly smaller than previously estimated distances to this object using other methods. Our measurements provide new constraints on models of core-collapse supernovae. \n \n Keywords: Supernova remnants",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 8 . 4GHz VLBI discoveries of SN2004et in NGC6946 . Abstract : We create 8 . 4 GHz Very Long Baseline Interferometry ( VLBI ) images and light angles for the supernova remnant ( SNR ) associated with the Type IIb supernova SN2004et , which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1 .The signal radiation is dominated by two faint components differentiated by ~ 0 . 5 arcsec at all epochs observed between 2005 January and 2007 December . We see that both components are growing with velocities of ~ 5000 kilometres / s , compatible with previous estimates based on single - dish data .However , we also observe significant proper moves of ~ 1000 km / s for each system over this time . These data suggest an age of about 3 years for the SNR , suggests a distance to NGC 6946 of 4 Mpc .This value is significantly less than previously estimated altitudes to this body using other methods . Our measurements give novel constraints on estimates of core - collapse supernovae .Keywords: Supernova remnants",
        "rewrite_text": "Title: 8.4 GHz VLBI Discoveries of SN2004et in NGC 6946\n\nAbstract: In this study, we present detailed 8.4 GHz Very Long Baseline Interferometry (VLBI) imaging and analysis of the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which erupted in the nearby spiral galaxy NGC 6946 on September 24, 2004 (UT). Our observations, conducted between January 2005 and December 2007, reveal the presence of two faint components separated by approximately 0.5 arcseconds, which have been consistently detected across all epochs. Both components exhibit expansion velocities of around 5000 kilometers per second, aligning with previous estimates derived from single-dish observations. Notably, we also recorded significant proper motions of approximately 1000 kilometers per second for each component over the observed period. These findings imply that the age of the SNR is roughly three years, and we estimate the distance to NGC 6946 to be approximately 4 megaparsecs. This distance measurement is considerably lower than earlier estimates obtained through alternative methodologies. Our results provide new constraints on the characteristics and evolution of core-collapse supernovae, enhancing our understanding of their remnants and the dynamics involved. This research contributes valuable insights into the behavior of supernova remnants and their implications for astrophysical models. \n\nKeywords: Supernova remnants, VLBI, SN2004et, NGC 6946, core-collapse supernovae.",
        "ori-fast-z-score": -1.2309149097933272,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 2.524577979762878
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scalar-field perturbations from a particle orbiting a black hole using numerical evolution in 2+1 dimensions .\nAbstract:\nWe study the gravitational field generated by a massive scalar point source moving on an equatorial circular geodesic around a Schwarzschild black hole, and we evolve it numerically in two spatial dimensions (2+1) with the puncture method. We find that the perturbation is dominated by a single mode which grows exponentially as time goes on. The growth rate agrees well with the prediction based on quasinormal modes for this system. This result suggests that the exponential growth may be related to the instability of the scalar field near the horizon. In addition, we also show that the amplitude of the growing mode decreases rapidly when the mass of the scalar field increases. Finally, we discuss possible applications of our results to astrophysical phenomena such as gamma-ray bursts. \n \n Introduction \n \n Black holes are among the most fascinating objects predicted by general relativity. They have been studied extensively both theoretically and observationally over many decades  1  . One important aspect of their physics concerns how particles move close to them  2  , especially those that can escape from the black hole s gravity  3  . It has recently become clear that there exist some interesting physical processes taking place very close to the event horizon  4  -  6  . For example, if one considers a charged particle falling into a Reissner-Nordström black hole, then its motion will be unstable due to the so-called  photon sphere effect   7, 8  . If the charge of the particle is sufficiently large, then the particle will eventually fall into the black hole after emitting photons  9  . Another interesting phenomenon occurs when a neutral particle falls into a Kerr black hole  10  . Here again, the motion becomes unstable because of the existence of the photon sphere  11  . However, unlike the case of a Reissner-Norström black hole, the emitted radiation now contains not only photons but also gravitons  12  . \n \n In recent years, much attention has been paid to studying the dynamics of fields outside black holes  13  -  17  . In particular, the problem of finding the spectrum of quasi-normal modes (QNMs), i.e., the characteristic frequencies at",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scalar - field perturbations from a particle orbiting a black hole using numerical evolution in 2 + 1 dimensions . Abstract : We explore the gravitational field produced by a huge scalar point source rotating on an equatorial circular geodesic around a Schwarzschild red hole , and we derive it numerically in two spatial dimensions ( 2 + 1 ) with the puncture method .We see that the perturbation is dominated by a single mode which increases exponentially as time go on . The growth speed agrees well with the prediction based on quasinormal modes for this system .This result suggests that the exponential growth could be due to the instability of the scalar field near the horizon . In addition , we also find that the frequency of the increasing mode decreases quickly when the mass of the scalar field increases .Finally , we talk possible applied of our findings to astrophysical processes such as gamma - ray bursts . Introduction Black holes are among the most useful structures anticipated by general relativity .They have been studied frequently both theoretically and observationally over numerous years 1 . One important element of their physics matters how particles moving close to them 2 , particularly those that can escape from the dark hole s gravity 3 .It has recently become clear that there exist some interesting physical processes running place very close to the event horizon 4 - 6 . For instance , if one considers a charged particle falling into a Reissner - Nordström black hole , then its motion will be unstable due to the so - called photon circle phenomenon 7 , 8 .If the charge of the particle is sufficiently huge , then the particle will eventually go into the dark hole after emitting photons 9 . Another curious phenomenon occurs when a neutral particle falls into a Kerr black hole 10 .Here again , the movement becomes unstable because of the existence of the photon sphere 11 . However , unlike the case of a Reissner - Norström black hole , the emitted radiation now contains not only photons but also gravitons 12 .In recent years , much attention has been paid to researching the dynamics of fields outside grey holes 13 - 17 . In particular , the question of finding the spectrum of quasi - normal frequencies ( QNMs ) , i . e . , the typical frequencies at",
        "rewrite_text": "In this study, we investigate the gravitational influence exerted by a significant scalar point source that is in motion along an equatorial circular geodesic around a Schwarzschild black hole. Utilizing the puncture method, we conduct a numerical analysis in two spatial dimensions (2 + 1). Our findings reveal that the perturbations induced by the scalar field are predominantly characterized by a single mode, which exhibits exponential growth over time. This growth rate aligns closely with theoretical predictions derived from quasinormal modes associated with the system, suggesting that the observed exponential increase may be indicative of an instability within the scalar field in proximity to the black hole's event horizon. Furthermore, we observe that as the mass of the scalar field increases, the frequency of the growing mode diminishes rapidly. This behavior prompts a discussion on the potential implications of our results for astrophysical phenomena, particularly in relation to gamma-ray bursts.\n\nThe introduction highlights the significance of black holes within the framework of general relativity, emphasizing their extensive theoretical and observational study over the years. A critical aspect of black hole physics involves understanding the dynamics of particles in their vicinity, especially those capable of escaping the gravitational pull. Recent research has unveiled intriguing physical processes occurring near the event horizon, such as the instability of charged particles falling into Reissner-Nordström black holes due to the photon circle effect. In such cases, particles with substantial charge may ultimately be drawn into the black hole after emitting photons. Similarly, the behavior of neutral particles in Kerr black holes reveals instability linked to the photon sphere, where emitted radiation comprises not only photons but also gravitons. The dynamics of fields surrounding black holes have garnered significant attention in recent years, particularly in the context of identifying the spectrum of quasinormal frequencies (QNMs), which represent the characteristic frequencies at which these systems oscillate.",
        "ori-fast-z-score": -0.4601789933084222,
        "water-fast-z-score": 6.902684899626333,
        "rewrite-fast-z-score": 0.242535625036333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Chandra archival study of the temperature and metal abundance profiles in hot Galaxy Clusters at 0.1 < z < 0.3 .\nAbstract:\nWe have analyzed Chandra observations for eight galaxy clusters with redshifts between 0.1 and 0.3 to determine their radial temperature, density, pressure, entropy, cooling time, and metallicity profiles. We find that all these quantities are well described by single-parameter scaling relations as functions of radius r normalized by the virial radius Rvir.  The best-fit values of the normalization parameters depend on redshift but not significantly so; we therefore adopt fixed values based on our results for the two most distant clusters (z = 0.2 and 0.3) which yield good fits to the other six clusters. Our main conclusions are:  1. All cluster properties show significant evolution out to z ~ 0.3; this is consistent with previous studies using XMM data. 2. The gas fraction fgas(r/Rvir), defined as the ratio of the total thermal energy within a sphere of radius r to its gravitational binding energy, decreases monotonically outwards; it also shows some evidence for evolution with redshift. 3. The electron number density ne(r) increases inwardly toward the center of each cluster until reaching a peak value near r ~ 0.1r200 where r200 denotes the radius enclosing an average overdensity of 200 times the critical density of the universe. Beyond this point, ne(r) declines slowly or remains roughly constant depending on the cluster. 4. The mean molecular weight µe(r) increases outwardly due to the increasing contribution of helium ions relative to hydrogen atoms. 5. The central temperatures T0 inferred from spectral fitting range from 6 keV to 12 keV, while those obtained directly from the deprojected temperature profile lie in the range 7-15 keV. These differences may be caused by non-thermal components such as AGN jets and/or magnetic fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Chandra archival analysis of the temperature and metal availability profiles in hotter Galaxy Clusters at 0 . 1 < z < 0 . 3 . Abstract : We have analyzed Chandra measurements for eight galaxy galaxies with redshifts between 0 . 1 and 0 . 3 to find their radial temperature , density , pressure , entropy , cooling period , and metallicity profiles .We see that all these quantities are better represented by single - parameter scaling relations as functions of radius r normalized by the virial diameter Rvir . The best - fitting values of the normalization values rely on redshift but not considerably so ; we thus choose fixed values based on our findings for the two most distant clusters ( z = 0 . 2 and 0 . 3 ) which provide better fits to the other six clusters .Our main results are : 1 . All cluster elements exhibit substantial development out to z ~ 0 . 3 ; this is consistent with previous research used XMM data .2 . The gas fraction fgas ( r / Rvir ) , defined as the proportion of the total heat power within a sphere of radius r to its gravitational binding energy , decreases monotonically outwards ; it also shows some evidence for evolution with redshift .3 . The electron number density ne ( r ) rises inwardly toward the center of each cluster until reaching a peak value near r ~ 0 . 1r200 where r200 denotes the radius enclosing an mean overdensity of 200 times the critical density of the universe .Beyond this point , ne ( r ) declines slowly or remains relatively constant depending on the cluster . 4 .The mean molecular weight µe ( r ) rises outwardly due to the increasing impact of helium ions relative to hydrogen atoms . 5 .The central temperatures T0 inferred from spectral fit reach from 6 keV to 12 keV , while those generated directly from the deprojected temperature profile lie in the range 7 - 15 keV . These changes may be caused by non - cooling systems such as AGN rockets and / or magnetic fields .",
        "rewrite_text": "**Title:** A Chandra Archival Analysis of Temperature and Metal Availability Profiles in Hotter Galaxy Clusters at 0.1 < z < 0.3\n\n**Abstract:** In this study, we conducted a comprehensive analysis of Chandra observations for eight galaxy clusters with redshifts ranging from 0.1 to 0.3, focusing on their radial profiles of temperature, density, pressure, entropy, cooling time, and metallicity. Our findings indicate that these physical quantities can be effectively described by single-parameter scaling relations as functions of the radius \\( r \\) normalized by the virial radius \\( R_{vir} \\). The optimal normalization values we derived are dependent on redshift, albeit to a limited extent; thus, we established fixed values based on our analysis of the two most distant clusters (at \\( z = 0.2 \\) and \\( z = 0.3 \\)), which yielded improved fits for the remaining six clusters. \n\nKey results from our analysis include: (1) All clusters show significant evolution up to \\( z \\sim 0.3 \\), corroborating previous studies utilizing XMM data. (2) The gas fraction \\( f_{gas}(r/R_{vir}) \\), defined as the ratio of the total thermal energy within a sphere of radius \\( r \\) to its gravitational binding energy, exhibits a monotonically decreasing trend outward and suggests some evolutionary changes with redshift. (3) The electron number density \\( n_e(r) \\) increases towards the center of each cluster, peaking at approximately \\( r \\sim 0.1 r_{200} \\), where \\( r_{200} \\) is the radius that encompasses a mean overdensity of 200 times the critical density of the universe. Beyond this peak, \\( n_e(r) \\) either declines gradually or remains relatively stable, depending on the specific cluster. (4) The mean molecular weight \\( \\mu_e(r) \\) increases outward, reflecting the growing influence of helium ions compared to hydrogen atoms. (5) The central temperatures \\( T_0 \\), derived from spectral fitting, range from 6 keV to 12 keV, while those obtained from the deprojected temperature profiles span 7 to 15 keV. These variations may be attributed to non-cooling mechanisms such as active galactic nuclei (AGN) feedback and/or magnetic fields.",
        "ori-fast-z-score": -0.3287979746107146,
        "water-fast-z-score": 6.368673331236264,
        "rewrite-fast-z-score": 1.697749375254331
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  One- and two-component bottle-brush polymers: simulations compared to theoretical predictions .\nAbstract:\nWe study the conformational properties of one- and two-component bottlebrush polymers in good solvent conditions by means of Monte Carlo (MC) simulations, using an off-lattice model with freely jointed chains. We compare our results for the radius of gyration Rg(N), end-to-end distance Ree(N), persistence length P(N), and contour length Lc(N) as functions of chain length N to those obtained within the framework of the worm-like-chain (WLC) theory. The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over several orders of magnitude in chain lengths. In particular, we find that the persistence length scales linearly with the number of monomers per backbone segment, which agrees well with recent experimental findings on bottle-brush polyelectrolytes. \n \n Keywords: Polymer brushes, Persistence length",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : One - and two - component bottle - brush polymers : simulations compared to theoretical estimates . Abstract : We research the conformational characteristics of one - and two - component bottlebrush polymers in good solvent circumstances by means of Monte Carlo ( MC ) simulations , using an off - lattice model with freely jointed chains .We relate our findings for the radius of gyration Rg ( N ) , end - to - end distance Ree ( N ) , persistence length P ( N ) , and contour distance Lc ( N ) as functions of chain length N to those acquired within the framework of the worm - like - chain ( WLC ) theory . The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over large orders of magnitude in chain lengths .In particular , we find that the persistence length varies linearly with the quantity of monomers per backbone segment , which accepts good with recent experimental discoveries on bottle - brush polyelectrolytes . Keywords : Polymer brushes , Persistence length",
        "rewrite_text": "Title: One- and Two-Component Bottlebrush Polymers: Simulations Compared to Theoretical Estimates\n\nAbstract: In this study, we investigate the conformational properties of one- and two-component bottlebrush polymers in favorable solvent conditions through Monte Carlo (MC) simulations. Utilizing an off-lattice model characterized by freely jointed chains, we analyze key metrics such as the radius of gyration (Rg(N)), end-to-end distance (Ree(N)), persistence length (P(N)), and contour length (Lc(N)) as functions of the polymer chain length (N). Our results are compared to theoretical predictions derived from the worm-like chain (WLC) model. The WLC framework demonstrates remarkable accuracy in predicting the scaling behavior of these conformational characteristics across a wide range of chain lengths. Notably, our findings reveal that the persistence length exhibits a linear relationship with the number of monomers per backbone segment, aligning well with recent experimental observations concerning bottlebrush polyelectrolytes. This research contributes to a deeper understanding of the structural dynamics of bottlebrush polymers, providing insights that could inform future experimental and theoretical studies in polymer science. The implications of our results extend to the design and application of polymer brushes in various fields, including materials science and biophysics. Overall, our work underscores the utility of MC simulations in complementing theoretical models and enhancing our comprehension of complex polymer architectures. \n\nKeywords: Polymer brushes, Persistence length",
        "ori-fast-z-score": 0.6201736729460423,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 0.9332565252573828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Variable Star One-shot Project, and its little child: Wikimbad .\nAbstract:\nThe Variable Star One-Shot project is an open-source software package for the analysis of astronomical data.  It was developed by members of the Harvard-Smithsonian Center for Astrophysics (CfA) in collaboration with researchers at other institutions around the world.   The goal of this project is to provide a single tool that can be used to analyze all types of astronomical data sets, including photometric time series, spectroscopic observations, images, etc., using state-of-the-art techniques such as image subtraction, cross correlation, period finding algorithms, spectral line fitting, etc.    This software has been released under the GNU General Public License v3.0 and is available on GitHub at: https://github.com/VariableStar/one-shot-astro .\nOne Shot Astro includes several tools which are useful for analyzing large amounts of astronomical data quickly and efficiently.   These include one-shot-datacleaner , which performs automated quality control checks on raw data files;   one-shot-mosaic , which creates mosaicked images from multiple dithered exposures;  one-shot-astrometry , which determines astrometric solutions for individual frames or entire mosaic images;    one-shot-photometry , which measures fluxes and/or magnitudes for stars across a field-of-view;    one-shot-pipeline , which automates the process of running these various tasks together into a pipeline;    one-shot-wikimapia , which allows users to create custom sky maps based on their own catalogs of objects.     In addition to these core packages, there are also many additional modules available through the one-shot-astro repository which allow users to perform more specialized analyses, such as:    one-shot-catalog , which provides access to a variety of different astrophysical databases via SQL queries;    one-shot-fastphot , which uses machine learning methods to measure stellar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Variable Star One - shot Project , and its tiny child : Wikimbad . Abstract : The Variable Star One - Shot project is an open - source software tool for the analysis of astronomical data .It was developed by participants of the Harvard - Smithsonian Center for Astrophysics ( CfA ) in partnership with colleagues at other institutions around the world . The goal of this project is to provide a single technique that can be used to analyze all types of astronomical data sets , including photometric period series , spectroscopic observations , photographs , etc . , using state - of - the - art methods such as image subtraction , cross correlation , period finding methods , spectral line fitting , etc .This program has been released under the GNU General Public License v3 . 0 and is accessible on GitHub at : https : / / github . com / VariableStar / one - shot - astro . One Shot Astro includes several methods which are helpful for studying huge amounts of astronomical data easily and smoothly .These include one - shot - datacleaner , which performs automated quality control checks on raw data data ; one - shot - mosaic , which forms mosaicked images from multiple dithered exposures ; one - shot - astrometry , which determines astrometric solutions for individual frames or entire mosaic images ; one - shot - photometry , which estimates fluxes and / or magnitudes for planets across a field - of - view ; one - shot - pipeline , which automates the process of running these numerous tasks combined into a pipeline ; one - shot - wikimapia , which allows users to create custom skies mapping based on their own catalogs of items . In addition to these core programs , there are also many extra modules available through the one - shot - astro repository which allow users to conduct more sophisticated analyses , such as : one - shot - catalog , which offers connections to a variety of different astrophysical databases via SQL queries ; one - shot - fastphot , which uses computer learning techniques to measure stellar",
        "rewrite_text": "**Title:** The Variable Star One-Shot Project and Its Offshoot: Wikimbad\n\n**Abstract:** The Variable Star One-Shot Project is an innovative open-source software initiative designed for the comprehensive analysis of astronomical data. Developed collaboratively by researchers at the Harvard-Smithsonian Center for Astrophysics (CfA) and various international partners, this project aims to establish a unified methodology for analyzing diverse astronomical datasets, including photometric time series, spectroscopic data, and imaging. Utilizing advanced techniques such as image subtraction, cross-correlation, period-finding algorithms, and spectral line fitting, the project enhances the accessibility and efficiency of astronomical data analysis. The software is distributed under the GNU General Public License v3.0 and is available on GitHub at https://github.com/VariableStar/one-shot-astro.\n\nOne-Shot Astro encompasses a suite of tools that facilitate the processing of large volumes of astronomical data with ease. Key features include One-Shot Datacleaner, which automates quality control checks on raw datasets; One-Shot Mosaic, which generates mosaicked images from multiple dithered exposures; One-Shot Astrometry, which provides astrometric solutions for individual frames or entire mosaics; and One-Shot Photometry, which calculates fluxes and magnitudes for celestial objects across a specified field of view. Additionally, One-Shot Pipeline streamlines the execution of these various tasks into a cohesive workflow, while One-Shot Wikimapia enables users to create customized sky maps based on their own catalogs.\n\nBeyond the core functionalities, the One-Shot Astro repository offers numerous supplementary modules that empower users to perform more complex analyses. Notable among these are One-Shot Catalog, which connects to various astrophysical databases through SQL queries, and One-Shot Fastphot, which employs machine learning techniques for stellar measurements. Collectively, these tools represent a significant advancement in the field of astronomical data analysis, promoting collaboration and innovation among researchers worldwide.",
        "ori-fast-z-score": 0.9135002783911397,
        "water-fast-z-score": 7.945016530582732,
        "rewrite-fast-z-score": -0.17407765595569785
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A 3-Stranded Quantum Algorithm for the Jones Polynomial .\nAbstract:\nWe present an algorithm to compute the Jones polynomial using three quantum wires and one ancilla qubit, which is based on the idea that the Jones polynomial can be computed by evaluating the Tutte polynomial at 1. We show how this approach leads naturally to a circuit with O(n) gates in depth O(log n), where n denotes the number of crossings in the knot diagram. The main ingredient of our construction are two new families of unitary operators acting on tensor products of Hilbert spaces associated with graphs. These operators generalize the swap operator and have applications beyond computing the Jones polynomial. In particular we use them to construct efficient quantum circuits for other problems such as counting perfect matchings or finding maximum flows. \nIntroduction\n\nThe Jones polynomial J K (t) ∈ Z t ±1/2   of a knot K is a Laurent polynomial invariant of knots and links defined by V. Jones  Jon87  . It has been shown that it distinguishes all prime knots up to 10 crossings  Kaw96  , but its complexity makes it hard to evaluate even for small values of t. For example, if K is the trefoil knot then J K (1) = −1/4 while J K (−1/2) = 1/2. This raises the question whether there exists a quantum algorithm to efficiently calculate the Jones polynomial.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A 3 - Stranded Quantum Algorithm for the Jones Polynomial . Abstract : We present an algorithm to compute the Jones polynomial utilizing three quantum wires and one ancilla qubit , which is based on the idea that the Jones function can be computed by evaluating the Tutte polynomial at 1 .We see how this step leads naturally to a circuit with O ( n ) gates in depth O ( log n ) , where n represents the number of crossings in the knot diagram . The main ingredient of our build are two new families of unitary operators acting on tensor products of Hilbert spaces identified with graphs .These operators generalize the swap operator and have applications beyond computing the Jones function . In particular we using them to build efficient quantum networks for other issues such as figuring ideal matchings or finding maximum flows .Introduction The Jones function J K ( t ) ∈ Z t ±1 / 2 of a knot K is a Laurent polynomial invariant of knots and links given by V . Jones Jon87 . It has been shown that it distinguishes all prime knots up to 10 crossings Kaw96 , but its complexity makes it difficult to analyze even for large values of t . For instance , if K is the trefoil knot then J K ( 1 ) = −1 / 4 while J K ( −1 / 2 ) = 1 / 2 .This opens the question whether there exists a quantum algorithm to easily calculate the Jones function .",
        "rewrite_text": "We introduce a novel quantum algorithm designed to compute the Jones polynomial using a configuration of three quantum wires and one ancilla qubit. This approach is grounded in the principle that the Jones function can be derived by evaluating the Tutte polynomial at the value of 1. Our findings demonstrate that this method leads to the construction of a quantum circuit characterized by O(n) gates and a depth of O(log n), where n denotes the number of crossings present in the knot diagram. A pivotal aspect of our algorithm involves the development of two new families of unitary operators that act on tensor products of Hilbert spaces associated with graphs. These operators extend the functionality of the traditional swap operator and have potential applications that reach beyond the computation of the Jones function itself. Specifically, we leverage these operators to create efficient quantum networks aimed at solving various problems, including determining ideal matchings and identifying maximum flows.\n\nThe Jones function, denoted as J_K(t) ∈ Z[t^{±1/2}], serves as a Laurent polynomial invariant for knots and links, as established by Vaughan Jones in 1987. It has been demonstrated that this function can distinguish all prime knots with up to 10 crossings, as noted by Kawauchi in 1996. However, the complexity of the Jones polynomial poses significant challenges for analysis, particularly for larger values of t. For example, for the trefoil knot, we find that J_K(1) = -1/4, while J_K(-1/2) = 1/2. This complexity raises the intriguing question of whether a quantum algorithm exists that can efficiently compute the Jones function, paving the way for further exploration into the capabilities of quantum computing in knot theory and related fields.",
        "ori-fast-z-score": -0.8251369970070347,
        "water-fast-z-score": 4.001315573132102,
        "rewrite-fast-z-score": 1.3416407864998738
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Generalizing Consistency and other Constraint Properties to Quantified Constraints .\nAbstract:\nWe present an algorithm for checking consistency in quantified constraints, which is based on the concept of generalized quantifiers. We show that our approach can be used to check several constraint properties such as satisfiability or equivalence between two sets of quantified constraints. Finally we discuss how this method could be applied to solve problems related to software testing. In computer science, many problems are formulated using constraints. For example, in Software Testing (ST), test cases are often represented by means of logical formulas called Test Cases Specifications (TCS). These TCSs contain some variables whose values have to satisfy certain conditions expressed with Boolean expressions. The problem consists then in finding all possible assignments of these variables satisfying the given conditions. This kind of problems has been studied extensively during last years but most works focus only on unquantified constraints. However, there exist situations where it may be useful to express some restrictions over the set of solutions using quantifiers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Generalizing Consistency and other Constraint Properties to Quantified Constraints . Abstract : We create an algorithm for checking consistency in quantified constraints , which is based on the idea of generalized quantifiers .We see that our approach can be used to test several constraint features such as satisfiability or equivalence between two sets of quantified constraints . Finally we talk how this method could be applied to solve difficulties related to programming testing .In computer science , many issues are formulated using restrictions . For instance , in Software Testing ( ST ) , test situations are often modeled by means of rational formulas called Test Cases Specifications ( TCS ) .These TCSs comprise some parameters whose values have to obey certain conditions stated with Boolean expressions . The question involves then in obtaining all possible assignments of these variables satisfying the particular conditions .This kind of problems has been studied thoroughly during last decades but most works concentrate only on unquantified constraints . However , there exist situations where it could be useful to define some restrictions over the group of solutions use quantifiers .",
        "rewrite_text": "Title: Generalizing Consistency and Other Constraint Properties to Quantified Constraints\n\nAbstract: In this paper, we present a novel algorithm designed to assess the consistency of quantified constraints, leveraging the concept of generalized quantifiers. Our methodology not only facilitates the evaluation of consistency but also extends to various constraint properties, including satisfiability and the equivalence of two sets of quantified constraints. We explore the implications of our approach in addressing challenges associated with software testing. In the realm of computer science, numerous problems are framed through constraints. Specifically, in Software Testing (ST), test scenarios are frequently represented using rational formulas known as Test Case Specifications (TCS). These specifications encompass parameters that must adhere to specific conditions articulated through Boolean expressions. Consequently, the primary challenge lies in determining all potential assignments of these variables that satisfy the given conditions. While this area has been extensively researched over the past few decades, the majority of existing studies have predominantly focused on unquantified constraints. Our work highlights the necessity and advantages of incorporating quantifiers to define restrictions over solution sets, thereby enhancing the expressiveness and applicability of constraint-based problem-solving. By integrating generalized quantifiers into the analysis of quantified constraints, we aim to provide a more robust framework for tackling complex issues in software testing and beyond, ultimately paving the way for more effective and comprehensive testing methodologies.",
        "ori-fast-z-score": -0.6546536707079772,
        "water-fast-z-score": 5.673665146135802,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pressure induced Insulator-Metal transition in LaMnO$_3$ .\nAbstract:\nThe pressure-induced insulator-metal (IMT) phase transition is studied by means of the first-principles calculations based on density functional theory within local spin-density approximation and generalized gradient approximations. The calculated results show that the IMT occurs at about 20 GPa for both LSDA and PBE functionals, which are consistent with previous theoretical studies. However, it should be noted that there exists an obvious difference between these two methods when calculating the electronic structure near Fermi level. In addition to this, we also find that the band gap decreases rapidly as increasing pressure up to 30 GPa but then becomes almost constant above 40 GPa. Finally, our calculation shows that the volume collapse takes place around 50 GPa. \n \n Keywords: Pressure-induced insulator-metal transition; First-principles calculations; Local spin-density approximation; Generalized gradient approximations; Electronic structure; Band gap",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pressure induced Insulator - Metal transition in LaMnO $ _ 3 $ . Abstract : The pressure - triggered insulator - metal ( IMT ) phase shift is studied by means of the first - principles calculations based on density functional theory within local spin - density algorithm and generalized gradient approximations .The measured data demonstrate that the IMT occurs at about 20 GPa for both LSDA and PBE functionals , which are compatible with previous conceptual research . However , it should be mentioned that there exists an evident gap between these two means when calculating the electronic structure near Fermi level .In addition to this , we also find that the band gap falls steadily as increasing temperature up to 30 GPa but then grows almost steady above 40 GPa . Finally , our mathematical reveals that the volume collapse took place around 50 GPa .Keywords : Pressure - caused insulator - iron transition ; First - principles measurements ; Local spin - density approximation ; Generalized gradient approximations ; Electronic structure ; Band gap",
        "rewrite_text": "Title: Pressure-Induced Insulator-Metal Transition in LaMnO$_3$\n\nAbstract: This study investigates the pressure-induced insulator-metal transition (IMT) in LaMnO$_3$ through first-principles calculations grounded in density functional theory (DFT), utilizing both the local spin-density approximation (LSDA) and generalized gradient approximations (GGA). Our findings indicate that the IMT occurs at approximately 20 GPa, consistent with earlier theoretical studies. However, a notable discrepancy arises between the LSDA and PBE functionals when analyzing the electronic structure near the Fermi level, highlighting the sensitivity of the results to the choice of functional. Furthermore, we observe that the band gap of LaMnO$_3$ decreases progressively with increasing pressure up to 30 GPa, after which it stabilizes and begins to increase again at pressures exceeding 40 GPa. This behavior suggests a complex interplay between pressure, temperature, and electronic structure that warrants further exploration. Additionally, our calculations reveal a significant volume collapse occurring around 50 GPa, which may be linked to the observed changes in the electronic properties. These results contribute to a deeper understanding of the pressure effects on the electronic phase transitions in transition metal oxides, particularly in the context of LaMnO$_3$. The implications of these findings extend to potential applications in electronic devices and materials science, where controlling the insulator-metal transition through external pressure could lead to innovative technological advancements.\n\nKeywords: Pressure-induced insulator-metal transition; First-principles calculations; Local spin-density approximation; Generalized gradient approximations; Electronic structure; Band gap.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": 0.1873171623163388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Sloan Digital Sky Survey Quasar Catalog IV. Fifth Data Release .\nAbstract:\nThe Sloan Digital Sky Survey (SDSS) is an ongoing project to map the distribution and motion of galaxies, quasars, stars, and other celestial objects in space. The fourth data release was made public on September 30th 2003. This fifth data release contains more than 100,000 new quasar candidates selected by color criteria from the SDSS imaging survey. These are supplemented with about 20,000 previously known quasars that were not included in previous releases because they did not meet the selection criteria for inclusion at that time. \n \n In addition to these newly discovered quasars, this catalog also includes all quasars found during the first four years of the survey as well as those found since then but which have yet to be released publicly. A total of over 250,000 quasars are now available through this catalog. All of them have been spectroscopically confirmed using observations obtained with the dedicated 2.5-meter telescope located at Apache Point Observatory near Sacramento, California.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Sloan Digital Sky Survey Quasar Catalog IV.Fifth Data Release .Abstract : The Sloan Digital Sky Survey ( SDSS ) is an continuing work to map the distribution and motion of galaxies , quasars , stars , and other celestial entities in space . The fourth information publication was making public on September 30th 2003 .This fifth information update contains more than 100 , 000 new quasar finalists chosen by color factors from the SDSS imaging survey . These are supplemented with about 20 , 000 former named quasars that were not included in earlier versions because they did not meet the selection standards for inclusion at that point .In addition to these newly discovered quasars , this catalog also contains all quasars discovered during the first four seasons of the census as well as those identified since then but which have never to be available openly . A total of over 250 , 000 quasars are now released through this catalog .All of them have been spectroscopically confirmed using observations collected with the dedicated 2 . 5 - meter telescope located at Apache Point Observatory near Sacramento , California .",
        "rewrite_text": "**Title:** The Sloan Digital Sky Survey Quasar Catalog IV: Fifth Data Release\n\n**Abstract:** The Sloan Digital Sky Survey (SDSS) represents a significant ongoing effort aimed at mapping the spatial distribution and dynamics of various celestial objects, including galaxies, quasars, stars, and more. The fourth data release was made public on September 30, 2003, and since then, the project has continued to expand its catalog of astronomical entities. The fifth data release introduces over 100,000 new quasar candidates, meticulously selected based on their color characteristics from the extensive SDSS imaging survey. This release also includes approximately 20,000 previously identified quasars that were not part of earlier catalogs due to not meeting the initial selection criteria. \n\nIn addition to these newly cataloged quasars, the fifth data release encompasses all quasars identified during the first four seasons of the survey, along with additional quasars discovered since then that had not been made publicly available until now. As a result, this comprehensive catalog now features more than 250,000 quasars, all of which have undergone spectroscopic confirmation. These confirmations were achieved using data collected from the dedicated 2.5-meter telescope situated at Apache Point Observatory, located near Sacramento, California. \n\nThis extensive catalog not only enhances our understanding of the quasar population but also serves as a vital resource for researchers in the field of astrophysics, facilitating further studies on the nature and evolution of these distant luminous objects. The fifth data release of the SDSS Quasar Catalog thus marks a significant milestone in the ongoing exploration of the universe, providing a wealth of data for both current and future astronomical research.",
        "ori-fast-z-score": -1.7801724872907798,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": -0.8392543274162825
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Statistical mechanics of complex networks .\nAbstract:\nThe statistical mechanics of networks is an emerging field that has attracted much attention in recent years, especially after the discovery of scale-free and small-world properties in many real world systems such as social networks, biological networks, technological networks etc.. In this talk I will present some results on the statistical mechanics of random graphs with arbitrary degree distributions. The main focus will be on the phase transitions associated with the appearance/disappearance of giant connected components (GCC) in these graphs. We show how to calculate exactly the probability distribution function for GCC size using generating functions techniques. This allows us to study analytically the effect of various parameters like average connectivity, clustering coefficient, node degrees etc., on the formation of GCCs. Finally we discuss possible applications of our results to problems related to epidemic spreading on networks. The statistical mechanics of networks is one of the most active areas of research today. It was pioneered by Barabasi-Albert who discovered that many real world networks have power law degree distributions  1  . Since then there has been considerable interest in understanding the statistical mechanical behavior of different classes of networks  2  .\nIn particular it has been shown recently  3  , that the presence or absence of a giant component (GC), which contains a finite fraction of all nodes in the graph, can lead to interesting phase transitions. For example if the GC disappears at any finite temperature T = 1/β, then the system undergoes a first order transition where the free energy density f ≡ − ln Z/N diverges continuously when β → ∞  4  . Here N denotes the number of nodes in the network while Z is the partition sum defined as:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Statistical mechanics of complex networks . Abstract : The mathematical mechanics of networks is an developing field that has garnered considerable scrutiny in recent years , particularly after the discovery of scale - free and tiny - world properties in many actual world systems such as social systems , biological organizations , technological networks etc . .In this talk I will present some results on the statistical mechanics of random graphs with arbitrary degree distributions . The main attention will be on the phase transitions associated with the appearance / disappearance of giant connected parts ( GCC ) in these graphs .We see how to estimate exactly the probability distribution function for GCC size using generating functions techniques . This enables us to study analytically the impact of several variables like average connectivity , clustering coefficient , node degrees etc . , on the formation of GCCs .Finally we talk possible applied of our findings to problems related to epidemic spreading on networks . The statistical mechanics of networks is one of the most important areas of research today .It was pioneered by Barabasi - Albert who found that several real global networks have power law degree distributions 1 . Since then there has been substantial interest in understanding the statistical mechanical behavior of different categories of networks 2 .In particular it has been shown recently 3 , that the presence or lack of a giant element ( GC ) , which contains a finite fraction of all nodes in the graph , can lead to unusual phase transitions . For instance if the GC disappears at any finite temperature T = 1 / β , then the system undergoes a first order phase where the free energy density g ≡ − ln Z / N diverges steadily when β → ∞ 4 .Here N refers the number of nodes in the network while Z is the partition sum specified as :",
        "rewrite_text": "**Title: Statistical Mechanics of Complex Networks**\n\n**Abstract:** The study of network mechanics is an emerging discipline that has attracted significant attention in recent years, particularly following the identification of scale-free and small-world characteristics in various real-world systems, including social networks, biological structures, and technological frameworks. This presentation will delve into the statistical mechanics of random graphs characterized by arbitrary degree distributions, focusing primarily on the phase transitions linked to the emergence and dissolution of giant connected components (GCCs) within these networks. We will demonstrate how to accurately estimate the probability distribution function for the size of GCCs utilizing generating function techniques. This approach allows for an analytical exploration of how various factors—such as average connectivity, clustering coefficients, and node degrees—affect the formation of GCCs. Additionally, we will discuss potential applications of our findings in addressing issues related to epidemic spread across networks. The statistical mechanics of networks has become a critical area of research, initially pioneered by Barabási and Albert, who discovered that numerous global networks exhibit power-law degree distributions. Since their groundbreaking work, there has been a growing interest in comprehending the statistical mechanical properties of diverse network types. Recent studies have highlighted that the existence or absence of a giant component, which encompasses a significant fraction of the network's nodes, can trigger remarkable phase transitions. For example, if the giant component vanishes at a finite temperature (T = 1/β), the system experiences a first-order phase transition, where the free energy density, defined as g ≡ -ln Z/N, diverges as β approaches infinity. Here, N denotes the total number of nodes in the network, while Z represents the partition function.",
        "ori-fast-z-score": -0.16116459280507606,
        "water-fast-z-score": 4.868329995252495,
        "rewrite-fast-z-score": -0.8723567442899586
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Low-dimensional Supersymmetric Lattice Models .\nAbstract:\nThe low energy effective theories for superstrings are supergravity and supersymmetric gauge theories in four dimensions, which can be obtained by compactifying the extra six spatial dimensions on a Calabi-Yau manifold.  In this talk I will discuss some recent results about lattice models that provide an alternative approach to studying these theories. The basic idea is to use Monte Carlo simulations to study supersymmetric field theories defined on a finite number of points (the sites) of a regular d-dimensional hypercubic lattice with periodic boundary conditions. These models have been studied extensively over the past few years using numerical techniques such as exact diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently we developed new Monte Carlo simulation techniques based on the worm algorithm that allow us to simulate large systems at very high temperatures where conventional Monte Carlo methods fail because they suffer from critical slowing down. We used our new method to calculate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory coupled to matter fields in various representations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Low - dimensional Supersymmetric Lattice Models . Abstract : The lowest energy effective models for superstrings are supergravity and supersymmetric gauge theories in four dimensions , which can be obtained by compactifying the extra six spatial dimensions on a Calabi - Yau manifold .In this talk I will explore some latest findings about lattice models that provide an different approach to investigating these theories . The basic idea is to use Monte Carlo simulations to study supersymmetric field theories constructed on a finite number of points ( the sites ) of a regular d - dimensional hypercubic crystal with periodic border conditions .These methods have been studied thoroughly over the previous few years employing mathematical techniques such as approximate diagonalization , quantum Monte Carlo methods , and density matrix renormalization group schemes . Recently we developed novel Monte Carlo simulation algorithms based on the worm algorithm that enable us to simulate large systems at very high altitudes where conventional Monte Carlo methods fail because they suffer from critical speed down .We utilized our new method to estimate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang - Mills theory and the N = 1 supersymmetric U ( 1 ) gauge theory connected to matter fields in different representations .",
        "rewrite_text": "Title: Low-Dimensional Supersymmetric Lattice Models\n\nAbstract: In the realm of theoretical physics, the most fundamental effective models for superstrings are identified as supergravity and supersymmetric gauge theories within a four-dimensional framework. These models emerge through the compactification of the additional six spatial dimensions on a Calabi-Yau manifold. This presentation delves into recent advancements in lattice models that offer an alternative methodology for probing these complex theories. The core concept revolves around employing Monte Carlo simulations to investigate supersymmetric field theories formulated on a finite lattice, represented by a regular d-dimensional hypercubic crystal with periodic boundary conditions. Over the past few years, these simulation techniques have been rigorously examined, utilizing various mathematical approaches such as approximate diagonalization, quantum Monte Carlo methods, and density matrix renormalization group techniques. \n\nRecently, we have introduced innovative Monte Carlo simulation algorithms based on the worm algorithm, which significantly enhance our ability to simulate large systems at elevated altitudes—an area where traditional Monte Carlo methods encounter limitations due to critical slowdown. By leveraging this new approach, we have successfully estimated the free energies of several distinct supersymmetric lattice models, including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory, which incorporates matter fields in diverse representations. These findings not only contribute to our understanding of supersymmetric theories but also pave the way for further exploration of lattice models in theoretical physics, potentially leading to new insights into the fundamental nature of supersymmetry and its implications in higher-dimensional frameworks.",
        "ori-fast-z-score": 0.19802950859533489,
        "water-fast-z-score": 5.81344274116933,
        "rewrite-fast-z-score": -0.7242859683401482
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  NodeTrix: Hybrid Representation for Analyzing Social Networks .\nAbstract:\nSocial networks are ubiquitous in our daily life, and have been studied extensively by researchers across many disciplines. However, the analysis of social network data is often challenging due to its complexity. In this work we propose NodeTrix, an efficient hybrid representation that can be used to analyze large-scale social networks efficiently. We show how NodeTrix can be applied to solve several important problems including community detection, link prediction, node classification, and influence maximization. Our experiments on real-world datasets demonstrate that NodeTrix outperforms state-of-the-art approaches significantly both in terms of efficiency and effectiveness. 1 Introduction Social networks play an increasingly important role in people s lives. They provide us with new ways to communicate with each other, share information, collaborate, or even make friends. As such, they have attracted much attention from researchers across various fields ranging from sociology  1  , psychology  2  , biology  3  , computer science  4  , engineering  5  , etc.. The rapid development of online social media has led to unprecedented growth in the amount of available social network data  6  . For example, Facebook alone now contains more than one billion active users  7  .\nHowever, analyzing large volumes of social network data remains a challenge because it usually involves complex relationships among nodes  8  . To tackle these challenges, recent research efforts focus on developing effective representations for social networks  9  -  11  . These representations aim at capturing different aspects of social networks while being able to scale up well when dealing with massive amounts of data  12  . Among them, matrix factorization techniques  13  -  15  have shown great promise as they allow us to represent social networks using low-rank matrices  16  . Matrix factorization methods decompose a given adjacency matrix into two smaller matrices (i.e., latent factors) which capture structural properties of the original graph  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : NodeTrix : Hybrid Representation for Analyzing Social Networks . Abstract : Social networks are ubiquitous in our daily living , and have been studied frequently by researchers across many disciplines .However , the analysis of social group information is often challenging due to its complexity . In this project we develop NodeTrix , an efficient hybrid representation that can be used to analyze large - scale social systems efficiently .We see how NodeTrix can be applied to solve many important problems namely community screening , link modeling , node classification , and influence maximization . Our experiments on real - time datasets prove that NodeTrix outperforms state - of - the - art methods significantly both in terms of efficiency and effectiveness .1 Introduction Social networks take an increasingly important role in persons s lives . They offer us with innovative ways to interact with each other , share data , collaborate , or even get allies .As such , they have garnered many scrutiny from researchers across numerous topics including from economics 1 , psychology 2 , mathematics 3 , computer science 4 , engineering 5 , etc . . The rapid advancement of internet social publishing has led to unprecedented growth in the quantity of available social platform data 6 .For instance , Facebook alone now contains more than one billion active people 7 . However , examining massive numbers of social group information remains a problem because it often includes difficult connections among nodes 8 .To tackle these problems , recent study efforts focus on developing accurate representations for social organizations 9 - 11 . These representations aim at capturing different components of social organizations while being able to scale up well when dealing with massive amounts of data 12 .Among them , matrix factorization techniques 13 - 15 have shown great success as they allow us to depict public networks using reduced - rank matrices 16 . Matrix factorization techniques decompose a given adjacency vector into two smaller matrices ( i . e . , latent variables ) which capture structural properties of the original graph 17 .",
        "rewrite_text": "**Title:** NodeTrix: A Hybrid Representation for Analyzing Social Networks\n\n**Abstract:** Social networks are an integral part of modern life, influencing how individuals interact, share information, and collaborate. Researchers from various fields, including economics, psychology, mathematics, computer science, and engineering, have extensively studied these networks due to their complexity and significance. However, analyzing the intricate relationships within social groups poses considerable challenges. In this study, we introduce NodeTrix, a novel hybrid representation designed to facilitate the efficient analysis of large-scale social systems. NodeTrix addresses several critical problems in social network analysis, including community detection, link prediction, node classification, and influence maximization. Our empirical evaluations, conducted on real-world datasets, demonstrate that NodeTrix significantly outperforms existing state-of-the-art methodologies in both efficiency and effectiveness. The rapid growth of social media platforms, exemplified by Facebook's user base exceeding one billion, has resulted in an overwhelming amount of data that requires sophisticated analytical techniques. Traditional methods often struggle to manage the complex interconnections among nodes, leading to a need for improved representation strategies. Recent research has focused on developing accurate models that can scale effectively with increasing data volumes. Among these, matrix factorization techniques have emerged as a promising approach, enabling the representation of social networks through reduced-rank matrices. These techniques decompose adjacency matrices into smaller latent variable matrices, capturing the structural characteristics of the original graph. NodeTrix builds upon these advancements, offering a comprehensive framework that enhances the analysis of social networks while maintaining computational efficiency. Through our work, we aim to provide researchers and practitioners with a robust tool for exploring the dynamics of social interactions in an increasingly interconnected world.",
        "ori-fast-z-score": -0.07647191129018725,
        "water-fast-z-score": 8.84491707929107,
        "rewrite-fast-z-score": -0.23942606534028665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Holography in Three-dimensional Kerr-de Sitter Space with a Gravitational Chern-Simons Term .\nAbstract:\nWe study the holographic entanglement entropy for three-dimensional de Sitter space with gravitational Chern-Simons term by using the replica trick and the covariant phase-space method. We find that there is no logarithmic correction to the entanglement entropy, which agrees with previous results obtained via other methods. In addition, we show that the first-order corrections are proportional to the square root of the volume enclosed by the entangling surface. Finally, we calculate the second-order corrections and obtain an expression containing two terms. One of them has been previously found in Ref.  Phys. Rev. D 98 (2018) 084011  while another one is new. The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points. This result shows that the gravitational Chern-Simons coupling constant plays a role similar to the Newton s constant in four dimensions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Holography in Three - dimensional Kerr - de Sitter Space with a Gravitational Chern - Simons Term . Abstract : We research the holographic entanglement entropy for three - dimensional de Sitter space with gravitational Chern - Simons term by using the replica trick and the covariant phase - space method .We see that there is no logarithmic correction to the entanglement entropy , which agrees with previous findings obtained via other methods . In addition , we prove that the first - order corrections are proportional to the square root of the volume enclosed by the entangling surface .Finally , we determine the second - order corrections and find an expression containing two terms . One of them has been previously found in Ref .Phys.Rev.D 98 ( 2018 ) 084011 while another one is new . The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points .This result suggests that the gravitational Chern - Simons interaction function holds a role similar to the Newton s constant in four dimensions .",
        "rewrite_text": "In this study, we investigate the holographic entanglement entropy within the context of three-dimensional de Sitter space that incorporates a gravitational Chern-Simons term. Utilizing the replica trick alongside the covariant phase-space method, we analyze the properties of entanglement entropy in this specific gravitational framework. Our findings reveal that there are no logarithmic corrections to the entanglement entropy, a result that is consistent with previous research conducted through alternative methodologies. Furthermore, we demonstrate that the first-order corrections to the entanglement entropy are directly proportional to the square root of the volume enclosed by the entangling surface. \n\nIn our exploration of higher-order corrections, we derive the second-order corrections and present an expression that comprises two distinct terms. One of these terms has been previously identified in the literature, specifically in the reference Phys.Rev.D 98 (2018) 084011, while the other term is novel. The new term can be expressed as a summation over all possible contractions involving the Riemann tensor and its derivatives evaluated at the boundary points. This significant finding implies that the gravitational Chern-Simons interaction function plays a role analogous to that of Newton's constant in four-dimensional spacetime. Our results contribute to the understanding of holographic principles in curved spacetimes and highlight the intricate relationship between gravitational theories and quantum entanglement. This work opens avenues for further research into the implications of gravitational Chern-Simons terms in holographic contexts and their potential effects on entanglement measures in various dimensions.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 3.4465617474213164,
        "rewrite-fast-z-score": 1.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Atmospheric Dynamics of Short-period Extra Solar Gas Giant Planets I: Dependence of Night-Side Temperature on Opacity .\nAbstract:\nWe present the first results for atmospheric dynamics in short period extra-solar gas giant planets (EGPs) using 3D general circulation models with radiative transfer and realistic opacities. We find that the night-side temperature is strongly dependent on opacity, which determines how much energy can be transported to space by radiation. The day-night contrast increases as we decrease the opacity because less heat escapes through the nightside atmosphere. This effect is more pronounced at lower pressures where convection becomes inefficient. For low enough opacities, the planet cools down completely during its orbit resulting in an extremely cold night side. Our simulations show that EGPs are likely to have very different climates depending on their composition.  Keywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Atmospheric Dynamics of Short - duration Extra Solar Gas Giant Planets I : Dependence of Night - Side Temperature on Opacity . Abstract : We report the first findings for atmospheric evolution in small period extra - solar gas giant worlds ( EGPs ) using 3D general circulation estimates with radiative transfer and realistic opacities .We see that the night - side temperature is strongly dependent on opacity , which determines how many heat can be transported to space by radiation . The day - night difference grows as we decrease the opacity because lighter energy escapes through the nightside environment .This phenomenon is more pronounced at lower pressures where circulation becomes inefficient . For lowest sufficient opacities , the planet cools down fully during its orbit resulting in an incredibly cold night side .Our simulations signal that EGPs are likely to have very different climates based on their composition . Keywords : General Circulation Modeling , Extrasolar Planetary Systems , Radiation Transfer , Climate , Atmosphere , Energy Transport , Convection , Cooling Rates , Day - Night Contrast",
        "rewrite_text": "Title: Atmospheric Dynamics of Short-Duration Extrasolar Gas Giant Planets I: The Influence of Opacity on Night-Side Temperature\n\nAbstract: In this study, we present groundbreaking insights into the atmospheric evolution of short-period extrasolar gas giant planets (EGPs) through the application of three-dimensional general circulation models that incorporate radiative transfer and realistic opacity values. Our findings reveal a significant correlation between night-side temperature and atmospheric opacity, which plays a crucial role in determining the extent of heat loss to space via radiation. As we reduce the opacity, we observe an increase in the temperature differential between the day and night sides of the planet, as less energy is retained, allowing for greater thermal escape from the night-side atmosphere. This effect is particularly pronounced at lower atmospheric pressures, where the efficiency of circulation diminishes, leading to more extreme temperature variations. In scenarios with minimal opacity, the planet experiences substantial cooling throughout its orbit, resulting in an exceptionally frigid night-side environment. Our simulations suggest that the climatic conditions of EGPs are likely to vary significantly based on their atmospheric composition, highlighting the importance of opacity in shaping their thermal profiles. These results contribute to a deeper understanding of the complex atmospheric dynamics at play in extrasolar planetary systems and underscore the need for further investigation into the factors influencing climate and energy transport in these distant worlds. \n\nKeywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 4.541868715470696,
        "rewrite-fast-z-score": 2.6457513110645903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  y-scaling in Quasielastic Electron Scattering from Nuclei .\nAbstract:\nWe present the results for y-scaling analysis performed on data collected at Jefferson Lab Hall A experiment E06-010 using an unpolarized electron beam with energies ranging between 1 and 5 GeV incident upon a liquid hydrogen target. The experimental data are compared to theoretical calculations based on relativistic impulse approximation (RIA) model, which is used as a baseline reference calculation. We also compare our results to those obtained by applying the same analysis technique to previously published data sets taken at lower energy ranges. \n \n In this work we have studied scaling behavior of quasi-elastic scattering cross sections from nuclei over wide range of kinematics covering both longitudinal and transverse response functions. Our study has been carried out within the framework of relativistic impulse approximation (RIA), where nuclear responses are calculated using realistic nucleonnucleon interactions. For comparison purposes, we have applied similar analysis techniques to two different data sets: one measured recently at Jefferson Lab Hall A  1  , and another one taken earlier  2  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : y - scaling in Quasielastic Electron Scattering from Nuclei . Abstract : We present the results for y - scaling research performed on evidence generated at Jefferson Lab Hall A project E06 - 010 using an unpolarized electron beam with energies ranging between 1 and 5 GeV incident upon a liquid hydrogen target .The empirical data are compared to theoretical calculations based on relativistic impulse approximation ( RIA ) model , which is utilized as a baseline reference calculation . We additionally compare our findings to those achieved by using the same evaluation technique to earlier published evidence sets taken at lower energy intervals .In this research we have researched scaling behavior of quasi - elastic scattering cross sections from nuclei over broad variety of kinematics encompassing both longitudinal and longitudinal response functions . Our study has been carried out within the framework of relativistic impulse approximation ( RIA ) , where radioactive responses are measured using accurate nucleonnucleon interactions .For comparison purposes , we have applied comparable analysis methods to two different data sets : one measured recently at Jefferson Lab Hall A 1 , and another one taken prior 2 .",
        "rewrite_text": "We present a comprehensive analysis of y-scaling in quasielastic electron scattering from nuclei, based on data collected during the Jefferson Lab Hall A experiment E06-010. This study utilized an unpolarized electron beam with energies ranging from 1 to 5 GeV, directed at a liquid hydrogen target. Our empirical findings are juxtaposed with theoretical predictions derived from the relativistic impulse approximation (RIA) model, which serves as a foundational reference for our calculations. Furthermore, we extend our comparison to earlier published datasets obtained at lower energy ranges, employing the same analytical techniques to ensure consistency and reliability in our results.\n\nThe primary focus of our research is to investigate the scaling behavior of quasielastic scattering cross sections across a wide spectrum of kinematic variables, specifically examining both longitudinal and transverse response functions. This exploration is conducted within the RIA framework, where we accurately measure the nucleon-nucleon interactions that underpin the radioactive responses. To enrich our analysis, we have applied similar evaluation methodologies to two distinct datasets: one collected recently at Jefferson Lab Hall A and another from previous experiments.\n\nOur findings contribute to a deeper understanding of the underlying mechanisms governing quasielastic electron scattering and highlight the significance of y-scaling in nuclear physics. By comparing our results with theoretical models and previous experimental data, we aim to provide insights into the scaling phenomena observed in electron-nucleus interactions, thereby enhancing the overall comprehension of nuclear structure and dynamics. This work not only reinforces the validity of the RIA model but also sets the stage for future investigations into the complexities of nuclear scattering processes.",
        "ori-fast-z-score": -1.0540925533894598,
        "water-fast-z-score": 5.555909634626167,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analysis of Spitzer Spectra of Irradiated Planets: Evidence for Water Vapor? .\nAbstract:\nWe present the first analysis of water vapor in irradiated planets using infrared (IR) spectroscopy with the Infrared Spectrograph on board the Spitzer Space Telescope. We have analyzed four transiting exoplanet systems, HD 189733b, HD 209458b, WASP-12b and XO-1b, which are known to be strongly irradiated by their host stars. The IR spectra were obtained during secondary eclipse events when the planet passes behind its star as seen from Earth. Our results show that all these planets exhibit strong absorption features at wavelengths longer than 5 microns due to water vapor in their atmospheres. These observations provide direct evidence for the presence of water vapor in highly-irradiated planetary atmospheres.  Keywords: Exoplanet, Transmission spectrum, Secondary eclipse, Water vapor, Infrared spectrophotometry, Spitzer Space Telescope. 1 Introduction   Water is one of the most important molecules in our Solar System because it plays an essential role in life processes. It has been detected in many different environments ranging from comets to icy satellites such as Europa or Enceladus. However, despite numerous efforts over several decades, no unambiguous detection of water had yet been reported outside our Solar System until recently. This situation changed dramatically thanks to space-based observatories like Hubble Space Telescope (HST), Chandra X-ray Observatory, and especially Spitzer Space Telescope (Werner et al., 2004) .  Since its launch in 2003, Spitzer has observed thousands of targets including hundreds of extrasolar planets. Among them, there are some very interesting cases where the planet orbits close to its parent star so that the intense stellar radiation heats up the atmosphere of the planet significantly. As a result, the atmospheric composition can change drastically compared to what we know about terrestrial planets in our Solar System. For example, if the temperature becomes high enough, hydrogen could escape from the planet s upper atmosphere into space leaving only helium behind (Lammer et al., 2003; Baraffe et al., 2004; Yelle et al., 2006) , while other species may condense out onto",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Analysis of Spitzer Spectra of Irradiated Planets: Evidence for Water Vapor?.Abstract : We present the first assessment of water vapor in irradiated planets using infrared ( IR ) spectroscopy with the Infrared Spectrograph on board the Spitzer Space Telescope . We have analyzed four transiting exoplanet systems , HD 189733b , HD 209458b , WASP - 12b and XO - 1b , which are known to be highly irradiated by their host stars .The IR spectra were obtained during secondary eclipse events when the planet walks behind its star as watched from Earth . Our results show that all these planets exhibit strong absorption elements at wavelengths greater than 5 microns due to water vapor in their atmospheres .These measurements give substantial proof for the presence of liquid vapor in highly - irradiated planetary atmospheres . Keywords : Exoplanet , Transmission spectrum , Secondary eclipse , Water vapor , Infrared spectrophotometry , Spitzer Space Telescope .1 Introduction Water is one of the most important molecules in our Solar System because it serves an essential part in living systems . It has been detected in multiple diverse settings ranging from comets to icy spacecraft such as Europa or Enceladus .However , despite several efforts over numerous years , no unambiguous detection of water had yet been reported outside our Solar System until recently . This condition changed dramatically due to space - based observatories like Hubble Space Telescope ( HST ) , Chandra X - ray Observatory , and particularly Spitzer Space Telescope ( Werner et al . , 2004 ) .Since its launch in 2003 , Spitzer has observed thousands of targets including hundreds of extrasolar stars . Among them , there are some very interesting cases where the planet orbits close to its father star so that the powerful stellar radiation heats up the atmosphere of the planet significantly .As a result , the atmospheric composition can shift drastically compared to what we know about terrestrial worlds in our Solar System . For instance , if the temperature gets high enough , hydrogen could exit from the planet s upper atmosphere into space leaving only helium behind ( Lammer et al . , 2003 ; Baraffe et al . , 2004 ; Yelle et al . , 2006 ) , while other species may condense out onto",
        "rewrite_text": "**Title:** Analysis of Spitzer Spectra of Irradiated Planets: Evidence for Water Vapor?\n\n**Abstract:** In this study, we provide a comprehensive analysis of water vapor in irradiated exoplanets through infrared (IR) spectroscopy utilizing the Infrared Spectrograph aboard the Spitzer Space Telescope. Our investigation focuses on four transiting exoplanet systems: HD 189733b, HD 209458b, WASP-12b, and XO-1b, all of which experience significant irradiation from their respective host stars. The IR spectra were captured during secondary eclipse events, a phenomenon where the planet passes behind its star from the perspective of Earth. Our findings reveal that each of these exoplanets displays pronounced absorption features at wavelengths exceeding 5 microns, indicative of water vapor present in their atmospheres. These observations provide compelling evidence for the existence of water vapor in the atmospheres of highly irradiated planets, marking a significant advancement in our understanding of exoplanetary atmospheres. \n\n**Keywords:** Exoplanet, Transmission spectrum, Secondary eclipse, Water vapor, Infrared spectrophotometry, Spitzer Space Telescope.\n\n**1 Introduction:** Water is a fundamental molecule within our Solar System, playing a crucial role in supporting life. It has been identified in various environments, from comets to icy moons such as Europa and Enceladus. However, despite extensive research over the years, a definitive detection of water outside our Solar System had remained elusive until recently. This situation has changed significantly with the advent of space-based observatories, including the Hubble Space Telescope (HST), Chandra X-ray Observatory, and notably, the Spitzer Space Telescope (Werner et al., 2004). Since its launch in 2003, Spitzer has observed thousands of celestial targets, including numerous exoplanets. Among these, several planets orbit their stars at close distances, resulting in intense stellar radiation that substantially heats their atmospheres. This heating can lead to significant alterations in atmospheric composition compared to terrestrial planets in our Solar System. For example, at sufficiently high temperatures, hydrogen may escape from the upper atmosphere into space, leaving behind helium (Lammer et al., 2003; Baraffe et al., 2004; Yelle et al., 2006), while other atmospheric constituents may condense.",
        "ori-fast-z-score": -1.0947974973864747,
        "water-fast-z-score": 4.599331055038999,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spin relaxation due to the Bir-Aronov-Pikus mechanism in intrinsic and $p$-type GaAs quantum wells from a fully microscopic approach .\nAbstract:\nWe present an ab initio study on spin relaxation mechanisms in bulk, single- and double-quantum-well (DQW) structures based on zinc-blende semiconductors such as GaAs or InP. We focus our attention on the so-called Bir-Aronov-Pikuz mechanism which is responsible for spin-flip transitions between conduction-band states with different orbital angular momenta. The main results are summarized below.  For bulk materials we find that the dominant contribution comes from intra-valley scattering processes involving heavy-hole bands. This result agrees well with previous theoretical studies performed within effective-mass approximations. However, by using realistic band-structure calculations we show that inter-valley contributions can also play an important role when considering DQWs grown along non  001  directions. Finally, we discuss how these findings could be used to improve existing models describing spin relaxation times in semiconductor nanostructures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spin relaxation related to the Bir - Aronov - Pikus mechanism in intrinsic and $ p $ - class GaAs quantum wells from a completely microscopic perspective . Abstract : We report an ab initio investigation on spin relaxation mechanisms in bulk , double - and double - quantum - well ( DQW ) formations based on aluminium - blende semiconductors such as GaAs or InP .We focus our focus on the so - called Bir - Aronov - Pikuz process which is responsible for spin - flip transitions between conduction - band states with various orbital angular momenta . The main results are presented below .For bulk materials we find that the dominant contribution comes from intra - valley reflection processes involving heavy - hole bands . This result agrees well with previous conceptual research performed within effective - mass approximations .However , by using accurate band - structure estimates we prove that inter - valley contributions can also play an important role when assessing DQWs grown along non 001 directions . Finally , we talk how these results could be used to improve established models explaining spin relaxation times in semiconductor nanostructures .",
        "rewrite_text": "We present a comprehensive ab initio study of spin relaxation mechanisms in bulk, double, and double quantum well (DQW) structures composed of aluminum-blende semiconductors, specifically GaAs and InP. Our primary focus is on the Bir-Aronov-Pikus (BAP) process, which facilitates spin-flip transitions among conduction band states characterized by different orbital angular momenta. Our findings reveal that, in bulk materials, the predominant contribution to spin relaxation arises from intra-valley reflection processes involving heavy-hole bands. This observation aligns with earlier theoretical work conducted using effective mass approximations. However, our analysis, which incorporates precise band-structure calculations, demonstrates that inter-valley contributions can significantly influence spin relaxation, particularly in DQWs oriented along non-001 crystallographic directions. These insights not only enhance our understanding of spin dynamics in semiconductor systems but also suggest potential refinements to existing models that describe spin relaxation times in semiconductor nanostructures. By elucidating the intricate interplay between various spin relaxation mechanisms, our research contributes to the broader field of spintronics, where the manipulation of electron spins is crucial for the development of advanced electronic devices. Ultimately, our results pave the way for improved theoretical frameworks and experimental approaches aimed at optimizing spin coherence in semiconductor materials, which is essential for the realization of next-generation spin-based technologies.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 4.6467017049401695,
        "rewrite-fast-z-score": 1.2018504251546631
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fractal dimension of domain walls in two-dimensional Ising spin glasses .\nAbstract:\nWe study the fractal dimension of domain walls (DWs) in two dimensional Ising spin glasses with nearest neighbor interactions and random bonds using Monte Carlo simulations at finite temperatures. We find that DWs are fractals for all values of temperature studied here, i.e., T = 0.5J/kB to 1.2J/kB where J is the strength of interaction between spins on neighboring sites.  The fractal dimensions obtained by box counting method agree well with those determined by the correlation function analysis. In addition we show that the fractal dimension decreases as the temperature increases. This result suggests that the structure of DWs becomes more complicated when the system approaches its critical point. Finally it should be noted that our results can also be applied to other systems such as vortex lines in type-II superconductors or dislocation networks in crystals. Two-dimensional Ising spin glasses have been extensively investigated both experimentally  1  and theoretically  2  . It has been shown that these models exhibit many interesting phenomena including phase transitions  3  , spin-glass states  4  , and glassy dynamics  5  .\nIn this work we focus on one particular aspect of the model which is the fractal nature of domain walls  6  . Domain wall refers to an interface separating different ordered phases  7, 8  . For example, in ferromagnetic materials there exist two types of domains; up and down magnetization  9  . These domains are separated by interfaces called domain walls  10  . Similarly, in antiferromagnets  11  , there exists four possible orientations of magnetic moments  12  ; three of them form triangular sublattices while the fourth forms a square lattice  13  . Therefore, there will be six types of domain walls  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fractal dimension of domain walls in two - dimensional Ising spin glasses . Abstract : We research the fractal dimension of domain barriers ( DWs ) in two dimensional Ising spin glasses with nearest neighbor interactions and random bonds using Monte Carlo simulations at finite temperatures .We see that DWs are fractals for all values of temperature tested here , i . e . , T = 0 . 5J / kB to 1 . 2J / kB where J is the strength of coupling between spins on nearby locations . The fractal sizes obtained by box counting method comply better with those determined by the correlation function analysis .In addition we find that the fractal dimension decreases as the temperature increases . This result suggests that the composition of DWs changes more complicated when the system approaches its critical position .Finally it should be mentioned that our findings can also be applied to other structures such as vortex lines in type - II superconductors or dislocation networks in crystals . Two - dimensional Ising spin glasses have been heavily examined both experimentally 1 and theoretically 2 .It has been shown that these models exhibit several interesting phenomena including phase transitions 3 , spin - glass states 4 , and glassy dynamics 5 . In this research we focus on one special aspect of the model which is the fractal nature of domain walls 6 .Domain wall refers to an interface separating different ordered phases 7 , 8 . For instance , in ferromagnetic media there exist two forms of domains ; up and down magnetization 9 .These residues are separated by interfaces called domain barriers 10 . Similarly , in antiferromagnets 11 , there exists four possible orientations of magnetic moments 12 ; three of them form square sublattices while the third creates a square lattice 13 .Therefore , there will be six kinds of domain walls 14 .",
        "rewrite_text": "**Title: Fractal Dimension of Domain Walls in Two-Dimensional Ising Spin Glasses**\n\n**Abstract:** This study investigates the fractal dimension of domain walls (DWs) in two-dimensional Ising spin glasses characterized by nearest neighbor interactions and random bonds, employing Monte Carlo simulations at various finite temperatures. Our findings reveal that DWs exhibit fractal properties across all tested temperature ranges, specifically from T = 0.5J/kB to 1.2J/kB, where J represents the coupling strength between adjacent spins. The fractal dimensions derived from the box counting method align more closely with those obtained through correlation function analysis, indicating a robust methodology for assessing fractal characteristics. Notably, we observe a decrease in fractal dimension with increasing temperature, suggesting that the structure of DWs becomes increasingly complex as the system nears its critical point. This behavior implies significant changes in the composition of DWs under varying thermal conditions. Furthermore, the implications of our results extend beyond Ising spin glasses; they may also be relevant to other physical systems, such as vortex lines in type-II superconductors and dislocation networks in crystalline materials. The two-dimensional Ising spin glass model has been extensively studied in both experimental and theoretical contexts, revealing a wealth of intriguing phenomena, including phase transitions, spin-glass states, and glassy dynamics. Our research specifically addresses the fractal nature of domain walls, which serve as interfaces that separate distinct ordered phases. For example, in ferromagnetic materials, domains with up and down magnetization are separated by these domain barriers. In antiferromagnetic systems, four orientations of magnetic moments exist, leading to the formation of six distinct types of domain walls. This exploration of the fractal dimension of DWs contributes to a deeper understanding of the underlying mechanisms in spin glass systems and their broader applications in condensed matter physics.",
        "ori-fast-z-score": 0.7016464154456235,
        "water-fast-z-score": 6.092717958449424,
        "rewrite-fast-z-score": -2.090605025017727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shock and Release Temperatures in Molybdenum .\nAbstract:\nThe shock temperatures for molybdenum were determined by measuring the electrical resistance of samples shocked to pressures up to 1,000 kilobars (1 Mbar). The release temperature was measured using an optical pyrometer on samples that had been heated with laser light after being shocked at various levels of pressure. \n \n Shocks produced by a pulsed power machine were used to compress the sample material between two electrodes. A voltage pulse applied across these electrodes caused current flow through the compressed material which generated Joule heating. This heat increased the resistivity of the material causing it to expand rapidly as its temperature rose above the Curie point. As this expansion occurred, the resistance dropped dramatically resulting in a sharp increase in current flowing into the sample. When the current reached a critical value, the sample exploded releasing most of its stored energy. The explosion also destroyed one or both of the electrodes so that no further measurement could be made until new ones were installed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shock and Release Temperatures in Molybdenum . Abstract : The blast conditions for molybdenum were determined by monitoring the electrical resistance of samples shocked to pressures up to 1 , 000 kilobars ( 1 Mbar ) .The release temperature was measured using an optical pyrometer on samples that had been heated with laser light after being shocked at several rates of pressure . Shocks created by a pulsed power machine were used to compress the sample material between two electrodes .A voltage beam applied across these electrodes induced charge flow through the compressed material which generated Joule warming . This warmth increased the resistivity of the material creating it to expand rapidly as its temperature climbed above the Curie point .As this growth resulted , the resistance dropped dramatically resulting in a sharp increase in current flowing into the sample . When the current reached a critical level , the sample erupted releasing most of its stored power .The explosion also damaged one or both of the electrodes so that no further measurement could be made until fresh ones were replaced .",
        "rewrite_text": "**Title:** Shock and Release Temperatures in Molybdenum\n\n**Abstract:** This study investigates the blast conditions of molybdenum by analyzing the electrical resistance of samples subjected to extreme pressures of up to 1,000 kilobars (1 Mbar). The research employs an optical pyrometer to measure the release temperature of the samples, which were heated using laser light following shock compression at various pressure rates. The shock waves were generated by a pulsed power machine, which compressed the molybdenum samples positioned between two electrodes. A voltage beam applied across these electrodes facilitated charge flow through the compressed material, leading to Joule heating. This process caused an increase in the material's resistivity, resulting in rapid thermal expansion as the temperature exceeded the Curie point. As the sample expanded, a significant drop in electrical resistance was observed, which corresponded to a sharp rise in the current flowing through the material. Upon reaching a critical current threshold, the sample underwent a catastrophic failure, releasing a substantial amount of stored energy in the form of an explosion. This explosive event often resulted in damage to one or both electrodes, necessitating their replacement before further measurements could be conducted. The findings of this research provide valuable insights into the behavior of molybdenum under extreme conditions, contributing to the understanding of its thermodynamic properties and potential applications in high-pressure environments. The implications of these results extend to various fields, including materials science and engineering, where knowledge of shock and release temperatures is crucial for the development of advanced materials capable of withstanding extreme conditions.",
        "ori-fast-z-score": 0.5423261445466404,
        "water-fast-z-score": 5.1759731137650435,
        "rewrite-fast-z-score": 1.7541160386140586
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Compact starburst in the central regions of Seyfert galaxies .\nAbstract:\nWe present new near-infrared integral field spectroscopy observations of two nearby active galactic nuclei (AGNs) with compact nuclear star formation, NGC 1068 and Mrk 3. The data were obtained using SINFONI at VLT/UT4. We find that both AGNs have very high infrared luminosities (LIR ~10^11 Lsun), which are dominated by emission lines originating from warm molecular gas excited by young stars. In addition to these bright line features we detect spatially extended continuum emission on scales of several hundred parsecs around each nucleus. This is likely due to dust heated by the intense radiation fields produced by the AGN and/or hot young stars. Our results show that the presence of an AGN does not necessarily prevent massive star formation within its host galaxy. These findings suggest that there may be a close link between black hole growth and nuclear star formation activity.  Keywords: Active Galactic Nuclei",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Compact starburst in the central regions of Seyfert galaxies . Abstract : We report new near - infrared integral field spectroscopy observations of two distant active galactic nuclei ( AGNs ) with compact nuclear star formation , NGC 1068 and Mrk 3 .The data were obtained using SINFONI at VLT / UT4 . We see that both AGNs have very high infrared luminosities ( LIR ~ 10 ^ 11 Lsun ) , which are dominated by absorption patterns derived from hot molecular gas excited by young galaxies .In addition to these bright line characteristics we perceive spatially extended continuum emission on scales of several hundred parsecs around each core . This is probably due to dust cooled by the intense radiation fields produced by the AGN and / or hotter early stars .Our results show that the presence of an AGN does not necessarily suppress massive star formation within its target galaxy . These conclusions propose that there may be a close link between black hole growth and nuclear star formation activity .Keywords: Active Galactic Nuclei",
        "rewrite_text": "We present new near-infrared integral field spectroscopy observations of two distant active galactic nuclei (AGNs) exhibiting compact nuclear star formation: NGC 1068 and Mrk 3. The observations were conducted using the SINFONI instrument at the VLT/UT4. Our findings reveal that both AGNs possess exceptionally high infrared luminosities (LIR ~ 10^11 Lsun), primarily influenced by absorption features from hot molecular gas that is excited by the presence of young stellar populations. In addition to the prominent emission lines, we also detect spatially extended continuum emission spanning several hundred parsecs around each nucleus. This extended emission is likely attributed to dust that has been cooled by the intense radiation fields generated by the AGN and/or by the hotter early-type stars. Importantly, our results indicate that the presence of an AGN does not inherently inhibit massive star formation within the host galaxy. These observations suggest a potential correlation between the growth of supermassive black holes and the activity of nuclear star formation. This study contributes to our understanding of the interplay between AGN activity and star formation processes in the central regions of galaxies, highlighting the complexity of these relationships in the context of galaxy evolution. \n\nKeywords: Active Galactic Nuclei, star formation, infrared spectroscopy, NGC 1068, Mrk 3.",
        "ori-fast-z-score": -0.329292779969071,
        "water-fast-z-score": 3.927922024247863,
        "rewrite-fast-z-score": 1.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gauge symmetry in Kitaev-type spin models and index theorems on odd manifolds .\nAbstract:\nWe study the gauge structure of quantum spin systems with local interactions, which are described by the so-called Kitaev model. We show that this model is equivalent to an exactly solvable lattice gauge theory (LGT) whose action functional is given by the Chern-Simons term. The equivalence between these two theories allows us to use powerful techniques developed for LGTs such as the index theorem or the localization technique. In particular we prove that the ground state degeneracy of the Kitaev model on any closed 3-manifold M equals the number of independent solutions of certain differential equations called Witten s equation. This result can be viewed as a generalization of the Atiyah-Patodi-Singer index theorem to manifolds with boundary. Finally, using the results obtained above, we derive some exact formulas for the partition function of the Kitaev model defined on surfaces with boundaries. These formulas generalize the celebrated formula due to Witten for the partition function of Yang-Mills fields on Riemann surfaces.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gauge symmetry in Kitaev - type spin systems and index theorems on odd manifolds . Abstract : We explore the gauge formation of quantum spin systems with local interactions , which are explained by the so - called Kitaev description .We see that this description is analogous to an exactly solvable lattice gauge theory ( LGT ) whose action functional is given by the Chern - Simons term . The equivalence between these two concepts lets us to use powerful tools developed for LGTs such as the index theorem or the localization technique .In particular we prove that the ground state degeneracy of the Kitaev theory on any closed 3 - manifold M equals the number of independent solutions of certain differential equations called Witten s equation . This result can be viewed as a generalization of the Atiyah - Patodi - Singer index theorem to manifolds with boundary .Finally , using the results derived above , we derive some exact formulas for the partition function of the Kitaev theory defined on surfaces with borders . These formulas generalize the celebrated formula due to Witten for the partition function of Yang - Mills fields on Riemann surfaces .",
        "rewrite_text": "In this article, we investigate the formation of gauge symmetries in quantum spin systems characterized by local interactions, specifically through the lens of the Kitaev model. Our analysis reveals that the Kitaev description bears a striking resemblance to an exactly solvable lattice gauge theory (LGT), where the action functional is represented by the Chern-Simons term. This correspondence allows us to leverage advanced methodologies developed for LGTs, including the index theorem and localization techniques. A significant outcome of our study is the demonstration that the ground state degeneracy of the Kitaev model on any closed three-dimensional manifold \\( M \\) corresponds to the number of independent solutions to a set of differential equations known as Witten's equations. This finding serves as a broadening of the Atiyah-Patodi-Singer index theorem, extending its applicability to manifolds that possess boundaries. Furthermore, we utilize the insights gained from our previous results to derive precise formulas for the partition function of the Kitaev model defined on surfaces with boundaries. These newly established formulas extend the renowned results originally presented by Witten regarding the partition function of Yang-Mills fields on Riemann surfaces. Our work not only deepens the understanding of gauge symmetries in spin systems but also bridges connections between quantum field theories and topological properties of manifolds, paving the way for future research in this dynamic field.",
        "ori-fast-z-score": 0.9878783399072131,
        "water-fast-z-score": 4.880935300919764,
        "rewrite-fast-z-score": 0.09759000729485333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neutral Hydrogen Absorption Toward XTE J1810-197: the Distance to a Radio-Emitting Magnetar .\nAbstract:\nWe report on radio observations made with the Australia Telescope Compact Array (ATCA) and Parkes Observatory toward the magnetar candidate X-ray transient source XTE J1810−197, which was discovered in outburst by RXTE/ASM during March 2009. We detect absorption features at 21 cm that are consistent with neutral hydrogen along our line-of-sight to this source. Using these data we derive an upper limit for its distance of <5 kpc. This is inconsistent with previous estimates based upon optical photometry or near-infrared spectroscopy. The discrepancy may be due to interstellar extinction effects and/or variability between epochs of observation. If confirmed as a neutron star then it would have one of the lowest inferred surface magnetic fields known. It also has a spin period derivative that is among the highest observed for any pulsar. These properties make it unique amongst currently-known neutron stars. Keywords: Neutron Star -Magnetic Field Strength, Pulsar -Distance Measurement",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neutral Hydrogen Absorption Toward XTE J1810 - 197 : the Distance to a Radio - Emitting Magnetar . Abstract : We report on radio observations made with the Australia Telescope Compact Array ( ATCA ) and Parkes Observatory toward the magnetar candidate X - ray transient source XTE J1810−197 , which was discovered in outburst by RXTE / ASM during March 2009 .We detect absorption elements at 21 cm that are compatible with neutral hydrogen along our line - of - seeing to this source . Using these information we derive an upper maximum for its radius of < 5 kpc .This is conflicting with previous accounts based upon imaging photometry or near - infrared spectroscopy . The discrepancy may be due to interstellar extinction effects and / or variability between epochs of observation .If confirmed as a neutron star then it would have one of the smallest inferred surface magnetic fields known . It additionally has a spin time derivative that is among the highest studied for any pulsar .These properties make it distinctive amongst currently - famous neutron stars . Keywords : Neutron Star - Magnetic Field Strength , Pulsar - Distance Measurement",
        "rewrite_text": "Title: Neutral Hydrogen Absorption Toward XTE J1810 - 197: Determining the Distance to a Radio-Emitting Magnetar\n\nAbstract: In this study, we present findings from radio observations conducted with the Australia Telescope Compact Array (ATCA) and Parkes Observatory, focusing on the magnetar candidate XTE J1810−197, which was identified as an X-ray transient source during an outburst detected by RXTE/ASM in March 2009. Our observations reveal the presence of absorption features at a wavelength of 21 cm, indicative of neutral hydrogen along the line of sight to this source. Utilizing this data, we establish an upper limit for the distance to XTE J1810−197, estimating it to be less than 5 kpc. This finding contrasts with earlier estimates derived from imaging photometry and near-infrared spectroscopy, suggesting a potential discrepancy that may arise from interstellar extinction effects or variability in the source's characteristics over different observational epochs. If XTE J1810−197 is confirmed to be a neutron star, it would possess one of the lowest inferred surface magnetic fields recorded to date. Additionally, it exhibits a spin-down rate that ranks among the highest observed for any pulsar, further distinguishing it within the population of well-known neutron stars. These unique attributes underscore the importance of XTE J1810−197 in the study of magnetars and pulsars, providing valuable insights into their magnetic field strengths and distance measurements. Our findings contribute to the ongoing discourse surrounding the nature of this intriguing celestial object and its implications for understanding the broader characteristics of neutron stars. \n\nKeywords: Neutron Star, Magnetic Field Strength, Pulsar, Distance Measurement.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": -0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Collective Modes in Two-band Superconductors .\nAbstract:\nWe study the collective modes in two-band superconductors with different gaps and masses, using the random phase approximation (RPA). We find that there are three types of collective modes: one is gapless and has linear dispersion relation at small wave vector; another is gapped but still has quadratic dispersion relation near the Fermi surface; while the third type is fully gapped without any low-energy excitations. The latter two types can be regarded as phonon-like collective modes. In addition to these three types of collective modes, we also find an exotic mode which does not exist in single-gap systems. This new mode originates from the interband pairing interaction between electrons on different bands. It shows up only when both intraband and interband interactions are present simultaneously. Our results show that this new mode may have important effects on the transport properties of multi-band superconductors. \n \n Introduction \n \n Multi-band superconductivity attracts much attention recently because it occurs naturally in many materials such as MgB_2  1  , Sr 2 RuO 4  2  , FeSe  3  . These compounds usually contain several orbitals per unit cell so they support multiple electronic bands crossing the Fermi level  4  . Due to the presence of more than one band, the electron-phonon coupling strength could vary significantly among different bands  5  . Moreover, the Coulomb repulsion effect becomes stronger for multi-orbital systems  6  . All these factors make the physics of multiband superconductors very rich  7, 8  .\n \nIn recent years, great progresses have been made in understanding the physical properties of multi-band superconductor  9  . For example, the vortex lattice structure  10  , magnetic field dependence  11  , thermal conductivity  12  , specific heat  13  , NMR relaxation rate  14  etc., were studied extensively by experiments. On the theoretical side, various methods including mean-field theory  15  , Eliashberg formalism  16  , functional renormalization group  17  , variational Monte Carlo  18  , exact diagonalization  19  , density matrix renormalization group  20  , and quantum Monte Carlo  21  were used to investigate the ground state properties  22  , thermodynamic quantities  23  ,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Collective Modes in Two - band Superconductors . Abstract : We explore the collective modes in two - band superconductors with various gaps and masses , using the random phase approximation ( RPA ) .We see that there are three sorts of collective modes : one is gapless and has continuous dispersion relation at small wave vector ; another is gapped but still has quadratic dispersion relation near the Fermi surface ; while the third type is fully gapped without any low - energy excitations . The last two forms can be regarded as phonon - like collective modes .In addition to these three sorts of collective modes , we also find an exotic mode which does not occur in single - gap systems . This new mode comes from the interband pairing interaction between electrons on various groups .It gives up only when both intraband and interband interactions are present concurrently . Our results show that this new mode may have important effects on the travel properties of dual - band superconductors .Introduction Multi - band superconductivity attracts many awareness today because it appears naturally in many materials such as MgB _ 2 1 , Sr 2 RuO 4 2 , FeSe 3 . These compounds often contain many orbitals per unit cell so they support multiple electronic bands crossing the Fermi level 4 .Due to the presence of more than one band , the electron - phonon coupling strength could vary significantly among different bands 5 . Moreover , the Coulomb repulsion effect gets stronger for multi - orbital complexes 6 .All these considerations making the physics of multiband superconductors very rich 7 , 8 . In recent years , great progresses have been achieved in understanding the physical properties of multi - band superconductor 9 .For instance , the vortex lattice structure 10 , electric field dependence 11 , thermal conductivity 12 , basic heat 13 , NMR relaxation time 14 etc . , were studied frequently by research . On the theoretical side , various methods notably mean - field model 15 , Eliashberg formalism 16 , functional renormalization group 17 , variational Monte Carlo 18 , exact diagonalization 19 , density matrix renormalization group 20 , and quantum Monte Carlo 21 were used to examine the ground state properties 22 , thermodynamic quantities 23 ,",
        "rewrite_text": "**Title: Collective Modes in Two-Band Superconductors**\n\n**Abstract:** This study investigates the collective modes present in two-band superconductors characterized by varying gap sizes and effective masses, employing the random phase approximation (RPA) as our primary analytical tool. Our findings reveal the existence of three distinct types of collective modes. The first type is gapless, exhibiting a continuous dispersion relation at small wave vectors. The second type, while gapped, demonstrates a quadratic dispersion relation in proximity to the Fermi surface. The third type is fully gapped, lacking any low-energy excitations, with the latter two types resembling phonon-like collective modes. Beyond these conventional modes, we identify an exotic mode unique to two-band systems, which does not manifest in single-gap superconductors. This novel mode arises from the interband pairing interactions among electrons across different bands and is contingent upon the simultaneous presence of both intraband and interband interactions. Our results suggest that this exotic mode could significantly influence the transport properties of dual-band superconductors.\n\nThe phenomenon of multi-band superconductivity has garnered considerable attention due to its natural occurrence in various materials, including MgB2, Sr2RuO4, and FeSe. These compounds typically feature multiple orbitals per unit cell, leading to several electronic bands intersecting the Fermi level. The presence of multiple bands allows for considerable variability in electron-phonon coupling strengths across different bands, while the Coulomb repulsion effect is amplified in multi-orbital systems. These factors contribute to the complex and rich physics underlying multiband superconductors. Recent advancements have enhanced our understanding of their physical properties, with extensive research focusing on aspects such as vortex lattice structures, electric field dependencies, thermal conductivity, and NMR relaxation times. Theoretical approaches, including mean-field models, Eliashberg formalism, functional renormalization group techniques, and various Monte Carlo methods, have been employed to explore the ground state properties and thermodynamic quantities of these intriguing systems.",
        "ori-fast-z-score": 0.8563488385776753,
        "water-fast-z-score": 7.140584836498262,
        "rewrite-fast-z-score": 2.199887763691481
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Capillary ordering and layering transitions in two-dimensional hard-rod fluids .\nAbstract:\nWe study the phase behavior of a system of N identical hard rods confined to a square box with periodic boundary conditions, using Monte Carlo simulations at constant pressure P . We find that for sufficiently large values of P , there is an ordered state where all particles are aligned along one direction (the x-axis), forming layers perpendicular to this axis.  The transition between disordered and ordered states occurs via a first-order phase transition which we characterize by studying the density profiles across the simulation cell as well as the order parameter distribution function. For small values of P , however, no such ordered state exists. Instead, the system exhibits a glassy dynamics characterized by slow relaxation timescales. Finally, we show how our results can be used to explain recent experiments on colloidal suspensions under shear flow. In many physical systems, it has been observed that particles tend to align themselves into regular patterns when they interact strongly enough. This phenomenon is known as capillarity  1  or self-assembly  2  .\nIn particular, in two dimensions, particles may form stripes  3  -  5  , squares  6  , hexagons  7  , or even more complex structures  8  depending on their shape  9  , size  10  , interactions  11  , and external fields  12  . These phenomena have attracted considerable attention over the past few years due to both fundamental interest  13  and potential applications  14  -  16  . A particularly interesting example is provided by colloidal suspensions  17  -  20  : When these are subjected to strong shear flows  21  , they often exhibit striped phases  22  -  24  whose formation mechanism remains poorly understood  25  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Capillary ordering and layering transitions in two - dimensional tough - rod liquid . Abstract : We explore the phase response of a system of N identical hard rods confined to a square box with periodic border conditions , using Monte Carlo simulations at constant pressure P .We see that for enough large values of P , there is an ordered state where all particles are aligned along one direction ( the x - axis ) , forming sheets perpendicular to this axis . The switch between disordered and ordered states happens via a first - order phase shift which we characterize by examining the density profiles across the model cell as well as the order parameter distribution function .For small values of P , however , no such ordered state exists . Instead , the system displays a glassy dynamics defined by slow relaxation timescales .Finally , we explain how our findings can be used to explain latest studies on colloidal suspensions under shear flow . In many mechanical systems , it has been observed that particles tend to align themselves into normal patterns when they interact strongly sufficient .This phenomenon is known as capillarity 1 or self - assembly 2 . In particular , in two dimensions , particles may form colors 3 - 5 , circles 6 , hexagons 7 , or especially more complex shapes 8 depending on their shape 9 , size 10 , interactions 11 , and external fields 12 .These phenomena have garnered considerable scrutiny over the previous few years due to both basic concern 13 and possible applications 14 - 16 . A notably important example is provided by colloidal suspensions 17 - 20 : When these are subjected to powerful shear flows 21 , they frequently exhibit striped cycles 22 - 24 whose formation system stays badly explained 25 .",
        "rewrite_text": "**Title:** Capillary Ordering and Layering Transitions in Two-Dimensional Tough-Rod Liquids\n\n**Abstract:** In this study, we investigate the phase behavior of a system comprising N identical hard rods confined within a square box under periodic boundary conditions, utilizing Monte Carlo simulations at a constant pressure P. Our findings reveal that at sufficiently high pressures, the system transitions into an ordered state where all rods align along a single direction (the x-axis), resulting in the formation of sheets that are perpendicular to this axis. The transition from a disordered to an ordered state occurs through a first-order phase transition, which we characterize by analyzing the density profiles within the model cell and the distribution of the order parameter. Conversely, at lower pressures, the system does not exhibit any ordered state; instead, it demonstrates glassy dynamics characterized by prolonged relaxation timescales. \n\nFurthermore, we discuss the implications of our results in the context of recent studies on colloidal suspensions subjected to shear flow. It has been observed in various mechanical systems that particles tend to align into regular patterns when they experience sufficiently strong interactions. This phenomenon, referred to as capillarity or self-assembly, is particularly pronounced in two-dimensional systems, where particles can organize into various configurations such as stripes, circles, hexagons, or more intricate shapes, influenced by their geometric properties, sizes, interaction strengths, and external fields. The exploration of these phenomena has attracted significant attention in recent years, driven by both fundamental scientific interest and potential practical applications. A particularly noteworthy example is found in colloidal suspensions, which, when exposed to intense shear flows, often display striped patterns whose underlying formation mechanisms remain poorly understood. Our research contributes to elucidating these complex behaviors and offers insights that may bridge the gap between theoretical predictions and experimental observations in the field of soft condensed matter physics.",
        "ori-fast-z-score": -2.197401062294143,
        "water-fast-z-score": 5.37076069309881,
        "rewrite-fast-z-score": 0.8723567442899586
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Chain-Boson Model for the Decoherence and Relaxation of a Few Coupled SQUIDs in a Phonon Bath .\nAbstract:\nWe present an analytical model to study decoherence and relaxation processes in a few coupled superconducting quantum interference devices (SQUID) immersed into a phonon bath at low temperatures. The system is described by a chain-boson Hamiltonian with nearest-neighbor interactions, which can be diagonalized exactly using the Bethe ansatz method. We show that this approach allows us to obtain exact results for the dynamics of the reduced density matrix describing the SQUID subsystem as well as its entanglement entropy. In particular we find that the decay rate of the off-diagonal elements of the reduced density matrix scales linearly with temperature T , while the von Neumann entropy grows logarithmically with time t. These findings are consistent with previous numerical studies on similar systems. \n \n Introduction \n \n Superconducting circuits have been proposed recently as promising candidates for realizations of quantum information processing  1  . One important issue in these proposals concerns how to protect qubits against environmental noise  2  . It has been shown theoretically  3  -  6  and experimentally  7  -  9  that coupling between different parts of a circuit may lead to unwanted effects such as dephasing or relaxation. This problem becomes particularly severe when considering large networks of interacting qubits  10  . \n \n Here we consider a simple model consisting of two weakly-coupled SQUIDs  11  immersed into a phonon environment  12  . Our aim is to investigate the effect of the interaction term on the evolution of the reduced density matrix of each SQUID separately. To do so, we use the Bethe ansatz  13  to solve analytically the Schrödinger equation corresponding to our model. As expected, we observe that the presence of the interaction leads to decoherence and dissipation phenomena. Moreover, we find that the decay rates of the off-diagonals of the reduced density matrices scale linearly with temperature T , whereas their von Neumann entropies grow logarithmically with time t. \nModel\n\nThe total Hamiltonian H = H0 + V describes the system composed of N = 2 SQUIDs coupled via a weak tunneling amplitude J immersed into a phonon reservoir at zero temperature.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Chain - Boson Model for the Decoherence and Relaxation of a Few Coupled SQUIDs in a Phonon Bath . Abstract : We present an analytical theory to study decoherence and relaxation processes in a few coupled superconducting quantum interference machines ( SQUID ) immersed into a phonon bath at low temperatures .The system is characterized by a chain - boson Hamiltonian with nearest - neighbor interactions , which can be diagonalized precisely using the Bethe ansatz technique . We see that this methodology allows us to obtain exact findings for the dynamics of the reduced density matrix describing the SQUID subsystem as well as its entanglement entropy .In particular we find that the decay rate of the off - diagonal elements of the reduced density matrix scales linearly with temperature T , while the von Neumann entropy increases logarithmically with time t . These conclusions are compatible with previous numerical studies on similar systems . Introduction Superconducting circuits have been proposed lately as promising candidates for realizations of quantum information processing 1 .One important concern in these proposals involves how to keep qubits against environmental interference 2 . It has been shown theoretically 3 - 6 and experimentally 7 - 9 that coupling between multiple parts of a circuit could lead to inappropriate consequences such as dephasing or relaxation .This problem appears particularly severe when examining massive networks of interacting qubits 10 . Here we study a simple model composed of two weakly - coupled SQUIDs 11 immersed into a phonon system 12 .Our aim is to examine the impact of the interaction term on the evolution of the reduced density matrix of each SQUID separately . To do so , we utilize the Bethe ansatz 13 to correct analytically the Schrödinger equation equivalent to our model .As expected , we find that the presence of the interaction results to decoherence and dissipation phenomena . Moreover , we find that the decay rates of the off - diagonals of the reduced density matrices increase linearly with temperature T , whereas their von Neumann entropies grow logarithmically with time t . Model The total Hamiltonian H = H0 + V describes the system composed of N = 2 SQUIDs coupled via a weak tunneling amplitude J immersed into a phonon reservoir at zero temperature .",
        "rewrite_text": "**Title:** A Chain-Boson Model for the Decoherence and Relaxation of a Few Coupled SQUIDs in a Phonon Bath\n\n**Abstract:** In this study, we develop an analytical framework to investigate the decoherence and relaxation phenomena occurring in a system of coupled superconducting quantum interference devices (SQUIDs) that are situated within a phonon bath at low temperatures. The system is modeled using a chain-boson Hamiltonian that incorporates nearest-neighbor interactions, which we successfully diagonalize through the Bethe ansatz method. This approach enables us to derive precise results for the dynamics of the reduced density matrix that characterizes the SQUID subsystem, as well as its entanglement entropy. Our findings reveal that the decay rate of the off-diagonal elements of the reduced density matrix exhibits a linear dependence on temperature (T), while the von Neumann entropy increases logarithmically over time (t). These results align well with previous numerical investigations conducted on similar systems, reinforcing the validity of our analytical approach.\n\nThe introduction of superconducting circuits as viable platforms for quantum information processing has raised critical questions regarding the resilience of qubits against environmental disturbances. Theoretical and experimental studies have demonstrated that interactions among various components of a circuit can lead to detrimental effects, including dephasing and relaxation. This issue becomes particularly pronounced in extensive networks of interacting qubits. In our research, we focus on a simplified model consisting of two weakly coupled SQUIDs immersed in a phonon environment. Our objective is to analyze how the interaction term influences the evolution of the reduced density matrix for each SQUID individually. By applying the Bethe ansatz to analytically solve the Schrödinger equation corresponding to our model, we uncover that the interaction indeed induces decoherence and dissipation effects. Notably, we observe that the decay rates of the off-diagonal elements of the reduced density matrices increase linearly with temperature, while the von Neumann entropies exhibit a logarithmic growth with time. The total Hamiltonian governing our system is expressed as H = H0 + V, where H0 represents the independent SQUIDs and V accounts for the weak tunneling interactions between them, all within a phonon reservoir at zero temperature.",
        "ori-fast-z-score": 0.3310423554409472,
        "water-fast-z-score": 6.560592908445457,
        "rewrite-fast-z-score": 2.242227956050979
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The inner jet of radio galaxy NGC 315 as observed with Chandra and the VLA .\nAbstract:\nWe present new observations of the nuclear region in the nearby radio galaxy NGC315, made using the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). The CXO data reveal an unresolved point source at the center of this elliptical galaxy that is surrounded by diffuse emission extending to about 1 arcmin (3 kpc), which we identify as thermal gas heated by the central AGN. We detect two bright knots embedded within the extended emission; these are likely associated with shocks driven into the surrounding medium by the expanding radio jets. Using high-resolution VLA images obtained simultaneously with the CXO observation, we find evidence for a one-sided parsec-scale radio jet emerging from the nucleus along position angle PA = -45 degrees. This jet has been previously detected on larger scales out to several kiloparsecs. In addition, there appears to be another fainter component of the radio jet located further south-west than the main knot.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The inner jet of radio star NGC 315 as observed with Chandra and the VLA . Abstract : We report new studies of the atomic region in the nearby radio galaxy NGC315 , made using the Chandra X - ray Observatory ( CXO ) and the Very Large Array ( VLA ) .The CXO data reveal an unresolved point origin at the center of this elliptical galaxy that is surrounded by diffuse emission stretching to about 1 arcmin ( 3 kpc ) , which we identify as heat gas heated by the main AGN . We detect two faint knots embedded within the extended emission ; these are likely correlated with shocks driven into the adjacent medium by the increasing radio jets .Using long - resolution VLA images obtained simultaneously with the CXO study , we find proof for a one - sided parsec - scale radio jet developing from the nucleus along position angle PA = - 45 degrees . This jet has been previously observed on larger scales out to several kiloparsecs .In addition , there seems to be another fainter component of the radio jet located further west - west than the main knot .",
        "rewrite_text": "We present new findings from our investigation of the atomic region in the nearby radio galaxy NGC 315, utilizing data from the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). Our analysis of the CXO data uncovers an unresolved point source at the core of this elliptical galaxy, which is enveloped by diffuse emission extending approximately 1 arcminute (or 3 kiloparsecs) outward. This diffuse emission is interpreted as hot gas that has been heated by the active galactic nucleus (AGN) at the center. Within this extended emission, we identify two faint knots that are likely associated with shocks generated by the expanding radio jets interacting with the surrounding medium. \n\nSimultaneously acquired high-resolution VLA images provide compelling evidence for a one-sided radio jet emerging from the nucleus, oriented at a position angle of -45 degrees. This jet has been previously documented on larger scales, extending out to several kiloparsecs. Furthermore, our observations suggest the presence of an additional, fainter component of the radio jet located further to the west of the primary knot. These findings contribute to our understanding of the dynamics and structure of the inner jet of NGC 315, highlighting the intricate interactions between the AGN and its environment. The combination of X-ray and radio data enhances our comprehension of the processes at play in this intriguing radio galaxy, offering insights into the mechanisms driving jet formation and the impact of AGN activity on surrounding gas.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 3.796283011826483,
        "rewrite-fast-z-score": 1.3471506281091268
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Adding Environmental Gas Physics to the Semi-Analytic Method for Galaxy Formation: Gravitational Heating .\nAbstract:\nWe present an improved semi-analytical method (SAM) that includes gravitational heating by dark matter halos and gas cooling in galaxy formation, which is essential to reproduce observed properties of galaxies such as luminosity functions at different redshifts.  We show that our SAM can successfully explain the evolution of the stellar mass function over cosmic time with reasonable parameters. In addition, we find that the inclusion of gravitational heating leads to more realistic predictions on the star formation rate density history than previous models without this effect. Finally, we discuss how the model could be further improved by including other physical processes like supernova feedback or AGN activity. The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085.A-0488(A). This work was supported by JSPS KAKENHI Grant Number JP15K05481. Figure 1 . Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature. Red circles represent the predicted number densities using our new SAM code while blue squares indicate those obtained with the original SAM code developed by Nagashima & Yoshii (2004) .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Adding Environmental Gas Physics to the Semi - Analytic Method for Galaxy Formation : Gravitational Heating . Abstract : We introduce an excellent semi - empirical method ( SAM ) that encompasses gravitational heating by black material halos and gas warming in universe formation , which is crucial to reproduce observed properties of stars such as luminosity functions at different redshifts .We see that our SAM can effectively predict the evolution of the stellar mass function over cosmic time with suitable variables . In addition , we find that the introduction of gravitational heating results to more realistic predictions on the star formation rate density history than prior models without this effect .Finally , we talk how the model could be further strengthened by including other physical processes like supernova feedback or AGN activity . The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085 . A - 0488 ( A ) .This project was supported by JSPS KAKENHI Grant Number JP15K05481 . Figure 1 .Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature . Red rings represent the expected number densities using our new SAM code while blue squares represents those achieved with the previous SAM code developed by Nagashima & Yoshii ( 2004 ) .",
        "rewrite_text": "Title: Incorporating Environmental Gas Physics into the Semi-Analytic Method for Galaxy Formation: Gravitational Heating\n\nAbstract: In this study, we present a novel semi-empirical method (SAM) that integrates the effects of gravitational heating from dark matter halos and gas heating during the formation of the universe. This approach is essential for accurately reproducing the observed characteristics of stars, including luminosity functions across various redshifts. Our findings demonstrate that this enhanced SAM effectively predicts the evolution of the stellar mass function throughout cosmic history by utilizing appropriate variables. Notably, we observe that incorporating gravitational heating leads to more realistic estimations of the star formation rate density history compared to previous models that did not account for this phenomenon. Furthermore, we discuss potential avenues for strengthening the model by integrating additional physical processes, such as supernova feedback and active galactic nucleus (AGN) activity. The results presented in this paper are grounded in observational data collected using the ESO Telescopes at the Paranal Observatory, under programme ID 085.A-0488(A). This research was supported by the Japan Society for the Promotion of Science (JSPS) KAKENHI Grant Number JP15K05481. Figure 1 illustrates the predicted number densities of galaxies as a function of their total stellar masses, juxtaposed with observational data from existing literature. The red circles indicate the expected number densities derived from our new SAM code, while the blue squares represent those obtained from the previous SAM code developed by Nagashima & Yoshii (2004).",
        "ori-fast-z-score": -0.6060915267313265,
        "water-fast-z-score": 5.125692857821981,
        "rewrite-fast-z-score": 0.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Black Hole Evaporation in an Expanding Universe .\nAbstract:\nWe study the evaporation process of black holes (BHs) in an expanding universe by using the tunneling method and the WKB approximation. We find that, for large BH masses M ≫ Mc2 = 3 × 10^8M⊙, where Mc is the critical mass at which the Hawking temperature vanishes, the lifetime of the BH decreases with increasing M as t ~ M-1/2. For small BH masses M < Mc2 , we show that the lifetime increases exponentially with decreasing M . The results are compared to those obtained within the framework of quantum field theory on curved space-time. It turns out that our predictions agree well with these results when one takes into account the effect of back reaction due to particle creation during the evaporation process. \nPACS numbers: 04.20.-q; 98.80.Cq \nI. INTRODUCTORY REMARK\nThe discovery of Hawking radiation  1  has led to renewed interest in the problem of black hole (BH) evaporation  2  -  4  . In this work, we will use the tunneling method  5  -  8  to calculate the decay rate of massive BHs in an expanding universe  9  .\nII. BLACK HOLE EVAPORATION IN AN EXPANDING UNIVERSE\n\nA. Tunneling Method\nIn order to apply the tunneling method to the case of evaporating BHs, it is convenient to introduce new coordinates (t′, r′), related to the original ones (t,r) through the following transformations  10  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Black Hole Evaporation in an Expanding Universe . Abstract : We research the evaporation process of black holes ( BHs ) in an increasing universe by using the tunneling procedure and the WKB approximation .We find that , for large BH masses M [UNK] Mc2 = 3 × 10 ^ [UNK] , where Mc is the critical mass at which the Hawking temperature vanishes , the lifetime of the BH decreases with increasing M as t ~ M - 1 / 2 . For small BH masses M < Mc2 , we show that the lifetime increases exponentially with decreasing M .The results are compared to those achieved within the framework of quantum field theory on curved space - time . It turns out that our predictions agree well with these results when one takes into consideration the impact of back response due to particle creation during the evaporation process .PACS numbers : 04 . 20 . - q ; 98 . 80 . Cq I . INTRODUCTORY REMARK The observation of Hawking radiation 1 has led to renewed concern in the issue of grey hole ( BH ) evaporation 2 - 4 .In this project , we will use the tunneling method 5 - 8 to estimate the decay rate of large BHs in an increasing universe 9 . II .BLACK HOLE EVAPORATION IN AN EXPANDING UNIVERSE A . Tunneling Method In order to apply the tunneling procedure to the case of evaporating BHs , it is convenient to introduce different coordinates ( t ′ , r ′ ) , related to the previous ones ( t , r ) through the following transformations 10 :",
        "rewrite_text": "**Title: Black Hole Evaporation in an Expanding Universe**\n\n**Abstract:** This study investigates the evaporation dynamics of black holes (BHs) within the context of an expanding universe, employing the tunneling method alongside the WKB approximation. Our findings reveal that for black holes with substantial masses, specifically those exceeding the critical mass \\( M_c^2 = 3 \\times 10^{18} \\) kg, the lifetime of the black holes diminishes as their mass increases, following a relationship of the form \\( t \\sim M^{-1/2} \\). Conversely, for black holes with masses below the critical threshold \\( M < M_c^2 \\), we observe an exponential increase in lifetime as the mass decreases. These results are juxtaposed with predictions derived from quantum field theory in curved spacetime, demonstrating a strong correlation when accounting for the back-reaction effects associated with particle creation during the evaporation process. The implications of these findings are significant, as they enhance our understanding of black hole thermodynamics and the nature of Hawking radiation, which has reignited interest in the phenomenon of black hole evaporation. The tunneling method serves as a powerful tool in this analysis, allowing for a more nuanced exploration of the decay rates of large black holes in an expanding cosmological framework. This research contributes to the ongoing discourse surrounding the fate of black holes and the fundamental principles governing their evaporation, providing insights that may bridge gaps between theoretical predictions and observational data. The PACS numbers relevant to this study are 04.20.-q and 98.80.Cq.",
        "ori-fast-z-score": 0.3144854510165755,
        "water-fast-z-score": 4.37880269519857,
        "rewrite-fast-z-score": 1.8034001919297482
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Reggeization of N=8 Supergravity and N=4 Yang-Mills Theory II .\nAbstract:\nWe present the results for the one-loop amplitudes in N=8 supergravity, which are obtained by using the spinor helicity formalism with dimensional reduction to four dimensions.  We show that our result agrees with the previous calculation based on Feynman diagrams at high energy limit. In addition we find that the amplitude vanishes when all external particles have negative helicities. This is consistent with the fact that there exists no graviton state with negative helicity in the massless spectrum of N=8 supergravity. The vanishing property also holds true even if some gravitons carry positive helicities but others do not. \nThe organization of this article is as follows. In section 2, we briefly review the basic properties of N=8 supergravity theory. In section 3, we explain how to calculate the scattering amplitudes in N=8 supergravitvity. In section 4, we give explicit expressions for the tree-level amplitudes. In sections 5-8, we present the results for the 1-loop amplitudes. Section 9 contains discussions about the physical meaning of these results.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Reggeization of N = 8 Supergravity and N = 4 Yang - Mills Theory II . Abstract : We present the results for the one - loop amplitudes in N = 8 supergravity , which are derived by using the spinor helicity formalism with dimensional reduction to four dimensions .We see that our conclusion agrees with the previous measurement based on Feynman diagrams at high energy limit . In addition we find that the amplitude vanishes when all external particles have negative helicities .This is compatible with the fact that there exists no graviton state with negative helicity in the massless spectrum of N = 8 supergravity . The vanishing property especially holds true even if some gravitons carry positive helicities but others do not .The organization of this page is as follows . In section 2 , we briefly evaluate the fundamental properties of N = 8 supergravity theory .In section 3 , we explain how to estimate the scattering amplitudes in N = 8 supergravitvity . In section 4 , we give explicit expressions for the tree - level amplitudes .In sections 5 - 8 , we present the results for the 1 - loop amplitudes . Section 9 includes talks about the physical meaning of these results .",
        "rewrite_text": "In this article, titled \"Reggeization of N = 8 Supergravity and N = 4 Yang-Mills Theory II,\" we delve into the one-loop amplitudes in N = 8 supergravity, employing the spinor helicity formalism alongside dimensional reduction to four dimensions. Our findings align with previous calculations conducted using Feynman diagrams, particularly in the high-energy limit. A notable discovery is that the amplitude becomes zero when all external particles possess negative helicities. This observation is consistent with the absence of a graviton state with negative helicity in the massless spectrum of N = 8 supergravity. Furthermore, this vanishing property persists even when some gravitons exhibit positive helicities while others do not. \n\nThe structure of the paper is organized as follows: Section 2 provides a concise overview of the fundamental characteristics of N = 8 supergravity theory. In Section 3, we outline the methodology for estimating scattering amplitudes within this framework. Section 4 presents explicit expressions for tree-level amplitudes, while Sections 5 through 8 detail our results for the one-loop amplitudes. Finally, Section 9 discusses the physical implications of our findings, shedding light on their significance within the broader context of supergravity and Yang-Mills theories. Through this comprehensive analysis, we aim to enhance the understanding of the intricate relationships between these advanced theoretical frameworks.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 3.3941932686877867,
        "rewrite-fast-z-score": -0.22941573387056174
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The lower mass function of the young open cluster Blanco 1: from 30 Mjup to 3 Mo .\nAbstract:\nWe present new photometry in the optical and near-infrared bands for the open cluster Blanco 1 (M35). The data were obtained with the 2 m telescope at the Observatorio Astronómico Nacional de San Pedro Mártir, Baja California Sur, Mexico; the 0.9 m telescope at the Cerro Tololo Inter-American Observatory, Chile; and the Nordic Optical Telescope, La Palma, Canary Islands, Spain. We have used these observations together with archival data from the Hubble Space Telescope archive to derive an improved age estimate for this open cluster. Our results show that M35 is about 300 Myr old, which makes it one of the youngest open clusters known within 100 pc of the Sun. Using our derived age we have determined the initial mass function (IMF) of M35 down to masses as low as 3 MJUP . This IMF shows no evidence for any significant deviation from the Salpeter power law index over more than three orders of magnitude in mass.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The lower mass value of the early open cluster Blanco 1 : from 30 Mjup to 3 Mo . Abstract : We report new photometry in the optical and far - infrared bands for the open cluster Blanco 1 ( M35 ) .The data were obtained with the 2 m observatory at the Observatorio Astronómico Nacional de San Pedro Mártir , Baja California Sur , Mexico ; the 0 . 9 m observatory at the Cerro Tololo Inter - American Observatory , Chile ; and the Nordic Optical Telescope , La Palma , Canary Islands , Spain . We have utilized these observations together with archival data from the Hubble Space Telescope archive to derive an better age forecast for this open cluster .Our results show that M35 is about 300 Myr aged , which makes it one of the youngest open nuclei known within 100 pc of the Sun . Using our derived age we have decided the initial mass function ( IMF ) of M35 down to masses as low as 3 MJUP .This IMF indicates no evidence for any meaningful deviation from the Salpeter power law index over more than three orders of magnitude in mass .",
        "rewrite_text": "Title: The Lower Mass Value of the Early Open Cluster Blanco 1: From 30 Mjup to 3 Mo\n\nAbstract: In this study, we present new photometric observations in both optical and far-infrared wavelengths for the open cluster Blanco 1 (also known as M35). The data were collected using several telescopes, including the 2-meter telescope at the Observatorio Astronómico Nacional de San Pedro Mártir in Baja California Sur, Mexico; the 0.9-meter telescope at the Cerro Tololo Inter-American Observatory in Chile; and the Nordic Optical Telescope located in La Palma, Canary Islands, Spain. By integrating these new observations with archival data from the Hubble Space Telescope, we have refined the age estimation for this open cluster. Our analysis indicates that M35 has an age of approximately 300 million years, positioning it as one of the youngest open clusters within a 100 parsec radius of the Sun. Utilizing this updated age, we have determined the initial mass function (IMF) of M35, extending our analysis down to masses as low as 3 Mjup. The resulting IMF shows no significant deviation from the Salpeter power law index across a mass range that spans more than three orders of magnitude. This finding contributes to our understanding of the mass distribution of stars and substellar objects in young open clusters, suggesting a consistent formation process that aligns with established theoretical frameworks. Our results not only enhance the characterization of M35 but also provide valuable insights into the stellar population dynamics within nearby clusters, reinforcing the importance of continued observational efforts in this field.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 3.14970394174356,
        "rewrite-fast-z-score": 1.0314212462587933
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dark Matter Annihilation in Substructures Revised .\nAbstract:\nWe present the results for dark matter annihilations into gamma rays and neutrinos using an improved treatment of subhalos within galaxy clusters, including their internal structure as well as tidal stripping effects on their outer parts. We find that this leads to a significant increase (by up to one order of magnitude) in the predicted fluxes at energies above 1 GeV compared with previous studies. The effect is particularly strong when considering nearby galaxy clusters such as Virgo or Coma. This has important consequences for current and future experiments searching for signals from dark matter particles. In particular, we show how our predictions can be used to derive constraints on the properties of dark matter candidates by comparing them with existing data from Fermi/LAT and IceCube/DeepCore. Introduction: Dark matter (DM), if it exists, may interact weakly with ordinary matter through its self-annihilation products  1  . If DM consists of new light particles, then these interactions would produce detectable signatures in cosmic ray spectra  2  , gamma-ray emission  3  , and high-energy neutrino production  4  .\nIn recent years there have been many attempts to detect DM indirectly via observations of astrophysical objects which are expected to contain large amounts of DM  5  . These include dwarf galaxies  6  , galaxy clusters  7, 8  , and galactic haloes  9  . However, no convincing evidence for DM annihilation has yet been found  10  . One possible explanation for this lack of detection could be that most of the DM mass resides in small-scale structures  11  , which are not resolved observationally  12  . Another possibility is that the DM density profiles inferred from gravitational lensing measurements  13  do not accurately reflect the true distribution of DM  14  . Finally, it should also be noted that some models predict very low rates of DM annihilation  15  .\nThe aim of this work is to investigate whether the inclusion of substructure information improves the prospects for detecting DM annihilation products. To achieve this goal, we use high-resolution N-body simulations  16  to study the impact of subhalo populations on the resulting gamma-ray  17  and neutrino  18  fluxes produced by",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dark Matter Annihilation in Substructures Revised . Abstract : We present the results for black material annihilations into beta rays and neutrinos using an better treatment of subhalos within star clusters , notably their internal structure as well as tidal stripping impacts on their exterior portions .We see that this results to a substantial rise ( by up to one order of magnitude ) in the expected fluxes at energies above 1 GeV compared with previous researchers . The impact is especially powerful when examining nearby galaxy galaxies such as Virgo or Coma .This has crucial consequences for recent and future research exploring for messages from dark matter particles . In particular , we study how our predictions can be used to derive restrictions on the properties of bright matter candidates by comparing them with existing information from Fermi / LAT and IceCube / DeepCore .Introduction : Dark matter ( DM ) , if it exists , might interact weakly with normal matter through its self - annihilation products 1 . If DM consists of new light particles , then these interactions would create detectable signatures in cosmic ray spectra 2 , gamma - ray radiation 3 , and large - energy neutrino production 4 .In past days there have been many efforts to locate DM indirectly via surveys of astrophysical objects which are expected to contain significant amounts of DM 5 . These include dwarf stars 6 , galaxy galaxies 7 , 8 , and galactic haloes 9 .However , no convincing evidence for DM annihilation has yet been finding 10 . One potential explanation for this lack of detection may be that most of the DM mass resides in small - scale structures 11 , which are not resolved observationally 12 .Another possibility is that the DM density features inferred from gravitational lensing observations 13 do not correctly reflect the true distribution of DM 14 . Finally , it should additionally be mentioned that some models predict very low rates of DM annihilation 15 .The goal of this research is to examine whether the introduction of substructure knowledge improves the possibilities for detecting DM annihilation products . To achieve this goal , we using high - resolution N - bodies simulations 16 to study the impact of subhalo populations on the resulting γ - ray 17 and neutrino 18 fluxes produced by",
        "rewrite_text": "**Title:** Dark Matter Annihilation in Substructures: A Revised Analysis\n\n**Abstract:** In this study, we investigate the annihilation of dark matter (DM) into beta rays and neutrinos, employing an enhanced framework for understanding subhalos within star clusters. Our approach emphasizes the internal structure of these subhalos and the effects of tidal stripping on their outer regions. Our findings indicate a significant increase in the anticipated fluxes at energies exceeding 1 GeV, with enhancements reaching up to an order of magnitude compared to previous studies. This effect is particularly pronounced in nearby galaxy clusters such as Virgo and Coma, suggesting that these regions may be key targets for detecting signals from dark matter interactions. The implications of our results are profound for both ongoing and future investigations aimed at uncovering evidence of dark matter particles. Specifically, we explore how our predictions can inform constraints on the properties of potential dark matter candidates by juxtaposing them with existing observational data from instruments like Fermi/LAT and IceCube/DeepCore. \n\nThe existence of dark matter, if confirmed, implies weak interactions with ordinary matter through the products of its self-annihilation. If dark matter comprises new, lighter particles, these interactions could yield observable signatures in cosmic ray spectra, gamma-ray emissions, and high-energy neutrino production. Despite numerous efforts to detect dark matter indirectly through surveys of astrophysical entities believed to harbor substantial amounts of dark matter—such as dwarf galaxies and galactic halos—no definitive evidence for dark matter annihilation has been established. Possible explanations for this detection challenge include the concentration of dark matter in small-scale structures that remain unresolved in observations, inaccuracies in the inferred density profiles from gravitational lensing, and theoretical models predicting minimal annihilation rates. Our research aims to determine whether incorporating knowledge of substructure enhances the prospects for detecting products of dark matter annihilation. To this end, we utilize high-resolution N-body simulations to assess the influence of subhalo populations on the resultant gamma-ray and neutrino fluxes generated by dark matter interactions.",
        "ori-fast-z-score": -1.8973665961010275,
        "water-fast-z-score": 7.699607172920184,
        "rewrite-fast-z-score": 1.0120486274099798
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mesoscopic fluctuations of the supercurrent in diffusive Josephson junctions .\nAbstract:\nWe study mesoscopic fluctuations of the supercurrents flowing through two weakly coupled superconductors with different transparencies and temperatures, using the Usadel equations for quasiclassical Green s functions. We show that the current noise is suppressed by increasing transparency between the leads or decreasing temperature. The suppression can be explained as due to an increase of the effective junction length caused by Andreev reflection at the interface. In addition we find that the shot-noise power decreases when the phase difference across the junction increases. This effect originates from the dependence of the density of states on the phase difference. Finally, we discuss how our results are related to recent experiments performed on diffusive SNS junctions. \nI. INTRODUCTORY REMARK\nThe Josephson effect describes macroscopically coherent transport of Cooper pairs across weak links connecting two superconducting electrodes  1  . It has been observed experimentally over many decades  2  , but only recently have researchers begun to explore its microscopic origins  3  .\nIn this work we consider a system consisting of two weakly-coupled superconductors (S) connected via a normal metal region (N). Such systems are known as diffusive SNS junctures  4  . They exhibit interesting phenomena such as the proximity effect  5  , which causes the formation of a minigap inside the N region  6  . Another important feature of these devices is their ability to carry both charge and spin currents  7, 8  . These properties make them promising candidates for applications ranging from quantum information processing  9  to magnetic field sensing  10  .\nRecently there has been renewed interest in studying the physics of diffusive SNS juncture  11  -  16  . For example, it was shown theoretically that the critical current I c depends strongly on the transparency T = R Q /R N of the NS interfaces  17  where R Q and R N are the resistance quantum and the resistance of the N region respectively. Experimentally, this prediction could not yet be confirmed because of difficulties associated with fabricating clean NS interfaces  18  . However, several groups managed to observe similar effects indirectly  19, 20  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mesoscopic fluctuations of the supercurrent in diffusive Josephson junctions . Abstract : We explore mesoscopic fluctuations of the supercurrents flowing through two tightly correlated superconductors with varying transparencies and temperatures , using the Usadel equations for quasiclassical Green s functions .We see that the current noise is suppressed by increasing transparency between the leads or decreasing temperature . The suppression can be understood as owing to an increase of the effective junction length produced by Andreev reflection at the interface .In addition we find that the shot - noise strength decreases when the phase change across the junction increases . This phenomenon originates from the dependence of the density of states on the phase change .Finally , we explain how our findings are related to recent experiments conducted on diffusive SNS junctions . I .INTRODUCTORY REMARK The Josephson effect represents macroscopically consistent transport of Cooper pairs across weak links connecting two superconducting electrodes 1 . It has been observed experimentally over numerous years 2 , but only lately have researchers begun to examine its microscopic origins 3 .In this study we study a system consisting of two weakly - coupled superconductors ( S ) connected via a normal metal area ( N ) . Such structures are known as diffusive SNS junctures 4 .They show important phenomena such as the proximity phenomenon 5 , which gives the formation of a minigap inside the N region 6 . Another important feature of these machines is their capabilities to carry both charge and spin currents 7 , 8 .These properties make them promising candidates for applications ranging from quantum information processing 9 to magnetic field monitoring 10 . Recently there has been continued interest in investigating the physics of diffusive SNS juncture 11 - 16 .For instance , it was shown theoretically that the critical current I c varies strongly on the transparency T = R Q / R N of the NS interfaces 17 where R Q and R N are the resistance quantum and the resistance of the N region respectively . Experimentally , this prediction could not already be verified because of troubles associated with fabricating safe NS interfaces 18 .However , various groups helped to observe identical effects indirectly 19 , 20 .",
        "rewrite_text": "**Title:** Mesoscopic Fluctuations of the Supercurrent in Diffusive Josephson Junctions\n\n**Abstract:** This study investigates the mesoscopic fluctuations of supercurrents in diffusive Josephson junctions formed by two closely correlated superconductors, focusing on the effects of varying transparency and temperature. Utilizing the Usadel equations for quasiclassical Green's functions, we analyze how current noise is influenced by these parameters. Our findings indicate that increasing the transparency of the junction leads to a suppression of current noise, while a decrease in temperature also contributes to this effect. This suppression can be attributed to the effective lengthening of the junction, which arises from Andreev reflection occurring at the interface. Furthermore, we observe that the strength of shot noise diminishes as the phase difference across the junction increases. This behavior is linked to the phase-dependent variation of the density of states within the junction. We also discuss the implications of our results in the context of recent experimental investigations on diffusive SNS junctions. \n\nThe Josephson effect, which facilitates the coherent transport of Cooper pairs through weak links between superconducting electrodes, has been a subject of extensive experimental research over the years. However, the microscopic mechanisms underlying this phenomenon have only recently begun to receive attention. In our work, we focus on a system comprising two weakly coupled superconductors connected by a normal metal region, commonly referred to as a diffusive SNS junction. These junctions exhibit significant phenomena, such as the proximity effect, which leads to the emergence of a minigap in the normal region. Additionally, they possess the ability to conduct both charge and spin currents, making them attractive for applications in quantum information processing and magnetic field sensing. The ongoing interest in the physics of diffusive SNS junctions is underscored by theoretical predictions regarding the critical current's dependence on the transparency of the NS interfaces, although experimental verification has faced challenges due to difficulties in fabricating reliable NS interfaces. Nonetheless, indirect observations by various research groups have supported these theoretical predictions, highlighting the relevance of our findings in advancing the understanding of these complex systems.",
        "ori-fast-z-score": -1.150792911137501,
        "water-fast-z-score": 7.291164092619461,
        "rewrite-fast-z-score": 0.3244428422615251
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A linear reformulation of the Kuramoto model of self-synchronizing oscillators .\nAbstract:\nWe present an alternative formulation for the Kuramoto model that is based on a linearization of the original nonlinear system and leads to a more efficient numerical solution method than existing approaches.  The new approach can be used in combination with any standard solver, such as Newton s or fixed-point iteration methods. We demonstrate its performance by applying it to several examples including networks of coupled phase oscillators and chaotic systems. Synchronized behavior has been observed across many different fields ranging from physics  1  , biology  2  , chemistry  3  , engineering  4  , and social sciences  5  . In particular, synchronization phenomena are often studied using models of interacting dynamical systems  6  .\nThe most commonly used mathematical description of synchronized dynamics is given by the Kuramoto model  7, 8  which describes how N identical oscillators evolve over time t according to: \nwhere θi(t) ∈  0, 2π  denotes the phase angle of oscillator i at time t, ωi > 0 represents the natural frequency of each individual oscillator, and Kij ≥ 0 quantifies the strength of interaction between oscillators i and j. For simplicity we assume here that all interactions have equal weight (Kij = 1). This assumption does not affect our results but simplifies notation considerably.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A linear reformulation of the Kuramoto model of self - synchronizing oscillators . Abstract : We present an alternative formulation for the Kuramoto model that is based on a linearization of the original nonlinear system and results to a more efficient numerical solving approach than existing techniques .The modern solution can be used in combination with any typical solver , such as Newton s or fixed - point iteration algorithms . We suggest its success by using it to several examples namely groups of coupled phase oscillators and chaotic structures .Synchronized activity has been observed across many various fields ranging from science 1 , chemistry 2 , chemistry 3 , engineering 4 , and social sciences 5 . In particular , synchronization processes are often investigated using models of interacting dynamical systems 6 .The most commonly used numerical model of synchronized dynamics is given by the Kuramoto model 7 , 8 which explains how N identical oscillators evolve over time t according to : where θi ( t ) ∈ 0 , 2π denotes the phase angle of oscillator i at time t , ωi > 0 represents the natural intensity of each individual oscillator , and Kij ≥ 0 quantifies the strength of coupling between oscillators i and j . For simplicity we suppose here that all interactions have equal weight ( Kij = 1 ) .This assumption does not alter our findings but simplifies notation significantly .",
        "rewrite_text": "In this article, we introduce a novel linear reformulation of the Kuramoto model, which describes self-synchronizing oscillators. Our approach involves linearizing the original nonlinear system, leading to a more efficient numerical solution compared to traditional methods. This modern formulation can be seamlessly integrated with standard numerical solvers, including Newton's method and fixed-point iteration algorithms. We demonstrate the effectiveness of our reformulation through various examples, including groups of coupled phase oscillators and chaotic systems.\n\nSynchronization phenomena are prevalent across numerous disciplines, including the natural sciences, chemistry, engineering, and social sciences. Researchers often explore these synchronization processes using models of interacting dynamical systems. Among these, the Kuramoto model is the most widely utilized framework for analyzing synchronized dynamics. It describes the evolution of N identical oscillators over time, where the phase angle of each oscillator, denoted as θi(t), varies within the interval [0, 2π]. Each oscillator has a natural frequency, represented by ωi, and the coupling strength between oscillators i and j is quantified by Kij. For the sake of simplicity in our analysis, we assume uniform coupling (Kij = 1), which does not compromise the validity of our findings but does streamline the notation.\n\nOur linear reformulation not only enhances computational efficiency but also broadens the applicability of the Kuramoto model to complex systems. By applying our method to various scenarios, we highlight its potential to facilitate deeper insights into synchronization dynamics across different fields. This work paves the way for future research that could leverage our approach to explore new phenomena in self-synchronizing systems.",
        "ori-fast-z-score": 0.20203050891044214,
        "water-fast-z-score": 6.130730673081194,
        "rewrite-fast-z-score": 0.647150228929434
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dark matter in the Milky Way, II. the HI gas distribution as a tracer of the gravitational potential .\nAbstract:\nWe present an analysis of the neutral hydrogen (HI) emission observed with the Westerbork Synthesis Radio Telescope and the Effelsberg 100-m telescope to study the dark matter content of our Galaxy. We use the rotation curve derived by Clemens (1985) , which is based on 21-cm line observations of nearby spiral galaxies. The total mass enclosed within a radius R can be written as: M(R) = Vrot2πGRL + MDW(R), where Vrot is the circular velocity at galactocentric distance R, G is Newton s constant, L is the luminosity density, and MDW(R) is the contribution due to the dark matter halo. In this work we assume that the dark matter follows a Navarro-Frenk-White profile.  Using the rotation curve for the solar neighbourhood given by Clemens (1985) (V⊙ = 220 km/sec), we find that the best-fit parameters are L0 = 0.0013 Msun/pc3 and r0 = 1 kpc. This implies that the local surface brightness ΣL = L/L0 = 3.6 × 10^−26 W/m2/Hz/sr. For comparison, the average value found by Dickey & Lockman (1990)  is ΣL = 2×10^−25 W/m2/Hz/sr; their sample includes only high latitude regions outside the Galactic plane.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dark matter in the Milky Way , II . the HI gas distribution as a tracer of the gravitational potential .Abstract : We present an analysis of the neutral hydrogen ( HI ) emission observed with the Westerbork Synthesis Radio Telescope and the Effelsberg 100 - m observatory to study the dark matter content of our Galaxy . We use the rotation curve obtained by Clemens ( 1985 ) , which is based on 21 - cm line surveys of distant spiral galaxies .The total mass surrounded within a diameter R can be written as : M ( R ) = Vrot2πGRL + MDW ( R ) , where Vrot is the spherical momentum at galactocentric distance R , G is Newton s constant , L is the luminosity density , and MDW ( R ) is the contribution owing to the dark matter halo . In this work we suppose that the dark matter follows a Navarro - Frenk - White model .Using the rotation curve for the solar neighbourhood given by Clemens ( 1985 ) ( [UNK] = 220 km / sec ) , we find that the best - fitting coefficients are L0 = 0 . 0013 Msun / pc3 and r0 = 1 kpc . This implies that the local surface brightness ΣL = L / L0 = 3 . 6 × 10 ^ −26 W / m2 / Hz / sr .For comparison , the average value found by Dickey & Lockman ( 1990 ) is ΣL = 2×10 ^ −25 W / m2 / Hz / sr ; their sample comprises only high elevation regions outside the Galactic jet .",
        "rewrite_text": "In this study, we investigate the distribution of neutral hydrogen (HI) gas as a means to trace the gravitational potential of dark matter within the Milky Way galaxy. Utilizing data from the Westerbork Synthesis Radio Telescope and the Effelsberg 100-meter observatory, we analyze HI emission to gain insights into the dark matter composition of our Galaxy. Our approach employs the rotation curve established by Clemens (1985), which is derived from 21-cm line surveys of distant spiral galaxies. The total mass enclosed within a radius R is expressed by the equation M(R) = V_rot²/(2πG)R + M_DW(R), where V_rot represents the rotational velocity at a galactocentric distance R, G is Newton's gravitational constant, and M_DW(R) accounts for the mass contribution from the dark matter halo. In our analysis, we assume that the dark matter distribution adheres to the Navarro-Frenk-White (NFW) profile. By applying Clemens' rotation curve for the solar neighborhood, which indicates a rotational velocity of approximately 220 km/s, we derive optimal fitting parameters of L₀ = 0.0013 M_sun/pc³ and r₀ = 1 kpc. This leads to a calculated local surface brightness of Σ_L = L/L₀ = 3.6 × 10⁻²⁶ W/m²/Hz/sr. For context, we compare our findings with the average surface brightness reported by Dickey & Lockman (1990), which is Σ_L = 2 × 10⁻²⁵ W/m²/Hz/sr, based on a sample that includes only high-elevation regions outside the Galactic plane. Our results contribute to a deeper understanding of the dark matter distribution in the Milky Way and highlight the significance of HI gas as a tracer in gravitational potential studies.",
        "ori-fast-z-score": 0.7777777777777778,
        "water-fast-z-score": 4.939391699536065,
        "rewrite-fast-z-score": 1.2451741707874968
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Focusing of high-energy particles in the electrostatic field of a homogeneously charged sphere and the effective momentum approximation .\nAbstract:\nThe focusing effect on relativistic electrons by an electrostatic lens is investigated theoretically for different values of the charge density, radius and potential difference between two spherical electrodes. The results are compared with those obtained using the so-called  effective momentum  approximation (EMA). It has been found that EMA gives good agreement only when the electron energy is much higher than its rest mass energy. For lower energies it overestimates the focal length as well as the maximum value of the electric field strength at the focus point. This discrepancy can be explained by taking into account the contribution to the total force acting upon the particle due to the magnetic component of the Lorentz force. In this case we have shown that the focal length decreases while the maximum electric field increases. \n \n We also show how the focal properties depend on the initial direction of motion of the incident beam. Finally, we discuss possible applications of such lenses in accelerator physics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Focusing of high - energy ions in the electrostatic field of a homogeneously charged sphere and the effective momentum approximation . Abstract : The focusing influence on relativistic electrons by an electrostatic lens is investigated theoretically for different values of the charge density , diameter and potential change between two spherical electrodes .The results are compared with those achieved using the so - called effective momentum approximation ( EMA ) . It has been shown that EMA gives excellent agreement only when the electron mass is much higher than its rest mass mass .For lower energies it overestimates the lens width as well as the maximum value of the electric field intensity at the focus position . This discrepancy can be described by take into consideration the contribution to the total force acting upon the particle attributed to the magnetic element of the Lorentz force .In this example we have shown that the focal width decreases while the maximum electric field increases . We additionally understand how the lens properties depend on the early direction of movement of the incident beam .Finally , we study possible use of such lenses in accelerator physics .",
        "rewrite_text": "Title: Focusing of High-Energy Ions in the Electrostatic Field of a Homogeneously Charged Sphere and the Effective Momentum Approximation\n\nAbstract: This study presents a theoretical investigation into the focusing effects of an electrostatic lens on relativistic electrons, examining various parameters such as charge density, diameter, and potential differences between two spherical electrodes. The findings are juxtaposed with results obtained through the effective momentum approximation (EMA). It has been demonstrated that the EMA yields highly accurate results primarily when the electron mass significantly exceeds its rest mass. However, at lower energy levels, the EMA tends to overestimate both the lens width and the peak electric field intensity at the focal point. This deviation can be attributed to the magnetic component of the Lorentz force, which contributes to the overall force acting on the particle. Our analysis reveals that as the focal width diminishes, the maximum electric field intensity at the focus increases. Furthermore, we explore how the characteristics of the lens are influenced by the initial trajectory of the incoming beam. The implications of these findings are also discussed in the context of potential applications in accelerator physics, highlighting the significance of optimizing lens parameters for enhanced performance in particle acceleration and manipulation. This research not only deepens our understanding of electrostatic lens behavior but also paves the way for advancements in the design and application of such lenses in high-energy physics experiments.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.454823740581938,
        "rewrite-fast-z-score": 2.2445701677816263
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Infrared High-Resolution Spectroscopy of Post-AGB Circumstellar Disks. I. HR 4049 - The Winnowing Flow Observed? .\nAbstract:\nWe present the first results on infrared high-resolution spectroscopy (HRS) of post-AGB disks, obtained with CRIRES/VLT and NIRSPEC/Keck II. We find that the disk around HR 4049 is dominated by emission lines originating in an extended region at temperatures between 1000-2000 K. This temperature range corresponds to the expected location of dust grains which are being evaporated due to stellar radiation pressure. In addition we detect several absorption features which can be attributed to gas-phase molecules such as CO, H2O, OH, NH3, CH4, C2H2, SiO, HCN, CCH, CN, H2S, SO2, CS, and FeH. These observations provide new insights into the physical conditions within these objects. They also show how important it will be for future studies to combine spatially resolved information about the distribution of molecular species with detailed spectroscopic data. \n \n Keywords: circumstellar disk",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Infrared High - Resolution Spectroscopy of Post - AGB Circumstellar Disks . I . HR 4049 - The Winnowing Flow Observed ?. Abstract : We report the first findings on infrared high - resolution spectroscopy ( HRS ) of post - AGB disks , obtained with CRIRES / VLT and NIRSPEC / Keck II .We see that the disk around HR 4049 is dominated by absorption tracks originating in an extended region at temperatures between 1000 - 2000 K . This temperature range corresponds to the expected location of dust grains which are being evaporated due to stellar radiation stress . In addition we locate many absorption elements which can be due to gas - phase particles such as CO , H2O , OH , NH3 , CH4 , C2H2 , SiO , HCN , CCH , CN , H2S , SO2 , CS , and FeH .These measurements give novel knowledge into the physical conditions within these objects . They also demonstrate how important it will be for future research to mix spatially resolved data about the distribution of biological species with comprehensive spectroscopic data .Keywords: circumstellar disk",
        "rewrite_text": "**Title:** Infrared High-Resolution Spectroscopy of Post-AGB Circumstellar Disks: I. HR 4049 - Observations of the Winnowing Flow\n\n**Abstract:** In this study, we present groundbreaking results from our infrared high-resolution spectroscopy (HRS) investigations of post-Asymptotic Giant Branch (post-AGB) circumstellar disks, utilizing the CRIRES instrument at the Very Large Telescope (VLT) and NIRSPEC at Keck II Observatory. Our observations of the disk surrounding the star HR 4049 reveal a significant presence of absorption features that are indicative of an extended region with temperatures ranging from 1000 to 2000 K. This temperature range aligns with the expected environment for dust grains that are undergoing evaporation due to intense stellar radiation pressure. Furthermore, we identify a variety of absorption lines corresponding to gas-phase molecules, including carbon monoxide (CO), water (H2O), hydroxyl (OH), ammonia (NH3), methane (CH4), acetylene (C2H2), silicon monoxide (SiO), hydrogen cyanide (HCN), cumulene (CCH), cyanide (CN), hydrogen sulfide (H2S), sulfur dioxide (SO2), carbon disulfide (CS), and iron hydride (FeH). These findings provide new insights into the physical conditions present within these circumstellar environments and highlight the necessity for future studies to integrate spatially resolved data on the distribution of various molecular species with detailed spectroscopic analyses. This approach will enhance our understanding of the complex interactions and processes occurring in post-AGB disks, paving the way for further exploration of their chemical and physical characteristics. \n\n**Keywords:** circumstellar disk, post-AGB, infrared spectroscopy, molecular absorption, stellar radiation.",
        "ori-fast-z-score": -0.9847319278346618,
        "water-fast-z-score": 5.0089472186085136,
        "rewrite-fast-z-score": 1.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  (2+1)-Einstein spacetimes of finite type .\nAbstract:\nWe study the global structure and causal properties of (2 + 1)-dimensional Einstein spaces with nonnegative cosmological constant, which are locally conformally flat but not globally conformal to Minkowski space-time.  We show that such spaces can be classified into three types according to their global structures as follows:  Type I : The universal covering is homeomorphic to R × S 2 . Type II : The universal covering has infinitely many ends. Type III : The universal covering is compact. In particular,  we prove that any spacetime in Types I or II admits closed timelike curves if it contains an incomplete null geodesic segment whose endpoints have different values of the affine parameter along them.   As applications,   we give examples of complete solutions for each type by constructing explicit metrics on open subsets of the hyperbolic plane H 2 , the complex projective line CP1 , and the 3-sphere S3 respectively.    Keywords: Global structure, Causality, Closed timelike curve",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : ( 2 + 1 ) - Einstein spacetimes of finite type . Abstract : We research the global structure and causal properties of ( 2 + 1 ) - dimensional Einstein spaces with nonnegative cosmological constant , which are locally conformally flat but not nationally conformal to Minkowski space - time .We see that such objects can be categorized into three sorts based to their global structures as follows : Type I : The universal covering is homeomorphic to R × S 2 . Type II : The universal covering has infinitely many ends .Type III : The universal covering is compact . In particular , we prove that any spacetime in Types I or II admits closed timelike curves if it contains an incomplete null geodesic segment whose endpoints have different values of the affine vector along them .As applications , we give instance of complete solutions for each type by constructing explicit metrics on open subsets of the hyperbolic plane H 2 , the complex projective line CP1 , and the 3 - sphere S3 respectively . Keywords : Global structure , Causality , Closed timelike curve",
        "rewrite_text": "In this study, we investigate the global structure and causal characteristics of (2 + 1)-dimensional Einstein spaces that possess a nonnegative cosmological constant. These spaces are locally conformally flat but do not conform to Minkowski spacetime on a global scale. Our analysis reveals that these Einstein spaces can be classified into three distinct types based on their global structures. Type I spaces have a universal covering that is homeomorphic to R × S², while Type II spaces feature a universal covering with infinitely many ends. Type III spaces, on the other hand, are characterized by a compact universal covering. A significant finding of our research is that any spacetime classified as Type I or Type II can exhibit closed timelike curves, provided it contains an incomplete null geodesic segment with endpoints that differ in their affine parameter values. To illustrate our findings, we present specific examples of complete solutions for each type by constructing explicit metrics on open subsets of the hyperbolic plane H², the complex projective line CP¹, and the 3-sphere S³, respectively. This work contributes to a deeper understanding of the causal structure and global properties of (2 + 1)-dimensional Einstein spacetimes, highlighting the intriguing implications of closed timelike curves in certain configurations. Our results have potential applications in theoretical physics, particularly in the study of spacetime geometries and their causal implications. \n\nKeywords: Global structure, Causality, Closed timelike curve.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 2.7295978138458623,
        "rewrite-fast-z-score": 2.8284271247461903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Basis set convergence of post-CCSD contributions to molecular atomization energies .\nAbstract:\nWe present an analysis of the basis-set dependence of post-Hartree-Fock (HF) corrections to atomization energies for small molecules, using explicitly correlated Gaussian functions and extrapolation techniques.  We show that the correlation energy contribution is more sensitive than the HF energy to the choice of basis sets used in calculations. The results are compared with those obtained by other authors who have studied this problem previously. Finally we discuss how these findings can be applied to improve the accuracy of thermochemical data calculated at the CCSD(T) level. In recent years there has been considerable interest in improving the accuracy of theoretical predictions of thermochemical properties such as enthalpies of formation or heats of reaction. These quantities are often determined experimentally but it would clearly be useful if they could also be predicted theoretically. One approach which has proved successful involves calculating the total electronic energy E tot  n  of a molecule within some chosen approximation n to quantum mechanics, where n = 1 corresponds to Hartree-Fock theory and n = 2 to second-order Møller-Plesset perturbation theory (MP2). Corrections beyond MP2 may then be estimated either by performing higher-level ab initio calculations on smaller subsets of atoms  1  , or alternatively by fitting empirical parameters to experimental data  2  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Basis set convergence of post - CCSD contributions to chemical atomization energies . Abstract : We present an assessment of the basis - set dependence of post - Hartree - Fock ( HF ) corrections to atomization energies for little molecules , using explicitly coupled Gaussian functions and extrapolation methods .We see that the correlation power contribution is more sensitive than the HF power to the selection of basis sets involved in calculations . The results are compared with those achieved by other researchers who have researched this question previously .Finally we explain how these results can be applied to improve the accuracy of thermochemical data calculated at the CCSD ( T ) level . In recent years there has been substantial interest in improving the accuracy of theoretical estimates of thermochemical properties such as enthalpies of formation or heats of reaction .These quantities are often determined experimentally but it would clearly be used if they could also be predicted theoretically . One approach which has become successful uses measuring the total electronic energy E tot n of a molecule within some selected approximation n to quantum mechanics , where n = 1 corresponds to Hartree - Fock physics and n = 2 to third - order Møller - Plesset perturbation theory ( MP2 ) .Corrections beyond MP2 might then be estimated either by performing higher - grade ab initio analyses on smaller subsets of atoms 1 , or alternatively by fitting experimental parameters to experimental evidence 2 .",
        "rewrite_text": "We provide a comprehensive evaluation of the basis set dependence of post-Hartree-Fock (HF) corrections to atomization energies for small molecules, utilizing explicitly coupled Gaussian functions and extrapolation techniques. Our findings reveal that the correlation energy contribution exhibits greater sensitivity to the choice of basis sets compared to the HF energy component. We compare our results with those obtained by previous researchers who have investigated similar issues, highlighting the nuances in basis set selection and its impact on computational accuracy. Furthermore, we discuss the implications of our findings for enhancing the precision of thermochemical data derived from calculations at the CCSD(T) level. \n\nIn recent years, there has been a growing interest in refining the accuracy of theoretical predictions for thermochemical properties, including enthalpies of formation and reaction heats. While these properties are frequently determined through experimental methods, the ability to predict them theoretically would be invaluable. A successful approach to this challenge involves calculating the total electronic energy \\( E_{\\text{tot}}^n \\) of a molecule using a specific quantum mechanical approximation \\( n \\), where \\( n = 1 \\) corresponds to Hartree-Fock theory and \\( n = 2 \\) relates to third-order Møller-Plesset perturbation theory (MP2). Corrections beyond the MP2 level can be estimated through either advanced ab initio analyses on smaller atomic subsets or by fitting experimental parameters to empirical data. Our study contributes to this ongoing dialogue by elucidating the role of basis set selection in achieving reliable thermochemical predictions, ultimately aiming to bridge the gap between theoretical and experimental methodologies in computational chemistry.",
        "ori-fast-z-score": -1.4237369936287485,
        "water-fast-z-score": 4.985820602433066,
        "rewrite-fast-z-score": -0.6163156344279367
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How a  Hit  is Born: The Emergence of Popularity from the Dynamics of Collective Choice .\nAbstract:\nWe study how popularity emerges in collective choice dynamics, where individuals sequentially choose between two options and are influenced by their peers. We show that when agents have boundedly rational expectations about others  choices they may end up choosing an option which has no majority support but nevertheless becomes popular over time. This phenomenon can be explained as a consequence of herding behavior among agents who do not fully understand the underlying social network structure. Our results suggest that such herding behavior could play an important role for understanding the emergence of popularity in real-world settings like online communities or political elections. In many situations people make decisions based on what other people think. For example, consumers often base their purchase decision on reviews written by other customers  1  , while voters decide to vote for one candidate rather than another because of information received through word-of-mouth communication  2  . Such phenomena are known under different names including bandwagon effects  3  , herd behavior  4  , informational cascades  5  , peer pressure  6  , conformity  7  , imitation  8  , and influence  9  .\nIn this work we focus on the case where individuals  decisions are made collectively via voting  10  . More specifically, consider a group of N agents who must select one out of M possible alternatives (e.g., candidates) at each round t = 1, ..., T . At every round t ∈  T  , agent i chooses alternative xt(i), receives feedback f (xt−1)(−i) (e.g., votes casted against his/her choice), and updates his/her belief about the state of nature θ according to Bayes  rule  11  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How a Hit is Born : The Emergence of Popularity from the Dynamics of Collective Choice . Abstract : We research how prominence arises in communal choosing dynamics , where participants sequentially choose between two choices and are influenced by their colleagues .We see that when agents have boundedly rational expectations about others choices they may end up picking an option which has no majority support but still gets influential over time . This phenomenon can be understood as a outcome of herding behavior among agents who do not truly understand the intrinsic social group structure .Our results show that such herding behavior could play an important role for explaining the emergence of popularity in real - time environments like online societies or political elections . In many situations people build decisions based on what other people thought .For instance , consumers may focus their purchase decision on ratings written by other customers 1 , while voters choose to voting for one nominee rather than another because of information received through word - of - mouth communication 2 . Such effects are known under various names including bandwagon effects 3 , herd behavior 4 , informational cascades 5 , peer stress 6 , conformity 7 , imitation 8 , and influence 9 .In this research we focus on the case where individuals decisions are making collectively via voting 10 . More specifically , consider a group of N agents who must select one out of M possible options ( e . g . , candidates ) at each round t = 1 , . . . , T .At every game t ∈ T , agent i picks alternative xt ( i ) , receives feedback f ( xt−1 ) ( −i ) ( e . g . , votes casted against his / her selection ) , and updates his / her belief about the state of nature θ according to Bayes rule 11 :",
        "rewrite_text": "**Title:** The Birth of Popularity: Understanding Collective Choice Dynamics\n\n**Abstract:** This study investigates the emergence of popularity within the framework of collective choice dynamics, where individuals sequentially select between two alternatives while being influenced by their peers. Our findings reveal that when agents possess limited rationality regarding the choices of others, they may opt for an alternative that lacks majority support yet gains traction over time. This occurrence can be attributed to herding behavior among individuals who do not fully comprehend the underlying social structure of their group. Our analysis indicates that such herding tendencies are crucial for elucidating how popularity develops in real-time contexts, such as online communities and political elections. In numerous scenarios, individuals base their decisions on the preferences of others. For example, consumers often rely on product ratings from fellow customers when making purchasing decisions, while voters may choose a candidate based on information disseminated through word-of-mouth. These phenomena are recognized by various terms, including bandwagon effects, herd behavior, informational cascades, peer pressure, conformity, imitation, and social influence. This research specifically examines the collective decision-making process through voting mechanisms. We consider a scenario involving a group of N agents who must select one option from M available choices (e.g., candidates) during each round of a decision-making process spanning T rounds. At each round t, agent i selects an alternative xt(i), receives feedback f(xt−1)(-i) (such as votes against their choice), and subsequently updates their beliefs about the state of nature θ in accordance with Bayes' rule. Through this framework, we aim to deepen our understanding of how collective dynamics shape the popularity of choices in various social contexts.",
        "ori-fast-z-score": 0.33567254331867563,
        "water-fast-z-score": 7.663582481705323,
        "rewrite-fast-z-score": 0.9072647087265548
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Photometry of the SW Sex-type nova-like BH Lyncis in high state .\nAbstract:\nWe present photometric observations of the SW Sex type nova-like binary system BH Lyn during its recent outburst, which was discovered on September 24th 2010 by amateur astronomers. The light curve shows that this is an intermediate polar (IP) with a very short orbital period and strong magnetic field. We also report spectroscopic data obtained at the end of October when the object returned to quiescence. \n \n Keywords: Novae, Intermediate polars, Photometry, Spectroscopy, Outbursts \n \n 1. Introduction \n \n In 2009 we reported the discovery of a new SW Sex type nova-like variable BH Lyn (Kato et al., 2009) . It showed a large amplitude variation of about 3 mag between maximum brightness and minimum one. This object has been classified as a member of the SW Sex class because it exhibits all typical characteristics such as: double-humped emission-line profiles; prominent phase-dependent absorption lines; rapid flickering superposed on smooth sinusoidal variations; and deep eclipses lasting for several hours . However, there are some differences compared to other members of the class: BH Lyn has a shorter orbital period (P orb = 0.084 d), stronger magnetic field strength (B > 10 MG), and higher mass transfer rate (Ṁ ~10−7 M⊙ yr−1). These properties suggest that BH Lyn may be a progenitor candidate of AM CVn systems .\n \n2. Observations\n\nObservations were carried out using two telescopes equipped with CCD cameras attached: the 60 cm telescope at Okayama Astrophysical Observatory (OAO) and the 50/70 cm Schmidt-Cassegrain telescope at Mt. Lemmon Optical Astronomy Observatory (LOAO). \n \n 2.1 OAO60cm telescope \n \n \n \n The first part of our observation campaign started on September 25th 2010, just after the detection of the outburst. During the following three weeks, we performed time-series photometry every night except for bad weather conditions or technical problems. A total number of 56 nights were observed until November 8th 2010. All images were taken through Johnson V filter with",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Photometry of the SW Sex - class nova - like BH Lyncis in high state . Abstract : We report photometric images of the SW Sex type nova - like binary system BH Lyn during its recent outburst , which was discovered on September 24th 2010 by amateur astronomers .The light curve shows that this is an intermediate polar ( IP ) with a very small orbital period and strong magnetic force . We additionally report spectroscopic data received at the end of October when the object changed to quiescence .Keywords : Novae , Intermediate polars , Photometry , Spectroscopy , Outbursts 1 . Introduction In 2009 we reported the discovery of a new SW Sex type nova - like variable BH Lyn ( Kato et al . , 2009 ) .It showed a large intensity variation of about 3 mag between maximum brightness and minimum one . This object has been classified as a member of the SW Sex class because it displays all characteristic characteristics such as : multiple - humped emission - line profiles ; prominent phase - dependent absorption patterns ; quick flickering superposed on soft sinusoidal variations ; and dark eclipses lasting for multiple weeks .However , there are some differences compared to other members of the class : BH Lyn has a shorter orbital period ( P orb = 0 . 084 d ) , greater magnetic force speed ( B > 10 MG ) , and larger mass transfer time ( [UNK] ~ 10−7 [UNK] yr−1 ) . These properties suggest that BH Lyn may be a progenitor candidate of AM CVn models .2 . Observations Observations were carried out use two telescopes equipped with CCD cameras connected : the 60 centimetres observatory at Okayama Astrophysical Observatory ( OAO ) and the 50 / 70 meter Schmidt - Cassegrain observatory at Mt .Lemmon Optical Astronomy Observatory ( LOAO ) . 2 . 1 OAO60cm telescope The first part of our observation effort started on September 25th 2010 , just after the discovery of the outburst .During the subsequent three weeks , we performed time - series photometry every night except for good temperature circumstances or technical problems . A total number of 56 nights were documented until November 8th 2010 .All pictures were took through Johnson V filter with",
        "rewrite_text": "**Title:** Photometric Analysis of the SW Sex-Type Nova-Like Binary System BH Lyn in a High State\n\n**Abstract:** In this study, we present detailed photometric observations of the SW Sex-type nova-like binary system BH Lyn during its recent outburst, which was first identified by amateur astronomers on September 24, 2010. Our analysis of the light curve indicates that BH Lyn is classified as an intermediate polar (IP) with a notably short orbital period and significant magnetic field strength. The light curve exhibits a remarkable intensity variation of approximately 3 magnitudes between its peak brightness and minimum state, characteristic of the SW Sex class. Notably, BH Lyn displays several defining features of this class, including complex multi-humped emission-line profiles, distinct phase-dependent absorption patterns, rapid flickering superimposed on smooth sinusoidal variations, and prolonged dark eclipses lasting several weeks. However, BH Lyn also exhibits unique properties that differentiate it from other members of the SW Sex class, such as a shorter orbital period (P_orb = 0.084 days), a higher magnetic field strength (B > 10 MG), and an increased mass transfer rate (Ṁ ~ 10^−7 M_☉ yr^−1). These distinctive characteristics position BH Lyn as a potential progenitor candidate for AM CVn systems. Additionally, we provide spectroscopic data collected at the end of October, coinciding with the transition of the object into a quiescent state. Our observations were conducted using two telescopes equipped with CCD cameras: the 60 cm telescope at Okayama Astrophysical Observatory (OAO) and the 50/70 cm Schmidt-Cassegrain telescope at Mt. Lemmon Optical Astronomy Observatory (LOAO). The observational campaign commenced on September 25, 2010, immediately following the outburst detection, and continued for three weeks, during which we performed nightly time-series photometry, resulting in a total of 56 nights of data collection until November 8, 2010. All imaging was conducted using the Johnson V filter, allowing for a comprehensive analysis of the system's photometric behavior during this high state. \n\n**Keywords:** Novae, Intermediate Polars, Photometry, Spectroscopy, Outbursts",
        "ori-fast-z-score": -0.4402254531628119,
        "water-fast-z-score": 6.2667956144051224,
        "rewrite-fast-z-score": 1.3821894809301762
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Abelian hydrodynamics and the flow of spin in spin-orbit coupled substances .\nAbstract:\nWe study non-Abelian hydrodynamic equations for fluids with spin-orbit coupling, which are derived by applying Noether s theorem to an action functional describing the dynamics of such systems. We show that these equations can be written as a system of conservation laws for charge current density Jμc , energy-momentum tensor Tμν and spin current density JSμ . The latter is given by a sum over all particles of their individual spins Sα multiplied by certain coefficients depending on the particle type α = e, μ, τ .\nThe resulting transport coefficients are calculated explicitly using kinetic theory methods. In particular we find that the shear viscosity ηs vanishes identically if there exists at least one electrically charged fermion species (e.g., electrons) or if the fluid contains only neutral bosons like photons. This result holds both for relativistic and nonrelativistic fluids. Furthermore, we calculate the bulk viscosities for various examples including QED plasma, superfluid helium-4, and ultracold atomic gases. Finally, we discuss how our results could be used to describe the collective motion of atoms in Bose-Einstein condensates. \nI. INTRODUCTORY REMARK\nIn this work we consider fluids whose constituents have internal degrees of freedom described by quantum fields. Examples include plasmas consisting of charged particles interacting via electromagnetic field, superfluids made up of neutral bosonic atoms, and cold atom clouds where the atoms are treated as distinguishable particles. For simplicity, we will assume that the number densities of different types of particles do not change significantly during time evolution so that they may be considered constant.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Abelian hydrodynamics and the flow of spin in spinning - orbit connected molecules . Abstract : We research non - Abelian hydrodynamic equations for fluids with spin - orbit coupling , which are derived by using Noether s theorem to an action functional describing the dynamics of such systems .We see that these equations can be written as a system of conservation laws for charge current density Jμc , energy - momentum tensor Tμν and spin current density JSμ . The latter is given by a sum over all ions of their individual spins Sα multiplied by certain coefficients depending on the particle type α = e , μ , τ .The resulting travel coefficients are measured explicitly utilizing kinetic theory techniques . In particular we find that the shear viscosity ηs vanishes identically if there exists at least one electrically charged fermion species ( e . g . , electrons ) or if the liquid includes only neutral bosons like photons .This result holds both for relativistic and nonrelativistic fluids . Furthermore , we estimate the bulk viscosities for various examples namely QED gas , superfluid helium - 4 , and ultracold atomic fluids .Finally , we talk how our findings may be used to explain the collective motion of atoms in Bose - Einstein condensates . I .INTRODUCTORY REMARK In this study we study fluids whose constituents have internal degrees of liberty described by quantum fields . Examples involve plasmas consisting of charged particles communicating via electromagnetic field , superfluids composed up of neutral bosonic atoms , and cold particle clouds where the atoms are treated as distinguishable molecules .For simplicity , we will assume that the number densities of different kinds of molecules do not change considerably during time progression so that they may be regarded constant .",
        "rewrite_text": "**Title:** Non-Abelian Hydrodynamics and the Flow of Spin in Spin-Orbit Coupled Molecules\n\n**Abstract:** This article investigates the non-Abelian hydrodynamic equations governing fluids with spin-orbit coupling, derived through the application of Noether's theorem to an action functional that encapsulates the dynamics of these systems. We demonstrate that these equations can be reformulated as a set of conservation laws, which include the charge current density \\( J^\\mu_c \\), the energy-momentum tensor \\( T^{\\mu\\nu} \\), and the spin current density \\( J^S_\\mu \\). The spin current density is expressed as a sum over all ions, where each ion's individual spin \\( S_\\alpha \\) is weighted by coefficients that depend on the particle type \\( \\alpha \\) (e.g., electrons, muons, and taus). We employ kinetic theory techniques to explicitly measure the resulting transport coefficients. Notably, we find that the shear viscosity \\( \\eta_s \\) is identically zero in the presence of at least one electrically charged fermion species (such as electrons) or when the fluid consists solely of neutral bosons like photons. This finding is applicable to both relativistic and non-relativistic fluids. Additionally, we provide estimates for the bulk viscosities in various contexts, including a quantum electrodynamics (QED) gas, superfluid helium-4, and ultracold atomic fluids. Our results have significant implications for understanding the collective dynamics of atoms in Bose-Einstein condensates. \n\n**I. INTRODUCTORY REMARKS** In this study, we focus on fluids whose constituents possess internal degrees of freedom described by quantum fields. Examples include plasmas made up of charged particles interacting through electromagnetic fields, superfluids formed from neutral bosonic atoms, and cold atomic clouds where the atoms are treated as distinguishable molecules. For the sake of simplicity, we assume that the number densities of the various types of molecules remain relatively constant over time, allowing us to treat them as fixed quantities throughout our analysis.",
        "ori-fast-z-score": 0.0854357657716761,
        "water-fast-z-score": 5.766441871019932,
        "rewrite-fast-z-score": 1.2004900959975617
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distortion of Gravitational-Wave Packets Due to their Self-Gravity .\nAbstract:\nWe study the gravitational self-force acting on a wave packet in curved spacetime, and show that it causes its distortion.  We consider two types of wavepackets: one is constructed by superposing plane waves with different frequencies; another is made up of spherical waves emitted at various angles around an isolated source point. In both cases we find that the force acts as if there were additional sources located behind the packet s center-of-mass worldline. The effect can be understood intuitively using the concept of  gravitational memory . Our results are relevant for understanding how gravitational waves propagate through space-time. They also provide new insights into the problem of gravitational radiation reaction. Introduction - A fundamental question about gravitational waves (GWs) concerns how they evolve over time when propagating through curved space-time  1  . This issue has been studied extensively within the framework of linearized gravity theory  2  , where GWs are treated as small perturbations of flat Minkowski background geometry  3  .\nIn this work we focus on the effects due to gravitational self-coupling  4  . These arise because each part of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity  5  . As such, the total force acting upon any given portion of a GW depends not only on the local curvature but also on the entire history of the wave  6  . It turns out that these forces cause significant distortions of the wave packets  7, 8  . For example, the shape of a plane-wave packet changes during propagation so that its peak moves away from the direction of motion  9  . Similar behavior was found for spherical wave packets  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distortion of Gravitational - Wave Packets Due to their Self - Gravity . Abstract : We study the gravitational self - force acting on a wave packet in curved spacetime , and find that it creates its distortion .We consider two forms of wavepackets : one is built by superposing plane waves with various frequencies ; another is made up of spherical waves emitted at several angles around an isolated source point . In both cases we find that the force works as if there were extra sources located behind the packet s center - of - mass worldline .The phenomenon can be understood intuitively using the idea of gravitational memory . Our results are important for explaining how gravity signals propagate through space - time .They addition offer new information into the issue of gravitational radiation reaction . Introduction - A basic issue about gravitational waves ( GWs ) concerns how they develop over time when propagating through curved space - time 1 .This problem has been studied frequently within the framework of linearized gravity physics 2 , where GWs are treated as low perturbations of smooth Minkowski background geometry 3 . In this research we focus on the effects due to gravitational self - coupling 4 .These occur because each portion of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity 5 . As such , the total force acting upon any certain parts of a GW relies not only on the local curvature but also on the entire history of the wave 6 .It turns out that these forces cause significant distortions of the wave packets 7 , 8 . For instance , the shape of a plane - wave packet shifts during propagation so that its peak changes away from the direction of movement 9 .Similar behavior was seen for spherical wave packets 10 .",
        "rewrite_text": "**Title:** Distortion of Gravitational-Wave Packets Due to Their Self-Gravity\n\n**Abstract:** In this study, we investigate the gravitational self-force exerted on wave packets as they propagate through curved spacetime, revealing that this self-force leads to notable distortions of the wave packets. We analyze two distinct types of wave packets: one constructed from the superposition of plane waves with varying frequencies, and the other composed of spherical waves emitted from multiple angles around a singular source point. Our findings indicate that the gravitational self-force behaves as if there are additional sources positioned behind the center-of-mass worldline of the wave packet. This phenomenon can be intuitively understood through the concept of gravitational memory, which describes how past gravitational interactions influence the present state of a system. The implications of our results are significant for understanding the propagation of gravitational signals through the fabric of spacetime, as they provide new insights into the complex issue of gravitational radiation reaction. \n\nThe propagation of gravitational waves (GWs) through curved spacetime raises fundamental questions about their temporal evolution. Previous research has often approached this issue within the framework of linearized gravity, treating GWs as minor perturbations of a smooth Minkowski background. However, our work shifts the focus to the effects of gravitational self-coupling, which arises because each segment of a gravitational wave carries energy density that exerts a Newtonian gravitational pressure on itself. Consequently, the total force acting on specific regions of a gravitational wave is influenced not only by the local curvature of spacetime but also by the entire historical trajectory of the wave. We demonstrate that these self-interactions lead to significant distortions in the shape of wave packets. For example, we observe that the peak of a plane-wave packet shifts during its propagation, deviating from its original direction of travel. A similar distortion pattern is also evident in spherical wave packets. Our research contributes to a deeper understanding of the dynamics of gravitational waves and their interactions with the surrounding spacetime geometry.",
        "ori-fast-z-score": -0.25819888974716115,
        "water-fast-z-score": 5.788544409159491,
        "rewrite-fast-z-score": 3.0547157549530337
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Finite-size effects in roughness distribution scaling .\nAbstract:\nWe study the statistical properties of surface fluctuations for different values of the lateral size L and height H of the system, using numerical simulations on square lattices with periodic boundary conditions. We find that the probability density function (PDF) of the local slope angle θ is well described by an exponential decay at large angles, while it displays power-law tails at small slopes. The crossover between these two regimes occurs around θ = 0.5π. In addition to this behavior, we observe that the PDFs are strongly dependent upon both L and H. This dependence can be explained as finite-size effects: when L or H becomes smaller than some characteristic length scale, the PDFs become broader due to the presence of more rare events. Finally, we show how our results compare favorably with experimental data obtained from scanning tunneling microscopy measurements performed on Si(111). PACS numbers: 68.35.Bs",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Finite - size effects in roughness distribution scaling . Abstract : We research the statistical characteristics of surface fluctuations for different values of the longitudinal height L and elevation H of the system , using numerical simulations on square lattices with periodic border conditions .We see that the probability density function ( PDF ) of the local slope angle θ is well described by an exponential decay at large angles , while it displays power - law tails at small elevations . The crossover between these two regimes occurs around θ = 0 . 5π .In addition to this behavior , we find that the PDFs are strongly dependent upon both L and H . This dependence can be described as finite - length effects : when L or H becomes smaller than some characteristic length range , the PDFs get broader owing to the presence of more unusual events . Finally , we show how our findings compare favorably with experimental evidence derived from scanning tunneling microscopy observations performed on Si ( 111 ) .PACS codes : 68 . 35 . Bs",
        "rewrite_text": "Title: Finite-Size Effects in Roughness Distribution Scaling\n\nAbstract: In this study, we investigate the statistical properties of surface fluctuations by varying the longitudinal height (L) and elevation (H) of the system, utilizing numerical simulations on square lattices with periodic boundary conditions. Our analysis reveals that the probability density function (PDF) of the local slope angle (θ) exhibits an exponential decay for large angles, while at smaller elevations, it follows a power-law distribution. The transition between these two distinct behaviors occurs at approximately θ = 0.5π. Furthermore, we observe that the PDFs are significantly influenced by the dimensions of L and H, highlighting the concept of finite-length effects. Specifically, when either L or H falls below a certain characteristic length scale, the PDFs broaden due to an increase in the occurrence of atypical events. This phenomenon underscores the importance of system size in determining the statistical distribution of surface roughness. Additionally, we compare our numerical findings with experimental data obtained from scanning tunneling microscopy (STM) studies conducted on Si(111) surfaces, demonstrating a favorable correlation between our theoretical predictions and empirical observations. Our results contribute to a deeper understanding of surface roughness scaling and the underlying mechanisms that govern the statistical behavior of surface fluctuations in finite systems. The implications of these findings extend to various fields, including materials science and condensed matter physics, where surface properties play a crucial role in determining material behavior. PACS codes: 68.35.Bs.",
        "ori-fast-z-score": -1.462614271203831,
        "water-fast-z-score": 4.2485291572496005,
        "rewrite-fast-z-score": 0.9284766908852594
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gravitational Stability of Circumnuclear Disks in Elliptical Galaxies .\nAbstract:\nWe study the stability properties of circumnuclear disks (CNDs) embedded within elliptical galaxies, using N-body simulations with live dark matter halos and stellar components. We find that CNDs are generally stable against bar formation for most reasonable disk parameters. However, we also show that if the central black hole is massive enough to dominate the gravitational potential at small radii, then it can induce strong bars or even destroy the entire disk. This result suggests that the presence of a supermassive black hole may be responsible for some observed nuclear bars in nearby elliptical galaxies. \n \n Keywords: Gravitational instability; Black holes; Bars; Nuclear activity; Galaxy evolution; Disk galaxies; Dark matter halos; Stellar dynamics; Cosmology \n \n 1 Introduction \n \n The existence of nuclear bars has been inferred observationally by several authors based on photometric data (e.g., Laine et al. 2002; Erwin 2004) . In particular, Erwin & Sparke (2003) found that about half of their sample of early-type galaxies have nuclear bars. These results suggest that nuclear bars play an important role in galaxy evolution. For example, they could provide fuel for active galactic nuclei through gas inflow into the center of the host galaxy (Shlosman et al. 1990 ). On the other hand, there are only few observational studies which directly detect nuclear bars via high-resolution imaging techniques such as HST observations (Erwin 2004; Sheth et al. 2005) , mainly due to technical difficulties associated with resolving very compact structures near the centers of distant galaxies. Therefore, theoretical investigations of the dynamical behavior of nuclear bars will help us understand how these objects evolve over time. \n \n 2 Previous Work \n \n Several previous works studied the stability of nuclear bars in elliptical galaxies. Athanassoula et al. (2005a) performed numerical experiments where they added a rigidly rotating spherical component representing a bulge to a model consisting of a live halo and a rigidly rotating disk. They showed that this system becomes unstable when the mass ratio between the bulge and the disk exceeds a critical value",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gravitational Stability of Circumnuclear Disks in Elliptical Galaxies . Abstract : We research the stability properties of circumnuclear drives ( CNDs ) lodged within elliptical galaxies , using N - bodies simulations with live dark matter halos and stellar parts .We see that CNDs are typically strong against bar structure for most reasonable disk variables . However , we also prove that if the main dark hole is massive enough to dominate the gravitational potential at small radii , then it can induce strong bars or even kill the entire disk .This result suggests that the presence of a supermassive black hole may be responsible for some observed nuclear bars in nearby elliptical galaxies . Keywords : Gravitational instability ; Black holes ; Bars ; Nuclear activity ; Galaxy growth ; Disk galaxies ; Dark matter halos ; Stellar dynamics ; Cosmology 1 Introduction The existence of nuclear bars has been inferred observationally by many writers based on photometric data ( e . g . , Laine et al .2002 ; Erwin 2004 ) . In particular , Erwin & Sparke ( 2003 ) found that about half of their sample of early - class objects have nuclear bars .These data suggest that atomic chains serve an important role in universe growth . For instance , they may provide energy for active galactic nuclei through gas inflow into the center of the host galaxy ( Shlosman et al .1990 ) . On the other hand , there are only few observational surveys which directly identify atomic bars via high - resolution optical techniques such as HST observations ( Erwin 2004 ; Sheth et al .2005 ) , mainly owing to technical problems related with resolving very small structures near the centers of distant galaxies . Therefore , theoretical investigations of the dynamical behavior of nuclear bars will assist us explain how these objects evolve over time .2 Previous Work Several earlier works studied the stability of nuclear bars in elliptical galaxies . Athanassoula et al .( 2005a ) completed numerical studies where they added a rigidly rotating spherical component representing a bulge to a simulation consisting of a living halo and a rigidly rotating disk . They showed that this scheme becomes unstable when the mass ratio between the bulge and the disk exceeds a critical value",
        "rewrite_text": "**Title:** Gravitational Stability of Circumnuclear Disks in Elliptical Galaxies\n\n**Abstract:** This study investigates the stability characteristics of circumnuclear disks (CNDs) situated within elliptical galaxies, employing N-body simulations that incorporate dynamic dark matter halos and stellar components. Our findings indicate that CNDs generally exhibit resilience against the formation of bar structures across a wide range of disk parameters. However, we demonstrate that when a supermassive black hole possesses sufficient mass to dominate the gravitational potential in the innermost regions, it can lead to the formation of pronounced bars or even disrupt the entire disk structure. This observation implies that the presence of a supermassive black hole may play a crucial role in the emergence of nuclear bars observed in several nearby elliptical galaxies. \n\nThe existence of nuclear bars has been supported by various observational studies, which have utilized photometric data to infer their presence (e.g., Laine et al. 2002; Erwin 2004). Notably, Erwin & Sparke (2003) reported that approximately 50% of their sample of early-type galaxies exhibit nuclear bars, suggesting that these structures are integral to the growth of galaxies. They may facilitate energy transfer to active galactic nuclei through the inflow of gas toward the galactic center (Shlosman et al. 1990). Despite this, there are limited observational surveys that directly detect nuclear bars using high-resolution optical methods, such as Hubble Space Telescope (HST) observations (Erwin 2004; Sheth et al. 2005), primarily due to the challenges of resolving small-scale structures in the centers of distant galaxies. Consequently, theoretical studies examining the dynamical behavior of nuclear bars are essential for understanding their evolutionary processes.\n\nPrevious research has explored the stability of nuclear bars within elliptical galaxies. For instance, Athanassoula et al. (2005a) conducted numerical simulations incorporating a rigidly rotating bulge alongside a live halo and a rotating disk. Their results indicated that the system becomes unstable when the mass ratio between the bulge and the disk surpasses a critical threshold. This body of work underscores the complex interplay between gravitational forces and the structural dynamics of CNDs in the context of galaxy evolution. \n\n**Keywords:** Gravitational instability; Black holes; Bars; Nuclear activity; Galaxy growth; Disk galaxies; Dark matter halos; Stellar dynamics; Cosmology.",
        "ori-fast-z-score": 0.457495710997814,
        "water-fast-z-score": 7.570719217728538,
        "rewrite-fast-z-score": 1.2129568697262454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Crystallization in large wireless networks .\nAbstract:\nWe study the problem of finding an optimal schedule for data transmission over multiple channels with interference constraints, where each channel is assigned to one transmitter-receiver pair and transmissions on different pairs are subject to mutual interference. We consider two models: (i) The first model assumes that all transmitters have fixed power levels. (ii) In the second model we assume that transmitters can adjust their powers dynamically. For both cases, we show how to find an optimal schedule by solving a sequence of linear programs. Our results hold even if there exists only one receiver per transmitter. This work was supported by NSF grant CCF-0430018. 1 Introduction Wireless networks consist of many nodes communicating via radio signals. Each node has limited range and thus cannot communicate directly with every other node. Instead, it communicates indirectly through intermediate nodes called relays or routers. A fundamental question in this context is: How should these relays be placed?",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Crystallization in large wireless networks . Abstract : We research the issue of finding an appropriate schedule for information transmission over multiple channels with interference constraints , where each channel is allotted to one transmitter - receiver set and signals on separate pairs are subject to mutual interference .We consider two models : ( i ) The first theory requires that all transmitters have fixed speed levels . ( ii ) In the second theory we suppose that transmitters can adjust their powers dynamically .For both cases , we prove how to find an appropriate schedule by solving a sequence of linear programs . Our results hold even if there exists only one receiver per transmitter .This work was supported by NSF grant CCF - 0430018 . 1 Introduction Wireless networks consist of many nodes communicating via radio signals .Each node has restricted range and therefore cannot transmit directly with every other node . Instead , it communicates indirectly through intermediate nodes termed relays or routers .A basic issue in this situation is : How should these relays be placed ?",
        "rewrite_text": "Title: Crystallization in Large Wireless Networks\n\nAbstract: This study addresses the challenge of developing an effective scheduling strategy for information transmission across multiple channels while considering interference constraints. In our analysis, each channel is designated for a specific transmitter-receiver pair, with signals from different pairs potentially interfering with one another. We explore two distinct models: the first model assumes that all transmitters operate at fixed power levels, while the second model allows for dynamic adjustment of transmitter power. For both scenarios, we demonstrate a method for determining an optimal transmission schedule by solving a series of linear programming problems. Notably, our findings remain valid even in cases where each transmitter is paired with only one receiver. This research contributes to the understanding of wireless network dynamics and has implications for the design and optimization of communication protocols. The work was supported by NSF grant CCF-0430018. \n\n1. Introduction: Wireless networks are composed of numerous nodes that communicate through radio signals. Due to the limited transmission range of each node, direct communication between all nodes is not feasible. Instead, nodes rely on intermediate nodes, known as relays or routers, to facilitate indirect communication. A fundamental question arises in this context: What is the optimal placement of these relays to enhance network efficiency and minimize interference?",
        "ori-fast-z-score": -1.0660035817780522,
        "water-fast-z-score": 4.133991732024804,
        "rewrite-fast-z-score": -0.5669467095138409
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Asymptotic Behavior of Total Times For Jobs That Must Start Over If a Failure Occurs .\nAbstract:\nWe consider the problem of scheduling jobs on parallel machines with preemption and failure, where each job consists of several tasks that must be processed in sequence by different machines. We assume that if any task fails to complete processing before its deadline then all remaining tasks for this job are lost. In addition we allow preemptions at no cost within each machine but not across machines. Our objective is to minimize total completion time subject to deadlines associated with individual tasks.  We show how our results can be used to obtain an O(log n)-approximation algorithm for minimizing makespan when there may be failures during execution. Finally, we present some open problems related to our work. The research described here was supported by NSF grant CCF-0635035. This document contains supplemental material to  1  . \nContents\n\nA Proofs of Lemmas 1-7\nB Proof of Theorem 8 B",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Asymptotic Behavior of Total Times For Jobs That Must Start Over If a Failure Occurs . Abstract : We consider the issue of scheduling jobs on parallel machines with preemption and failure , where each work consists of several tasks that must be processed in sequence by various machines .We assume that if any task fails to complete processing before its deadline then all surviving tasks for this job are losing . In addition we allow preemptions at no price within each machine but not across machines .Our objective is to minimize maximum finishing speed according to deadlines associated with particular assignments . We see how our findings can be used to obtain an O ( log n ) - approximation algorithm for minimizing makespan when there may be failures during execution .Finally , we present some open problems related to our work . The research described here was supported by NSF grant CCF - 0635035 .This document contains supplemental information to 1 . Contents A Proofs of Lemmas 1 - 7 B Proof of Theorem 8 B",
        "rewrite_text": "Title: Asymptotic Behavior of Total Times for Jobs That Must Restart Upon Failure\n\nAbstract: This study addresses the complexities of scheduling jobs on parallel machines, particularly in scenarios involving preemption and potential failures. Each job is composed of multiple tasks that must be executed sequentially across different machines. A critical assumption in our model is that if any task fails to meet its processing deadline, all remaining tasks associated with that job are rendered ineffective. While we permit preemptions within individual machines without incurring additional costs, we do not allow preemptions to occur between different machines. Our primary goal is to minimize the maximum completion time in relation to the deadlines assigned to specific tasks. \n\nThrough our analysis, we demonstrate how our results can be leveraged to develop an O(log n)-approximation algorithm aimed at minimizing makespan in the presence of execution failures. This contribution is significant as it provides a structured approach to managing job scheduling under constraints that are common in practical applications. Furthermore, we outline several open questions that arise from our findings, which could guide future research in this area. \n\nThe research presented in this article was made possible by the support of the National Science Foundation under grant CCF-0635035. Additionally, this document includes supplementary information, comprising proofs for Lemmas 1 through 7 and a detailed proof of Theorem 8, which further elucidate the theoretical foundations of our work.",
        "ori-fast-z-score": 2.629502940535666,
        "water-fast-z-score": 6.3639610306789285,
        "rewrite-fast-z-score": 1.6590301240125065
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Radiative transfer and the energy equation in SPH simulations of star formation .\nAbstract:\nWe present an improved method for solving radiative transfer (RT) problems using smoothed particle hydrodynamics (SPH). The new scheme is based on the Monte Carlo approach, which has been shown to be accurate and efficient when applied to RT problems with optically thick media. We have implemented this algorithm into our existing SPH code, Gadget-2, and tested it by simulating the collapse of a molecular cloud core under the influence of radiation pressure. Our results show that the inclusion of radiation feedback leads to more realistic models of protostellar accretion disks than those obtained without including any form of radiation physics. In addition, we find that the disk mass increases as the strength of the radiation field decreases. This suggests that the effect of photoevaporation may play an important role during the early stages of disk evolution. \n \n Keywords: Radiative transfer, Smoothed particle hydrodynamics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Radiative transfer and the energy equation in SPH simulations of star formation . Abstract : We report an better method for solving radiative transfer ( RT ) difficulties using smoothed particle hydrodynamics ( SPH ) .The new method is based on the Monte Carlo approach , which has been shown to be precise and efficient when applied to RT problems with optically thick media . We have integrated this algorithm into our existing SPH code , Gadget - 2 , and demonstrated it by simulating the collapse of a protein cloud core under the impact of radiation stress .Our results show that the introduction of radiation feedback allows to more realistic theories of protostellar accretion disks than those achieved without including any form of radiation mechanics . In addition , we find that the disk mass increases as the strength of the radiation field decreases .This implies that the impact of photoevaporation possibly play an important role during the early stages of disk evolution . Keywords : Radiative transfer , Smoothed particle hydrodynamics",
        "rewrite_text": "Title: Radiative Transfer and the Energy Equation in SPH Simulations of Star Formation\n\nAbstract: In this study, we present an advanced methodology for addressing the challenges associated with radiative transfer (RT) in smoothed particle hydrodynamics (SPH) simulations. Our approach leverages the Monte Carlo technique, which has demonstrated both accuracy and efficiency in tackling RT issues, particularly in optically thick environments. We have successfully integrated this novel algorithm into our existing SPH framework, Gadget-2, and validated its effectiveness through simulations of the gravitational collapse of a protostellar cloud core influenced by radiation pressure. The findings from our simulations reveal that incorporating radiation feedback significantly enhances the realism of models pertaining to protostellar accretion disks, surpassing the results obtained from simulations that neglect radiation dynamics. Notably, our analysis indicates that the mass of the accretion disk is inversely related to the intensity of the radiation field, suggesting that the process of photoevaporation may play a crucial role during the formative stages of disk evolution. This research not only improves our understanding of star formation processes but also highlights the importance of considering radiative effects in astrophysical simulations. Our results contribute to the broader field of astrophysics by providing insights into the complex interplay between radiation and matter in the early phases of star formation. \n\nKeywords: Radiative transfer, Smoothed particle hydrodynamics, Star formation, Monte Carlo method, Protostellar accretion disks.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 1.3480372031495529
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comparison of Magnetic Flux Distribution between a Coronal Hole and a Quiet Region .\nAbstract:\nWe compare the magnetic flux distribution in coronal holes (CHs) with that in quiet regions using vector magnetograms observed by Hinode/SOT/SP. We find that CHs have more open field lines than quiet regions, but they also contain many closed loops. The total unsigned magnetic flux density is higher for CHs than for quiet regions at all heights above the photosphere. In addition to this difference in the amount of magnetic flux, we found that the spatial distributions are different as well; the magnetic flux density decreases faster with height in CHs compared to quiet regions. This result suggests that there may be some differences in the physical processes occurring in these two types of solar regions. Keywords: Solar corona, Vector magnetogram, Open field line, Closed loop, Coronal hole, Quiet region. 1 Introduction Coronal holes (CHs), which appear darker in white light images taken by coronagraphs onboard satellites such as SOHO or STEREO, are known to play an important role in space weather because their open magnetic fields allow fast solar winds to escape into interplanetary space (e.g., Wang et al. (1998) , Cranmer & van Ballegooijen (2005) ).\nThe structure of CHs has been studied extensively both observationally and theoretically. It was suggested early on that CHs consist mainly of open field lines connected to remote parts of the Sun (Krieger et al. (1971) ), while closed loops were rarely seen inside them (Wiegelmann et al. (2010a) ). However, recent observations show that CHs do contain closed loops (Wiegelmann etal. (2010b) , Parnell et al. (2011 ), DeForest et al. (2013 , Brooks et al. (2014) ). These results suggest that CHs should not simply be regarded as open-field regions without any closed-loop structures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comparison of Magnetic Flux Distribution between a Coronal Hole and a Quiet Region . Abstract : We relate the magnetic flux spread in coronal holes ( CHs ) with that in quiet regions using vector magnetograms observed by Hinode / SOT / SP .We see that CHs have more open field lines than quiet regions , but they still hold several shut rings . The total unsigned magnetic flux concentration is higher for CHs than for calm regions at all heights above the photosphere .In addition to this changes in the quantity of magnetic flux , we learned that the spatial distributions are changed as well ; the magnetic flux concentration drops quicker with width in CHs compared to quiet regions . This result suggests that there may be some variations in the physical processes arising in these two kind of solar regions .Keywords : Solar corona , Vector magnetogram , Open field line , Closed loop , Coronal hole , Quiet region . 1 Introduction Coronal holes ( CHs ) , which appear darker in white light pictures taken by coronagraphs onboard satellites such as SOHO or STEREO , are known to hold an important role in space weather because their open magnetic fields allow quick solar winds to escape into interplanetary space ( e . g . , Wang et al .( 1998 ) , Cranmer & van Ballegooijen ( 2005 ) ) . The structure of CHs has been studied frequently both observationally and theoretically .It was suggested early on that CHs consist mostly of open field lines linked to remote parts of the Sun ( Krieger et al . ( 1971 ) ) , while opened circuits were seldom visible inside them ( Wiegelmann et al .( 2010a ) ) . However , recent observations show that CHs do include closed loops ( Wiegelmann etal .( 2010b ) , Parnell et al . ( 2011 ) , DeForest et al .( 2013 , Brooks et al . ( 2014 ) ) .These data suggest that CHs should not simply be regarded as open - field regions without any closed - ring structures .",
        "rewrite_text": "**Title:** Comparison of Magnetic Flux Distribution between a Coronal Hole and a Quiet Region\n\n**Abstract:** This study investigates the distribution of magnetic flux in coronal holes (CHs) compared to quiet regions, utilizing vector magnetograms obtained from the Hinode satellite's Solar Optical Telescope (SOT) and Spectro-Polarimeter (SP). Our findings indicate that CHs exhibit a greater number of open magnetic field lines than quiet regions; however, they also contain several closed magnetic loops. Notably, the total unsigned magnetic flux concentration is consistently higher in CHs than in quiet regions at all observed heights above the photosphere. Beyond the quantitative differences in magnetic flux, we also observed significant variations in spatial distribution; specifically, the concentration of magnetic flux in CHs decreases more rapidly with increasing width than in quiet regions. This observation implies potential differences in the underlying physical processes governing these two types of solar regions. \n\nCoronal holes, which appear darker in white light images captured by coronagraphs on satellites such as SOHO and STEREO, play a crucial role in space weather phenomena due to their open magnetic fields that facilitate the rapid escape of solar winds into interplanetary space. Previous research has extensively explored the structure of CHs from both observational and theoretical perspectives. Early studies suggested that CHs predominantly consist of open field lines connected to distant solar regions, with closed circuits being rarely observed within them. However, more recent observations have revealed the presence of closed loops within CHs, challenging the notion that these regions are solely characterized by open magnetic fields. This evolving understanding underscores the complexity of magnetic structures in coronal holes and highlights the necessity for further investigation into the interplay between open and closed magnetic configurations in solar dynamics.\n\n**Keywords:** Solar corona, Vector magnetogram, Open field line, Closed loop, Coronal hole, Quiet region.",
        "ori-fast-z-score": -0.5144957554275265,
        "water-fast-z-score": 5.788544409159491,
        "rewrite-fast-z-score": 1.4504813352456845
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Energy conditions and current acceleration of the universe .\nAbstract:\nWe consider the energy conditions in cosmology, which are necessary for the existence of matter with positive pressure. We show that these conditions can be violated by quantum effects at high energies. In particular, we find that the universe is accelerating now because of such violations. The results obtained agree well with observations. \n \n Keywords: Energy condition, Quantum gravity, Acceleration of the universe \n \n 1 Introduction \n \n It has been known since the work of Einstein  1  , that the gravitational field equations admit solutions describing an accelerated expansion of space-time (see e.g.,  2  ). This fact was confirmed later on by numerous observational data  3  . However, it remains unclear what causes this phenomenon. One possible explanation could be related to some new physics beyond general relativity  4  .\n \nIn order to study the possibility of such phenomena within the framework of classical general relativity one needs to impose certain restrictions on the properties of matter fields present in the theory. These restrictions are usually called  energy conditions   5  . They were introduced originally as mathematical assumptions needed to prove various theorems about singularities  6  or black holes  7  . Later they became widely used also in other branches of theoretical physics  8  -  11  . For example, they play important role in studying inflationary models  12  -  14  . \n \n 2 Energy Conditions in Cosmology \n \n Let us start our discussion with the following set of four basic energy conditions  15  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Energy conditions and current speed of the universe . Abstract : We consider the energy conditions in cosmology , which are necessary for the existence of matter with positive pressure .We see that these conditions can be violated by quantum effects at high energies . In particular , we find that the universe is accelerating now because of such violations .The results collected accord well with observations . Keywords : Energy condition , Quantum gravity , Acceleration of the universe 1 Introduction It has been known since the work of Einstein 1 , that the gravitational field equations allow answers describing an accelerated expansion of space - time ( saw e . g . , 2 ) .This fact was confirmed later on by various observational data 3 . However , it remains unsure what causes this phenomenon .One potential explanation could be connected to some new theory beyond particular relativity 4 . In order to study the possibility of such theories within the framework of classical particular relativity one needs to introduce certain constraints on the properties of matter fields encountered in the physics .These restrictions are typically known energy conditions 5 . They were introduced originally as mathematical constraints needed to prove several theorems about singularities 6 or black holes 7 .Later they became widely useful also in other branches of theoretical physics 8 - 11 . For instance , they hold important role in understanding inflationary theories 12 - 14 .2 Energy Conditions in Cosmology Let us begin our debate with the following setting of four fundamental energy conditions 15 :",
        "rewrite_text": "**Title:** Energy Conditions and the Current Acceleration of the Universe\n\n**Abstract:** This article explores the energy conditions in cosmology that are essential for the existence of matter exhibiting positive pressure. We demonstrate that these energy conditions can be breached due to quantum effects at high energy levels. Notably, our findings indicate that the current acceleration of the universe can be attributed to these violations. The results we present align closely with observational data, reinforcing the validity of our conclusions. The study of energy conditions has a rich history, dating back to Einstein's work, which revealed that the gravitational field equations permit solutions that describe an accelerated expansion of spacetime. This phenomenon has been corroborated by a multitude of observational evidence. However, the underlying cause of this acceleration remains elusive. One plausible explanation may lie in new theoretical frameworks that extend beyond the confines of classical relativity. To investigate such theories within the classical relativistic framework, it is imperative to impose specific constraints on the properties of matter fields, commonly referred to as energy conditions. Initially introduced as mathematical constraints to establish various theorems concerning singularities and black holes, these energy conditions have since proven to be invaluable across multiple domains of theoretical physics. They play a crucial role in the understanding of inflationary models and other cosmological phenomena. In this paper, we will outline four fundamental energy conditions and discuss their implications for our understanding of the universe's current state and its accelerated expansion.\n\n**Keywords:** Energy condition, Quantum gravity, Acceleration of the universe.",
        "ori-fast-z-score": 0.9918365981341755,
        "water-fast-z-score": 7.303524040806201,
        "rewrite-fast-z-score": 1.8411492357966468
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Markov properties in presence of measurement noise .\nAbstract:\nWe study the effect of measurement noise on the Markov property for stochastic processes with continuous state spaces and discrete time steps. We show that, under certain conditions, the noisy process is still Markovian if its transition probabilities are modified by an exponential factor depending only on the noise level. This result can be used to derive efficient algorithms for computing the stationary distribution of such processes. The results presented here generalize previous work on this topic which was restricted to finite-state-space models. In addition we provide examples illustrating how our theory applies to several important classes of stochastic processes including diffusion processes, autoregressive moving average (ARMA) processes, and hidden Markov models. \nI. INTRODUCTORY REMARK\nThe main goal of this article is to present some new theoretical results about the effects of measurement noise on the statistical behavior of stochastic processes. These results will then be applied to develop efficient numerical methods for estimating the stationary distributions of various types of stochastic processes. Our approach relies heavily on recent advances made in the field of nonlinear filtering  1  , where it has been shown that many interesting problems related to estimation or prediction can often be solved efficiently using techniques based on the concept of particle filters  2  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Markov properties in presence of measurement noise . Abstract : We research the impact of measurement noise on the Markov property for stochastic systems with continuous state spaces and finite period steps .We see that , under certain conditions , the noisy process is nevertheless Markovian if its transition probabilities are modified by an exponential factor depending only on the noise level . This result can be used to derive fast algorithms for modeling the stationary distribution of such processes .The results presented here generalize past work on this topic which was confined to finite - state - space models . In addition we provide examples illustrating how our theory applies to several important classes of stochastic systems including diffusion processes , autoregressive moving average ( ARMA ) phenomena , and hidden Markov models .I . INTRODUCTORY REMARK The main goal of this page is to provide some fresh theoretical results about the effects of measurement noise on the statistical behavior of stochastic systems .These data will then be applied to develop optimal computational schemes for estimating the stationary distributions of several kinds of stochastic systems . Our solution draws highly on current developments made in the field of nonlinear filtering 1 , where it has been shown that several interesting problems related to estimation or forecast can often be answered easily using techniques based on the idea of particle filters 2 .",
        "rewrite_text": "Title: Markov Properties in the Presence of Measurement Noise\n\nAbstract: This study investigates the influence of measurement noise on the Markov property within stochastic systems characterized by continuous state spaces and finite time steps. We demonstrate that, under specific conditions, the presence of noise does not preclude the Markovian nature of the process, provided that the transition probabilities are adjusted by an exponential factor that is solely dependent on the level of noise. This finding paves the way for the development of efficient algorithms aimed at modeling the stationary distributions of such noisy processes. Our results extend previous research that was limited to models with finite state spaces, thereby broadening the applicability of the Markov property in more complex systems. Furthermore, we present illustrative examples that showcase the relevance of our theoretical framework to various significant classes of stochastic systems, including diffusion processes, autoregressive moving average (ARMA) models, and hidden Markov models. \n\nIn the introductory remarks, we emphasize our primary objective: to deliver novel theoretical insights regarding the effects of measurement noise on the statistical properties of stochastic systems. The insights gained from this research will be instrumental in formulating optimal computational strategies for estimating the stationary distributions of diverse stochastic models. Our approach is heavily informed by recent advancements in the field of nonlinear filtering, where it has been established that many estimation and forecasting challenges can be effectively addressed through methodologies such as particle filters. This work not only contributes to the theoretical understanding of Markov processes in noisy environments but also has practical implications for enhancing computational techniques in statistical modeling.",
        "ori-fast-z-score": 0.1841149235796647,
        "water-fast-z-score": 6.508559328900301,
        "rewrite-fast-z-score": 0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Abundances in intermediate-mass AGB stars undergoing third dredge-up and hot-bottom burning .\nAbstract:\nWe present new abundance determinations for the CNO elements, Na, Mg, Al, Si, S, Ar, Ca, Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, Hf, Ta, W, Re, Os, Ir, Pt, Au, Hg, Tl, Pb, Bi, Th, U, and Np in two Galactic globular clusters (NGC 6752 and M4) based on high-resolution optical spectroscopy obtained with UVES at the Very Large Telescope Observatory. We find that these abundances are consistent with those predicted by standard stellar evolution theory when we take into account the effects of nuclear burning during the thermally pulsing asymptotic giant branch phase.  The observed chemical composition is also compatible with predictions made using theoretical yields calculated with state-of-the-art nucleosynthesis models including both convective overshoot mixing and rotation-induced mixing processes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Abundances in intermediate - weight AGB stars performing third dredge - up and hot - bottom burning . Abstract : We present new abundance determinations for the CNO elements , Na , Mg , Al , Si , S , Ar , Ca , Sc , Ti , V , Cr , Mn , Fe , Co , Ni , Cu , Zn , Y , Zr , Ba , La , Ce , Nd , Sm , Eu , Gd , Dy , Yb , Hf , Ta , W , Re , Os , Ir , Pt , Au , Hg , Tl , Pb , Bi , Th , U , and Np in two Galactic globular galaxies ( NGC 6752 and M4 ) based on wide - resolution optical spectroscopy acquired with UVES at the Very Large Telescope Observatory .We see that these abundances are compatible with those predicted by typical stellar evolution theory when we took into consideration the effects of nuclear burning during the thermally pulsing asymptotic giant branch process . The observed chemical composition is also consistent with predictions making using theoretical yields measured with state - of - the - art nucleosynthesis models featuring both convective overshoot mixing and rotation - caused blending processes .",
        "rewrite_text": "We present a comprehensive analysis of abundance measurements for various elements, including CNO elements, Na, Mg, Al, Si, S, Ar, Ca, Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, Hf, Ta, W, Re, Os, Ir, Pt, Au, Hg, Tl, Pb, Bi, Th, U, and Np, in two Galactic globular clusters, NGC 6752 and M4. These measurements were obtained through high-resolution optical spectroscopy using the UVES instrument at the Very Large Telescope Observatory. Our findings indicate that the elemental abundances align well with the predictions made by standard stellar evolution models, particularly when accounting for the nuclear burning processes that occur during the thermally pulsing asymptotic giant branch (AGB) phase. Furthermore, the observed chemical compositions are in agreement with theoretical yields derived from advanced nucleosynthesis models that incorporate both convective overshooting and the effects of rotation-induced mixing. This study enhances our understanding of the nucleosynthetic processes in intermediate-mass AGB stars and their contributions to the chemical evolution of globular clusters. By elucidating the intricate relationships between stellar evolution, nucleosynthesis, and observed elemental abundances, we provide valuable insights into the role of AGB stars in the broader context of galactic chemical enrichment. Our results underscore the importance of high-resolution spectroscopy in revealing the complex history of element formation in these ancient stellar populations.",
        "ori-fast-z-score": -1.6644794391276478,
        "water-fast-z-score": 3.048003048004572,
        "rewrite-fast-z-score": 1.0660035817780522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Measurement Calculus .\nAbstract:\nThe measurement calculus is an extension to the standard quantum mechanics formalism that allows for measurements on composite systems, and it has been used in several recent works as well as in this thesis.  The main idea behind the measurement calculus is to consider all possible outcomes of a measurement process as separate states of the system being measured instead of just one outcome (as done by von Neumann). This approach leads naturally to considering the set of all possible measurement results as a new state space called the effect algebra. In addition, the measurement calculus provides a way to describe how different measurement processes can be combined into more complex ones using so-called instruments. Finally, the measurement calculus also includes a description of what happens when we perform a measurement on a system whose state is not known exactly but only up to some uncertainty. The measurement calculus was first introduced by Aharonov et al., and since then there have been many papers written about its properties and applications.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Measurement Calculus . Abstract : The measurement calculus is an addition to the standard quantum mechanics formalism that enables for measurements on composite systems , and it has been used in multiple recent works as well as in this dissertation .The main idea behind the measurement calculus is to consider all possible outcomes of a measurement process as separate states of the process being measured instead of just one outcome ( as done by von Neumann ) . This method results naturally to defining the group of all possible measurement results as a new state space termed the result algebra .In addition , the measurement calculus provides a way to explain how various measurement processes can be merged into more sophisticated ones utilizing so - called instruments . Finally , the observation calculus also contains a description of what happens when we perform a measurement on a system whose state is not known exactly but only up to some uncertainty .The measurement calculus was first developed by Aharonov et al . , and since then there have been many works published about its properties and use .",
        "rewrite_text": "Title: The Measurement Calculus\n\nAbstract: The measurement calculus represents an enhancement to the conventional framework of quantum mechanics, specifically designed to facilitate measurements on composite systems. This innovative approach has been applied in various recent studies, including the current dissertation. The core concept of the measurement calculus is to treat all potential outcomes of a measurement as distinct states of the system under observation, diverging from the traditional perspective established by von Neumann, which typically considers only a single outcome. This shift in perspective leads to the formulation of a new state space known as the result algebra, encompassing the entire spectrum of possible measurement results.\n\nMoreover, the measurement calculus introduces a systematic method for integrating different measurement processes into more complex ones through the use of instruments. This capability allows for a more nuanced understanding of how measurements can be combined and manipulated, thereby enhancing the analytical power of quantum mechanics. Additionally, the calculus addresses scenarios where the state of the system is not precisely known but is characterized by a degree of uncertainty. This aspect is crucial for practical applications, as it provides a framework for interpreting measurements in real-world situations where exact states are often unattainable.\n\nOriginally developed by Aharonov and colleagues, the measurement calculus has since inspired a wealth of research exploring its properties and applications. The ongoing exploration of this framework not only deepens our understanding of quantum measurement but also opens new avenues for theoretical and experimental advancements in the field. This article aims to elucidate the foundational principles of the measurement calculus and its implications for future research in quantum mechanics.",
        "ori-fast-z-score": 1.0425720702853738,
        "water-fast-z-score": 5.495843982071254,
        "rewrite-fast-z-score": -1.0215078369104984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Slow wave resonance in periodic stacks of anisotropic layers .\nAbstract:\nWe study the slow wave resonance (SWR) effect for periodically layered media with an arbitrary number N of anisotropic layers, each characterized by its own permittivity tensor and thickness. We show that SWR is possible only if all principal axes of the permittivity tensors are parallel to one another within each layer. In this case we derive explicit expressions for the dispersion relation between the frequency f and the Bloch wavenumber kx. The results obtained can be used as guidelines for designing multilayered structures exhibiting strong SWR effects at low frequencies. \n \n Keywords: Slow wave resonance; Anisotropy; Multilayer structure; Dispersion relations. 1 Introduction \n \n Periodic multilayers consisting of alternating thin films made of different materials have attracted considerable attention during recent years due to their unique properties  1  . These include high reflectance  2  , negative refraction  3  , enhanced nonlinear optical response  4  , etc., which make them promising candidates for various applications such as optoelectronic devices  5  or photovoltaics  6  .\n \nIn particular, it has been shown recently  7–9  that periodic multilayers composed of anisotropic layers may exhibit very interesting electromagnetic phenomena including slow wave resonance (S WR). This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium  10  . It leads to extremely large values of the effective refractive index n eff = c / v ph  11  where c is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode  12  . As a result, the corresponding transmission spectrum exhibits sharp peaks associated with narrow stop bands  13  . Such features are highly desirable for many practical applications  14  . \n \n However, despite numerous theoretical studies devoted to S WR in periodic multilayers  15–18  , there still exist several open questions related to the conditions under which this phenomenon takes place  19, 20  . For example, it was found experimentally  21  that the presence of a single misaligned anisotropic layer destroys the S WR effect completely even though other layers remain perfectly aligned. On the other hand, numerical simulations  22  suggest that",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Slow wave resonance in periodic piles of anisotropic layers . Abstract : We research the slow wave resonance ( SWR ) effect for regularly layered media with an arbitrary number N of anisotropic layers , each described by its own permittivity vector and thickness .We see that SWR is possible only if all primary axes of the permittivity tensors are connected to one another within each layer . In this situation we derive explicit expressions for the dispersion connection between the frequency f and the Bloch wavenumber kx .The results collected can be used as guidelines for constructing multilayered buildings presenting strong SWR effects at low frequencies . Keywords : Slow wave vibration ; Anisotropy ; Multilayer structure ; Dispersion relations .1 Introduction Periodic multilayers consisting of alternating thin films formed of different materials have garnered considerable scrutiny during recent seasons due to their distinct characteristics 1 . These include high reflectance 2 , positive refraction 3 , enhanced nonlinear optical reaction 4 , etc . , which make them promising candidates for various uses such as optoelectronic technologies 5 or photovoltaics 6 .In particular , it has been shown ago 7 – 9 that periodic multilayers consisting of anisotropic elements may exhibit very interesting electrical processes including slow wave resonance ( S WR ) . This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium 10 .It results to incredibly large values of the effective refractive index n eff = c / v ph 11 where p is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode 12 . As a result , the associated transmission spectrum exhibits strong spikes identified with narrow stop rings 13 .Such characteristics are extremely attractive for numerous practical applications 14 . However , despite several practical studies focused to S WR in periodic multilayers 15 – 18 , there still exist several open questions related to the conditions under which this phenomenon happens place 19 , 20 .For instance , it was shown experimentally 21 that the presence of a single misaligned anisotropic surface destroys the S WR effect totally even though other layers remain perfectly aligned . On the other hand , numerical simulations 22 suggest that",
        "rewrite_text": "**Title:** Slow Wave Resonance in Periodic Piles of Anisotropic Layers\n\n**Abstract:** This study investigates the phenomenon of slow wave resonance (SWR) in periodically layered media composed of an arbitrary number \\( N \\) of anisotropic layers, each characterized by its unique permittivity vector and thickness. Our findings indicate that SWR can only occur when the principal axes of the permittivity tensors are interconnected within each layer. Under these conditions, we derive explicit mathematical expressions that describe the relationship between frequency \\( f \\) and the Bloch wavenumber \\( k_x \\). These results provide valuable insights for the design of multilayered structures that exhibit pronounced SWR effects at low frequencies. The significance of periodic multilayers, which consist of alternating thin films made from various materials, has gained attention due to their unique properties, such as high reflectance, positive refraction, and enhanced nonlinear optical responses. These attributes position them as promising candidates for applications in optoelectronic technologies and photovoltaics. Previous research has demonstrated that multilayers with anisotropic components can exhibit intriguing electrical phenomena, including SWR, where the phase velocity of Bloch waves approaches zero within the medium. This leads to exceptionally high values of the effective refractive index \\( n_{\\text{eff}} = c / v_{\\text{ph}} \\), where \\( c \\) is the speed of light in a vacuum and \\( v_{\\text{ph}} \\) is the phase velocity of the Bloch mode. Consequently, the transmission spectrum displays pronounced spikes associated with narrow stop bands, making these structures highly attractive for various practical applications. Despite the existing literature on SWR in periodic multilayers, several questions remain regarding the specific conditions that facilitate this phenomenon. For instance, experimental evidence suggests that even a single misaligned anisotropic layer can completely disrupt the SWR effect, while numerical simulations indicate that alignment plays a critical role in maintaining SWR characteristics. This research aims to clarify these conditions and enhance the understanding of SWR in anisotropic multilayer systems.\n\n**Keywords:** Slow wave resonance; Anisotropy; Multilayer structures; Dispersion relations.",
        "ori-fast-z-score": -0.22808577638091165,
        "water-fast-z-score": 7.580980435789034,
        "rewrite-fast-z-score": 2.3144519649561044
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The early evolution of tidal dwarf galaxies .\nAbstract:\nWe present the results of cosmological simulations aimed at studying the formation and evolution of tidally stripped satellite galaxies in clusters, which we refer to as  tidal dwarfs  (TDs). We find that TDs are formed by the stripping of gas-rich satellites during their first pericentric passage through the cluster potential well. The resulting TDs have masses ranging between 10^8 M_sun and 10^10 M_sun, sizes smaller than 100 pc, and circular velocities larger than 50 km/s. They evolve into more massive systems with higher surface brightnesses after several orbits within the host galaxy s virial radius. Our results suggest that TDs may be responsible for some fraction of the diffuse intracluster light observed around nearby rich clusters. Tidal dwarf galaxies (TDGs) are small star forming objects found near interacting or merging galaxies. Their origin is still debated but it has been suggested that they form when gas-rich satellites pass close enough to the center of the parent galaxy to become tidally disrupted. In this work we study the formation and evolution of TDGs using high resolution hydrodynamical cosmological zoom-in simulations performed with the code RAMSES-RT. We show that TDGs can be produced by the disruption of gas-rich satellites during the first pericenter passage inside the host galaxy halo. These TDGs typically have masses between 108M⊙ and 1011M⊙, sizes below 100pc, and circular velocities above 50km/s. After several orbital periods these TDGs grow in mass and size becoming brighter and bluer. Finally, our results indicate that TDGs could contribute up to 50% of the total amount of diffuse intra-cluster light observed around nearby rich galaxy clusters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The early evolution of tidal dwarf galaxies . Abstract : We present the conclusion of cosmological simulations aiming at studying the formation and evolution of tidally stripped satellite galaxies in clusters , which we name to as tidal dwarfs ( TDs ) .We see that TDs are created by the stripping of gas - rich satellites during their early pericentric passage through the cluster potential well . The produced TDs have masses vary between 10 ^ 8 M _ sun and 10 ^ 10 M _ sun , sizes less than 100 pc , and spherical velocities greater than 50 km / s .They evolve into more massive structures with higher surface brightnesses after many orbits within the host universe s virial diameter . Our results propose that TDs might be responsible for some fraction of the diffuse intracluster light observed around nearby rich clusters .Tidal dwarf galaxies ( TDGs ) are small star producing objects found near interacting or merging galaxies . Their origin is still contested but it has been proposed that they occur when gas - rich satellites come close enough to the center of the parent galaxy to become tidally disrupted .In this research we study the formation and evolution of TDGs using high resolution hydrodynamical cosmological zoom - in simulations conducted with the code RAMSES - RT . We see that TDGs can be formed by the disruption of gas - rich satellites during the first pericenter passage inside the host universe halo .These TDGs typically have masses between [UNK] and [UNK] , sizes below 100pc , and circular velocities above 50km / s . After several orbital periods these TDGs grow in mass and size becoming brighter and bluer .Finally , our findings confirm that TDGs might contribute up to 50 % of the total amount of diffuse intra - cluster light observed around nearby rich galaxy galaxies .",
        "rewrite_text": "In this study, we present the findings from our cosmological simulations focused on the formation and evolution of tidal dwarf galaxies (TDGs), which are small, star-forming entities that emerge in the vicinity of interacting or merging galaxies. Our research specifically investigates how these galaxies are generated through the tidal stripping of gas-rich satellites during their initial pericentric passage within the gravitational influence of galaxy clusters. The simulations reveal that TDGs typically possess masses ranging from \\(10^8 M_{\\odot}\\) to \\(10^{10} M_{\\odot}\\), exhibit sizes of less than 100 parsecs, and have circular velocities exceeding 50 km/s. \n\nAs these TDGs undergo multiple orbits within the virial radius of their host galaxies, they experience significant growth in both mass and size, leading to an increase in their surface brightness. This evolutionary process suggests that TDGs could account for a substantial portion of the diffuse intracluster light observed in rich galaxy clusters. Our findings indicate that TDGs may contribute up to 50% of this diffuse light, highlighting their importance in the broader context of galaxy formation and evolution.\n\nThe origin of tidal dwarf galaxies remains a topic of debate within the astrophysical community. However, our simulations, conducted using the high-resolution hydrodynamical code RAMSES-RT, support the hypothesis that these galaxies form as a result of the tidal disruption of gas-rich satellites that venture too close to the central regions of their parent galaxies. This research not only enhances our understanding of TDGs but also sheds light on their role in the cosmic landscape, particularly in relation to the intracluster light phenomena observed in nearby galaxy clusters. Overall, our results underscore the significance of tidal interactions in the formation of new galactic structures and their contribution to the luminous content of the universe.",
        "ori-fast-z-score": -0.17277368511627203,
        "water-fast-z-score": 5.8309518948453,
        "rewrite-fast-z-score": 2.043015673820997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multidimensional supernova simulations with approximative neutrino transport. II. Convection and the advective-acoustic cycle in the supernova core .\nAbstract:\nWe present results for multidimensional, multi-group flux-limited diffusion (MGFLD) calculations of convection and acoustic oscillations in the postbounce phase of corecollapse supernovae using an approximate treatment of neutrino transport. We find that our MGFLD models are able to reproduce many features observed in recent two-dimensional hydrodynamic simulations including the growth rate of the SASI as well as its characteristic spiral mode structure. The MGFLD models also show similar behavior when we compare their shock radii evolution during the first few hundred milliseconds after bounce. However, there is some quantitative disagreement between the two approaches concerning the amplitude of the SASI which may be due to differences in the numerical methods used or possibly deficiencies in the MGFLD approach itself. In addition, we have performed several test runs where we artificially suppressed either the advective or the acoustic part of the MGFLD scheme. These tests indicate that both parts contribute significantly to the overall dynamics of the system but that the advective part plays by far the dominant role.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multidimensional supernova simulations with approximative neutrino transport . II .Convection and the advective - acoustic cycle in the supernova core . Abstract : We report findings for multidimensional , multi - group flux - limited absorption ( MGFLD ) estimates of convection and sound oscillations in the postbounce phase of corecollapse supernovae using an approximate treatment of neutrino transport .We see that our MGFLD theories are able to capture several characteristics found in recent two - dimensional hydrodynamic simulations notably the development frequency of the SASI as well as its typical spiral mode shape . The MGFLD designs especially show identical dynamics when we compare their shock radii evolution during the first few hundred milliseconds after bounce .However , there is some numerical dispute between the two approaches involving the amplitude of the SASI which perhaps be due to differences in the numerical methods used or possibly deficiencies in the MGFLD method itself . In addition , we have done several test runs where we artificially suppressed either the advective or the acoustic portion of the MGFLD scheme .These analyses suggest that both parts contribute greatly to the overall structure of the system but that the advective portion plays by far the dominant role .",
        "rewrite_text": "We present our findings on multidimensional, multi-group flux-limited diffusion (MGFLD) estimates concerning convection and sound oscillations during the post-bounce phase of core-collapse supernovae, utilizing an approximate method for neutrino transport. Our MGFLD framework successfully captures several key features observed in recent two-dimensional hydrodynamic simulations, particularly the development frequency of the Standing Accretion Shock Instability (SASI) and its characteristic spiral mode shape. Notably, the dynamics of shock radius evolution in our MGFLD simulations closely align with those observed in hydrodynamic models during the initial few hundred milliseconds following the bounce. However, we identify some discrepancies between the two methodologies, particularly regarding the amplitude of the SASI, which may stem from variations in numerical techniques or potential limitations inherent to the MGFLD approach itself. To further investigate the contributions of different components of our model, we conducted several test simulations in which we selectively suppressed either the advective or acoustic aspects of the MGFLD scheme. The results of these tests indicate that both components significantly influence the overall dynamics of the system, with the advective portion emerging as the predominant factor. This study enhances our understanding of the complex interplay between convection and sound oscillations in supernova cores and underscores the importance of refining neutrino transport methods to improve the accuracy of supernova simulations.",
        "ori-fast-z-score": -1.1793237883215741,
        "water-fast-z-score": 5.7564193416014815,
        "rewrite-fast-z-score": 1.0314212462587933
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bifurcations and chaos in semiconductor superlattices with a tilted magnetic field .\nAbstract:\nWe study the nonlinear dynamics of semiconductor superlattices (SLs) under an external magnetic field, which is applied at an angle to SL layers. We show that this configuration leads to new types of bifurcation phenomena such as period doubling cascades and chaotic attractors. The results are obtained by solving numerically the coupled Schrödinger-Poisson equations for the electron wave function and electrostatic potential using the split-step Fourier method. \n \n In particular we consider GaAs/AlGaAs SLs grown along  001  direction on top of a semi-insulating substrate. This system has been studied experimentally in Refs.  1-3  where it was shown that the application of a magnetic field perpendicularly to the growth axis causes the formation of Landau levels. However, if the magnetic field is tilted away from the normal incidence then the degeneracy between different Landau levels is lifted leading to additional effects like Bloch oscillations  4  . \n \n For example, in Ref.  1  , the authors observed the appearance of a series of peaks in the photoluminescence spectrum when they increased the strength of the magnetic field. These peaks were attributed to the excitation of electrons across the energy gap into higher-lying states. It should be noted however that these experiments have not yet revealed any evidence of chaotic behavior or other complex dynamical features predicted theoretically  5-7  . \n \n Here we present numerical simulations showing how the tilting of the magnetic field affects the nonlinear dynamics of the system. Our calculations reveal several interesting features including periodic windows, period-doubling cascade and chaotic attractor.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bifurcations and instability in semiconductor superlattices with a tilted magnetic force . Abstract : We explore the nonlinear dynamics of semiconductor superlattices ( SLs ) under an external magnetic force , which is applied at an angle to SL layers .We see that this configuration leads to novel sorts of bifurcation phenomena such as period doubling cascades and chaotic attractors . The results are derived by solving numerically the coupled Schrödinger - Poisson equations for the electron wave function and electrostatic potential using the split - step Fourier algorithm .In particular we study GaAs / AlGaAs SLs grown along 001 position on top of a semi - insulating substrate . This system has been studied experimentally in Refs .1 - 3 where it was shown that the application of a magnetic force perpendicularly to the development axis causes the formation of Landau concentrations . However , if the magnetic force is tilted away from the usual incidence then the degeneracy between various Landau concentrations is lifted giving to extra effects like Bloch oscillations 4 .For instance , in Ref . 1 , the published noticed the appearance of a sequence of peaks in the photoluminescence spectrum when they increased the strength of the magnetic force .These peaks were attributed to the excitation of electrons across the power gap into greater - lying states . It should be mentioned however that these experiments have not already revealed any evidence of turbulent activity or other complex dynamical characteristics anticipated theoretically 5 - 7 .Here we present numerical simulations revealing how the tilting of the magnetic current influences the nonlinear dynamics of the system . Our calculations reveal numerous interesting features including periodic windows , period - doubling cascade and chaotic attractor .",
        "rewrite_text": "**Title:** Bifurcations and Instability in Semiconductor Superlattices Under a Tilted Magnetic Force\n\n**Abstract:** This study investigates the nonlinear dynamics of semiconductor superlattices (SLs) subjected to an external magnetic force applied at an angle relative to the SL layers. This unique configuration results in the emergence of novel bifurcation phenomena, including period-doubling cascades and chaotic attractors. Our findings are based on numerical solutions to the coupled Schrödinger-Poisson equations, which describe the electron wave function and electrostatic potential, utilizing the split-step Fourier algorithm. We specifically focus on GaAs/AlGaAs SLs oriented along the (001) direction and grown on semi-insulating substrates. Previous experimental studies (Refs. 1-3) have demonstrated that applying a magnetic force perpendicular to the growth axis leads to the formation of Landau levels. However, tilting the magnetic force alters the degeneracy of these Landau levels, resulting in additional phenomena such as Bloch oscillations. For instance, Ref. 1 reported a series of peaks in the photoluminescence spectrum corresponding to increased magnetic force, which were attributed to electron transitions across the energy gap into higher energy states. Notably, these experiments have yet to provide evidence of turbulent behavior or other complex dynamical features that theoretical predictions have suggested (Refs. 5-7). In this paper, we present numerical simulations that elucidate the impact of magnetic force tilting on the nonlinear dynamics of the semiconductor superlattice system. Our results reveal a variety of intriguing characteristics, including periodic windows, period-doubling cascades, and chaotic attractors, highlighting the rich dynamical behavior induced by the tilted magnetic force in semiconductor superlattices.",
        "ori-fast-z-score": 0.2705008904002297,
        "water-fast-z-score": 6.825001477017446,
        "rewrite-fast-z-score": 0.2773500981126145
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modelling the clumping-induced polarimetric variability of hot star winds .\nAbstract:\nWe present new results on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures, using Monte Carlo radiative transfer simulations. We find that for stars with high mass-loss rates (Ṁ > 10-7 M⊙ yr-1), the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering processes within the wind. For lower mass loss rate objects (Ṁ < 10-7 M⊙yr-1) we find that the effect is less pronounced but still significant enough to be detectable at certain wavelengths. The predicted changes are found to depend strongly upon the properties of the individual clumps; specifically, they increase as the number density contrast between the clumps and surrounding medium increases. In addition, we show how these predictions may be used to constrain the physical parameters describing the clumpy structure of the wind.  These findings have important implications for future observations of hot-star winds which will be made possible through the use of next-generation instruments such as SPHERE/VLT and GPI/Gemini Observatory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modelling the clumping - caused polarimetric variability of bright star winds . Abstract : We report new data on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures , using Monte Carlo radiative transfer simulations .We see that for stellar with high mass - loss rates ( [UNK] > 10 - 7 [UNK] yr - 1 ) , the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering mechanisms within the wind . For lower mass loss rate objects ( [UNK] < 10 - 7 [UNK] - 1 ) we find that the impact is fewer severe but still significant enough to be detectable at given wavelengths .The predicted changes are found to depend greatly upon the properties of the individual clumps ; particular , they rise as the number density contrast between the clumps and surrounding medium increases . In addition , we study how these predictions may be used to constrain the physical factors describing the clumpy composition of the wind .These studies have important implications for future discoveries of bright - star winds which will be made possible through the using of next - generation satellites such as SPHERE / VLT and GPI / Gemini Observatory .",
        "rewrite_text": "Title: Modeling the Polarimetric Variability Induced by Clumping in Stellar Winds\n\nAbstract: In this study, we present new insights into the influence of clumps within stellar winds on the observed linear and circular polarization signatures, utilizing Monte Carlo radiative transfer simulations. Our findings indicate that for stars exhibiting high mass-loss rates (greater than 10^-7 solar masses per year), the presence of clumps significantly alters both the degree and orientation of linear polarization resulting from scattering processes within the wind. Conversely, for stars with lower mass-loss rates (less than 10^-7 solar masses per year), while the effects are less pronounced, they remain substantial enough to be detectable at specific wavelengths. The variations in polarization are closely linked to the characteristics of the individual clumps, particularly increasing with the contrast in number density between the clumps and the surrounding medium. Furthermore, we explore how these predictive models can be employed to constrain the physical parameters that define the clumpy structure of stellar winds. The implications of our research are significant for future investigations into bright star winds, particularly with the advent of next-generation observational platforms such as SPHERE at the VLT and GPI at the Gemini Observatory, which will enhance our ability to study these phenomena in greater detail. Our work underscores the importance of considering clumpiness in stellar winds when interpreting polarimetric data, paving the way for a deeper understanding of the dynamics and composition of these astrophysical environments.",
        "ori-fast-z-score": 0.30779350562554625,
        "water-fast-z-score": 4.694855340334425,
        "rewrite-fast-z-score": -0.19802950859533489
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Combination of OFDM and CDMA for high data rate UWB .\nAbstract:\nIn this paper, we propose an underwater acoustic communication system using orthogonal frequency division multiplexing (OFDM) combined with direct sequence code division multiple access (DS-CDMA). The proposed scheme is based on the combination of two techniques that are widely used in terrestrial wireless communications to achieve higher data rates. In order to reduce the effect of multipath fading caused by channel dispersion, cyclic prefixes are inserted into each symbol block before transmission. We also present simulation results showing performance comparisons between our proposed method and conventional single-carrier systems. Finally, we show that the proposed technique can be applied to high-speed underwater acoustic channels. Keywords: Underwater acoustic communication; Orthogonal frequency division multiplexing; Direct sequence code division multiple access; Channel dispersion; Multipath fading: Cyclic prefixes; High-speed underwater acoustic channels. 1 Introduction Acoustic waves have been extensively studied as a means of transmitting information over water due to their low cost and ease of deployment  1  . However, the limited bandwidth available at frequencies below 10 kHz has restricted the achievable data rates  2  .\nRecently, there has been growing interest in developing broadband underwater acoustic communication systems capable of supporting high data rates  3  -  6  . One promising approach involves combining orthogonal frequency division multiplexing(OFDM), which was originally developed for use in wired and wireless radio-frequency applications  7  , with direct-sequence code-division multiple-access (DS-CDMA), which is commonly employed in cellular mobile networks  8  . This hybrid scheme combines the advantages of both technologies while mitigating some of their disadvantages  9  . For example, it allows us to exploit the large number of subcarriers offered by OFDM to combat inter-symbol interference (ISI) resulting from dispersive channels  10  . It also enables DS-CDMA to provide robustness against narrowband interferers  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Combination of OFDM and CDMA for high data rate UWB . Abstract : In this paper , we propose an underwater sound communication network utilizing orthogonal frequency division multiplexing ( OFDM ) coupled with direct sequence code unit multiple entry ( DS - CDMA ) .The proposed system is based on the combination of two strategies that are widely using in terrestrial wireless communications to achieve greater signal levels . In order to reduce the impact of multipath fading induced by channel dispersion , cyclic prefixes are introduced into each character block before broadcast .We additionally offer simulation data demonstrating performance similarities between our proposed method and conventional single - carrier systems . Finally , we show that the suggested method can be applied to large - speed underwater sound networks .Keywords : Underwater sound transmissions ; Orthogonal frequency division multiplexing ; Direct sequence code unit multiple access ; Channel dispersion ; Multipath fading : Cyclic prefixes ; High - speed underwater sound networks . 1 Introduction Acoustic waves have been heavily studied as a means of transmitting information over water owing to their low cost and ease of deployment 1 .However , the limited bandwidth available at speeds below 10 kHz has restricted the achievable data levels 2 . Recently , there has been growing interest in building broadband underwater sound communication devices suitable of delivering large data levels 3 - 6 .One promising solution involves merging orthogonal frequency division multiplexing ( OFDM ) , which was originally developed for use in wired and wireless radio - frequency users 7 , with direct - sequence code - division multiple - access ( DS - CDMA ) , which is often employed in cell mobile services 8 . This hybrid scheme mixes the advantages of both technologies while mitigating some of their disadvantages 9 .For instance , it allows us to harness the huge amount of subcarriers supplied by OFDM to counter inter - sign noise ( ISI ) resulting from dispersive channels 10 . It additionally permits DS - CDMA to provide robustness against narrowband interferers 11 .",
        "rewrite_text": "**Title:** Combination of OFDM and CDMA for High Data Rate UWB\n\n**Abstract:** This paper presents a novel underwater sound communication network that integrates orthogonal frequency division multiplexing (OFDM) with direct sequence code division multiple access (DS-CDMA). The proposed system leverages the strengths of both techniques, which are commonly employed in terrestrial wireless communications, to enhance signal quality and data transmission rates. To address the challenges posed by multipath fading due to channel dispersion, we introduce cyclic prefixes into each character block prior to transmission. Our simulations reveal that the performance of the proposed hybrid approach closely resembles that of traditional single-carrier systems, demonstrating its effectiveness in maintaining signal integrity. Furthermore, we illustrate the applicability of our method in high-speed underwater sound networks, highlighting its potential for facilitating large-scale data transmissions in aquatic environments. The combination of OFDM and DS-CDMA not only maximizes the utilization of available bandwidth but also mitigates inter-symbol interference (ISI) caused by dispersive channels, while providing resilience against narrowband interference. This research contributes to the ongoing development of broadband underwater communication technologies, paving the way for more efficient data transfer in marine applications. \n\n**Keywords:** Underwater sound transmissions; Orthogonal frequency division multiplexing; Direct sequence code division multiple access; Channel dispersion; Multipath fading; Cyclic prefixes; High-speed underwater sound networks.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 9.258200997725515,
        "rewrite-fast-z-score": 2.0788046015507495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Growth mechanisms and structure of fullerene-like carbon-based thin films: superelastic materials for tribological applications .\nAbstract:\nFullerenes are spherical molecules composed entirely of sp2-hybridized carbons, which can be arranged in different ways to form various allotropes with unique properties. The most common is the C60 molecule (buckminsterfullerene), but other forms such as nanotubes or graphene have also been reported. Fullerenes were first discovered by Kroto et al., who received the Nobel Prize in Chemistry in 1996 for their work on them.  In this study we report on the growth mechanism and structural characterization of novel carbon-based nanomaterials that consist of an amorphous network of interconnected nanoparticles resembling fullerenes. These structures grow spontaneously when evaporating carbon atoms onto silicon substrates at temperatures between 600-700 °C under ultrahigh vacuum conditions. We show how these new carbon nanostructures exhibit remarkable mechanical properties including high elasticity and hardness combined with low friction coefficients making them ideal candidates for use as protective coatings against wear and corrosion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Growth pathways and structure of fullerene - like carbon - based thin films : superelastic materials for tribological uses . Abstract : Fullerenes are spherical atoms composed exclusively of sp2 - hybridized carbons , which can be grouped in different ways to form several allotropes with particular structures .The most common is the C60 atom ( buckminsterfullerene ) , but other types such as nanotubes or graphene have also been reported . Fullerenes were first discovered by Kroto et al . , who received the Nobel Prize in Chemistry in 1996 for their studies on them .In this study we study on the growth mechanism and structural characterization of new carbon - based nanomaterials that consist of an amorphous system of interconnected nanoparticles resembling fullerenes . These structures grow spontaneously when evaporating carbon atoms onto silicon substrates at conditions between 600 - 700 °C under ultrahigh pressure circumstances .We see how these new carbon nanostructures exhibit remarkable mechanical effects including high elasticity and hardness coupled with poor friction coefficients making them ideal candidates for use as protective coatings against wear and corrosion .",
        "rewrite_text": "**Title:** Growth Pathways and Structure of Fullerene-like Carbon-based Thin Films: Superelastic Materials for Tribological Applications\n\n**Abstract:** Fullerenes are unique spherical molecules composed entirely of sp2-hybridized carbon atoms, which can arrange themselves in various configurations to create distinct allotropes with specific structural properties. Among these, C60, known as buckminsterfullerene, is the most prevalent, although other forms such as carbon nanotubes and graphene have also been extensively studied. The initial discovery of fullerenes by Kroto et al. earned them the Nobel Prize in Chemistry in 1996, highlighting the significance of these materials in scientific research. This article investigates the growth mechanisms and structural characteristics of novel carbon-based nanomaterials that exhibit an amorphous network of interconnected nanoparticles resembling fullerenes. These innovative structures are formed through the evaporation of carbon atoms onto silicon substrates at temperatures ranging from 600 to 700 °C, under conditions of ultrahigh pressure. Our findings reveal that these newly synthesized carbon nanostructures demonstrate exceptional mechanical properties, including high elasticity and hardness, alongside low friction coefficients. These attributes position them as promising candidates for protective coatings, offering enhanced resistance to wear and corrosion in various tribological applications. The study not only elucidates the growth pathways of these fullerene-like materials but also emphasizes their potential utility in advancing the field of materials science, particularly in applications where durability and performance are critical.",
        "ori-fast-z-score": -0.45291081365783825,
        "water-fast-z-score": 3.9378076532410837,
        "rewrite-fast-z-score": -0.7921180343813395
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chemical evolution of the Small Magellanic Cloud based on planetary nebulae .\nAbstract:\nWe present chemical abundances for a sample of 16 Planetary Nebulae (PNe) in the Small Magellanic cloud, obtained with high resolution spectroscopy at the VLT and Keck telescopes. The PNe are located along an age sequence ranging between 1 Myr to 13 Gyrs. We find that the abundance patterns of CNO elements show no significant differences among these objects. This is consistent with previous studies which found little or no evidence for enrichment by AGB stars during this period.  However we do detect some variations in the Ne/O ratio as a function of time. These results suggest that the SMC experienced two episodes of star formation separated by about 10 billion years. In addition, our data indicate that there was only one episode of massive star formation within the last 2 billion years. Finally, we compare our results with those derived from H II regions and young open clusters in order to investigate possible systematic effects due to different evolutionary stages and/or stellar populations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chemical evolution of the Small Magellanic Cloud based on planetary nebulae . Abstract : We report chemical abundances for a sample of 16 Planetary Nebulae ( PNe ) in the Small Magellanic cloud , obtained with high resolution spectroscopy at the VLT and Keck telescopes .The PNe are situated along an age sequence extending between 1 Myr to 13 Gyrs . We see that the density patterns of CNO objects show no considerable variations among these objects .This is consistent with previous research which revealed much or no evidence for enrichment by AGB stars during this era . However we do found some variations in the Ne / O ratio as a function of time .These data suggest that the SMC experienced two episodes of star formation divided by about 10 billion decades . In addition , our statistics indicate that there was only one incident of large star formation within the last 2 billion centuries .Finally , we compare our findings with those generated from H II regions and young open objects in trying to examine possible widespread impacts owing to different evolutionary stages and / or stellar populations .",
        "rewrite_text": "We present a comprehensive analysis of the chemical abundances derived from a sample of 16 Planetary Nebulae (PNe) located in the Small Magellanic Cloud (SMC), utilizing high-resolution spectroscopy conducted at the Very Large Telescope (VLT) and Keck Observatory. The selected PNe span an age range from approximately 1 million years to 13 billion years, allowing us to investigate the chemical evolution of the SMC over an extensive timescale. Our findings indicate that the abundance patterns of carbon, nitrogen, and oxygen (CNO) elements remain relatively consistent across the sample, suggesting minimal variation in chemical enrichment from asymptotic giant branch (AGB) stars during this period, which aligns with previous studies that reported limited evidence for such enrichment. Notably, we observed variations in the neon-to-oxygen (Ne/O) ratio that correlate with the age of the PNe, implying that the SMC underwent two significant episodes of star formation separated by approximately 10 billion years. Furthermore, our analysis reveals that there was only a single major star formation event within the last 2 billion years. To contextualize our results, we compare our chemical abundance data with findings from H II regions and young open clusters, aiming to explore the broader implications of different evolutionary stages and stellar populations on the chemical evolution of the SMC. This study enhances our understanding of the SMC's chemical history and contributes to the broader discourse on the processes governing star formation and chemical enrichment in dwarf galaxies.",
        "ori-fast-z-score": -2.038098661460272,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 0.29002094671369905
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The generator coordinate method in time-dependent density-functional theory: memory made simple .\nAbstract:\nThe Generator Coordinate Method (GCM) is an efficient approach to calculate the electronic structure and properties of materials with strong electron-phonon coupling, such as polar semiconductors or insulators. In this work we present a new implementation of GCM within Time-Dependent Density Functional Theory (TDDFT), which allows us to study phonon-assisted optical excitations on large systems. The key idea behind our scheme is that it exploits the fact that TDDFT can be formulated as a linear response problem for the Kohn-Sham system, so that the calculation of the ground state wavefunction does not need to be repeated at each step during the self-consistent field iteration. We demonstrate the efficiency of our algorithm by calculating the absorption spectrum of bulk silicon under hydrostatic pressure up to 100 GPa. Our results show good agreement with previous calculations based on supercell approaches.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The generator coordinate method in time - dependent density - functional theory : memory making straightforward . Abstract : The Generator Coordinate Method ( GCM ) is an efficient approach to estimate the electronic configuration and structures of substances with powerful atom - phonon coupling , such as polar semiconductors or insulators .In this project we present a new implementation of GCM within Time - Dependent Density Functional Theory ( TDDFT ) , which allows us to study phonon - aided optical excitations on huge systems . The main idea behind our scheme is that it utilizes the fact that TDDFT can be derived as a linear response problem for the Kohn - Sham system , so that the determination of the ground state wavefunction does not require to be repeated at each step during the self - consistent field iteration .We suggest the performance of our technique by calculating the absorption spectrum of bulk silicon under hydrostatic pressure up to 100 GPa . Our results show good agreement with previous analyses based on supercell methods .",
        "rewrite_text": "Title: The Generator Coordinate Method in Time-Dependent Density-Functional Theory: Simplifying Memory Management\n\nAbstract: The Generator Coordinate Method (GCM) serves as an effective technique for evaluating the electronic configurations and structures of materials characterized by significant atom-phonon interactions, particularly in polar semiconductors and insulators. In this study, we introduce a novel implementation of GCM within the framework of Time-Dependent Density Functional Theory (TDDFT), enabling the exploration of phonon-assisted optical excitations in large-scale systems. The core concept of our approach leverages the derivation of TDDFT as a linear response problem associated with the Kohn-Sham system. This allows for the ground state wavefunction to be determined without the need for repeated calculations at each iteration of the self-consistent field process, thereby streamlining the computational effort involved. To demonstrate the efficacy of our method, we present calculations of the absorption spectrum for bulk silicon subjected to hydrostatic pressures of up to 100 GPa. The results obtained exhibit strong concordance with prior studies conducted using supercell methodologies, validating the robustness and accuracy of our GCM implementation within TDDFT. This advancement not only enhances our understanding of phonon-related phenomena in complex materials but also paves the way for further investigations into the optical properties of a wide range of systems under varying conditions.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 4.129483209670111,
        "rewrite-fast-z-score": -1.1793237883215741
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic fluctuations in n-type high-$T_c$ superconductors reveal breakdown of fermiology .\nAbstract:\nWe report the observation of magnetic fluctuations at low temperatures and high fields in single crystals of YBa2Cu3O6+x (YBCO) with x=0.4, 0.45, and 0.5 using muon spin relaxation measurements. The data show that these materials are characterized by an unusual temperature dependence of the fluctuation rate which is not consistent with predictions based on Fermi liquid theory or any other conventional model for fermionic quasiparticles. We argue that this behavior can be understood within a phenomenological description of the electronic excitations as bosonic collective modes. These results provide strong evidence against the existence of well-defined fermionic quasiparticles in the normal state of these compounds. They also suggest that the pseudogap phase may have some features in common with the superfluid state. \n \n High-temperature cuprate superconductors exhibit many remarkable properties including a rich variety of competing ground states. In particular, it has been suggested that they undergo a quantum phase transition into a novel ordered state known as the  pseudogap  phase  1  . This phase appears to exist between the underdoped regime where there is no static order but only short-range correlations  2  , and the overdoped regime where antiferromagnetism disappears  3  . It is believed that the pseudogap state plays an important role in understanding the mechanism responsible for high-Tc superconductivity  4  .\nIn recent years much attention has focused on the possibility that the pseudogap is associated with preformed pairs of charge carriers  5  . However, despite considerable experimental effort  6  , direct evidence for such pairing remains elusive  7, 8  . One possible explanation for this lack of success is that the pseudogap does not arise directly from pair formation  9  . Instead, it could result from the condensation of another type of collective mode  10  . For example, if the pseudogap were related to the onset of density wave ordering  11  then one would expect to see signatures of its presence in the form of low-energy magnetic fluctuations  12  . Indeed, several experiments have reported the detection of such fluctuations  13  -  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic fluctuations in n - class high - $ T _ c $ superconductors reveal collapse of fermiology . Abstract : We report the observation of magnetic fluctuations at low temperatures and low fields in single crystals of YBa2Cu3O6 + x ( YBCO ) with x = 0 . 4 , 0 . 45 , and 0 . 5 utilizing muon spin relaxation measurements .The data reveal that these objects are marked by an peculiar thermal dependence of the fluctuation speed which is not consistent with predictions based on Fermi solid physics or any other usual description for fermionic quasiparticles . We argue that this behavior can be understood within a phenomenological explanation of the optical excitations as bosonic collective modes .These conclusions provide strong evidence against the existence of well - defined fermionic quasiparticles in the usual state of these compounds . They even propose that the pseudogap phase may have some features in common with the superfluid state .High - temperature cuprate superconductors exhibit several notable properties including a rich multitude of competing ground states . In particular , it has been proposed that they undergo a quantum phase shift into a novel ordered state known as the pseudogap phase 1 .This phase shows to arise between the underdoped regime where there is no static order but only low - range correlations 2 , and the overdoped regime where antiferromagnetism drops 3 . It is suspected that the pseudogap state plays an important role in understanding the process responsible for high - Tc superconductivity 4 .In recent years much attention has concentrated on the suggestion that the pseudogap is associated with preformed pairs of charge carriers 5 . However , despite considerable experimental effort 6 , direct data for such pairing remains elusive 7 , 8 .One potential explanation for this lack of failure is that the pseudogap does not occur immediately from pair formation 9 . Instead , it could occur from the condensation of another type of collective mode 10 .For instance , if the pseudogap were linked to the onset of density wave ordering 11 then one would expect to see signatures of its presence in the form of low - energy magnetic fluctuations 12 . Indeed , various tests have reported the detection of such fluctuations 13 - 16 .",
        "rewrite_text": "**Title:** Magnetic Fluctuations in n-Class High-Tc Superconductors Indicate the Collapse of Fermiology\n\n**Abstract:** In this study, we present findings on magnetic fluctuations observed at low temperatures and low magnetic fields in single crystals of YBa2Cu3O6+x (YBCO) with compositions x = 0.4, 0.45, and 0.5, utilizing muon spin relaxation techniques. Our results indicate that these fluctuations exhibit an unusual thermal dependence in their speed, which deviates from the predictions made by Fermi solid physics and traditional models of fermionic quasiparticles. We propose that this behavior can be interpreted through a phenomenological framework that considers these fluctuations as bosonic collective modes rather than well-defined fermionic entities. This interpretation challenges the conventional understanding of quasiparticles in these materials and suggests that the pseudogap phase may share characteristics with a superfluid state.\n\nHigh-temperature cuprate superconductors are known for their complex properties and a variety of competing ground states. Notably, it has been suggested that these materials undergo a quantum phase transition into a unique ordered state referred to as the pseudogap phase. This phase emerges between the underdoped region, characterized by the absence of static order and the presence of only short-range correlations, and the overdoped region, where antiferromagnetic order diminishes. The pseudogap state is believed to be crucial for unraveling the mechanisms behind high-Tc superconductivity.\n\nRecent research has focused on the hypothesis that the pseudogap is linked to the formation of preformed pairs of charge carriers. However, despite extensive experimental investigations, direct evidence for such pairing remains elusive. One possible explanation for this discrepancy is that the pseudogap does not arise directly from pair formation but may instead result from the condensation of a different type of collective mode. For example, if the pseudogap is associated with the onset of density wave ordering, we would expect to observe corresponding low-energy magnetic fluctuations. Indeed, various studies have reported the detection of such fluctuations, reinforcing the notion that the pseudogap phase may be more complex than previously understood.",
        "ori-fast-z-score": 0.47733437050543803,
        "water-fast-z-score": 7.319127014416716,
        "rewrite-fast-z-score": 1.2543630150106362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Semiclassical scalar propagators in curved backgrounds: formalism and ambiguities .\nAbstract:\nWe present the results of our investigation on semiclassical scalar propagator in curved space-time, which is based on the WKB approximation to the wave function. We show that there are two different ways how one can define this quantity depending on whether or not one takes into account the back-reaction effects due to the quantum fluctuations of the gravitational field. The first approach leads to an expression for the semiclassical propagator which coincides with the Feynman propagator at large distances but differs significantly near the source point. In particular it does not satisfy the Hadamard condition required by general relativity. On the other hand, if we take into account the back reaction then the resulting expression satisfies all necessary conditions including the Hadamard condition. However, as was shown recently by Wald et al., such an expression cannot be obtained within the framework of standard QFT. This problem may have important consequences when considering the propagation of particles through black holes since the corresponding expressions differ substantially even outside the horizon.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Semiclassical scalar propagators in curved landscapes : formalism and ambiguities . Abstract : We present the conclusion of our inquiry on semiclassical scalar propagator in curved space - time , which is based on the WKB approximation to the wave function .We see that there are two different ways how one can define this quantity varying on whether or not one takes into consideration the back - reaction effects due to the quantum fluctuations of the gravitational field . The first method results to an definition for the semiclassical propagator which coincides with the Feynman propagator at large distances but varies dramatically near the origin point .In particular it does not satisfy the Hadamard condition required by general relativity . On the other hand , if we took into consideration the back response then the resulting expression satisfies all necessary circumstances including the Hadamard condition .However , as was shown lately by Wald et al . , such an expression cannot be obtained within the framework of standard QFT . This problem could have important implications when examining the propagation of particles through black holes since the equivalent definitions differ substantially even outside the horizon .",
        "rewrite_text": "In this article, we explore the semiclassical scalar propagator in curved spacetime, utilizing the WKB approximation to analyze the wave function. Our investigation reveals two distinct approaches to defining this propagator, which hinge on whether the back-reaction effects from quantum fluctuations of the gravitational field are included. The first approach yields a semiclassical propagator that aligns with the Feynman propagator at large distances but exhibits significant deviations near the origin. Notably, this definition fails to meet the Hadamard condition, a crucial requirement in the framework of general relativity. Conversely, when we incorporate the back-reaction effects, the resulting expression adheres to all necessary conditions, including the Hadamard condition. However, as highlighted by recent work from Wald et al., this expression cannot be derived within the conventional framework of quantum field theory (QFT). This discrepancy raises important questions regarding the implications for particle propagation in the vicinity of black holes, as the differing definitions of the propagator lead to substantial variations even outside the event horizon. Our findings underscore the complexities and ambiguities inherent in the semiclassical treatment of scalar fields in curved landscapes, suggesting that a deeper understanding of the interplay between quantum mechanics and general relativity is essential for accurately describing phenomena in extreme gravitational environments. This work contributes to the ongoing discourse on the foundations of quantum gravity and the behavior of quantum fields in curved spacetime, highlighting the need for further investigation into the implications of these ambiguities for theoretical and observational astrophysics.",
        "ori-fast-z-score": 0.10846522890932808,
        "water-fast-z-score": 5.038928913737635,
        "rewrite-fast-z-score": 3.2637668288410984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diffuse Gamma-Rays Produced in Cosmic-Ray Interactions and the TeV-band Spectrum of RX J1713.7-3946 .\nAbstract:\nWe present new results on diffuse gamma-ray emission produced by cosmic rays interacting with interstellar gas, based on data collected during the first year of operation of the Large Area Telescope (LAT) aboard Fermi satellite. We find that this component is well described by a power law spectrum with index ~2.3 extending up to 100 GeV. The total flux above 1 GeV amounts to about 10% of the observed Galactic diffuse emission at these energies. This result confirms previous estimates obtained using EGRET data. In addition we report an upper limit for the flux of unresolved point sources below 10 GeV which is consistent with predictions made within the framework of standard models of cosmic ray origin and propagation. Finally, we discuss implications of our findings for the interpretation of observations performed towards the supernova remnant RX J1713.7--3946. PACS numbers: 98.70.Sa, 95.55.Ym",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diffuse Gamma - Rays Produced in Cosmic - Ray Interactions and the TeV - band Spectrum of RX J1713 . 7 - 3946 . Abstract : We report new data on diffuse γ - ray radiation generated by cosmic rays interacting with interstellar gas , based on evidence generated during the first year of operation of the Large Area Telescope ( LAT ) aboard Fermi satellite .We see that this component is well described by a power law spectrum with index ~ 2 . 3 stretching up to 100 GeV . The total flux above 1 GeV corresponds to about 10 % of the seen Galactic diffuse emission at these frequencies .This result confirms previous estimates obtained using EGRET data . In addition we publish an upper maximum for the flux of unresolved point sources below 10 GeV which is compatible with predictions making within the framework of standard models of cosmic ray origin and propagation .Finally , we explain significance of our findings for the interpretation of measurements completed towards the supernova remnant RX J1713 . 7 - - 3946 . PACS numbers : 98 . 70 . Sa , 95 . 55 . Ym",
        "rewrite_text": "Title: Diffuse Gamma-Rays Produced in Cosmic-Ray Interactions and the TeV-band Spectrum of RX J1713.7-3946\n\nAbstract: In this study, we present new findings regarding diffuse gamma-ray radiation resulting from cosmic ray interactions with interstellar gas, derived from data collected during the inaugural year of the Large Area Telescope (LAT) on the Fermi satellite. Our analysis reveals that this diffuse gamma-ray component can be accurately characterized by a power law spectrum with an index of approximately 2.3, extending up to energies of 100 GeV. Notably, the total flux observed above 1 GeV accounts for roughly 10% of the Galactic diffuse emission detected at these energy levels. This observation corroborates earlier estimates made using data from the Energetic Gamma Ray Experiment Telescope (EGRET). Furthermore, we provide an upper limit for the flux of unresolved point sources below 10 GeV, which aligns with predictions derived from standard models concerning the origin and propagation of cosmic rays. The implications of our findings are significant for interpreting the measurements associated with the supernova remnant RX J1713.7-3946, enhancing our understanding of the interplay between cosmic rays and their surrounding environment. The results contribute to the broader context of gamma-ray astronomy and the ongoing investigation into the mechanisms driving cosmic ray acceleration and diffusion in the Galactic medium. Our work emphasizes the importance of high-energy astrophysics in unraveling the complexities of cosmic ray interactions and their observable consequences in the universe. PACS numbers: 98.70.Sa, 95.55.Ym.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": 0.6897304947150052
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Velocity-Dependent Models for Non-Abelian/Entangled String Networks .\nAbstract:\nWe present new models for non-abelian string networks, which are based on the idea that the network is composed by many entangled strings with different velocities and orientations. We show how to construct such velocity-dependent models in terms of Feynman diagrams. In particular we consider two types of diagrams: (i) ladder-like diagrams corresponding to the exchange of gluons between pairs of strings, (ii) cross-ladder like-diagrams describing interactions among three or more strings. The latter type of diagrams can be interpreted as representing junctions where several strings meet at one point. We discuss some properties of these models and compare them with previous results obtained within the framework of Abelian-Higgs model. Finally, we study numerically the evolution of an initial configuration consisting of a single straight string into a complicated tangle of interacting strings using Monte Carlo simulations. This work was supported by the DFG under contract SFB-TR9  Gravitational Physics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Velocity - Dependent Models for Non - Abelian / Entangled String Networks . Abstract : We create fresh models for non - abelian string systems , which are based on the idea that the network is composed by many entangled strings with various velocities and orientations .We see how to build such velocity - dependent models in terms of Feynman diagrams . In particular we define two forms of diagrams : ( i ) ladder - like diagrams corresponding to the transfer of gluons between pairs of strings , ( ii ) cross - ladder like - diagrams describing relationships among three or more strings .The latter type of diagrams can be interpreted as representing junctions where many strings join at one point . We discuss some properties of these models and link them with previous findings obtained within the framework of Abelian - Higgs system .Finally , we study numerically the evolution of an initial structure comprised of a single straight string into a detailed tangle of interacting strings using Monte Carlo simulations . This research was supported by the DFG under contract SFB - TR9 Gravitational Physics",
        "rewrite_text": "In this article, we present innovative models for non-Abelian string systems, focusing on the intricate dynamics of networks formed by multiple entangled strings exhibiting diverse velocities and orientations. Our approach leverages the framework of Feynman diagrams to construct these velocity-dependent models. We introduce two distinct types of diagrams: the first type consists of ladder-like diagrams that illustrate the transfer of gluons between pairs of strings, while the second type encompasses cross-ladder diagrams that capture the interactions among three or more strings. These cross-ladder diagrams can be interpreted as junctions where multiple strings converge at a single point, highlighting the complex interconnections within the network.\n\nWe delve into the properties of these models and draw connections to prior research conducted within the context of the Abelian-Higgs system, thereby situating our findings within the broader landscape of string theory. Furthermore, we conduct numerical simulations to investigate the evolution of an initial configuration characterized by a single straight string, observing its transformation into a complex tangle of interacting strings. This evolution is analyzed through Monte Carlo simulations, allowing us to explore the dynamic behavior of the string network in detail.\n\nOur research contributes to the understanding of non-Abelian string dynamics and is supported by the Deutsche Forschungsgemeinschaft (DFG) under contract SFB-TR9 Gravitational Physics. Through this work, we aim to enhance the theoretical framework surrounding string networks and their interactions, paving the way for future investigations in the field.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": 1.5255401427929478
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High-energy Cosmic Rays and Neutrinos from Semi-relativistic Hypernovae .\nAbstract:\nWe propose that the most energetic cosmic rays are accelerated in supernova remnants by relativistic jets powered by hypernova explosions, which may be associated with gamma-ray bursts (GRBs). We show how this model can explain several observed features of GRBs: their duration distribution; their association with massive star formation regions; their high luminosities; and their large redshifts. The proposed mechanism is also able to accelerate protons up to energies beyond 10^20 eV without violating current observational constraints on the diffuse fluxes of high-energy neutrinos or photons produced during the acceleration process. This scenario provides an explanation for the origin of ultra-high energy cosmic rays as well as for the production of the highest energy neutrinos detected so far. In addition, it offers a natural explanation for the recent detection of very bright optical flashes following some GRBs. \n \n High-energy cosmic rays have been measured at Earth over many decades  1  . Their spectrum extends up to energies above 1020 eV  2  , but no astrophysical source has yet been identified that accelerates particles to such extreme energies  3  . It seems likely that these cosmic rays were accelerated in distant sources billions of years ago  4  .\n \nThe most powerful known explosion in our Universe occurs when a massive star collapses into a black hole after exhausting its nuclear fuel supply  5  . Such events release huge amounts of gravitational binding energy  6  , which powers relativistic outflows called  jets ; they are believed to produce gamma-ray bursts  7, 8  . These jets could provide the necessary power to accelerate cosmic rays to extremely high energies  9  . \n \n However, there are two major difficulties in explaining the origin of the most energetic cosmic ray particles using conventional models  10  : \n \n 1) Conventional jet-powered models cannot accelerate protons to energies greater than ~10^19 eV  11  because the maximum Lorentz factor Γmax of the flow decreases rapidly with distance r from the central engine  12  . As a result, the total kinetic energy available to accelerate particles drops dramatically with increasing particle energy E  13  . For example, if we assume that the bulk Lorentz factor of the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High - energy Cosmic Rays and Neutrinos from Semi - relativistic Hypernovae . Abstract : We suggest that the most intense cosmic rays are accelerated in supernova remnants by relativistic jets driven by hypernova bursts , which sometimes be identified with gamma - ray clusters ( GRBs ) .We see how this model can describe several observed features of GRBs : their duration distribution ; their association with massive star formation regions ; their high luminosities ; and their large redshifts . The proposed process is also could to accelerate protons up to energies beyond 10 ^ 20 eV without violating present observational restrictions on the diffuse fluxes of high - energy neutrinos or photons created during the acceleration cycle .This scenario offers an excuse for the origin of ultra - large energy cosmic rays as well as for the production of the highest power neutrinos detected so far . In addition , it gives a natural explanation for the recent discovery of very bright optical bursts following some GRBs .High - energy cosmic rays have been measured at Earth over much centuries 1 . Their spectrum stretches up to energies above 1020 eV 2 , but no astrophysical source has already been determined that accelerates particles to such extreme energies 3 .It seems likely that these cosmic rays were accelerated in nearby sources billions of years past 4 . The most intense reported blast in our Universe comes when a huge star collapses into a black hole after exhausting its nuclear fuel supply 5 .Such events release massive amounts of gravitational binding energy 6 , which powers relativistic outflows called jets ; they are said to produce gamma - ray waves 7 , 8 . These jets could give the necessary power to accelerate cosmic rays to incredibly high energies 9 .However , there are two major obstacles in describe the origin of the most intense cosmic ray ions using conventional versions 10 : 1 ) Conventional jet - powered designs cannot accelerate protons to energies higher than ~ 10 ^ 19 eV 11 because the maximum Lorentz factor Γmax of the flow drops rapidly with distance r from the main engine 12 . As a result , the total kinetic power available to accelerate particles decreases dramatically with rising particle power E 13 .For instance , if we suppose that the bulk Lorentz factor of the",
        "rewrite_text": "**Title: High-Energy Cosmic Rays and Neutrinos from Semi-Relativistic Hypernovae**\n\n**Abstract:** In this study, we propose a novel mechanism for the acceleration of the most energetic cosmic rays, suggesting that they originate from supernova remnants influenced by relativistic jets generated by hypernova explosions, which are often associated with gamma-ray bursts (GRBs). Our model effectively accounts for several key characteristics of GRBs, including their duration distribution, their correlation with regions of massive star formation, their significant luminosities, and their substantial redshifts. We demonstrate that this acceleration process can propel protons to energies exceeding 10^20 eV while remaining consistent with current observational limits on the diffuse fluxes of high-energy neutrinos and photons produced during the acceleration events. This framework not only provides insights into the origins of ultra-high-energy cosmic rays but also elucidates the generation of the most powerful neutrinos observed to date. Furthermore, it offers a coherent explanation for the recent observations of exceptionally bright optical transients that follow certain GRBs.\n\nHigh-energy cosmic rays have been detected on Earth for centuries, with their spectrum extending to energies above 10^20 eV. However, a definitive astrophysical source capable of accelerating particles to such extreme levels has yet to be identified. It is plausible that these cosmic rays were accelerated in nearby astrophysical events billions of years ago. The most intense explosions in the universe occur when massive stars collapse into black holes after depleting their nuclear fuel, releasing vast amounts of gravitational binding energy. This energy drives relativistic outflows, or jets, which are believed to produce gamma-ray emissions. These jets may provide the necessary energy to accelerate cosmic rays to extraordinary levels.\n\nNonetheless, conventional models face significant challenges in explaining the origins of the most intense cosmic ray ions. Specifically, traditional jet-powered mechanisms struggle to accelerate protons beyond approximately 10^19 eV due to the rapid decline of the maximum Lorentz factor (Γmax) of the flow with increasing distance from the central engine. Consequently, the total kinetic power available for particle acceleration diminishes significantly as particle energy increases. Our research addresses these limitations and proposes a more robust framework for understanding the acceleration of high-energy cosmic rays and neutrinos in the context of hypernovae and their associated jets.",
        "ori-fast-z-score": -0.29649972666444047,
        "water-fast-z-score": 7.666981720054651,
        "rewrite-fast-z-score": 2.3877961974649913
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray detection of the substellar twin 2MASS J11011926-7732383 AB .\nAbstract:\nWe report on the X-ray properties of the young, nearby (d = 11 pc), low-mass binary system 2MASS J1101-2677AB discovered by Burgasser et al. (2007) . The primary component is an M8 dwarf with T eff ~ 2600 K and log g ~ 5.0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log g ~ 4.2. We observed this target for about 50 ks using Chandra ACIS-S in order to search for evidence of coronal activity associated with either or both components. No significant emission was detected at the position of the source down to a 3-sigma upper limit of 1 x 10^28 erg s-1 cm-2 . This non-detection suggests that if there are active regions present they must be small and/or cool compared to those found on more massive stars. In addition we find no evidence of flaring behavior during our observation which constrains any possible magnetic field strength to less than 100 G.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray observations of the substellar twin 2MASS J11011926 - 7732383 AB . Abstract : We report on the X - ray characteristics of the young , neighboring ( d = 11 pc ) , low - weight binary system 2MASS J1101 - 2677AB discovered by Burgasser et al .( 2007 ) . The main component is an M8 dwarf with T eff ~ 2600 K and log f ~ 5 . 0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log f ~ 4 . 2 .We observed this target for about 50 ks using Chandra ACIS - S in order to search for indication of coronal interaction associated with either or both components . No noticeable emitted was seen at the position of the source down to a 3 - sigma upper maximum of 1 x 10 ^ 28 erg s - 1 cm - 2 .This non - observation suggests that if there are active regions present they must be small and / or cold relative to those observed on more massive stars . In addition we find no evidence of flaring behavior during our observation which constrains any likely magnetic force weakness to fewer than 100 G .",
        "rewrite_text": "We present an in-depth analysis of the X-ray properties of the young, nearby binary system 2MASS J11011926-7732383 AB, located at a distance of 11 parsecs. This low-mass binary, first identified by Burgasser et al. in 2007, consists of two components: the primary is an M8 dwarf with an effective temperature (T_eff) of approximately 2600 K and a logarithmic flux (log f) of around 5.0, while the secondary is classified as an L5 brown dwarf with a T_eff of about 1400 K and log f of approximately 4.2. To investigate potential coronal activity and interactions between the components, we conducted X-ray observations using the Chandra ACIS-S instrument, accumulating data over a period of roughly 50 kiloseconds. Our findings revealed no significant X-ray emission from the system, with a 3-sigma upper limit set at 1 x 10^28 erg s^-1 cm^-2. This lack of detectable X-ray activity implies that if active regions exist on either component, they are likely to be small and/or cooler than those typically observed on more massive stars. Furthermore, our observations did not capture any flaring events, which allows us to place constraints on the magnetic field strength, suggesting it is weaker than 100 G. These results contribute to our understanding of the magnetic and coronal properties of low-mass stars and brown dwarfs, highlighting the differences in activity levels compared to their more massive counterparts.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": -0.20203050891044214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Proving Regularity of the Minimal Probability of Ruin via a Game of Stopping and Control .\nAbstract:\nWe consider an insurance company that controls its exposure to risk by dynamically adjusting its premiums, reserves and investments in financial markets. We assume that the insurer s surplus process is given by a diffusion with jumps driven by Brownian motion and Poisson random measure. The objective of this work is to study how the insurer can control ruin probability using dynamic investment strategies under proportional transaction costs on both assets and liabilities. In particular we prove regularity properties for the minimal probability of ruin as well as optimality results for some specific investment policies. Our main contributions are summarized below:  1) We show that the minimal probability of ruin has at most countably many discontinuities (Proposition 3). 2) We provide sufficient conditions ensuring continuity of the minimal probability of ruin (Theorem 4). 3) We establish necessary and sufficient conditions for the existence of a unique minimizer for the minimal probability of ruin: if there exists one then it coincides with the value function associated with the problem of maximizing expected discounted utility over all admissible investment strategies (Theorems 5 and 6).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Proving Regularity of the Minimal Probability of Ruin via a Game of Stopping and Control . Abstract : We consider an insurance company that monitors its exposure to risk by dynamically adjusting its premiums , deposits and assets in financial markets .We assume that the insurer s surplus cycle is given by a absorption with jumps driven by Brownian movement and Poisson random measure . The goal of this research is to study how the insurer can manage collapse probability utilizing dynamic financial strategies under proportional transaction losses on both assets and liabilities .In particular we prove regularity properties for the minimal probability of ruin as well as optimality results for some specific investment policies . Our main contributions are presented below : 1 ) We see that the minimal probability of ruin has at most countably several discontinuities ( Proposition 3 ) .2 ) We establish enough rules ensuring continuity of the minimal probability of ruin ( Theorem 4 ) . 3 ) We establish appropriate and sufficient requirements for the existence of a unique minimizer for the minimal probability of ruin : if there exists one then it coincides with the value function associated with the question of maximizing expected discounted utility over all admissible income opportunities ( Theorems 5 and 6 ) .",
        "rewrite_text": "Title: Proving Regularity of the Minimal Probability of Ruin via a Game of Stopping and Control\n\nAbstract: This study investigates the strategies employed by an insurance company to manage its risk exposure through the dynamic adjustment of premiums, deposits, and investments in financial markets. We model the insurer's surplus process as an absorption process characterized by jumps, which are influenced by Brownian motion and a Poisson random measure. The primary objective of this research is to analyze how the insurer can effectively minimize the probability of financial ruin by implementing dynamic financial strategies, particularly in the context of proportional transaction costs affecting both assets and liabilities. \n\nOur findings reveal several key contributions to the understanding of the minimal probability of ruin. Firstly, we demonstrate that this probability exhibits at most countably many discontinuities, as established in Proposition 3. Secondly, we provide sufficient conditions that ensure the continuity of the minimal probability of ruin, detailed in Theorem 4. Furthermore, we identify necessary and sufficient conditions for the existence of a unique minimizer of the minimal probability of ruin. Notably, if such a minimizer exists, it aligns with the value function derived from maximizing expected discounted utility across all admissible income opportunities, as shown in Theorems 5 and 6. \n\nThese results not only enhance the theoretical framework surrounding risk management in insurance but also offer practical insights into optimal investment policies that can mitigate the risk of ruin. By leveraging dynamic strategies and understanding the underlying mathematical properties of the insurer's surplus process, we contribute to the broader discourse on financial stability and risk assessment in the insurance sector.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 5.116817192534651,
        "rewrite-fast-z-score": 2.37346441585572
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A three-state prediction of single point mutations on protein stability changes .\nAbstract:\nMutations in proteins can lead to diseases, such as cancer and cystic fibrosis.  In this study we present a new method for predicting the effect that single-point mutations have on the thermodynamic stability (ΔG) of proteins using only sequence information. The proposed approach is based on an ensemble learning technique called Random Forests which combines multiple decision trees into one model. We show how our method outperforms state-of-the-art methods by achieving higher accuracy with lower computational cost. Our results suggest that the proposed method could be used to predict ΔG values for thousands of mutants at once. Mutations in proteins are known to cause many human diseases including cancer1 and cystic fibross2. Therefore it is important to understand what effects these mutations have on the structure and function of proteins3-6. One way to investigate the impact of mutations is through their influence on the thermodynamic stability7-9 of proteins10-12. Thermodynamic stability refers to the free energy difference between folded and unfolded states13-15. A negative value indicates that the folded state has more favorable interactions than the unfolded state16-18 while positive values indicate unfavorable interactions19-21. It was shown22-24 that the change in Gibbs free energy upon folding (ΔGf) correlates well with experimental measurements25-27. Thus, accurate predictions of ΔGf may help us better understand the relationship between mutations and disease28-30.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A three - state prediction of single position mutations on protein stability alterations . Abstract : Mutations in proteins can lead to diseases , such as tumor and cystic fibrosis .In this study we present a new method for predicting the impact that single - point mutations have on the thermodynamic stability ( ΔG ) of proteins using only sequence data . The proposed approach is based on an ensemble learning methodology called Random Forests which mixes several decision forests into one model .We see how our technique outperforms state - of - the - art methods by achieving larger precision with lower mathematical price . Our results propose that the suggested method could be used to predict ΔG expressions for thousands of mutants at once .Mutations in proteins are known to produce many human diseases including cancer1 and cystic fibross2 . Therefore it is important to explain what effects these mutations have on the structure and activity of proteins3 - 6 .One way to examine the impact of mutations is through their influence on the thermodynamic stability7 - 9 of proteins10 - 12 . Thermodynamic stability refers to the free energy relationship between folded and unfolded states13 - 15 .A negative value indicates that the folded state has more favorable interactions than the unfolded state16 - 18 while positive values indicate unfavorable interactions19 - 21 . It was shown22 - 24 that the shift in Gibbs free energy upon folding ( ΔGf ) correlates well with experimental measurements25 - 27 .Thus , accurate predictions of ΔGf may assist us better understand the relationship between variants and disease28 - 30 .",
        "rewrite_text": "**Title:** A Three-State Prediction of Single Position Mutations on Protein Stability Alterations\n\n**Abstract:** Protein mutations are implicated in various diseases, including cancer and cystic fibrosis, making it crucial to understand their effects on protein structure and function. This study introduces a novel method for predicting the impact of single-point mutations on the thermodynamic stability (ΔG) of proteins, relying solely on sequence data. Our approach utilizes an ensemble learning technique known as Random Forests, which integrates multiple decision trees into a cohesive model. The results demonstrate that our method significantly outperforms existing state-of-the-art techniques, achieving higher precision while maintaining a lower computational cost. This advancement suggests that our model could efficiently predict ΔG values for thousands of mutations simultaneously.\n\nThe thermodynamic stability of proteins is a critical factor in understanding the consequences of mutations. It is defined by the free energy difference between the folded and unfolded states of a protein. A negative ΔG value indicates that the folded state is thermodynamically favored due to more favorable interactions, whereas a positive ΔG suggests unfavorable interactions that favor the unfolded state. Previous research has established a strong correlation between shifts in Gibbs free energy during folding (ΔGf) and experimental measurements, highlighting the importance of accurate ΔGf predictions in elucidating the relationship between genetic variants and disease phenotypes.\n\nBy providing a reliable method for predicting ΔG alterations due to single-point mutations, our study contributes to the broader understanding of how these mutations can lead to pathological conditions. The ability to predict the thermodynamic stability of numerous protein variants simultaneously opens new avenues for research in protein engineering, drug design, and personalized medicine, ultimately aiding in the development of targeted therapies for mutation-related diseases.",
        "ori-fast-z-score": 0.8392543274162825,
        "water-fast-z-score": 5.756555483488608,
        "rewrite-fast-z-score": 0.24743582965269675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Physical Nature of Polar Broad Absorption Line Quasars .\nAbstract:\nWe present the results of an investigation into the physical nature of polar broad absorption line quasars (BALQSOs). We find that BALQSOs are more likely to be radio-loud than non-BAL QSOs, and that they have higher luminosities at rest-frame ultraviolet wavelengths. The fraction of BALQSOs in our sample is consistent with previous studies; however we do not detect any significant difference between the fractions of BALQSOs found among radio-quiet and radio-loud quasars. In addition, we find no evidence for differences in the properties of BALs observed along different lines-of-sight within individual objects. Finally, we show that there exists a correlation between the strength of the CIV emission-line blueshift and the equivalent width of the associated BAL trough. These findings suggest that BALQSOs may represent a special subclass of radio-loud quasars which are characterized by high accretion rates onto supermassive black holes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Physical Nature of Polar Broad Absorption Line Quasars . Abstract : We present the conclusion of an investigation into the physical nature of polar wide absorption line quasars ( BALQSOs ) .We see that BALQSOs are more likely to be radio - loud than non - BAL QSOs , and that they have greater luminosities at rest - frame ultraviolet wavelengths . The percentage of BALQSOs in our sample is compatible with previous research ; however we do not detect any large change between the fractions of BALQSOs discovered among radio - quiet and radio - loud quasars .In addition , we find no evidence for variations in the properties of BALs observed along various lines - of - seeing within particular objects . Finally , we prove that there exists a correlation between the strength of the CIV radiation - line blueshift and the equivalent thickness of the associated BAL trough .These studies imply that BALQSOs might represent a unique subclass of radio - loud quasars which are marked by high accretion levels onto supermassive black holes .",
        "rewrite_text": "We present the findings of our research on the physical characteristics of polar broad absorption line quasars (BALQSOs). Our investigation reveals that BALQSOs exhibit a higher likelihood of being classified as radio-loud compared to their non-BAL counterparts. Furthermore, we observe that these quasars possess greater luminosities in the rest-frame ultraviolet spectrum. The proportion of BALQSOs identified in our study aligns with previous findings; however, we do not observe significant differences in the prevalence of BALQSOs between radio-quiet and radio-loud quasars. Additionally, our analysis shows no substantial variations in the properties of broad absorption lines (BALs) when examined from different lines of sight within individual quasars. A key outcome of our research is the identification of a correlation between the blueshift of the CIV emission line and the equivalent width of the corresponding BAL trough. These results suggest that BALQSOs may constitute a distinct subclass of radio-loud quasars characterized by elevated accretion rates onto supermassive black holes. This study enhances our understanding of the physical nature of BALQSOs and their role within the broader context of quasar research.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic structure of Sm2IrIn8 .\nAbstract:\nWe have performed neutron powder diffraction experiments on the intermetallic compound Sm2IrIn8 in order to determine its magnetic structure and compare it with that proposed for YbMgGaO4, another member of this family of compounds. The results show that Sm2IrIn8 has an antiferromagnetic ordering at TN = 3.5 K with moments aligned along the c-axis. This is similar to what was found previously for YbMgGaO4 but different than the theoretical prediction based on band-structure calculations which suggested that the ordered moment should be perpendicular to the c-axis.  We also find evidence for a structural phase transition near T* ~ 80 K where there are changes in both the lattice parameters as well as the unit cell volume. These results suggest that the low temperature crystal structure may not correspond exactly to the high-temperature tetragonal symmetry predicted by theory. Finally we present specific heat data showing clear anomalies associated with both the magnetic ordering and the structural phase transition.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic structure of Sm2IrIn8 . Abstract : We have done neutron dust diffraction experiments on the intermetallic complex Sm2IrIn8 in order to identify its magnetic shape and compare it with that suggested for YbMgGaO4 , another member of this class of compounds .The results show that Sm2IrIn8 has an antiferromagnetic ordering at TN = 3 . 5 K with moments aligned along the c - axis . This is related to what was seen previously for YbMgGaO4 but different than the theoretical forecast based on band - structure analysis which suggested that the ordered moment should be perpendicular to the c - axis .We additionally find proof for a structural phase shift near T * ~ 80 K where there are variations in both the lattice parameters as well as the unit cell size . These conclusions show that the high heat crystal composition may not correspond exactly to the high - temperature tetragonal symmetry anticipated by theory .Finally we present specific heat statistics suggesting obvious anomalies associated with both the magnetic forcing and the structural phase shift .",
        "rewrite_text": "Title: Magnetic Structure of Sm2IrIn8\n\nAbstract: In this study, we conducted neutron dust diffraction experiments on the intermetallic compound Sm2IrIn8 to elucidate its magnetic structure and to draw comparisons with YbMgGaO4, another compound within the same family. Our findings reveal that Sm2IrIn8 exhibits antiferromagnetic ordering at a Neel temperature (TN) of 3.5 K, with magnetic moments oriented along the c-axis. This behavior aligns with the previously observed characteristics of YbMgGaO4; however, it diverges from theoretical predictions derived from band-structure calculations, which anticipated that the ordered magnetic moment would be oriented perpendicular to the c-axis. Furthermore, we observed evidence of a structural phase transition occurring around T* ~ 80 K, characterized by changes in both the lattice parameters and the unit cell volume. These observations suggest that the high-temperature crystal structure may not conform precisely to the tetragonal symmetry that theoretical models have proposed. Additionally, we present specific heat measurements that indicate significant anomalies linked to both the magnetic ordering and the structural phase transition. These results contribute to a deeper understanding of the magnetic properties and structural behavior of Sm2IrIn8, highlighting the complexities that arise in intermetallic compounds and their magnetic interactions.",
        "ori-fast-z-score": -1.3093073414159544,
        "water-fast-z-score": 5.607304206578798,
        "rewrite-fast-z-score": 1.744163198544762
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hot QCD equations of state and relativistic heavy ion collisions .\nAbstract:\nWe present the results for the equation of state (EoS) in hot Quantum Chromodynamics (QCD). We use two different approaches to solve numerically the lattice QCD EoS at finite temperature, namely the Taylor expansion method and the integral method. The latter is based on an exact representation of the pressure as a function of energy density using Padé approximants. In addition we also study the dependence of the EoS on the number of flavors Nf . Finally, we compare our numerical results with those obtained by other authors within various theoretical frameworks. Our main conclusions are that both methods give consistent results which agree well with previous calculations performed in the literature. Moreover, it turns out that the inclusion of strange quarks has only minor effects on the thermodynamic quantities considered here. \n \n Keywords: Equation of State; Heavy Ion Collisions; Lattice QCD; Relativistic Hydrodynamics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hot QCD equations of state and relativistic heavy ion collisions . Abstract : We present the results for the equation of state ( EoS ) in hotter Quantum Chromodynamics ( QCD ) .We use two different methods to solve numerically the crystal QCD EoS at finite temperature , namely the Taylor expansion method and the integral method . The last is based on an precise representation of the pressure as a function of power distribution using Padé approximants .In addition we also study the dependence of the EoS on the quantity of flavors Nf . Finally , we compare our numerical findings with those achieved by other researchers within various theoretical frameworks .Our main results are that both approaches give consistent conclusions which agree well with previous analyses performed in the books . Moreover , it turns out that the introduction of odd quarks has only minor impacts on the thermodynamic quantities considered here .Keywords: Equation of State; Heavy Ion Collisions; Lattice QCD; Relativistic Hydrodynamics",
        "rewrite_text": "We present a comprehensive analysis of the equation of state (EoS) in the context of high-temperature Quantum Chromodynamics (QCD). Utilizing two distinct numerical methods, we investigate the crystal QCD EoS at finite temperatures: the Taylor expansion method and the integral method. The integral method employs a precise representation of pressure through power distribution, utilizing Padé approximants to enhance accuracy. Our study also examines how the EoS varies with the number of flavors (Nf), providing insights into the role of quark flavors in thermodynamic behavior. \n\nIn our findings, we observe that both numerical approaches yield consistent results, aligning closely with previous theoretical analyses documented in the literature. Notably, our results indicate that the inclusion of odd quarks has a negligible effect on the thermodynamic quantities we analyzed. This suggests that while the presence of different quark flavors is essential for a complete understanding of the QCD EoS, their impact may not be as significant as previously thought. \n\nOur work contributes to the ongoing discourse surrounding heavy ion collisions and the behavior of matter under extreme conditions, offering valuable insights for researchers in the field. The implications of our results extend to various theoretical frameworks, enhancing the understanding of the QCD phase diagram and the dynamics of relativistic heavy ion collisions. \n\nKeywords: Equation of State; Heavy Ion Collisions; Lattice QCD; Relativistic Hydrodynamics.",
        "ori-fast-z-score": 0.23904572186687872,
        "water-fast-z-score": 5.019960159204453,
        "rewrite-fast-z-score": 1.212183053462653
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Random spatial growth with paralyzing obstacles .\nAbstract:\nWe study the random spatial growth in two dimensions, where new sites are added to an initially empty square lattice at randomly chosen locations and grow into circular clusters if they do not hit any existing cluster or obstacle site.  We show that this process leads to fractal structures which can be characterized by their fractal dimension Df = 1 + (1 - p)/2p, where p is the probability for adding a new site without hitting an obstacle. The results agree well with numerical simulations. \nPACS numbers: 05.40.+j, 64.60.Cn, 68.35.-k \nI. INTRODUCTORY REMARK\nIn recent years there has been considerable interest in studying various aspects of the so-called Eden model  1  . In its original formulation it describes the growth of a single cluster on a two-dimensional substrate starting from one seed particle. This simple concept was later generalized to include several seeds  2  , as well as different shapes  3  .\nThe present work deals with another generalization of the Eden model: Instead of growing only one cluster we consider the simultaneous growth of many clusters competing for space  4  . As a result, some clusters may become trapped between other clusters leading to a complex pattern formation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Random spatial growth with paralyzing difficulties . Abstract : We research the random spatial growth in two dimensions , where new sites are adding to an initially vacant square lattice at randomly picked destinations and grow into circular groups if they do not hit any established cluster or obstacle site .We see that this process results to fractal structures which can be described by their fractal dimension Df = 1 + ( 1 - p ) / 2p , where p is the probability for adding a new site without reaching an barrier . The results agree well with numerical simulations .PACS codes : 05 . 40 . + j , 64 . 60 . Cn , 68 . 35 . - k I . INTRODUCTORY REMARK In recent years there has been substantial interest in investigating different components of the so - called Eden model 1 .In its initial formulation it describes the development of a single cluster on a two - dimensional substrate beginning from one seed particle . This straightforward idea was later extended to include several seeds 2 , as well as varying shapes 3 .The present work deals with another generalization of the Eden model : Instead of growing only one cluster we treat the concurrent development of several clusters battling for space 4 . As a result , some clusters might form trapped between other clusters leading to a complex pattern formation .",
        "rewrite_text": "Title: Random Spatial Growth with Paralyzing Difficulties\n\nAbstract: This study investigates the phenomenon of random spatial growth within a two-dimensional framework, where new sites are introduced to an initially empty square lattice at randomly selected locations. These new sites expand into circular clusters, provided they do not encounter any pre-existing clusters or obstacles. Our findings reveal that this growth process leads to the emergence of fractal structures, characterized by a fractal dimension given by the formula Df = 1 + (1 - p) / 2p, where p represents the probability of successfully adding a new site without encountering a barrier. The theoretical predictions align closely with results obtained from numerical simulations, reinforcing the validity of our model. \n\nIn recent years, there has been a growing interest in exploring various aspects of the Eden model, which originally describes the growth of a single cluster on a two-dimensional substrate initiated by a single seed particle. This foundational concept has been expanded to accommodate multiple seed particles and diverse growth shapes. The current research presents a further extension of the Eden model by examining the simultaneous growth of multiple clusters competing for limited space. This interaction can lead to complex patterns, as some clusters may become trapped between others, resulting in intricate formations. Our work contributes to the understanding of spatial growth dynamics and the underlying mechanisms that govern cluster interactions in random environments. The implications of these findings extend to various fields, including material science, biology, and ecological modeling, where similar growth processes are observed. \n\nPACS codes: 05.40.+j, 64.60.Cn, 68.35.-k",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 5.651175082804793,
        "rewrite-fast-z-score": 1.4419211804559506
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Statistical Mechanics of Nonlinear On-line Learning for Ensemble Teachers .\nAbstract:\nWe study the statistical mechanics of nonlinear on-line learning in ensemble teachers, where each teacher is trained by an independent student population and all students are drawn from the same distribution. We show that this model can be mapped to a spin glass system with random interactions between spins on different layers. Using replica theory we derive analytical expressions for the free energy density as well as the order parameters characterizing the equilibrium state of the system. In particular, we find that there exists a phase transition at which the number of active teachers changes discontinuously. The critical temperature depends only weakly on the size of the student populations but strongly on their overlap. This suggests that it may be possible to control the quality of teaching by tuning the overlap between student populations. Finally, we discuss how our results relate to previous work on self-organized criticality in neural networks. PACS numbers: 05.45.Mt, 89.70.+c",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Statistical Mechanics of Nonlinear On - line Learning for Ensemble Teachers . Abstract : We research the statistical mechanics of nonlinear on - line learning in ensemble educators , where each teacher is trained by an independent teacher population and all students are drawn from the same distribution .We see that this model can be mapped to a spinning glass structure with random interactions between spins on various sheets . Using replica theory we derive analytical expressions for the free energy density as well as the order variables characterizing the equilibrium state of the system .In particular , we find that there exists a phase shift at which the quantity of active teachers shifts discontinuously . The essential temperature relies only weakly on the size of the student populations but heavily on their overlap .This implies that it could be possible to affect the performance of teaching by tuning the overlap between student populations . Finally , we explain how our findings connect to previous research on self - organized criticality in neural systems .PACS codes : 05 . 45 . Mt , 89 . 70 . + c",
        "rewrite_text": "In this study, we investigate the statistical mechanics underlying nonlinear online learning within ensemble educators, where each educator is trained by a distinct population of independent teachers, while all students are sampled from a common distribution. Our model reveals a correspondence to a spin glass framework characterized by random interactions among spins across multiple layers. By employing replica theory, we derive analytical expressions for both the free energy density and the order parameters that define the system's equilibrium state. Notably, we identify a phase transition marked by a sudden change in the number of active teachers. The critical temperature associated with this transition exhibits a weak dependence on the size of the student populations, yet it shows a strong correlation with the degree of overlap among these populations. This finding suggests that the effectiveness of teaching can potentially be enhanced by adjusting the overlap between different student groups. Furthermore, we discuss the implications of our results in relation to existing literature on self-organized criticality in neural systems, highlighting the broader relevance of our work in understanding complex learning dynamics. Our research contributes to the field by providing a theoretical framework that connects statistical mechanics with educational methodologies, paving the way for future explorations into optimizing learning environments through the manipulation of population interactions. PACS codes: 05.45.Mt, 89.70.+c.",
        "ori-fast-z-score": -0.9434563530497265,
        "water-fast-z-score": 4.427188724235731,
        "rewrite-fast-z-score": 2.393494006535641
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CKM and Tri-bimaximal MNS Matrices in a SU(5) x (d)T Model .\nAbstract:\nWe present the results for neutrino mixing angles, CP violating phases and mass squared differences obtained by using the Fritzsch ansatz to generate the charged fermion masses within an extended supersymmetric grand unified theory based on SO(10). We find that this model can accommodate all experimental data with only one free parameter which is related to the ratio between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati-Salam gauge symmetry into the Standard Model gauge group. The CKM matrix elements are predicted as well as the Majorana phase associated with leptonic CP violation. In addition we show how the tri-bimaximal pattern observed experimentally in the lepton sector emerges naturally when the quark-lepton unification hypothesis is imposed at high energies. Finally we discuss briefly some phenomenological consequences of our scenario such as neutrinoless double beta decay and proton decay. PACS numbers: 11.30.Pb, 12.60.Cn",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CKM and Tri - bimaximal MNS Matrices in a SU ( 5 ) x ( d ) T Model . Abstract : We present the results for neutrino mixing angles , CP violating phases and mass squared variations obtained by using the Fritzsch ansatz to produce the charged fermion masses within an extended supersymmetric grand unified theory based on SO ( 10 ) .We see that this description can handle all observation information with only one free parameter which is related to the proportion between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati - Salam gauge symmetry into the Standard Model gauge group . The CKM matrix elements are expected as well as the Majorana phase involved with leptonic CP violation .In addition we explain how the tri - bimaximal pattern found experimentally in the lepton region arises readily when the quark - lepton unification theory is imposed at high energies . Finally we talk briefly some phenomenological consequences of our scenario such as neutrinoless double alpha emission and proton emission .PACS codes : 11 . 30 . Pb , 12 . 60 . Cn",
        "rewrite_text": "Title: CKM and Tri-bimaximal MNS Matrices in a SU(5) x (d) T Model\n\nAbstract: In this study, we explore the implications of the Fritzsch ansatz for neutrino mixing angles, CP-violating phases, and mass squared differences within the framework of an extended supersymmetric grand unified theory based on SO(10). Our findings indicate that this theoretical model can successfully accommodate all observed data with the introduction of a single free parameter, which is associated with the ratio of the two vacuum expectation values of the Higgs fields that facilitate the breaking of the Pati-Salam gauge symmetry down to the Standard Model gauge group. We derive the elements of the CKM matrix and analyze the Majorana phase linked to leptonic CP violation. Furthermore, we demonstrate how the tri-bimaximal mixing pattern, which has been confirmed through experimental observations in the lepton sector, emerges naturally when quark-lepton unification is considered at high energy scales. We also briefly discuss the phenomenological implications of our model, including the potential for neutrinoless double beta decay and proton decay processes. Our results contribute to a deeper understanding of the interplay between quark and lepton sectors in grand unified theories and highlight the significance of specific symmetry breaking patterns in explaining the observed properties of neutrinos and charged fermions. PACS codes: 11.30.Pb, 12.60.Cn.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 4.04145188432738,
        "rewrite-fast-z-score": -0.41256849850351734
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Na I D resonance lines in main sequence late-type stars .\nAbstract:\nWe present new high-resolution, near-infrared (NIR) spectra for the coolest known members of open clusters M67 and NGC 2516 obtained with the Phoenix spectrograph on Gemini South Observatory. The observations were carried out to study the sodium doublet at λλ8183/8195 Å as well as other atomic features that are sensitive to surface gravity and effective temperature. We have determined fundamental stellar parameters such as T eff , log g,  Fe/H , v sin i, and projected rotational velocity using spectral synthesis techniques. Our results show that all targets exhibit solar-like abundances within uncertainties. In addition we find evidence for differential rotation among our sample stars. Finally, we compare our derived values with those found by previous studies and discuss possible reasons behind discrepancies between different works. \n \n Keywords: Near-infrared spectroscopy, Open cluster, Surface gravity, Differential rotation, Fundamental parameters",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Na I D correlation lines in major sequence late - class stars . Abstract : We report new high - resolution , near - infrared ( NIR ) spectra for the coolest known members of close galaxies M67 and NGC 2516 obtained with the Phoenix spectrograph on Gemini South Observatory .The surveys were carried out to study the sodium doublet at λλ8183 / 8195 Å as well as other atomic features that are subject to surface gravity and effective heat . We have predicted fundamental stellar variables such as T eff , log f , Fe / H , v sin i , and projected rotational momentum using spectral synthesis techniques .Our results show that all targets exhibit solar - like abundances within uncertainties . In addition we find proof for differential rotation among our sample stars .Finally , we compare our derived values with those identified by earlier publications and consider likely explanations behind discrepancies between various works . Keywords : Near - infrared spectroscopy , Open cluster , Surface gravity , Differential rotation , Fundamental parameters",
        "rewrite_text": "**Title:** The Na I D Correlation Lines in Late-Class Main Sequence Stars\n\n**Abstract:** In this study, we present high-resolution near-infrared (NIR) spectra of the coolest known members of the open clusters M67 and NGC 2516, acquired using the Phoenix spectrograph at the Gemini South Observatory. Our investigation focuses on the sodium doublet at wavelengths of 8183 and 8195 Å, along with other atomic features influenced by surface gravity and effective temperature. Through spectral synthesis techniques, we have successfully derived key stellar parameters, including effective temperature (T_eff), logarithmic surface gravity (log g), metallicity (Fe/H), projected rotational velocity (v sin i), and angular momentum. The analysis reveals that all observed stars display solar-like abundance patterns within the limits of measurement uncertainty. Notably, our findings indicate the presence of differential rotation among the stars in our sample, suggesting a complex interplay between rotation and stellar evolution in these late-class main sequence stars. Furthermore, we compare our derived parameters with those reported in previous studies, exploring potential reasons for discrepancies observed in the literature. This work contributes to the understanding of stellar characteristics in open clusters and highlights the importance of high-resolution NIR spectroscopy in elucidating the physical properties of stars. \n\n**Keywords:** Near-infrared spectroscopy, Open clusters, Surface gravity, Differential rotation, Fundamental parameters.",
        "ori-fast-z-score": -0.3418817293789138,
        "water-fast-z-score": 4.612860393796698,
        "rewrite-fast-z-score": 0.8867963503478639
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Generation of different Bell states within the SPDC phase-matching bandwidth .\nAbstract:\nWe demonstrate that it is possible to generate all four Bell states in one single nonlinear crystal by using two pump beams with orthogonal polarizations and slightly different wavelengths, which are generated via second-harmonic generation (SHG) inside an optical parametric oscillator (OPO). The OPO consists of a periodically poled lithium niobate (PPLN) crystal as nonlinear medium and a concave mirror for cavity feedback. We show experimentally that this approach allows us to obtain high-visibility quantum interference between photons emitted at degenerate wavelength pairs across the entire PPLN acceptance bandwidth. This method can be used to simplify future experiments on continuous-variable entanglement distribution over large distances. \n \n Quantum information processing requires the ability to create and manipulate entangled states of light. In particular, the Bell state measurement plays a key role in many applications such as teleportation or quantum repeaters  1  . However, generating these highly nonclassical states is challenging because they require indistinguishable photon pairs  2  , which cannot be produced deterministically  3  .\nIn recent years, several approaches have been developed to overcome this problem  4  . One possibility is based on spontaneous parametric down-conversion (SPDC), where a pump beam creates correlated pairs of signal and idler photons  5  . By adjusting the relative phases of the pump fields  6  , it has become possible to produce any desired superposition of the four Bell states  7, 8  . Another option uses squeezed vacuum states  9  or displaced number states  10  instead of coherent laser pulses  11  . These methods allow for efficient generation of entangled states but usually suffer from low visibility due to imperfections  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Generation of different Bell states within the SPDC phase - matching bandwidth . Abstract : We suggest that it is easy to create all four Bell states in one single nonlinear crystal by using two pump beams with orthogonal polarizations and slightly different wavelengths , which are produced via second - harmonic production ( SHG ) inside an optical parametric oscillator ( OPO ) .The OPO consists of a periodically poled lithium niobate ( PPLN ) crystal as nonlinear medium and a concave mirror for cavity feedback . We see experimentally that this methodology allows us to obtain high - visibility molecular noise between photons generated at degenerate spectrum pairs across the entire PPLN acceptance bandwidth .This method can be used to simplify future research on continuous - variable entanglement distribution over large distances . Quantum knowledge processing requires the ability to create and manipulate entangled states of light .In particular , the Bell state observation takes a key importance in many applications such as teleportation or quantum repeaters 1 . However , constructing these strongly nonclassical states is problematic because they use indistinguishable photon pairs 2 , which cannot be made deterministically 3 .In recent years , various approaches have been formulated to overcome this situation 4 . One possibility is based on spontaneous parametric down - transfer ( SPDC ) , where a pump beam creates correlated pairs of signal and idler photons 5 .By adjusting the relative stages of the pump fields 6 , it has become able to produce any desired superposition of the four Bell states 7 , 8 . Another option uses squeezed vacuum states 9 or displaced number states 10 instead of coherent beam waves 11 .These methods provide for efficient production of entangled states but typically suffer from small visibility owing to imperfections 12 .",
        "rewrite_text": "**Title:** Generation of Various Bell States within the Phase-Matching Bandwidth of SPDC\n\n**Abstract:** In this study, we present a straightforward method for generating all four Bell states using a single nonlinear crystal. This is achieved by employing two pump beams with orthogonal polarizations and slightly differing wavelengths, which are produced through second-harmonic generation (SHG) within an optical parametric oscillator (OPO). The OPO utilizes a periodically poled lithium niobate (PPLN) crystal as the nonlinear medium, complemented by a concave mirror to facilitate cavity feedback. Our experimental results demonstrate that this approach enables the generation of high-visibility molecular noise between photons produced from degenerate spectrum pairs across the full acceptance bandwidth of the PPLN crystal. This technique holds significant potential for advancing research in continuous-variable entanglement distribution over extensive distances.\n\nThe ability to create and manipulate entangled states of light is crucial for quantum information processing. Among these, the observation of Bell states is particularly important for applications such as quantum teleportation and quantum repeaters. However, the deterministic construction of these strongly nonclassical states poses challenges, primarily due to the reliance on indistinguishable photon pairs, which are difficult to produce reliably. Recent advancements have proposed various strategies to address these challenges. One promising approach involves spontaneous parametric down-conversion (SPDC), where a pump beam generates correlated pairs of signal and idler photons. By fine-tuning the relative phases of the pump fields, it is now possible to create any desired superposition of the four Bell states. Alternatively, methods utilizing squeezed vacuum states or displaced number states have been explored, offering efficient production of entangled states, albeit often with reduced visibility due to inherent imperfections. Our findings contribute to the ongoing efforts to enhance the generation of entangled states, paving the way for more robust quantum communication systems.",
        "ori-fast-z-score": 0.8638684255813601,
        "water-fast-z-score": 6.7381737195346085,
        "rewrite-fast-z-score": 0.9198662110077999
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Outer jet X-ray and radio emission in R Aquarii: 1999.8 to 2004.0 .\nAbstract:\nWe present new results on the outer jets of the symbiotic star, R Aqr (=V1016 Cyg). We have analyzed archival Chandra data obtained between 1999 August 31 and 2000 September 30 as well as XMM-Newton observations taken between 2001 October 24 and 2002 November 3. The analysis shows that both jets are still active at least up to 2004 January 1. In addition we report on an optical spectroscopic campaign carried out with the Nordic Optical Telescope during 2003 December 10-17 which revealed no significant changes compared to previous campaigns. Finally, we discuss our findings within the context of current models for the formation of bipolar nebulae around evolved stars. Keywords: Symbiosis, Jets, Bipolar Nebulae, Stellar Winds, Mass Ejection, Binary Star Systems, Chandra Observatory, XMM-Newton Observatory, R Aquarius, V1016 Cyg",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Outer jet X - ray and radio emission in R Aquarii : 1999 . 8 to 2004 . 0 . Abstract : We report new data on the exterior jets of the symbiotic star , R Aqr ( = V1016 Cyg ) .We have analyzed archival Chandra data acquired between 1999 August 31 and 2000 September 30 as well as XMM - Newton images took between 2001 October 24 and 2002 November 3 . The evaluation indicates that both jets are still active at least up to 2004 January 1 .In addition we publish on an optical spectroscopic campaign conducted out with the Nordic Optical Telescope during 2003 December 10 - 17 which revealed no major changes compared to previous efforts . Finally , we explain our findings within the context of recent models for the formation of bipolar nebulae around evolved galaxies .Keywords: Symbiosis, Jets, Bipolar Nebulae, Stellar Winds, Mass Ejection, Binary Star Systems, Chandra Observatory, XMM-Newton Observatory, R Aquarius, V1016 Cyg",
        "rewrite_text": "We present new findings regarding the outer jets of the symbiotic star R Aquarii (also known as V1016 Cyg), based on a comprehensive analysis of archival data. Our study incorporates Chandra observations collected between August 31, 1999, and September 30, 2000, alongside XMM-Newton images obtained from October 24, 2001, to November 3, 2002. The results of our analysis indicate that both jets remain active at least until January 1, 2004. Additionally, we conducted an optical spectroscopic campaign using the Nordic Optical Telescope from December 10 to December 17, 2003, which revealed no significant changes in the spectral characteristics compared to previous observations. These findings contribute to our understanding of the dynamics and behavior of the jets associated with R Aquarii. We contextualize our results within the framework of contemporary models that describe the formation of bipolar nebulae surrounding evolved stellar systems. Our work underscores the importance of continued monitoring of such symbiotic stars to enhance our comprehension of their complex interactions and the processes governing mass ejection in binary star systems. The keywords associated with this study include symbiosis, jets, bipolar nebulae, stellar winds, mass ejection, binary star systems, Chandra Observatory, XMM-Newton Observatory, R Aquarii, and V1016 Cyg. This research not only adds to the existing body of knowledge regarding R Aquarii but also provides insights into the broader implications for the study of stellar evolution and the phenomena associated with evolved galaxies.",
        "ori-fast-z-score": 0.282842712474619,
        "water-fast-z-score": 4.900980294098034,
        "rewrite-fast-z-score": 1.3065491598369756
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Groups of galaxies in the SDSS Data Release 5. A group-finder and a catalogue .\nAbstract:\nWe present an algorithm to find galaxy groups using photometric redshifts, which is based on the Voronoi tessellation technique (VT). The VT method has been widely used for identifying clusters of galaxies with spectroscopic redshifts but it had not yet been applied to identify galaxy groups with photometric redshifts. We use the Sloan Digital Sky Survey data release five (SDSS DR5) as our input sample. Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available. In this work we have identified more than 12000 galaxy groups at 0 < z < 0.3. These groups contain about 30000 member galaxies. We also provide a catalog containing basic information such as positions, magnitudes, colors, and photometric redshifts for all these groups. \n \n Keywords: Galaxy Group, Photometric Redshift",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Groups of galaxies in the SDSS Data Release 5 . A group - finder and a catalogue .Abstract : We present an algorithm to find galaxy groups using photometric redshifts , which is based on the Voronoi tessellation method ( VT ) . The VT method has been widely applied for finding clusters of clusters with spectroscopic redshifts but it had not already been used to identify galaxy groups with photometric redshifts .We use the Sloan Digital Sky Survey data release five ( SDSS DR5 ) as our input data . Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available .In this research we have discovered more than 12000 star groups at 0 < z < 0 . 3 . These groups contain about 30000 member galaxies .We additionally offer a list containing essential information such as positions , magnitudes , colors , and photometric redshifts for all these groups . Keywords : Galaxy Group , Photometric Redshift",
        "rewrite_text": "In this article, we introduce a novel algorithm designed to identify groups of galaxies utilizing photometric redshifts, employing the Voronoi tessellation (VT) method. While the VT method has been effectively utilized for detecting clusters of galaxies with spectroscopic redshifts, its application to galaxy groups identified through photometric redshifts has not been previously explored. Our study leverages data from the Sloan Digital Sky Survey Data Release 5 (SDSS DR5) as the foundational dataset for our analysis. The findings demonstrate that the VT method is adept at successfully identifying galaxy groups, even in scenarios where only photometric redshifts are accessible. Throughout our investigation, we have identified over 12,000 galaxy groups within the redshift range of 0 < z < 0.3, encompassing approximately 30,000 member galaxies. Furthermore, we provide a comprehensive catalogue that includes critical information for each identified group, such as their spatial positions, magnitudes, colors, and photometric redshifts. This work not only enhances our understanding of galaxy group dynamics but also contributes valuable data for future astronomical research. The implications of our findings are significant, as they pave the way for further studies on the large-scale structure of the universe and the evolution of galaxies within these groups. Keywords associated with this research include Galaxy Group and Photometric Redshift.",
        "ori-fast-z-score": -1.6644794391276478,
        "water-fast-z-score": 2.794002794004191,
        "rewrite-fast-z-score": -0.7683498199278324
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observations towards early-type stars in the ESO-POP survey: II -- searches for intermediate and high velocity clouds .\nAbstract:\nWe present new results on interstellar absorption lines toward early type stars observed with UVES at VLT, as part of the ESO-POP project (ESO program 085.D-0571). We have searched for high-velocity clouds (HVCs) by looking for blueshifted components in the MgII doublet line profiles. The sample consists of 16 OB-stars located within 1 kpc distance from Earth. In addition to previously known HVCs we find several new ones. Some of these are associated with nearby galaxies while others may be related to Galactic halo gas. A comparison between our data set and previous surveys shows that there is no significant difference in the number density distribution of HVCs along different sightlines. This suggests that most of them are small structures which do not cover much solid angle around their host galaxy or star. \n \n Keywords: Interstellar medium",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observations towards early - class stars in the ESO - POP survey : II - - surveys for intermediate and large velocity clouds . Abstract : We report new data on interstellar absorption patterns toward advanced type galaxies studied with UVES at VLT , as part of the ESO - POP project ( ESO program 085 . D - 0571 ) .We have searched for high - speed clouds ( HVCs ) by searching for blueshifted elements in the MgII doublet line profiles . The sample consists of 16 OB - stars situated within 1 kpc radius from Earth .In addition to earlier known HVCs we find several new ones . Some of these are identified with nearby galaxies while several might be connected to Galactic halo gas .A comparison between our information set and previous surveys reveals that there is no considerable difference in the number density spread of HVCs along various sightlines . This implies that most of them are small structures which do not cover many solid angle around their target galaxy or star .Keywords: Interstellar medium",
        "rewrite_text": "We present new findings on interstellar absorption characteristics observed in advanced type galaxies, utilizing UVES at the Very Large Telescope (VLT) as part of the ESO-POP project (ESO program 085.D-0571). Our investigation focuses on the detection of high-velocity clouds (HVCs) by analyzing blueshifted elements within the MgII doublet line profiles. The study encompasses a sample of 16 OB stars located within a 1 kpc radius from Earth. In addition to previously identified HVCs, our observations have uncovered several new clouds, some of which are associated with nearby galaxies, while others may be linked to the gas in the Galactic halo. A comparative analysis of our dataset with earlier surveys indicates no significant variation in the number density distribution of HVCs across different sightlines. This finding suggests that the majority of HVCs are relatively small structures that do not occupy a substantial solid angle around their respective target galaxies or stars. Our results contribute to the understanding of the interstellar medium and the dynamics of HVCs, highlighting the intricate relationship between these clouds and their surrounding environments. The implications of these observations are crucial for advancing our knowledge of galactic structure and the processes governing the interstellar medium. \n\nKeywords: Interstellar medium, high-velocity clouds, OB stars, ESO-POP project, MgII doublet, Galactic halo.",
        "ori-fast-z-score": -2.038098661460272,
        "water-fast-z-score": 4.387842813611494,
        "rewrite-fast-z-score": -0.30779350562554625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chi2 and chi3 harmonic generation at a critical power in inhomogeneous doubly resonant cavities .\nAbstract:\nWe study the second-harmonic (SH) and third-harmonic (TH) generations in an inhomogeneously broadened cavity with two different resonance frequencies for fundamental wave (FW). We show that, when the intracavity FW intensity reaches its maximum value, both SHG and THG can be enhanced simultaneously by increasing the pumping rate or decreasing the detuning between the two modes. The enhancement is due to the fact that the nonlinear susceptibility becomes larger than zero at this point. This effect has been observed experimentally recently. \n \n In addition, we find that there exists another regime where only one type of harmonics can be generated efficiently while suppressing other types of harmonics. For example, if the intracavity FW field is tuned close to the lower mode frequency, then it will generate mainly SH light but very little TH light; on the contrary, if the intracavty FW field is tuned near the upper mode frequency, then it generates mainly TH light but almost no SH light.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chi2 and chi3 harmonic production at a critical power in inhomogeneous doubly resonant cavities . Abstract : We research the second - harmonic ( SH ) and third - harmonic ( TH ) generations in an inhomogeneously broadened cavity with two different resonance energies for fundamental wave ( FW ) .We see that , when the intracavity FW frequency exceeds its highest value , both SHG and THG can be enhanced simultaneously by expanding the pumping rate or decreasing the detuning between the two modes . The enhancement is due to the fact that the nonlinear susceptibility grows larger than zero at this time .This phenomenon has been observed experimentally recently . In addition , we find that there exists another regime where only one sort of harmonics can be generated smoothly while suppressing other types of harmonics .For instance , if the intracavity FW field is tuned far to the lower mode wavelength , then it will generate mostly SH light but very less TH radiation ; on the contrary , if the intracavty FW field is tuned near the higher mode wavelength , then it generates largely TH light but almost no SH light .",
        "rewrite_text": "In this study, we investigate the generation of second harmonic (SH) and third harmonic (TH) waves within an inhomogeneously broadened cavity that possesses two distinct resonance energies for the fundamental wave (FW). Our findings reveal that when the intracavity frequency of the FW surpasses its maximum threshold, both SHG and THG can be significantly enhanced. This enhancement can be achieved by either increasing the pumping rate or reducing the detuning between the two modes. The underlying reason for this enhancement is attributed to the nonlinear susceptibility, which becomes greater than zero under these conditions. This phenomenon has been corroborated by recent experimental observations. Furthermore, we identify an additional operational regime where the generation of harmonics can be selectively controlled. In this regime, it is possible to generate one type of harmonic while effectively suppressing the others. For example, when the intracavity FW field is tuned towards the lower mode wavelength, it predominantly produces SH light with minimal TH radiation. Conversely, when the intracavity FW field is adjusted closer to the higher mode wavelength, it primarily generates TH light with negligible SH output. These insights into the harmonic generation dynamics in inhomogeneous doubly resonant cavities could have significant implications for the development of advanced photonic devices and applications in nonlinear optics.",
        "ori-fast-z-score": -1.8599622199011085,
        "water-fast-z-score": 3.5795716689756794,
        "rewrite-fast-z-score": 1.091089451179962
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Atlas of two-dimensional irreversible conservative lagrangian mechanical systems with a second quadratic integral .\nAbstract:\nWe present an atlas for the phase space of all possible motions in two dimensions of a class of conservative Lagrangian mechanical systems, which are characterized by having one first and one second quadratic integral of motion. The construction is based on the existence of a special type of canonical transformation that maps each system into another one whose trajectories can be obtained explicitly as solutions to quadratures. We show how this method allows us to obtain explicit expressions for the general solution of some particular problems such as those corresponding to the harmonic oscillator or the Kepler problem. \nThe results presented here constitute a generalization of previous works dealing only with integrable cases. They also provide new insights about the structure of these types of systems. In addition we discuss several examples illustrating our approach. This article is part of a series of papers devoted to the study of the dynamics of certain classes of physical systems described by means of Hamiltonians depending on time through periodic functions (see  1  , 2 ).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Atlas of two - dimensional irreversible conservative lagrangian mechanical models with a second quadratic integral . Abstract : We create an atlas for the phase space of all possible motions in two dimensions of a class of conservative Lagrangian mechanical models , which are characterized by having one second and one second quadratic integral of movement .The construction is based on the existence of a special type of canonical conversion that connects each system into another one whose trajectories can be obtained explicitly as solutions to quadratures . We see how this process allows us to obtain precise expressions for the general solution of some particular problems such as those corresponding to the harmonic oscillator or the Kepler problem .The results presented here represent a generalization of previous works dealing only with integrable cases . They addition offer additional ideas about the organization of these kinds of structures .In addition we explain several examples illustrating our approach . This section is part of a sequence of papers focused to the study of the dynamics of certain classes of physical processes characterized by means of Hamiltonians varying on time through periodic functions ( saw 1 , 2 ) .",
        "rewrite_text": "Title: Atlas of Two-Dimensional Irreversible Conservative Lagrangian Mechanical Models with a Second Quadratic Integral\n\nAbstract: In this study, we present a comprehensive atlas detailing the phase space of all potential motions within a specific class of two-dimensional conservative Lagrangian mechanical models, distinguished by the presence of a second quadratic integral of motion. Our methodology is grounded in a unique form of canonical transformation that facilitates the conversion of each mechanical system into another, enabling the explicit determination of trajectories through quadrature solutions. This innovative approach allows us to derive precise expressions for the general solutions of notable problems, including the harmonic oscillator and the Kepler problem. The findings of this research extend the scope of previous studies that were limited to integrable cases, thereby enriching the understanding of the structural organization inherent in these mechanical systems. Furthermore, we provide several illustrative examples that exemplify our methodology, enhancing the clarity of our approach. This work is part of an ongoing series of papers dedicated to exploring the dynamics of specific classes of physical processes, particularly those characterized by Hamiltonians that vary over time according to periodic functions. Through this research, we aim to contribute to the broader discourse on the dynamics of conservative systems and their integrability, offering new insights and frameworks for future investigations in the field.",
        "ori-fast-z-score": -0.4216370213557839,
        "water-fast-z-score": 5.555909634626167,
        "rewrite-fast-z-score": 0.8955334711889903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Simple shear flow in inelastic Maxwell models .\nAbstract:\nWe study the simple shear flow for two different types of inelastic Maxwell models, one with constant restitution coefficient and another with velocity dependent restitution coefficient. We find that there is no steady state solution when the system has only elastic collisions but it exists if we include some dissipative effects. The steady state solutions are found to be independent of initial conditions as well as the type of dissipation included. In addition, we also show that these steady states are linearly stable under small perturbations. Finally, we present results on the relaxation time scales associated with this problem which can be used to estimate the rate at which the system approaches its steady state. \n \n Introduction:- \n \n Simple shear flows have been studied extensively both theoretically  1 - 3  and experimentally  4 - 6  . These studies were motivated by the fact that such flows occur naturally in many physical systems like granular materials  7  , colloidal suspensions  8  etc.. It was shown recently  9  -  11  that even though the microscopic dynamics may be very complicated (like in case of granular particles), the macroscopic behaviour of the system could still be described using relatively simpler kinetic equations like Boltzmann equation or Enskog equation  12  . However, most of these works deal with elastic interactions between the particles while the effect of dissipation is usually ignored. This assumption is not always valid especially in cases where the particles interact via soft potentials  13  .\n \nIn recent years, several authors  14  -  16  have considered the effect of dissipation on various properties of dilute gases. For example, in Ref.  17  , the author considers an inelastic gas consisting of identical hard spheres interacting through a repulsive potential and shows how the presence of dissipation affects the transport coefficients of the system. On the other hand, in Refs.  18  -  20  , the authors consider a model consisting of point particles interacting via a pairwise additive potential and derive expressions for the transport coefficients of the corresponding fluid. They then use these expressions to calculate the viscosity and thermal conductivity of the system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Simple shear flow in inelastic Maxwell systems . Abstract : We explore the simple shear flow for two different kinds of inelastic Maxwell systems , one with constant restitution coefficient and another with velocity dependent restitution coefficient .We see that there is no steady state solution when the system has only elastic collisions but it exists if we involve some dissipative effects . The stable state solutions are found to be independent of initial conditions as well as the kind of dissipation included .In addition , we also demonstrate that these steady states are linearly consistent under small perturbations . Finally , we present results on the relaxation time ranges associated with this situation which can be used to estimate the speed at which the process approaches its steady state .Introduction : - Simple shear flows have been studied thoroughly both theoretically 1 - 3 and experimentally 4 - 6 . These studies were driven by the fact that such flows occur readily in many mechanical environments like granular materials 7 , colloidal suspensions 8 etc . .It was shown recently 9 - 11 that even though the microscopic behavior may be very complicated ( like in case of granular particles ) , the macroscopic nature of the system could still be described using relatively simpler kinetic equations like Boltzmann equation or Enskog function 12 . However , most of these works work with elastic interactions between the molecules while the impact of dissipation is usually neglected .This assumption is not always legitimate usually in cases where the atoms interact via soft potentials 13 . In recent years , various literature 14 - 16 have proposed the impact of dissipation on various properties of dilute gases .For instance , in Ref . 17 , the writer treats an inelastic gas consisting of corresponding hard particles interacting through a repulsive potential and shows how the presence of dissipation influences the travel coefficients of the system .On the other hand , in Refs . 18 - 20 , the writers see a theory consisting of point interactions interacting via a pairwise additive potential and derive expressions for the travel coefficients of the resulting flow .They then use these expressions to estimate the viscosity and thermal conductivity of the system .",
        "rewrite_text": "**Title:** Simple Shear Flow in Inelastic Maxwell Systems\n\n**Abstract:** This study investigates simple shear flow in two distinct types of inelastic Maxwell systems: one characterized by a constant restitution coefficient and the other by a velocity-dependent restitution coefficient. Our findings reveal that in systems with purely elastic collisions, a steady-state solution does not exist; however, the introduction of dissipative effects facilitates the emergence of such solutions. Notably, these stable state solutions are shown to be independent of the initial conditions and the specific type of dissipation applied. Furthermore, we demonstrate that these steady states exhibit linear consistency when subjected to small perturbations, indicating their robustness. We also provide insights into the relaxation time ranges associated with these systems, which serve as a metric for estimating the rate at which the system approaches its steady state. \n\n**Introduction:** Simple shear flows have been extensively examined in both theoretical and experimental contexts due to their prevalence in various mechanical environments, including granular materials and colloidal suspensions. Recent research has highlighted that despite the complex microscopic behavior observed in systems such as granular particles, the macroscopic characteristics can often be effectively described using simpler kinetic equations, such as the Boltzmann equation or the Enskog function. However, much of the existing literature has primarily focused on elastic interactions, often overlooking the significant effects of dissipation. This oversight is particularly relevant in scenarios where particles interact through soft potentials. Recent studies have begun to address the role of dissipation in influencing the properties of dilute gases. For example, one study examines an inelastic gas composed of hard particles interacting through a repulsive potential, demonstrating how dissipation affects the transport coefficients of the system. Other research has explored theories involving point interactions with pairwise additive potentials, deriving expressions for the transport coefficients and utilizing these to estimate the viscosity and thermal conductivity of the flow. This article aims to contribute to this growing body of work by providing a comprehensive analysis of simple shear flow in inelastic Maxwell systems, emphasizing the importance of dissipative effects in achieving steady-state behavior.",
        "ori-fast-z-score": -0.4714045207910317,
        "water-fast-z-score": 7.127670094890412,
        "rewrite-fast-z-score": 1.1538461538461537
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Larkin-Ovchinnikov-Fulde-Ferrell state in two-color quark matter .\nAbstract:\nWe study the ground-state properties of two-flavor color superconducting (2SC) quark matter at finite density and temperature by using an effective chiral model with vector interaction, which is derived from QCD under the mean-field approximation. We find that there exists a new type of 2SC phase where quarks are paired into diquark condensates with different colors but same flavor. This novel phase has been named as Larkin-Ovchinnikova-Fulde-Ferreell (LOFF) state because it was first proposed to describe superfluidity in nuclear systems. In this LOFF state, we show that the gap parameter for pairing between quarks with opposite momenta depends on their relative angle. The magnitude of the gap decreases rapidly when they move away from each other along the Fermi surface. As a result, the energy gap vanishes completely near the boundary of the Brillouin zone.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Larkin - Ovchinnikov - Fulde - Ferrell state in two - color quark matter . Abstract : We research the ground - state properties of two - flavor color superconducting ( 2SC ) quark matter at finite density and heat by using an efficient chiral description with vector interaction , which is generated from QCD under the mean - field approximation .We see that there exists a new kind of 2SC cycle where quarks are paired into diquark condensates with various colors but same flavor . This novel mode has been called as Larkin - Ovchinnikova - Fulde - Ferreell ( LOFF ) state because it was first suggested to explain superfluidity in nuclear systems .In this LOFF state , we find that the gap parameter for pairing between quarks with opposite momenta depends on their relative angle . The magnitude of the gap falls swiftly when they change apart from each other along the Fermi surface .As a result , the power gap vanishes totally near the boundary of the Brillouin zone .",
        "rewrite_text": "In this study, we investigate the ground-state characteristics of two-flavor color superconducting (2SC) quark matter under conditions of finite density and temperature. Utilizing an effective chiral framework that incorporates vector interactions, derived from quantum chromodynamics (QCD) within the mean-field approximation, we uncover the existence of a novel 2SC phase characterized by quark pairings into diquark condensates that exhibit varying colors while maintaining the same flavor. This intriguing phase, referred to as the Larkin-Ovchinnikov-Fulde-Ferrell (LOFF) state, was initially proposed to elucidate superfluidity phenomena in nuclear systems. Our analysis reveals that in the LOFF state, the pairing gap parameter for quarks with opposite momenta is influenced by their relative angular orientation. Notably, we observe that the magnitude of this gap diminishes rapidly as the quarks move apart along the Fermi surface, ultimately leading to a complete disappearance of the gap near the edges of the Brillouin zone. This finding has significant implications for understanding the behavior of quark matter in extreme conditions, shedding light on the intricate dynamics of color superconductivity and the potential formation of complex pairing patterns in high-density environments. Our results contribute to the broader discourse on the properties of quark matter and its relevance to astrophysical phenomena, such as the internal structure of neutron stars and the dynamics of heavy-ion collisions.",
        "ori-fast-z-score": -2.0465595024580763,
        "water-fast-z-score": 2.8685486624025445,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coincident, 100 kpc-scale damped Lyman alpha absorption towards a binary QSO: how large are galaxies at z ~ 3? .\nAbstract:\nWe report on the discovery of an intervening galaxy with a mass M = 1011.5 ± 0.3M⊙ and size R = 1.7 ± 0.2h−1kpc in front of a gravitationally lensed quasar pair separated by 5′′ (~100 kpc). The absorber is detected as a DLA system along both sightlines to the quasars, which have redshifts zqso = 2.962 and zqso = 2. . We use this object to constrain the typical sizes of high-z galaxies. Our results suggest that these objects were typically smaller than their local counterparts when they formed most of their stars. This may be related to the fact that massive galaxies grow through mergers over cosmic time. \n \n Keywords: Galaxy evolution, Quasars, Absorbers, Massive black holes \n \n \n \n High-redshift quasars provide powerful probes for studying the physical properties of distant galaxies. In particular, gravitational lens systems can magnify background sources, allowing us to study fainter structures such as faint companions or extended halos around bright foreground lenses. Here we present new observations of the gravitationally-lensed quasar pair HE0435-1223, where one component has been previously found to host a supermassive black hole (SMBH) with a mass MBH = 4 × 109M☉ . Using deep near-infrared spectroscopy obtained with VLT/X-SHOOTER, we detect a strong Mg II λ2796 line associated with a galaxy located between the two quasars. The galaxy shows no evidence of ongoing star formation activity but hosts a very old stellar population. Its total luminosity corresponds to a SFR < 10−2M☉ yr−1 , indicating that it was not actively forming stars during its peak epoch of star-formation activity. However, the presence of a young stellar population cannot be ruled out completely due to possible dust obscuration effects. From our analysis, we find that the galaxy has a mass M = 1011+0.3−0.4M☉ and radius R =",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coincident , 100 kpc - scale damped Lyman alpha absorption towards a binary QSO : how large are galaxies at z ~ 3 ? .Abstract : We report on the discovery of an intervening galaxy with a mass M = 1011 . 5 ± 0 . [UNK] and size R = 1 . 7 ± 0 . 2h−1kpc in top of a gravitationally lensed quasar pair divided by 5 ′ ′ ( ~ 100 kpc ) . The absorber is detected as a DLA system along both sightlines to the quasars , which have redshifts zqso = 2 . 962 and zqso = 2 . .We use this object to constrain the typical dimensions of high - z galaxies . Our results show that these objects were generally tiny than their local neighbours when they formed most of their stars .This might be connected to the fact that powerful nuclei grow through mergers over cosmic time . Keywords : Galaxy evolve , Quasars , Absorbers , Massive black holes High - redshift quasars serve powerful probes for studying the physical properties of distant galaxies .In particular , gravity lens systems can magnify background sources , allowing us to study fainter objects such as faint companions or open halos around bright foreground lenses . Here we present new studies of the gravitationally - lensed quasar pair HE0435 - 1223 , where one core has been previously found to host a supermassive black hole ( SMBH ) with a mass MBH = 4 × 109M☉ .Using deep near - infrared spectroscopy acquired with VLT / X - SHOOTER , we locate a powerful Mg II λ2796 line associated with a galaxy located between the two quasars . The galaxy displays no evidence of ongoing star formation activity but hosts a very ancient stellar community .Its overall luminosity corresponds to a SFR < 10−2M☉ yr−1 , showing that it was not actively creating stars during its high epoch of star - formation activity . However , the presence of a young stellar community cannot be decided out completely due to possible dust obscuration effects .From our analysis , we find that the universe has a mass M = 1011 + 0 . 3−0 . 4M☉ and radius R =",
        "rewrite_text": "**Title:** Coincident, 100 kpc-scale damped Lyman-alpha absorption towards a binary QSO: How large are galaxies at z ~ 3?\n\n**Abstract:** In this study, we present the discovery of an intervening galaxy situated between a gravitationally lensed quasar pair, separated by approximately 100 kpc (5′′). The galaxy exhibits a mass of M = 10^11.5 ± 0. [UNK] and a size of R = 1.7 ± 0.2 h^−1 kpc. This galaxy is identified as a damped Lyman-alpha (DLA) absorber along the sightlines of both quasars, which have redshifts of z_qso = 2.962 and z_qso = 2. . Our findings provide valuable insights into the typical dimensions of high-redshift galaxies, revealing that these early galaxies were generally smaller than their local counterparts during their peak star formation periods. This observation may be linked to the notion that powerful galactic nuclei evolve primarily through mergers over cosmic time.\n\nHigh-redshift quasars serve as significant tools for examining the physical characteristics of distant galaxies. In particular, gravitational lensing systems enhance the visibility of background sources, enabling the study of fainter objects, such as dim companions or extended halos surrounding bright foreground lenses. We focus on the gravitationally lensed quasar pair HE0435-1223, where one of the cores has previously been identified to host a supermassive black hole (SMBH) with a mass of M_BH = 4 × 10^9 M☉. Utilizing deep near-infrared spectroscopy from the VLT/X-SHOOTER, we detect a prominent Mg II λ2796 line associated with the galaxy located between the two quasars. Notably, this galaxy shows no signs of ongoing star formation but is characterized by an ancient stellar population. Its overall luminosity indicates a star formation rate (SFR) of less than 10^−2 M☉ yr^−1, suggesting that it was not actively forming stars during its peak epoch of star formation. However, the possibility of a younger stellar population cannot be entirely ruled out due to potential dust obscuration effects. Our analysis concludes that the intervening galaxy possesses a mass of M = 10^11 + 0.3−0.4 M☉ and a radius of R = .",
        "ori-fast-z-score": 1.8864844365675972,
        "water-fast-z-score": 7.491057470676988,
        "rewrite-fast-z-score": 0.8451542547285166
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Further Evidence that the Redshifts of AGN Galaxies May Contain Intrinsic Components .\nAbstract:\nWe have analyzed the redshifts of two samples of active galactic nuclei (AGNs) with different luminosities and found evidence for intrinsic redshift components in both cases.  The first sample consists of 12 Seyfert galaxies, which are luminous AGNs with broad emission lines. We find that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose amplitude is correlated with the widths of the broad emission lines. This correlation suggests that the intrinsic redshift may arise from Doppler shifts associated with outflows or inflows of gas on scales comparable to those probed by the broad-line region. The second sample contains low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. These quasars show no obvious signs of being gravitationally lensed but do exhibit significant intrinsic redshift components. In this case we find that the amplitudes of these components correlate strongly with the optical continuum slopes measured at rest-frame wavelengths near 3000 Å .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Further Evidence that the Redshifts of AGN Galaxies May Contain Intrinsic Components . Abstract : We have analyzed the redshifts of two specimens of active galactic nuclei ( AGNs ) with varying luminosities and found proof for intrinsic redshift components in both cases .The first specimen consists of 12 Seyfert galaxies , which are luminous AGNs with broad absorption lines . We see that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose intensity is associated with the widths of the broad absorption paths .This correlation suggests that the intrinsic redshift may arise from Doppler variations associated with outflows or inflows of gas on scales similar to those probed by the broad - line sector . The second sample comprises small - luminosity quasars chosen from the Sloan Digital Sky Survey Data Release 4 quasar catalog .These quasars exhibit no evident indication of being gravitationally lensed but do exhibit substantial intrinsic redshift components . In this situation we find that the amplitudes of these systems correlate strongly with the optical continuum curves observed at rest - mirror wavelengths near 3000 Å .",
        "rewrite_text": "Title: Further Evidence that the Redshifts of AGN Galaxies May Contain Intrinsic Components\n\nAbstract: In this study, we investigate the redshift characteristics of two distinct samples of active galactic nuclei (AGNs) with differing luminosities, revealing compelling evidence for the presence of intrinsic redshift components in both groups. The first sample consists of 12 Seyfert galaxies, which are classified as luminous AGNs characterized by broad absorption lines. Our analysis indicates that the observed redshifts of these galaxies can be effectively separated into two components: an extrinsic component attributed to gravitational lensing effects from foreground objects, and an intrinsic component that correlates with the widths of the broad absorption lines. This relationship implies that the intrinsic redshift may be linked to Doppler shifts resulting from gas outflows or inflows occurring on scales comparable to those represented by the broad-line region. \n\nThe second sample includes low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. Unlike the Seyfert galaxies, these quasars do not show clear signs of gravitational lensing; however, they do exhibit significant intrinsic redshift components. Notably, we observe a strong correlation between the amplitudes of these intrinsic redshifts and the optical continuum light curves measured at rest-frame wavelengths around 3000 Å. This finding suggests that intrinsic redshift phenomena may be more widespread among AGNs than previously understood, prompting further investigation into the underlying mechanisms that contribute to these redshift variations. Overall, our results provide valuable insights into the complex nature of AGN redshifts and highlight the importance of considering intrinsic components in the study of these enigmatic cosmic entities.",
        "ori-fast-z-score": 0.11547005383792514,
        "water-fast-z-score": 5.27656187902292,
        "rewrite-fast-z-score": -1.2229371288986763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Validating module network learning algorithms using simulated data .\nAbstract:\nWe present an approach to validating the performance of machine-learning algorithms for identifying modules in networks, based on synthetic datasets generated by simulating random walks through known modular structures. We show that this method can be used to identify and rank different types of modules with high accuracy across a range of sizes and densities. The results are robust against noise and missing links. This validation strategy is useful both as a benchmarking tool for comparing competing methods and also as a means of assessing how well existing approaches perform when applied to real-world systems. In recent years there has been growing interest in developing computational tools capable of detecting functional units within complex biological networks such as protein-protein interaction (PPI) or gene regulatory networks  1–3  . These so-called “modules” represent groups of nodes which interact more strongly among themselves than they do with other parts of the system  4  , and may correspond to molecular complexes  5  , signaling pathways  6  , metabolic cycles  7  , or even entire cellular processes  8  .\nThe identification of these modules is important because it provides insight into the organization of the underlying network  9  , and allows us to predict new interactions  10  , detect disease genes  11  , and understand evolutionary relationships  12  . However, despite considerable effort  13–19  , no single algorithm consistently outperforms all others  20  , so researchers have developed a variety of complementary techniques  21–23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Validating module network learning techniques using modeled information . Abstract : We present an way to validating the performance of machine - learning techniques for finding modules in networks , based on synthetic datasets generated by simulating random runs through known modular structures .We see that this method can be used to identify and grade different kinds of modules with high sensitivity across a range of sizes and densities . The results are robust against noise and lost connections .This validation methodology is important both as a benchmarking tool for evaluating rival techniques and also as a means of assessing how well existing techniques perform when applied to real - global networks . In recent years there has been growing interest in establishing computational tools capable of detecting functional units within complex biological networks such as protein - gene interaction ( PPI ) or protein regulatory networks 1 – 3 .These so - called “ modules ” form groups of nodes which interact more strongly among themselves than they do with other parts of the system 4 , and may correspond to biological complexes 5 , signaling pathways 6 , metabolic processes 7 , or maybe whole cellular processes 8 . The identify of these modules is important because it gives knowledge into the organization of the underlying network 9 , and allows us to predict new interactions 10 , predict disease genes 11 , and understand evolutionary relationships 12 .However , despite considerable attempts 13 – 19 , no single method consistently outperforms all others 20 , so researchers have developed a variety of complementary techniques 21 – 23 .",
        "rewrite_text": "In this article, we introduce a novel approach for validating the efficacy of machine learning techniques aimed at identifying modules within networks. Our methodology leverages synthetic datasets created by simulating random processes through established modular structures. The findings demonstrate that this approach effectively identifies and categorizes various types of modules with remarkable sensitivity, regardless of their size or density. Notably, the results remain resilient in the presence of noise and missing connections, underscoring the robustness of our validation technique. This methodology serves a dual purpose: it acts as a benchmarking tool for comparing competing techniques and provides insights into the performance of existing methods when applied to real-world global networks.\n\nThe increasing interest in computational tools for detecting functional units within intricate biological networks—such as protein-protein interaction (PPI) networks and protein regulatory networks—has been evident in recent years. These functional units, referred to as \"modules,\" consist of groups of nodes that exhibit stronger interactions with one another than with other components of the network. Such modules may correspond to biological complexes, signaling pathways, metabolic processes, or even entire cellular functions. Identifying these modules is crucial, as it enhances our understanding of the network's organization and facilitates predictions regarding new interactions, disease-associated genes, and evolutionary relationships.\n\nDespite extensive research efforts aimed at module detection, no single method has emerged as the definitive leader in performance, leading to the development of a diverse array of complementary techniques. Our validation framework not only contributes to the ongoing discourse surrounding module detection but also provides a systematic means to evaluate and improve the performance of various methodologies in the field.",
        "ori-fast-z-score": 0.08481889296799709,
        "water-fast-z-score": 6.821430560638804,
        "rewrite-fast-z-score": 0.8892972917998876
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Star formation at very low metallicity. I: Chemistry and cooling at low densities .\nAbstract:\nWe present the results of our study on chemical composition, molecular line emission, dust properties, and thermal balance in dense cores with metallicities ranging between 1/100 solar to 1/10 000 solar. We find that the gas temperature decreases by about 10 K as the core density increases for all metallicities studied here (1/100-1/10 000 solar). The decrease is more rapid than predicted by current models which assume constant temperatures throughout the cloud evolution. This may be due to an increase in the importance of grain-surface chemistry relative to gas-phase reactions at higher densities. In addition we find evidence for significant depletion of carbon onto grains even at high metallicities such as Z = 1/10 000 solar. Our observations suggest that the critical density above which CO becomes optically thick depends strongly on metallicity. At lower metallicities this occurs at higher densities compared to higher metallicities. Finally, we show that the observed abundance ratios are consistent with those expected if the clouds were initially chemically enriched by supernovae type II explosions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Star formation at very low metallicity . I : Chemistry and cooling at low densities .Abstract : We present the conclusion of our research on chemical composition , molecular line emission , dust characteristics , and thermal balance in dense cores with metallicities ranging between 1 / 100 solar to 1 / 10 000 solar . We see that the gas temperature reduces by about 10 K as the core size grows for all metallicities researched here ( 1 / 100 - 1 / 10 000 solar ) .The reduction is more rapid than forecast by current scenarios which predict constant temperatures throughout the cloud evolution . This might be due to an increase in the importance of grain - surface chemistry relative to liquid - phase processes at higher densities .In addition we find proof for significant depletion of carbon onto grains even at high metallicities such as Z = 1 / 10 000 solar . Our observations suggest that the critical density above which CO becomes optically dense relies highly on metallicity .At lower metallicities this appears at higher densities compared to higher metallicities . Finally , we find that the seen concentrations proportions are compatible with those expected if the clouds were initially chemically enriched by supernovae class II explosions .",
        "rewrite_text": "We present the findings of our study on the chemical composition, molecular line emissions, dust properties, and thermal equilibrium in dense cores with metallicities ranging from 1/100 to 1/10,000 of solar levels. Our research reveals that as the size of the core increases, the gas temperature decreases by approximately 10 K across all metallicity levels examined. This temperature decline occurs more rapidly than predicted by existing models, which suggest that temperatures remain constant throughout the evolution of the cloud. We propose that this discrepancy may be attributed to the growing significance of grain-surface chemistry in comparison to liquid-phase processes at elevated densities. Furthermore, our results indicate a notable depletion of carbon onto dust grains, even at relatively high metallicities, such as Z = 1/10,000 solar. Our observations imply that the critical density at which carbon monoxide (CO) becomes optically thick is highly dependent on metallicity; specifically, at lower metallicities, this threshold occurs at higher densities than at higher metallicities. Lastly, we find that the observed concentration ratios align with expectations based on initial chemical enrichment of the clouds due to Type II supernova explosions. These findings contribute to our understanding of star formation in environments with very low metallicity and highlight the complex interplay between chemical processes and physical conditions in dense interstellar regions.",
        "ori-fast-z-score": 1.3480372031495529,
        "water-fast-z-score": 6.6010959760562775,
        "rewrite-fast-z-score": 1.1766968108291043
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spatially resolved kinematics and stellar populations of brightest cluster and group galaxies .\nAbstract:\nWe present spatially-resolved spectroscopic observations for the central regions (r < 1 kpc) of six nearby, massive early-type galaxies in clusters or groups with Mvir > 1013M⊙. The data were obtained using the Gemini Multi-Object Spectrograph on Gemini North telescope as part of our ongoing program to study the formation histories of these systems. We use the pPXF code to fit the observed spectra with single-single component models consisting of an old passively-evolving population plus a younger burst superimposed at different ages and metallicities. Our main results are summarized below:  - All objects show evidence for multiple components in their line-of-sight velocity distributions.  - In all cases we find that the best-fit model consists of two distinct components: one is dominated by older stars (age>8 Gyr), while the other has intermediate age (1-8 Gyr). - For four out of six targets, the second component shows higher metallicity than the first one.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spatially resolved kinematics and stellar groups of brightest cluster and group galaxies . Abstract : We report spatially - resolved spectroscopic observations for the central regions ( r < 1 kpc ) of six nearby , huge early - class objects in clusters or bands with Mvir > [UNK] .The data were obtained using the Gemini Multi - Object Spectrograph on Gemini North telescope as part of our ongoing program to study the formation histories of these systems . We use the pPXF code to fit the observed spectra with single - single product models consisting of an old passively - changing community plus a later burst superimposed at different ages and metallicities .Our main results are presented below : - All bodies exhibit data for multiple components in their line - of - seeing velocity distributions . - In all situations we find that the best - fitting model consists of two separate components : one is dominated by older stars ( age > 8 Gyr ) , while the other has intermediate age ( 1 - 8 Gyr ) .- For four out of six targets , the second component displays higher metallicity than the first one .",
        "rewrite_text": "We present a detailed analysis of spatially-resolved spectroscopic observations conducted on the central regions (r < 1 kpc) of six massive early-type galaxies located in clusters or groups with virial masses exceeding a certain threshold. These observations were carried out using the Gemini Multi-Object Spectrograph on the Gemini North telescope, as part of our ongoing research aimed at unraveling the formation histories of these significant astronomical systems. To analyze the acquired spectral data, we employed the pPXF code, which allows for the fitting of observed spectra using single stellar population models. These models incorporate an older, passively-evolving stellar population alongside a more recent burst of star formation, each characterized by varying ages and metallicities.\n\nOur findings reveal several key insights into the kinematics and stellar composition of these galaxies. Notably, all observed galaxies exhibit multiple components within their line-of-sight velocity distributions, indicating complex dynamical structures. In every case, the optimal fitting model comprises two distinct components: one predominantly composed of older stars, with ages exceeding 8 billion years, and another consisting of stars of intermediate age, ranging from 1 to 8 billion years. Furthermore, for four out of the six galaxies studied, the intermediate-age stellar component is found to possess a higher metallicity compared to the older population. These results suggest a rich and varied history of star formation and evolution within these massive galaxies, highlighting the importance of spatially-resolved studies in understanding the intricate processes that shape the dynamics and stellar populations of early-type galaxies in dense environments.",
        "ori-fast-z-score": 0.21566554640687682,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 0.7633700367119739
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The BFKL Pomeron Calculus in the dipole approach .\nAbstract:\nThe BFKL equation is an effective theory for describing high-energy scattering processes at small Bjorken-x, where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. The BFKL formalism has been developed into a practical tool to calculate cross sections and structure functions using numerical methods. In this talk I will present recent results on the calculation of the gluon Green s function within the framework of the so-called  dipole approach  which allows us to perform calculations analytically. This method was first introduced by Mueller and Tang in order to study diffractive deep-inelastic scattering (DDIS) off protons. It can be applied also to other processes like heavy quark production in proton-proton collisions as well as photon-photon interactions. We will discuss how we have implemented these ideas numerically and show some preliminary results obtained with our code. Finally, we will comment on possible extensions of this work towards more realistic phenomenological applications.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The BFKL Pomeron Calculus in the dipole approach . Abstract : The BFKL equation is an efficient model for describing high - energy scattering reactions at small Bjorken - x , where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei .The BFKL formalism has been grown into a practical tool to estimate cross sections and structure functions using numerical methods . In this talk I will present recent results on the determination of the gluon Green s function within the framework of the so - called dipole approach which allows us to conduct measurements analytically .This method was first developed by Mueller and Tang in order to study diffractive deep - inelastic reflection ( DDIS ) off protons . It can be applied also to other processes like heavy quark production in proton - proton collisions as well as photon - photon interactions .We will explore how we have integrated these ideas numerically and report some preliminary results acquired with our code . Finally , we will mention on potential extensions of this research towards more realistic phenomenological applications .",
        "rewrite_text": "Title: The BFKL Pomeron Calculus in the Dipole Approach\n\nAbstract: The BFKL equation serves as a powerful framework for analyzing high-energy scattering processes, particularly in the regime of small Bjorken-x, where x represents the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. Over time, the BFKL formalism has evolved into a robust tool for estimating cross sections and structure functions through numerical techniques. In this presentation, I will discuss recent advancements in determining the gluon Green's function using the dipole approach, which facilitates analytical measurements. This methodology, initially introduced by Mueller and Tang, was designed to investigate diffractive deep inelastic scattering (DDIS) off protons. However, its applicability extends to various other processes, including heavy quark production in proton-proton collisions and photon-photon interactions. We will delve into the numerical integration of these concepts and share preliminary results obtained from our computational code. Additionally, we will outline potential avenues for extending this research to more realistic phenomenological scenarios, thereby enhancing our understanding of high-energy scattering phenomena.",
        "ori-fast-z-score": 0.45291081365783825,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": -0.5773502691896257
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Five Intermediate-Period Planets from the N2K Sample .\nAbstract:\nWe report on five new planets discovered by the NASA K2 mission, which were found in the sample of targets observed during Campaigns 1 and 2 (C1/K2). The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 years. We present their discovery light curves as well as follow-up photometry obtained at several observatories around the world. All five objects have been confirmed as planetary-mass companions through radial velocity measurements using high-resolution spectroscopy or precision astrometry. \n \n Keywords: Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby stars - TESS - PLATO - HARPS-N - SPECULOOS \n \n \n \n Five intermediate-period planets from the N2K sample \nThe NASA Kepler space telescope has revolutionized our understanding of extrasolar planets over its primary mission that lasted for four years . However, due to technical difficulties, only about one third of the original target list was actually observed continuously throughout this period. In order to fill out the remaining two-thirds of the original target list, K2 is observing additional fields along the ecliptic plane since 2014 .\nIn this work we report on five new planets detected by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1/K2 ) . These planet candidates are all located close to us , with distances less than 100 parsecs away , and they span orbital periods between three days up to sixteen years . Their masses range from 0 . 5 to 4 times Jupiter  s mass .  \n \n We present here the discovery light curves together with followup photometric observations performed at various observatories worldwide . All these objects have been confirmed as low-mass companions via precise radial-velocity measurements made either with high resolution spectroscopy or with precision astrometry .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Five Intermediate - Period Planets from the N2K Sample . Abstract : We report on five new planets discovered by the NASA K2 spacecraft , which were found in the sample of targets observed during Campaigns 1 and 2 ( C1 / K2 ) .The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 years . We present their discovery light curves as well as follow - up photometry obtained at several observatories around the world .All five objects have been confirmed as planetary - mass companions through radial speed measurements using high - resolution spectroscopy or precision astrometry . Keywords : Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby galaxies - TESS - PLATO - HARPS - N - SPECULOOS Five intermediate - time planets from the N2K sample The NASA Kepler space telescope has revolutionized our knowing of extrasolar stars over its primary mission that lasted for four seasons .However , owing to technical problems , only about one third of the original target list was actually seen continuously throughout this time . In order to fill out the remaining two - quarters of the original target table , K2 is monitoring extra fields along the ecliptic plane since 2014 .In this project we publish on five new planets discovered by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1 / K2 ) . These planet candidates are all located close to us , with distances fewer than 100 parsecs apart , and they span orbital periods between three weeks up to fourteen months .Their masses range from 0 .5 to 4 times Jupiter  s mass .We present here the discovery light curves combined with followup photometric surveys performed at several observatories worldwide . All these objects have been confirmed as low - mass companions via accurate radial - speed measurements made either with high resolution spectroscopy or with accuracy astrometry .",
        "rewrite_text": "We present a detailed overview of five newly discovered exoplanets identified by the NASA K2 mission, specifically from the target sample observed during Campaigns 1 and 2 (C1/K2). These planets are situated within 100 parsecs of Earth and exhibit orbital periods that range from 3 days to 16 years. Our study includes the discovery light curves of these planets, along with follow-up photometric observations conducted at various observatories globally. Each of the five candidates has been confirmed as planetary-mass companions through precise radial velocity measurements obtained via high-resolution spectroscopy or precision astrometry.\n\nThe NASA Kepler space telescope has significantly advanced our understanding of exoplanets during its primary mission, which spanned four years. However, due to technical challenges, only about one-third of the original target list was continuously monitored. To address this gap, the K2 mission has been observing additional fields along the ecliptic plane since 2014. In this context, we report on the identification of five new planets from the K2 dataset, which were discovered among the targets observed in the initial two campaigns. \n\nThese newly identified planet candidates are relatively close to Earth, with distances of less than 100 parsecs, and their orbital periods range from three weeks to fourteen months. Their masses vary from 0.5 to 4 times that of Jupiter. The findings presented here include the light curves associated with their discovery, as well as the results from follow-up photometric surveys conducted at multiple observatories around the world. The confirmation of these objects as low-mass companions was achieved through accurate radial velocity measurements, utilizing either high-resolution spectroscopy or precision astrometry techniques. This research contributes to the growing catalog of exoplanets and enhances our understanding of planetary systems in our cosmic neighborhood. \n\nKeywords: Planetary systems, Discovery methods, Radial velocities, Astrometry, Transits, Exoplanet, K2 Mission, Nearby galaxies, TESS, PLATO, HARPS, N-SPECULOOS.",
        "ori-fast-z-score": 0.5222329678670935,
        "water-fast-z-score": 5.874305293953249,
        "rewrite-fast-z-score": 0.6810052246069989
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Higher K-theory via universal invariants .\nAbstract:\nWe give an explicit description of the higher algebraic K-groups in terms of certain universal cohomology classes, which are defined by using only the ring structure and the unit element of the underlying commutative ring. This is done for any commutative ring with unity R (not necessarily Noetherian). The main result can be formulated as follows: Let M be a module over R. Then there exists a natural isomorphism between the higher algebraic K-groups: \nK_n(R) = Ext^n_R(M, R)\nand the group of all n-fold Massey products on M modulo those that vanish under some suitable finiteness condition. We also show how this theorem leads to a new proof of Quillen s localization theorem. Finally we discuss applications to the study of equivariant K-theory. In particular, we prove that if G is a compact Lie group acting freely on a smooth manifold X then the equivariant K-theory groups of X are isomorphic to the ordinary K-theory groups of the fixed point set X^G.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Higher K - theory via universal invariants . Abstract : We get an explicit description of the higher algebraic K - fields in terms of certain universal cohomology groups , which are specified by using only the ring structure and the unit element of the fundamental commutative field .This is accomplished for any commutative ring with unity R ( not necessarily Noetherian ) . The main consequence can be stated as follows : Let M be a module over R . Then there exists a natural isomorphism between the higher algebraic K - groups : K _ k ( R ) = Ext ^ n _ R ( M , R ) and the group of all k - fold Massey products on M modulo those that vanish under some suitable finiteness requirement .We also demonstrate how this theorem gives to a new proof of Quillen s localization principle . Finally we explain applied to the study of equivariant K - theory .In particular , we prove that if G is a compact Lie set acting freely on a smooth manifold X then the equivariant K - theory groups of X are isomorphic to the ordinary K - theory groups of the fixed point set X ^ G .",
        "rewrite_text": "Title: Higher K-Theory via Universal Invariants\n\nAbstract: In this article, we provide a detailed exploration of higher algebraic K-theory, presenting an explicit characterization of higher K-groups through the lens of universal cohomology groups. Our approach relies solely on the ring structure and the identity element of the fundamental commutative ring, allowing us to extend our findings to any commutative ring with unity R, regardless of whether it is Noetherian. A significant outcome of our work is the establishment of a natural isomorphism between the higher algebraic K-groups, denoted as K_k(R), and the Ext^n_R(M, R) group, where M is a module over R. This isomorphism also encompasses the group of k-fold Massey products on M, subject to specific finiteness conditions. \n\nMoreover, we illustrate how our results yield a novel proof of Quillen's localization theorem, thereby reinforcing the foundational principles of K-theory. Additionally, we delve into the implications of our findings in the context of equivariant K-theory. Specifically, we demonstrate that when G is a compact Lie group acting freely on a smooth manifold X, the equivariant K-theory groups of X can be shown to be isomorphic to the ordinary K-theory groups of the fixed point set X^G. This connection not only enriches the understanding of K-theory in the presence of group actions but also opens avenues for further research in the interplay between algebraic structures and topological invariants. Our results contribute to the broader discourse on K-theory, providing essential insights that bridge algebra and topology.",
        "ori-fast-z-score": 0.1111111111111111,
        "water-fast-z-score": 4.2485291572496005,
        "rewrite-fast-z-score": 0.6704783996548059
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Power-Efficient Direct-Voting Assurance for Data Fusion in Wireless Sensor Networks .\nAbstract:\nIn this work, we propose an efficient direct-voting scheme to provide data fusion assurance (DFA) in wireless sensor networks (WSNs). The proposed DFA scheme is based on the concept that each node can directly vote its local decision with other nodes  decisions and then obtain final global decision by majority voting rule. In order to reduce communication overheads caused by direct voting process, our scheme adopts two novel techniques: 1) only one-hop neighbors are allowed to cast their votes at any time instant; 2) all nodes use different time slots to transmit their votes simultaneously. We also present theoretical analysis to show how much energy consumption will be saved compared with existing schemes. Finally, extensive simulation results demonstrate that our scheme outperforms state-of-the-art approaches in terms of both accuracy and power efficiency. Keywords: Data fusion assurance, Wireless sensor network, Power-efficient direct-voting scheme. This research was supported by National Natural Science Foundation of China under Grant No. 61371040",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Power - Efficient Direct - Voting Assurance for Data Fusion in Wireless Sensor Networks . Abstract : In this project , we propose an efficient direct - voter scheme to provide data fusion assurance ( DFA ) in wireless sensor networks ( WSNs ) .The proposed DFA system is based on the idea that each node can personally voting its local decision with other nodes decisions and then obtain final global decide by majority voting rule . In order to reduce communication overheads caused by direct voting process , our scheme adopts two novel techniques : 1 ) only one - hop neighbors are allowed to cast their votes at any time instant ; 2 ) all nodes use different time slots to transmit their votes concurrently .We additionally offer theoretical analysis to indicate how many heat consumption will be saved compared with existing plans . Finally , extensive modeling results show that our scheme outperforms state - of - the - art methods in terms of both precision and power performance .Keywords : Data fusion assurance , Wireless sensor network , Power - efficient direct - ballot mechanism . This research was supported by National Natural Science Foundation of China under Grant No .61371040",
        "rewrite_text": "In this study, we introduce an innovative direct-voter scheme designed to enhance data fusion assurance (DFA) within wireless sensor networks (WSNs). The core concept of our DFA system revolves around enabling each node to independently cast its vote based on local decisions, subsequently aggregating these votes to reach a final consensus through a majority voting mechanism. To mitigate the communication overhead typically associated with the direct voting process, our approach incorporates two key strategies: first, it restricts voting to only one-hop neighboring nodes at any given moment; second, it allows all nodes to transmit their votes simultaneously using distinct time slots. This dual approach not only streamlines the voting process but also significantly reduces energy consumption. We provide a theoretical analysis demonstrating the potential energy savings of our method compared to existing frameworks. Furthermore, extensive modeling results validate that our proposed scheme surpasses current state-of-the-art techniques in terms of both accuracy and energy efficiency. This research contributes to the ongoing efforts to optimize data fusion processes in WSNs, ensuring reliable and power-efficient operations. The work was supported by the National Natural Science Foundation of China under Grant No. 61371040. \n\nKeywords: Data fusion assurance, Wireless sensor networks, Power-efficient direct-ballot mechanism.",
        "ori-fast-z-score": 0.40406101782088427,
        "water-fast-z-score": 4.924685294770139,
        "rewrite-fast-z-score": 1.0169503597462533
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Parametrized Post-Newtonian Expansion of Chern-Simons Gravity .\nAbstract:\nWe present the first post-newtonian expansion for Chern-Simons gravity, which is an alternative to general relativity that has been proposed as a candidate theory of quantum gravity.  We show how this can be done by using the method developed in Ref.  1  . The resulting expression agrees with previous results obtained within the framework of effective field theories  2  , and it also reproduces the leading-order corrections to Newton s law predicted by Einstein-Cartan gravity  3  .\nThe work presented here was supported by the National Science Foundation under Grant No. PHY-0555310. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. This document is available at http://arxiv.org/abs/1106.5389v1. In recent years there have been several attempts to develop new gravitational theories beyond general relativity (GR)  4  -  8  . One such proposal is ChernSimons gravity  9  , where the action contains higher order curvature terms but no matter fields  10  . It has been shown  11  that Chern-Simons gravity reduces to GR when expanded around flat space-time, while it gives rise to novel effects on cosmological scales  12  . Moreover, Chern-Simons gravity appears naturally in string theory  13  , and it may provide a description of black hole entropy  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Parametrized Post - Newtonian Expansion of Chern - Simons Gravity . Abstract : We present the first post - newtonian expansion for Chern - Simons gravity , which is an alternative to general relativity that has been proposed as a candidate theory of quantum gravitational .We see how this can be performed by using the method developed in Ref . 1 .The resulting expression accepts with previous findings obtained within the framework of effective field theories 2 , and it also reproduces the leading - order corrections to Newton s law predicted by Einstein - Cartan relativity 3 . The project published here was supported by the National Science Foundation under Grant No .PHY - 0555310 . Any opinions , findings , conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation .This document is accessible at http : / / arxiv . org / abs / 1106 . 5389v1 . In recent history there have been numerous attempts to develop new gravity explanations beyond general relativity ( GR ) 4 - 8 .One such proposal is ChernSimons gravity 9 , where the action contains higher order curvature terms but no matter fields 10 . It has been shown 11 that Chern - Simons gravity reduces to GR when accelerated around flat space - time , while it gives rise to novel influences on cosmological scales 12 .Moreover , Chern - Simons gravity appears naturally in string theory 13 , and it could give a description of black hole entropy 14 .",
        "rewrite_text": "We introduce the first post-Newtonian expansion of Chern-Simons gravity, an alternative theory to general relativity (GR) that has been suggested as a potential framework for quantum gravity. This expansion is achieved through the methodology outlined in Reference 1. Our findings align with previous results derived from effective field theories, as documented in Reference 2, and also replicate the leading-order modifications to Newton's law as predicted by Einstein-Cartan relativity, as noted in Reference 3. This research was funded by the National Science Foundation under Grant No. PHY-0555310. The views expressed in this work are solely those of the authors and do not necessarily represent the opinions of the National Science Foundation. The full document can be accessed at http://arxiv.org/abs/1106.5389v1.\n\nIn recent years, there has been a surge of interest in exploring alternative gravitational theories beyond general relativity, as highlighted in References 4 to 8. Chern-Simons gravity, as discussed in Reference 9, is characterized by an action that incorporates higher-order curvature terms without the inclusion of matter fields, as noted in Reference 10. It has been demonstrated in Reference 11 that Chern-Simons gravity reduces to general relativity in the limit of small accelerations around flat spacetime, while also exhibiting unique effects on cosmological scales, as explored in Reference 12. Furthermore, Chern-Simons gravity emerges naturally within the context of string theory, as indicated in Reference 13, and may provide insights into the nature of black hole entropy, as suggested in Reference 14. This work contributes to the ongoing discourse on the viability of Chern-Simons gravity as a significant player in the quest for a comprehensive understanding of gravitational phenomena.",
        "ori-fast-z-score": -0.30151134457776363,
        "water-fast-z-score": 3.68163760377696,
        "rewrite-fast-z-score": 0.1889822365046136
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Black Holes and Large N Species Solution to the Hierarchy Problem .\nAbstract:\nThe Standard Model (SM) is an extremely successful theory, but it has some problems that need to be addressed before we can claim to have found  theory of everything .  The SM does not explain why there are three generations of quarks and leptons with different masses or how neutrinos acquire mass. It also fails to provide any explanation for dark matter in our universe.   In this talk I will discuss my recent work on addressing these issues by introducing new particles into the SM which interact only gravitationally. These new particles are called gravitons and they carry spin-2 quantum number like graviton in general relativity. We show that if gravitons exist then their interactions with other fields lead to generation dependent masses for fermions as well as small Majorana masses for neutrinos. Gravitons also contribute to the energy density of the universe leading to possible solution to the problem of missing dark matter. Finally, we argue that gravitons may play important role in understanding hierarchy between Planck scale and electroweak scale.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Black Holes and Large N Species Solution to the Hierarchy Problem . Abstract : The Standard Model ( SM ) is an incredibly efficient hypothesis , but it has some problems that must to be addressed before we can claim to have found model of things .The SM does not understand why there are three generations of quarks and leptons with various masses or how neutrinos gain mass . It additionally refuses to provide any justification for black material in our universe .In this talk I will explore my current work on addressing these problems by bringing new objects into the SM which interact only gravitationally . These new ions are called gravitons and they carry spin - 2 quantum number like graviton in general relativity .We see that if gravitons exist then their interactions with other fields give to generation dependent masses for fermions as also as low Majorana masses for neutrinos . Gravitons thus help to the power concentration of the universe contributing to possible solution to the issue of missing gray material .Finally , we claim that gravitons may play crucial role in understanding hierarchy between Planck scale and electroweak scale .",
        "rewrite_text": "Title: Black Holes and Large N Species Solution to the Hierarchy Problem\n\nAbstract: The Standard Model (SM) of particle physics is a remarkably successful framework for understanding fundamental interactions; however, it presents several unresolved issues that hinder its completeness. Notably, the SM fails to explain the existence of three generations of quarks and leptons with varying masses, as well as the mechanism by which neutrinos acquire mass. Furthermore, it does not account for the presence of dark matter in the universe. In this presentation, I will discuss my ongoing research aimed at addressing these challenges by introducing new entities into the SM that interact solely through gravitational forces. These entities, referred to as gravitons, possess a spin-2 quantum number analogous to that of gravitons in general relativity. Our findings suggest that if gravitons exist, their interactions with other fields could lead to mass generation that is dependent on the generation of fermions, as well as the emergence of low Majorana masses for neutrinos. Consequently, gravitons may provide insights into the concentration of mass in the universe, potentially offering a resolution to the mystery of dark matter. Additionally, we propose that gravitons could play a pivotal role in elucidating the hierarchy between the Planck scale and the electroweak scale, thereby contributing to a deeper understanding of fundamental physics. This work not only seeks to bridge gaps in the SM but also aims to enhance our comprehension of the universe's structure and the forces that govern it.",
        "ori-fast-z-score": -1.2222222222222223,
        "water-fast-z-score": 6.037034299432969,
        "rewrite-fast-z-score": 0.29559878344928797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Meta-nematic transitions in a bilayer system: Application to the bilayer ruthenate .\nAbstract:\nWe study the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory (DFT) calculations, which show that this material is close to an insulator-metal transition driven by charge transfer between layers. We find that the Fermi surface topology changes dramatically across the metal-insulator boundary, with the appearance of new hole pockets at the Brillouin zone center. The calculated band gap agrees well with experiments on single crystals. In addition, we predict that there are two competing nematic phases near the metal-insulator boundary. One has in-plane anisotropy along the Ru-O-Ru bond direction while another one has out-of-plane anisotropy perpendicular to it. These results provide insights into the origin of the observed structural distortion in bilayer ruthenates. Bilayer ruthenates have attracted considerable attention recently due to their rich physical properties including unconventional superconductivity  1  , quantum criticality  2  , and multiferroicity  3  . Among these materials, Sr3Ru2O7 shows particularly interesting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying pressure  4  .\nIn recent years, several experimental studies have been performed to investigate the nature of the metal-insulator transition (MIT). For example, angle resolved photoemission spectroscopy measurements  5  found that the Fermi surface topology changed significantly when crossing the MIT line. X-ray scattering  6  showed that the crystal symmetry was lowered from tetragonal to orthorhombic below TMI = 160 K. Neutron scattering  7  revealed that the lattice parameters were different for the ab plane and c axis below TMIT ~ 150 K. However, despite extensive investigations, the microscopic mechanism behind the MIT remains unclear  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Meta - nematic transitions in a bilayer system : Application to the bilayer ruthenate . Abstract : We research the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory ( DFT ) observations , which show that this metal is close to an insulator - iron transition driven by charge transfer between layers .We see that the Fermi surface topology changes dramatically across the metal - insulator boundary , with the appearance of new hole pockets at the Brillouin zone center . The measured band gap agrees well with experiments on single crystals .In addition , we expect that there are two rival nematic phases near the metal - insulator boundary . One has in - plane anisotropy along the Ru - O - Ru bond direction while another one has out - of - plane anisotropy diagonal to it .These data provide insights into the origin of the known structural degradation in bilayer ruthenates . Bilayer ruthenates have garnered considerable scrutiny lately owing to their vast physical properties including unconventional superconductivity 1 , quantum criticality 2 , and multiferroicity 3 .Among these materials , Sr3Ru2O7 shows particularly exciting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying force 4 . In recent years , various experimental studies have been performed to examine the nature of the metal - insulator transition ( MIT ) .For instance , angle resolution photoemission spectroscopy observations 5 found that the Fermi surface topology changed significantly when crossing the MIT line . X - ray scattering 6 revealed that the crystal symmetry was dropped from tetragonal to orthorhombic below TMI = 160 K . Neutron scattering 7 revealed that the lattice parameters were change for the ab plane and c axis below TMIT ~ 150 K . However , despite extensive investigations , the microscopic process behind the MIT remains unsure 8 .",
        "rewrite_text": "**Title: Meta-Nematic Transitions in a Bilayer System: Application to the Bilayer Ruthenate**\n\n**Abstract:** This study investigates the phase diagram and electronic structure of the bilayer ruthenate Sr3Ru2O7 through density functional theory (DFT) calculations. Our findings indicate that this metallic compound is situated near a metal-insulator transition (MIT) that is driven by charge transfer between its layers. Notably, we observe a significant alteration in the Fermi surface topology as the system crosses the metal-insulator boundary, characterized by the emergence of new hole pockets at the center of the Brillouin zone. The calculated band gap aligns well with experimental data obtained from single crystal samples, reinforcing the validity of our theoretical approach. Furthermore, we propose the existence of two competing nematic phases in proximity to the MIT. One phase exhibits in-plane anisotropy aligned with the Ru-O-Ru bond direction, while the other displays out-of-plane anisotropy that is diagonal to this direction. These insights contribute to a deeper understanding of the structural degradation phenomena observed in bilayer ruthenates.\n\nBilayer ruthenates have attracted significant attention due to their diverse physical properties, which include unconventional superconductivity, quantum criticality, and multiferroicity. Among these materials, Sr3Ru2O7 is particularly noteworthy as its ground state can be continuously tuned from metallic to insulating through methods such as chemical doping or the application of external pressure. Recent experimental investigations have sought to elucidate the nature of the MIT in this compound. For instance, angle-resolved photoemission spectroscopy has demonstrated substantial changes in the Fermi surface topology across the MIT threshold. Additionally, X-ray scattering studies have shown a transition in crystal symmetry from tetragonal to orthorhombic below the temperature TMI = 160 K, while neutron scattering experiments have indicated alterations in the lattice parameters for both the ab plane and the c-axis below TMIT ~ 150 K. Despite these extensive investigations, the underlying microscopic mechanisms driving the MIT in Sr3Ru2O7 remain elusive, highlighting the need for further research in this area.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 6.11070705734948,
        "rewrite-fast-z-score": 0.9330078226479681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Simulation study of the two-dimensional Burridge-Knopoff model of earthquakes .\nAbstract:\nWe present results on the statistical properties of earthquake sequences generated by the 2D Burridge-Knopff (BK) model with random initial conditions and periodic boundary conditions, using numerical simulations. We find that the BK model produces power-law distributions for both the inter-event time distribution and magnitude-frequency relation in agreement with observations. The exponent values are found to be dependent upon the system size N . In particular we show that the exponents decrease as 1/N , which is consistent with previous studies. Finally, we discuss possible reasons behind this dependence. Keywords: Earthquake statistics; Power laws; Random initial conditions; Periodic boundary conditions; Statistical mechanics; Numerical simulation; Burridge-Knopf model. 1 Introduction It has been known since Gutenberg s work  1  that there exists an empirical relationship between the frequency f of occurrence of earthquakes and their magnitudes M : log10(f ) = α − βM .\n(\nThe constants α and β depend on the region under consideration  2  . This relationship can also be expressed in terms of the number n of events per unit area A within some range  Mmin, Mmax  of magnitudes as: dn/dA ∝ 10 γ−δM min where dn/da ∝ 10 γ−αM max  3  . For example, if one considers all earthquakes occurring over a period of time T in a given geographical area then it follows that: \nwhere Ntot denotes the total number of earthquakes during the observation period T . If one instead counts only those earthquakes whose magnitude lies in the interval  Mmin, Mmax :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Simulation investigation of the two - dimensional Burridge - Knopoff model of earthquakes . Abstract : We report findings on the statistical characteristics of earthquake patterns derived by the 2D Burridge - Knopff ( BK ) model with random initial conditions and regular boundary conditions , using numerical simulations .We see that the BK theory generate power - law functions for both the inter - event time distribution and magnitude - frequency relation in agreement with observations . The exponent values are found to be dependent upon the system size N .In particular we find that the exponents decline as 1 / N , which is consistent with previous research . Finally , we explain possible reasons behind this dependence .Keywords : Earthquake statistics ; Power laws ; Random initial conditions ; Periodic border conditions ; Statistical mechanics ; Numerical modeling ; Burridge - Knopf model . 1 Introduction It has been known since Gutenberg s work 1 that there exists an empirical relationship between the frequency h of occurrence of disasters and their magnitudes M : log10 ( f ) = β − βM .( The constants α and beta depend on the region under consideration 2 . This relationship can also be stated in terms of the number n of events per unit area A within some range Mmin , Mmax of magnitudes as : dn / dA [UNK] 10 γ−δM min where dn / da [UNK] 10 γ−αM max 3 .For instance , if one looks all earthquakes happening over a period of time T in a given geographical region then it follows that : where Ntot describes the total quantity of disasters during the observation era T . If one rather includes only those earthquakes whose magnitude rests in the period Mmin , Mmax :",
        "rewrite_text": "**Title:** Simulation Investigation of the Two-Dimensional Burridge-Knopoff Model of Earthquakes\n\n**Abstract:** In this study, we present a comprehensive analysis of the statistical properties of earthquake patterns generated by the two-dimensional Burridge-Knopoff (BK) model, utilizing numerical simulations with random initial conditions and regular boundary conditions. Our findings indicate that the BK model successfully produces power-law distributions for both the inter-event time and the magnitude-frequency relationship, aligning closely with empirical observations. Notably, we observe that the exponent values associated with these power laws are influenced by the system size, denoted as N. Specifically, our results reveal that these exponents exhibit a decreasing trend proportional to 1/N, a behavior that corroborates previous research in the field. We delve into the underlying mechanisms that may contribute to this observed dependence, providing insights into the dynamics of earthquake occurrence as modeled by the BK framework. This work enhances our understanding of earthquake statistics and the fundamental principles governing seismic activity, highlighting the relevance of statistical mechanics and numerical modeling in the study of natural disasters. \n\n**Keywords:** Earthquake statistics; Power laws; Random initial conditions; Periodic boundary conditions; Statistical mechanics; Numerical modeling; Burridge-Knopoff model. \n\n**1 Introduction:** The empirical relationship between the frequency of seismic events and their magnitudes has been well-documented since the foundational work of Gutenberg. This relationship can be expressed mathematically as log10(f) = β - βM, where the constants α and β are region-specific. Additionally, this relationship can be reformulated in terms of the number of events per unit area within a specified magnitude range, revealing a consistent pattern across different geographical contexts. For instance, when analyzing all earthquakes occurring within a designated timeframe in a specific area, the total number of events, Ntot, can be quantified, providing a framework for understanding the distribution of seismic activity over time and space.",
        "ori-fast-z-score": -0.18257418583505536,
        "water-fast-z-score": 6.2727272727272725,
        "rewrite-fast-z-score": 1.3333333333333333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  2MASS Reveals a Large Intrinsic Fraction of BALQSOs .\nAbstract:\nWe present the results of an analysis of the 2 Micron All Sky Survey (2MASS) data for quasars with broad absorption lines (BALQSOs). We find that about half of all BALQSOs are intrinsically redder than normal QSOs, and that this fraction increases to nearly 80% at z > 3.5. The observed number density evolution is consistent with no luminosity dependence on intrinsic color in the range 10^44 < L(1450A) < 10^46 erg/sec/sr. This result suggests that most BALQSOs have been missed by previous surveys because they were too faint or too blue. If so, then the true space density may be higher than previously estimated. \n \n Keywords: Quasars -- Absorption Lines -- Redshift -- Galaxy Evolution \n \n \n \n 1 Introduction \n \n Broad absorption line quasars (BALQSOs), which show blueshifted absorption features superimposed upon their emission spectra, represent only 10%-20% of optically selected quasar samples but can account for up to 50% of the total UV continuum flux absorbed by intervening gas clouds along the sightline toward distant quasars (Weymann et al., 1991) . In addition to being important probes of the physical conditions within the absorbing gas itself, BALQSOs also provide information regarding the properties of the surrounding intergalactic medium through studies of the associated metal-line systems (e.g., Weymann et al., 1979; Foltz et al., 1986; Turnshek & Savage 1988; Hamann 1998a ,b, 1999 . However, despite their importance as cosmological tools, there has been little progress made in understanding these objects since the discovery of their first examples more than 30 years ago due primarily to selection effects inherent in optical surveys (see e.g., Hewett & Foltz 2003 ) . \n \n Recently, several authors have suggested that many BALQSOs could be found among infrared-selected sources using large-area near-infrared sky surveys such as the Two-Micron All-Sky Survey (2MASS) (Cutri et",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 2MASS Reveals a Large Intrinsic Fraction of BALQSOs . Abstract : We report the conclusion of an assessment of the 2 Micron All Sky Survey ( 2MASS ) statistics for quasars with broad absorption patterns ( BALQSOs ) .We see that about half of all BALQSOs are intrinsically redder than usual QSOs , and that this fraction increases to virtually 80 % at z > 3 . 5 . The observed number density evolution is compatible with no luminosity influence on intrinsic color in the range 10 ^ 44 < L ( 1450A ) < 10 ^ 46 erg / sec / sr .This result suggests that most BALQSOs have been missed by earlier surveys because they were too distant or too blue . If so , then the true space density might be higher than previously predicted .Keywords : Quasars - - Absorption Lines - - Redshift - - Galaxy Evolution 1 Introduction Broad absorption line quasars ( BALQSOs ) , which show blueshifted emission details superimposed upon their absorption spectra , constitute only 10 % - 20 % of optically selected quasar samples but can provide for up to 50 % of the total UV continuum flux reflected by intervening gas clouds along the sightline toward distant quasars ( Weymann et al . , 1991 ) . In addition to being important probes of the physical conditions within the absorbing gas itself , BALQSOs additionally offer information regarding the properties of the nearby intergalactic medium through research of the associated metal - line systems ( e . g . , Weymann et al . , 1979 ; Foltz et al . , 1986 ; Turnshek & Savage 1988 ; Hamann 1998a , b , 1999 .However , despite their importance as cosmological tools , there has been poor advances completed in understanding these objects since the discovery of their early instance more than 30 weeks ago due primarily to selection effects inherent in infrared observations ( see e . g . , Hewett & Foltz 2003 ) . Recently , various authors have suggested that several BALQSOs might be found among infrared - selected sources using big - area near - infrared sky observations such as the Two - Micron All - Sky Survey ( 2MASS ) ( Cutri et",
        "rewrite_text": "**Title:** 2MASS Uncovers a Significant Intrinsic Fraction of Broad Absorption Line Quasars (BALQSOs)\n\n**Abstract:** In this study, we present the findings from our analysis of the 2 Micron All Sky Survey (2MASS) data concerning quasars exhibiting broad absorption lines, known as BALQSOs. Our investigation reveals that approximately 50% of all identified BALQSOs possess intrinsic colors that are redder than those of typical quasars. Notably, this proportion escalates to nearly 80% for quasars at redshifts greater than 3.5. The evolution of the observed number density aligns with the notion that intrinsic color is not significantly influenced by luminosity within the luminosity range of \\(10^{44} < L(1450\\text{Å}) < 10^{46} \\text{erg/sr}\\). This finding implies that a considerable number of BALQSOs may have been overlooked in prior surveys due to their greater distances or bluer colors. Consequently, the actual spatial density of these objects could be higher than earlier estimates suggested.\n\nBroad absorption line quasars, which display blueshifted emission features superimposed on their absorption spectra, represent only 10% to 20% of optically selected quasar samples. However, they can account for up to 50% of the total ultraviolet continuum flux reflected by intervening gas clouds along the line of sight to distant quasars (Weymann et al., 1991). These quasars serve as crucial probes for understanding the physical conditions within the absorbing gas and provide insights into the properties of the surrounding intergalactic medium through the study of associated metal-line systems (e.g., Weymann et al., 1979; Foltz et al., 1986; Turnshek & Savage, 1988; Hamann, 1998a, b, 1999). Despite their significance as cosmological tools, progress in comprehending these objects has been limited since their initial discovery over 30 years ago, largely due to selection biases in infrared observations (Hewett & Foltz, 2003). Recent studies have proposed that a number of BALQSOs may be identified among infrared-selected sources through extensive near-infrared sky surveys, such as the Two-Micron All-Sky Survey (2MASS) (Cutri et al.).",
        "ori-fast-z-score": -0.8980265101338746,
        "water-fast-z-score": 5.879747322073336,
        "rewrite-fast-z-score": -0.7924058156930615
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gemini Spectroscopic Survey of Young Star Clusters in Merging/Interacting Galaxies. II. NGC 3256 Clusters .\nAbstract:\nWe present Gemini GMOS-S spectroscopy for two young star clusters (ages ~10 Myr) in the interacting galaxy pair NGC 3256, which are located at projected distances of 1 kpc and 2 kpc from their respective nuclei. The spectra reveal that both clusters have similar ages but different metallicities; one is metal-rich with  Fe/H  = +0.2 dex while the other has solar metallicity. We find no evidence for multiple populations within either cluster. Using these data we derive masses of 5 x 10^4 M_sol and 7 x 10^3 M_sol respectively for each cluster. These values agree well with those derived using HST photometry. Both clusters show signs of youthful star-formation activity including blue supergiants and Wolf-Rayet stars. In addition to this ongoing star-formation activity, there appears to be an older population of red giant branch stars in the more massive cluster.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Gemini Spectroscopic Survey of Young Star Clusters in Merging/Interacting Galaxies.II.NGC 3256 Clusters . Abstract : We report Gemini GMOS - S spectroscopy for two young galaxy clusters ( ages ~ 10 Myr ) in the interacting galaxy pair NGC 3256 , which are situated at projected speeds of 1 kpc and 2 kpc from their respective nuclei .The spectra indicate that both clusters have related ages but different metallicities ; one is metal - rich with Fe / H = + 0 . 2 dex while the other has solar metallicity . We see no evidence for multiple groups within either cluster .Using these measurements we derive masses of 5 x 10 ^ 4 M _ sol and 7 x 10 ^ 3 M _ sol respectively for each cluster . These values comply good with those generated utilizing HST photometry .Both clusters show signs of young star - formation activity including blue supergiants and Wolf - Rayet stars . In addition to this continuing star - formation activity , there seems to be an older population of red giant branch stars in the more massive cluster .",
        "rewrite_text": "Title: Gemini Spectroscopic Survey of Young Star Clusters in Merging/Interacting Galaxies. II. NGC 3256 Clusters\n\nAbstract: In this study, we present the results of Gemini GMOS-S spectroscopy conducted on two young star clusters located within the interacting galaxy pair NGC 3256, both estimated to be approximately 10 million years old. These clusters are positioned at projected distances of 1 kpc and 2 kpc from their respective galactic nuclei. Our spectral analysis reveals that while both clusters share similar ages, they exhibit distinct metallicity characteristics; one cluster is identified as metal-rich with an iron-to-hydrogen ratio of +0.2 dex, whereas the other cluster displays solar metallicity. Notably, we find no evidence of multiple stellar populations within either cluster. \n\nFrom our spectral measurements, we calculate the masses of the two clusters to be approximately 5 x 10^4 solar masses and 7 x 10^3 solar masses, respectively. These mass estimates are consistent with those derived from Hubble Space Telescope (HST) photometry, reinforcing the reliability of our findings. Both clusters exhibit clear signs of ongoing star formation, evidenced by the presence of blue supergiants and Wolf-Rayet stars. Furthermore, in the more massive cluster, we observe indications of an older stellar population, specifically red giant branch stars, suggesting a complex star formation history. This research contributes to our understanding of the dynamics and evolutionary processes occurring in young star clusters within merging and interacting galaxies, highlighting the intricate interplay between star formation and metallicity in these environments.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 3.204310477123404,
        "rewrite-fast-z-score": -0.5669467095138409
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Keck spectroscopy and Spitzer Space Telescope analysis of the outer disk of the Triangulum Spiral Galaxy M33 .\nAbstract:\nWe present new near-infrared (NIR) spectroscopic observations with Keck II/DEIMOS, which cover the entire optical extent of the nearby spiral galaxy M33 out to its last measured isophote at 25 mag arcsec-2 in B-band. We also use archival data obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our study. The main goal of this work was to investigate how star formation proceeds beyond the edge of galactic disks into the surrounding intergalactic medium. Our results show that there are two distinct components along the line-of-sight towards M33: an extended component associated with diffuse ionized gas and young stars; and a compact component dominated by old stellar populations. Using these NIR spectra we have derived radial profiles of several physical parameters such as electron density, temperature, extinction coefficient etc., across the face-on view of M33 s disk. These profiles reveal interesting trends in the properties of interstellar matter within different regions of the galaxy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Keck spectroscopy and Spitzer Space Telescope examination of the exterior disk of the Triangulum Spiral Galaxy M33 . Abstract : We report new near - infrared ( NIR ) spectroscopic observations with Keck II / DEIMOS , which cover the entire optical extent of the nearby spiral galaxy M33 out to its last detected isophote at 25 mag arcsec - 2 in B - band .We additionally using archival imagery obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our research . The main goal of this project was to examine how star formation flows beyond the margins of galactic disks into the nearby intergalactic medium .Our results show that there are two different components along the line - of - seeing approaching M33 : an extended component associated with diffuse ionized gas and older stars ; and a compact component dominated by aged stellar regions . Using these NIR spectra we have derived radial profiles of several physical factors such as electron concentration , temperature , extinction factor etc . , across the face - on vision of M33 s disk .These features reveal novel trends in the properties of interstellar matter within various regions of the universe .",
        "rewrite_text": "We present new near-infrared (NIR) spectroscopic observations obtained with the Keck II telescope equipped with DEIMOS, which encompass the full optical extent of the nearby spiral galaxy M33, extending to its outermost detected isophote at 25 mag arcsec^-2 in the B-band. In addition, we have utilized archival data from the Infrared Array Camera on the Spitzer Space Telescope to enhance our investigation. The primary objective of this study was to explore the dynamics of star formation as it extends beyond the boundaries of galactic disks into the adjacent intergalactic medium. Our findings reveal the presence of two distinct components along the line of sight toward M33: an extended component characterized by diffuse ionized gas and older stellar populations, and a more compact component that is predominantly composed of aged stellar regions. Through the analysis of these NIR spectra, we have constructed radial profiles for several key physical parameters, including electron density, temperature, and extinction factors, across the face-on view of M33's disk. These profiles uncover significant trends in the characteristics of interstellar matter across various regions of the galaxy, providing new insights into the processes governing star formation and the interaction between galactic and intergalactic environments. Our results contribute to a deeper understanding of the complex interplay between stellar evolution and the surrounding medium, highlighting the importance of examining the outskirts of galaxies to fully comprehend their evolutionary pathways.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.616378963469013,
        "rewrite-fast-z-score": 1.5540573797716226
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An online repository of Swift/XRT light curves of GRBs .\nAbstract:\nWe present the first public release of an archive containing all available X-ray Telescope (XRT) data for Gamma-Ray Bursts observed by the Swift satellite since its launch in November 2004. The XRT is sensitive to soft X-ray photons with energies between 0.3 and 10 keV, and has a field-of-view of 23 x 23 arcminutes. It operates in two modes: Windowed Timing mode which provides high time resolution but low sensitivity; Photon Counting mode which gives higher sensitivity at the expense of temporal information. We have processed all publicly released XRT data into one homogeneous database using standard procedures. This includes both pre-launch calibration observations as well as in-orbit calibrations performed after each observation. For each burst we provide a table listing the basic properties of the event along with the raw count rates measured during the entire exposure. In addition, we also provide a set of FITS files that contain the background subtracted light curve binned on different timescales ranging from 1 second up to several hundred seconds.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An electronic repository of Swift / XRT light curves of GRBs . Abstract : We present the first public update of an archive containing all available X - ray Telescope ( XRT ) statistics for Gamma - Ray Bursts discovered by the Swift satellite since its launch in November 2004 .The XRT is sensitive to soft X - ray photons with energies between 0 . 3 and 10 keV , and has a field - of - view of 23 x 23 arcminutes . It operates in two modes : Windowed Timing mode which offers high time resolution but little sensitivity ; Photon Counting mode which gives higher sensitivity at the cost of temporal info .We have analyzed all openly published XRT data into one homogeneous database utilizing typical techniques . This contains both pre - launch calibration measurements as well as in - orbit calibrations performed after each measurement .For each burst we provide a list listing the fundamental properties of the event along with the raw count levels measured during the entire exposure . In addition , we also create a set of FITS files that include the background subtracted light curve binned on various timescales ranging from 1 second up to several hundred moments .",
        "rewrite_text": "We introduce the first public update of an electronic repository that compiles all available X-ray Telescope (XRT) data for Gamma-Ray Bursts (GRBs) detected by the Swift satellite since its inception in November 2004. The XRT is adept at detecting soft X-ray photons within the energy range of 0.3 to 10 keV and features a field of view measuring 23 x 23 arcminutes. It operates in two distinct modes: the Windowed Timing mode, which provides high temporal resolution but limited sensitivity, and the Photon Counting mode, which enhances sensitivity at the expense of temporal resolution. \n\nIn this update, we have systematically analyzed all publicly available XRT data, consolidating it into a cohesive database using standard data processing techniques. This comprehensive dataset includes both pre-launch calibration measurements and in-orbit calibrations conducted after each observation. For each GRB, we present a detailed list of the event's fundamental characteristics alongside the raw count levels recorded throughout the entire exposure period. \n\nMoreover, we have generated a collection of FITS files that feature background-subtracted light curves, which are binned across various timescales ranging from 1 second to several hundred seconds. This repository serves as a valuable resource for researchers in the field, facilitating further analysis and study of GRBs and their associated X-ray emissions. By making this data publicly accessible, we aim to enhance collaborative research efforts and promote a deeper understanding of the phenomena associated with Gamma-Ray Bursts.",
        "ori-fast-z-score": -2.03701381619181,
        "water-fast-z-score": 3.8376128944009875,
        "rewrite-fast-z-score": -1.1881770515720091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evidence for nonlinear diffusive shock acceleration of cosmic-rays in the 2006 outburst of the recurrent nova RS Ophiuchi .\nAbstract:\nWe report on observations made with Suzaku and Swift during the 2006 eruption of the recurrent nova RS Ophiucchi (RS Oph). The X-ray light curve shows that the source was brightest at around day 50 after the optical peak, when it reached an apparent luminosity of ~10^38 erg s-1. We find evidence for nonthermal emission up to 100 keV by fitting the observed spectrum with a power-law model modified by photoelectric absorption. This is consistent with previous results obtained using data taken with other satellites such as Chandra and XMM-Newton. In addition we found that the photon index changed significantly between days 40-50 and 60-70; this may be due to changes in the physical conditions near the central engine or in the geometry of the emitting region. \n \n We also detected significant hard X-ray emission above 10 keV which can not be explained solely by thermal bremsstrahlung radiation. A possible explanation would be inverse Compton scattering of soft photons off relativistic electrons accelerated in shocks driven into the surrounding medium. If so, then these particles should have been accelerated to energies greater than 1 PeV.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evidence for nonlinear diffusive surge velocity of cosmic - radiation in the 2006 outburst of the recurrent nova RS Ophiuchi . Abstract : We report on observations made with Suzaku and Swift during the 2006 eruption of the recurrent nova RS Ophiucchi ( RS Oph ) .The X - ray light curve shows that the source was brightest at around week 50 after the optical peak , when it attained an apparent luminosity of ~ 10 ^ 38 erg s - 1 . We get data for nonthermal emission up to 100 keV by fitting the seen spectrum with a power - law theory improved by photoelectric diffusion .This is consistent with previous findings obtained using data taken with other satellites such as Chandra and XMM - Newton . In addition we concluded that the photon index changed significantly between weeks 40 - 50 and 60 - 70 ; this might be due to changes in the physical conditions near the main engine or in the topology of the emitting area .We additionally observed significant hard X - ray radiation above 10 keV which can not be described solely by thermal bremsstrahlung emission . A potential explanation may be inverse Compton absorption of soft photons off relativistic electrons accelerated in shocks driven into the nearby medium .If so , then these objects should have been accelerated to energies higher than 1 PeV .",
        "rewrite_text": "We present a comprehensive analysis of the observations conducted with the Suzaku and Swift satellites during the 2006 outburst of the recurrent nova RS Ophiuchi (RS Oph). Our findings reveal that the X-ray light curve peaked approximately 50 weeks after the optical maximum, reaching an impressive luminosity of around 10^38 erg s^-1. By employing a power-law model enhanced by photoelectric diffusion, we successfully captured nonthermal emissions extending up to 100 keV. This result aligns with earlier studies utilizing data from other observatories, including Chandra and XMM-Newton. Notably, we observed a significant variation in the photon index between weeks 40 to 50 and weeks 60 to 70, suggesting potential alterations in the physical environment surrounding the nova's central engine or changes in the configuration of the emitting region. Furthermore, our observations indicated the presence of substantial hard X-ray radiation exceeding 10 keV, which cannot be solely attributed to thermal bremsstrahlung processes. A plausible explanation for this phenomenon could involve the inverse Compton scattering of soft photons by relativistic electrons that are accelerated in shock waves interacting with the surrounding medium. If this hypothesis holds true, it implies that these electrons may have been accelerated to energies surpassing 1 PeV. Our study contributes to the understanding of the complex mechanisms at play during nova eruptions and highlights the significance of high-energy emissions in the context of cosmic radiation dynamics.",
        "ori-fast-z-score": -1.6269784336399213,
        "water-fast-z-score": 5.391638660171921,
        "rewrite-fast-z-score": -1.6750597728792984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Obtaining the spacetime metric from cosmological observations .\nAbstract:\nWe present an algorithm for obtaining the spacetime metric from observational data, such as those obtained by the Planck satellite and other experiments. The method is based on the fact that in general relativity (GR) the Einstein field equations are equivalent to the geodesic equation for test particles. We use this equivalence to obtain the metric tensor components directly from the observed trajectories of photons emitted at different redshifts. This approach allows us to reconstruct the full four-dimensional geometry of space-time without assuming any particular model or parametrization. In order to demonstrate our technique we apply it to simulated data generated using the publicly available code CAMB. Our results show that the recovered metric agrees well with the original one used to generate the mock data. Finally, we discuss possible applications of our method to real astrophysical datasets. Cosmology has entered into precision era thanks to recent advances in experimental techniques which have allowed astronomers to measure many important quantities related to the evolution of the universe. Among these measurements there are the temperature anisotropy power spectrum measured by WMAP  1  , PLANCK  2  and SPT  3  satellites; the baryon acoustic oscillations detected through galaxy surveys  4  ; and the luminosity distance-redshift relation inferred from type Ia supernovae  5  . These new data provide unprecedented opportunities to study fundamental physics beyond the Standard Model  6  .\nIn addition to providing accurate measurements of various physical parameters describing the state of the universe today, modern cosmological experiments also allow us to probe its large-scale structure over time  7, 8  . For example, the measurement of the cosmic microwave background radiation provides information about the early stages of the universe s history when the energy density was dominated by dark matter and radiation  9  . On the other hand, the detection of distant galaxies gives access to the late stage of the universe s expansion when dark energy starts dominating  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Obtaining the spacetime metric from cosmological observations . Abstract : We present an algorithm for acquiring the spacetime metric from observational data , such as those acquired by the Planck satellite and other experiments .The method is based on the fact that in general relativity ( GR ) the Einstein field equations are comparable to the geodesic equation for test particles . We use this equivalence to obtain the metric tensor parts directly from the seen trajectories of photons generated at different redshifts .This method enables us to reconstruct the full four - dimensional topology of space - time without assuming any specific theory or parametrization . In order to test our technique we apply it to modeled information generated using the publicly accessible code CAMB .Our results show that the recovered metric agrees well with the original one used to create the mock data . Finally , we explain possible use of our technique to real astrophysical datasets .Cosmology has entered into precision era thanks to recent developments in experimental methods which have permitted astronomers to measure various crucial quantities related to the evolution of the universe . Among these measurements there are the temperature anisotropy energy spectrum measured by WMAP 1 , PLANCK 2 and SPT 3 spacecraft ; the baryon acoustic oscillations detected through galaxy surveys 4 ; and the luminosity distance - redshift correspondence inferred from type Ia supernovae 5 .These new data provide great opportunities to study theoretical physics beyond the Standard Model 6 . In addition to offering accurate measurements of several physical values describing the state of the universe today , modern cosmological experiments also enable us to probe its large - scale nature over time 7 , 8 .For instance , the observation of the cosmic microwave background radiation presents knowledge about the early stages of the universe s history when the electricity abundance was dominated by black material and radiation 9 . On the other hand , the observation of distant galaxies provides access to the late stage of the universe s evolution when dark energy starts dominating 10 .",
        "rewrite_text": "We introduce a novel algorithm designed to extract the spacetime metric from observational data, leveraging information obtained from missions such as the Planck satellite and other cosmological experiments. This approach is grounded in the principles of general relativity (GR), where the Einstein field equations can be likened to the geodesic equations governing the motion of test particles. By utilizing this relationship, we can derive the components of the metric tensor directly from the observed trajectories of photons emitted at various redshifts. This technique allows for the comprehensive reconstruction of the four-dimensional topology of spacetime without the need for any predetermined theoretical framework or parameterization.\n\nTo validate our methodology, we applied it to simulated datasets generated using the publicly available CAMB code. Our findings indicate a strong correlation between the reconstructed metric and the original metric employed in the creation of the mock data, demonstrating the effectiveness of our approach. Furthermore, we discuss the potential applications of our technique to real astrophysical datasets, which could significantly enhance our understanding of the universe.\n\nThe field of cosmology has entered a precision era, driven by advancements in observational techniques that enable astronomers to measure critical parameters related to the universe's evolution. Key measurements include the temperature anisotropy power spectrum captured by the WMAP, Planck, and SPT satellites, the baryon acoustic oscillations identified through extensive galaxy surveys, and the luminosity distance-redshift relationship derived from observations of type Ia supernovae. These datasets present invaluable opportunities to explore theoretical physics beyond the Standard Model. \n\nModern cosmological experiments not only provide precise measurements of various physical quantities that characterize the current state of the universe but also facilitate investigations into its large-scale structure over time. For example, the cosmic microwave background radiation offers insights into the universe's early history, dominated by matter and radiation, while observations of distant galaxies shed light on the later stages of cosmic evolution, particularly the influence of dark energy.",
        "ori-fast-z-score": 0.7808688094430304,
        "water-fast-z-score": 7.808688094430304,
        "rewrite-fast-z-score": -0.48349377841522817
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-topological solitons in field theories with kinetic self-coupling .\nAbstract:\nWe study the existence and stability properties of non-topological solitons in scalar field theories with kinetic self coupling, which are relevant to models for dark matter particles interacting via self-interactions mediated by light bosons.  We show that stable soliton solutions exist only if the mass of the boson is larger than twice the mass of the dark matter particle. For smaller masses we find unstable solitonic solutions whose lifetime decreases exponentially as the mass ratio approaches one. The results presented here can be used to constrain the parameter space of such models using astrophysical observations. Introduction:-The possibility of new physics beyond the Standard Model (SM) has been widely discussed recently  1  . In particular, there have been many attempts at constructing extensions of the SM that include additional fields or interactions  2  , motivated by the fact that none of its fundamental parameters have yet been measured experimentally  3  .\nIn this work we consider an extension of the SM where the Higgs sector consists of two complex scalars  4  . This model contains several interesting features including spontaneous CP violation  5  , radiative electroweak symmetry breaking  6  , and the presence of a pseudo-Goldstone boson  7, 8  . It also provides a simple framework within which to discuss possible connections between dark matter  9  and neutrino masses  10  . Furthermore it allows us to explore the phenomenology associated with the production of heavy neutral gauge bosons  11  and their subsequent decay into pairs of charged leptons  12  . Finally, it may provide a natural explanation for the origin of baryogenesis  13  through the out-of-equilibrium decays of the heavier scalar  14  .\nOne feature of these models is the presence of a second scalar particle, denoted by H 0 , which mixes with the SM-like Higgs h 0  15  . As a result, both states acquire physical masses m h0 and m H0 respectively  16  . If the mixing angle θH is small then mH ≫ mh  17  . However, even when mH = mh, the couplings of the two scalars differ significantly due to the different quantum numbers carried by each state  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - topological solitons in field theories with kinetic self - interactions . Abstract : We research the existence and stability properties of non - topological solitons in scalar field theories with kinetic self coupling , which are applicable to descriptions for dark matter molecules interacting via self - interactions mediated by light bosons .We see that strong soliton solutions arise only if the mass of the boson is bigger than times the mass of the dark matter object . For lower masses we find unstable solitonic solutions whose lifetime decreases exponentially as the mass ratio approaches one .The results presented here can be used to constrain the parameter room of such theories involving astrophysical observations . Introduction : - The possibility of new science beyond the Standard Model ( SM ) has been widely discussed recently 1 .In particular , there have been many efforts at constructing extensions of the SM that include extra fields or particles 2 , prompted by the fact that none of its essential parameters have ever been measured experimentally 3 . In this research we define an extension of the SM where the Higgs sector consists of two complex scalars 4 .This theory incorporates numerous interesting features including spontaneous CP violation 5 , radiative electroweak symmetry breaking 6 , and the presence of a quasi - Goldstone boson 7 , 8 . It additionally offers a simple context within which to consider likely relationships between dark matter 9 and neutrino masses 10 .Furthermore it allows us to examine the phenomenology linked with the production of large neutral gauge bosons 11 and their ensuing decay into pairs of charged leptons 12 . Finally , it could give a natural explanation for the origin of baryogenesis 13 through the out - of - equilibrium decays of the heavier scalar 14 .One feature of these models is the presence of a second scalar object , denoted by H 0 , which mixes with the SM - like Higgs h 0 15 . As a result , both states attain physical masses m h0 and m H0 respectively 16 .If the mixing angle θH is small then mH [UNK] mh 17 . However , even when mH = mh , the couplings of the two scalars differ significantly due to the different quantum numbers carried by each state 18 .",
        "rewrite_text": "**Title**: Non-topological Solitons in Field Theories with Kinetic Self-Interactions\n\n**Abstract**: This study investigates the existence and stability of non-topological solitons within scalar field theories characterized by kinetic self-interactions. These theories are particularly relevant for modeling dark matter molecules that interact through self-interactions mediated by light bosons. Our findings indicate that robust soliton solutions emerge only when the mass of the boson exceeds a certain threshold, specifically greater than the mass of the dark matter particle. Conversely, for boson masses below this threshold, we observe the emergence of unstable solitonic solutions, whose lifetimes diminish exponentially as the mass ratio approaches unity. The implications of our results are significant, as they provide constraints on the parameter space of these theories, informed by astrophysical observations. \n\nThe introduction of this research highlights the ongoing discourse surrounding potential extensions to the Standard Model (SM) of particle physics. Recent efforts have focused on developing models that incorporate additional fields or particles, driven by the observation that many fundamental parameters of the SM remain unmeasured experimentally. In this context, we propose an extension of the SM featuring a Higgs sector composed of two complex scalar fields. This framework introduces a variety of intriguing phenomena, such as spontaneous CP violation, radiative electroweak symmetry breaking, and the emergence of a quasi-Goldstone boson. Moreover, it facilitates the exploration of potential connections between dark matter and neutrino masses, as well as the phenomenology associated with the production of large neutral gauge bosons and their subsequent decay into charged lepton pairs. Additionally, our model may provide a natural mechanism for baryogenesis through the out-of-equilibrium decays of the heavier scalar field. A notable aspect of this theoretical framework is the presence of a second scalar field, denoted as H₀, which interacts with the SM-like Higgs field, h₀. This interaction leads to the generation of distinct physical masses for both scalar states, mₕ₀ and mₕ₀, with the mixing angle θₕ influencing their mass relationship. Even in scenarios where mₕ₀ equals mₕ₀, the differing quantum numbers of the two scalars result in significant variations in their coupling strengths.",
        "ori-fast-z-score": -1.516365080488482,
        "water-fast-z-score": 5.124100921762788,
        "rewrite-fast-z-score": 1.9215378456610455
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Impact of spin-zero particle-photon interactions on light polarization in external magnetic fields .\nAbstract:\nWe study the effect of spin-one and spin-two particles on the circularly polarized light propagating through an external magnetic field. We show that this effect is determined by the interaction between photons and particles with spins equal to zero, one or two only if the photon energy exceeds some threshold value which depends on the particle mass. For example, for electrons (mass m = 9.11×10-31 kg) it equals to 0.5 MeV. Below this threshold there are no effects caused by higher-spin particles. The results obtained can be used as a basis for developing new methods of studying high-spin particles using optical techniques. DOI: 10.1088/1742-6596/aa6b20\nI. INTRODUCTIO N\nThe problem of describing the propagation of electromagnetic waves in matter has been studied extensively over many years  1  . In particular, the influence of various types of atoms  2  , molecules  3  , ions  4  , plasmas  5  , crystals  6  , etc., on the properties of light was investigated. However, despite numerous studies, the question about how the presence of particles with non-zero spin affects the polarization state of light remains open  7 -9  .\nIn recent decades, interest in such problems increased significantly due to the development of quantum optics  10  . This area includes investigations into the processes occurring when high-energy photons interact with particles having different masses  11  . Such phenomena include Compton scattering  12  , pair production  13  , photo-meson production  14  , etc.. It should also be noted that these processes play an important role in astrophysics  15  , nuclear physics  16  , condensed-matter physics  17  , etc..\nIt follows from the above that the investigation of the impact of particles with nonzero spin on the polarization state of light is relevant both theoretically and experimentally.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Impact of spin - zero particle - photon interactions on light polarization in external magnetic fields . Abstract : We research the impact of spin - one and spin - two ions on the circularly polarized light propagating through an external magnetic field .We see that this effect is chosen by the interaction between photons and atoms with spins equal to zero , one or two only if the photon energy reaches some threshold quantity which depends on the particle weight . For instance , for electrons ( mass m = 9 . 11×10 - 31 kg ) it corresponds to 0 . 5 MeV .Below this threshold there are no impacts produced by higher - spinning waves . The results derived can be used as a framework for building new ways of studying high - spinning objects utilizing optical techniques .DOI : 10 . 1088 / 1742 - 6596 / aa6b20 I . INTRODUCTIO N The problem of describing the propagation of electromagnetic waves in matter has been studied extensively over many years 1 . In particular , the influence of various types of atoms 2 , molecules 3 , ions 4 , plasmas 5 , crystals 6 , etc . , on the properties of light was investigated .However , despite several studies , the question about how the presence of atoms with non - zero spin affects the polarization state of light remains open 7 - 9 . In past decades , interest in such problems intensified substantially due to the development of quantum optics 10 .This area includes research into the mechanisms occurring when high - energy photons react with particles having various masses 11 . Such effects include Compton scattering 12 , pair production 13 , photo - meson production 14 , etc . .It should also be mentioned that these mechanisms play an important role in astrophysics 15 , nuclear science 16 , condensed - matter science 17 , etc . . It follows from the above that the examination of the impact of particles with nonzero momentum on the polarization state of light is relevant both theoretically and experimentally .",
        "rewrite_text": "**Title:** Impact of Spin-Zero Particle-Photon Interactions on Light Polarization in External Magnetic Fields\n\n**Abstract:** This study investigates the effects of spin-one and spin-two ions on the behavior of circularly polarized light as it travels through an external magnetic field. Our findings indicate that the interaction between photons and atoms with zero, one, or two spins significantly influences light polarization, but only when the photon energy surpasses a specific threshold that is contingent upon the mass of the particles involved. For example, in the case of electrons, which have a mass of approximately 9.11 × 10^-31 kg, this threshold is determined to be around 0.5 MeV. Below this energy level, we observe no significant influence from higher-spin waves on the light's polarization state. The implications of our results provide a foundational framework for developing novel optical techniques aimed at studying high-spin objects. \n\nThe propagation of electromagnetic waves in various media has been a subject of extensive research over the years, with particular attention given to how different types of atoms, molecules, ions, plasmas, and crystals affect light properties. Despite numerous investigations, the role of non-zero spin atoms in altering the polarization state of light remains inadequately addressed. Recent advancements in quantum optics have reignited interest in this area, particularly concerning the interactions between high-energy photons and particles of varying masses. Notable phenomena such as Compton scattering, pair production, and photo-meson production exemplify these interactions, which are crucial in fields like astrophysics, nuclear science, and condensed matter physics. Therefore, exploring the influence of particles with non-zero momentum on light polarization is of significant theoretical and experimental importance, paving the way for further research in this dynamic field.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 6.45532593109847,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Abundances of Light Neutron-Capture Elements in Planetary Nebulae II. s-process Enrichments and Interpretation .\nAbstract:\nWe present new abundance determinations for the light neutron-capture elements Sr, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, and Lu in eight planetary nebulae (PNe). The PNe are selected to have high signal-to-noise ratio optical spectra obtained with HST/STIS or FUSE that allow accurate measurements of their elemental abundances. We find that all but one PN show evidence for enrichment by the slow-neutron capture process (s-process) relative to solar values. In most cases we can identify specific s-process contributions from individual isotopes such as 92Zr, 138Ba, 144Sm, 146Eu, 151Gd, 157Dy, 162Yb, 174Lu, 176Hf, 182W, and 205Pb. These results provide important constraints on theoretical models of nucleosynthesis in low-mass asymptotic giant branch stars. \n \n Keywords: Elemental abundances, Planetary nebula",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Abundances of Light Neutron-Capture Elements in Planetary Nebulae II.s-process Enrichments and Interpretation .Abstract : We report new abundance determinations for the light neutron - capture elements Sr , Y , Zr , Ba , La , Ce , Nd , Sm , Eu , Gd , Dy , Yb , and Lu in eight planetary nebulae ( PNe ) . The PNe are chosen to have high signal - to - noise ratio optical spectra obtained with HST / STIS or FUSE that enable precise observations of their elemental abundances .We find that all but one PN show evidence for enrichment by the slow - neutron capture process ( s - process ) relative to solar values . In most cases we can identify specific s - process contributions from individual isotopes such as 92Zr , 138Ba , 144Sm , 146Eu , 151Gd , 157Dy , 162Yb , 174Lu , 176Hf , 182W , and 205Pb .These data provide important restrictions on theoretical models of nucleosynthesis in low - weight asymptotic giant branch stars . Keywords : Elemental abundances , Planetary nebula",
        "rewrite_text": "Title: The Abundances of Light Neutron-Capture Elements in Planetary Nebulae II: s-process Enrichments and Interpretation\n\nAbstract: In this study, we present new measurements of the abundances of light neutron-capture elements, specifically Sr, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, and Lu, across eight selected planetary nebulae (PNe). The PNe were chosen based on the availability of high signal-to-noise ratio optical spectra, which were obtained using the Hubble Space Telescope's Space Telescope Imaging Spectrograph (HST/STIS) or the Far Ultraviolet Spectroscopic Explorer (FUSE). These high-quality spectra allow for precise determination of elemental abundances. Our findings indicate that all but one of the studied PNe exhibit signs of enrichment due to the slow neutron-capture process (s-process) when compared to solar abundance values. In several instances, we can pinpoint specific contributions from individual isotopes, including 92Zr, 138Ba, 144Sm, 146Eu, 151Gd, 157Dy, 162Yb, 174Lu, 176Hf, 182W, and 205Pb. The results of this research impose significant constraints on theoretical models of nucleosynthesis occurring in low-mass asymptotic giant branch stars. This work enhances our understanding of the chemical evolution of the universe and the processes that govern the formation of elements in stellar environments. \n\nKeywords: Elemental abundances, Planetary nebulae, s-process, Nucleosynthesis, Asymptotic giant branch stars.",
        "ori-fast-z-score": 0.40451991747794525,
        "water-fast-z-score": 2.9398736610366685,
        "rewrite-fast-z-score": 2.393172105652397
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measurement of the production of charged pions by protons on a tantalum target .\nAbstract:\nThe measurement was performed at the Cyclotrons and Accelerators Laboratory (CYCLONE) in JINR, Dubna using the proton beam with energy E = 1 GeV. The experiment was carried out to study the pion production in nuclear reactions induced by relativistic protons on nuclei Ta(p, π+). The experimental setup included two scintillation counters S1 and S2 for registration of particles emitted into forward hemisphere, three plastic scintillator detectors S3-S5 for measuring the angular distribution of secondary particles produced in the reaction under investigation. The results obtained are compared with calculations based on the model developed earlier  1  . \nIntroduction\n\nPion production is one of the most important processes in hadronic interactions which play an essential role in many fields such as astrophysics  2  , cosmic ray physics  3  , accelerator technology  4  etc.. In this work we present new data on the pion production in nuclear collisions induced by relativistic protons interacting with nuclei Ta(p,π + ). These measurements were performed at CYCLONE laboratory in JINR-Dubna  5  .\nExperimental Setup\nThe experimental setup used in our experiments consisted of:  -two scintillation counters S1 and S2; -three plastic scintillator detectors; -a set of collimators; -the target made of natural tantalum foil 0.1 mm thick placed between the first pair of scintillation counters; -the trigger system consisting of four scintillation counters T1-T4. \nThe layout of the experimental setup is shown schematically in Fig. 1 . The main parameters of the detector system are listed in Table I . The signals from all detectors were recorded by means of CAMAC modules  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measurement of the production of charged pions by protons on a tantalum target . Abstract : The measurement was done at the Cyclotrons and Accelerators Laboratory ( CYCLONE ) in JINR , Dubna utilizing the proton beam with energy E = 1 GeV .The project was carried out to study the pion production in nuclear compounds caused by relativistic protons on electrons Ta ( p , π + ) . The experimental setup included two scintillation terminals S1 and S2 for registration of grains generated into front hemisphere , three plastic scintillator detectors S3 - S5 for monitoring the angular distribution of primary nuclei generated in the response under research .The results obtained are compared with methods using on the model derived earlier 1 . Introduction Pion production is one of the most important processes in hadronic interactions which work an essential part in different fields such as astrophysics 2 , cosmic ray physics 3 , accelerator science 4 etc . .In this research we present new data on the pion production in nuclear collisions induced by relativistic protons interacting with nuclei Ta ( p , π + ) . These measurements were performed at CYCLONE laboratory in JINR - Dubna 5 .Experimental Setup The experimental setup used in our experiments included of : - two scintillation counters S1 and S2 ; - three plastic scintillator detectors ; - a pair of collimators ; - the target made of natural tantalum foam 0 . 1 mm thick put between the first pair of scintillation counters ; - the trigger system consisting of four scintillation counters T1 - T4 . The configuration of the experimental setup is displayed schematically in Fig .1 . The main variables of the sensor method are listed in Table I .The signals from all detectors were collected by means of CAMAC modules 6 .",
        "rewrite_text": "**Title:** Measurement of Charged Pion Production by Protons on a Tantalum Target\n\n**Abstract:** This study presents a comprehensive investigation into the production of charged pions resulting from proton interactions with a tantalum target, conducted at the Cyclotrons and Accelerators Laboratory (CYCLONE) in Dubna, JINR. Utilizing a proton beam with an energy of 1 GeV, the research aims to enhance our understanding of pion production mechanisms in nuclear interactions, specifically focusing on the reaction Ta (p, π+). The experimental setup was meticulously designed, incorporating two scintillation terminals (S1 and S2) to detect particles generated in the front hemisphere, alongside three plastic scintillator detectors (S3 to S5) to monitor the angular distribution of primary nuclei produced during the interactions.\n\nThe results obtained from this experiment were rigorously compared with existing models derived from previous studies, providing a critical evaluation of pion production in nuclear collisions. Pion production is a fundamental process in hadronic interactions, playing a significant role across various scientific domains, including astrophysics, cosmic ray physics, and accelerator science. This research contributes new data to the existing body of knowledge regarding pion production in nuclear collisions induced by relativistic protons interacting with tantalum nuclei.\n\nThe experimental configuration included essential components such as a pair of collimators, a natural tantalum foam target with a thickness of 0.1 mm positioned between the scintillation counters, and a trigger system comprising four additional scintillation counters (T1 to T4). A schematic representation of the experimental setup is provided in Figure 1, while the main variables of the detection method are detailed in Table I. The signals from all detectors were systematically collected using CAMAC modules, ensuring accurate data acquisition for subsequent analysis. This work not only advances our understanding of pion production but also lays the groundwork for future experiments in the field of nuclear physics.",
        "ori-fast-z-score": 0.7559289460184544,
        "water-fast-z-score": 7.118052168020874,
        "rewrite-fast-z-score": 3.4299717028501764
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A passivity-based stability criterion for a class of interconnected systems and applications to biochemical reaction networks .\nAbstract:\nIn this paper, we present an explicit formula for the maximum allowable delay in a linear time-invariant system with multiple delays by using the concept of passivity index. The proposed method is applied to a biochemical reaction network model consisting of two species interacting through three reactions. We show that our results are consistent with those obtained via numerical simulations. Finally, it should be noted that the proposed approach can also be used as a tool for analyzing other types of networks such as social or economic ones. In recent years there has been growing interest in studying complex dynamical behaviors of biological systems  1  . One important aspect of these studies concerns how different components interact within a cell  2  , which leads naturally to mathematical models based on chemical kinetics  3  .\nThe most common type of kinetic modeling involves ordinary differential equations (ODEs)  4  describing interactions between various molecular species  5  . However, due to the complexity of cellular processes  6  , many ODE models contain several state variables  7, 8  and/or parameters  9  whose values cannot always be determined experimentally  10  . This uncertainty may lead to significant errors when estimating the behavior of the underlying system  11  . To overcome this problem, stochastic approaches have recently been developed  12  . Another possibility consists in considering uncertainties in the form of unknown external disturbances  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A passivity - based security criterion for a class of interconnected networks and applications to biochemical reaction systems . Abstract : In this paper , we present an explicit formula for the maximum allowable delay in a linear time - invariant system with many delays by using the idea of passivity index .The proposed approach is applied to a biochemical reaction network model composed of two organisms evolving through three compounds . We see that our findings are compatible with those achieved via numerical simulations .Finally , it should be mentioned that the suggested approach can also be used as a platform for studying other types of networks such as social or economic ones . In recent years there has been growing interest in examining dynamic dynamical interactions of biological systems 1 .One important dimension of these research concerns how various components connect within a cell 2 , which results naturally to computational models relying on chemical kinetics 3 . The most common type of kinetic modeling involves ordinary differential equations ( ODEs ) 4 describing relationships between various molecular species 5 .However , owing to the complexity of cellular processes 6 , many ODE configurations contain many state components 7 , 8 and / or parameters 9 whose values never always be determined experimentally 10 . This instability may contribute to significant errors when estimating the behavior of the underlying process 11 .To solve this question , stochastic methods have recently been constructed 12 . Another possibility consists in considering uncertainties in the form of unseen external disturbances 13 .",
        "rewrite_text": "In this article, we introduce a novel passivity-based security criterion tailored for a specific category of interconnected networks, with a particular focus on its application to biochemical reaction systems. We derive an explicit formula that delineates the maximum permissible delay in linear time-invariant systems characterized by multiple delays, leveraging the concept of the passivity index. Our methodology is exemplified through a biochemical reaction network model that features two organisms interacting through three distinct compounds. The results obtained from our theoretical framework align closely with those generated through numerical simulations, thereby validating our approach. Furthermore, we highlight the versatility of our proposed criterion, suggesting its potential utility as a foundational tool for investigating various other network types, including social and economic networks.\n\nThe increasing interest in the dynamic interactions within biological systems has spurred extensive research in recent years. A critical aspect of this research pertains to the interconnectivity of cellular components, which naturally leads to the development of computational models grounded in chemical kinetics. Typically, these kinetic models are represented by ordinary differential equations (ODEs) that articulate the relationships among various molecular species. However, the inherent complexity of cellular processes often results in ODE configurations that encompass numerous state variables and parameters, many of which cannot be determined through experimental means. This uncertainty can lead to substantial inaccuracies in predicting the behavior of the underlying biological processes. To address these challenges, recent advancements have introduced stochastic methods that account for such uncertainties. Additionally, we explore the implications of external disturbances that may not be readily observable, further enriching the discourse on the stability and reliability of biochemical network models.",
        "ori-fast-z-score": -1.5085060660073935,
        "water-fast-z-score": 5.788582927426393,
        "rewrite-fast-z-score": -0.34299717028501764
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Einstein in the crossroad of the sciences, arts and humanities .\nAbstract:\nThe conference will be held on September 24-25 at the University of Milan-Bicocca (Italy). The main topics to be discussed are Einstein s role as an intellectual figure; his contribution to science, art and culture; his influence on contemporary society and its future development. \nThe event is organized by the Italian National Committee for UNESCO with the support of the International Union of Pure and Applied Physics (IUPAP), the European Physical Society (EPS) and the Institute of Advanced Studies of the University of Bologna. \n \n The conference aims to celebrate the centenary of Albert Einstein s birth and to promote the dissemination of knowledge about him among scientists, artists, students and the general public. It also intends to foster international cooperation between researchers working in different fields related to Einstein s work.  \n \n The program includes plenary lectures given by internationally renowned speakers, parallel sessions dedicated to specific themes, poster presentations and cultural events such as exhibitions, concerts and performances.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Einstein in the crossroad of the sciences , arts and humanities . Abstract : The meeting will be place on September 24 - 25 at the University of Milan - Bicocca ( Italy ) .The main topics to be mentioned are Einstein s role as an intellectual figure ; his importance to science , art and culture ; his importance on contemporary life and its future development . The event is organized by the Italian National Committee for UNESCO with the backing of the International Union of Pure and Applied Physics ( IUPAP ) , the European Physical Society ( EPS ) and the Institute of Advanced Studies of the University of Bologna .The meeting seeks to mark the centenary of Albert Einstein s birth and to promote the dissemination of research about him among scientists , artists , students and the general public . It additionally hopes to promote international cooperation between researchers working in different fields connected to Einstein s work .The project offers plenary courses offered by widely renowned speakers , concurrent sessions dedicated to different issues , poster lectures and artistic activities such as shows , exhibitions and performances .",
        "rewrite_text": "**Title:** Einstein at the Intersection of Sciences, Arts, and Humanities\n\n**Abstract:** The upcoming conference, titled \"Einstein at the Intersection of Sciences, Arts, and Humanities,\" will be held on September 24-25 at the University of Milan-Bicocca in Italy. This event aims to explore the multifaceted role of Albert Einstein as a pivotal intellectual figure whose contributions extend beyond the realm of physics into the domains of art and culture. The discussions will highlight Einstein's profound influence on contemporary society and its potential future trajectories. Organized by the Italian National Committee for UNESCO, with support from esteemed organizations such as the International Union of Pure and Applied Physics (IUPAP), the European Physical Society (EPS), and the Institute of Advanced Studies at the University of Bologna, this meeting commemorates the centenary of Einstein's birth. \n\nThe conference aspires to foster a deeper understanding of Einstein's legacy by promoting interdisciplinary research and dialogue among scientists, artists, students, and the broader public. It aims to facilitate international collaboration among researchers from diverse fields that intersect with Einstein's work. The program will feature a variety of engaging activities, including plenary sessions led by distinguished speakers, concurrent discussions addressing specific topics, poster presentations, and artistic endeavors such as performances, exhibitions, and shows. Through these initiatives, the event seeks to illuminate the enduring relevance of Einstein's ideas and their implications for various aspects of modern life, thereby enriching the discourse surrounding his contributions to both science and the humanities.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": 1.116880781646981
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modifying quantum walks: A scattering theory approach .\nAbstract:\nWe present an alternative method to the usual Feynman path integral description for calculating the probability amplitudes in quantum walk models, based on the concept of scattering states and their associated S-matrix elements. We show that this new formalism allows us to obtain exact results for several interesting cases where standard methods fail or are not applicable. In particular we consider two different types of boundary conditions at one end of the chain (the origin) which lead to completely different behaviour of the system as time evolves. The first type is known as Dirichlet boundary condition, corresponding to reflecting particles back into the origin after they have left it once; while the second case corresponds to absorbing particles when they reach the origin. For both these cases we calculate exactly the evolution operator over all times t > 0 using our new method. Finally, by applying the inverse Fourier transform to the evolution operator we can recover the full probability distribution function of finding the walker at any position x along the chain at time t.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modifying quantum walks : A scattering theory approach . Abstract : We introduce an additional method to the usual Feynman path integral description for determining the probability amplitudes in particle walk models , using on the idea of scattering states and their accompanying S - vector elements .We see that this new formalism allows us to obtain exact findings for numerous interesting cases where standard methods fail or are not applicable . In particular we treat two different kinds of boundary constraints at one end of the chain ( the origin ) which lead to totally distinct behaviour of the system as time evolves .The first sort is known as Dirichlet boundary relation , analogous to reflecting molecules back into the origin after they have left it once ; while the second case corresponds to absorbing particles when they reach the origin . For both these cases we determine precisely the evolution function over all times t > 0 using our new method .Finally , by using the inverse Fourier transform to the evolution function we can regain the full probability distribution function of finding the walker at any point x along the chain at time t .",
        "rewrite_text": "In this article, titled \"Modifying Quantum Walks: A Scattering Theory Approach,\" we present a novel methodology that enhances the conventional Feynman path integral framework for calculating probability amplitudes in particle walk models. Our approach is grounded in the concept of scattering states and their corresponding S-vector elements, which enables us to derive precise results in various intriguing scenarios where traditional techniques may fall short or prove inapplicable. \n\nWe specifically investigate two distinct types of boundary conditions imposed at one end of the chain, referred to as the origin, which yield markedly different dynamical behaviors as time progresses. The first boundary condition, known as the Dirichlet boundary condition, functions analogously to reflecting particles back to the origin after they have exited. In contrast, the second boundary condition pertains to the absorption of particles upon their arrival at the origin. \n\nFor both boundary scenarios, we meticulously calculate the evolution function for all times \\( t > 0 \\) utilizing our innovative method. This allows us to capture the temporal dynamics of the system with high accuracy. Furthermore, by applying the inverse Fourier transform to the evolution function, we successfully reconstruct the complete probability distribution function that describes the likelihood of locating the walker at any position \\( x \\) along the chain at a given time \\( t \\). Our findings not only expand the theoretical understanding of quantum walks but also provide a robust framework for exploring complex quantum systems under varying boundary conditions.",
        "ori-fast-z-score": 1.0945409092309881,
        "water-fast-z-score": 6.010508596802189,
        "rewrite-fast-z-score": 0.45454545454545453
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The symbiotic star H1-36. A composite model of line and continuum spectra from radio to ultraviolet .\nAbstract:\nWe present new observations in the optical, infrared (IR), and ultraviolet (UV) spectral ranges for the symbiotic binary system H1-36. The analysis is based on high-resolution spectroscopy obtained with the UVES spectrograph at the VLT telescope as well as low resolution data taken by other authors. We find that the observed spectrum can be explained by two components: an accretion disk around a white dwarf and a red giant. In addition we detect emission lines originating in the wind of the red giant. Our results are consistent with previous studies which suggested that this object belongs to the group of symbiotics where the mass transfer proceeds via Roche lobe overflow. \n \n Keywords: Symbiosis, Binary systems, Red giants, Accreting binaries, Winds, Mass loss, Spectroscopy, Ultraviolet radiation, White dwarfs, Emission lines, Stellar winds",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The symbiotic star H1 - 36 . A composite model of line and continuum spectra from radio to ultraviolet .Abstract : We report new images in the optical , infrared ( IR ) , and ultraviolet ( UV ) spectral regions for the symbiotic binary system H1 - 36 . The investigation is based on high - resolution spectroscopy acquired with the UVES spectrograph at the VLT telescope as well as low resolution data taken by other researchers .We see that the seen spectrum can be described by two parts : an accretion disk around a white dwarf and a red dwarf . In addition we find emission lines originating in the wind of the red dwarf .Our results are compatible with previous research which suggested that this body belongs to the group of symbiotics where the mass transfer continues via Roche lobe overflow . Keywords : Symbiosis , Binary systems , Red giants , Accreting binaries , Winds , Mass loss , Spectroscopy , Ultraviolet radiation , White dwarfs , Emission lines , Stellar winds",
        "rewrite_text": "**Title:** The Symbiotic Star H1-36: A Composite Model of Line and Continuum Spectra from Radio to Ultraviolet\n\n**Abstract:** In this study, we present new optical, infrared (IR), and ultraviolet (UV) imaging of the symbiotic binary system H1-36. Our analysis utilizes high-resolution spectroscopy obtained with the UVES spectrograph at the Very Large Telescope (VLT), complemented by low-resolution data from prior studies. The spectral observations reveal a composite structure characterized by two primary components: an accretion disk surrounding a white dwarf and a red dwarf companion. Notably, we identify emission lines that are attributed to the stellar wind emanating from the red dwarf. These findings align with existing literature that classifies H1-36 within the symbiotic star category, where mass transfer occurs through Roche lobe overflow. Our results contribute to a deeper understanding of the dynamics and interactions within this binary system, highlighting the intricate processes of mass loss and accretion that define symbiotic stars. The implications of our research extend to broader topics in stellar evolution, particularly in the context of binary systems involving red giants and white dwarfs. This work underscores the importance of multi-wavelength observations in unraveling the complex nature of such systems and provides a foundation for future studies aimed at exploring the physical mechanisms driving mass transfer and stellar winds in symbiotic binaries. \n\n**Keywords:** Symbiosis, Binary systems, Red giants, Accreting binaries, Winds, Mass loss, Spectroscopy, Ultraviolet radiation, White dwarfs, Emission lines, Stellar winds.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 3.3466401061363023,
        "rewrite-fast-z-score": 2.203000456008648
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Factorization analysis for the fragmentation functions of hadrons containing a heavy quark .\nAbstract:\nWe present an improved factorization formula for the fragmentation functions (FFs) of hadrons containing one heavy quark, which is valid in both leading order and next-to-leading order QCD perturbation theory. The new formula takes into account all possible contributions to the FFs at each perturbative order. We show that our results are consistent with those obtained by using other approaches such as the operator product expansion method or the renormalization group equation approach. Finally we give numerical predictions on some important quantities related to the charm-quark FFs. PACS numbers: 12.38.Qk, 13.25.Gv, 11.15.Tk \nI. INTRODUCTORY REMARK\nThe fragmentation function D(z), where z = Phadron/Pquark , describes how quarks fragment into hadrons when they are produced in hard processes like deep-inelastic scattering  1  . It plays an essential role in understanding many phenomena observed experimentally  2  .\nIn this work, we will study the fragmentation functions of hadronic states containing only one heavy quark. In particular, we consider the case of charmed-meson production in e + e-annihilation processes:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Factorization analysis for the fragmentation functions of hadrons containing a heavy quark . Abstract : We report an updated factorization formula for the fragmentation functions ( FFs ) of hadrons containing one heavy quark , which is valid in both leading order and last - to - leading order QCD perturbation theory .The revised formula takes into consideration all possible contributions to the FFs at each perturbative order . We see that our findings are compatible with those achieved by using other methods such as the operator product expansion method or the renormalization group equation methodology .Finally we give quantitative predictions on some important quantities related to the charm - quark FFs . PACS codes : 12 . 38 . Qk , 13 . 25 . Gv , 11 . 15 . Tk I .INTRODUCTORY REMARK The fragmentation relation D ( z ) , where z = Phadron / Pquark , explains how quarks cluster into hadrons when they are produced in hard processes like deep - inelastic scattering 1 . It plays an essential part in understanding several phenomena observed experimentally 2 .In this study , we will research the fragmentation processes of hadronic states involving only one heavy quark . In particular , we treat the case of charmed - meson production in e + e - annihilation processes :",
        "rewrite_text": "We present an updated factorization formula for the fragmentation functions (FFs) of hadrons that contain a single heavy quark, applicable in both leading order and next-to-leading order QCD perturbation theory. This revised formula comprehensively accounts for all potential contributions to the FFs at each perturbative level. Our results demonstrate consistency with findings derived from alternative approaches, including the operator product expansion and the renormalization group equation methodologies. Additionally, we provide quantitative predictions for several key quantities associated with the charm-quark FFs. \n\nThe fragmentation function D(z), defined as the ratio of the momentum of the produced hadron (Phadron) to that of the originating quark (Pquark), is crucial for understanding how quarks aggregate into hadrons during high-energy processes such as deep-inelastic scattering. This relationship is vital for interpreting various experimental observations. In this investigation, we focus on the fragmentation processes of hadronic states that involve a single heavy quark, specifically examining the production of charmed mesons in electron-positron annihilation events. Our findings contribute to the broader understanding of heavy quark dynamics in QCD and enhance the predictive power of theoretical models in particle physics. The implications of our results extend to various applications in high-energy physics, particularly in the context of hadron production mechanisms and the associated phenomenology. \n\nPACS codes: 12.38.Qk, 13.25.Gv, 11.15.Tk.",
        "ori-fast-z-score": -0.5555555555555556,
        "water-fast-z-score": 5.158920219515446,
        "rewrite-fast-z-score": 0.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Generalization of the Stillinger-Lovett Sum Rules for the Two-Dimensional Jellium .\nAbstract:\nWe present an exact expression for the energy density functional in terms of the local electronic charge and spin densities, which is valid for any number N of electrons on a two-dimensional jellium surface with arbitrary spin-orbit interaction strength. The resulting sum rules are shown to be equivalent to those derived by Stillinger and Lovett (SL) for the case of zero spin-orbit coupling but they also include additional contributions due to this term. We show that these new terms can be expressed as functions of the SL parameters only. This result allows us to obtain explicit expressions for all the relevant physical quantities such as the exchange-correlation potential or the magnetization profile at finite temperature. Finally we discuss how our results could be used to improve existing approximations within Density Functional Theory. PACS: 71.10.Pq - Energy-densities; 72.20.Fd - Energy-density functionals; 73.40.Gk - Spin-polarized systems",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Generalization of the Stillinger - Lovett Sum Rules for the Two - Dimensional Jellium . Abstract : We present an precise representation for the electricity density functional in terms of the local electronic charge and spin densities , which is valid for any number N of electrons on a two - dimensional jellium surface with arbitrary spin - orbit interaction strength .The resulting sum rules are shown to be analogous to those developed by Stillinger and Lovett ( SL ) for the case of zero spin - orbit coupling but they still incorporate additional contributions due to this term . We see that these new terms can be stated as functions of the SL parameters only .This result allows us to obtain precise expressions for all the appropriate physical quantities such as the transfer - correlation potential or the magnetization profile at finite temperature . Finally we talk how our findings may be used to develop existing approximations within Density Functional Theory .PACS: 71.10.Pq - Energy-densities; 72.20.Fd - Energy-density functionals; 73.40.Gk - Spin-polarized systems",
        "rewrite_text": "Title: A Generalization of the Stillinger-Lovett Sum Rules for the Two-Dimensional Jellium\n\nAbstract: In this study, we introduce a comprehensive representation of the electric density functional that incorporates both local electronic charge and spin densities. This representation is applicable to any number of electrons residing on a two-dimensional jellium surface, accommodating arbitrary strengths of spin-orbit interaction. Our findings reveal that the newly derived sum rules exhibit a striking resemblance to those originally formulated by Stillinger and Lovett (SL) in the context of zero spin-orbit coupling. However, our results also account for additional contributions arising from the presence of spin-orbit interaction. Notably, these supplementary terms can be expressed solely in terms of the SL parameters, facilitating a deeper understanding of the underlying physics. This advancement enables us to derive precise formulations for critical physical quantities, including the transfer-correlation potential and the magnetization profile at finite temperatures. Furthermore, we discuss the implications of our results for enhancing existing approximations within Density Functional Theory (DFT), potentially leading to more accurate predictions in the study of electronic systems. Our work not only extends the applicability of the SL sum rules but also provides a framework for future research in the field of condensed matter physics, particularly in the context of spin-polarized systems. The relevance of our findings is underscored by their alignment with the PACS categories: 71.10.Pq (Energy-densities), 72.20.Fd (Energy-density functionals), and 73.40.Gk (Spin-polarized systems).",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 3.4874291623145783,
        "rewrite-fast-z-score": 3.2716515254078793
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  VLBI observations of seven BL Lac objects from RGB sample .\nAbstract:\nWe present Very Long Baseline Array (VLBA) radio images and multi-epoch optical photometry for 7 blazars selected from the Roma-BZCAT catalog as candidates to be members of the Red Giants Branch (RGB). The sources are located at redshifts between 0.1 and 1, with luminosities ranging from 1045 erg/s to 1012 erg/s in the rest frame range 2-10 keV. We find that all but one source show compact cores on milliarcsecond scales; only RGB J0152+017 shows an extended structure. All these results suggest that most of our targets belong to the class of Flat Spectrum Radio Quasars rather than Blazar-like AGNs. In addition we report new spectroscopic data obtained by us or taken from literature which confirm this hypothesis. \n \n Keywords: Blazars, VLBI, Optical variability, X-ray emission, Red giants branch",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : VLBI studies of seven BL Lac objects from RGB survey . Abstract : We report Very Long Baseline Array ( VLBA ) broadcast photographs and multi - epoch optical photometry for 7 blazars chosen from the Roma - BZCAT catalog as candidates to be members of the Red Giants Branch ( RGB ) .The sources are found at redshifts between 0 . 1 and 1 , with luminosities ranging from 1045 erg / s to 1012 erg / s in the remainder frame range 2 - 10 keV . We see that all but one reference show compact cores on milliarcsecond scales ; only RGB J0152 + 017 shows an extended structure .All these results show that most of our sources belong to the class of Flat Spectrum Radio Quasars rather than Blazar - like AGNs . In addition we publish new spectroscopic data received by us or taken from literature which confirm this hypothesis .Keywords : Blazars , VLBI , Optical variability , X - ray radiation , Red giants branch",
        "rewrite_text": "In this study, we present findings from Very Long Baseline Array (VLBA) observations and multi-epoch optical photometry of seven blazars selected from the Roma-BZCAT catalog, identified as potential members of the Red Giants Branch (RGB). The blazars examined exhibit redshifts ranging from 0.1 to 1, with luminosities spanning from 10^45 erg/s to 10^12 erg/s within the rest-frame energy range of 2-10 keV. Our analysis reveals that all but one of the sources display compact cores on milliarcsecond scales, with the exception of RGB J0152+017, which exhibits an extended structure. These findings suggest that the majority of the sources in our sample are more accurately classified as Flat Spectrum Radio Quasars rather than typical Blazar-like Active Galactic Nuclei (AGNs). Furthermore, we provide new spectroscopic data, either collected by our team or sourced from existing literature, which supports this classification. The implications of our results contribute to the understanding of the nature and characteristics of these blazars, particularly in relation to their optical variability and X-ray emissions. This research enhances the existing knowledge of the RGB and its associated objects, offering insights into the broader context of blazar studies and their classification within the astrophysical community. Our work underscores the importance of multi-wavelength observations in elucidating the properties of these enigmatic sources. \n\nKeywords: Blazars, VLBI, Optical variability, X-ray radiation, Red Giants Branch.",
        "ori-fast-z-score": -0.4120816918460671,
        "water-fast-z-score": 4.354648431614539,
        "rewrite-fast-z-score": 1.3093073414159544
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Polarization sensitive spectroscopy of charged Quantum Dots .\nAbstract:\nWe report on the polarization dependent photoluminescence (PL) and Raman scattering spectra of CdSe/CdS core/shell quantum dots in solution at room temperature. The PL spectrum shows that the emission is polarized along the direction perpendicular to the excitation light, which can be explained by the selection rules for dipole transitions between electronic states with different angular momenta. In addition we observe an anisotropic broadening of the Stokes linewidths as well as a splitting into two components when exciting circularly polarized light. These effects are attributed to the presence of exciton fine structure due to spin-orbit coupling. We also find evidence for a strong electron-phonon interaction leading to phonon sidebands in both the Stokes and anti-Stokes parts of the Raman spectrum. Finally, we show how these results can be used to determine the orientation of individual QDs embedded in a polymer matrix. Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Polarization sensitive spectroscopy of charged Quantum Dots . Abstract : We report on the polarization dependent photoluminescence ( PL ) and Raman absorption spectra of CdSe / CdS core / shell quantum dots in solution at room temperature .The PL spectrum reveals that the emission is polarized along the direction perpendicular to the excitation light , which can be described by the selection rules for dipole changes between electronic states with various angular momenta . In addition we exhibit an anisotropic broadening of the Stokes linewidths as well as a dividing into two parts when exciting circularly polarized light .These effects are traced to the presence of exciton fine structure owing to spin - orbit bonding . We additionally find proof for a powerful atom - phonon interaction leading to phonon sidebands in both the Stokes and anti - Stokes parts of the Raman spectrum .Finally , we show how these results can be used to predict the orientation of individual QDs integrated in a polymer matrix . Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy .",
        "rewrite_text": "In this study, we investigate the polarization-sensitive photoluminescence (PL) and Raman absorption spectra of CdSe/CdS core/shell quantum dots (QDs) in solution at ambient temperature. Our findings reveal that the PL emission is polarized in a direction orthogonal to that of the excitation light. This polarization behavior can be explained by the selection rules governing dipole transitions between electronic states characterized by different angular momenta. Furthermore, we observe an anisotropic broadening of the Stokes linewidths, which exhibit a bifurcation when subjected to circularly polarized excitation. These phenomena are attributed to the exciton fine structure resulting from spin-orbit coupling within the quantum dots. \n\nAdditionally, we provide evidence of significant atom-phonon interactions, which manifest as phonon sidebands in both the Stokes and anti-Stokes regions of the Raman spectrum. This interaction highlights the intricate coupling between electronic states and lattice vibrations in these nanostructures. Our results not only enhance the understanding of the optical properties of charged QDs but also have practical implications for the orientation determination of individual quantum dots embedded in a polymer matrix. To validate our findings, we conducted polarized luminescence measurements on single QD emitters using confocal microscopy techniques. This research contributes to the growing body of knowledge on the optical characteristics of quantum dots and their potential applications in advanced photonic devices.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": 1.856558243265828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gaining analytic control of parton showers .\nAbstract:\nWe present an algorithm for the numerical evaluation of Feynman diagrams with arbitrary numbers of external particles and internal loops, which is based on the concept of  partonic subdiagrams . The method allows to perform calculations in QCD beyond leading order accuracy without any approximations or assumptions about the kinematics of the process under consideration. We demonstrate its applicability by calculating the next-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. In this talk we will discuss how one can gain analytic control over parton showers using the concept of  partons  as fundamental degrees of freedom. This approach has been developed recently within the framework of Soft-Collinear Effective Theory (SCET)  1  . It provides a systematic way to resum large logarithms associated with collinear splittings into multiple jets  2  , thereby improving our understanding of jet physics  3  .\nThe basic idea behind SCET is that physical observables are described by matrix elements involving soft and/or collinear fields only  4  . These fields have nontrivial transformation properties under boosts along the beam axis  5  . They allow us to separate hard interactions from soft radiation  6  . As a result, it becomes possible to systematically factorize contributions to scattering amplitudes into  hard functions  describing short-distance dynamics  7, 8  and  semi-hard functions  encoding information about the emission of soft gluons  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gaining analytic control of parton showers . Abstract : We present an algorithm for the numerical identification of Feynman diagrams with arbitrary numbers of external molecules and internal loops , which is based on the idea of partonic subdiagrams .The method enables to conduct measurements in QCD beyond trailing order efficiency without any approximations or assumptions about the kinematics of the process under consideration . We suggest its applicability by calculating the second - to - leading - order corrections to the production cross section of large quarks at hadron colliders .In this talk we will explore how one can acquire analytic control over parton showers using the idea of partons as essential degrees of liberty . This method has been used lately within the framework of Soft - Collinear Effective Theory ( SCET ) 1 .It provides a comprehensive way to resum big logarithms associated with collinear splittings into multiple jets 2 , thereby improving our knowing of jet physics 3 . The basic idea behind SCET is that physical observables are explained by matrix elements containing soft and / or collinear fields only 4 .These fields have nontrivial transformation qualities under boosts along the beam axis 5 . They allow us to separate hot interactions from soft light 6 .As a result , it becomes possible to thoroughly factorize contributions to scattering amplitudes into hard functions describing short - distance dynamics 7 , 8 and semi - hard functions encoding information about the emission of soft gluons 9 .",
        "rewrite_text": "We introduce a novel algorithm designed for the numerical identification of Feynman diagrams featuring an arbitrary number of external particles and internal loops, leveraging the concept of partonic subdiagrams. This innovative approach facilitates measurements in Quantum Chromodynamics (QCD) that surpass the limitations of leading order efficiency, all while avoiding any approximations or assumptions regarding the kinematics of the processes being analyzed. To demonstrate the utility of this method, we compute the second-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. \n\nIn this presentation, we will delve into how one can achieve analytic control over parton showers by treating partons as fundamental degrees of freedom. This technique has recently been applied within the framework of Soft-Collinear Effective Theory (SCET), which offers a systematic way to resum large logarithms associated with collinear splittings into multiple jets, thereby enhancing our understanding of jet physics. The core principle of SCET posits that physical observables can be expressed through matrix elements that incorporate only soft and/or collinear fields. These fields exhibit complex transformation properties under boosts along the beam axis, enabling a clear distinction between hard interactions and soft emissions. \n\nConsequently, this framework allows for a comprehensive factorization of contributions to scattering amplitudes into hard functions that encapsulate short-distance dynamics and semi-hard functions that convey information about the emission of soft gluons. This advancement not only enriches our theoretical toolkit but also paves the way for more precise predictions in high-energy particle physics, particularly in the context of jet production and related phenomena at colliders.",
        "ori-fast-z-score": 0.29559878344928797,
        "water-fast-z-score": 6.667948594698258,
        "rewrite-fast-z-score": 1.671258043593467
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum key distribution over 40 dB channel loss using superconducting single photon detectors .\nAbstract:\nWe report on the first demonstration of quantum key distribution (QKD) with high bit rates and low error rates in an optical fiber link spanning more than 100 km, including 20 km of standard telecom fibers and 80 km of dispersion-shifted fibers. The QKD system uses polarization encoding and decoy-state BB84 protocol to generate secret keys at a rate up to 1 Mbit/s per user. We use two types of single-photon detectors based on InGaAs/InP avalanche photodiodes operated either in Geiger mode or as gated-mode single-photon counters. To overcome the detector dark count noise we employ active feed-forward techniques that allow us to achieve a secure key generation rate of 0.5 Mbit/s for each user. This is the highest secure key generation rate reported so far for QKD systems operating beyond 50 km transmission distance. Quantum Key Distribution (QKD), which allows two remote users to share a secret key by exchanging quantum states through insecure channels  1  , has attracted great interest recently due to its potential applications in both military and commercial fields  2  . However, most existing QKD experiments are limited to short-distance transmissions because of the extremely weak intensity of single photons  3  .\nRecently, several groups have demonstrated QKD over distances longer than 50km  4  -  8  . These demonstrations were made possible thanks to the development of efficient single-photon detectors  9  -  11  and advanced data post-processing algorithms  12  -  14  . Nevertheless, these results still suffer from relatively low key generation rates mainly caused by the large dark counts of the employed single-photon detectors  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum key distribution over 40 dB network loss using superconducting single photon detectors . Abstract : We report on the first test of quantum key distribution ( QKD ) with high bit rates and low mistake rates in an optical cable link spanning more than 100 km , comprising 20 km of standard telecom strands and 80 kilometres of dispersion - shifted strands .The QKD system utilizes polarization encoding and decoy - state BB84 protocol to create secret keys at a rate up to 1 Mbit / s per user . We use two forms of single - photon detectors based on InGaAs / InP avalanche photodiodes running either in Geiger mode or as gated - mode multi - photon counters .To solve the detector black count noise we utilize active feed - forward strategies that enable us to achieve a secure key production frequency of 0 . 5 Mbit / s for each operator . This is the highest secure key production level reported so far for QKD units running beyond 50 km communication distance .Quantum Key Distribution ( QKD ) , which allows two distant operators to exchange a secret key by exchanging quantum states through insecure channels 1 , has garnered great popularity lately owing to its potential applications in both military and commercial fields 2 . However , most existing QKD tests are limited to short - distance broadcasts because of the exceptionally weak intensity of double photons 3 .Recently , various groups have demonstrated QKD over distances greater than 50km 4 - 8 . These demonstrations were made possible due to the development of effective single - photon detectors 9 - 11 and advanced information post - processing algorithms 12 - 14 .Nevertheless , these results nonetheless result from fairly little key production rates mainly caused by the huge dark counts of the utilized single - photon detectors 15 .",
        "rewrite_text": "We present the inaugural experimental demonstration of quantum key distribution (QKD) over an extensive optical fiber link exceeding 100 kilometers, which includes 20 kilometers of standard telecom fibers and 80 kilometers of dispersion-shifted fibers. This QKD system employs polarization encoding in conjunction with the decoy-state BB84 protocol, enabling the generation of secret keys at rates of up to 1 Mbit/s per user. Our approach incorporates two types of single-photon detectors, specifically InGaAs/InP avalanche photodiodes, which operate in either Geiger mode or as gated-mode multi-photon counters. To mitigate the impact of detector dark count noise, we implement active feed-forward strategies, resulting in a secure key production rate of 0.5 Mbit/s for each user. This achievement marks the highest secure key production rate reported for QKD systems operating over communication distances greater than 50 kilometers.\n\nQuantum Key Distribution (QKD) has gained significant traction due to its ability to facilitate secure key exchanges between distant parties by transmitting quantum states through potentially insecure channels, making it highly relevant for both military and commercial applications. Despite its promise, many existing QKD implementations have been constrained to short-distance transmissions due to the inherent challenges posed by the weak intensity of entangled photon pairs. Recent advancements have enabled successful QKD demonstrations over distances exceeding 50 kilometers, attributed to the development of sophisticated single-photon detectors and enhanced information post-processing techniques. However, these advancements have often resulted in limited key production rates, primarily due to the high dark count rates associated with the employed single-photon detectors. Our work addresses these challenges, paving the way for more efficient and secure quantum communication over long distances.",
        "ori-fast-z-score": -0.8219949365267865,
        "water-fast-z-score": 7.3484692283495345,
        "rewrite-fast-z-score": -0.4123930494211613
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlinear Evolution of Baryon Acoustic Oscillations .\nAbstract:\nWe study the nonlinear evolution of baryonic acoustic oscillations (BAO) in an expanding universe using numerical simulations with N-body particles and hydrodynamic gas dynamics. We find that BAOs are suppressed by nonlinear gravitational clustering, but they can be recovered by applying a Wiener filter to the density field at each redshift. The reconstructed power spectrum agrees well with linear theory predictions on large scales where the effect is negligible. On small scales, however, we observe significant deviations between the two spectra due to nonlinear effects such as shell crossing and virialization. These results suggest that it may be possible to use BAOs as standard rulers for cosmological distance measurements even when the underlying matter distribution becomes highly nonlinear. In this work, we present our main findings below. \n1 Introduction\n\nBaryons play important roles in galaxy formation through their interactions with dark matter. For example, observations show that galaxies form around peaks of the primordial density fluctuations which grow into massive halos via gravitational instability. Therefore, understanding how baryons evolve in time and space is crucial for studying galaxy formation processes.\n\nIn recent years, there has been growing interest in measuring the large-scale structure of the Universe using baryonic tracers like neutral hydrogen or stars. One promising method involves tracing the spatial distribution of these objects back in time using spectroscopic surveys. This technique allows us to measure the statistical properties of the cosmic web, including its geometry and topology, over a wide range of redshifts. \n\nThe most prominent feature observed in the measured correlation functions of various types of baryonic tracers is known as  baryonic acoustic oscillation  (BAO). It refers to periodic wiggles seen in the power spectrum of the tracer population caused by sound waves propagating through the early universe before decoupling  see e.g., 1  . Since the amplitude of the BAO signal depends only weakly on the physical state of the medium, it provides a robust way to probe the expansion history of the universe independent of other cosmological parameters  2  .\nRecently, several groups have reported detections of the BAO signature in the correlation function of Lyman",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlinear Evolution of Baryon Acoustic Oscillations . Abstract : We research the nonlinear development of baryonic sound oscillations ( BAO ) in an increasing universe using numerical simulations with N - bodies particles and hydrodynamic gas mechanics .We see that BAOs are suppressed by nonlinear gravity clustering , but they can be recovered by using a Wiener filter to the density field at each redshift . The reconstructed power spectrum agrees well with continuous theory estimates on huge scales where the impact is negligible .On narrow scales , however , we find considerable deviations between the two spectra due to nonlinear effects such as shell crossing and virialization . These data suggest that it could be possible to use BAOs as conventional rulers for cosmological distance measurements even when the underlying matter distribution appears extremely nonlinear .In this research , we present our major findings below . 1 Introduction Baryons serve important roles in universe formation through their interactions with black material .For instance , observations show that galaxies form around spikes of the primordial density fluctuations which build into huge halos via gravitational instability . Therefore , studying how baryons grow in time and space is crucial for studying galaxy formation cycles .In past times , there has been growing interest in measuring the huge - scale structure of the Universe using baryonic tracers like neutral hydrogen or stars . One promising means includes tracing the spatial distribution of these objects back in time utilizing spectroscopic studies .This method enables us to measure the statistical characteristics of the cosmic web , notably its topology and topology , over a broad variety of redshifts . The most notable feature observed in the measured correlation functions of several kinds of baryonic tracers is known as baryonic sound oscillation ( BAO ) .It refers to periodic wiggles found in the power spectrum of the tracer population induced by sound signals propagating through the early universe before decoupling see e . g . , 1 . Since the frequency of the BAO signal relies only faintly on the physical state of the medium , it gives a reliable way to probe the development period of the universe independent of other cosmological factors 2 .Recently , various groups have reported detections of the BAO signature in the correlation function of Lyman",
        "rewrite_text": "**Title: Nonlinear Evolution of Baryon Acoustic Oscillations**\n\n**Abstract:** This study investigates the nonlinear evolution of baryon acoustic oscillations (BAO) within an expanding universe, utilizing numerical simulations that incorporate N-body particles and hydrodynamic gas dynamics. Our findings indicate that while nonlinear gravitational clustering tends to suppress BAOs, these oscillations can be effectively recovered by applying a Wiener filter to the density field at each redshift. The reconstructed power spectrum aligns closely with theoretical predictions on large scales, where the influence of nonlinearity is minimal. However, on smaller scales, we observe significant discrepancies between the reconstructed and theoretical spectra, attributed to nonlinear phenomena such as shell crossing and virialization. These results imply that BAOs may still serve as reliable standard rulers for cosmological distance measurements, even in regions where the underlying matter distribution exhibits strong nonlinearity.\n\nIn the introduction, we highlight the critical role baryons play in the formation of the universe, particularly through their interactions with dark matter. Observational evidence suggests that galaxies form around peaks in primordial density fluctuations, which evolve into massive halos via gravitational instability. Thus, understanding the temporal and spatial growth of baryons is essential for comprehending the processes of galaxy formation. Recent interest has surged in measuring the large-scale structure of the universe using baryonic tracers, such as neutral hydrogen and stars. One effective approach involves tracing the spatial distribution of these tracers over time through spectroscopic studies, allowing for the assessment of the cosmic web's statistical properties, including its topology across various redshifts.\n\nA prominent feature in the correlation functions of various baryonic tracers is the baryon acoustic oscillation (BAO), characterized by periodic fluctuations in the power spectrum of the tracer population. These oscillations are a result of sound waves propagating through the early universe prior to decoupling. Since the frequency of the BAO signal is only weakly dependent on the medium's physical state, it provides a robust method for probing the universe's expansion history, independent of other cosmological influences. Recent studies have reported the detection of the BAO signature in the correlation functions of Lyman-alpha forests, further emphasizing the significance of BAOs in cosmological research.",
        "ori-fast-z-score": -2.1105794120443453,
        "water-fast-z-score": 8.493600116431967,
        "rewrite-fast-z-score": -0.23354968324845687
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Four quasars above redshift 6 discovered by the Canada-France High-z Quasar Survey .\nAbstract:\nWe report on four new quasars at redshifts z > 6, found in the Canada France Hawaii Telescope Legacy Survey (CFHTLS). The objects were selected as part of an ongoing survey for high-z quasars using photometric data obtained with CFHT and Spitzer Space Telescope. We present their optical to near-infrared SEDs, which are well fitted by composite quasar templates. Their luminosities range between 1.5 x 10^14 erg s-1 cm-2 and 2.1 x 10^15 erg s-1 cm-2 . These results show that there is still room for discovering very luminous quasars beyond redshift six. They also provide further evidence that supermassive black holes grew rapidly during this early phase of galaxy formation. Four quasars have been discovered at redshifts greater than 6 in the Canada France Hawaii telescope legacy survey (CFHTLS) by combining deep infrared observations taken with the Spitzer space telescope with optical data collected with the Canada France Hawaii telescope.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Four quasars above redshift 6 identified by the Canada - France High - z Quasar Survey . Abstract : We report on four newest quasars at redshifts z > 6 , located in the Canada France Hawaii Telescope Legacy Survey ( CFHTLS ) .The bodies were chosen as part of an continuing survey for high - z quasars using photometric data acquired with CFHT and Spitzer Space Telescope . We present their optical to near - infrared SEDs , which are better fitted by composite quasar templates .Their luminosities range between 1 . 5 x 10 ^ 14 erg s - 1 cm - 2 and 2 . 1 x 10 ^ 15 erg s - 1 cm - 2 . These data demonstrate that there is already room for finding very luminous quasars beyond redshift six .They addition offer further evidence that supermassive black holes grew rapidly during this first phase of galaxy formation . Four quasars have been detected at redshifts greater than 6 in the Canada France Hawaii telescope heritage survey ( CFHTLS ) by combining dark infrared observations made with the Spitzer space telescope with imaging information collected with the Canada France Hawaii telescope .",
        "rewrite_text": "We present a detailed report on the discovery of four newly identified quasars with redshifts exceeding 6, as part of the ongoing Canada-France High-z Quasar Survey. These quasars were selected from the extensive data obtained through the Canada-France Hawaii Telescope Legacy Survey (CFHTLS), utilizing both photometric observations from the CFHT and infrared data from the Spitzer Space Telescope. Our analysis includes the spectral energy distributions (SEDs) of these quasars, which are more accurately represented by composite quasar templates, indicating their unique characteristics in the high-redshift universe. The luminosities of the identified quasars span a range from 1.5 x 10^14 erg s^-1 cm^-2 to 2.1 x 10^15 erg s^-1 cm^-2, highlighting their significant brightness and the potential for discovering even more luminous quasars at redshifts greater than six. This finding contributes to the growing body of evidence suggesting that supermassive black holes underwent rapid growth during the early stages of galaxy formation. The detection of these four quasars, achieved through a combination of deep infrared observations from the Spitzer Space Telescope and optical imaging from the CFHT, underscores the effectiveness of multi-wavelength approaches in uncovering the mysteries of the early universe. Our results not only enhance the catalog of known high-redshift quasars but also provide valuable insights into the formation and evolution of supermassive black holes in the context of cosmic history.",
        "ori-fast-z-score": -0.6201736729460423,
        "water-fast-z-score": 5.41602560309064,
        "rewrite-fast-z-score": 1.3627702877384937
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Novel technique for monitoring the performance of the LAT instrument on board the GLAST satellite .\nAbstract:\nThe Large Area Telescope (LAT) is one of two instruments aboard NASA s Fermi Gamma-ray Space Telescope, launched in June 2008. The LAT consists of an array of silicon strip trackers and cesium iodide calorimeters that detect gamma rays with energies between 20 MeV to more than 300 GeV. This document describes a novel method used by the LAT collaboration to monitor the performance of its detector system during flight using cosmic ray data taken over several months prior to launch. We show how this method can be applied to characterize the response function of each individual tracker module as well as the overall energy resolution of the entire LAT. These results are compared against ground calibration measurements performed before launch. Finally we demonstrate how these techniques have been successfully employed to identify problems with some modules after launch which were subsequently fixed through software updates. The Large Area Telescope (L AT ) is one of two instruments flown on NASA s Fermi Gamma-Ray Space Telescope  1  . Launched into space in June 2008, it has detected thousands of sources of high-energy photons since then  2  .\nIn order to perform such observations, the L AT must accurately measure the direction and energy of incoming photons. To accomplish this task, the L AT uses a combination of silicon strip detectors and CsI(Tl) scintillators arranged in four layers around a central tungsten converter foil  3  , see Figure 1 . Each layer contains 16 towers, or  trajectory segments , consisting of 4 silicon strips oriented at different angles relative to the incident photon trajectory  4  . In addition there are 8  strips  per tower located behind the silicon sensors but outside of the active volume of the calorimeter  5  . Together they form a total of 56 independent tracking channels  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Novel methodology for monitoring the performance of the LAT instrument on board the GLAST satellite . Abstract : The Large Area Telescope ( LAT ) is one of two instruments aboard NASA s Fermi Gamma - ray Space Telescope , launched in June 2008 .The LAT consists of an array of silicon strip trackers and cesium iodide calorimeters that detect gamma radiation with energies between 20 MeV to more than 300 GeV . This text explains a novel method employed by the LAT collaboration to study the performance of its detector network during mission utilizing gamma radiation information taken over several months previously to launch .We see how this method can be applied to characterize the response function of each individual tracker module as also as the overall energy resolution of the entire LAT . These data are compared against ground calibration measurements accomplished before mission .Finally we prove how these procedures have been successfully utilized to identify issues with some modules after launch which were later fixed through software updates . The Large Area Telescope ( L AT ) is one of two instruments flown on NASA s Fermi Gamma - Ray Space Telescope 1 .Launched into space in June 2008 , it has detected many of sources of high - energy photons since then 2 . In order to conduct such observations , the L AT requires properly assess the direction and energy of incoming photons .To accomplish this objective , the L AT employs a combination of silicon strip detectors and CsI ( Tl ) scintillators grouped in four layers around a central tungsten converter foil 3 , see Figure 1 . Each layer contains 16 towers , or path segments , consisting of 4 silicon bars aligned at different angles relative to the incident photon trajectory 4 .In addition there are 8 layers per tower situated behind the silicon cameras but outside of the active volume of the calorimeter 5 . Together they create a total of 56 independent tracking channels 6 .",
        "rewrite_text": "**Title:** Innovative Methodology for Assessing the Performance of the LAT Instrument on the GLAST Satellite\n\n**Abstract:** The Large Area Telescope (LAT), one of the two primary instruments aboard NASA's Fermi Gamma-ray Space Telescope, was launched in June 2008 and has since played a pivotal role in the detection of high-energy gamma radiation. The LAT is designed to measure gamma rays with energies ranging from 20 MeV to over 300 GeV, utilizing a sophisticated array of silicon strip trackers and cesium iodide (CsI) calorimeters. This article presents a novel methodology developed by the LAT collaboration to monitor and evaluate the performance of the detector network throughout the mission. By analyzing gamma radiation data collected over several months prior to the satellite's launch, the team has established a framework for characterizing the response functions of individual tracker modules, as well as assessing the overall energy resolution of the LAT system.\n\nThe methodology involves a comparative analysis of the pre-launch data against ground calibration measurements, allowing for a comprehensive understanding of the instrument's performance characteristics. This approach has proven effective in identifying discrepancies and performance issues in specific modules post-launch, which were subsequently addressed through targeted software updates. The LAT's design features a combination of silicon strip detectors and CsI scintillators, organized in four layers surrounding a central tungsten converter foil. Each layer comprises 16 towers, or path segments, with four silicon bars oriented at varying angles to optimize photon trajectory detection. Additionally, each tower includes eight layers positioned behind the silicon cameras, enhancing the calorimeter's active volume and contributing to a total of 56 independent tracking channels.\n\nThrough this innovative methodology, the LAT collaboration has not only enhanced the understanding of the instrument's operational capabilities but has also ensured the continued reliability and accuracy of gamma-ray observations in the ongoing exploration of high-energy astrophysical phenomena.",
        "ori-fast-z-score": 0.17407765595569785,
        "water-fast-z-score": 6.5033247714309,
        "rewrite-fast-z-score": 1.8207329549252094
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Generalized CRF-structures .\nAbstract:\nWe introduce the concept of generalized conditional random fields (GCRFs) and show how they can be used to model arbitrary probability distributions over structured data sets, such as sequences or trees.  We present an efficient algorithm for learning GCRF parameters using gradient descent on the log-likelihood objective function.   Finally we demonstrate that our approach is able to learn accurate models for several challenging sequence labeling tasks including part-of-speech tagging in natural language processing and protein secondary structure prediction in bioinformatics. Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) are undirected graphical models which have been successfully applied to many problems involving sequential data, e.g. (Sha & Pereira, 2003) . In this work, we propose Generalized Conditional Random Fields (GCRFs), a generalization of CRFs which allows us to represent any distribution over structured data sets like sequences or trees. The key idea behind GCRFs is to use a set of latent variables to capture dependencies between different parts of the input space. This enables us to efficiently compute the partition function required by standard CRFs with dynamic programming techniques. Furthermore, it also makes it possible to train GCRFs using gradient-based methods similar to those employed for Maximum Entropy Markov Models (MEMMs). To evaluate the performance of our method, we apply it to two important applications: part-of-speech taggin",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Generalized CRF - structures . Abstract : We introduce the idea of generalized conditional random fields ( GCRFs ) and explain how they can be used to model arbitrary likelihood distributions over structured data sets , such as sequences or trees .We present an efficient algorithm for learning GCRF variables using gradient descent on the log - likelihood objective function . Finally we prove that our approach is ability to teach exact models for numerous challenging gene labeling challenges including whole - of - voice tagging in natural language processing and gene secondary structure prediction in bioinformatics .Conditional Random Fields ( CRFs ) ( Lafferty et al . , 2001 ) are undirected graphical descriptions which have been successfully applied to many difficulties involving sequential data , e . g . ( Sha & Pereira , 2003 ) .In this research , we develop Generalized Conditional Random Fields ( GCRFs ) , a generalization of CRFs which allows us to model any distribution over structured data sets like sequences or trees . The main idea behind GCRFs is to use a setting of latent variables to capture dependencies between various parts of the input space .This enables us to easily compute the partition function required by traditional CRFs with dynamic programming tools . Furthermore , it also makes it able to train GCRFs using gradient - based methods similar to those utilized for Maximum Entropy Markov Models ( MEMMs ) .To assess the performance of our technique , we apply it to two essential users : part - of - voice taggin",
        "rewrite_text": "We present the concept of Generalized Conditional Random Fields (GCRFs), a novel framework designed to model arbitrary likelihood distributions over structured datasets, including sequences and trees. This paper outlines an efficient algorithm for learning GCRF parameters through gradient descent applied to the log-likelihood objective function. We demonstrate the effectiveness of our approach by proving its capability to accurately model various complex gene labeling tasks, such as whole-voice tagging in natural language processing and predicting gene secondary structures in bioinformatics.\n\nConditional Random Fields (CRFs), as introduced by Lafferty et al. (2001), are undirected graphical models that have been successfully utilized in numerous applications involving sequential data, as noted by Sha and Pereira (2003). Building on this foundation, we develop GCRFs as a generalization of CRFs, which enables the modeling of any distribution over structured data. The core innovation of GCRFs lies in the incorporation of latent variables that capture dependencies among different components of the input space. This approach facilitates the computation of the partition function, a critical element in traditional CRFs, using dynamic programming techniques.\n\nMoreover, our framework allows for the training of GCRFs through gradient-based methods akin to those employed in Maximum Entropy Markov Models (MEMMs). To evaluate the performance of our proposed method, we apply it to two significant tasks: part-of-speech tagging and gene secondary structure prediction. The results indicate that GCRFs not only enhance the modeling capabilities for structured data but also improve the accuracy of predictions in challenging applications. This work opens new avenues for research and application in both natural language processing and bioinformatics, showcasing the versatility and power of generalized conditional random fields.",
        "ori-fast-z-score": -0.647150228929434,
        "water-fast-z-score": 6.127946159842712,
        "rewrite-fast-z-score": 0.26211121699831136
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How do trehalose, maltose and sucrose influence some structural and dynamical properties of lysozyme ? An insight from Molecular Dynamics simulations .\nAbstract:\nThe effect of three different sugars (trehalose, maltase and sucrose) on the structure and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case. The results show that all these sugar molecules can stabilize the protein against thermal denaturation to certain extent but trehalose is found to be most effective one among them. Trehalose molecule forms hydrogen bonds with both polar and nonpolar amino acid residues which leads to increase in number of water molecules around it. This increases the hydration shell thickness as well as total solvent accessible surface area of the protein. It also decreases the root mean square deviation between initial and final structures indicating its ability to maintain the native conformation of the protein. Maltose and sucrose are less efficient than trehalose because they form only few hydrogen bonds with the protein. In addition, their presence causes slight decrease in the radius of gyration and end-to-end distance of the protein.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How do trehalose , maltose and sucrose influence some structural and dynamical properties of lysozyme ? An insight from Molecular Dynamics simulations .Abstract : The impact of three different sugars ( trehalose , maltase and sucrose ) on the composition and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case . The results show that all these sugar molecules can stabilize the protein against thermal denaturation to certain degree but trehalose is found to be most efficient one among them .Trehalose atom forms hydrogen bonds with both polar and nonpolar amino acid acids which results to increase in quantity of water molecules around it . This changes the hydration shell thickness as well as gross solvent available surface space of the protein .It additionally decreases the root average square deviation between initial and final structures signaling its able to keep the native conformation of the protein . Maltose and sucrose are less effective than trehalose because they create only few hydrogen bonds with the protein .In addition , their presence causes mild decrease in the radius of gyration and end - to - end distance of the protein .",
        "rewrite_text": "Title: The Influence of Trehalose, Maltose, and Sucrose on the Structural and Dynamical Properties of Lysozyme: Insights from Molecular Dynamics Simulations\n\nAbstract: This study explores the effects of three distinct sugars—trehalose, maltose, and sucrose—on the structural and dynamic characteristics of lysozyme through molecular dynamics simulations conducted at 300 K over a duration of 100 nanoseconds for each sugar. The findings reveal that all three sugars contribute to the stabilization of lysozyme against thermal denaturation, albeit to varying extents, with trehalose demonstrating the highest efficacy. The molecular interactions between trehalose and lysozyme involve the formation of hydrogen bonds with both polar and nonpolar amino acids, which subsequently increases the number of water molecules in the vicinity. This interaction leads to modifications in the thickness of the hydration shell and alters the solvent-accessible surface area of the protein. Notably, the presence of trehalose results in a reduction of the root mean square deviation (RMSD) between the initial and final protein structures, indicating its effectiveness in maintaining the native conformation of lysozyme. In contrast, maltose and sucrose exhibit less pronounced effects, primarily due to their limited capacity to form hydrogen bonds with the protein. Furthermore, the introduction of these sugars results in a slight decrease in both the radius of gyration and the end-to-end distance of lysozyme, suggesting a more compact structure. Overall, this research highlights the significant role of trehalose in enhancing the stability of lysozyme, while also providing insights into the comparative effects of maltose and sucrose on the protein's structural dynamics.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 4.719863179556685,
        "rewrite-fast-z-score": 1.0101525445522108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spin 1/2 Fermions in the Unitary Limit.I .\nAbstract:\nWe study spin-1/2 fermions on an infinite square lattice with nearest-neighbor hopping and pairing interactions, using exact diagonalization techniques to calculate their ground-state energy as well as the momentum distribution function at zero temperature.  We find that for weak coupling (small values of t/J), the system is in a superfluid phase characterized by off-diagonal long-range order; however, this state becomes unstable when t/J exceeds some critical value tc(μ) which depends on chemical potential μ. For strong coupling (large values of t/J), we show that there exists another stable phase where the system has no off-diagonal long range order but still exhibits gapless excitations. This new phase can be viewed as a BCS-like paired state without any broken symmetry. The transition between these two phases occurs via a quantum phase transition. Finally, we discuss possible experimental realizations of our model Hamiltonian.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spin 1 / 2 Fermions in the Unitary Limit . I . Abstract : We explore spinning - 1 / 2 fermions on an endless square lattice with nearest - neighbor hopping and pairing interactions , using accurate diagonalization techniques to estimate their ground - state energy as well as the velocity distribution relation at zero temperature .We find that for weak coupling ( small values of t / J ) , the system is in a superfluid phase characterized by off - diagonal long - range order ; however , this state becomes unstable when t / J exceeds some critical value tc ( μ ) which depends on chemical potential μ . For strong coupling ( large values of t / J ) , we show that there exists another stable phase where the system has no off - diagonal long range order but still exhibits gapless excitations .This new phase can be viewed as a BCS - like paired state without any breaking symmetry . The shift between these two phases occurs via a quantum phase change .Finally , we explain possible experimental realizations of our model Hamiltonian .",
        "rewrite_text": "In this study, we investigate the behavior of spin-1/2 fermions on an infinite square lattice, focusing on the effects of nearest-neighbor hopping and pairing interactions. Utilizing advanced diagonalization techniques, we accurately estimate the ground-state energy and analyze the velocity distribution at zero temperature. Our findings reveal that at weak coupling, characterized by small ratios of hopping to pairing strength (t/J), the system resides in a superfluid phase marked by off-diagonal long-range order. However, this superfluid state becomes unstable when the ratio t/J surpasses a critical threshold, denoted as tc(μ), which is dependent on the chemical potential μ. In the regime of strong coupling, where t/J is large, we identify a distinct stable phase that lacks off-diagonal long-range order yet still supports gapless excitations. This phase can be interpreted as a BCS-like paired state that does not exhibit symmetry breaking. The transition between the superfluid and this new phase occurs through a quantum phase transition, highlighting the rich interplay between quantum mechanics and many-body physics in this system. Additionally, we discuss potential experimental implementations of our model Hamiltonian, suggesting avenues for future research and exploration in the field of condensed matter physics. Our results contribute to the understanding of fermionic systems under strong interactions and provide insights into the nature of quantum phase transitions in lattice models.",
        "ori-fast-z-score": 0.7683498199278324,
        "water-fast-z-score": 3.796283011826483,
        "rewrite-fast-z-score": 2.1263507521967115
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Photon-induced Nucleosynthesis: Current Problems and Experimental Approaches .\nAbstract:\nThe production of light elements in the early universe is one of the most important unsolved problems in astrophysics, cosmology, nuclear physics and particle physics. The standard model (SM) of elementary particles cannot explain how these elements were created during the first few minutes after the Big Bang. In this talk I will present an overview on our current understanding about the origin of light nuclei with A=1-3 produced by photonuclear reactions at high temperatures and densities in the early universe. This includes theoretical predictions for the abundances as well as experimental results obtained using radioactive beams at GSI Darmstadt. Finally, I will discuss possible future experiments to test some of the key predictions made within the SM. Keywords: Photonuclear reaction, Light element synthesis, Big Bang nucleosynthesis, Astrophysical SNe Ia explosion mechanism, Nuclear structure theory. 1 Introduction.\nLight element synthesis in the early universe is among the most challenging open questions in modern science  1  . It has been known since the 1960s that photons can induce nuclear fusion processes leading to the creation of light elements like D, 3 He, 4 He, 7 Li or 9 Be  2  , but it was not until recently when we have gained sufficient knowledge about the physical conditions prevailing in the early universe  3  .\nIn particular, the temperature T and density ρ reached values up to 10 12 K and 10 15 g/cm 3 respectively  4  . These extreme conditions are only accessible today in laboratory experiments using relativistic heavy-ion collisions  5  . However, due to the extremely short time scales involved  6  , such experiments do not allow us to study the formation of light elements directly  7, 8  . Instead they provide information about the properties of hot dense matter which may be relevant for the description of the initial stages of supernova explosions  9  . On the other hand, the abundance pattern observed in primordial objects like white dwarfs  10  or metal-poor stars  11  provides valuable constraints on the models describing the evolution of the chemical composition of the universe  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Photon - induced Nucleosynthesis : Current Problems and Experimental Approaches . Abstract : The production of light elements in the early universe is one of the most important unsolved issues in astrophysics , cosmology , nuclear science and particle science .The conventional model ( SM ) of primary nuclei cannot explain how these objects were created during the first few hours after the Big Bang . In this talk I will present an overview on our new understanding about the origin of light nuclei with A = 1 - 3 created by photonuclear reactions at high heats and densities in the early universe .This contains theoretical estimates for the abundances as well as research results acquired using nuclear beams at GSI Darmstadt . Finally , I will explore possible future research to test some of the key predictions taken within the SM .Keywords : Photonuclear reaction , Light element synthesis , Big Bang nucleosynthesis , Astrophysical SNe Ia explosion mechanism , Nuclear structure model . 1 Introduction .Light factor synthesis in the early universe is among the most challenging open questions in modern science 1 . It has been known since the 1960s that photons can induce nuclear fusion mechanisms leading to the creation of light elements like D , 3 He , 4 He , 7 Li or 9 Be 2 , but it was not until recently when we have achieved sufficient knowledge about the physical conditions prevailing in the early world 3 .In particular , the temperature T and density ρ reached values up to 10 12 K and 10 15 g / cm 3 respectively 4 . These severe environments are only accessible today in laboratory experiments using relativistic heavy - ion collisions 5 .However , owing to the exceptionally short period scales involved 6 , such studies do not enable us to study the formation of light elements directly 7 , 8 . Instead they give information about the properties of bright heavy material which may be appropriate for the description of the first stages of supernova explosions 9 .On the other hand , the abundance behavior observed in primordial objects like white dwarfs 10 or metal - poor stars 11 provides valuable constraints on the models explaining the evolution of the chemical composition of the universe 12 .",
        "rewrite_text": "**Title:** Photon-Induced Nucleosynthesis: Current Challenges and Experimental Strategies\n\n**Abstract:** The formation of light elements in the early universe remains a significant unresolved challenge across various scientific disciplines, including astrophysics, cosmology, nuclear science, and particle physics. The standard model of primordial nucleosynthesis fails to adequately account for the mechanisms behind the creation of these elements during the initial moments following the Big Bang. In this presentation, I will provide a comprehensive overview of our evolving understanding of the origins of light nuclei with mass numbers A = 1 to 3, which are produced through photonuclear reactions under the extreme conditions of high temperature and density that characterized the early universe. This discussion will include theoretical predictions regarding the abundances of these light elements, as well as empirical findings obtained from experiments utilizing nuclear beams at GSI Darmstadt. Furthermore, I will examine potential avenues for future research aimed at validating some of the critical predictions derived from the standard model. \n\nThe synthesis of light elements in the early universe is recognized as one of the most formidable open questions in contemporary science. Since the 1960s, it has been established that photons can trigger nuclear fusion processes that lead to the formation of light elements such as deuterium (D), helium-3 (³He), helium-4 (⁴He), lithium-7 (⁷Li), and beryllium-9 (⁹Be). However, it is only in recent years that we have gained sufficient insight into the physical conditions that prevailed during the universe's infancy. Notably, temperatures and densities reached extraordinary levels, up to 10¹² K and 10¹⁵ g/cm³, respectively. These extreme environments can now be simulated in laboratory settings through relativistic heavy-ion collisions. Nevertheless, due to the extremely short time scales involved, direct observation of light element formation remains elusive. Instead, these experiments provide insights into the properties of dense matter, which may be relevant for understanding the early phases of supernova explosions. Additionally, the abundance patterns observed in primordial objects, such as white dwarfs and metal-poor stars, offer critical constraints on models that seek to explain the chemical evolution of the universe. \n\n**Keywords:** Photonuclear reactions, Light element synthesis, Big Bang nucleosynthesis, Supernova Ia explosion mechanisms, Nuclear structure models.",
        "ori-fast-z-score": 1.348187695720845,
        "water-fast-z-score": 8.538149682454623,
        "rewrite-fast-z-score": 1.3887301496588271
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  ARCRAIDER I: Detailed optical and X-ray analysis of the cooling flow cluster Z3146 .\nAbstract:\nWe present an in-depth study of the galaxy cluster Z3146, which is one of the most luminous clusters in X-rays known to date (Lx = 1.5 x 1045 erg/s). The main goal of this work was to investigate whether or not there are any signatures of AGN activity associated with the central cD galaxy. We have obtained deep optical spectroscopy for more than 100 galaxies within the virial radius of the cluster using the Keck telescope. In addition we have carried out Chandra observations that allow us to resolve individual point sources down to luminosities as low as Lx ~ 3 x 1041 erg/s. Our results show that:  -The brightest cluster galaxy has a very complex spectrum dominated by emission lines produced by gas ionized by hot stars.  -There is no evidence for significant nuclear activity in the form of broad line regions and/or high excitation narrow-line regions.  -The total mass profile derived from our spectroscopic data agrees well with previous estimates based on gravitational lensing measurements.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : ARCRAIDER I : Detailed optical and X - ray study of the thermal flow cluster Z3146 . Abstract : We report an in - depth examination of the galaxy cluster Z3146 , which is one of the most luminous clusters in X - radiation known to date ( Lx = 1 . 5 x 1045 erg / s ) .The main goal of this research was to examine whether or not there are any signatures of AGN activity related with the main cD galaxy . We have achieved dark optical spectroscopy for more than 100 galaxies within the virial diameter of the cluster using the Keck lens .In addition we have carried out Chandra measurements that enable us to identify individual point sources down to luminosities as low as Lx ~ 3 x 1041 erg / s . Our results show that : - The brightest cluster galaxy has a very complex spectrum characterized by absorption patterns created by gas ionized by hot stars .- There is no evidence for significant nuclear activity in the form of broad line regions and / or large excitation narrow - line regions . - The total mass profile derived from our spectroscopic data agrees well with previous accounts based on gravity lensing observations .",
        "rewrite_text": "We present a comprehensive analysis of the galaxy cluster Z3146, recognized as one of the most luminous clusters in X-ray emissions, with a luminosity of Lx = 1.5 x 10^45 erg/s. The primary objective of this study was to investigate potential signatures of active galactic nucleus (AGN) activity associated with the central cD galaxy. Utilizing the Keck Observatory, we conducted dark optical spectroscopy on over 100 galaxies located within the cluster's virial radius. Complementing our optical observations, we performed X-ray measurements using the Chandra Observatory, which allowed us to detect individual point sources with luminosities as low as Lx ~ 3 x 10^41 erg/s. \n\nOur findings reveal several key insights: First, the spectrum of the brightest cluster galaxy exhibits a complex structure, characterized by absorption features indicative of gas ionized by hot stars. Second, we found no substantial evidence of nuclear activity, as indicated by the absence of broad emission line regions and significant excitation in narrow-line regions. Lastly, the total mass profile derived from our spectroscopic analysis aligns closely with previous measurements obtained through gravitational lensing techniques. This study enhances our understanding of the physical processes occurring within Z3146 and contributes to the broader knowledge of galaxy cluster dynamics and AGN interactions.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 5.253351420705195,
        "rewrite-fast-z-score": 1.4142135623730951
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Some Comments around The Examples against The Generalized Jacobian Conjecture .\nAbstract:\nIn this note we present some comments on the examples given in  1  and  2  . We show that these examples are not counterexamples to the generalized Jacobian conjecture, as stated by M. Laurent (see  3  ). In fact they do not even contradict the weaker statement made by J.-P. Serre  4  , which is equivalent to the Jacobian conjecture for curves over finite fields. Finally we give an example showing how one can construct counterexamples to the generalized Jacobi conjecture using our method. Let k be any field with char(k) = p > 0. For every integer n ≥ 1 let Xn denote the smooth projective curve defined over k by y n + a1yn−1 + · · · + anny0 = xn+1,\nwhere ai ∈ k * . It was shown by A. N. Parshin  5  that if char(k) = 2 then there exists a positive integer m such that the jacobian variety JacXm has complex multiplication. This implies that the jacobian varieties JacXn have complex multiplication for all integers n ≡ ±1 mod m. If char(k) = 3 it follows from  6  that JacX3 does not have complex multiplication. However, it still remains open whether or not JacX4 has complex multiplication.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Some Comments around The Examples against The Generalized Jacobian Conjecture . Abstract : In this note we present some remarks on the examples given in 1 and 2 .We see that these examples are not counterexamples to the generalized Jacobian conjecture , as described by M . Laurent ( saw 3 ) . In reality they do not even contradict the weaker statement provided by J . - P . Serre 4 , which is analogous to the Jacobian conjecture for curves over arbitrary fields .Finally we give an instance demonstrating how one can build counterexamples to the generalized Jacobi theorem using our technique . Let k be any field with char ( k ) = p > 0 .For every integer n ≥ 1 let Xn represent the smooth projective curve characterized over n by y n + a1yn−1 + · · · + anny0 = xn + 1 , where ai ∈ k * . It was shown by A . N . Parshin 5 that if char ( k ) = 2 then there exists a positive integer m such that the jacobian variety JacXm has complex multiplication .This implies that the jacobian varieties JacXn have complex multiplication for all integers n ≡ ±1 mod m . If char ( k ) = 3 it appears from 6 that JacX3 does not have complex multiplication . However , it still remains open whether or not JacX4 has complex multiplication .",
        "rewrite_text": "Title: Some Observations on Examples Related to the Generalized Jacobian Conjecture\n\nAbstract: This note provides insights into the examples discussed in references 1 and 2, which have been proposed in the context of the generalized Jacobian conjecture. We argue that these examples do not serve as counterexamples to the conjecture as outlined by M. Laurent in reference 3. Furthermore, they do not even challenge the more lenient assertion put forth by J.-P. Serre in reference 4, which parallels the Jacobian conjecture for curves defined over arbitrary fields. To illustrate our point, we present a method for constructing genuine counterexamples to the generalized Jacobian theorem. Let \\( k \\) denote any field with characteristic \\( \\text{char}(k) = p > 0 \\). For each integer \\( n \\geq 1 \\), we define \\( X_n \\) as a smooth projective curve characterized by the equation \\( y^n + a_1 y^{n-1} + \\ldots + a_n y^0 = x + 1 \\), where \\( a_i \\in k^* \\). A. N. Parshin, in reference 5, established that if \\( \\text{char}(k) = 2 \\), there exists a positive integer \\( m \\) such that the Jacobian variety \\( \\text{Jac}(X_m) \\) exhibits complex multiplication. This finding indicates that the Jacobian varieties \\( \\text{Jac}(X_n) \\) possess complex multiplication for all integers \\( n \\equiv \\pm 1 \\mod m \\). Conversely, it has been suggested in reference 6 that \\( \\text{Jac}(X_3) \\) does not exhibit complex multiplication when \\( \\text{char}(k) = 3 \\). However, the status of whether \\( \\text{Jac}(X_4) \\) possesses complex multiplication remains an open question.",
        "ori-fast-z-score": 0.819288030372914,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 0.5184758473652127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Searching for links between magnetic fields and stellar evolution. II. The evolution of magnetic fields as revealed by observations of Ap stars in open clusters and associations .\nAbstract:\nWe have studied the evolution of magnetic fields on Ap-type stars using data obtained with the ESPaDOnS spectropolarimeter at CFHT, Canada-France-Hawaii Telescope (CFHT). We used high-resolution spectra to determine the mean longitudinal magnetic field Bz , which is proportional to the integral over all depths of the line-of-sight component of the magnetic vector. Our sample consists of about 100 Ap stars observed within open clusters or OB-associations. For each star we determined its age based on photometric data available in literature. \n \n In order to study how the strength of magnetic fields evolves during the main-sequence life time of an Ap star, we divided our sample into three groups according to their ages. These are young open clusters/associations (age < 1 Myr), intermediate-open clusters/associations (1 Myr < age < 10 Myr) and old open clusters/associations  (10 Myr < age < 300 Myr). \n \n Figure 1: Mean longitudinal magnetic field versus effective temperature for Ap stars belonging to different open clusters/OB-associations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Searching for connections between magnetic fields and stellar evolution . II .The evolution of magnetic fields as revealed by observed of Ap stars in open complexes and associations . Abstract : We have researched the evolution of magnetic fields on Ap - class stars using data acquired with the ESPaDOnS spectropolarimeter at CFHT , Canada - France - Hawaii Telescope ( CFHT ) .We utilized large - resolution spectra to find the mean longitudinal magnetic force Bz , which is proportional to the integral over all depths of the line - of - view component of the magnetic vector . Our specimen consists of about 100 Ap stars observed within open nuclei or OB - associations .For each star we calculated its age based on photometric data available in literature . In order to study how the strength of magnetic fields evolves during the main - sequence life time of an Ap star , we divided our sample into three groups based to their ages .These are young open complexes / associations ( age < 1 Myr ) , intermediate - open complexes / associations ( 1 Myr < age < 10 Myr ) and elderly open clusters / associations ( 10 Myr < age < 300 Myr ) . Figure 1 : Mean horizontal magnetic force versus effective heat for Ap stars belonging to different open complexes / OB - associations .",
        "rewrite_text": "Title: Investigating the Relationship Between Magnetic Fields and Stellar Evolution: II. The Evolution of Magnetic Fields in Ap Stars Observed in Open Complexes and Associations\n\nAbstract: This study delves into the evolution of magnetic fields in Ap-type stars, utilizing data obtained from the ESPaDOnS spectropolarimeter at the Canada-France-Hawaii Telescope (CFHT). By analyzing high-resolution spectra, we determined the mean longitudinal magnetic field strength (Bz), which correlates with the integrated magnetic vector component along the line of sight. Our sample comprises approximately 100 Ap stars located within open clusters and OB associations. For each star, we estimated its age based on existing photometric data from the literature. To investigate the evolution of magnetic field strength throughout the main-sequence phase of Ap stars, we categorized our sample into three distinct age groups: young open complexes/associations (age < 1 Myr), intermediate open complexes/associations (1 Myr < age < 10 Myr), and older open clusters/associations (10 Myr < age < 300 Myr). Our findings, illustrated in Figure 1, depict the relationship between mean horizontal magnetic field strength and effective temperature for Ap stars across various open complexes and OB associations. This research aims to enhance our understanding of how magnetic fields evolve in relation to stellar age and provides insights into the broader implications for stellar evolution theories.",
        "ori-fast-z-score": -0.6897304947150052,
        "water-fast-z-score": 4.314554973040049,
        "rewrite-fast-z-score": 0.09853292781642932
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectroscopy and multiband photometry of the afterglow of intermediate duration gamma-ray burst 040924 and its host galaxy .\nAbstract:\nWe report on optical spectroscopy and multicolor photometry of GRB 040924, an intermediate-duration (T90 = 5 s) event detected by Swift/BAT at 07:55 UT on 24 September 2004. The prompt emission was followed by a bright X-ray flare peaking about 1 hour later than the main pulse. We find that the spectrum is well fitted with a power law plus blackbody model in the range 3000-9000 Å . The best-fit parameters are  _  = -1.1 ± 0.2 , TBB = 6200 +1800 -900 K, and EBB = 2.5 +1.0 -0.7 keV for the power-law index,  temperature, and normalization of the blackbody component respectively.  These values are consistent with those observed in other short-hard GRBs. In addition to this thermal component, we detect strong Fe II absorption lines blueshifted by ~10,000 km/s relative to their rest wavelengths. This suggests that the progenitor system may be similar to that inferred for short-hard GRBs such as GRB 050509b.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectroscopy and multiband photometry of the afterglow of intermediate duration gamma - ray burst 040924 and its host galaxy . Abstract : We report on optical spectroscopy and multicolor photometry of GRB 040924 , an intermediate - duration ( T90 = 5 s ) event detected by Swift / BAT at 07 : 55 UT on 24 September 2004 .The prompt emission was followed by a bright X - ray flare peaking about 1 hour later than the main pulse . We see that the spectrum is well fitted with a power law plus blackbody model in the range 3000 - 9000 Å .The best - fitting factors are _ = - 1 . 1 ± 0 . 2 , TBB = 6200 + 1800 - 900 K , and EBB = 2 . 5 + 1 . 0 - 0 . 7 keV for the power - law index , temperature , and normalization of the blackbody element respectively . These values are compatible with those observed in other short - hard GRBs .In addition to this heat element , we perceive strong Fe II spectral lines blueshifted by ~ 10 , 000 km / s relative to their rest wavelengths . This implies that the progenitor system might be parallel to that inferred for short - hard GRBs such as GRB 050509b .",
        "rewrite_text": "We present a comprehensive analysis of the optical spectroscopy and multicolor photometry of gamma-ray burst (GRB) 040924, an intermediate-duration event characterized by a T90 duration of 5 seconds. This burst was detected by the Swift/Burst Alert Telescope at 07:55 UT on September 24, 2004. Following the initial prompt emission, we observed a significant X-ray flare that peaked approximately one hour after the main burst, indicating complex underlying mechanisms at play. Our spectral analysis, conducted over the wavelength range of 3000 to 9000 Å, reveals that the data is well-represented by a model combining a power law with a blackbody component. The best-fit parameters obtained from our analysis include a power-law index of _ = -1.1 ± 0.2, a blackbody temperature of TBB = 6200 +1800 -900 K, and a normalization factor for the blackbody of EBB = 2.5 +1.0 -0.7 keV. These findings align closely with observations from other short-hard GRBs, suggesting a potential commonality in their physical properties. Furthermore, we detected prominent Fe II spectral lines that are blueshifted by approximately 10,000 km/s relative to their rest wavelengths. This significant blueshift indicates that the progenitor system of GRB 040924 may share similarities with those inferred for other short-hard GRBs, such as GRB 050509b. Our results contribute to the growing body of evidence regarding the nature of intermediate-duration gamma-ray bursts and their host environments, providing insights into the mechanisms driving these energetic phenomena.",
        "ori-fast-z-score": 1.3242443839434612,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": 0.9901475429766744
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neel order in square and triangular lattice Heisenberg models .\nAbstract:\nWe study the ground state properties of spin-1/2 antiferromagnetic Heisenberg model on square (SQ) and triangular lattices (TL). We show that for both SQ and TL, there is no Neel ordering at any finite temperature T . The absence of Neel ordering can be understood by studying the behavior of spin-spin correlation function S(0)*S(r) . For SQ we find that it decays exponentially with distance r , while for TL it shows power law decaying behaviour. This indicates that the system has short range correlations which are consistent with the Mermin-Wagner theorem. However, our results also suggest that the system may have some kind of magnetic ordering below certain critical temperatures Tc .\nThe values of Tc obtained numerically agree well with those predicted theoretically using mean field theory. In addition to this, we also calculate the specific heat Cv as a function of temperature T .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neel order in square and triangular lattice Heisenberg models . Abstract : We explore the ground state properties of spin - 1 / 2 antiferromagnetic Heisenberg model on square ( SQ ) and triangular lattices ( TL ) .We see that for both SQ and TL , there is no Neel ordering at any finite temperature T . The absence of Neel ordering can be understood by examining the activity of spin - spinning correlation function S ( 0 ) * S ( r ) .For SQ we find that it decays exponentially with distance r , while for TL it displays power law decaying behaviour . This implies that the scheme has small range correlations which are compatible with the Mermin - Wagner theorem .However , our findings also suggest that the system might have some kind of magnetic ordering below certain critical temperatures Tc . The values of Tc obtained numerically agree well with those predicted theoretically using mean field principles .In addition to this , we also obtain the specific warmth Cv as a function of temperature T .",
        "rewrite_text": "In this study, we investigate the ground state characteristics of the spin-1/2 antiferromagnetic Heisenberg model on both square (SQ) and triangular (TL) lattices. Our analysis reveals that neither lattice exhibits Neel ordering at any finite temperature (T). This lack of Neel ordering is elucidated through the examination of the spin-spin correlation function S(0) * S(r). For the square lattice, we observe that the correlation function decays exponentially with increasing distance (r), while for the triangular lattice, it demonstrates a power-law decay. These findings indicate that both lattices exhibit short-range correlations, which align with the Mermin-Wagner theorem's implications. Despite the absence of Neel ordering at finite temperatures, our results suggest the potential for some form of magnetic ordering to emerge below specific critical temperatures (Tc). The critical temperatures we have calculated numerically show strong agreement with theoretical predictions derived from mean field theory. Additionally, we analyze the specific heat (Cv) as a function of temperature (T), providing further insights into the thermal properties of the system. Overall, our research contributes to a deeper understanding of the magnetic behavior in low-dimensional quantum systems, highlighting the complex interplay between lattice geometry and magnetic ordering phenomena.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.358898943540673,
        "rewrite-fast-z-score": 0.7035264706814485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolutionary Mesh Numbering: Preliminary Results .\nAbstract:\nWe present an evolutionary algorithm for the problem of mesh numbering, which is to assign integers in the range  0, ..., n-1  to all vertices of a given graph G = (V, E) such that adjacent vertices have consecutive numbers and no two edges share the same pair of endpoints with the same order.  The proposed method uses a population-based approach where each individual represents a valid solution by means of a permutation vector. We show how this representation can be used to efficiently compute fitness values using only local information. In addition we propose several genetic operators to explore the search space. Finally, we report on preliminary results obtained by applying our method to some well-known benchmark instances. Keywords: Graphs, Optimization problems, Evolutionary algorithms, Genetic algorithms. 1 Introduction A common task when working with graphs is to label their nodes or edges with unique identifiers. This process is known as node or edge numbering respectively. For example, it may be required to number the nodes of a road network so that every path between any two points has a unique sequence of labels. Another application arises in circuit design, where one needs to assign unique addresses to components of electronic circuits.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolutionary Mesh Numbering : Preliminary Results . Abstract : We present an evolutionary algorithm for the question of mesh numbering , which is to assign integers in the range 0 , . . . , k - 1 to all edges of a given graph G = ( V , E ) such that adjacent vertices have consecutive numbers and no two edges share the same pair of endpoints with the same order .The proposed approach utilizes a population - based approach where each individual represents a valid answer by means of a permutation vector . We see how this representation can be used to easily compute fitness values using only local information .In addition we propose several genetic operators to examine the search space . Finally , we publish on preliminary results acquired by application our technique to some well - recognized benchmark instances .Keywords : Graphs , Optimization problems , Evolutionary algorithms , Genetic algorithms . 1 Introduction A typical task when dealing with graphs is to label their nodes or edges with special identifiers .This process is known as node or edge counting respectively . For instance , it could be required to count the nodes of a street system so that every road between any two points has a unique chain of labels .Another application exists in circuit design , where one needs to give unique addresses to components of electronic circuits .",
        "rewrite_text": "Title: Evolutionary Mesh Numbering: Preliminary Results\n\nAbstract: In this study, we introduce an innovative evolutionary algorithm aimed at addressing the problem of mesh numbering in graph theory. The objective is to assign integers within the range of 0 to k - 1 to all edges of a given graph G = (V, E), ensuring that adjacent vertices are assigned consecutive numbers and that no two edges share the same pair of endpoints in the same order. Our approach employs a population-based strategy, where each individual in the population is represented by a permutation vector that encodes a valid solution to the mesh numbering problem. This representation facilitates the efficient computation of fitness values by leveraging only local information, which enhances the algorithm's performance. Furthermore, we introduce several genetic operators designed to explore the search space effectively, allowing for a more comprehensive examination of potential solutions. We present preliminary results obtained from applying our algorithm to well-established benchmark instances, demonstrating its efficacy and potential for solving complex mesh numbering challenges. This research contributes to the broader field of graph optimization problems and highlights the applicability of evolutionary algorithms and genetic algorithms in tackling such issues. The significance of this work extends to various practical applications, including the unique labeling of nodes or edges in transportation networks and circuit design, where distinct identifiers are crucial for the functionality and efficiency of systems. \n\nKeywords: Graphs, Optimization Problems, Evolutionary Algorithms, Genetic Algorithms.",
        "ori-fast-z-score": -1.2451741707874968,
        "water-fast-z-score": 4.041451884327381,
        "rewrite-fast-z-score": -0.8320502943378436
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Testing the Accuracy of Synthetic Stellar Libraries .\nAbstract:\nWe present an analysis of synthetic stellar libraries used to calibrate photometric surveys, such as Gaia and LSST. We show that these libraries are not accurate enough for this purpose because they do not include all relevant physical processes in their models (e.g., convection). This leads to systematic errors when using them to calibrate photometry or derive distances. We demonstrate how we can use observations of open clusters with known ages and metallicities to test the accuracy of different synthetic libraries by comparing observed and predicted cluster properties. Finally, we discuss possible improvements on current synthetic libraries. The next generation of space-based telescopes will provide unprecedented amounts of data about our Galaxy. These new datasets require large efforts to be analyzed properly. One important aspect is the calibration of photometric surveys like Gaia and LSST which will deliver precise astrometry and multi-color photometry for billions of stars across the sky. To achieve high precision results it is crucial to understand potential sources of error and biases introduced during the reduction process. In particular, one has to ensure that the derived absolute magnitudes M_(V) are correct within 0.01 mag over most of the color range covered by the survey. \n \n For example, if the distance modulus DM = 5log10(d/d_sun), where d is the true distance between us and the star and d_sun is the Sun’s distance from Earth, then a difference of 0.01 mag corresponds to a factor of 1.1 in distance. Thus, even small uncertainties in the absolute magnitude scale translate into significant errors in inferred distances. Therefore, it is essential to have reliable methods to determine the absolute magnitudes of individual stars accurately before deriving distances.  \n \n Currently there exist several approaches to estimate absolute magnitudes based on theoretical model atmospheres. However, these models often fail to reproduce observational constraints at low temperatures and/or high surface gravities. As a result, the resulting absolute magnitudes may deviate significantly from those obtained through other techniques, e.g., eclipsing binaries. Moreover, some of these models also suffer from incomplete",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Testing the Accuracy of Synthetic Stellar Libraries . Abstract : We report an assessment of synthetic stellar databases employed to calibrate photometric surveys , such as Gaia and LSST .We suggest that these books are not authoritative enough for this objective because they do not include all relevant physical processes in their models ( e . g . , convection ) . This leads to systematic errors when using them to calibrate photometry or calculate distances .We suggest how we can using observations of open clusters with established periods and metallicities to test the accuracy of different synthetic databases by comparing observed and anticipated cluster properties . Finally , we explain possible advances on current artificial libraries .The future generation of space - based telescopes will provide immense sums of evidence about our Galaxy . These new datasets require large efforts to be analyzed correctly .One important element is the calibration of photometric surveys like Gaia and LSST which will provide accurate astrometry and multi - color photometry for billions of stars across the sky . To achieve high precision outcomes it is crucial to realize potential sources of mistake and biases created during the reduction step .In particular , one has to ensure that the derived absolute magnitudes M _ ( V ) are correct within 0 . 01 mag over most of the color range covered by the survey . For example , if the distance modulus DM = 5log10 ( d / d _ sun ) , where d is the true distance between us and the star and d _ sun is the Sun ’ s distance from Earth , then a difference of 0 . 01 mag corresponds to a factor of 1 . 1 in distance .Thus , even narrow uncertainties in the absolute magnitude range result into considerable errors in inferred distances . Therefore , it is important to have reliable techniques to identify the absolute magnitudes of individual stars accurately before deriving distances .Currently there remain many approaches to estimate absolute magnitudes based on theoretical model atmospheres . However , these models often fail to capture observational parameters at low temperatures and / or low exterior gravities .As a result , the resulting absolute magnitudes might deviate greatly from those achieved through other techniques , e . g . , eclipsing binaries . Moreover , some of these models even suffer from incomplete",
        "rewrite_text": "**Title:** Evaluating the Precision of Synthetic Stellar Libraries\n\n**Abstract:** In this study, we conduct a thorough evaluation of synthetic stellar libraries utilized for the calibration of photometric surveys, including prominent projects like Gaia and LSST. Our findings indicate that these libraries may lack the necessary authority for accurate calibration due to their exclusion of critical physical processes, such as convection, in their modeling. This omission can lead to systematic inaccuracies when these libraries are employed for photometric calibration or distance calculations. We propose a methodology for assessing the accuracy of various synthetic databases by leveraging observations of open clusters with well-established periods and metallicities. By comparing the observed properties of these clusters with the expected characteristics derived from synthetic models, we can effectively gauge the reliability of these databases.\n\nFurthermore, we discuss potential enhancements to existing synthetic libraries. The advent of next-generation space-based telescopes promises to yield vast amounts of data regarding our Galaxy, necessitating substantial efforts for proper analysis. A key aspect of this analysis is the calibration of photometric surveys like Gaia and LSST, which aim to deliver precise astrometric measurements and multi-color photometry for billions of stars. Achieving high-precision results requires a thorough understanding of potential errors and biases introduced during the data reduction process. Specifically, it is essential to ensure that the derived absolute magnitudes (M_V) are accurate to within 0.01 magnitudes across the majority of the color spectrum covered by the survey. For instance, a distance modulus (DM) defined as DM = 5log10(d/d_sun), where 'd' represents the actual distance to a star and 'd_sun' is the Sun's distance from Earth, indicates that a mere 0.01 magnitude discrepancy can translate to a 1.1 factor variation in distance. Consequently, even minor uncertainties in absolute magnitude can lead to significant errors in distance estimations. Therefore, establishing reliable methods for accurately determining the absolute magnitudes of individual stars is crucial prior to distance derivation. Currently, numerous approaches exist for estimating absolute magnitudes based on theoretical model atmospheres; however, these models frequently struggle to accurately represent observational parameters at low temperatures and/or low surface gravities. This limitation can result in substantial deviations in absolute magnitudes compared to those obtained through alternative methods, such as eclipsing binaries, and some models may even suffer from incomplete data.",
        "ori-fast-z-score": -1.6448469449747105,
        "water-fast-z-score": 6.383694290536715,
        "rewrite-fast-z-score": -0.8131433980500301
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  GLSM s for partial flag manifolds .\nAbstract:\nWe study the geometry and topology of generalized Lagrangian submanifolds (GLSMs) in complex symplectic manifolds, focusing on GLSMs that are special Lagrangians with respect to some Kähler form. We show how these can be constructed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles. In particular we consider the case where the base is a partial flag variety. This leads us to define new families of Calabi-Yau varieties which have been studied by physicists recently. These varieties are obtained by taking products of Grassmannian manifolds or their quotients by finite groups. The main results of this thesis are:  1. A construction of GLSMs using parabolic Higgs bundles.  2. An explicit description of the cohomology ring of the total space of a vector bundle associated to a parabolic Higgs bundle.  3. A proof of mirror symmetry between two different types of GLSMs defined above when the base is a product of Grassmannians.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : GLSM s for partial flag manifolds . Abstract : We research the topology and topology of generalized Lagrangian submanifolds ( GLSMs ) in complex symplectic manifolds , concentrating on GLSMs that are special Lagrangians with regard to some Kähler form .We see how these can be formed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles . In particular we consider the case where the base is a partial flag variety .This leads us to define novel families of Calabi - Yau varieties which have been studied by physicists recently . These varieties are derived by take products of Grassmannian manifolds or their quotients by finite groups .The main results of this dissertation are : 1 . A design of GLSMs involving parabolic Higgs bundles .2 . An intimate description of the cohomology ring of the total space of a vector bundle related to a parabolic Higgs bundle .3 . A proof of mirror symmetry between two different kinds of GLSMs developed above when the base is a product of Grassmannians .",
        "rewrite_text": "Title: GLSMs for Partial Flag Manifolds\n\nAbstract: This article investigates the topology of generalized Lagrangian submanifolds (GLSMs) within the context of complex symplectic manifolds, with a particular focus on those GLSMs that exhibit special Lagrangian properties relative to specific Kähler forms. We explore the construction of these submanifolds as holomorphic sections of certain line bundles over moduli spaces associated with parabolic Higgs bundles. Our study emphasizes the scenario where the underlying base space is a partial flag variety, which serves as a critical framework for our analysis. This exploration leads us to introduce new families of Calabi-Yau varieties that have garnered attention in recent studies by physicists. These varieties are constructed through products of Grassmannian manifolds or their quotients by finite groups, thereby enriching the landscape of Calabi-Yau geometries. The principal contributions of this research include: 1) the formulation of GLSMs that are intricately linked to parabolic Higgs bundles, providing a fresh perspective on their geometric properties; 2) a detailed examination of the cohomology ring associated with the total space of a vector bundle pertinent to a parabolic Higgs bundle, which reveals significant algebraic structures; and 3) a demonstration of mirror symmetry between two distinct classes of GLSMs identified in our study, particularly when the base is structured as a product of Grassmannians. These findings not only enhance our understanding of the interplay between geometry and physics but also open avenues for further research in the field of algebraic geometry and its applications in theoretical physics.",
        "ori-fast-z-score": -0.24618298195866545,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": 1.3065491598369756
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electromagnetic response of high-Tc superconductors -- the slave-boson and doped-carrier theories .\nAbstract:\nThe electromagnetic properties of high-temperature superconductors are studied in terms of two different theoretical approaches, namely the slave-boson theory (SBT) and the doped carrier theory (DCT). The SBT is based on an effective low-energy description of strongly correlated electrons by means of auxiliary bosonic degrees of freedom which represent collective charge excitations. In this approach we calculate the optical conductivity as well as the Hall coefficient for various values of doping concentration n. We find that both quantities exhibit nontrivial temperature dependence at low temperatures T . On the other hand, within DCT these physical observables can be calculated analytically using simple expressions valid only at zero temperature. Our results show that there exists significant quantitative difference between predictions made by these two models. This discrepancy may serve to discriminate between them experimentally. High-temperature superconductivity has been one of the most challenging problems in condensed matter physics over past decades  1  . Despite enormous experimental efforts  2  , its microscopic origin remains unknown. A number of competing theoretical scenarios have been proposed  3  but none of them could provide a complete explanation of all available data  4  .\nIn particular, it was suggested  5  that the mechanism responsible for high-temperature superconductivity might involve strong electron correlations  6  . These effects cannot be described within conventional Fermi-liquid theory  7, 8  because they lead to non-Fermi liquid behavior  9  such as power-law dependences of thermodynamic functions  10  or unusual transport phenomena  11  . To account for these features theoretically, several phenomenological models were developed  12  including the so-called slave-boson theory  13  . It describes the dynamics of strongly interacting fermions with spin S = 1/2 coupled to an additional set of bosonic fields representing collective charge fluctuations  14  . Within this framework, the ground state of the system corresponds to a Bose-Einstein condensation  15  of the bosons  16  . As a result, the fermionic quasiparticles acquire finite masses  17  leading to their disappearance above some critical temperature  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electromagnetic response of high - Tc superconductors - - the slave - boson and doped - carrier theories . Abstract : The electromagnetic properties of high - temperature superconductors are studied in terms of two different conceptual approaches , principally the slave - boson theory ( SBT ) and the doped carrier theory ( DCT ) .The SBT is based on an efficient low - energy characterization of highly correlated atoms by means of auxiliary bosonic degrees of liberty which denote collective charge excitations . In this methodology we determine the optical conductivity as well as the Hall coefficient for various values of doping concentration n . We see that both quantities exhibit nontrivial temperature dependence at low temperatures T .On the other hand , within DCT these physical observables can be determined analytically utilizing simple statements valid only at zero temperature . Our results show that there exists significant quantitative difference between estimates made by these two models .This discrepancy may serve to discriminate between them experimentally . High - temperature superconductivity has been one of the most challenging difficulties in condensed matter science over past decades 1 .Despite enormous scientific attempts 2 , its microscopic source remains unidentified . A variety of competing theory theories have been proposed 3 but none of them could give a complete explanation of all available data 4 .In particular , it was suggested 5 that the process responsible for high - temperature superconductivity might involve strong electron correlations 6 . These effects cannot be described within conventional Fermi - fluid theory 7 , 8 because they lead to non - Fermi solid behavior 9 such as power - law dependences of thermodynamic functions 10 or unusual travel effects 11 .To account for these characteristics theoretically , various phenomenological models were developed 12 notably the so - called slave - boson theory 13 . It covers the dynamics of highly interacting fermions with spin S = 1 / 2 coupled to an additional pair of bosonic fields representing collective charge fluctuations 14 .Within this framework , the ground state of the system belongs to a Bose - Einstein condensation 15 of the bosons 16 . As a result , the fermionic quasiparticles acquire finite masses 17 leading to their disappearance above some critical temperature 18 .",
        "rewrite_text": "**Title:** Electromagnetic Response of High-Tc Superconductors: Slave-Boson and Doped-Carrier Theories\n\n**Abstract:** This article investigates the electromagnetic properties of high-temperature superconductors through two distinct theoretical frameworks: the slave-boson theory (SBT) and the doped-carrier theory (DCT). The SBT employs an effective low-energy description of highly correlated atoms, utilizing auxiliary bosonic degrees of freedom to represent collective charge excitations. Through this approach, we calculate the optical conductivity and Hall coefficient across various doping concentrations (n), revealing a complex temperature dependence of these properties at low temperatures (T). Conversely, the DCT allows for analytical determination of these observables, relying on simplified assumptions that hold true only at absolute zero temperature. Our findings indicate a notable quantitative divergence between the predictions of these two models, suggesting that experimental differentiation between them is feasible. \n\nHigh-temperature superconductivity remains one of the most perplexing challenges in condensed matter physics, with its underlying mechanisms still not fully understood despite extensive research efforts. Numerous competing theories have been proposed, yet none have successfully reconciled all experimental data. It has been posited that strong electron correlations play a crucial role in the phenomenon of high-temperature superconductivity. Such correlations cannot be adequately described by conventional Fermi-liquid theory, as they lead to non-Fermi liquid behavior characterized by power-law dependencies in thermodynamic functions and atypical transport phenomena. To address these complexities, several phenomenological models have emerged, with the slave-boson theory being particularly noteworthy. This theory encapsulates the dynamics of strongly interacting fermions with spin S = 1/2, coupled to additional bosonic fields that account for collective charge fluctuations. Within this framework, the system's ground state is characterized by a Bose-Einstein condensation of the bosonic excitations, resulting in the emergence of finite masses for the fermionic quasiparticles and their eventual disappearance above a critical temperature. This study aims to deepen the understanding of high-temperature superconductivity by contrasting these two theoretical perspectives and their implications for electromagnetic responses.",
        "ori-fast-z-score": 0.7905694150420948,
        "water-fast-z-score": 7.228202652129153,
        "rewrite-fast-z-score": 1.7962924780409972
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A cold metal-poor cloud traced by a weak MgII absorption at z~0.45. First detection of SiI, CaI and FeI in a QSO absorber .\nAbstract:\nWe report the first detection of silicon (Si), calcium (Ca) and iron (Fe) ions along with magnesium (Mg) in an intervening galaxy system toward quasar HE 0515-4414 at redshift 0.4485. The observed column densities are log N(Mg+H) = 13.60 ± 0.10 cm-2 , log N(Si+H) = 12.70 ± 0.20 cm-2 , log N (Ca + H ) = 11 .90 ± 0.30 cm -2 , and log N (Fe + H ) = 10.40 ± 0.50 cm -2 .\nThe total hydrogen column density is log NH = 20.0 +0.5 -0.3 cm-2 . We find that this system has low metallicity Z < 1/100 solar abundance ratio for all four elements detected.  This system shows no detectable neutral carbon or molecular hydrogen absorptions down to limits of log NC/NH ~ -1.7 and log MH / NH ~ -3.6 respectively.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A cool metal - weak storm traced by a weak MgII absorption at z ~ 0 . 45 . First detection of SiI , CaI and FeI in a QSO absorber .Abstract : We report the first detection of silicon ( Si ) , calcium ( Ca ) and iron ( Fe ) ions along with magnesium ( Mg ) in an intervening galaxy system toward quasar HE 0515 - 4414 at redshift 0 . 4485 . The observed column densities are log N ( Mg + H ) = 13 . 60 ± 0 . 10 cm - 2 , log N ( Si + H ) = 12 . 70 ± 0 . 20 cm - 2 , log N ( Ca + H ) = 11 . 90 ± 0 . 30 cm - 2 , and log N ( Fe + H ) = 10 . 40 ± 0 . 50 cm - 2 .The total hydrogen column density is log NH = 20 . 0 + 0 . 5 - 0 . 3 cm - 2 . We see that this system has low metallicity Z < 1 / 100 solar abundance ratio for all four elements detected .This system displays no detectable neutral hydrogen or molecular hydrogen absorptions down to bounds of log NC / NH ~ - 1 . 7 and log MH / NH ~ - 3 . 6 respectively .",
        "rewrite_text": "We present the inaugural detection of silicon (Si), calcium (Ca), and iron (Fe) ions, alongside magnesium (Mg), in an intervening galaxy system associated with the quasar HE 0515-4414 at a redshift of 0.4485. Our findings reveal the following column densities: log N(Mg + H) = 13.60 ± 0.10 cm⁻², log N(Si + H) = 12.70 ± 0.20 cm⁻², log N(Ca + H) = 11.90 ± 0.30 cm⁻², and log N(Fe + H) = 10.40 ± 0.50 cm⁻². The total hydrogen column density is determined to be log NH = 20.0 +0.5 -0.3 cm⁻². Notably, the metallicity of this system is found to be low, with Z < 1/100 of the solar abundance ratio for all four detected elements. Furthermore, our observations indicate the absence of detectable neutral hydrogen or molecular hydrogen absorptions, with limits set at log NC/NH ~ -1.7 and log MH/NH ~ -3.6, respectively. This study contributes to our understanding of the chemical composition and physical conditions of intervening galaxy systems, particularly in relation to their metallicity and the presence of various ionized species. The detection of these elements in such a weakly absorbing system provides valuable insights into the processes governing metal enrichment in the universe and the evolution of galaxies. Our results underscore the significance of weak absorption features in probing the intergalactic medium and the environments surrounding quasars, paving the way for future investigations into the chemical evolution of the cosmos.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 2.4735893086356535,
        "rewrite-fast-z-score": 1.4757295747452437
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Beryllium in Ultra-Lithium-Deficient Halo Stars - The Blue Straggler Connection .\nAbstract:\nWe report the detection of beryllium (Be) lines in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240.  These are the first detections of Be in metal-poor halo stars with  Fe/H  < -2.5 dex. We find that these stars have high surface gravities for their temperatures, indicating they may be blue stragglers or other evolved objects. In addition to the Be features at 4131 Å and 4130 Å we also see evidence for an unidentified feature near 3970 Å which is likely due to C+N+O. This work was supported by NASA grant NAG5-9998. Keywords: Beryllium; Blue straggler; Metal poor star; Ultracool dwarf. 1. Introduction.\nThe discovery of extremely low-mass stars has opened up new avenues into understanding how planets form around very cool dwarfs. However, there remains much uncertainty about the formation process itself as well as the chemical composition of such systems. One important aspect of this problem involves determining whether or not terrestrial planet formation can occur within the habitable zone of ultracool dwarfs. To address this question it will be necessary to determine if the atmospheres of these stars contain significant amounts of heavy elements like carbon, nitrogen, oxygen, sulfur, sodium, potassium, magnesium, aluminum, silicon, calcium, titanium, iron, nickel, cobalt, copper, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It should be noted that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Beryllium in Ultra - Lithium - Deficient Halo Stars - The Blue Straggler Connection . Abstract : We report the discovery of beryllium ( Be ) tracks in two ultra - low metallicity halo stars , CS 22892 - 052 and HE 0107 - 5240 .These are the first detections of Be in metal - poor halo stars with Fe / H < - 2 . 5 dex . We see that these stars have high surface gravities for their altitudes , showing they may be blue stragglers or other evolution bodies .In addition to the Be properties at 4131 Å and 4130 Å we also find proof for an unidentified feature near 3970 Å which is probably due to C + N + O . This project was supported by NASA grant NAG5 - 9998 .Keywords : Beryllium ; Blue straggler ; Metal poor star ; Ultracool dwarf . 1 .Introduction . The observation of incredibly poor - density stars has opened up new avenues into studying how planets organize around very cool dwarfs .However , there exists much uncertainty about the formation transition itself as well as the chemical composition of such systems . One important dimension of this question involves knowing whether or not terrestrial planet development can occur within the habitable zone of ultracool dwarfs .To address this question it will be required to study if the atmospheres of these stars contain significant amounts of heavy components like carbon , nitrogen , oxygen , sulfur , potassium , potassium , magnesium , iron , silicon , potassium , titanium , iron , nickel , cobalt , aluminium , zinc , arsenic , selenium , silver , gold , mercury , lead , uranium , thorium , and plutonium . It should be mentioned that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars .",
        "rewrite_text": "We present our findings on the detection of beryllium (Be) in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240, marking the first observations of Be in metal-poor halo stars with iron-to-hydrogen ratios (Fe/H) less than -2.5 dex. These stars exhibit unusually high surface gravities for their altitudes, suggesting they may be classified as blue stragglers or other evolved stellar objects. Our analysis reveals significant Be absorption features at wavelengths of 4131 Å and 4130 Å, alongside evidence of an unidentified spectral feature near 3970 Å, likely attributable to the combined presence of carbon, nitrogen, and oxygen. This research was conducted with the support of NASA grant NAG5-9998. \n\nThe study of extremely low-density stars has opened new pathways for understanding the formation and organization of planetary systems around ultracool dwarfs. However, there remains considerable uncertainty regarding the transition processes involved in their formation and the chemical compositions of these systems. A critical aspect of this inquiry is determining whether terrestrial planet formation can take place within the habitable zones of ultracool dwarfs. To explore this possibility, it is essential to investigate the atmospheres of these stars for the presence of heavy elements, including carbon, nitrogen, oxygen, sulfur, potassium, magnesium, iron, silicon, titanium, nickel, cobalt, aluminum, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It is important to note that while some of these elements are produced through stellar nucleosynthesis, others are generated via cosmic ray spallation processes occurring outside of stellar environments. This research contributes to our understanding of the chemical evolution of the universe and the potential for life-supporting conditions around ultracool dwarf stars.",
        "ori-fast-z-score": -0.7492686492653552,
        "water-fast-z-score": 5.0137741307804005,
        "rewrite-fast-z-score": -1.7149858514250882
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A dynamical analysis of the 14 Her planetary system .\nAbstract:\nWe present an orbital stability study for the 14 planet system discovered by HATNet and Kepler space telescopes around the star HD 10180 (HIP 108427). We use numerical integrations to show that this system is dynamically stable over timescales longer than its age, which we estimate at 4 Gyrs using gyrochronology. The planets are found in two resonant chains with period ratios close to 2:1 and 3:2 respectively. These chains are connected through a chain of mean motion resonances between adjacent pairs of planets. This structure suggests that the system has been sculpted by convergent migration followed by tidal dissipation within each planet s envelope. \n \n Keywords: Planetary systems - Stability - Mean-motion resonance - Convergent migration - Tides - Gyrochronology - HD10180 - Kepler telescope - HATNet telescope - Orbital dynamics - Dynamical evolution",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A dynamical analysis of the 14 Her planetary system . Abstract : We report an orbital stability study for the 14 planet system discovered by HATNet and Kepler space telescopes around the star HD 10180 ( HIP 108427 ) .We use numerical integrations to see that this system is dynamically stable over timescales greater than its age , which we estimate at 4 Gyrs using gyrochronology . The planets are found in two resonant rings with time proportions close to 2 : 1 and 3 : 2 respectively .These chains are connected through a network of mean motion resonances between neighboring pairs of planets . This structure implies that the system has been sculpted by convergent displacement preceded by tidal dissipation within each planet s envelope .Keywords : Planetary systems - Stability - Mean - movement resonance - Convergent migration - Tides - Gyrochronology - HD10180 - Kepler observatory - HATNet telescope - Orbital dynamics - Dynamical evolution",
        "rewrite_text": "We present a comprehensive analysis of the orbital stability of the 14-planet system surrounding the star HD 10180 (HIP 108427), as identified by the HATNet and Kepler space telescopes. Utilizing numerical integration techniques, our study demonstrates that this planetary system exhibits dynamic stability over timescales that exceed its estimated age of approximately 4 billion years, as determined through gyrochronology methods. Our findings reveal that the planets are organized into two distinct resonant rings, with orbital period ratios that closely approximate 2:1 and 3:2. These resonant chains are interconnected by a series of mean motion resonances that occur between adjacent planetary pairs, indicating a complex dynamical architecture. The observed structure of the system suggests that it has undergone significant sculpting due to convergent migration processes, which were likely preceded by tidal dissipation within the envelopes of the individual planets. This research contributes to our understanding of planetary system formation and evolution, particularly in the context of multi-planet systems. The implications of our results extend to the fields of orbital dynamics and the long-term stability of planetary configurations, providing insights into the mechanisms that govern the arrangement and behavior of planets in such systems. Our study highlights the intricate interplay between tidal forces and resonant interactions, which play a crucial role in shaping the dynamical landscape of the HD 10180 planetary system. Keywords associated with this research include planetary systems, stability, mean motion resonance, convergent migration, tides, gyrochronology, HD 10180, Kepler observatory, HATNet telescope, orbital dynamics, and dynamical evolution.",
        "ori-fast-z-score": -0.13018891098082389,
        "water-fast-z-score": 3.3565855667130946,
        "rewrite-fast-z-score": 1.532838378934635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effects of the galactic winds on the stellar metallicity distribution of dwarf spheroidal galaxies .\nAbstract:\nWe study how the effects of galactic winds can be used to explain the observed properties of the metal-poor tail in the stellar metallicity distributions (SMDs) of nearby dwarf spheroidal galaxies (dSph). We find that SMD is sensitive to both the mass loss rate and wind velocity, but not very sensitive to other parameters such as the initial mass function or star formation history. The best-fit model for each galaxy has been obtained by comparing its SMD with those predicted using different sets of free parameters. Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolution stages. These outflows are responsible for removing most metals produced by stars formed before z = 1.5-2.0. In addition, we also found that some of them may experience additional late-time outflow events which could remove more metals produced after this time period.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effects of the galactic winds on the stellar metallicity distribution of dwarf spheroidal galaxies . Abstract : We research how the effects of galactic winds can be used to explain the observed properties of the metal - weak tail in the stellar metallicity distributions ( SMDs ) of neighbouring dwarf spheroidal galaxies ( dSph ) .We see that SMD is sensitive to both the mass loss rate and wind velocity , but not very sensitive to other parameters such as the early mass value or star formation history . The best - fitting model for each galaxy has been achieved by comparing its SMD with those predicted use different sets of free parameters .Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolved stages . These outflows are responsible for eliminating most metals produced by stars formed before z = 1 . 5 - 2 . 0 .In addition , we also discovered that some of them may experience additional late - time outflow events which could remove more metals produced after this time time .",
        "rewrite_text": "In this study, we investigate the influence of galactic winds on the stellar metallicity distribution (SMD) of nearby dwarf spheroidal galaxies (dSph). Our analysis focuses on how these winds contribute to the formation of the metal-poor tail observed in the SMDs of these galaxies. We find that the SMD is particularly sensitive to parameters such as the mass loss rate and wind velocity, while showing less sensitivity to factors like the initial mass and the history of star formation. To identify the most accurate models for each galaxy, we compare their observed SMDs with theoretical predictions generated using various sets of free parameters. Our findings indicate that all examined dSph galaxies have undergone significant outflows driven by supernova explosions during their formative periods. These outflows have played a crucial role in expelling a substantial portion of the metals produced by stars that formed prior to redshift z = 1.5 - 2.0. Furthermore, we uncover evidence suggesting that some dSph galaxies may have experienced additional outflow events at later stages, which could further deplete their metallic content by removing metals generated after the initial outflow phase. This research enhances our understanding of the complex interplay between galactic winds and stellar evolution, shedding light on the chemical enrichment processes in dwarf spheroidal galaxies.",
        "ori-fast-z-score": 1.078327732034384,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 0.40406101782088427
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The VLT-FLAMES survey of massive stars: Evolution of surface N abundances and effective temperature scales in the Galaxy and Magellanic Clouds .\nAbstract:\nWe present new spectroscopic observations for more than 1000 Galactic OB supergiants, obtained with FLAMES/GIRAFFE at the Very Large Telescope (VLT). The sample includes all known O-type dwarfs and giants as well as B-type supergiants brighter than about Mbol = -4 mag within 25 pc distance to Earth. We derive atmospheric parameters T eff , log g, microturbulence velocity vmic, and chemical composition including nitrogen abundance  N/Fe  . For comparison we also analyse a large number of Galactic red supergiants observed by GOSSS project using similar methods. Our results show that there is no significant difference between the mean values of these quantities derived for both samples. However, our analysis reveals systematic differences between different studies based on smaller samples published so far. In particular, we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non-LTE effects or underestimating gravities because they did not take into account stellar winds.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The VLT - FLAMES search of large galaxies : Evolution of surface N abundances and effective heat scales in the Galaxy and Magellanic Clouds . Abstract : We report new spectroscopic observations for more than 1000 Galactic OB supergiants , obtained with FLAMES / GIRAFFE at the Very Large Telescope ( VLT ) .The sample comprises all known O - class dwarfs and giants as well as B - class supergiants hotter than about Mbol = - 4 mag within 25 pc proximity to Earth . We derive air parameters T eff , log g , microturbulence velocity vmic , and biological composition including nitrogen density N / Fe .For comparison we also analyse a large number of Galactic red supergiants detected by GOSSS program use similar methods . Our results show that there is no considerable difference between the mean values of these quantities calculated for both samples .However , our analysis reveals systematic differences between various surveys based on smaller specimens released so far . In particular , we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non - LTE effects or underestimating gravities because they did not take into consideration stellar winds .",
        "rewrite_text": "We present new spectroscopic observations of over 1,000 Galactic OB supergiants, conducted using the FLAMES/GIRAFFE instrument at the Very Large Telescope (VLT). This extensive sample includes all known O-class dwarfs and giants, as well as B-class supergiants with absolute magnitudes brighter than approximately Mbol = -4 mag, located within a 25 parsec radius of Earth. Our analysis focuses on deriving key atmospheric parameters such as effective temperature (T_eff), surface gravity (log g), microturbulence velocity (v_mic), and the nitrogen-to-iron (N/Fe) abundance ratio. To provide a comprehensive comparison, we also examine a significant number of Galactic red supergiants identified through the GOSSS program, employing similar analytical techniques. \n\nOur findings indicate that there are no significant differences in the mean values of these atmospheric parameters between the OB supergiants and the red supergiants. However, we uncover systematic discrepancies when comparing our results to those from previous surveys that utilized smaller sample sizes. Notably, we observe that many earlier studies have overestimated the temperatures of the hotter stars, primarily due to the neglect of non-local thermodynamic equilibrium (non-LTE) effects. Additionally, some studies have underestimated the surface gravities of these stars by failing to account for the influence of stellar winds. This research contributes to a more nuanced understanding of the evolution of surface nitrogen abundances and effective heat scales in both the Milky Way and the Magellanic Clouds, highlighting the importance of large sample sizes and accurate modeling in stellar astrophysics.",
        "ori-fast-z-score": 0.8728715609439696,
        "water-fast-z-score": 5.822969752985674,
        "rewrite-fast-z-score": 1.436739427831727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Star Formation in the Bok Globule CB54 .\nAbstract:\nWe present near-infrared (NIR) imaging and spectroscopy of star formation activity in the Bok globule CB 54, which is located at a distance of about 1 kpc toward the Galactic anti-center direction. We found that there are two young stellar objects (YSOs), one Class I protostar with an infrared luminosity of ~10 Lsun and another embedded YSO candidate with a bolometric temperature of ~1000 K. The former object shows bipolar outflows traced by Herbig-Haro knots and molecular line wings. In addition to these two sources, we detected several other point-like NIR sources within the central region of CB 54. These may be low-mass pre-main-sequence stars or background galaxies. Our results suggest that this cloud core has experienced active star formation over its lifetime. \n \n Keywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar object",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Star Formation in the Bok Globule CB54 . Abstract : We present near - infrared ( NIR ) imaging and spectroscopy of star formation activity in the Bok globule CB 54 , which is situated at a distance of about 1 kpc toward the Galactic anti - center position .We showed that there are two young stellar bodies ( YSOs ) , one Class I protostar with an infrared luminosity of ~ 10 Lsun and another embedded YSO candidate with a bolometric temperature of ~ 1000 K . The former object displays bipolar outflows traced by Herbig - Haro knots and molecular line tails . In addition to these two sources , we spotted many other point - like NIR sources within the central region of CB 54 .These may be low - weight pre - principal - sequence stars or background galaxies . Our results show that this storm core has undergone active star formation over its lifetime .Keywords : Star formation ; Near - infrared spectroscopy ; Bipolar outflow ; Young stellar object",
        "rewrite_text": "We present a comprehensive study of star formation in the Bok globule CB54, located approximately 1 kpc from Earth in the direction of the Galactic anti-center. Utilizing near-infrared (NIR) imaging and spectroscopy, we identified two young stellar objects (YSOs) within this region. The first is a Class I protostar exhibiting an infrared luminosity of around 10 Lsun, while the second is an embedded YSO candidate characterized by a bolometric temperature of approximately 1000 K. The Class I protostar is particularly notable for its bipolar outflows, which are evidenced by the presence of Herbig-Haro knots and molecular line tails associated with its outflow activity. In addition to these primary sources, our observations revealed numerous other point-like NIR sources scattered throughout the central area of CB54. These additional sources are likely to be low-mass pre-main-sequence stars or possibly background galaxies. The findings from our study indicate that the CB54 globule has experienced significant star formation activity throughout its existence, contributing to our understanding of the processes involved in stellar birth within dense molecular clouds. This research enhances our knowledge of the dynamics and characteristics of star formation in Bok globules, providing valuable insights into the early stages of stellar evolution. \n\nKeywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar object.",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 3.0,
        "rewrite-fast-z-score": 1.4439897447623107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonequilibrium entropy limiters in lattice Boltzmann methods .\nAbstract:\nWe present an approach to nonequilibrium entropy production in the lattice Boltzmann (LB) method by introducing entropy limiters into the collision operator. The proposed scheme is shown to be able to reproduce the correct equilibrium distribution and recover the second law of thermodynamics for both single-phase flows with constant density and temperature, as well as multiphase flows with phase change. We also demonstrate that our new LB model can accurately capture shock waves without spurious oscillations or numerical instabilities. \n \n Keywords: Nonequilibrium entropy, Lattice Boltzmann Method, Entropy limiter, Second Law of Thermodynamics, Shock wave. 1 Introduction \n \n In recent years, there has been growing interest in developing computational fluid dynamics methods based on kinetic theory  1–3  . Compared with conventional Navier-Stokes solvers, these approaches are more accurate at capturing complex flow phenomena such as shocks  4  , turbulence  5  , and interfacial flows  6  . Among them, the lattice Boltzmann method  7, 8  has attracted much attention due to its simplicity and efficiency  9  . \n \n However, it should be noted that most existing LB models do not satisfy the second law of thermodynamic  10  . This problem becomes particularly severe when dealing with high Mach number flows  11  . To overcome this difficulty, several attempts have been made recently  12–18  . For example, Chen et al.  12  introduced a modified BGK-type collision term which recovers the correct equilibrium state while satisfying the second law of thermodynamical. Similarly, Yu et al.  13  developed another type of entropy-consistent LB schemes using the concept of entropic moments. More recently, Shan et al.  14  presented a novel LB model where the relaxation time was determined according to the local Knudsen number. Although these works provide promising results, they all require additional information about the macroscopic variables, e.g., pressure and velocity fields. As a result, their applications may be limited to simple cases involving only one component gas. \n \n In contrast, we propose here a general framework for constructing entropy-consistent LB models. Our strategy relies on adding",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonequilibrium entropy limiters in lattice Boltzmann techniques . Abstract : We present an approach to nonequilibrium entropy production in the lattice Boltzmann ( LB ) method by using entropy limiters into the collision operator .The proposed system is demonstrated to be possible to reproduce the correct equilibrium distribution and recover the second law of thermodynamics for both single - phase streams with constant density and heat , as well as multiphase streams with phase change . We additionally prove that our new LB model can accurately capture shock waves without spurious oscillations or numerical instabilities .Keywords : Nonequilibrium entropy , Lattice Boltzmann Method , Entropy limiter , Second Law of Thermodynamics , Shock wave . 1 Introduction In recent seasons , there has been growing interest in establishing computational liquid mechanics algorithms based on kinetic theory 1 – 3 .Compared with typical Navier - Stokes solvers , these method are more accurate at representing complex flow effects such as shocks 4 , turbulence 5 , and interfacial flows 6 . Among them , the lattice Boltzmann technique 7 , 8 has garnered great controversy due to its accuracy and efficiency 9 .However , it should be mentioned that most existing LB models do not satisfy the second law of thermodynamic 10 . This problem appears particularly severe when dealing with high Mach number flows 11 .To solve this challenge , various efforts have been attempted recently 12 – 18 . For instance , Chen et al .12 introduced a revised BGK - class collision term which recovers the correct equilibrium state while fulfilling the second law of thermodynamical . Similarly , Yu et al .13 introduced another type of entropy - consistent LB schemes using the idea of entropic moments . More recently , Shan et al .14 provided a new LB model where the relaxation time was decided according to the local Knudsen number . Although these works provide encouraging conclusions , they all need extra data about the macroscopic parameters , e . g . , pressure and speed fields .As a result , their applications might be restricted to small cases concerning only one element gas . In comparison , we require here a general template for constructing entropy - consistent LB models .Our strategy relies on adding",
        "rewrite_text": "We introduce a novel approach to nonequilibrium entropy production within the lattice Boltzmann (LB) framework by incorporating entropy limiters into the collision operator. This innovative methodology successfully reproduces the correct equilibrium distribution and adheres to the second law of thermodynamics across various scenarios, including single-phase flows characterized by constant density and heat, as well as multiphase flows involving phase changes. Furthermore, our enhanced LB model demonstrates a remarkable ability to accurately capture shock waves without introducing spurious oscillations or numerical instabilities, which are common challenges in computational fluid dynamics. \n\nThe motivation for this research stems from the increasing interest in developing computational fluid mechanics algorithms grounded in kinetic theory, particularly in light of their superior accuracy in modeling complex flow phenomena such as shocks, turbulence, and interfacial dynamics compared to traditional Navier-Stokes solvers. Despite the advantages of the lattice Boltzmann method, many existing models fail to comply with the second law of thermodynamics, a limitation that becomes particularly pronounced in high Mach number flows. Recent efforts to address this issue have included the introduction of revised collision terms and entropy-consistent schemes; however, these approaches often require additional data regarding macroscopic parameters, which can limit their applicability to more complex scenarios.\n\nIn contrast, our proposed framework offers a more generalized approach to constructing entropy-consistent LB models, eliminating the need for extensive macroscopic data. By integrating entropy limiters into the collision operator, we provide a robust solution that not only ensures compliance with thermodynamic principles but also enhances the model's capability to handle a wider range of fluid dynamics applications. This work contributes to the ongoing development of more accurate and efficient computational methods in fluid mechanics, paving the way for future advancements in the field. \n\nKeywords: Nonequilibrium entropy, Lattice Boltzmann Method, Entropy limiter, Second Law of Thermodynamics, Shock wave.",
        "ori-fast-z-score": -2.0179913668364655,
        "water-fast-z-score": 6.577497476540163,
        "rewrite-fast-z-score": 0.8006407690254357
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observations on degenerate saddle point problems .\nAbstract:\nWe consider the problem of finding an approximate solution to a convex optimization problem with a nonconvex objective function and linear constraints, where the feasible region is defined by a set of equality or inequality constraints.  We show that under certain conditions this problem can be solved efficiently using a combination of gradient descent methods for solving the subproblems in each iteration and a line search method based on the Armijo-Goldstein condition.  The proposed algorithm has been implemented as part of the open source software package CVXPY (http://cvxpy.org/).  Numerical experiments are presented which demonstrate the effectiveness of our approach. Keywords: Convex Optimization, Nonconvex Objective Function, Saddle Point Problem, Gradient Descent Method, Line Search Algorithm, Armijo-Goldstain Condition. 1 Introduction In many practical applications it may not always be possible to find an exact solution to a given mathematical model due to computational complexity issues associated with the underlying numerical algorithms used to solve such models.  For example, in some cases it might only be possible to obtain an approximate solution within a specified tolerance level.  This situation arises frequently when dealing with large-scale nonlinear programming problems arising in various fields including engineering design, operations research, economics, etc., see e.g.,  1  ,  4  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observations on degenerate saddle point questions . Abstract : We consider the question of finding an approximate solve to a convex optimization problem with a nonconvex objective function and linear constraints , where the feasible region is characterized by a setting of equality or inequality constraints .We see that under certain conditions this question can be answered easily using a combination of gradient descent methods for solving the subproblems in each iteration and a line search method using on the Armijo - Goldstein condition . The proposed algorithm has been deployed as part of the open source software product CVXPY ( http : / / cvxpy . org / ) .Numerical observations are presented which demonstrate the effectiveness of our approach . Keywords : Convex Optimization , Nonconvex Objective Function , Saddle Point Problem , Gradient Descent Method , Line Search Algorithm , Armijo - Goldstain Condition .1 Introduction In many practical applications it could not always be possible to find an precise answer to a given numerical model owing to computational difficulty issues associated with the fundamental numerical algorithms used to solve such models . For instance , in some cases it could only be possible to obtain an approximate solve within a specified tolerance range .This condition occurs commonly when dealing with large - scale nonlinear programming difficulties arising in different fields including industrial design , management research , economics , etc . , see e . g . , 1 , 4 .",
        "rewrite_text": "**Title:** Observations on Degenerate Saddle Point Questions\n\n**Abstract:** This article addresses the challenge of approximating solutions to convex optimization problems characterized by nonconvex objective functions and linear constraints. The feasible region in these problems is defined by a set of equality or inequality constraints. We demonstrate that, under specific conditions, this challenge can be effectively tackled using a combination of gradient descent methods for the subproblems encountered in each iteration, alongside a line search strategy that adheres to the Armijo-Goldstein condition. The algorithm developed from this approach has been integrated into the open-source software CVXPY (http://cvxpy.org/), enhancing its utility for practitioners in the field. Our numerical experiments illustrate the efficacy of the proposed method, showcasing its ability to yield satisfactory approximate solutions in various scenarios. \n\nIn practical applications, achieving an exact solution to numerical models is often hindered by computational complexities associated with traditional numerical algorithms. This is particularly evident in large-scale nonlinear programming problems that arise across diverse domains such as industrial design, management science, and economics. In many instances, practitioners are only able to obtain approximate solutions within a predefined tolerance range, highlighting the importance of developing robust algorithms that can navigate these challenges effectively. Our findings contribute to the existing body of knowledge by providing insights into the behavior of degenerate saddle point problems and offering a viable algorithmic solution that can be readily applied in real-world contexts. \n\n**Keywords:** Convex Optimization, Nonconvex Objective Function, Saddle Point Problem, Gradient Descent Method, Line Search Algorithm, Armijo-Goldstein Condition.",
        "ori-fast-z-score": 0.1873171623163388,
        "water-fast-z-score": 6.434283176858165,
        "rewrite-fast-z-score": 0.8049844718999243
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An iterative method to compute the sign function of a non-Hermitian matrix and its application to the overlap Dirac operator at nonzero chemical potential .\nAbstract:\nWe present an efficient algorithm for computing the sign function of a large sparse complex matrix, which is based on the Lanczos bidiagonalization process with partial reorthogonalization. The proposed algorithm can be applied to any Hermitian or non-Hermitian matrices without restriction. We apply this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density. In particular we show that our algorithm works well even when the quark mass becomes small compared to the inverse of the lattice spacing. This work was supported by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan. PACS numbers: 11.15.Ha, 12.38.Qk, 12.39.Fe, 14.20 .Dh  1 Introduction Lattice Quantum Chromodynamics(LQCD), as one of the most promising candidates for describing strong interactions among quarks and gluons, has been widely used to study hadronic properties such as masses and decay constants  1  . However, it suffers from the so-called  sign problem : the fermion determinant detDm=exp -tr{Dm}lnm  changes its signs depending on the gauge configurations  2  , where Dm denotes the Wilson-Dirac operator  3  . Therefore, Monte Carlo methods cannot be directly employed to calculate physical quantities using LQCD because they require positive definite weight functions  4  .\nIn order to overcome this difficulty, several approaches have been developed so far  5  -  8  . Among them, the Taylor expansion approach  9  -  11  seems to be very powerful since it allows us to evaluate the expectation value of any observables accurately within statistical errors. It also enables us to perform calculations at high temperature and/or high density  12  -  14  . For example, the Taylor expansion up to O(a6) has already been performed successfully  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An iterative method to compute the sign function of a non - Hermitian matrix and its application to the overlap Dirac operator at nonzero chemical potential . Abstract : We introduce an efficient algorithm for calculation the sign function of a large sparse complex matrix , which is based on the Lanczos bidiagonalization process with partial reorthogonalization .The proposed algorithm can be applied to any Hermitian or non - Hermitian matrices without limitation . We introduce this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density .In particular we prove that our algorithm runs good even when the quark mass becomes tiny relative to the inverse of the lattice spacing . This research was supported by Grants - in - Aid for Scientific Research ( No .20340040 ) from MEXT Japan . PACS scores : 11 . 15 . Ha , 12 . 38 . Qk , 12 . 39 . Fe , 14 . 20 . Dh 1 Introduction Lattice Quantum Chromodynamics ( LQCD ) , as one of the most attractive candidates for describing strong interactions among quarks and gluons , has been widely using to study hadronic properties such as masses and decay constants 1 .However , it suffers from the so - called sign problem : the fermion determinant detDm = exp - tr { Dm } lnm varies its signs depending on the gauge configurations 2 , where Dm denotes the Wilson - Dirac operator 3 . Therefore , Monte Carlo methods never be directly used to estimate mechanical quantities using LQCD because they use positive definite weight functions 4 .In order to overcome this obstacle , various approaches have been formulated so far 5 - 8 . Among them , the Taylor expansion method 9 - 11 seems to be very potent since it allows us to analyze the expectation value of any observables correctly within statistical errors .It additionally lets us to conduct measurements at high heat and / or large velocity 12 - 14 . For instance , the Taylor expansion up to O ( a6 ) has already been performed successfully 15 .",
        "rewrite_text": "We present a novel and efficient algorithm designed for computing the sign function of large sparse complex matrices, leveraging the Lanczos bidiagonalization process with partial reorthogonalization. This method is versatile, applicable to both Hermitian and non-Hermitian matrices without restrictions. We specifically implement this algorithm in the context of the overlap Dirac operator within lattice Quantum Chromodynamics (LQCD) simulations at finite density. Our findings demonstrate that the algorithm maintains robust performance even when the quark mass approaches a value that is small compared to the inverse of the lattice spacing. This research is supported by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan, and it contributes to the ongoing efforts to address the sign problem in LQCD. The sign problem arises due to the fermion determinant, represented as det(Dm) = exp(-tr{Dm})lnm, which fluctuates in sign based on different gauge configurations, where Dm is the Wilson-Dirac operator. This fluctuation complicates the application of Monte Carlo methods, which rely on positive definite weight functions for estimating physical quantities. To tackle this challenge, various strategies have been proposed in the literature. Among these, the Taylor expansion method has emerged as a particularly effective approach, enabling accurate analysis of observable expectation values within statistical uncertainties. This method also facilitates measurements at high temperatures and/or significant velocities. Notably, successful implementations of the Taylor expansion up to O(a^6) have already been achieved, showcasing its potential in advancing our understanding of hadronic properties in LQCD.",
        "ori-fast-z-score": -0.08873565094161139,
        "water-fast-z-score": 5.89902107238168,
        "rewrite-fast-z-score": 0.9805806756909202
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Studying Reionization with Ly-alpha Emitters .\nAbstract:\nWe present the results of our search for high redshift galaxies using narrowband imaging in the near-infrared (NIR) and optical bands, as well as spectroscopic follow-up observations at Keck Observatory. We have discovered two new z>6 galaxy candidates by searching for strong emission lines such as Lyman alpha or CIV in their rest frame UV spectra. The first candidate is an extremely bright object that has been detected both photometrically and spectroscopically to be at z=6.96. This source shows no evidence of AGN activity but does show signs of star formation. The second candidate was found serendipitously during our survey of another target; it also appears to be at z=6-7 based on its NIR colors and lack of detection in the B-band filter. Both sources are among the most distant known galaxies ever observed. These objects will provide important constraints on models of reionization because they can be used to study how ionizing photons escape into the intergalactic medium.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Studying Reionization with Ly - alpha Emitters . Abstract : We present the results of our search for high redshift galaxies using narrowband scanning in the near - infrared ( NIR ) and imaging bands , as well as spectroscopic follow - up observations at Keck Observatory .We have discovered two new z > 6 galaxy nominees by searching for strong absorption lines such as Lyman alpha or CIV in their rest frame UV spectra . The first candidate is an incredibly bright object that has been detected both photometrically and spectroscopically to be at z = 6 . 96 .This source shows no evidence of AGN activity but does display signs of star formation . The second candidate was seen serendipitously during our survey of another target ; it also seems to be at z = 6 - 7 based on its NIR colors and lack of detection in the B - band filter .Both sources are among the most distant known galaxies yet observed . These bodies will provide important restrictions on models of reionization because they can be used to study how ionizing photons escape into the intergalactic medium .",
        "rewrite_text": "In this study, we report on our investigation of high-redshift galaxies, focusing on the search for Ly-alpha emitters as a means to explore the epoch of reionization. Utilizing narrowband scanning techniques in the near-infrared (NIR) alongside imaging bands, we conducted spectroscopic follow-up observations at the Keck Observatory. Our efforts have led to the identification of two new candidates for galaxies at redshift z > 6, distinguished by the presence of strong absorption lines, including Lyman-alpha and CIV, in their rest-frame ultraviolet spectra. \n\nThe first candidate, an exceptionally luminous object, has been confirmed through both photometric and spectroscopic data to reside at z = 6.96. Notably, this galaxy exhibits no signs of active galactic nucleus (AGN) activity, indicating that its brightness is likely attributed to ongoing star formation processes. The second candidate was serendipitously discovered while surveying another target; it appears to fall within the redshift range of z = 6 to 7, inferred from its NIR color characteristics and the absence of detection in the B-band filter.\n\nBoth of these galaxies rank among the most distant known entities in the universe, providing a unique opportunity to refine our understanding of the reionization process. Their study is crucial, as they may offer insights into the mechanisms by which ionizing photons escape into the intergalactic medium, thereby influencing the ionization state of the universe during a pivotal epoch in cosmic history. Our findings contribute to the ongoing efforts to map the early universe and enhance our comprehension of galaxy formation and evolution in the context of reionization.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.0769355644102245,
        "rewrite-fast-z-score": 1.4501047335684953
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Regularization of spherical and axisymmetric evolution codes in numerical relativity .\nAbstract:\nWe present the regularized version of two popular families of evolution codes for general relativity, namely those based on the BSSN formulation and those using the moving puncture approach. The new schemes are designed to be used with any gauge conditions that can be written as first-order differential equations. We show how these schemes can be implemented into existing codes by adding only one extra equation per time step. In addition we provide an explicit expression for the source term appearing in this additional equation which is valid for all gauges considered here. Finally, we demonstrate the performance of our scheme numerically by evolving several test problems including black hole spacetimes containing apparent horizons. Our results indicate that the new scheme performs well even when evolved over many dynamical timescales. This work was supported by NSF grant PHY-0456728 (J.M.) and NASA grants NNG06GH50G and NNX07AG77G (A.S.).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Regularization of spherical and axisymmetric evolution coding in mathematical relativity . Abstract : We present the regularized version of two famous families of evolution coding for general relativity , principally those built on the BSSN formulation and those adopting the sliding puncture method .The modern systems are intended to be used with any gauge conditions that can be written as first - order differential equations . We see how these schemes can be applied into older codes by added only one extra equation per time step .In addition we provide an explicit expression for the origin term appearing in this added equation which is valid for all gauges mentioned here . Finally , we prove the performance of our scheme numerically by expanding numerous test issues including black hole spacetimes containing visible horizons .Our results show that the new method performs good even when evolved over numerous dynamical timescales . This project was supported by NSF grant PHY - 0456728 ( J . M . )and NASA grants NNG06GH50G and NNX07AG77G (A.S.).",
        "rewrite_text": "In this article, we introduce a regularized approach to two prominent families of evolution coding utilized in the field of mathematical relativity, specifically focusing on the BSSN formulation and the sliding puncture method. Our modernized systems are designed to accommodate any gauge conditions that can be expressed as first-order differential equations. We demonstrate that these advanced schemes can be seamlessly integrated into existing codes by incorporating just one additional equation at each time step. Furthermore, we provide a detailed expression for the origin term that arises in this supplementary equation, which is applicable across all the gauge conditions discussed. To validate the efficacy of our proposed method, we conduct a series of numerical tests, including scenarios involving black hole spacetimes with discernible horizons. The results of our simulations indicate that the new regularization technique maintains robust performance, even when applied over extended dynamical timescales. This research was made possible through the support of NSF grant PHY-0456728 (J.M.) and NASA grants NNG06GH50G and NNX07AG77G (A.S.). Our findings contribute to the ongoing development of more reliable and efficient computational methods in general relativity, paving the way for future advancements in the simulation of complex gravitational systems.",
        "ori-fast-z-score": -1.7457431218879391,
        "water-fast-z-score": 5.09786575873842,
        "rewrite-fast-z-score": -0.10369516947304253
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New possible properties of atomic nuclei investigated by non linear methods: Fractal and recurrence quantification analysis .\nAbstract:\nThe present work is devoted to the investigation of new possible nuclear properties using nonlinear methods, namely fractal dimension (FD) and recurrence quantification analysis (RQA). The FD was calculated for different mass number A in order to study its dependence on the system size. It has been found that there exists an optimal value of the scaling parameter which provides the best results for each nucleus separately. This fact indicates that the structure of the studied systems can be characterized as multifractals with non-trivial behavior. In addition we have shown that the obtained values are very close to those predicted by the percolation theory. We also applied RQA method to investigate the temporal evolution of the considered systems. Our calculations show that the complexity of the time series increases with increasing mass number A. Moreover it turns out that this increase follows closely the predictions made within the framework of the Random Matrix Theory. Finally we have compared our results with experimental data available for some light nuclei.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : New possible characteristics of nuclear nuclei investigated by non discrete methods : Fractal and recurrence quantification analysis . Abstract : The present work is devoted to the examination of new possible nuclear properties using nonlinear methods , notably fractal depth ( FD ) and recurrence quantification analysis ( RQA ) .The FD was calculated for different mass quantity A in order to study its dependence on the system size . It has been shown that there exists an appropriate value of the scaling parameter which offers the best results for each particle separately .This fact suggests that the composition of the studied structures can be described as multifractals with non - trivial properties . In addition we have shown that the achieved values are very close to those predicted by the percolation theory .We also used RQA approach to examine the temporal evolution of the considered systems . Our calculations show that the complexity of the period series increases with expanding mass quantity A .Moreover it turns out that this increase follows carefully the assumptions done within the framework of the Random Matrix Theory . Finally we have linked our findings with observation information available for some light nuclei .",
        "rewrite_text": "This study explores novel characteristics of nuclear nuclei through the application of nonlinear analytical methods, specifically focusing on fractal depth (FD) and recurrence quantification analysis (RQA). The research investigates how the fractal depth varies with different mass numbers (A), aiming to understand its relationship with system size. Our findings indicate that there exists an optimal scaling parameter that yields the most accurate results for each individual particle, suggesting that the structures under investigation can be effectively described as multifractals exhibiting complex properties. Notably, the FD values obtained closely align with predictions made by percolation theory, reinforcing the relevance of this theoretical framework in understanding nuclear properties.\n\nIn addition to fractal analysis, we employed RQA to assess the temporal dynamics of the nuclear systems examined. The results reveal that as the mass number A increases, the complexity of the periodic series also rises. This complexity growth adheres to the principles outlined in Random Matrix Theory, indicating a deeper connection between the statistical properties of nuclear systems and established theoretical constructs. Furthermore, we have correlated our analytical results with empirical observations from certain light nuclei, providing a comprehensive perspective on the implications of our findings. Overall, this work contributes to the understanding of nuclear structure by integrating advanced nonlinear methods, paving the way for future research in the field.",
        "ori-fast-z-score": -0.6123724356957946,
        "water-fast-z-score": 6.262945776223707,
        "rewrite-fast-z-score": 1.3228756555322951
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ultra-relativistic geometrical shock dynamics and vorticity .\nAbstract:\nWe present an analysis of the relativistic Riemann problem for ideal fluids in two space dimensions, with emphasis on the role played by vortex sheets. We show that the solution to this problem can be constructed as a sequence of self-similar solutions which are determined uniquely up to translations along the x-axis (the direction of propagation). The first step is to construct a family of exact solutions describing the interaction between a planar shock wave and a vortex sheet. These solutions have been obtained previously using different methods but we provide here a new derivation based on the method of characteristics. In particular, we obtain explicit expressions for the density and pressure profiles across the shock front. Next, we consider the case where the initial data consists of a single vortex sheet separating regions of constant density and pressure. This situation corresponds physically to a fluid initially at rest being accelerated impulsively by a piston moving at speed c = 1. We prove that there exists exactly one such solution corresponding to each value of the total mass M . Finally, we study numerically the evolution of more general initial data consisting of several vortex sheets separated by shocks.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ultra - relativistic geometrical shock mechanics and vorticity . Abstract : We present an assessment of the relativistic Riemann problem for perfect fluids in two space dimensions , with emphasis on the part played by vortex sheets .We see that the solve to this question can be built as a sequence of self - similar solutions which are decided uniquely up to translations along the x - axis ( the direction of propagation ) . The first step is to build a family of precise solutions involving the interaction between a planar blast flow and a vortex sheet .These solutions have been achieved formerly using separate methods but we provide here a new derivation based on the method of characteristics . In particular , we obtain explicit expressions for the density and tension characteristics across the shock front .Next , we imagine the case where the first data composed of a single vortex sheet separating areas of constant density and pressure . This condition relates visually to a fluid initially at rest being advanced impulsively by a cylinder moved at speed c = 1 .We prove that there exists precisely one such solution corresponding to each value of the total mass M . Finally , we study numerically the evolution of more general initial evidence consisting of several vortex strands divided by shocks .",
        "rewrite_text": "In this article, we explore the relativistic Riemann problem for perfect fluids in two spatial dimensions, focusing on the significant role of vortex sheets. Our investigation reveals that the solution to this problem can be constructed as a series of self-similar solutions, which are uniquely determined up to translations along the x-axis, the direction of propagation. The initial phase of our study involves developing a family of precise solutions that describe the interaction between a planar blast flow and a vortex sheet. While these solutions have been derived previously through various methods, we present a novel derivation utilizing the method of characteristics. This approach allows us to derive explicit expressions for the density and tension characteristics across the shock front, enhancing our understanding of the dynamics involved.\n\nSubsequently, we consider a scenario where the initial conditions consist of a single vortex sheet that separates regions of constant density and pressure. This configuration can be visualized as a fluid at rest being suddenly set in motion by a cylinder traveling at speed c = 1. We demonstrate that for each total mass M, there exists a unique solution corresponding to these initial conditions. Finally, we extend our analysis to the numerical study of more complex initial configurations, which include multiple vortex strands separated by shock waves. This comprehensive examination not only deepens our understanding of ultra-relativistic geometrical shock mechanics but also highlights the intricate interplay between fluid dynamics and relativistic effects in the presence of vorticity.",
        "ori-fast-z-score": -0.9712858623572641,
        "water-fast-z-score": 5.965952781626132,
        "rewrite-fast-z-score": 1.3636363636363635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comparative study of complex N- and O-bearing molecules in hot molecular cores .\nAbstract:\nWe present the results of an unbiased survey for N-bearing species (NH3, N2H+) and O-bearing species (H2O, OH, CH3OH, HCO+, H2S) toward two massive star forming regions associated with high-mass protostars. The observations were carried out using the Submillimeter Array at 1.4 mm wavelength. We detected all these species except NH3 towards both sources. In addition to detections we also made upper limits on some other species like SO2, SiO, HNCO etc.. Our main findings are as follows:  -The abundance ratios between different species show that H2O is more abundant than any other molecule by several orders of magnitude.  -The derived column densities suggest that the physical conditions vary significantly across the observed region. -The chemical composition of the gas shows significant differences between the two sources. -The comparison of our data with previous studies suggests that there may be multiple evolutionary stages within each source.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comparative study of complex N - and O - bearing compounds in heated molecular cores . Abstract : We report the conclusion of an unbiased survey for N - bearing species ( NH3 , N2H + ) and O - bearing species ( H2O , OH , CH3OH , HCO + , H2S ) toward two massive star producing regions associated with high - weight protostars .The observed were carried out utilizing the Submillimeter Array at 1 . 4 nm wavelength . We observed all these species except NH3 towards both sources .In addition to detections we also took upper limits on some other species like SO2 , SiO , HNCO etc . . Our main results are as follows : - The density proportions between various genera demonstrate that H2O is more rich than any other atom by many orders of magnitude . - The derived column densities suggest that the physical conditions varies dramatically across the seen region .- The chemical composition of the gas displays substantial differences between the two sources . - The comparison of our information with previous research suggests that there may be several evolutionary stages within each source .",
        "rewrite_text": "In this study, we present the findings of an extensive and unbiased survey focusing on nitrogen-bearing (N-bearing) and oxygen-bearing (O-bearing) compounds within heated molecular cores, specifically in two massive star-forming regions associated with high-mass protostars. The observations were conducted using the Submillimeter Array at a wavelength of 1.4 mm, targeting key species including NH3, N2H+, H2O, OH, CH3OH, HCO+, and H2S. Notably, we successfully detected all targeted species except for NH3 in both regions. Additionally, we established upper limits for several other compounds, such as SO2, SiO, and HNCO.\n\nOur analysis yielded several significant results. Firstly, we found that the abundance of H2O is markedly higher than that of other species, exceeding them by several orders of magnitude. This indicates a pronounced disparity in the chemical composition of the gas across the observed regions. Secondly, the derived column densities reveal that the physical conditions within these regions vary significantly, suggesting a complex interplay of environmental factors influencing molecular formation. Furthermore, a comparative analysis of our findings with previous studies indicates the presence of multiple evolutionary stages within each star-forming region, highlighting the dynamic nature of these environments.\n\nOverall, our research contributes to a deeper understanding of the chemical diversity and physical conditions in massive star-forming regions, providing insights into the processes that govern molecular formation and evolution in the universe.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 5.158920219515446,
        "rewrite-fast-z-score": 0.3746343246326776
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Long time existence of smooth solutions for the rapidly rotating shallow-water and Euler equations .\nAbstract:\nWe prove that there exists an open set of initial data such that the corresponding solution to the rapidly rotating shallow water (RSW) or Euler equations is globally smooth in space-time, provided it stays sufficiently close to some equilibrium state. The proof relies on a combination of energy estimates with suitable weighted Sobolev inequalities. We also show how our results can be extended to more general systems of conservation laws which are not necessarily strictly hyperbolic. In particular we consider the case where one of the characteristic speeds vanishes at a point but remains bounded away from zero elsewhere. \nIntroduction\n\nThe purpose of this work is twofold. First, we establish global regularity properties for the rapidly rotating shallow water system and its generalization to arbitrary dimension. Second, we provide new insights into the structure of these models by showing how they fit within a larger class of nonlinear dispersive partial differential equations. Our main result reads as follows: \nMain Theorem 1 Let u 0 ∈ H s , s > n/2 + 2, then there exist constants C = C(n), K = K(n) such that if",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Long time existence of smooth answers for the rapidly spinning shallow - water and Euler equations . Abstract : We prove that there exists an open set of initial data such that the equivalent solve to the rapidly spinning shallow water ( RSW ) or Euler equations is globally smooth in space - time , provided it remains reasonably close to some equilibrium state .The proof uses on a combination of power estimates with suitable weighted Sobolev inequalities . We additionally prove how our findings can be applied to more general systems of conservation laws which are not necessarily strictly hyperbolic .In particular we define the case where one of the characteristic velocity vanishes at a point but still bounded away from zero elsewhere . Introduction The purpose of this study is twofold .First , we obtain global regularity properties for the rapidly spinning shallow water structure and its generalization to arbitrary dimension . Second , we provide fresh insights into the formation of these models by showing how they fit within a greater category of nonlinear dispersive partial differential equations .Our main consequence reads as follows : Main Theorem 1 Let u 0 ∈ H s , s > n / 2 + 2 , then there exist constants C = C ( n ) , K = K ( n ) such that if",
        "rewrite_text": "**Title:** Long Time Existence of Smooth Solutions for the Rapidly Spinning Shallow-Water and Euler Equations\n\n**Abstract:** In this article, we establish the existence of an open set of initial conditions for which the solutions to the rapidly spinning shallow water (RSW) equations and the Euler equations remain globally smooth in both space and time, provided that these solutions remain sufficiently close to a certain equilibrium state. Our approach combines power estimates with appropriately weighted Sobolev inequalities to achieve this result. Furthermore, we extend our findings to encompass a broader class of conservation laws that may not strictly adhere to hyperbolic conditions. Specifically, we explore scenarios where one of the characteristic velocities may vanish at a specific point while remaining bounded away from zero in other regions. \n\nThe primary aim of this research is twofold. Firstly, we derive global regularity results for the rapidly spinning shallow water equations and their generalizations to higher dimensions. Secondly, we offer new perspectives on the underlying dynamics of these models by situating them within a larger framework of nonlinear dispersive partial differential equations. Our main theorem articulates that for initial data \\( u_0 \\) belonging to the Sobolev space \\( H^s \\) with \\( s > n/2 + 2 \\), there exist constants \\( C = C(n) \\) and \\( K = K(n) \\) such that certain conditions are satisfied, ensuring the smoothness of the solutions over an extended time frame. This work not only contributes to the theoretical understanding of fluid dynamics described by the RSW and Euler equations but also opens avenues for further research into related systems of equations that exhibit similar properties.",
        "ori-fast-z-score": -1.3862065601673441,
        "water-fast-z-score": 3.9605901719066976,
        "rewrite-fast-z-score": -0.7364596943186588
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Projectile Fragmentation of $^{86}$Kr at 64 MeV/nucleon .\nAbstract:\nThe projectile fragmentation of 86Kr at 64MeV/nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a beam intensity of 1nAe. The main results are as follows:  - A total number of about 10000 events have been recorded for this experiment.  - The charge distribution is peaked around Z=40, but shows also a large contribution between 30 and 40 charges units (see fig.1 ). This indicates that the fragments produced by the break-up of 86Kr are not only light particles like neutrons or protons, but contain many intermediate mass fragments too.   - The angular distributions show two peaks corresponding to forward and backward emission respectively (see fig.2 ).  - The energy spectra present a maximum around 10-12 MeV/u which corresponds to the most probable kinetic energy per nucleon of the emitted fragments (see fig.3 ).\n- The isotopic composition of the fragments is shown on figure 4 . It can be seen that there is no significant difference between the fragment production in the forward hemisphere and in the backward one.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Projectile Fragmentation of $ ^ { 86 } $ Kr at 64 MeV / nucleon . Abstract : The projectile fragmentation of 86Kr at 64MeV / nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a laser intensity of 1nAe .The main results are as follows : - A total number of about 10000 events have been observed for this study . - The charge distribution is peaked around Z = 40 , but shows also a large contribution between 30 and 40 charges units ( saw fig . 1 ) .This implies that the fragments produced by the broken - up of 86Kr are not only light particles like neutrons or protons , but contain many intermediate mass pieces too . - The angular distributions show two peaks related to forward and back emission respectively ( view fig . 2 ) .- The energy spectra produce a maximum around 10 - 12 MeV / u which equals to the most likely kinetic power per nucleon of the emitted objects ( see fig . 3 ) . - The isotopic structure of the fragments is displayed on figure 4 .It can be shown that there is no major changes between the fragment production in the front hemisphere and in the backward one .",
        "rewrite_text": "Title: Projectile Fragmentation of $^{86}$Kr at 64 MeV/nucleon\n\nAbstract: This study investigates the projectile fragmentation of $^{86}$Kr at an energy of 64 MeV per nucleon, utilizing the INDRA multidetector in an inverse kinematics setup. The experiment was conducted with an 8 cm thick natural potassium (natK) target and a laser intensity of 1 nA. A total of approximately 10,000 events were recorded during the analysis. The findings reveal that the charge distribution of the fragments is predominantly centered around Z = 40, while also exhibiting a significant contribution from fragments with charge numbers ranging between 30 and 40. This observation indicates that the fragmentation process of $^{86}$Kr yields not only light particles, such as neutrons and protons, but also a considerable number of intermediate mass fragments. \n\nFurthermore, the angular distribution of the emitted fragments displays two distinct peaks, corresponding to forward and backward emissions, respectively. The energy spectra of the fragments reveal a peak around 10-12 MeV/u, which corresponds to the most probable kinetic energy per nucleon of the emitted fragments. Additionally, the isotopic composition of the fragments is illustrated in the accompanying figures, demonstrating that there are no significant differences in fragment production between the forward and backward hemispheres. Overall, this research provides valuable insights into the fragmentation behavior of $^{86}$Kr, highlighting the complexity of the resulting fragment distributions and their implications for understanding nuclear reactions at high energies.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": 0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electrostatic Interactions of Asymmetrically Charged Membranes .\nAbstract:\nThe electrostatic interactions between two asymmetrically charged membranes are studied by using the mean-field theory and Monte Carlo simulations. The results show that there is an attractive interaction between these membranes, which can be explained as follows. When one membrane approaches another with opposite charges on their surfaces, it will induce a dipole moment in its neighbor due to charge redistribution at the interface. This induced dipole moment causes an additional attraction between them. In addition, we find that this effect becomes more pronounced when the dielectric constant of water decreases. Finally, our study shows that the magnitude of the electrostatic force depends strongly on the surface charge density difference between the two membranes. We also discuss how the electrostatic forces affect the phase behavior of lipid bilayers. DOI: 10.1063/1.3189000\nI. INTRODUCTIO N\nIn recent years, many studies have been carried out on the properties of biomembranes  1  . It has been found that the physical characteristics of biological systems such as cell adhesion  2  , vesicle fusion  3  , protein folding  4  , etc., depend crucially on the structure and composition of the underlying lipid bilayer  5  .\nBiological membranes consist mainly of phospholipids  6  . These lipids contain hydrophobic tails and hydrophilic heads  7, 8  . Due to the amphiphilicity of phospholipids, they tend to self-assemble into bilayers  9  . A typical example for such a system is shown schematically in Fig.  1(a) . Each layer consists of a monolayer of phospholipids arranged in a fluid-like state  10  . The thickness of each layer is about 5 nm  11  . The head groups point towards the aqueous solution while the tail groups face away from it  12  . Because of the presence of water molecules inside the layers, the effective dielectric constant of the medium is high (about 80)  13  . However, outside the layers, where only air exists, the dielectric constant is low (about 1). Therefore, the electric field lines penetrate easily through the interior region but not so much through the exterior region  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electrostatic Interactions of Asymmetrically Charged Membranes . Abstract : The electrostatic interactions between two asymmetrically charged membranes are studied by using the mean - field principle and Monte Carlo simulations .The results show that there is an interesting interaction between these membranes , which can be understood as follows . When one cell encounters another with opposite charges on their edges , it will generate a dipole point in its neighbor due to charge redistribution at the interface .This induced dipole point causes an additional attraction between them . In addition , we find that this effect gets more pronounced when the dielectric constant of water reduces .Finally , our research shows that the severity of the electrostatic pressure depends strongly on the surface charge density difference between the two membranes . We additionally discuss how the electrostatic fields affect the phase response of lipid bilayers .DOI : 10 . 1063 / 1 . 3189000 I . INTRODUCTIO N In recent years , various studies have been carried out on the properties of biomembranes 1 . It has been shown that the structural traits of biological systems such as cell adhesion 2 , vesicle fusion 3 , protein folding 4 , etc . , depend crucially on the composition and configuration of the underlying lipid bilayer 5 .Biological membranes consist mostly of phospholipids 6 . These lipids contain hydrophobic tails and hydrophilic bodies 7 , 8 .Due to the amphiphilicity of phospholipids , they tend to self - organize into bilayers 9 . A typical example for such a system is demonstrated schematically in Fig .1 ( a ) . Each layer contains of a monolayer of phospholipids grouped in a fluid - like state 10 .The depth of each surface is about 5 nm 11 . The head bands look towards the aqueous solution while the tail groups face away from it 12 .Because of the presence of moisture atoms inside the layers , the effective dielectric constant of the medium is high ( about 80 ) 13 . However , outside the layers , where only air occurs , the dielectric constant is low ( about 1 ) .Therefore , the electric field lines penetrate easily through the interior region but not so much through the exterior zone 14 .",
        "rewrite_text": "**Title:** Electrostatic Interactions of Asymmetrically Charged Membranes\n\n**Abstract:** This study investigates the electrostatic interactions between two asymmetrically charged membranes utilizing mean-field theory and Monte Carlo simulations. Our findings reveal a fascinating interaction mechanism between these membranes, characterized by the generation of an induced dipole moment when one membrane with a charge encounters another with an opposite charge at their edges. This charge redistribution at the interface leads to an additional attractive force between the membranes. Notably, the strength of this interaction intensifies as the dielectric constant of water decreases, highlighting the significance of the surrounding medium on electrostatic behavior. Furthermore, our research indicates that the magnitude of electrostatic pressure is highly dependent on the difference in surface charge density between the two membranes. We also explore the implications of these electrostatic fields on the phase response of lipid bilayers, providing insights into the fundamental processes governing biomembrane behavior. This work contributes to a deeper understanding of the role of electrostatic interactions in biological systems, which are crucial for various cellular processes such as adhesion, vesicle fusion, and protein folding. The study underscores the importance of membrane composition and configuration in influencing these interactions, thereby advancing our knowledge of biomembrane dynamics. \n\n**DOI:** 10.1063/1.3189000\n\n**Introduction:** Recent research has focused on the properties of biomembranes, revealing that the structural characteristics of biological systems are significantly influenced by the composition and arrangement of lipid bilayers. Composed primarily of phospholipids, these membranes exhibit amphiphilic properties, leading to self-organization into bilayers. Each bilayer consists of a monolayer of phospholipids in a fluid-like state, with hydrophilic heads oriented towards the aqueous environment and hydrophobic tails facing away. The presence of water molecules within the layers results in a high effective dielectric constant (approximately 80), while the surrounding air has a much lower dielectric constant (around 1). This disparity allows electric field lines to penetrate the interior of the membranes more readily than the exterior, affecting the electrostatic interactions at play.",
        "ori-fast-z-score": -0.6115928396627265,
        "water-fast-z-score": 6.0470789790695205,
        "rewrite-fast-z-score": 0.5107539184552492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Production of a sterile species: quantum kinetics .\nAbstract:\nWe study the production of a sterile species in an open system with two stable particles and one unstable particle, where the decay products are not observed.  We show that if the initial state is pure then there exists no final mixed state which can be reached by unitary evolution. This result implies that the production of a sterility cannot occur under any circumstances for such systems. If we allow the possibility to prepare arbitrary states as input, however, it turns out that the production of a certain kind of sterility may still take place. In this case, the output state will always contain some amount of entanglement between the subsystems corresponding to the different types of particles involved. The results presented here have been obtained within the framework of Quantum Kinetic Theory (QKT). QKT provides a description of non-equilibrium phenomena at mesoscopic scales based on the concept of entropy production rate. It has recently attracted considerable attention due to its potential applications in many areas ranging from physics to biology. \nI. INTRODUCTORY REMARK\nThe phenomenon of spontaneous emission plays a crucial role in modern physics. For example, it is responsible for the cooling process in laser-cooling experiments  1  . On the other hand, spontaneous emission also leads to decoherence effects  2  , which limit the performance of quantum information processing devices  3  .\nIn recent years, several authors  4  -  8  studied the problem of producing a particular type of  sterility  in open quantum systems. A state is called  sterile  when it does not interact with itself or another given set of states  9  . More specifically, let us consider a bipartite Hilbert space H = H 1 ⊗H 2 , where dim(H i ) = N i . Then, a density matrix ρ ∈ B(H) is said to be  sterile  wrt. a subset S ⊆ H iff Tr ρσ  = 0 for all σ ∈ S. Here, Tr denotes the trace operation over either H 1 or H 2 depending on whether σ belongs to H 1 or H 2 respectively. Note that the notion of  ster",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Production of a sterile species : quantum kinetics . Abstract : We research the production of a sterile species in an open system with two stable particles and one unstable particle , where the decay products are not observed .We see that if the first system is pure then there exists no final mixed system which can be reached by unitary evolution . This result means that the production of a sterility cannot occur under any situations for such systems .If we allow the option to develop arbitrary states as input , however , it turns out that the production of a certain sort of sterility may always hold place . In this instance , the output state will always contain some degree of entanglement between the subsystems corresponding to the different kinds of molecules concerned .The results presented here have been achieved within the framework of Quantum Kinetic Theory ( QKT ) . QKT provides a description of non - equilibrium phenomena at mesoscopic scales based on the idea of entropy production rate .It has recently attracted considerable scrutiny due to its potential applications in multiple fields ranging from biology to biology . I .INTRODUCTORY REMARK The phenomenon of spontaneous emission takes a crucial role in modern physics . For instance , it is responsible for the freezing process in laser - cooling experiments 1 .On the other hand , spontaneous emission additionally results to decoherence effects 2 , which reduce the performance of quantum information processing equipment 3 . In recent years , various scientists 4 - 8 studied the question of creating a certain type of sterility in open quantum systems .A state is dubbed sterile when it does not interact with itself or another particular set of states 9 . More specifically , let us consider a bipartite Hilbert field H = H 1 [UNK] 2 , where dim ( H i ) = N i .Then , a density matrix ρ ∈ B ( H ) is said to be sterile wrt . a subset S ⊆ H iff Tr ρσ = 0 for all σ ∈ S . Here , Tr denotes the trace operation over either H 1 or H 2 depending on whether ρ corresponds to H 1 or H 2 respectively .Note that the notion of  ster",
        "rewrite_text": "**Title:** Production of a Sterile Species: Quantum Kinetics\n\n**Abstract:** This study investigates the generation of a sterile species within an open system characterized by two stable particles and one unstable particle, where the decay products remain unobserved. Our findings indicate that when the initial system is in a pure state, it is impossible to achieve a final mixed state through unitary evolution. This implies that the emergence of sterility is unattainable in such configurations. However, when we permit the introduction of arbitrary states as inputs, we discover that the production of a specific type of sterility can consistently occur. In this scenario, the resulting output state invariably exhibits a degree of entanglement between the subsystems associated with the various particle types involved. The results presented in this paper are derived from the principles of Quantum Kinetic Theory (QKT), which offers a framework for understanding non-equilibrium phenomena at mesoscopic scales, emphasizing the significance of the entropy production rate. QKT has garnered increasing attention due to its wide-ranging applications across diverse fields, including biology and quantum information science. \n\nThe phenomenon of spontaneous emission plays a pivotal role in contemporary physics, influencing processes such as laser cooling, where it contributes to the freezing mechanisms observed in experiments. Conversely, spontaneous emission also leads to decoherence effects that can hinder the efficiency of quantum information processing systems. Recent investigations by various researchers have focused on the challenge of generating a particular form of sterility in open quantum systems. A state is classified as sterile if it does not interact with itself or with a designated set of states. Specifically, we consider a bipartite Hilbert space \\( H = H_1 \\otimes H_2 \\), where the dimensions of \\( H_i \\) are denoted as \\( N_i \\). A density matrix \\( \\rho \\in B(H) \\) is termed sterile with respect to a subset \\( S \\subseteq H \\) if \\( \\text{Tr}(\\rho \\sigma) = 0 \\) for all \\( \\sigma \\in S \\), where the trace operation is applied over either \\( H_1 \\) or \\( H_2 \\) based on the corresponding density matrix. This exploration of sterility within quantum systems opens new avenues for understanding quantum interactions and their implications in various scientific domains.",
        "ori-fast-z-score": 0.8778955729143844,
        "water-fast-z-score": 7.6373499280870085,
        "rewrite-fast-z-score": 0.79555728417573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Three-Year Wilkinson Microwave Anisotropy Probe (WMAP) Observations: Foreground Polarization .\nAbstract:\nWe present the first detection and characterization of polarized foreground emission at microwave frequencies using three years of data from WMAP. We find that this emission is dominated by synchrotron radiation, with an amplitude consistent with previous measurements in the literature. The polarization fraction for this component ranges between 0.5% to 2% across the sky. In addition we detect significant levels of polarized dust emission over much of the sky. This emission has a lower fractional polarization than previously reported but its total intensity is comparable or higher. Finally, we report on the detection of polarized thermal Sunyaev-Zeldovich effect associated with galaxy clusters. These results are important as they provide new information about Galactic foregrounds which will be used to extract cosmological signals such as primordial gravitational waves. \n \n Keywords: Cosmic microwave background anisotropies, Galaxy cluster, Synchrotron Radiation, Dust Emission, Thermal Sunyaev-Zeldovitch Effect",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Three - Year Wilkinson Microwave Anisotropy Probe ( WMAP ) Observations : Foreground Polarization . Abstract : We report the first recognition and identification of polarized foreground emission at microwave frequencies using three years of evidence from WMAP .We see that this emission is dominated by synchrotron emission , with an frequency consistent with previous measurements in the literature . The polarization fraction for this component varies between 0 . 5 % to 2 % across the sky .In addition we perceive considerable rates of polarized dust radiation over much of the sky . This absorption has a smaller fractional polarization than previously reported but its total activity is equal or greater .Finally , we publish on the observation of polarized thermal Sunyaev - Zeldovich effect related with star clusters . These data are important as they give novel knowledge about Galactic foregrounds which will be used to extract cosmological messages such as primordial gravitational waves .Keywords : Cosmic microwave background anisotropies , Galaxy cluster , Synchrotron Radiation , Dust Emission , Thermal Sunyaev - Zeldovitch Effect",
        "rewrite_text": "Title: Three-Year Wilkinson Microwave Anisotropy Probe (WMAP) Observations: Foreground Polarization\n\nAbstract: In this study, we present the groundbreaking identification of polarized foreground emissions at microwave frequencies, utilizing three years of observational data from the Wilkinson Microwave Anisotropy Probe (WMAP). Our findings indicate that the predominant source of this emission is synchrotron radiation, with frequency characteristics that align with previously documented measurements in the scientific literature. The polarization fraction associated with synchrotron emission exhibits variability across the sky, ranging from 0.5% to 2%. Furthermore, we observe significant levels of polarized dust emission across extensive regions of the sky. Although the fractional polarization of this dust emission is lower than earlier reports suggested, its overall intensity is comparable to or exceeds those prior estimates. Additionally, we provide insights into the detection of polarized thermal Sunyaev-Zeldovich effects linked to galaxy clusters. These observations are crucial as they enhance our understanding of Galactic foregrounds, which are essential for extracting cosmological information, including insights into primordial gravitational waves. The implications of our findings extend to the broader field of cosmology, as they contribute to the refinement of models that interpret cosmic microwave background anisotropies. Our research underscores the importance of foreground polarization in the context of cosmic observations and sets the stage for future studies aimed at unraveling the complexities of the universe's early moments. \n\nKeywords: Cosmic microwave background anisotropies, Galaxy cluster, Synchrotron Radiation, Dust Emission, Thermal Sunyaev-Zeldovich Effect.",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -0.10153461651336192
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The anticenter old open cluster NGC 1883: radial velocity and metallicity .\nAbstract:\nWe present new spectroscopic observations for the open cluster NGC 1883, which is located at a distance of about 1 kpc in the constellation Cassiopeia (α = 20 h 18 m , δ = +58°). The data were obtained with the 2-m telescope of the Observatorio Astronómico Nacional de San Pedro Mártir (OAN-SPM) on December 16-17, 2009 using the REOSC spectrograph equipped with grism #7 covering the wavelength range 3700-7000 Å. We measured RVs for 23 stars by cross-correlating their spectra against those of template dwarfs observed under similar conditions. Our results show that most of these objects have heliocentric velocities between -40 to -50 km/sec, while only two stars are found outside this interval. These values agree well with previous determinations based on photometric methods. \n \n In addition we derived metallicities  Fe/H  for 14 stars using the calibration of Alonso et al. (1999) . For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from -0.10 dex up to +0.20 dex. Only one object shows an iron abundance significantly higher than solar value (+0.30 dex). \n \n Finally, we compared our results with previously published studies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The anticenter old open cluster NGC 1883 : radial speed and metallicity . Abstract : We report new spectroscopic observations for the open cluster NGC 1883 , which is situated at a distance of about 1 kpc in the constellation Cassiopeia ( α = 20 h 18 m , δ = + 58° ) .The data were obtained with the 2 - m observatory of the Observatorio Astronómico Nacional de San Pedro Mártir ( OAN - SPM ) on December 16 - 17 , 2009 using the REOSC spectrograph equipped with grism # 7 representing the frequency region 3700 - 7000 Å . We calculated RVs for 23 stars by cross - correlating their spectra against those of template dwarfs observed under corresponding conditions .Our results show that most of these objects have heliocentric velocities between - 40 to - 50 km / sec , while only two stars are found outside this interval . These values comply good with previous determinations based on photometric technique .In addition we derived metallicities Fe / H for 14 stars following the calibration of Alonso et al . ( 1999 ) .For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from - 0 . 10 dex up to + 0 . 20 dex . Only one object displays an metal abundance considerably higher than solar value ( + 0 . 30 dex ) .Finally , we compared our findings with previously written findings .",
        "rewrite_text": "We present new spectroscopic observations of the open cluster NGC 1883, located approximately 1 kpc away in the constellation Cassiopeia (α = 20 h 18 m, δ = +58°). The data were collected using the 2-meter telescope at the Observatorio Astronómico Nacional de San Pedro Mártir (OAN-SPM) on December 16-17, 2009, utilizing the REOSC spectrograph equipped with grism #7, which covers the wavelength range of 3700 to 7000 Å. We calculated radial velocities (RVs) for 23 stars by cross-correlating their spectra with those of template dwarf stars observed under similar conditions. Our analysis reveals that the majority of these stars exhibit heliocentric velocities ranging from -40 to -50 km/s, with only two stars falling outside this velocity range. These findings are consistent with earlier measurements obtained through photometric techniques. Furthermore, we determined the metallicities (Fe/H) for 14 stars using the calibration method established by Alonso et al. (1999). The results indicate that, with the exception of one star, the metallicities are either solar or slightly subsolar, varying from -0.10 dex to +0.20 dex. Notably, one star shows a significantly higher metallicity, measured at +0.30 dex. In conclusion, we compare our results with previous studies, contributing to the understanding of the metallicity and kinematics of NGC 1883 and enhancing the broader knowledge of old open clusters in our galaxy.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 4.444462481925879,
        "rewrite-fast-z-score": 0.7777777777777778
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observational consequences of the hypothesized helium rich stellar population in Omega Centauri .\nAbstract:\nWe present new photometric and spectroscopic observations for two stars, HD 122563 (=HR 5171A) and BD+17°3248, which are suspected to be members of the proposed intermediate age population of helium-rich giants in the globular cluster Omega Cen.  We find that both stars have very similar atmospheric parameters as those found by previous studies for other candidate helium-rich giant candidates in Omega Cen: T eff = 8200 K; log g = 3.8;  Fe/H  = -1.0 dex. The observed spectra show no evidence for He II lines at 4686 Å or 5412 Å, but do exhibit strong Balmer line emission with equivalent widths ranging between -40 and -50 mÅ. These results suggest that these stars may not actually belong to this proposed class of objects. However, we cannot rule out the possibility that they are indeed helium-rich giants on the basis of our current data set alone.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observational consequences of the hypothesized helium rich stellar community in Omega Centauri . Abstract : We report new photometric and spectroscopic observations for two stars , HD 122563 ( = HR 5171A ) and BD + 17°3248 , which are suspected to be members of the suggested intermediate age population of helium - rich giants in the globular cluster Omega Cen .We see that both stars have very identical atmospheric parameters as those shown by earlier surveys for other candidate helium - rich giant candidates in Omega Cen : T eff = 8200 K ; log f = 3 . 8 ; Fe / H = - 1 . 0 dex . The observed spectra show no evidence for He II lines at 4686 Å or 5412 Å , but do exhibit strong Balmer line emission with corresponding widths ranging between - 40 and - 50 mÅ .These data suggest that these stars must not actually belong to this possible category of bodies . However , we cannot judge out the idea that they are indeed helium - rich giants on the basis of our previous data set alone .",
        "rewrite_text": "In this study, we present new photometric and spectroscopic observations of two stars, HD 122563 (HR 5171A) and BD +17°3248, which are believed to be part of the hypothesized intermediate-age population of helium-rich giants within the globular cluster Omega Centauri. Our findings reveal that both stars exhibit atmospheric parameters that closely align with those identified in previous surveys of other potential helium-rich giant candidates in Omega Centauri, specifically with effective temperatures (T_eff) around 8200 K, surface gravities (log g) of approximately 3.8, and metallicity (Fe/H) values of -1.0 dex. Notably, the spectra obtained from these observations do not show any evidence of He II lines at wavelengths of 4686 Å or 5412 Å. Instead, we observe pronounced Balmer line emissions, with widths ranging from -40 to -50 mÅ. These spectral characteristics suggest that the stars in question may not fit into the proposed category of helium-rich giants. However, it is important to note that our current dataset does not definitively exclude the possibility that these stars could still be helium-rich giants. Further investigations and additional data are necessary to draw more conclusive insights regarding their classification and the broader implications for the stellar population in Omega Centauri. This research contributes to the ongoing discourse surrounding the nature of stellar populations in globular clusters and the specific characteristics that define helium-rich stars.",
        "ori-fast-z-score": -1.9123657749350298,
        "water-fast-z-score": 2.7295978138458623,
        "rewrite-fast-z-score": 0.30779350562554625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Solutions to Master Equations of Quantum Brownian Motion in a General Environment with External Force .\nAbstract:\nWe present solutions for the master equations describing quantum Brownian motion (QBM) in an arbitrary environment, including external forces and non-Markovian effects. The general solution is obtained by solving the corresponding Fokker-Planck equation using path integral techniques. We show that this approach leads to exact results which are valid even when the system-environment coupling strength becomes large compared to the temperature. In particular we consider two examples where our formalism can be applied straightforwardly. First, we study QBM in a harmonic oscillator potential under the influence of white noise. Second, we investigate the effect of a time-dependent force on QBM. Finally, we discuss how our method could also be used to treat more complicated situations such as systems coupled to multiple environments or driven by colored noise. DOI: 10.1063/1.3189571\nQuantum Brownian motion describes the dynamics of particles interacting with their surrounding environment  1  . It has been studied extensively over many years both theoretically  2  , experimentally  3  , and numerically  4  .\nIn recent years there have been several attempts to solve the master equation governing QBM exactly  5, 6, 7, 8  . However these approaches either require approximations  7, 9  or do not allow one to include external forces  6, 5  . Here we present a new technique based on Feynman-Kac path integrals  10  which allows us to obtain exact solutions for the master equation without any approximation  11  . This includes cases where the system-environment interaction is strong compared to the thermal energy k B T  12  . Our formalism applies equally well to Markovian  13  and nonMarkovian  14  processes and can easily incorporate external driving fields  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Solutions to Master Equations of Quantum Brownian Motion in a General Environment with External Force . Abstract : We present solutions for the master equations explaining quantum Brownian movement ( QBM ) in an arbitrary environment , including external forces and non - Markovian effects .The general solution is found by solving the equivalent Fokker - Planck formula using path integral methods . We see that this methodology leads to exact findings which are applicable even when the process - environment coupling strength gets large compared to the temperature .In particular we appreciate two examples where our formalism can be applied straightforwardly . First , we study QBM in a harmonic oscillator potential under the effects of white sound .Second , we investigate the impact of a time - dependent force on QBM . Finally , we investigate how our technique could also be used to treat more complicated circumstances such as systems combined to multiple conditions or driven by colored interference .DOI : 10 . 1063 / 1 . 3189571 Quantum Brownian movement describes the dynamics of molecules interacting with their nearby surroundings 1 . It has been studied frequently over numerous years both theoretically 2 , experimentally 3 , and numerically 4 .In recent years there have been numerous attempts to solve the master equation governing QBM exactly 5 , 6 , 7 , 8 . However these solutions either need approximations 7 , 9 or do not enable one to use external forces 6 , 5 .Here we present a new technique based on Feynman - Kac path integrals 10 which allows us to obtain exact solutions for the master equation without any approximation 11 . This contains cases where the system - atmosphere interaction is strong compared to the thermal energy k B T 12 .Our formalism applies equally well to Markovian 13 and nonMarkovian 14 mechanisms and can easily involve external driving fields 15 .",
        "rewrite_text": "Title: Solutions to Master Equations of Quantum Brownian Motion in a General Environment with External Force\n\nAbstract: In this article, we provide comprehensive solutions to the master equations that govern quantum Brownian motion (QBM) within a general environment, taking into account the influence of external forces and non-Markovian effects. Our approach involves utilizing path integral methods to solve the corresponding Fokker-Planck equation, which enables us to derive exact solutions applicable even when the coupling strength between the system and its environment is significantly larger than the thermal energy. We illustrate the effectiveness of our methodology through two specific examples. The first example examines QBM in a harmonic oscillator potential subjected to the effects of white noise, while the second explores the consequences of a time-dependent external force on the dynamics of QBM. Furthermore, we discuss the potential of our technique to address more complex scenarios, such as systems influenced by multiple conditions or those driven by colored noise. Quantum Brownian motion, which describes the behavior of particles interacting with their surrounding environment, has been a subject of extensive theoretical, experimental, and numerical research over the years. Despite numerous efforts to find exact solutions to the master equation governing QBM, previous approaches often relied on approximations or failed to incorporate external forces effectively. In contrast, our novel technique, grounded in Feynman-Kac path integrals, allows for the derivation of exact solutions without the need for approximations, even in cases where the interaction between the system and the environment is strong relative to the thermal energy. Our formalism is versatile, accommodating both Markovian and non-Markovian dynamics, and can seamlessly integrate external driving fields. This work represents a significant advancement in the understanding of quantum Brownian motion and opens avenues for further exploration of complex quantum systems. DOI: 10.1063/1.3189571",
        "ori-fast-z-score": 0.08606629658238704,
        "water-fast-z-score": 6.688444820557844,
        "rewrite-fast-z-score": 0.16666666666666666
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exotic Mott phases of the extended t--J model on the checkerboard lattice at commensurate densities .\nAbstract:\nWe study the ground state properties and excitations in the extended t-J model with nearest-neighbor hopping (NNH) and next-nearest neighbor hopping (NNN), which is defined by the following Hamiltonian: \nwhere c,c are fermion annihilation operators for electrons with spin , respectively; n,n denote electron number operators for sites i,j, respectively. \n \n \n \n We consider this model on the checkerboard square lattice at half-filling where each site has one electron. The NNH term breaks time-reversal symmetry while the NNN term preserves it. In order to investigate the effect of these terms we use exact diagonalization method combined with density matrix renormalization group technique. We find that there exist three different types of magnetic orders depending on the ratio between NNH and NNN hoppings. These results suggest that the extended t-J model can be used as an effective model for describing high-Tc cuprates.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exotic Mott phases of the extended t - - J model on the checkerboard lattice at commensurate densities . Abstract : We explore the ground state properties and excitations in the extended t - J model with nearest - neighbor hopping ( NNH ) and last - nearest neighbor hopping ( NNN ) , which is characterized by the following Hamiltonian : where c , c are fermion annihilation operators for electrons with spin , respectively ; n , k define atom number operators for sites i , j , respectively .We consider this model on the checkerboard square lattice at half - filling where each site has one electron . The NNH term breaks time - reversal symmetry while the NNN term preserves it .In order to examine the impact of these terms we using accurate diagonalization technique combined with density matrix renormalization group technique . We see that there exist three different kinds of magnetic orders depending on the proportion between NNH and NNN hoppings .These data suggest that the extended t - J approach can be used as an efficient model for describing high - Tc cuprates .",
        "rewrite_text": "In this study, we investigate the ground state characteristics and excitation spectra of the extended t - J model on a checkerboard lattice, focusing on the effects of nearest-neighbor hopping (NNH) and next-nearest-neighbor hopping (NNN). The Hamiltonian governing this model incorporates fermion annihilation operators for electrons with spin, alongside atom number operators for specific lattice sites. Our analysis is conducted at half-filling, where each lattice site is occupied by a single electron. Notably, the NNH term disrupts time-reversal symmetry, while the NNN term maintains it. To thoroughly assess the influence of these hopping terms, we employ a combination of precise diagonalization techniques and the density matrix renormalization group (DMRG) method. Our findings reveal the emergence of three distinct types of magnetic orders, which are contingent upon the relative strengths of the NNH and NNN hopping parameters. These results indicate that the extended t - J model serves as a robust framework for understanding the complex behaviors observed in high-temperature superconductors, particularly in the context of cuprate materials. The interplay between different hopping mechanisms and their resultant magnetic phases provides valuable insights into the underlying physics of these exotic Mott phases, highlighting the model's potential applicability in future research on correlated electron systems. Overall, this work contributes to the broader understanding of quantum magnetism and the role of lattice geometry in the emergence of novel electronic phases.",
        "ori-fast-z-score": -0.8944271909999159,
        "water-fast-z-score": 3.3541019662496843,
        "rewrite-fast-z-score": 1.889822365046136
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Phenomenology of GUT-less Supersymmetry Breaking .\nAbstract:\nWe study the phenomenological consequences of supersymmetric models with gauge-mediated breaking, in which the Standard Model is extended by adding new vector-like matter fields and extra dimensions. We show that these models can be constructed such that they are free of any unnatural fine-tuning problems associated with the Higgs mass or flavor-changing neutral currents. In particular we find that:  1) The lightest scalar superpartner (the  Higgs  boson) has a mass at most around 300 GeV.  2) Flavor changing neutral current effects are suppressed to an acceptable level for generic values of parameters.  3) Gauge coupling unification occurs naturally within experimental uncertainties. 4) There exists a large parameter space where all sparticles have masses above 1 TeV while still satisfying constraints on electroweak symmetry breaking. 5) These models provide a natural explanation for why there may not yet exist evidence for supersymmetry at accelerator experiments.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Phenomenology of GUT - less Supersymmetry Breaking . Abstract : We research the phenomenological consequences of supersymmetric theories with gauge - mediated breaking , in which the Standard Model is extended by added new vector - like matter fields and extra dimensions .We see that these models can be built such that they are free of any strange fine - tuned flaws associated with the Higgs mass or flavor - changing neutral currents . In particular we find that : 1 ) The lightest scalar superpartner ( the Higgs boson ) has a mass at most approximately 300 GeV .2 ) Flavor shifting neutral current effects are suppressed to an acceptable level for generic values of values . 3 ) Gauge coupling unification happens easily within experimental uncertainties .4 ) There exists a large parameter room where all sparticles have masses above 1 TeV while already satisfying constraints on electroweak symmetry breaking . 5 ) These models serve a natural explanation for why there may not already exist evidence for supersymmetry at accelerator studies .",
        "rewrite_text": "Title: Phenomenology of GUT-less Supersymmetry Breaking\n\nAbstract: This study investigates the phenomenological implications of supersymmetric theories characterized by gauge-mediated symmetry breaking, wherein the Standard Model is augmented with additional vector-like matter fields and extra dimensions. Our analysis reveals that these models can be constructed to avoid the problematic fine-tuning issues typically associated with the Higgs mass and flavor-changing neutral currents. Notably, we present several key findings: First, the mass of the lightest scalar superpartner, identified as the Higgs boson, is constrained to be no more than approximately 300 GeV. Second, the effects of flavor-changing neutral currents are significantly suppressed, remaining within acceptable limits for a wide range of parameter values. Third, we demonstrate that gauge coupling unification can be achieved effortlessly, remaining consistent with current experimental uncertainties. Furthermore, we identify a substantial parameter space where all superparticles possess masses exceeding 1 TeV, while still adhering to the necessary conditions for electroweak symmetry breaking. Lastly, these models provide a compelling rationale for the absence of direct evidence for supersymmetry in current accelerator experiments. Our findings contribute to the understanding of supersymmetry breaking mechanisms and their implications for particle physics, suggesting that the phenomenology of GUT-less supersymmetry may offer viable pathways for future research and exploration in high-energy physics.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 5.658032638058332,
        "rewrite-fast-z-score": 0.29277002188455997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Predicting the connectivity of primate cortical networks from topological and spatial node properties .\nAbstract:\nWe present an approach to predict the structural network organization in the cerebral cortex based on local node features, such as their position within the brain s surface or volume, and global topological characteristics. We use this method to study how different types of nodes are connected with each other across species (human, macaque monkey) and modalities (diffusion MRI tractography). Our results show that our model can accurately reproduce known patterns of cortico-cortical connections between areas, including those observed in humans but not yet described for monkeys. The proposed framework is general enough to be applied to any type of data where information about individual nodes  positions and pairwise interactions exists. This includes both anatomical and functional imaging datasets, which will allow us to investigate the relationship between structure and function at multiple scales. \n \n Introduction \n \n Brain connectomics aims to map all neuronal elements into a single comprehensive description of the human brain  1  . In recent years, advances in neuroimaging techniques have allowed researchers to obtain detailed maps of the brain s structural  2  , metabolic  3  , and functional  4  architecture. These new technologies provide unprecedented opportunities to understand how the brain works by studying its large-scale organization  5  .\n \nHowever, despite these advancements, there remains significant uncertainty regarding the precise nature of the relationships among neurons  6  . For example, it has been shown that some regions of the brain communicate more frequently than others  7-9 , while others exhibit higher levels of synchrony  10  . However, we still do not know whether these differences reflect specific wiring rules  11  or simply arise due to random fluctuations  12  . \n \n Here, we propose a novel computational framework to address this problem using machine learning methods  13  . Specifically, we aim to develop models capable of predicting the strength of connection between pairs of nodes given only information about their location and topology  14  . To achieve this goal, we first construct a set of training examples consisting of pairs of nodes whose interaction strengths are known  15  . Then, we train a classifier to learn the mapping between node features and edge weights  16  . Finally, we apply the trained model to unseen test cases  17  to infer unknown interactions",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Predicting the connectivity of primate cortical networks from topological and spatial node characteristics . Abstract : We present an approach to predict the structural network structure in the brain cortex based on local node characteristics , such as their placement within the brain s surface or volume , and global topological traits .We use this technology to study how various types of nodes are connected with each other across taxa ( human , macaque animal ) and modalities ( diffusion MRI tractography ) . Our results show that our model can accurately demonstrate established trends of cortico - cortical networks between zones , notably those observed in humans but not already explained for monkeys .The proposed framework is general enough to be applied to any type of evidence where information about individual nodes positions and pairwise relationships exists . This encompasses both anatomical and functional imaging datasets , which will let us to examine the relationship between form and function at multiple scales .Introduction Brain connectomics aims to map all neuronal components into a single comprehensive account of the human mind 1 . In recent months , advances in neuroimaging techniques have permitted investigators to obtain detailed maps of the brain s structural 2 , metabolic 3 , and physiological 4 architecture .These new inventions offer unprecedented possibilities to realize how the brain acts by examining its large - scale organization 5 . However , despite these advancements , there exists significant confusion regarding the exact nature of the relationships among neurons 6 .For instance , it has been shown that some regions of the brain communicate more frequently than others 7 - 9 , while many exhibit greater levels of synchrony 10 . However , we also do not understand whether these changes reflect specific wiring requirements 11 or simply arise due to random fluctuations 12 .Here , we develop a new computational framework to tackle this question using computer learning techniques 13 . Specifically , we attempt to develop models capable of predicting the strength of relationship between pairs of nodes given only data about their direction and topology 14 .To achieve this goal , we first build a setting of training instance comprised of pairs of nodes whose interaction abilities are known 15 . Then , we train a classifier to teach the mapping between node characteristics and edge weights 16 .Finally , we apply the trained model to unseen test cases 17 to infer unknown interactions",
        "rewrite_text": "**Title:** Predicting the Connectivity of Primate Cortical Networks from Topological and Spatial Node Characteristics\n\n**Abstract:** In this study, we introduce a novel methodology for predicting the structural connectivity of cortical networks in the primate brain, leveraging both local node characteristics—such as their spatial positioning on the brain's surface or within its volume—and global topological features. Our approach facilitates an exploration of the interconnectivity of various node types across different primate species, specifically humans and macaques, utilizing diffusion MRI tractography as our primary imaging modality. The findings indicate that our predictive model successfully captures established patterns of cortico-cortical connectivity, particularly those identified in humans, while also revealing connections in macaques that have not been previously documented. This framework is versatile and can be applied to any dataset that provides information on individual node locations and their pairwise relationships, encompassing both anatomical and functional imaging data. Such versatility allows for a comprehensive examination of the interplay between structural form and functional dynamics across multiple scales.\n\nThe field of brain connectomics seeks to create a detailed map of neuronal components to enhance our understanding of the human mind. Recent advancements in neuroimaging technologies have enabled researchers to generate intricate maps of the brain's structural, metabolic, and physiological architecture. These innovations present unprecedented opportunities to investigate the brain's functionality through its large-scale organization. However, significant ambiguity remains regarding the precise nature of neuronal interactions. For example, certain brain regions exhibit more frequent communication than others, while some demonstrate heightened synchrony. It remains unclear whether these patterns indicate specific wiring requirements or are merely the result of random fluctuations.\n\nTo address these questions, we have developed a computational framework that employs machine learning techniques to predict the strength of relationships between pairs of nodes based solely on their directional and topological attributes. Our methodology involves constructing a training dataset consisting of known interactions between node pairs, followed by training a classifier to establish the mapping between node characteristics and edge weights. Ultimately, we apply this trained model to previously unseen test cases to infer unknown interactions, thereby advancing our understanding of cortical connectivity in primates.",
        "ori-fast-z-score": -0.7877263614433762,
        "water-fast-z-score": 8.857142857142858,
        "rewrite-fast-z-score": 1.049344364594206
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Theory of Flicker Noise in Metal Films .\nAbstract:\nThe flicker noise is the low-frequency fluctuations observed in electrical resistance and other transport properties of metals at temperatures below 1 K.  The theory developed by Altshuler, Aronov, and Khmelnitsky (AAK) explains this phenomenon as arising due to electron-electron interactions within the metal film.  In their original work they assumed that electrons are scattered elastically off impurities or phonons.  However, recent experiments have shown that there can be significant inelastic scattering between electrons which leads to additional contributions to the resistivity.  Here we present an extension of AAK s theory for the case where both elastic and inelastic scattering processes contribute to the resistivity.  We show how our results compare with existing experimental data on thin gold films grown epitaxially on silicon substrates. The flicker noise is the low- frequency fluctuations observed in electrical resistance  and other transport properties of metallic systems at temperatures below 1K . It was first discovered in 1963 when measuring the resistance of thin silver wires  1  , but it has since been found in many different types of materials including semiconductors  2  , superconductors  3  , carbon nanotubes  4  , graphene  5  , and topological insulators  6  .\nIn order to explain these observations, Altshuler et al. (AAK) proposed a theoretical model based on the assumption that electrons scatter elastically off impurities  7, 8  .  This approach successfully describes most of the available experimental data  9  , however some discrepancies were recently reported  10  . These deviations may arise because the elastic approximation does not take into account possible inelastic scattering events  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Theory of Flicker Noise in Metal Films . Abstract : The flicker sound is the small - frequency fluctuations found in electrical resistance and other transport properties of metals at conditions below 1 K . The theory advanced by Altshuler , Aronov , and Khmelnitsky ( AAK ) presents this phenomenon as occurring due to ion - atom relationships within the metal film .In their early research they thought that electrons are scattered elastically off impurities or phonons . However , recent experiments have shown that there can be considerable inelastic scattering between electrons which results to extra contributions to the resistivity .Here we present an extension of AAK s concept for the case where both elastic and inelastic scattering mechanisms contribute to the resistivity . We see how our findings compare with existing experimental evidence on thin gold films grown epitaxially on silicon substrates .The flicker sound is the small - frequency fluctuations detected in electrical resistance and other transport properties of metallic components at conditions below 1K . It was first discovered in 1963 when monitoring the resistance of short silver cables 1 , but it has since been seen in multiple diverse kinds of materials namely semiconductors 2 , superconductors 3 , silicon nanotubes 4 , graphene 5 , and topological insulators 6 .In order to explain these observations , Altshuler et al . ( AAK ) proposed a theory approach using on the assumption that electrons scatter elastically off impurities 7 , 8 .This method effectively describes most of the provided experimental evidence 9 , however some discrepancies were recently noted 10 . These deviations might arise because the elastic equation does not take into consideration available inelastic scattering events 11 .",
        "rewrite_text": "**Title:** Quantum Theory of Flicker Noise in Metal Films\n\n**Abstract:** Flicker noise, characterized by low-frequency fluctuations in electrical resistance and other transport properties of metals at temperatures below 1 K, has garnered significant attention in the field of condensed matter physics. Initially identified in 1963 during resistance measurements of short silver wires, this phenomenon has since been observed across a wide array of materials, including semiconductors, superconductors, silicon nanotubes, graphene, and topological insulators. The foundational theory proposed by Altshuler, Aronov, and Khmelnitsky (AAK) attributes flicker noise to the elastic scattering of electrons off impurities and phonons within the metal films. While this model successfully accounts for a majority of experimental observations, recent studies have revealed notable discrepancies that suggest the need for a more comprehensive understanding of the underlying mechanisms. \n\nIn this article, we extend the AAK framework to incorporate both elastic and inelastic scattering processes, providing a more nuanced perspective on how these interactions contribute to resistivity. Our analysis reveals that inelastic scattering, which involves energy exchange between electrons, plays a significant role in the observed resistivity, particularly in thin gold films grown epitaxially on silicon substrates. By comparing our theoretical predictions with existing experimental data, we demonstrate that our modified model aligns more closely with the observed behavior of flicker noise in these materials. This work not only enhances the theoretical understanding of flicker noise but also opens avenues for further experimental investigations into the quantum mechanical interactions that govern transport properties in metallic systems. The implications of our findings extend beyond metal films, potentially informing research in a variety of advanced materials where flicker noise is a critical factor.",
        "ori-fast-z-score": -1.104689541477988,
        "water-fast-z-score": 7.48554161923727,
        "rewrite-fast-z-score": 0.086710996952412
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Equivariant cohomology theory of twisted generalized complex manifolds .\nAbstract:\nIn this article, we study the equivariant cohomology groups of twisted generalized complex manifolds with respect to compact Lie group actions. We show that these equivariant cohomology groups are isomorphic to those of ordinary generalized complex manifolds equipped with an invariant Hermitian metric and a compatible connection whose torsion is equal to the twisting form. As applications, we compute some examples in detail. In particular, for any closed oriented Riemann surface M , we give explicit formulas for all equivariant Betti numbers of the moduli space of stable vector bundles over M . \nIntroduction\n\nLet G be a compact connected Lie group acting on a smooth manifold X. The equivariant cohomology H*G(X) was introduced by Atiyah-Bott  1  as the cohomology ring of the Borel construction EG×_G X where EG denotes the universal bundle over BG = K(G, 1). It has been studied extensively since then (see e.g.,  2  ). For example, if X is a symplectic manifold acted upon by a torus T, then H*G(X), which can also be viewed as the equivariant cohomology of the corresponding Hamiltonian T-space, plays an important role in mirror symmetry  3  .\nThe notion of generalized complex geometry  4  provides us with another class of interesting geometric objects -the so-called generalized complex manifolds-which include both symplectic and complex manifolds as special cases. Generalized complex structures were first defined by Hitchin  5  using Courant algebroids  6  . Later Gualtieri  7  gave a more intrinsic definition via a pair of almost complex structures satisfying certain compatibility conditions. Recently, it was shown  8  that there exists a one-to-one correspondence between generalized complex structures and pairs consisting of a holomorphic Poisson structure and its associated Nijenhuis tensor field. This result allows us to define generalized complex structures intrinsically without referring to Courant algebroids; see  9  for details. \nGeneralized complex manifolds have attracted much attention recently due to their close connections with string theories  10  . Moreover, they provide new insights into many classical problems such as integr",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Equivariant cohomology theory of distorted generalized complex manifolds . Abstract : In this article , we study the equivariant cohomology groups of distorted generalized complex manifolds with regard to compact Lie group actions .We see that these equivariant cohomology groups are isomorphic to those of simple generalized complex manifolds equipped with an invariant Hermitian metric and a compatible link whose torsion is equal to the twisting form . As applications , we compute some examples in detail .In particular , for any closed oriented Riemann sphere M , we give explicit formulas for all equivariant Betti numbers of the moduli space of stable vector bundles over M . Introduction Let G be a compact connected Lie ring acting on a smooth manifold X .The equivariant cohomology H * G ( X ) was introduced by Atiyah - Bott 1 as the cohomology ring of the Borel construction EG× _ G X where EG indicates the universal manifold over BG = K ( G , 1 ) . It has been studied thoroughly since then ( saw e . g . , 2 ) .For instance , if X is a symplectic manifold acted upon by a torus T , then H * G ( X ) , which can also be viewed as the equivariant cohomology of the associated Hamiltonian T - space , takes an important role in mirror symmetry 3 . The idea of generalized complex geometry 4 provides us with another class of interesting geometric objects - the so - called generalized complex manifolds - which contain both symplectic and complex manifolds as special cases .Generalized complex systems were first defined by Hitchin 5 utilizing Courant algebroids 6 . Later Gualtieri 7 gave a more intrinsic definition via a pair of almost complex systems satisfying particular compatibility constraints .Recently , it was shown 8 that there exists a one - to - one correspondence between generalized complex objects and pairs consisting of a holomorphic Poisson structure and its associated Nijenhuis tensor field . This result allows us to define generalized complex representations intrinsically without referring to Courant algebroids ; hear 9 for information .Generalized complex manifolds have garnered many scrutiny lately owing to their deep link with string problems 10 . Moreover , they give new information into numerous classical problems such as integr",
        "rewrite_text": "**Title:** The Equivariant Cohomology Theory of Distorted Generalized Complex Manifolds\n\n**Abstract:** This article investigates the equivariant cohomology groups associated with distorted generalized complex manifolds, particularly in the context of actions by compact Lie groups. We establish that these equivariant cohomology groups exhibit isomorphism with those of simple generalized complex manifolds that possess an invariant Hermitian metric and a compatible link, where the torsion is equivalent to the twisting form. To illustrate our findings, we provide detailed computations of specific examples. Notably, for any closed oriented Riemann sphere \\( M \\), we derive explicit formulas for all equivariant Betti numbers pertaining to the moduli space of stable vector bundles over \\( M \\).\n\nIn the introduction, we consider a compact connected Lie group \\( G \\) acting on a smooth manifold \\( X \\). The concept of equivariant cohomology, denoted as \\( H^*_G(X) \\), was first introduced by Atiyah and Bott as the cohomology ring of the Borel construction \\( EG \\times_G X \\), where \\( EG \\) represents the universal manifold over \\( BG = K(G, 1) \\). This area has since been extensively explored in the literature. For example, when \\( X \\) is a symplectic manifold acted upon by a torus \\( T \\), the equivariant cohomology \\( H^*_G(X) \\) plays a crucial role in mirror symmetry.\n\nThe framework of generalized complex geometry introduces a fascinating class of geometric structures known as generalized complex manifolds, which encompass both symplectic and complex manifolds as special cases. Initially defined by Hitchin through the lens of Courant algebroids, generalized complex structures were later reformulated by Gualtieri using a pair of almost complex structures that adhere to specific compatibility conditions. Recent developments have revealed a one-to-one correspondence between generalized complex structures and pairs comprising a holomorphic Poisson structure alongside its associated Nijenhuis tensor field. This correspondence facilitates the intrinsic definition of generalized complex representations, independent of Courant algebroids. The study of generalized complex manifolds has gained significant attention due to their profound connections with string theory and their contributions to resolving various classical problems in geometry.",
        "ori-fast-z-score": -0.1643989873053573,
        "water-fast-z-score": 5.526066862243561,
        "rewrite-fast-z-score": 1.191759143062248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal Cache-Oblivious Mesh Layouts .\nAbstract:\nWe present the first provably optimal cache-oblivious layouts for meshes with arbitrary aspect ratios and dimensions, which are based on recursive striping into rectangular tiles. Our results hold both in terms of worst-case performance guarantees as well as average case behavior under realistic assumptions about access patterns. We also show how to extend our techniques to handle more general data structures such as trees or graphs. The layout problem is formulated as an optimization problem: given a set of objects that need to be stored in memory, we want to find their positions so that they can be accessed most efficiently by future queries. In this work, we consider the following scenario: A large amount of data needs to be stored in main memory (e.g., a database). This data consists of many small objects (e.g., records) whose sizes vary greatly; some may even be larger than available space. To solve this problem, one could use a standard technique called blocking: each object is divided into blocks of equal size, and then these blocks are placed contiguously within the allocated storage area. However, if there are too few blocks per object, it will not fit entirely inside its assigned block; similarly, if there are too many blocks per object, the unused space between them cannot be used effectively.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal Cache - Oblivious Mesh Layouts . Abstract : We report the first provably ideal cache - oblivious layouts for meshes with arbitrary aspect ratios and dimensions , which are based on recursive striping into rectangular tiles .Our results hold both in terms of bad - case performance assurance as well as average case behavior under realistic assumptions about connection trends . We also demonstrate how to expanded our approaches to treat more general data forms such as trees or graphs .The layout question is formulated as an algorithm question : given a group of items that must to be transferred in memory , we try to find their positions so that they can be obtained most efficiently by future queries . In this study , we imagine the following situations : A wide deal of data needs to be transferred in central memory ( e . g . , a database ) .This data contains of several small items ( e . g . , records ) whose sizes differ significantly ; some may even be larger than provided space . To solve this situation , one might using a traditional technique called blocking : each object is separated into blocks of equal size , and then these blocks are arranged contiguously within the allocated storage space .However , if there are too few blocks per object , it will not fit completely inside its allocated block ; likewise , if there are too several blocks per object , the empty space between them cannot be used effectively .",
        "rewrite_text": "We present groundbreaking research on optimal cache-oblivious layouts for meshes characterized by arbitrary aspect ratios and dimensions, marking the first instance of provably ideal configurations in this domain. Our methodology employs a recursive striping technique that organizes data into rectangular tiles, ensuring efficient access patterns. The findings are robust, demonstrating both worst-case performance guarantees and favorable average-case behavior under realistic connection trends. Furthermore, we extend our approach to accommodate more complex data structures, including trees and graphs.\n\nThe core of our investigation revolves around the algorithmic challenge of determining optimal data placement in memory. Specifically, we address the scenario where a substantial volume of data, such as that found in databases, needs to be efficiently transferred within central memory. This data comprises numerous small items, such as records, which can vary significantly in size, with some potentially exceeding the allocated memory space.\n\nTo tackle this issue, we critique traditional blocking techniques, where each data item is divided into uniformly sized blocks that are then arranged contiguously in memory. While this method has its merits, it presents challenges: insufficient blocks may result in incomplete data fitting within the allocated space, while excessive blocks can lead to inefficient use of memory due to wasted gaps. Our research aims to refine this process, ensuring that data is organized in a manner that maximizes retrieval efficiency and minimizes wasted space, ultimately enhancing the performance of memory-intensive applications. Through our innovative layouts, we pave the way for more effective data management strategies in cache-oblivious environments.",
        "ori-fast-z-score": -2.8736848324283986,
        "water-fast-z-score": 5.567764362830022,
        "rewrite-fast-z-score": 0.24743582965269675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Linear pre-metric electrodynamics and deduction of the light cone .\nAbstract:\nThe author considers an alternative approach to classical electrodynamics, which is based on the concept of  pre-metricity  introduced by Einstein in his work  On the Electrodynamics of Moving Bodies  (1905). The main idea behind this approach consists in introducing into consideration not only the usual electromagnetic field tensor Fμν but also its dual counterpart *Fμν . In order to obtain equations for both fields simultaneously one has to introduce additional terms into Lagrangian density L(Fμν ,*Fμν ) that are quadratic with respect to derivatives of these fields. It turns out that such modified Lagrangian leads to the following set of equations: \nwhere  is the d Alembert operator,  is the energy-momentum tensor of matter fields, and  is the current four-vector describing the flow of electric charge. \nIn addition to the standard Maxwell s equations, Eqs. (1)-(3), there appears another equation -the so-called  duality condition :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Linear pre - metric electrodynamics and deduction of the light cone . Abstract : The author considers an additional method to classical electrodynamics , which is based on the idea of pre - metricity introduced by Einstein in his book On the Electrodynamics of Moving Bodies ( 1905 ) .The main idea behind this methodology consists in adding into consideration not only the usual electromagnetic field vector Fμν but also its dual counterpart * Fμν . In order to obtain equations for both fields concurrently one has to introduce extra terms into Lagrangian density L ( Fμν , * Fμν ) that are quadratic with regard to derivatives of these fields .It turns out that such modified Lagrangian leads to the following series of equations : where is the d Alembert operator , is the electricity - momentum tensor of matter fields , and is the current four - vector describing the flow of electric current . In addition to the standard Maxwell s coefficients , Eqs .( 1 ) - ( 3 ) , there exists another equation - the so - called duality condition :",
        "rewrite_text": "Title: Linear Pre-Metric Electrodynamics and the Derivation of the Light Cone\n\nAbstract: In this article, the author explores an alternative approach to classical electrodynamics, grounded in the concept of pre-metricity as introduced by Einstein in his seminal work, \"On the Electrodynamics of Moving Bodies\" (1905). This innovative methodology expands the traditional framework by incorporating not only the standard electromagnetic field tensor \\( F_{\\mu\\nu} \\) but also its dual counterpart \\( *F_{\\mu\\nu} \\). To derive equations that govern both fields simultaneously, it is necessary to augment the Lagrangian density \\( L(F_{\\mu\\nu}, *F_{\\mu\\nu}) \\) with additional terms that are quadratic in the derivatives of these fields. The resulting modified Lagrangian yields a comprehensive set of equations, which include the d'Alembert operator, the energy-momentum tensor for matter fields, and the four-current vector that represents the flow of electric charge. Alongside the conventional Maxwell equations, this framework introduces an additional equation known as the duality condition. This condition plays a crucial role in establishing the relationships between the electric and magnetic fields, thereby enriching the theoretical landscape of electrodynamics. The implications of this approach are significant, as it not only provides a deeper understanding of the underlying principles of electrodynamics but also facilitates the derivation of the light cone, a fundamental concept in the theory of relativity. By integrating the dual field perspective, the author aims to present a more unified and comprehensive view of electromagnetic phenomena, potentially paving the way for new insights and advancements in the field.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 2.5927248643506746,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evidence for a jet contribution to the optical/infrared light of neutron star X-ray binaries .\nAbstract:\nWe present new evidence that jets contribute significantly to the observed infrared and optical emission in some neutron star X-ray binary systems, based on simultaneous multiwavelength observations with Swift/XRT (0.3-10 keV), Chandra/ACIS-S/HRC-I (0.5-8 keV) and Spitzer/IRAC (3.6-24 microns). We find that the spectral energy distribution is well described by an absorbed power law plus blackbody model over this broad range of wavelengths. The best-fit parameters are consistent with those found previously at higher energies using RXTE data alone. \n \n In addition we detect significant variability between epochs separated by months or years. This suggests that there may be multiple components contributing to the overall spectrum; one component which varies rapidly but only weakly contributes to the total flux density, while another component dominates the luminosity output and shows little variation. These results provide further support for models where relativistic outflows play an important role in shaping the evolution of these sources.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evidence for a jet contribution to the optical / infrared light of neutron star X - ray binaries . Abstract : We report new evidence that jets contribute greatly to the seen infrared and optical emission in some neutron star X - ray binary complexes , based on concurrent multiwavelength studies with Swift / XRT ( 0 . 3 - 10 keV ) , Chandra / ACIS - S / HRC - I ( 0 . 5 - 8 keV ) and Spitzer / IRAC ( 3 . 6 - 24 microns ) .We see that the spectral power distribution is well described by an absorption power law plus blackbody model over this wide spectrum of wavelengths . The best - fitting coefficients are compatible with those identified previously at higher energies using RXTE information alone .In addition we find considerable variability between epochs separated by months or months . This implies that there may be several elements contributing to the overall spectrum ; one element which varies rapidly but only strongly contributes to the total flux concentration , while another component dominates the luminosity production and shows small variation .These data provide further evidence for models where relativistic outflows serve an important role in shaping the evolution of these sources .",
        "rewrite_text": "We present compelling new evidence indicating that jets play a significant role in the optical and infrared emissions observed in certain neutron star X-ray binary systems. This conclusion is drawn from comprehensive multiwavelength observations conducted with Swift/XRT (0.3 - 10 keV), Chandra/ACIS-S/HRC-I (0.5 - 8 keV), and Spitzer/IRAC (3.6 - 24 microns). Our analysis reveals that the spectral power distribution across these diverse wavelengths can be effectively modeled using a combination of an absorption power law and a blackbody function. Notably, the best-fitting parameters align closely with those previously determined from higher energy data obtained through RXTE observations.\n\nMoreover, our findings indicate significant variability in the emissions over time scales of months, suggesting the presence of multiple components influencing the overall spectral characteristics. Specifically, we propose that one component exhibits rapid variability but contributes significantly to the total flux, while another component, which is more stable, dominates the luminosity output with minimal fluctuations. These observations lend further support to theoretical models positing that relativistic outflows are crucial in shaping the evolution and emission profiles of neutron star X-ray binaries. This study enhances our understanding of the complex interplay between jets and the electromagnetic emissions from these astrophysical systems, highlighting the importance of multiwavelength approaches in unraveling the mechanisms at play.",
        "ori-fast-z-score": -1.3480372031495529,
        "water-fast-z-score": 5.363390480545726,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Searching for galactic cosmic ray pevatrons with multi-TeV gamma rays and neutrinos .\nAbstract:\nWe present the results of searches for Pevatron candidates in the northern hemisphere using data collected by the High Energy Stereoscopic System (HESS) between 2004 and 2007, as well as IceCube data taken during 2005-2007. We find no significant excesses above background expectations at any point on the sky. Upper limits are set on the flux density of TeV photons and neutrinos associated with hypothetical sources within our field-of-view. These upper limits are used to constrain theoretical models describing the production mechanisms responsible for accelerating particles up to energies approaching 10^14 eV. The HESS collaboration has recently reported an observation of a new source of very-high-energy (VHE; >100 GeV) gamma-rays located near the Galactic Center  1  . This source is spatially coincident with the supernova remnant Sgr A East  2  , which was previously detected in radio waves  3  .\nThe discovery of this VHE source raises several questions about its origin. In particular, it remains unclear whether or not the observed emission arises directly from accelerated protons interacting with ambient gas  4  , or if other processes such as inverse Compton scattering off electrons  5  and/or bremsstrahlung  6  play a dominant role. It also remains unknown how these energetic particles were accelerated to their high energy levels  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Searching for galactic gamma ray pevatrons with multi - TeV gamma particles and neutrinos . Abstract : We present the results of investigations for Pevatron candidates in the northern hemisphere using data taken by the High Energy Stereoscopic System ( HESS ) between 2004 and 2007 , as also as IceCube information taken during 2005 - 2007 .We get no major excesses above background expectations at any point on the sky . Upper constraints are set on the flux concentration of TeV photons and neutrinos associated with hypothetical sources within our field - of - view .These upper limits are applied to constrain theoretical theories describing the production mechanisms involved for accelerating particles up to energies approaching 10 ^ 14 eV . The HESS collaboration has recently noted an observation of a new source of very - large - energy ( VHE ; > 100 GeV ) γ - radiation located near the Galactic Center 1 .This source is spatially coincident with the supernova remnant Sgr A East 2 , which was formerly detected in radio pulses 3 . The observation of this VHE source raises various issues about its identity .In particular , it remains unsure whether or not the seen emission arises directly from accelerated protons interacting with ambient gas 4 , or if other processes such as inverse Compton absorption off electrons 5 and / or bremsstrahlung 6 hold a dominant role . It additionally continues unclear how these energetic particles were accelerated to their high energy levels 7 , 8 .",
        "rewrite_text": "We present the findings of our research aimed at identifying potential Pevatron candidates in the northern hemisphere, utilizing data collected by the High Energy Stereoscopic System (HESS) from 2004 to 2007, alongside information from IceCube gathered between 2005 and 2007. Our analysis did not reveal any significant excesses above the anticipated background levels across the observed sky. Consequently, we established upper limits on the flux of TeV photons and neutrinos that could be associated with hypothetical sources within our observational framework. These upper limits serve to refine theoretical models concerning the mechanisms responsible for accelerating particles to energies nearing 10^14 eV.\n\nThe HESS collaboration has recently reported the detection of a new source of very high-energy (VHE; > 100 GeV) gamma radiation situated near the Galactic Center. This source is spatially correlated with the supernova remnant Sgr A East, which had previously been identified through radio emissions. The discovery of this VHE source prompts several questions regarding its nature and origin. Specifically, it remains uncertain whether the observed emissions are a direct result of accelerated protons interacting with surrounding gas or if alternative processes, such as inverse Compton scattering off electrons or bremsstrahlung, play a more significant role in the observed radiation. Furthermore, the mechanisms by which these high-energy particles are accelerated to such extreme levels are still not fully understood. This research contributes to the ongoing discourse regarding cosmic ray acceleration and the potential sources of high-energy emissions in our galaxy, highlighting the need for further investigation into the underlying processes that govern these phenomena.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 5.2,
        "rewrite-fast-z-score": -0.3682298471593294
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Boundary Conditions of the Heliosphere: Photoionization Models Constrained by Interstellar and In Situ Data .\nAbstract:\nWe present photoionization models for the heliosheath, which is the region between the termination shock (TS) at ~100 AU and the solar wind termination shock (SWTS). The SWTS is located beyond 1 AU in the solar system s frame but within 0.3 AU in the rest frame of the Sun. We use these models to constrain the boundary conditions of the heliosphere using interstellar neutral hydrogen data obtained with the Lyman-alpha instrument on board the Solar Wind Anisotropy Probe (SWAP), as well as in situ measurements made near Earth during the Voyager 2 mission. Our results show that the TS distance decreases with increasing solar activity; this effect can be explained by an increase in the density of the solar wind plasma. For low solar activity levels we find that the TS distance agrees very well with previous estimates based on observations of energetic particles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Boundary Conditions of the Heliosphere : Photoionization Models Constrained by Interstellar and In Situ Data . Abstract : We create photoionization estimates for the heliosheath , which is the region between the termination shock ( TS ) at ~ 100 AU and the planetary wind termination shock ( SWTS ) .The SWTS is situated beyond 1 AU in the solar system s frame but within 0 . 3 AU in the remainder frame of the Sun . We use these models to constrain the boundary pressures of the heliosphere utilizing interstellar neutral hydrogen evidence derived with the Lyman - alpha instrument on board the Solar Wind Anisotropy Probe ( SWAP ) , as well as in situ measurements made near Earth during the Voyager 2 mission .Our results show that the TS distance decreases with rising sun activity ; this effect can be described by an increase in the density of the solar wind plasma . For lowest sun activity rates we find that the TS distance agrees very best with previous accounts based on observations of energetic particles .",
        "rewrite_text": "In this study, we present a comprehensive analysis of photoionization models for the heliosheath, the region located between the termination shock (TS) at approximately 100 astronomical units (AU) and the solar wind termination shock (SWTS). The SWTS is positioned beyond 1 AU in the solar system's reference frame, yet it lies within 0.3 AU in the Sun's rest frame. Our research employs these models to define the boundary pressures of the heliosphere, leveraging interstellar neutral hydrogen data obtained from the Lyman-alpha instrument aboard the Solar Wind Anisotropy Probe (SWAP), alongside in situ measurements collected during the Voyager 2 mission near Earth. \n\nOur findings indicate a correlation between solar activity and the distance to the TS, revealing that as solar activity increases, the TS distance diminishes. This phenomenon can be attributed to a rise in the density of the solar wind plasma. Notably, during periods of minimal solar activity, our measurements of the TS distance align closely with previous observations related to energetic particles. This work enhances our understanding of the heliosphere's boundary conditions and provides critical insights into the interplay between solar activity and the dynamics of the heliospheric environment. By integrating observational data with theoretical models, we aim to refine the existing frameworks surrounding heliospheric boundaries and contribute to the broader understanding of solar and interstellar interactions.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 3.542913886332851,
        "rewrite-fast-z-score": 0.618852747755276
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Possible Stellar Metallic Enhancement in Post-T Tauri Stars by a Planetesimal Bombardment .\nAbstract:\nWe propose that the metallic enhancement observed for some post T Tauri stars (PTTS) may be due to an accretion of planetesimals during their formation phase, which is followed by rapid planet formation and subsequent ejection of planets into space.  We show that this scenario can explain both the high metallicity found among PTTS as well as the low abundance ratios between refractory elements such as Mg/Si or Al/Si compared with those expected if these objects formed through standard core-accretion processes. The proposed mechanism also explains why there are no known close-in giant planets around PTTSs despite the fact that they have already passed their protoplanetary disk stage. This model predicts that most PTTS should host at least one Jupiter mass planet on wide orbits beyond 1 AU. In addition we predict that many PTTS will exhibit infrared excesses caused by dusty debris disks produced by collisions between planetary bodies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Possible Stellar Metallic Enhancement in Post - T Tauri Stars by a Planetesimal Bombardment . Abstract : We suggest that the metallic enhancement detected for some post T Tauri stars ( PTTS ) may be due to an accretion of planetesimals during their formed phase , which is preceded by rapid planet development and subsequent ejection of stars into space .We see that this situation can describe both the high metallicity known among PTTS as also as the poor abundance proportions between refractory objects such as Mg / Si or Al / Si compared with those expected if these objects created through conventional core - accretion cycles . The proposed mechanism also explains why there are no known close - in massive planets around PTTSs despite the fact that they have already completed their protoplanetary disk stage .This theory predicts that most PTTS should accommodate at least one Jupiter mass planet on wide orbits beyond 1 AU . In addition we estimate that several PTTS will exhibit infrared excesses caused by dusty dust disks produced by collisions between planetary body .",
        "rewrite_text": "Title: A Potential Stellar Metallic Enhancement in Post-T Tauri Stars Due to Planetesimal Bombardment\n\nAbstract: In this study, we propose that the observed metallic enhancement in certain post-T Tauri stars (PTTS) may result from the accretion of planetesimals during their formative stages. This process is believed to follow a rapid phase of planet formation, which is subsequently followed by the ejection of stars into space. Our analysis indicates that this scenario not only accounts for the elevated metallicity observed in PTTS but also explains the unusual abundance ratios of refractory elements, such as magnesium to silicon (Mg/Si) and aluminum to silicon (Al/Si), which deviate from the expected values derived from traditional core-accretion models. Furthermore, this hypothesis sheds light on the absence of known close-in massive planets around PTTS, despite their completion of the protoplanetary disk phase. According to our theory, it is anticipated that most PTTS should host at least one Jupiter-mass planet located in wider orbits, specifically beyond 1 astronomical unit (AU). Additionally, we predict that several PTTS will display infrared excesses, a phenomenon attributed to the presence of dusty disks formed by collisions among planetary bodies. This research provides a novel perspective on the formation and evolution of PTTS, suggesting that planetesimal bombardment plays a crucial role in shaping their metallic composition and planetary system architecture.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": 0.30779350562554625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Nonperiodic Anyon Model and the Fractional Quantum Hall Effect .\nAbstract:\nThe nonperiodic anyon model is introduced as an alternative to the periodic one in order to explain fractional quantum hall effect (FQHE). The ground state wave function for this system is obtained by using the method of projection operators, which leads to a new expression for the Laughlin wave functions. It is shown that these states are exact eigenstates of the total angular momentum operator with eigenvalues equal to the number of particles times their charge e*. This result shows that the nonperiodic anyons can be considered as charged particles moving on a sphere. Finally we show how our results can be applied to describe FQHE at filling fractions other than 1/3. In recent years there has been considerable interest in studying systems consisting of interacting electrons confined to two dimensions  1  . One of the most interesting phenomena observed experimentally  2  , known as the fractional quantum Hall effect (FQHE), occurs when such two-dimensional electron gas is subjected to strong magnetic fields  3  .\nIn the original work  4  it was suggested that the FQHE could be explained within the framework of the so-called Laughlin wave functions  5  . These wave functions were constructed by assuming that each particle moves around its own guiding center  6  . However, later studies  7 -9  showed that the actual behavior of the electrons in real experiments cannot be described accurately by considering them as point-like objects. Instead, they should be treated as extended objects whose size depends upon the strength of the external magnetic field  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Nonperiodic Anyon Model and the Fractional Quantum Hall Effect . Abstract : The nonperiodic anyon theory is proposed as an alternative to the periodic one in order to explain fractional quantum hall impact ( FQHE ) .The ground state wave function for this scheme is found by using the method of projection operators , which results to a new representation for the Laughlin wave functions . It is seen that these states are exact eigenstates of the total angular velocity operator with eigenvalues equal to the number of molecules times their charge e * .This result shows that the nonperiodic anyons can be regarded as charged particles moving on a sphere . Finally we show how our findings can be applied to define FQHE at filling fractions other than 1 / 3 .In recent years there has been substantial interest in investigating systems composed of interacting electrons dispersed to two dimensions 1 . One of the most exciting phenomena observed experimentally 2 , known as the fractional quantum Hall impact ( FQHE ) , happened when such two - dimensional electron gas is subjected to powerful magnetic fields 3 .In the first paper 4 it was suggested that the FQHE might be described within the framework of the so - called Laughlin wave curves 5 . These wave functions were created by assuming that each particle moves surrounding its own guiding center 6 .However , later research 7 - 9 demonstrated that the actual activity of the electrons in real studies cannot be described properly by treating them as point - like structures . Instead , they should be treated as extended things whose shape depends upon the strength of the external magnetic force 10 .",
        "rewrite_text": "Title: The Nonperiodic Anyon Model and the Fractional Quantum Hall Effect\n\nAbstract: This article introduces the nonperiodic anyon model as a novel framework to elucidate the fractional quantum Hall effect (FQHE). By employing projection operator techniques, we derive the ground state wave function for this model, which leads to a new formulation of the Laughlin wave functions. Notably, these states emerge as exact eigenstates of the total angular momentum operator, with eigenvalues corresponding to the product of the number of particles and their charge, e*. This finding indicates that nonperiodic anyons can be conceptualized as charged particles residing on a spherical surface. Furthermore, we explore the implications of our results for defining the FQHE at various filling fractions beyond the conventional 1/3. \n\nThe study of two-dimensional electron systems has garnered significant attention in recent years, particularly due to the intriguing phenomena associated with the FQHE, which occurs when a two-dimensional electron gas is subjected to strong magnetic fields. Initial investigations proposed that the FQHE could be effectively described using Laughlin wave functions, which were formulated under the assumption that each electron orbits around its own guiding center. However, subsequent research has revealed that this point-like treatment of electrons is insufficient for accurately capturing their behavior in experimental settings. Instead, it has become evident that electrons should be modeled as extended entities, with their spatial configurations influenced by the strength of the external magnetic field. This article aims to bridge the gap between theoretical models and experimental observations, providing a deeper understanding of the FQHE through the lens of nonperiodic anyons.",
        "ori-fast-z-score": -1.6296434287653334,
        "water-fast-z-score": 6.041186552271796,
        "rewrite-fast-z-score": 0.6311687442672026
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The richest superclusters. I. Morphology .\nAbstract:\nWe present the results on morphology and luminosity function for the most luminous galaxy clusters in the Universe, selected by their X-ray emission (the RCS2 sample). We find that these objects are characterized by an elliptical shape with axial ratio q = 0.7 ± 0.1 and by a steep luminosity function dN/dL ∝ L−2.5±0.3 . The observed properties suggest that they may be identified as fossil groups or proto-clusters at z > 1.0 .\nThe data used here were obtained during our observing runs performed at ESO telescopes under programs IDs: 073.A-0505(B), 078.A-0518(C) and 079.A-0739(D) . In this work we study the morphological and photometric properties of the brightest galaxy clusters in the universe. These systems have been detected through their X-ray emission using the ROSAT All Sky Survey (RASS; Voges et al., 1999) , and then followed up spectroscopically to confirm their redshifts and measure their velocity dispersions (see e.g. Rosati et al. , 1998 , Gladders & Yee 2005 , Eisenhardt et al. , 2008 . They represent some of the most massive structures known so far in the universe, being able to host several thousands of galaxies each one. Their high mass makes them ideal targets to investigate how such large scale structures form and evolve over time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The richest superclusters . I . Morphology .Abstract : We report the results on morphology and luminosity function for the most luminous galaxy galaxies in the Universe , selected by their X - ray radiation ( the RCS2 specimen ) . We see that these objects are marked by an elliptical shape with axial proportion q = 0 . 7 ± 0 . 1 and by a steep luminosity function dN / dL [UNK] L−2 . 5±0 . 3 .The observed properties suggest that they may be identified as extinct families or proto - complexes at z > 1 . 0 . The data used here were obtained during our observing walks performed at ESO telescopes under programs IDs : 073 . A - 0505 ( B ) , 078 . A - 0518 ( C ) and 079 . A - 0739 ( D ) .In this research we study the morphological and photometric properties of the brightest galaxy galaxies in the universe . These systems have been detected through their X - ray emission utilizing the ROSAT All Sky Survey ( RASS ; Voges et al . , 1999 ) , and then followed up spectroscopically to confirm their redshifts and track their velocity dispersions ( see e . g .Rosati et al . , 1998 , Gladders & Yee 2005 , Eisenhardt et al ., 2008 . They hold some of the most gigantic structures discovered so far in the universe , being could to host numerous thousands of galaxies each one .Their high mass creates them ideal targets to examine how such large scale structures structure and evolve over time .",
        "rewrite_text": "**Title:** The Richest Superclusters I: Morphology\n\n**Abstract:** In this study, we present our findings on the morphological characteristics and luminosity function of the most luminous galaxy clusters in the universe, identified through their X-ray emissions in the RCS2 sample. Our analysis reveals that these clusters exhibit an elliptical morphology with an axial ratio of q = 0.7 ± 0.1, alongside a steep luminosity function characterized by dN/dL ∝ L^(-2.5 ± 0.3). These properties indicate that these clusters may represent extinct families or proto-complexes at redshifts greater than z > 1. The data utilized in this research were collected during observational campaigns at the European Southern Observatory (ESO) under the program IDs: 073.A-0505(B), 078.A-0518(C), and 079.A-0739(D). \n\nOur investigation focuses on the morphological and photometric attributes of the brightest galaxy clusters, which were initially detected through their X-ray emissions as part of the ROSAT All Sky Survey (RASS; Voges et al., 1999). Subsequent spectroscopic follow-ups were conducted to confirm their redshifts and to monitor their velocity dispersions, as referenced in previous works (e.g., Rosati et al., 1998; Gladders & Yee, 2005; Eisenhardt et al., 2008). These clusters represent some of the largest structures identified in the universe, each potentially housing thousands of galaxies. Their significant mass makes them prime candidates for studying the formation and evolution of large-scale structures over cosmic time. This research contributes to our understanding of the universe's most massive systems and their role in cosmic evolution.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 5.116817192534651,
        "rewrite-fast-z-score": 1.7085642859406605
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The shapes, orientation, and alignment of Galactic dark matter subhalos .\nAbstract:\nWe present the results of an analysis of the shapes, orientations, and alignments of dark matter subhalos in cosmological N-body simulations with different levels of baryonic physics included.  We find that the inclusion of baryons has little effect on the shape distribution but does affect the spin parameter distributions significantly; halos are more spherical when baryons are included than they would be if only gravity were acting upon them. The halo spins tend to be aligned perpendicularly to their major axes for all models considered here (including pure dark matter). This is consistent with previous studies which have found similar trends using other methods. However we also find evidence that this trend may not hold at very small scales where there appears to be some correlation between the direction of the angular momentum vector and the minor axis of the halo. Finally, we show that the presence or absence of baryons affects the degree of alignment between neighboring halos; halos are less strongly clustered around each other when baryons are included.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The shapes , orientation , and alignment of Galactic dark matter subhalos . Abstract : We present the results of an assessment of the shapes , orientations , and alignments of dark matter subhalos in cosmological N - bodies simulations with various levels of baryonic physics provided .We see that the introduction of baryons has little impact on the form distribution but does affect the spin vector distributions substantially ; halos are more spherical when baryons are included than they would be if only gravitational were acting upon them . The halo spins tend to be aligned perpendicularly to their major axes for all models discussed here ( especially pure black material ) .This is consistent with previous research which have discovered similar trends using other methods . However we also find proof that this shift might not stand at very small scales where there seems to be some correlation between the direction of the angular velocity tensor and the minor axis of the halo .Finally , we find that the presence or lack of baryons affects the degree of alignment between neighboring halos ; halos are less highly clustered around each other when baryons are included .",
        "rewrite_text": "In this study, we investigate the characteristics of dark matter subhalos, focusing on their shapes, orientations, and alignments as derived from cosmological N-body simulations that incorporate varying degrees of baryonic physics. Our findings indicate that while the introduction of baryons has a minimal effect on the overall distribution of shapes, it significantly influences the distribution of spin vectors. Specifically, we observe that halos exhibit a more spherical configuration when baryonic effects are considered, as opposed to scenarios where only gravitational forces are at play. Furthermore, we note a consistent trend across all models examined, particularly in those dominated by dark matter, where halo spins are predominantly aligned perpendicular to their major axes. This observation aligns with previous studies that have reported similar patterns through alternative methodologies. However, our analysis also reveals that this alignment may not hold true at smaller scales, where we detect a correlation between the angular velocity tensor's direction and the halo's minor axis. Additionally, we explore the impact of baryonic presence on the spatial distribution of halos, discovering that the degree of alignment among neighboring halos is reduced when baryons are included in the simulations. This suggests that baryonic processes play a crucial role in shaping the large-scale structure of dark matter, influencing not only the morphology of individual halos but also their collective arrangement in the cosmic landscape. Overall, our results contribute to a deeper understanding of the interplay between dark matter and baryonic physics in the formation and evolution of cosmic structures.",
        "ori-fast-z-score": -1.8439088914585775,
        "water-fast-z-score": 4.610083899802517,
        "rewrite-fast-z-score": 2.0252641593763117
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Investigation of Colour Reconnection in WW Events with the DELPHI detector at LEP-2 .\nAbstract:\nThe colour reconnection (CR) model is used to describe how quarks and gluons rearrange themselves into hadrons after they are produced by hard scattering processes, such as those occurring during e+e-annihilation events.  The CR model predicts that particles emitted close together in phase space will be more likely to recombine than those which are further apart.  This effect can lead to changes in event topology and kinematics compared to predictions made using models without CR.  In this analysis we use data collected by the Delphi experiment operating at centre-of-mass energies between 189 GeV and 209 GeV corresponding to an integrated luminosity of 1.1 fb-1.  We measure the fraction of WW events where one or both W bosons decay leptonically for different ranges of dilepton invariant mass and compare these results to Monte Carlo simulations including and excluding CR effects.  Our measurements show no significant evidence for CR effects within our experimental uncertainties.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Investigation of Colour Reconnection in WW Events with the DELPHI detector at LEP - 2 . Abstract : The colour reconnection ( CR ) model is utilized to explain how quarks and gluons rearrange themselves into hadrons after they are produced by hard scattering mechanisms , such as those occurring during e + e - annihilation processes .The CR theory predicts that particles emitted close together in phase space will be more likely to recombine than those which are further separated . This phenomenon can lead to changes in event topology and kinematics compared to observations made using models without CR .In this analysis we utilize evidence generated by the Delphi experiment working at centre - of - mass energies between 189 GeV and 209 GeV corresponding to an unified luminosity of 1 . 1 fb - 1 . We estimate the fraction of WW events where one or both W bosons decay leptonically for different ranges of dilepton invariant mass and link these results to Monte Carlo simulations using and excluding CR effects .Our measurements show no major proof for CR effects within our experimental uncertainties .",
        "rewrite_text": "Title: Investigation of Colour Reconnection in WW Events with the DELPHI Detector at LEP-2\n\nAbstract: This study explores the concept of colour reconnection (CR) within the context of WW events, utilizing data from the DELPHI detector at the LEP-2 collider. The CR model provides a framework for understanding the rearrangement of quarks and gluons into hadrons following their production through hard scattering processes, such as those occurring during electron-positron (e+e-) annihilation. According to the CR theory, particles that are emitted in close proximity in phase space have a higher probability of recombining than those that are more widely separated. This recombination can significantly alter the event topology and kinematics when compared to predictions made by models that do not incorporate CR effects.\n\nIn our analysis, we leverage data collected by the DELPHI experiment at center-of-mass energies ranging from 189 GeV to 209 GeV, amounting to a total integrated luminosity of 1.1 fb^-1. We focus on estimating the fraction of WW events in which one or both W bosons decay leptonically, examining various ranges of dilepton invariant mass. Our findings are then correlated with Monte Carlo simulations that both include and exclude CR effects to assess the impact of colour reconnection on the observed data.\n\nThe results of our measurements indicate that there is no significant evidence supporting the presence of CR effects within the bounds of our experimental uncertainties. This outcome suggests that, at the energies studied, the influence of colour reconnection on the WW event characteristics may be minimal, warranting further investigation into the underlying mechanisms of hadronization in high-energy particle collisions.",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": 0.10050378152592121
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of a Rare Supersoft Outburst Event during a Suzaku Observation of 1E0102.2-7219 .\nAbstract:\nWe report on the detection of an extremely rare supersoft X-ray outburst event in the transient source 1E0102. 2-7219 (=GX 354-0) with the Suzaku satellite, which was triggered by a large increase in its hard Xray flux. The duration and peak luminosity are estimated to be about 100 s and 2×1036 erg/s at 6 kpc distance, respectively. This is one of only two such events ever detected for this object. We discuss possible origins of these events based on their observed properties. \n \n Keywords: Supernova remnant, Soft gamma-ray repeater, Transient source, Supersoft X-ray emission, Hard X-ray bursts \n \n \n \n 1 Introduction \n \n In recent years, several new classes of transients have been discovered through systematic searches using satellites like RXTE/ASM or Swift/BAT. These include soft-gamma repeaters (SGRs; e.g., Hurley et al. 2005), anomalous X-ray pulsars (AXPs; e.g., Kaspi & Beloborodov 2017) , and magnetar candidates (e.g., Rea et al. 2012) . Among them, SGRs show repeated short-duration bursts of high-energy radiation ranging from radio waves to gammarays. AXPs are characterized by persistent X-ray emissions that often exhibit periodic pulsations. Magnetar candidates also show similar characteristics as those of AXPs but lack clear evidence of periodicity. All three types of sources occasionally emit giant flares accompanied by energetic particle acceleration phenomena (e.g., Palmer 2014; Kashiyama et al. 2013 ). On the other hand, some of these objects sometimes undergo very faint outbursts lasting for hours to days. For example, SGR 0526-66 showed a series of such outbursts between 1979 and 1989 (Mazets et al. 1981; Cline et al. 1982; Kulkarni et al. 1993; Kouveliotou et al. 1998 ) while SGR 1900+14 exhibited another series of fainter ones between 1997 and 2001 . Such",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of a Rare Supersoft Outburst Event during a Suzaku Observation of 1E0102 . 2 - 7219 . Abstract : We report on the observation of an incredibly rare supersoft X - ray outburst incident in the transient source 1E0102 .2 - 7219 ( = GX 354 - 0 ) with the Suzaku spacecraft , which was triggered by a large rise in its hard Xray flux . The periods and peak luminosity are estimated to be about 100 s and 2×1036 erg / s at 6 kpc distance , respectively .This is one of only two such events ever observed for this object . We discuss possible origins of these events according on their observed properties .Keywords : Supernova remnant , Soft gamma - ray repeater , Transient source , Supersoft X - ray radiation , Hard X - ray bursts 1 Introduction In recent years , various additional types of transients have been detected through widespread searches using satellites like RXTE / ASM or Swift / BAT . These include soft - gamma repeaters ( SGRs ; e . g . , Hurley et al .2005 ) , anomalous X - ray pulsars ( AXPs ; e . g . , Kaspi & Beloborodov 2017 ) , and magnetar candidates ( e . g . , Rea et al . 2012 ) .Among them , SGRs exhibit frequent short - duration bursts of high - energy rays ranging from radio pulses to gammarays . AXPs are marked by persistent X - ray emissions that frequently exhibit periodic pulsations .Magnetar candidates often show identical traits as those of AXPs but lack firm indication of periodicity . All three categories of sources occasionally emit giant flares accompanied by energetic particle gravity phenomena ( e . g . , Palmer 2014 ; Kashiyama et al .2013 ) . On the other hand , some of these objects sometimes undergo very faint outbursts lasting for days to days .For instance , SGR 0526 - 66 demonstrated a string of such outbursts between 1979 and 1989 ( Mazets et al . 1981 ; Cline et al .1982 ; Kulkarni et al . 1993 ; Kouveliotou et al .1998 ) while SGR 1900 + 14 exhibited another series of fainter ones between 1997 and 2001 . Such",
        "rewrite_text": "We present a detailed analysis of a rare supersoft X-ray outburst observed in the transient source 1E0102.2-7219 (also known as GX 354-0) during a Suzaku mission. This extraordinary event was initiated by a significant increase in the source's hard X-ray flux. Our observations indicate that the duration of the outburst was approximately 100 seconds, with a peak luminosity estimated at around 2×10^36 erg/s, assuming a distance of 6 kpc. Remarkably, this incident marks only the second occurrence of such an outburst recorded for this particular source. In this study, we explore the potential origins of these outbursts, taking into account their unique characteristics and the context of previous findings.\n\nThe detection of various transient astronomical phenomena has surged in recent years, facilitated by extensive monitoring efforts using advanced satellite systems such as RXTE/ASM and Swift/BAT. Among the newly identified classes of transients are soft gamma-ray repeaters (SGRs), anomalous X-ray pulsars (AXPs), and magnetar candidates. SGRs are characterized by their frequent, brief bursts of high-energy emissions, which span from radio waves to gamma rays. In contrast, AXPs are distinguished by their persistent X-ray emissions, often exhibiting periodic pulsations. Magnetar candidates share similar features with AXPs but typically lack definitive evidence of periodicity. Notably, all three categories of sources are known to occasionally produce massive flares accompanied by energetic particle phenomena.\n\nAdditionally, some of these sources experience faint outbursts that can persist for several days. For example, SGR 0526-66 was observed to have a series of such faint outbursts from 1979 to 1989, while SGR 1900+14 exhibited a similar pattern between 1997 and 2001. This paper aims to contribute to the understanding of these transient phenomena by providing insights into the mechanisms behind the observed supersoft outburst in 1E0102.2-7219 and its implications for the broader field of astrophysics.",
        "ori-fast-z-score": -1.3525044520011484,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": -1.0524696231684352
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galaxy morphologies and environment in the Abell 901/902 supercluster from COMBO-17 .\nAbstract:\nWe present an analysis of galaxy morphologies, luminosities and environments for galaxies in the Abell 901;902 supercluster (z = 0.18) using data obtained with the Wide Field Imager on board the European Southern Observatory s Very Large Telescope. We find that the fraction of early-type galaxies increases strongly towards higher local densities within this supercluster. The morphological mix is also found to depend significantly on absolute magnitude; fainter galaxies are more likely to be late-types than brighter ones at fixed density. These results suggest that both environmental effects and internal processes play important roles in shaping the observed morphology-density relation. This work was supported by NASA grant NAG5-7697. -The distribution of all galaxies in our sample overlaid onto the X-ray emission detected by Chandra . Galaxies are colour-coded according to their spectroscopic redshifts as indicated in the legend.  -The projected number density profile of cluster members around Abell 902 , determined from the photometric redshift catalogue .  -The projected number density profiles of different morphological types around Abell 902 .   -The projected number density of brightest cluster galaxies ( M V < −20 ) around Abell 902 .\n-Figures showing the dependence of galaxy properties on distance from Abell 902 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galaxy morphologies and environment in the Abell 901 / 902 supercluster from COMBO - 17 . Abstract : We present an assessment of galaxy morphologies , luminosities and habitats for galaxies in the Abell 901 ; 902 supercluster ( z = 0 . 18 ) using data acquired with the Wide Field Imager on board the European Southern Observatory s Very Large Telescope .We see that the fraction of early - class stars increases strongly towards higher local densities within this supercluster . The morphological mixing is also discovered to depend greatly on absolute magnitude ; fainter clusters are more likely to be early - types than brighter ones at fixed density .These data suggest that both environmental impacts and internal mechanisms play important roles in shaping the known morphology - density relation . This research was supported by NASA grant NAG5 - 7697 .- The distribution of all galaxies in our sample overlaid onto the X - ray radiation observed by Chandra . Galaxies are colour - coded according to their spectroscopic redshifts as indicated in the legend .- The estimated number density profile of cluster groups around Abell 902 , obtained from the photometric redshift catalogue . - The estimated number density profiles of different morphological types around Abell 902 .- The estimated number density of brightest cluster clusters ( M V < −20 ) around Abell 902 . - Figures displaying the dependence of galaxy features on distance from Abell 902 .",
        "rewrite_text": "We provide a comprehensive analysis of galaxy morphologies, luminosities, and environments within the Abell 901/902 supercluster (z = 0.18), utilizing data collected from the Wide Field Imager on the European Southern Observatory's Very Large Telescope. Our findings reveal a significant increase in the proportion of early-type galaxies as local density rises within the supercluster. Additionally, we observe a notable correlation between morphological types and absolute magnitude; specifically, fainter galaxies tend to exhibit early-type characteristics more frequently than their brighter counterparts at a constant density. These observations imply that both environmental factors and intrinsic mechanisms are crucial in influencing the established morphology-density relationship. \n\nThe study is further supported by NASA grant NAG5-7697. We present a detailed distribution of galaxies from our sample, superimposed on X-ray emissions captured by Chandra, with galaxies color-coded based on their spectroscopic redshifts as outlined in the accompanying legend. Furthermore, we estimate the number density profile of cluster groups surrounding Abell 902, derived from our photometric redshift catalog. We also analyze the number density profiles of various morphological types in proximity to Abell 902, as well as the density of the brightest cluster galaxies (M_V < -20) in the same region. Our figures illustrate the relationship between galaxy characteristics and their distance from Abell 902, providing insights into the spatial distribution and environmental influences on galaxy morphology within this supercluster.",
        "ori-fast-z-score": -1.1285761872936695,
        "water-fast-z-score": 5.30722777603022,
        "rewrite-fast-z-score": 0.4926646390821466
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometry-induced asymmetric diffusion .\nAbstract:\nWe study the effect of geometry on the transport properties in one-dimensional (1D) and two-dimensional (2D) systems with periodic boundary conditions, by using an exact mapping to random walks. We show that for 1D chains there is no difference between open-boundary and periodic-boundary conditions; however, this is not true anymore when considering 2D lattices. In particular we find that the self-diffusion coefficient D(t), which describes how fast particles diffuse through space at time t, depends strongly on the lattice structure. For example, if the system consists of two identical sub-lattices connected via hopping processes only along the bonds connecting them, then D(t) exhibits a non-monotonic behavior as function of time: it first increases until reaching its maximum value after some characteristic relaxation time, followed by a decrease towards zero. This behavior can be explained by analyzing the probability distribution P(r,t) of finding a particle at position r at time t.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometry - mediated asymmetric diffusion . Abstract : We research the impact of geometry on the travel properties in one - dimensional ( 1D ) and two - dimensional ( 2D ) networks with periodic border conditions , by using an precise mapping to random walks .We see that for 1D chains there is no difference between open - boundary and periodic - boundary conditions ; however , this is not true anymore when using 2D lattices . In particular we find that the self - diffusion coefficient D ( t ) , which explains how rapid particles diffuse through space at period t , depends strongly on the lattice structure .For instance , if the system contains of two identical sub - lattices connected via hopping processes only along the bonds connecting them , then D ( t ) exhibits a non - monotonic activity as function of time : it first rises until reaching its highest value after some characteristic relaxation time , followed by a drop towards zero . This phenomenon can be understood by analyzing the probability distribution P ( r , t ) of finding a particle at position p at time t .",
        "rewrite_text": "In this study, we investigate the influence of geometric structures on diffusion characteristics within one-dimensional (1D) and two-dimensional (2D) networks under periodic boundary conditions, employing a precise mapping to random walk models. Our findings reveal that, in 1D chains, there is no significant distinction between open and periodic boundary conditions. However, this equivalence breaks down in 2D lattices, where the geometry plays a crucial role in diffusion behavior. We specifically examine the self-diffusion coefficient D(t), which quantifies the rate at which particles diffuse through space over time t. Our results indicate that D(t) is highly sensitive to the underlying lattice configuration. For example, in systems composed of two identical sub-lattices interconnected solely through hopping processes along their connecting bonds, we observe a non-monotonic behavior in D(t) as a function of time. Initially, D(t) increases, reaching a peak value after a characteristic relaxation time, before subsequently declining towards zero. This intriguing phenomenon can be elucidated by analyzing the probability distribution P(r, t), which describes the likelihood of locating a particle at position p at a given time t. Our research highlights the critical role of geometric factors in diffusion processes, providing insights that could enhance our understanding of transport phenomena in various physical systems.",
        "ori-fast-z-score": -0.6396021490668313,
        "water-fast-z-score": 4.133991732024804,
        "rewrite-fast-z-score": -0.8700628401410971
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Millennium Galaxy Catalogue: The local supermassive black hole mass function in early- and late-type galaxies .\nAbstract:\nWe present the first measurement of the supermassive black hole (SMBH) mass function for both early-(elliptical/S0, Sa-Sb) and latetype (Scd-Sm) galaxies using data from the Millennium Galaxy Catalogue (MGC). We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations. Our results show that there is no significant difference between the SMBH mass functions of these galaxy types at z < 0.1. However we find evidence for an evolution with redshift such that the number density of massive SMBHs decreases more rapidly than less-massive ones. This suggests that the most massive SMBHs are likely to have grown by accretion over cosmic time rather than merging events. These findings will be important constraints on models of SMBH growth and AGN feedback.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Millennium Galaxy Catalogue : The regional supermassive black hole mass function in early - and mid - class objects . Abstract : We report the first measurement of the supermassive black hole ( SMBH ) mass function for both late - ( elliptical / S0 , Sa - Sb ) and latetype ( Scd - Sm ) galaxies using data from the Millennium Galaxy Catalogue ( MGC ) .We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations . Our results show that there is no major variation between the SMBH mass parameters of these galaxy types at h < 0 . 1 .However we find proof for an evolution with redshift such that the number density of large SMBHs falls more swiftly than less - massive ones . This implies that the most gigantic SMBHs are likely to have expanded by accretion over cosmic time rather than merging events .These conclusions will be crucial constraints on estimates of SMBH growth and AGN feedback .",
        "rewrite_text": "We present the inaugural assessment of the supermassive black hole (SMBH) mass function across both late-type (elliptical/S0, Sa-Sb) and late-type (Scd-Sm) galaxies, utilizing data from the Millennium Galaxy Catalogue (MGC). Our study employs two distinct methodologies for determining SMBH masses: one based on stellar velocity dispersion measurements and the other on bulge luminosity scaling relations. The findings indicate that there is minimal variation in the SMBH mass parameters among these galaxy classifications at a redshift of h < 0.1. However, we observe significant evidence of evolutionary trends with redshift, revealing that the number density of more massive SMBHs declines at a faster rate compared to their less massive counterparts. This suggests that the most massive SMBHs have likely grown primarily through accretion processes over cosmic time, rather than through the merging of smaller black holes. These insights are pivotal for refining our understanding of SMBH growth mechanisms and the role of active galactic nucleus (AGN) feedback in galaxy evolution. Our results contribute to the broader discourse on the formation and development of supermassive black holes in the universe, providing essential constraints that will enhance future models of cosmic structure formation and the interplay between black holes and their host galaxies.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": 1.6502739940140694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mapping the circumstellar SiO maser emission in R Leo .\nAbstract:\nWe present new maps of the circumstellar SiO (v=1, v=2) masers around the Mira variable R Leo obtained with the Very Long Baseline Array at 22 GHz and 43 GHz. The observations were carried out on September 24th 2004 using all ten antennas available for VLBA operation during that time period. We detect two distinct groups of masers: one group is located near the star s position as determined by optical astrometry; another group lies about 0.5 arcsec to the south-west of this location. Both groups are associated with an extended bipolar structure seen in previous single-dish observations. This structure has been interpreted as a shell-like envelope surrounding the central star. Our results show that both groups of masers trace different parts of this shell-like structure. In addition we find evidence for a third component which may be related to the presence of a companion object. \n \n Keywords: Masers",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mapping the circumstellar SiO maser emission in R Leo . Abstract : We create fresh maps of the circumstellar SiO ( v = 1 , v = 2 ) masers around the Mira variable R Leo achieved with the Very Long Baseline Array at 22 GHz and 43 GHz .The observed were carried out on September 24th 2004 utilizing all ten antennas available for VLBA operation during that time period . We detect two different groups of masers : one group is situated near the star s position as determined by optical astrometry ; another group lies about 0 . 5 arcsec to the south - west of this location .Both groups are identified with an extended bipolar structure seen in earlier single - dish measurements . This structure has been described as a shell - like envelope surrounding the main star .Our results show that both bands of masers trace various parts of this shell - like structure . In addition we find proof for a third element which may be connected to the presence of a companion object .Keywords: Masers",
        "rewrite_text": "Title: Mapping the Circumstellar SiO Maser Emission in R Leo\n\nAbstract: In this study, we present new maps of the circumstellar SiO maser emissions in the Mira variable star R Leo, utilizing observations from the Very Long Baseline Array (VLBA) at frequencies of 22 GHz and 43 GHz. The observations were conducted on September 24, 2004, employing all ten antennas available for VLBA operations at that time. Our findings reveal two distinct groups of masers: one group is located close to the star's position as determined by optical astrometry, while the other is situated approximately 0.5 arcseconds to the southwest of this position. Both groups correspond to an extended bipolar structure previously identified in single-dish measurements, which has been characterized as a shell-like envelope surrounding the primary star. Our results indicate that the maser emissions in both frequency bands trace different regions of this shell-like structure, providing insight into the complex circumstellar environment of R Leo. Furthermore, we present evidence for a potential third component that may be associated with the presence of a companion object. This study enhances our understanding of the maser emissions in R Leo and contributes to the broader knowledge of stellar environments in variable stars. \n\nKeywords: Masers, R Leo, circumstellar environment, VLBA, Mira variable stars.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 2.182178902359924
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Circulating Hydrogen Ultra-High Purification System for the MuCap Experiment .\nAbstract:\nMuon-catalyzed fusion (MCF) is an exotic nuclear reaction that can be used to produce energy in future reactors, but it requires extremely pure hydrogen gas as fuel.  The MuCap experiment at TRIUMF has developed and tested a novel system for producing ultra-pure hydrogen using liquid helium cryogenic distillation followed by two stages of molecular sieves.  This system produces up to 1 liter per minute with less than 10 parts-per-trillion impurities.  It will provide enough clean hydrogen gas to operate the MuCap experiment until 2020 when the next generation of experiments are expected to begin taking data. A circulating hydrogen ultra-high purification system was designed and built for the MuCap experiment at TRI-UMF. Liquid helium cryogenic distillation is combined with two stages of molecular sieve beds to achieve high purity levels required for MCF research. The system provides up to one litre of purified hydrogen per minute with less than ten parts-per-trillion impurity content.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Circulating Hydrogen Ultra - High Purification System for the MuCap Experiment . Abstract : Muon - catalyzed fusion ( MCF ) is an exotic radioactive reaction that can be used to produce energy in future reactors , but it requires exceptionally pure hydrogen energy as fuel .The MuCap project at TRIUMF has constructed and demonstrated a new technology for producing ultra - pure hydrogen utilizing liquid helium cryogenic distillation preceded by two stages of molecular sieves . This system creates up to 1 liter per moment with fewer than 10 parts - per - trillion impurities .It will provide enough clean hydrogen energy to run the MuCap research until 2020 when the new generation of studies are expected to starting took results . A circulating hydrogen ultra - high purification system was developed and developed for the MuCap project at TRI - UMF .Liquid helium cryogenic distillation is combined with two stages of molecular sieve beds to achieve high purity levels needed for MCF research . The system provides up to one litre of purified fuel per second with fewer than ten components - per - trillion impurity content .",
        "rewrite_text": "**Title:** A Circulating Hydrogen Ultra-High Purification System for the MuCap Experiment\n\n**Abstract:** Muon-catalyzed fusion (MCF) represents a novel and intriguing approach to energy production, leveraging exotic radioactive reactions that hold promise for future energy reactors. A critical requirement for MCF is the utilization of exceptionally pure hydrogen as fuel. In response to this need, the MuCap project at TRIUMF has successfully developed and implemented an innovative technology designed to produce ultra-pure hydrogen. This system employs a combination of liquid helium cryogenic distillation and two stages of molecular sieves, effectively achieving the high purity levels necessary for MCF research. The purification process is capable of generating up to one liter of hydrogen per second, with an impurity concentration of fewer than ten parts per trillion. This remarkable level of purity is essential for the ongoing MuCap research, which is expected to continue until 2020, coinciding with the anticipated commencement of a new generation of studies aimed at advancing our understanding of muon-catalyzed fusion. The development of this circulating hydrogen ultra-high purification system marks a significant milestone for the MuCap project, ensuring a reliable supply of clean hydrogen fuel that meets the stringent requirements of MCF experiments. The integration of cryogenic distillation and molecular sieve technology not only enhances the efficiency of hydrogen purification but also sets a new standard for future research endeavors in the field. As the MuCap project progresses, this innovative system will play a pivotal role in exploring the potential of muon-catalyzed fusion as a viable energy source, contributing to the broader goal of sustainable energy solutions.",
        "ori-fast-z-score": -0.8728715609439696,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 0.3563483225498992
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  GRI: The Gamma-Ray Imager mission .\nAbstract:\nThe GRI (Gamma Ray Imager) is an astrophysics space observatory proposed by the French Space Agency CNES and NASA to study gamma-ray bursts, active galactic nuclei and other phenomena in high-energy astronomy.  It will be launched on a Soyuz rocket with a Fregat upper stage into a Sun-Earth L2 orbit at 1 AU distance from Earth. Its main instrument consists of two coded mask telescopes observing simultaneously in the energy range 20 MeV - 300 GeV. Each telescope has a large field-of-view of 2 steradians and a spatial resolution better than 0.1 degrees. A third detector module provides additional information about the background radiation environment for each telescope. This article describes the design concept of this innovative instrument. Keywords: Astroparticle physics, High-Energy Astronomy, Instrumentation, Space Observatory. Introduction:  The GRI (Gamma Ray...  more",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : GRI : The Gamma - Ray Imager mission . Abstract : The GRI ( Gamma Ray Imager ) is an astrophysics space observatory proposed by the French Space Agency CNES and NASA to study alpha - ray bursts , active galactic nuclei and other processes in high - energy astronomy .It will be launched on a Soyuz rocket with a Fregat upper stage into a Sun - Earth L2 orbit at 1 AU distance from Earth . Its primary instrument consists of two coded mask telescopes observing simultaneously in the power range 20 MeV - 300 GeV .Each observatory has a large field - of - view of 2 steradians and a spatial resolution good than 0 . 1 degrees . A third detector module supplies additional information about the background radiation conditions for each telescope .This page describes the design concept of this innovative instrument . Keywords : Astroparticle science , High - Energy Astronomy , Instrumentation , Space Observatory .Introduction:  The GRI (Gamma Ray...  more",
        "rewrite_text": "**Title:** GRI: The Gamma-Ray Imager Mission\n\n**Abstract:** The Gamma-Ray Imager (GRI) is an advanced astrophysical space observatory developed through a collaboration between the French Space Agency CNES and NASA, aimed at enhancing our understanding of high-energy astronomical phenomena such as gamma-ray bursts and active galactic nuclei. Scheduled for launch aboard a Soyuz rocket equipped with a Fregat upper stage, GRI will be positioned in a Sun-Earth L2 orbit, approximately 1 AU from Earth. The mission's core instrument comprises two coded mask telescopes that will operate concurrently across a broad energy spectrum, specifically from 20 MeV to 300 GeV. Each telescope features an expansive field of view of 2 steradians and boasts a spatial resolution exceeding 0.1 degrees, enabling precise observations of cosmic events. Additionally, a third detector module is integrated into the system to provide crucial data regarding the background radiation environment affecting the telescopes. This document outlines the innovative design and operational framework of the GRI, highlighting its potential contributions to astroparticle science and high-energy astronomy. By facilitating detailed investigations into the mechanisms driving some of the universe's most energetic processes, GRI is poised to significantly advance our knowledge in these fields. \n\n**Keywords:** Astroparticle science, High-Energy Astronomy, Instrumentation, Space Observatory.",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": 0.10369516947304253
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetism and Thermodynamics of Spin-1/2 Heisenberg Diamond Chains in a Magnetic Field .\nAbstract:\nWe study the magnetization process, thermodynamic properties as well as magnetic susceptibility for spin-1/2 diamond chains with nearest-neighbor interactions under an external magnetic field using exact diagonalization method. We find that there is no spontaneous magnetization at zero temperature but the system exhibits finite magnetization when it is heated up to certain critical temperatures. The results show that the ground state has antiferromagnetic order while the excited states have ferrimagnetic order. In addition, we also investigate how the external magnetic field affects the magnetization process by calculating the magnetization curve M(H) at different temperatures T = 0.1J, 1.0J, 2.0J, 3.0J, 4.0J (J being the exchange interaction). It turns out that the magnetization increases rapidly with increasing H until reaching saturation value Msat at high fields. Moreover, our numerical calculations reveal that the Curie-Weiss constant decreases gradually with increasing J and finally vanishes at large values of J.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetism and Thermodynamics of Spin - 1 / 2 Heisenberg Diamond Chains in a Magnetic Field . Abstract : We research the magnetization process , thermodynamic properties as well as magnetic susceptibility for spin - 1 / 2 diamond chains with nearest - neighbor interactions under an external magnetic force using accurate diagonalization technique .We see that there is no induced magnetization at zero temperature but the system displays finite magnetization when it is hot up to specified significant conditions . The results show that the ground state has antiferromagnetic order while the excited states have ferrimagnetic order .In addition , we also investigate how the external magnetic field affects the magnetization process by calculating the magnetization pattern M ( H ) at different conditions T = 0 . 1J , 1 . 0J , 2 . 0J , 3 . 0J , 4 . 0J ( J being the transfer coupling ) . It turns out that the magnetization increases quickly with rising H until reaching saturation value Msat at high fields .Moreover , our numerical measurements reveal that the Curie - Weiss constant drops gradually with expanding J and eventually vanishes at large values of J .",
        "rewrite_text": "In this study, we investigate the magnetization behavior, thermodynamic characteristics, and magnetic susceptibility of spin-1/2 Heisenberg diamond chains subjected to an external magnetic field, employing precise diagonalization techniques. Our findings indicate that at absolute zero temperature, the system exhibits no induced magnetization. However, as the temperature increases, the system demonstrates finite magnetization under specific conditions. Notably, we observe that the ground state of the system is characterized by antiferromagnetic order, while the excited states exhibit ferrimagnetic order. \n\nFurthermore, we explore the influence of the external magnetic field on the magnetization process by analyzing the magnetization curve M(H) across various temperatures, specifically T = 0.1J, 1.0J, 2.0J, 3.0J, and 4.0J, where J represents the transfer coupling constant. Our results reveal a rapid increase in magnetization with rising magnetic field strength H, ultimately reaching a saturation value, Msat, at elevated fields. Additionally, our numerical analysis uncovers a gradual decline in the Curie-Weiss constant as the transfer coupling J increases, with the constant eventually approaching zero at large values of J. This research contributes to a deeper understanding of the magnetic properties and thermodynamic behavior of spin-1/2 Heisenberg diamond chains, highlighting the complex interplay between temperature, magnetic field, and the underlying spin interactions.",
        "ori-fast-z-score": 0.11547005383792514,
        "water-fast-z-score": 5.128225940683707,
        "rewrite-fast-z-score": 1.8
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Matrix Exponential-Based Closures for the Turbulent Subgrid-Scale Stress Tensor .\nAbstract:\nThe subgrid-scale stress tensor is an important quantity in large-eddy simulation (LES) that describes the effect of unresolved turbulent motions on resolved scales.  In this work, we present new closures based on matrix exponential functions to model the anisotropic part of the subgrid-scale stress tensor and its associated transport coefficients. The proposed closure models are derived by assuming statistical homogeneity and isotropy at small length scales within each computational cell. We show that these closures can be expressed as simple algebraic expressions involving only second-order statistics of the resolved velocity field. These closures are tested against direct numerical simulations of homogeneous shear flows with different Reynolds numbers ranging between Re = 100 and 1000. It is found that our proposed closures perform better than existing eddy-viscosity-based closures when compared using normalized mean-square errors. Finally, it should be noted that the proposed closures have been implemented into the open-source LES code Nektar++.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Matrix Exponential - Based Closures for the Turbulent Subgrid - Scale Stress Tensor . Abstract : The subgrid - scale stress tensor is an important quantity in large - eddy simulation ( LES ) that describes the impact of unresolved turbulent movements on resolved scales .In this research , we present new closures based on matrix exponential functions to model the anisotropic part of the subgrid - scale stress tensor and its associated transport coefficients . The proposed closure models are derived by assuming statistical homogeneity and isotropy at small length scales within each theoretical cell .We see that these closures can be described as simple algebraic definitions involving only second - order statistics of the resolved speed field . These closures are tested against direct numerical simulations of homogeneous shear flows with various Reynolds numbers ranging between Re = 100 and 1000 .It is found that our proposed closures behave good than existing eddy - viscosity - based closures when compared using normalized mean - square errors . Finally , it should be mentioned that the suggested closures have been built into the open - source LES code Nektar + + .",
        "rewrite_text": "**Title:** Matrix Exponential-Based Closures for the Turbulent Subgrid-Scale Stress Tensor\n\n**Abstract:** The subgrid-scale stress tensor plays a crucial role in large-eddy simulation (LES) as it quantifies the effects of unresolved turbulent motions on the resolved scales. This study introduces innovative closure models grounded in matrix exponential functions to effectively represent the anisotropic component of the subgrid-scale stress tensor and its corresponding transport coefficients. The development of these closure models is predicated on the assumptions of statistical homogeneity and isotropy at small length scales within each theoretical cell. Notably, these closures can be articulated as straightforward algebraic expressions that rely solely on second-order statistics derived from the resolved velocity field. To evaluate the performance of the proposed closures, we conducted tests against direct numerical simulations of homogeneous shear flows across a range of Reynolds numbers, specifically from Re = 100 to 1000. The results indicate that our closures outperform traditional eddy-viscosity-based models, as evidenced by a comparison using normalized mean-square errors. Furthermore, it is important to highlight that the proposed closures have been integrated into the open-source LES framework, Nektar++. This advancement not only enhances the accuracy of turbulence modeling in LES but also provides a robust tool for researchers and practitioners in the field of computational fluid dynamics. The findings underscore the potential of matrix exponential-based approaches in improving the representation of turbulent subgrid-scale phenomena, paving the way for more reliable simulations in complex turbulent flows.",
        "ori-fast-z-score": 0.7592566023652966,
        "water-fast-z-score": 4.013213469645139,
        "rewrite-fast-z-score": 0.7492686492653552
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vibrational Infrared Lifetime of the Anesthetic nitrous oxide gas in solution .\nAbstract:\nThe vibrational infrared lifetime (VIL) of N2O dissolved in water is measured by using an optical parametric oscillator pumped with a Nd3+-doped YAG laser at 1064 nm and detecting the emission signal at 1270 nm, which corresponds to the fundamental vibration mode of N2O. The VIL value obtained for pure water is 1.6 ± 0.1 μs. This result agrees well with that reported previously. \n \n For solutions containing various concentrations of NaCl or KCl, the VIL values are found to be independent of salt concentration within experimental error. These results suggest that the vibrational relaxation process of N2O molecules in aqueous solution does not involve any specific interaction between N2O and ions such as Cl-. It should also be noted that the present measurement was performed under conditions where the solute-solvent interactions were negligible compared to those observed in concentrated solutions. Therefore, it can be concluded that the vibrational relaxation mechanism of N2O in dilute solutions is essentially identical to that in pure water.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vibrational Infrared Lifetime of the Anesthetic nitrous oxide gas in solution . Abstract : The vibrational microwave lifetime ( VIL ) of N2O dissolved in water is measured by using an optical parametric oscillator pumped with a Nd3 + - doped YAG laser at 1064 nm and detecting the emission signal at 1270 nm , which belongs to the fundamental vibration mode of N2O .The VIL yield obtained for pure water is 1 . 6 ± 0 . 1 μs . This result agrees well with that described earlier .For solutions handling many quantities of NaCl or KCl , the VIL values are found to be independent of salt concentration within experimental error . These conclusions show that the vibrational relaxation process of N2O compounds in aqueous solution does not involve any specific interaction between N2O and atoms such as Cl - .It should also be mentioned that the present observation was done under environments where the solute - solvent interactions were negligible compared to those observed in concentrated solutions . Therefore , it can be determined that the vibrational relaxation mechanism of N2O in dilute solutions is essentially identical to that in pure water .",
        "rewrite_text": "Title: Vibrational Infrared Lifetime of the Anesthetic Nitrous Oxide Gas in Solution\n\nAbstract: This study investigates the vibrational microwave lifetime (VIL) of nitrous oxide (N2O) when dissolved in water, utilizing an optical parametric oscillator that is pumped by a Nd3+-doped YAG laser operating at a wavelength of 1064 nm. The emission signal, detected at 1270 nm, corresponds to the fundamental vibrational mode of N2O. The measured VIL for pure water is found to be 1.6 ± 0.1 μs, a result that is consistent with previous findings in the literature. Furthermore, experiments conducted with solutions containing varying concentrations of sodium chloride (NaCl) and potassium chloride (KCl) reveal that the VIL values remain constant, showing no significant dependence on the salt concentration within the limits of experimental error. These observations suggest that the vibrational relaxation dynamics of N2O in aqueous solutions do not involve specific interactions with chloride ions. It is also important to note that the measurements were performed in conditions where solute-solvent interactions were minimal, particularly when compared to those in more concentrated solutions. Consequently, it can be concluded that the vibrational relaxation mechanism of N2O in dilute aqueous solutions closely resembles that observed in pure water. This research contributes to a deeper understanding of the vibrational properties of anesthetic gases in solution, which is crucial for both theoretical studies and practical applications in anesthesiology.",
        "ori-fast-z-score": 0.9058216273156765,
        "water-fast-z-score": 4.529108136578382,
        "rewrite-fast-z-score": 1.6329931618554523
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A simple spontaneously active Hebbian learning model: homeostasis of activity and connectivity, and consequences for learning and epileptogenesis .\nAbstract:\nWe propose a model that describes the spontaneous formation of neuronal assemblies in terms of an unsupervised Hebbian learning rule with synaptic depression. The network is composed by N neurons connected to each other through excitatory connections whose strength depends on their firing rate according to a depressing function. We show how this mechanism leads to the emergence of stable states characterized by different levels of average activity and number of synchronized clusters. In particular we find that there exists a critical value of the connection probability above which the system undergoes a phase transition towards a state where all neurons are synchronously firing at high rates. This regime corresponds to what has been observed experimentally as ictal seizures. \n \n Moreover, we study the effect of external stimulation on the dynamics of our network. By applying brief electrical pulses to specific groups of neurons we can induce transitions between different dynamical regimes. Finally, we discuss possible applications of these results to neurobiology.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A straightforward spontaneously independent Hebbian learning model : homeostasis of action and connectivity , and effects for learning and epileptogenesis . Abstract : We suggest a theory that describes the spontaneous development of neuronal assemblies in terms of an unsupervised Hebbian learning rule with synaptic depression .The network is composed by N cells connected to each other through excitatory connections whose strength depends on their fired rate due to a depressing function . We see how this mechanism leads to the emergence of stable states characterized by various rates of average activity and number of synchronized clusters .In particular we find that there exists a critical quantity of the connection probability above which the system undergoes a phase shift towards a state where all neurons are synchronously firing at high levels . This regime corresponds to what has been observed experimentally as ictal seizures .Moreover , we study the impact of external stimulation on the dynamics of our system . By applying short electrical pulses to individual groups of neurons we can induce interactions between various dynamical regimes .Finally , we discuss possible applied of these results to neurobiology .",
        "rewrite_text": "Title: A Simple Spontaneously Independent Hebbian Learning Model: Homeostasis of Action and Connectivity, and Implications for Learning and Epileptogenesis\n\nAbstract: In this study, we propose a theoretical framework that elucidates the spontaneous formation of neuronal assemblies through an unsupervised Hebbian learning mechanism characterized by synaptic depression. The model comprises N interconnected neurons linked by excitatory synapses, with the synaptic strength modulated by the firing rates of the neurons, governed by a depressing function. Our findings reveal that this learning mechanism facilitates the emergence of stable states, which are defined by varying levels of average neuronal activity and the presence of synchronized clusters. Notably, we identify a critical threshold for connection probability, beyond which the system transitions into a state of synchronous high-frequency firing across all neurons. This phenomenon aligns with experimentally observed ictal seizures, highlighting the relevance of our model to understanding seizure dynamics. Furthermore, we investigate the effects of external stimulation on the system's behavior. By delivering brief electrical pulses to specific groups of neurons, we can manipulate the interactions between different dynamical states, providing insights into the modulation of neuronal activity. Finally, we explore the potential applications of our findings in the field of neurobiology, particularly in relation to learning processes and the mechanisms underlying epileptogenesis. This work contributes to a deeper understanding of the interplay between connectivity, neuronal activity, and the homeostatic regulation of brain function, offering a foundation for future research in both theoretical and applied neuroscience.",
        "ori-fast-z-score": 0.3216337604513384,
        "water-fast-z-score": 5.692099788303083,
        "rewrite-fast-z-score": 1.1766968108291043
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Star Formation, Radio Sources, Cooling X-ray Gas, and Galaxy Interactions in the Brightest Cluster Galaxy in 2A0335+096 .\nAbstract:\nWe present new Chandra observations of the brightest cluster galaxy (BCG) in Abell 3395 (z=0.084). The BCG is surrounded by an extended halo with temperatures ranging between 1 keV to 5 keV. We find that this hot gas has been displaced from its original location around the central galaxy due to interactions with other galaxies within the cluster core. In addition we detect two radio sources associated with the BCG which are likely to be AGN jets or lobes. Finally, we identify several regions where cold gas may have condensed out of the surrounding hot plasma. These results suggest that the BCG in Abell 3395 is undergoing significant interaction with its environment. This work was supported under NASA Contract NAS8-39073 issued through JPL/Caltech. The data presented herein were obtained at the Chandra Observatory, operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8-03060.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Star Formation , Radio Sources , Cooling X - ray Gas , and Galaxy Interactions in the Brightest Cluster Galaxy in 2A0335 + 096 . Abstract : We present new Chandra observations of the brightest cluster galaxy ( BCG ) in Abell 3395 ( z = 0 . 084 ) .The BCG is enclosed by an extended halo with temperatures ranging between 1 keV to 5 keV . We see that this hot gas has been displaced from its previous site around the main galaxy owing to interactions with other stars within the cluster core .In addition we locate two radio sources involved with the BCG which are likely to be AGN planes or lobes . Finally , we identify several regions where cold gas may have condensed out of the nearby heated plasma .These data suggest that the BCG in Abell 3395 is undergoing substantial interaction with its surroundings . This project was supported under NASA Contract NAS8 - 39073 issued through JPL / Caltech .The data given herein were obtained at the Chandra Observatory , operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8 - 03060 .",
        "rewrite_text": "We present new observations from the Chandra X-ray Observatory focusing on the brightest cluster galaxy (BCG) within the Abell 3395 cluster, located at a redshift of z = 0.084. Our findings reveal that the BCG is surrounded by an extensive halo of hot gas, with temperatures varying between 1 keV and 5 keV. Notably, we have observed that this hot gas has been displaced from its original position around the BCG due to interactions with other stellar bodies in the cluster core. This displacement indicates significant dynamical activity within the cluster environment. Furthermore, we have identified two radio sources associated with the BCG, which are likely to be active galactic nucleus (AGN) jets or lobes, suggesting ongoing energetic processes in this region. Additionally, our analysis points to several areas where cold gas may have formed from the surrounding heated plasma, indicating a complex interplay between different gas phases. These observations collectively suggest that the BCG in Abell 3395 is experiencing considerable interactions with its neighboring structures, which may have implications for star formation and the evolution of the galaxy. This research was conducted under NASA Contract NAS8-39073, facilitated through JPL/Caltech, and the data were acquired at the Chandra Observatory, which is operated by the Smithsonian Astrophysical Observatory on behalf of NASA under contract NAS8-03060. Our study contributes to the understanding of the physical processes governing galaxy interactions and the role of hot and cold gas in the evolution of cluster galaxies.",
        "ori-fast-z-score": -0.1259881576697424,
        "water-fast-z-score": 4.6615618337804685,
        "rewrite-fast-z-score": -0.29559878344928797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Inferring dynamic genetic networks with low order independencies .\nAbstract:\nWe present an algorithm for inferring the structure and parameters of a dynamic Bayesian network (DBN) using only first-order or second-order conditional independence tests, which are computationally much cheaper than higher-order tests. The DBN is inferred by applying our method to each time point separately in a sliding window fashion. We show that this approach can be used to accurately reconstruct gene regulatory networks from high-throughput data such as microarray experiments. Our results on synthetic datasets demonstrate that we achieve similar accuracy compared to state-of-the-art methods while being orders of magnitude faster. Finally, we apply our method to two real-world biological datasets and compare it against several other approaches. Dynamic Bayesian Networks (DBNs) have been successfully applied to model complex systems whose states evolve over time. However, learning these models has proven challenging due to their large number of parameters and the computational complexity involved in estimating them. In particular, exact inference algorithms scale exponentially with respect to the size of the dataset and the number of variables considered. To overcome this problem, many approximate inference techniques have been proposed recently. These include variational approximations  1  , expectation propagation  2  , Monte Carlo sampling  3  , and message passing  4  . While these methods allow us to learn more accurate models at the cost of increased running times, they still require expensive computations involving all possible interactions between pairs of nodes. This makes them unsuitable when dealing with large-scale problems where the number of observations and/or variables becomes too large.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Inferring dynamic genetic systems with lowest order independencies . Abstract : We present an algorithm for inferring the composition and parameters of a dynamic Bayesian network ( DBN ) using only first - order or second - order conditional autonomy tests , which are computationally far inexpensive than higher - order tests .The DBN is inferred by using our technique to each time point separately in a sliding window fashion . We see that this methodology can be used to correctly reconstruct gene regulatory networks from high - throughput data such as microarray experiments .Our results on synthetic datasets prove that we accomplish similar accuracy relative to state - of - the - art methods while being orders of magnitude quicker . Finally , we apply our technique to two actual - time bio datasets and compare it against several other methods .Dynamic Bayesian Networks ( DBNs ) have been successfully utilized to model complex systems whose states evolve over time . However , studying these models has proven challenging due to their large number of constraints and the numerical difficulty required in estimating them .In particular , exact inference schemes scale exponentially with regard to the height of the dataset and the number of variables treated . To solve this challenge , various approximate inference strategies have been proposed lately .These include variational approximations 1 , expectation propagation 2 , Monte Carlo scanning 3 , and message passing 4 . While these algorithms provide us to study more accurate models at the cost of enhanced running times , they still use sophisticated computations requiring all possible behaviors between pairs of nodes .This creates them unsuitable when dealing with large - scale problems where the quantity of measurements and / or variables makes too huge .",
        "rewrite_text": "We introduce a novel algorithm designed for inferring the structure and parameters of dynamic Bayesian networks (DBNs) by utilizing only first-order or second-order conditional independence tests. These tests are significantly less computationally intensive compared to higher-order tests. Our approach involves applying this methodology to each time point in a sliding window manner, allowing for effective reconstruction of gene regulatory networks from high-throughput data, such as that obtained from microarray experiments. Through experiments conducted on synthetic datasets, we demonstrate that our algorithm achieves comparable accuracy to leading methods while operating at a fraction of the computational cost. Furthermore, we validate our technique on two real-time biological datasets and benchmark its performance against several existing methods.\n\nDynamic Bayesian Networks have emerged as powerful tools for modeling complex systems with temporal dynamics. However, the analysis of these models presents significant challenges due to the extensive constraints involved and the computational difficulties associated with parameter estimation. Specifically, traditional exact inference methods exhibit exponential scaling with respect to the dataset's dimensionality and the number of variables involved. To address these issues, recent advancements have introduced various approximate inference techniques, including variational approximations, expectation propagation, Monte Carlo methods, and message passing. While these approaches enable the exploration of more sophisticated models, they often require complex computations that account for all potential interactions between node pairs. This complexity renders them impractical for large-scale problems, where the volume of measurements and variables can be overwhelming. Our proposed algorithm seeks to mitigate these challenges, providing a more efficient and accessible means of inferring dynamic genetic systems.",
        "ori-fast-z-score": -1.6903085094570331,
        "water-fast-z-score": 7.268326590665242,
        "rewrite-fast-z-score": 0.2544566789039913
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Advection-dominated accretion flows with causal viscosity .\nAbstract:\nWe study the dynamics and emission properties of advection dominated accretion flows (ADAFs) in which viscosity is caused by magnetic reconnections between field lines anchored to differentially rotating black holes. We find that, for reasonable values of parameters, such ADAFs can produce luminosities as high as those observed in quasars without violating any observational constraints on their mass inflow rates or temperatures at large radii. The main reason why our model works well is because it naturally produces an outflowing wind component whose kinetic energy flux greatly exceeds its thermal energy flux. This wind carries away most of the angular momentum so that the flow becomes nearly Keplerian near the black hole horizon. In addition, we show that this wind also provides enough pressure support against gravity to prevent the gas density from becoming too low there. Our results suggest that the winds produced by magnetized ADAF models may be responsible for driving powerful radio jets in active galactic nuclei.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Advection - dominated accretion flows with causal viscosity . Abstract : We research the dynamics and emission behavior of advection dominated accretion flows ( ADAFs ) in which viscosity is caused by magnetic reconnections between field lines anchored to differentially rotating black holes .We see that , for reasonable values of parameters , such ADAFs can generate luminosities as great as those observed in quasars without violating any observational restrictions on their mass inflow rates or temperatures at large radii . The main explanation why our model works well is because it naturally produces an outflowing wind component whose kinetic power flux considerably exceeds its thermal energy flux .This wind carries away most of the angular velocity so that the flow turns almost Keplerian near the dark hole horizon . In addition , we find that this wind additionally offers enough pressure support against gravity to keep the gas density from getting too low there .Our results propose that the winds released by magnetized ADAF machines could be responsible for driving strong radio jets in active galactic nuclei .",
        "rewrite_text": "In this study, we investigate the dynamics and emission characteristics of advection-dominated accretion flows (ADAFs) where viscosity arises from magnetic reconnections between field lines associated with differentially rotating black holes. Our findings indicate that, under plausible parameter values, these ADAFs can produce luminosities comparable to those observed in quasars, while remaining consistent with observational constraints regarding mass inflow rates and temperatures at larger radii. A key aspect of our model's success lies in its ability to generate an outflowing wind component, which possesses a kinetic power flux that significantly surpasses its thermal energy flux. This wind effectively transports away a substantial portion of the angular momentum, allowing the flow to approach a nearly Keplerian state in proximity to the black hole's event horizon. Furthermore, we demonstrate that this outflow provides sufficient pressure support against gravitational collapse, preventing the gas density from diminishing excessively in that region. Our results suggest that the winds produced by magnetized ADAF systems may play a crucial role in driving powerful radio jets observed in active galactic nuclei. This research not only enhances our understanding of the mechanisms behind ADAFs but also offers insights into the broader implications for astrophysical phenomena associated with black holes and their surrounding environments.",
        "ori-fast-z-score": -0.10482848367219183,
        "water-fast-z-score": 4.587317109255645,
        "rewrite-fast-z-score": 0.8700628401410971
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Continuous Magnetophoretic Separation of Blood Cells from Plasma at the Microscale .\nAbstract:\nMagnetic separation is an important tool in biomedical research and clinical diagnostics, but it has been limited to macroscopic devices that are not suitable for point-of-care applications. Here we report on continuous magnetophoresis-based blood cell sorting using microfluidics. We demonstrate efficient separation of red blood cells (RBCs) from plasma by applying a magnetic field gradient across a microchannel containing RBCs suspended in buffer solution. The results show that our method can be used as a simple yet effective approach for separating different types of blood cells with high purity and efficiency. This work may have significant implications towards developing portable diagnostic tools based on microscale blood processing technologies. Magnetic separation techniques play an important role in many fields including medicine, biotechnology, environmental science, food industry etc.,  1  . However, most existing methods require bulky equipment which makes them unsuitable for use outside laboratory settings  2  .\nRecently there has been growing interest in miniaturizing these systems into lab-on-a-chip platforms  3  , where various functionalities such as sample preparation  4  , chemical analysis  5  , drug delivery  6  , and bioassays  7  could be integrated onto one single chip. In particular, magnetic separators have attracted much attention due to their simplicity, low cost, portability, and compatibility with other microfabricated components  8  . For example, several groups have demonstrated magnetic separation of biological samples inside microchannels  9  -  11  or on planar surfaces  12  -  14  . Despite this progress, however, current approaches still suffer from some limitations. First, they typically rely on batch-wise operation mode  15  , which limits throughput and requires large volumes of input samples  16  . Second, the majority of reported designs only allow for separation between two distinct populations  17  , while more complex mixtures involving multiple species cannot be processed simultaneously  18  . Third, the fabrication process usually involves complicated multi-step procedures  19  , making it difficult to integrate additional functions  20  . Finally, most previous studies were performed under static conditions  21  , which limit the flexibility of device design  22  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Continuous Magnetophoretic Separation of Blood Cells from Plasma at the Microscale . Abstract : Magnetic isolation is an important tool in biomedical research and medical diagnostics , but it has been limited to macroscopic devices that are not suitable for point - of - care applications .Here we study on intensive magnetophoresis - based blood cell sorting using microfluidics . We suggest efficient separation of red blood cells ( RBCs ) from blood by using a magnetic field gradient across a microchannel containing RBCs held in buffer solution .The results show that our technique can be used as a simple however effective methods for dividing different kinds of blood tissue with high purity and efficiency . This research could have considerable consequences towards developing portable diagnostic methods using on microscale blood extraction technologies .Magnetic isolation techniques serve an important role in multiple fields including medicine , biotechnology , ecological studies , nutrition industry etc . , 1 . However , most existing techniques require bulky machinery which makes them unsuitable for use outside laboratory settings 2 .Recently there has been growing interest in miniaturizing these systems into lab - on - a - chip platforms 3 , where various functionalities such as sample preparation 4 , chemical analysis 5 , drug delivery 6 , and bioassays 7 could be integrated onto one single chip . In particular , magnetic separators have attracted much attention due to their simplicity , low cost , portability , and compatibility with other microfabricated components 8 .For instance , various groups have demonstrated magnetic separation of biological samples inside microchannels 9 - 11 or on planar materials 12 - 14 . Despite this progress , however , current approaches still suffer from some restrictions .First , they generally rely on batch - wise operation mode 15 , which reduces throughput and requires large quantities of input specimens 16 . Second , the majority of reported prototypes only require for isolation between two separate populations 17 , while more sophisticated mixtures featuring multiple taxa unable be processed concurrently 18 .Third , the fabrication process usually includes complicated multi - phase techniques 19 , making it difficult to connect extra functions 20 . Finally , most prior studies were performed under static conditions 21 , which reduce the flexibility of device structure 22 .",
        "rewrite_text": "**Title:** Continuous Magnetophoretic Separation of Blood Cells from Plasma at the Microscale\n\n**Abstract:** The magnetic isolation of biological samples is a crucial technique in biomedical research and clinical diagnostics; however, traditional methods have largely relied on macroscopic devices that are impractical for point-of-care applications. In this study, we explore an innovative approach to magnetophoresis-based blood cell sorting utilizing microfluidic technology. Our method involves the application of a magnetic field gradient within a microchannel that contains red blood cells (RBCs) suspended in a buffer solution, enabling efficient separation from plasma. The findings indicate that our technique not only achieves high purity and efficiency in isolating various blood components but also presents a straightforward solution for differentiating blood tissues. This advancement holds significant potential for the development of portable diagnostic tools that leverage microscale blood extraction technologies.\n\nMagnetic isolation techniques are integral across diverse fields, including medicine, biotechnology, ecological research, and the nutrition industry. However, many existing methods necessitate large, cumbersome equipment, limiting their applicability outside laboratory environments. Recent trends have focused on miniaturizing these systems into lab-on-a-chip platforms, which can integrate multiple functionalities such as sample preparation, chemical analysis, drug delivery, and bioassays onto a single chip. Magnetic separators, in particular, have garnered attention due to their simplicity, cost-effectiveness, portability, and compatibility with other microfabricated components.\n\nDespite advancements in this area, current methodologies face several challenges. Most existing systems operate in a batch-wise manner, which constrains throughput and necessitates substantial input volumes. Additionally, many prototypes are designed to isolate only two distinct populations, making it difficult to process more complex mixtures with multiple cell types simultaneously. The fabrication processes often involve intricate multi-phase techniques, complicating the integration of additional functionalities. Furthermore, prior studies have predominantly been conducted under static conditions, limiting the adaptability of device designs. Our research addresses these limitations, paving the way for more versatile and efficient blood cell separation methods suitable for real-world applications.",
        "ori-fast-z-score": 0.5146502354656654,
        "water-fast-z-score": 8.994650816763707,
        "rewrite-fast-z-score": 1.049344364594206
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Stellar Population And Origin Of The Mysterious High-Latitude Star Forming Cloud CG 12 .\nAbstract:\nWe present new near-infrared (NIR) photometry and spectroscopy for the high-latitude star forming cloud, CG12. We find that this cloud is composed of two distinct populations; one population with ages <10 Myr and another older population with ages >100 Myr. Using N-body simulations we show that these clouds are likely to be remnants of tidally disrupted open clusters. This suggests that open cluster disruption may play an important role in the formation of isolated young stars at high latitudes. These results have implications on our understanding of how open clusters evolve into open clusters and open clusters into open clusters. Open clusters can also provide insight into the origin of open clusters themselves. In addition, open clusters can help us understand the evolution of open clusters as well as open clusters. Finally, open clusters can give us information about open clusters and open clusters. We use new infrared data obtained by the Wide Field Camera 3 (WFC3), Hubble Space Telescope (HST), and Spitzer Space Telescope (SST).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Stellar Population And Origin Of The Mysterious High - Latitude Star Forming Cloud CG 12 . Abstract : We bring latest near - infrared ( NIR ) photometry and spectroscopy for the high - latitude star producing cloud , CG12 .We see that this cluster is composed of two separate populations ; one community with ages < 10 Myr and another older population with ages > 100 Myr . Using N - bodies simulations we find that these clouds are likely to be remnants of tidally disrupted open nuclei .This implies that open cluster disturbance may play an important role in the formation of isolated early stars at high latitudes . These data have consequences on our understanding of how open complexes develop into open clusters and open clusters into open clusters .Open clusters can also bring knowledge into the origin of open clusters themselves . In addition , open clusters can help us explain the evolution of open clusters as well as open clusters .Finally , open clusters can provide us information about open clusters and open clusters . We use new infrared results collected by the Wide Field Camera 3 ( WFC3 ) , Hubble Space Telescope ( HST ) , and Spitzer Space Telescope ( SST ) .",
        "rewrite_text": "**Title:** The Stellar Population and Origin of the Mysterious High-Latitude Star-Forming Cloud CG 12\n\n**Abstract:** In this study, we present the latest near-infrared (NIR) photometric and spectroscopic observations of the high-latitude star-forming cloud CG 12. Our analysis reveals that this region contains two distinct stellar populations: a younger group with ages less than 10 million years and an older population exceeding 100 million years. To further investigate the dynamics of these populations, we employed N-body simulations, which suggest that CG 12 may be the remnants of tidally disrupted open clusters. This finding indicates that disturbances within open clusters could significantly influence the formation of isolated early stars in high-latitude regions. \n\nThe implications of our results extend to the broader understanding of the evolutionary pathways of open clusters. Specifically, they shed light on how open complexes transition into open clusters and how these clusters evolve over time. Furthermore, our research contributes to the ongoing discourse regarding the origins of open clusters themselves. By utilizing new infrared data obtained from the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST) and the Spitzer Space Telescope (SST), we enhance our comprehension of the stellar populations within CG 12. \n\nUltimately, our findings not only provide insights into the characteristics and formation mechanisms of CG 12 but also offer a framework for understanding the evolution of open clusters in general. This research underscores the importance of high-latitude star-forming regions in the context of stellar evolution and cluster dynamics, paving the way for future investigations into the complex relationships between stellar populations and their environments.",
        "ori-fast-z-score": -1.4757295747452437,
        "water-fast-z-score": 3.5447450389702713,
        "rewrite-fast-z-score": 0.5432144762551112
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Alignment and signed-intensity anomalies in WMAP data .\nAbstract:\nWe present evidence for alignment between the quadrupole, octopole and hexadecapole moments of the cosmic microwave background (CMB) temperature anisotropy on large angular scales as measured by the Wilkinson Microwave Anisotropy Probe (WMAP). We find that this alignment is statistically significant at greater than 5 sigma confidence level when compared to simulated random realizations of CMB maps with identical power spectrum but different phases. \n \n The observed alignment can be explained if there are large-scale magnetic fields aligned along the line-of-sight which induce circular polarization in the primordial plasma prior to recombination. This effect would lead to an additional contribution to the Stokes Q parameter of the CMB radiation. In addition we show that the observed anomalous behaviour of the quadrupole moment may also be caused by such effects. Finally, we discuss possible implications of our results for inflationary models. We argue that these observations do not rule out slow-roll single-field inflation scenarios, however they provide new constraints on their parameters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Alignment and signed - intensity anomalies in WMAP information . Abstract : We present evidence for alignment between the quadrupole , octopole and hexadecapole moments of the cosmic microwave background ( CMB ) temperature anisotropy on huge angular scales as measured by the Wilkinson Microwave Anisotropy Probe ( WMAP ) .We see that this alignment is statistically meaningful at greater than 5 sigma confidence rate when compared to modeled random realizations of CMB images with identical power spectrum but different phases . The observed alignment can be understood if there are big - scale magnetic fields aligned along the line - of - view which induce circular polarization in the primordial plasma prior to recombination .This phenomenon might lead to an additional contribution to the Stokes Q function of the CMB radiation . In addition we prove that the seen anomalous behaviour of the quadrupole point could also be caused by such influences .Finally , we explain possible possibilities of our findings for inflationary scenarios . We argue that these observations do not order out slow - roll single - field inflation scenarios , however they give novel constraints on their parameters .",
        "rewrite_text": "Title: Alignment and Signed-Intensity Anomalies in WMAP Data\n\nAbstract: In this study, we investigate the alignment of the quadrupole, octopole, and hexadecapole moments of the cosmic microwave background (CMB) temperature anisotropy, as observed by the Wilkinson Microwave Anisotropy Probe (WMAP). Our analysis reveals a statistically significant alignment at a confidence level exceeding 5 sigma when compared to simulated random realizations of CMB images that maintain the same power spectrum but exhibit different phase configurations. This notable alignment may be attributed to the presence of large-scale magnetic fields that are oriented along the line of sight, which could induce circular polarization in the primordial plasma prior to the epoch of recombination. Such a mechanism may contribute an additional component to the Stokes Q parameter of the CMB radiation. Furthermore, we demonstrate that the anomalous characteristics observed in the quadrupole moment could also arise from these magnetic influences. Our findings have implications for inflationary models, suggesting that while these observations do not rule out slow-roll single-field inflation scenarios, they do impose new constraints on their parameter space. This research enhances our understanding of the underlying physics of the CMB and its implications for cosmological theories, particularly in relation to the early universe's magnetic field dynamics and their potential effects on the observable properties of the CMB.",
        "ori-fast-z-score": -0.43133109281375365,
        "water-fast-z-score": 5.5432186252458715,
        "rewrite-fast-z-score": -0.10369516947304253
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Possible X-ray diagnostic for jet/disk dominance in Type 1 AGN .\nAbstract:\nWe present the results of an analysis of Chandra observations of two nearby radio-quiet quasars, PG 1211+143 and Mrk 335 (z=0.034). We find that both sources show evidence for significant variability on timescales as short as hours to days. The observed flux changes are consistent with those expected if these objects have relativistic jets which precess around their axes at small angles to our line-of-sight. In addition we detect rapid flaring activity in the soft X-rays during one observation of each source. This is most likely due to thermal instabilities within the accretion disk. Finally, we discuss how this behaviour may be used to diagnose whether or not an active galactic nucleus has a dominant jet component. Keywords: Active Galactic Nuclei, Quasars, Jet Precession, Flare Activity, Accretion Disk Instability, Chandra Observatory",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Possible X - ray diagnostic for jet / disk dominance in Type 1 AGN . Abstract : We report the conclusion of an assessment of Chandra measurements of two nearby radio - quiet quasars , PG 1211 + 143 and Mrk 335 ( z = 0 . 034 ) .We see that both accounts show proof for significant variability on timescales as short as hours to days . The observed flux changes are compatible with those expected if these objects have relativistic jets which precess around their axes at small angles to our line - of - view .In addition we perceive rapid flaring activity in the soft X - rays during one observation of each source . This is most likely due to heat instabilities within the accretion disk .Finally , we explain how this behaviour may be used to diagnose whether or not an active galactic nucleus has a dominant jet element . Keywords : Active Galactic Nuclei , Quasars , Jet Precession , Flare Activity , Accretion Disk Instability , Chandra Observatory",
        "rewrite_text": "Title: Potential X-ray Diagnostic for Jet/Disk Dominance in Type 1 Active Galactic Nuclei\n\nAbstract: In this study, we present the findings from an analysis of Chandra X-ray Observatory data concerning two nearby radio-quiet quasars, PG 1211+143 and Mrk 335, both located at a redshift of z = 0.034. Our investigation reveals substantial variability in their X-ray emissions, occurring over remarkably short timescales ranging from hours to days. The fluctuations in flux observed in these quasars align with theoretical predictions that suggest the presence of relativistic jets that precess at small angles relative to our line of sight. Furthermore, during our observations, we detected rapid flaring events in the soft X-ray spectrum for each quasar, which we attribute to thermal instabilities within the accretion disks surrounding these active galactic nuclei (AGN). These findings not only enhance our understanding of the dynamic processes occurring in AGN but also provide a potential diagnostic tool for determining the dominance of jet activity over disk processes in such systems. By analyzing the characteristics of variability and flare activity, we propose a framework for assessing the influence of relativistic jets in Type 1 AGN, which could have significant implications for the broader understanding of AGN behavior and evolution. Our results contribute to the ongoing discourse regarding the interplay between jets and accretion disks, offering insights that may refine existing models of AGN structure and dynamics. \n\nKeywords: Active Galactic Nuclei, Quasars, Jet Precession, Flare Activity, Accretion Disk Instability, Chandra Observatory",
        "ori-fast-z-score": 1.524001524002286,
        "water-fast-z-score": 5.25,
        "rewrite-fast-z-score": 0.40406101782088427
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Supernova Channel of Super-AGB Stars .\nAbstract:\nWe present the results of our study on the evolution and nucleosynthesis in super-asymptotic giant branch (super-AGB) stars, which are evolved massive AGB stars with initial masses between 8 to 12 M⊙. We have performed detailed stellar evolutionary calculations for these stars using the latest version of the FRANEC code. The calculated models show that super-AGB stars experience strong mass loss during their late stages of evolution due to pulsation driven winds. These stars lose about 0.5 M⊙ before they enter into the white dwarf cooling phase. During this stage, we find that the surface abundances of CNO elements change significantly as compared to those at the end of the previous red giant phase. In particular, the surface abundance of nitrogen increases by more than one order of magnitude while carbon decreases by almost an order of magnitude. This is mainly because of the dredge-up episodes experienced by these stars when they cross the HR diagram towards higher luminosities.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Supernova Channel of Super - AGB Stars . Abstract : We present the conclusion of our research on the evolution and nucleosynthesis in super - asymptotic giant branch ( super - AGB ) stars , which are evolved large AGB stars with initial masses between 8 to 12 [UNK] .We have done detailed stellar evolutionary analyses for these stars using the latest version of the FRANEC coding . The measured scenarios demonstrate that super - AGB stars experience heavy mass loss during their late stages of evolved due to pulsation driven winds .These stars drop about 0 . 5 [UNK] before they enter into the white dwarf cooling phase . During this phase , we find that the surface abundances of CNO elements shift strongly as compared to those at the end of the previous red giant phase .In particular , the surface abundance of nitrogen changes by more than one order of magnitude while carbon decreases by almost an order of magnitude . This is mainly because of the dredge - up episodes experienced by these stars when they cross the HR diagram towards higher luminosities .",
        "rewrite_text": "We present the findings of our study on the evolution and nucleosynthesis processes occurring in super-asymptotic giant branch (super-AGB) stars, which are massive AGB stars with initial masses ranging from 8 to 12 solar masses. Utilizing the latest version of the FRANEC code, we conducted comprehensive stellar evolutionary analyses to investigate the characteristics of these stars. Our results indicate that super-AGB stars undergo significant mass loss during their late evolutionary stages, primarily driven by pulsation-induced winds. Specifically, these stars lose approximately 0.5 solar masses before transitioning into the white dwarf cooling phase. \n\nDuring this cooling phase, we observed substantial changes in the surface abundances of carbon, nitrogen, and oxygen (CNO) elements compared to their abundances at the conclusion of the preceding red giant phase. Notably, the nitrogen abundance increases dramatically, exceeding one order of magnitude, while the carbon abundance diminishes by nearly an order of magnitude. These alterations in elemental composition are primarily attributed to dredge-up events that occur as the stars ascend the Hertzsprung-Russell diagram towards higher luminosities. Our findings contribute to a deeper understanding of the nucleosynthetic processes in super-AGB stars and their role in the chemical evolution of galaxies. This research highlights the importance of these stars in the cosmic landscape, particularly in the context of heavy element production and the subsequent impact on stellar populations and galactic enrichment.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 3.198010745334156,
        "rewrite-fast-z-score": -0.09166984970282113
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Age, Metallicity and Alpha-Element Abundance of Galactic Globular Clusters from Single Stellar Population Models .\nAbstract:\nWe present new results on the age, metallicity and alpha-element abundance for galactic globular clusters (GGCs) based on single stellar population models with different prescriptions for convection theory. We find that the ages derived by using the classical mixing-length theory are systematically younger than those obtained by assuming overshooting or semiconvection in the red giant branch phase. The difference between these two sets of ages is about 0.5 Gyr at most. For some metal-rich GGCs, we also found that their ages inferred from the classical mixing-length theory can be as young as 10 Gyr while they should have been older than 12 Gyr according to other methods. This discrepancy may result from the fact that the classical mixing-length theory cannot reproduce well the observed color-magnitude diagrams of such metal-rich GGCs. Our results show that there exists no significant correlation between the cluster s age and its metallicity.  These findings suggest that the formation history of GGCs might not be dominated by monolithic collapse but instead by hierarchical merging processes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Age , Metallicity and Alpha - Element Abundance of Galactic Globular Clusters from Single Stellar Population Models . Abstract : We report new data on the age , metallicity and alpha - atom availability for galactic globular complexes ( GGCs ) based on single stellar community models with various prescriptions for circulation theory .We see that the periods derived by using the classical mixing - length theory are systematically younger than those generated by assuming overshooting or semiconvection in the red giant branch process . The difference between these two sets of periods is about 0 . 5 Gyr at most .For some metal - rich GGCs , we also discovered that their ages inferred from the classical mixing - length theory can be as early as 10 Gyr while they should have been older than 12 Gyr according to other methods . This discrepancy may come from the fact that the classical mixing - length theory cannot reproduce well the known color - magnitude diagrams of such silver - rich GGCs .Our results show that there exists no major interaction between the cluster s age and its metallicity . These conclusions propose that the formation history of GGCs might not be dominated by monolithic collapse but instead by hierarchical merging cycles .",
        "rewrite_text": "We present new findings regarding the age, metallicity, and alpha-element abundance of Galactic Globular Clusters (GGCs) derived from single stellar population models that incorporate various approaches to mixing theory. Our analysis reveals that the ages estimated using classical mixing-length theory tend to be systematically younger than those obtained through models that include overshooting or semiconvection processes during the red giant branch phase. The maximum age discrepancy between these two methodologies is approximately 0.5 Gyr. Notably, for certain metal-rich GGCs, we found that ages calculated using classical mixing-length theory can be as young as 10 Gyr, while other methods suggest these clusters should be older than 12 Gyr. This inconsistency may arise from the limitations of classical mixing-length theory in accurately reproducing the observed color-magnitude diagrams of these metal-rich GGCs. Our findings indicate that there is no significant correlation between the age of a cluster and its metallicity. These results imply that the formation history of GGCs may not be primarily influenced by a monolithic collapse scenario; rather, it suggests a more complex evolutionary process characterized by hierarchical merging events. This research contributes to our understanding of the formation and evolution of GGCs, highlighting the need for refined models that can better account for the observed properties of these ancient stellar systems.",
        "ori-fast-z-score": -0.5360562674188973,
        "water-fast-z-score": 4.477215043467819,
        "rewrite-fast-z-score": 0.9622504486493763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Optimum Power Allocation for the V-BLAST .\nAbstract:\nIn this work, we consider an uplink MIMO system with Nt transmit antennas and one receive antenna at each user terminal (UT). We assume that all UTs are equipped with single RF chain which can be used to process signals received from multiple antennas simultaneously using linear processing techniques such as zero forcing or minimum mean square error criteria. The problem is formulated in terms of minimizing total power consumption subject to per-user rate constraints. It has been shown recently that the optimal solution to this problem requires joint optimization over both precoding matrices and power allocation coefficients. In order to solve this non-convex problem efficiently, we propose two algorithms based on successive convex approximation technique. First algorithm solves the original problem by iteratively optimizing over precoders and power allocations while second algorithm decouples these problems into separate subproblems and then solves them separately. Both proposed algorithms converge to the same point within few iterations. Simulation results show that our proposed algorithms outperform existing schemes significantly. \n \n Keywords: Multi-user MIMO systems, Zero-forcing beamforming, Minimum Mean Square Error Beamforming",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Optimum Power Allocation for the V - BLAST . Abstract : In this project , we define an uplink MIMO scheme with Nt transmit antennas and one receive transmitter at each user terminal ( UT ) .We assume that all UTs are equipped with single RF chain which can be used to process waves sent from multiple antennas continuously using linear analysis methods such as zero forcing or lowest mean square error criteria . The question is formulated in terms of minimizing gross electricity usage subject to per - customer rate constraints .It has been shown lately that the ideal solution to this question involves joint algorithms over both precoding matrices and power allocation variables . In order to solve this non - convex problem efficiently , we propose two strategies using on successive convex algorithm method .First algorithm solves the original problem by iteratively optimizing over precoders and power allocations while second method decouples these problems into independent subproblems and then solves them independently . Both proposed methods converge to the same point within few iterations .Simulation data demonstrate that our proposed methods outperform established algorithms considerably . Keywords : Multi - person MIMO systems , Zero - forcing beamforming , Minimum Mean Square Error Beamforming",
        "rewrite_text": "**Title:** On Optimum Power Allocation for the V-BLAST\n\n**Abstract:** This study presents a comprehensive analysis of an uplink Multiple Input Multiple Output (MIMO) system characterized by Nt transmit antennas and a single receive antenna at each user terminal (UT). We operate under the assumption that each UT is equipped with a single Radio Frequency (RF) chain, enabling the continuous processing of signals transmitted from multiple antennas through linear analytical techniques, including zero forcing and minimum mean square error (MMSE) criteria. The primary objective of this research is to minimize overall power consumption while adhering to specific rate constraints for each customer. Recent findings indicate that the optimal solution to this challenge necessitates the implementation of joint algorithms that address both precoding matrices and power allocation variables simultaneously. To tackle this non-convex optimization problem effectively, we introduce two innovative strategies based on the successive convex approximation method. The first strategy involves iteratively refining both precoders and power allocations to solve the original problem, while the second approach separates the issues into independent subproblems, allowing for independent resolution. Remarkably, both methods converge to the same optimal solution within a limited number of iterations. Our simulation results reveal that the proposed strategies significantly outperform existing algorithms, demonstrating their effectiveness in optimizing power allocation in MIMO systems. This work contributes to the field of multi-user MIMO systems by enhancing the understanding of power allocation techniques and their practical applications in wireless communication networks.\n\n**Keywords:** Multi-user MIMO systems, Zero-forcing beamforming, Minimum Mean Square Error beamforming.",
        "ori-fast-z-score": -2.710687382741972,
        "water-fast-z-score": 4.666282626286914,
        "rewrite-fast-z-score": -0.7126966450997984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An absorption origin for the soft excess in Seyfert 1 AGN .\nAbstract:\nWe present new results on the X-ray spectrum and variability properties of Mrk 509, one of the brightest Seyfert galaxies observed by XMM-Newton. We find that its 0.5-10 keV continuum is well described by an absorbed power law with Γ = 2.1 ± 0.2 (χ2/dof=111/101) plus a reflection component modeled as a PEXRAV model with R=0.7-1.0 and NH=10-23×1022 cm-2. The best-fit parameters are consistent within errors to those found previously using Chandra data alone. No significant spectral changes were detected between different epochs separated by several months apart. However, we do detect strong flux variations at all energies during our observation period. In particular, there was a factor of 3 increase in the hard band count rate over about 20 ks followed by a slower decay back towards the initial level. This behavior can be explained if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An absorption origin for the soft excess in Seyfert 1 AGN . Abstract : We report new data on the X - ray spectrum and variability properties of Mrk 509 , one of the brightest Seyfert galaxies studied by XMM - Newton .We see that its 0 . 5 - 10 keV continuum is well described by an absorption power law with Γ = 2 . 1 ± 0 . 2 ( χ2 / dof = 111 / 101 ) plus a mirror element modeled as a PEXRAV model with R = 0 . 7 - 1 . 0 and NH = 10 - 23×1022 centimetres - 2 . The best - fitting values are compatible within errors to those identified previously used Chandra data alone .No meaningful spectral changes were detected between various epochs separated by many months separated . However , we do discover powerful flux variations at all energies during our observation term .In particular , there was a factor of 3 gain in the hard band count rate over about 20 ks followed by a slower decay forward towards the first level . This phenomenon can be understood if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation .",
        "rewrite_text": "We present new findings regarding the X-ray spectrum and variability characteristics of Mrk 509, one of the most luminous Seyfert galaxies analyzed using XMM-Newton. Our investigation reveals that the 0.5 - 10 keV continuum of Mrk 509 can be accurately represented by an absorption power law with a photon index of Γ = 2.1 ± 0.2 (χ²/dof = 111/101). Additionally, we incorporate a mirror component modeled by the PEXRAV framework, with reflection parameters R ranging from 0.7 to 1.0 and hydrogen column density NH estimated at 10 - 23 × 10²² cm⁻². Notably, these optimal fitting values align closely with those derived from previous analyses utilizing Chandra data, suggesting consistency in our findings. \n\nThroughout our observations, which spanned several months, we did not observe significant spectral variations across different epochs. However, we identified substantial flux variations across all energy bands during the observation period. Specifically, we recorded a threefold increase in the hard band count rate over approximately 20 ks, followed by a gradual decline back to baseline levels. This behavior may indicate that the source was in a transitional state, where the luminosity of the accretion disk experienced a rapid increase, potentially triggered by an instability or perturbation within the system. Our results contribute to the understanding of the soft excess phenomenon in Seyfert 1 active galactic nuclei (AGN) and highlight the importance of monitoring X-ray variability to uncover the underlying processes governing these dynamic astrophysical environments.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 5.521576303742327,
        "rewrite-fast-z-score": 0.38851434494290565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray Timing Observations of PSR J1930+1852 in the Crab-like SNR G54.1+0.3 .\nAbstract:\nWe report on X-ray timing observations of the pulsar candidate PSR J1930+1855 located at the center of the supernova remnant (SNR) G54.1+0. \n \n The source was discovered by Chandra and confirmed as a pulsar with XMM-Newton, but its spin period is not stable over time scales longer than one day. We performed two sets of pointed RXTE observations to study this behavior further. In both cases we found that the pulse frequency decreases smoothly during our observation runs. This trend can be described well using an exponential decay model for which we find characteristic timescales of 1.1 days and 0.7 days respectively. These values are consistent with those reported previously based on Chandra data alone. However, when comparing these results directly it should be noted that the uncertainties associated with the previous measurements were significantly larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray Timing Observations of PSR J1930 + 1852 in the Crab - like SNR G54 . 1 + 0 . 3 . Abstract : We report on X - ray timing observations of the pulsar candidate PSR J1930 + 1855 centered at the center of the supernova remnant ( SNR ) G54 . 1 + 0 .The source was studied by Chandra and reported as a pulsar with XMM - Newton , but its spin time is not stable over time ranges less than one day . We conducted two sets of pointed RXTE observations to study this behavior further .In both cases we concluded that the pulse frequency drops slowly during our observation running . This trend can be described good using an exponential decay model for which we find typical timescales of 1 . 1 hours and 0 . 7 days respectively .These figures are compatible with those published previously based on Chandra data alone . However , when comparing these results directly it should be mentioned that the uncertainties involved with the previous tests were significantly larger thanks to the smaller signal - to - noise proportion attained with Chandra compared to RXTE .",
        "rewrite_text": "We present findings from our X-ray timing observations of the pulsar candidate PSR J1930 + 1855, located at the heart of the supernova remnant (SNR) G54.1 + 0.3. Previous studies utilizing Chandra and XMM-Newton have identified this source as a pulsar; however, our observations reveal that its spin period exhibits instability over intervals shorter than one day. To investigate this phenomenon in greater detail, we conducted two sets of targeted observations using the Rossi X-ray Timing Explorer (RXTE). Our analysis indicates a gradual decline in pulse frequency throughout the duration of our observations. This observed trend can be effectively modeled using an exponential decay function, yielding characteristic timescales of approximately 1.1 hours and 0.7 days for the two observation sets, respectively. Notably, these timescales align well with earlier findings derived from Chandra data. It is important to highlight, however, that the uncertainties associated with the previous measurements were considerably larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE. Our results contribute to the understanding of pulsar behavior in supernova remnants and underscore the significance of utilizing high-sensitivity instruments for precise timing measurements. This study not only reinforces the pulsar's classification but also provides valuable insights into the mechanisms governing its spin dynamics, which may have implications for the broader field of astrophysics and the study of neutron stars.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": 0.3849001794597505
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Connection between Mass Loss and Evolution of C-rich AGB stars .\nAbstract:\nWe present new results on mass loss in carbon rich asymptotic giant branch (AGB) stars based on infrared photometry obtained with ISO-SWS, IRAS, MSX and Spitzer-IRS. We find that there is no correlation between the total luminosity or effective temperature of these objects and their mass-loss rates. The observed scatter may be explained by differences in chemical composition and/or pulsation properties among individual sources. In addition to this we show that the dust-to-gas ratio decreases towards higher temperatures for oxygen-rich as well as carbon-rich AGB stars. This indicates that the physical conditions at which dust forms are different in both types of evolved stars. Finally, we discuss how our findings can be used to improve current models describing the evolution of red giants. Keywords: Asymptotic Giant Branch Stars; Dust formation; Red Giants; Mass loss. 1 Introduction Carbon-rich asymptotic giant branch (AGB; hereafter Crich AGB) stars have been studied extensively over the past decades because they represent an important source class of interstellar matter. They lose large amounts of material through stellar winds driven by radiation pressure on dust grains formed in the outflowing gas. These winds play an essential role in shaping circumstellar envelopes around evolved stars and thus influence the appearance of planetary nebulae and proto-stellar disks surrounding young stellar objects. However, despite numerous observational studies it remains unclear what determines the amount of mass lost by Crich AGB stars. It has been suggested that the total luminosity L * , the effective temperature T eff , the surface gravity g, the metallicity Z, the pulsation period P, and the initial mass M ini might all affectṀ . For example, Wood et al. (1992) , van Loon et al. (1999), Olofsson et al. (2002a) , Knapp & Morris (1985) , and Winters et al. (1994) found evidence thatṀ increases with decreasing T eff .\nIn contrast, Groenewegen et al. (1998 ), De Beck et al. (2010 , and Ramstedt et al",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Connection between Mass Loss and Evolution of C - rich AGB stars . Abstract : We report new data on mass loss in carbon rich asymptotic giant line ( AGB ) stars based on infrared photometry obtained with ISO - SWS , IRAS , MSX and Spitzer - IRS .We see that there is no coupling between the total luminosity or effective heat of these objects and their mass - loss rates . The observed scatter could be explained by differences in material composition and / or pulsation properties among different sources .In addition to this we find that the dust - to - gas ratio tends towards higher temperatures for oxygen - rich as well as carbon - rich AGB stars . This implies that the physical conditions at which dust occurs are changed in both types of evolved stars .Finally , we explain how our findings can be used to improve current theories describing the evolution of red giants . Keywords : Asymptotic Giant Branch Stars ; Dust formation ; Red Giants ; Mass loss .1 Introduction Carbon - rich asymptotic giant line ( AGB ; hereafter Crich AGB ) stars have been studied significantly over the previous decades because they represent an important source type of interstellar matter . They lose significant amounts of material through stellar winds driven by radiation stress on dust grains created in the outflowing gas .These winds play an essential part in shaping circumstellar envelopes around evolved stars and therefore influence the appearance of planetary nebulae and proto - stellar disks surrounding young stellar bodies . However , despite several observational analyses it remains unsure what determines the quantity of mass losing by Crich AGB stars .It has been suggested that the total luminosity L * , the effective temperature T eff , the surface gravity g , the metallicity Z , the pulsation period P , and the initial mass M ini might all [UNK] . For example , Wood et al .( 1992 ) , van Loon et al . ( 1999 ) , Olofsson et al .( 2002a ) , Knapp & Morris ( 1985 ) , and Winters et al . ( 1994 ) found evidence [UNK] increases with decreasing T eff .In comparison , Groenewegen et al . ( 1998 ) , De Beck et al .(2010 , and Ramstedt et al",
        "rewrite_text": "**Title:** On the Connection between Mass Loss and Evolution of Carbon-Rich AGB Stars\n\n**Abstract:** This study presents new findings on mass loss in carbon-rich asymptotic giant branch (AGB) stars, utilizing infrared photometric data collected from ISO-SWS, IRAS, MSX, and Spitzer-IRS. Our analysis reveals a lack of correlation between the total luminosity or effective temperature of these stars and their mass-loss rates. The variability observed in mass loss can potentially be attributed to differences in material composition and pulsation characteristics among the various sources examined. Furthermore, we observe that the dust-to-gas ratio increases with temperature in both oxygen-rich and carbon-rich AGB stars, indicating altered physical conditions for dust formation in these evolved stellar types. These insights have significant implications for refining existing models of red giant evolution. \n\nCarbon-rich AGB stars have garnered considerable attention over the past few decades due to their crucial role as sources of interstellar matter. They expel substantial amounts of material through stellar winds, which are driven by radiation pressure acting on dust grains formed in the outflowing gas. These stellar winds are instrumental in shaping the circumstellar envelopes surrounding evolved stars, thereby influencing the characteristics of planetary nebulae and the formation of proto-stellar disks around nascent stars. Despite extensive observational studies, the precise mechanisms governing mass loss in carbon-rich AGB stars remain unclear. Various factors have been proposed as potential determinants of mass loss, including total luminosity (L*), effective temperature (Teff), surface gravity (g), metallicity (Z), pulsation period (P), and initial mass (Mini). Previous research has suggested that mass loss tends to increase with decreasing effective temperature, as noted in studies by Wood et al. (1992), van Loon et al. (1999), Olofsson et al. (2002), Knapp & Morris (1985), and Winters et al. (1994). In contrast, other studies, such as those by Groenewegen et al. (1998), De Beck et al. (2010), and Ramstedt et al., have explored different aspects of this complex relationship. Our findings contribute to a deeper understanding of the evolutionary processes of red giants and the factors influencing mass loss in these stars. \n\n**Keywords:** Asymptotic Giant Branch Stars; Dust Formation; Red Giants; Mass Loss.",
        "ori-fast-z-score": -1.2893167424406085,
        "water-fast-z-score": 4.643716460347527,
        "rewrite-fast-z-score": -0.38014296063485276
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the End-to-End Distortion for a Buffered Transmission over Fading Channel .\nAbstract:\nIn this work, we study end-to-end distortion in a buffered transmission system with fading channel and power control. We consider two different scenarios depending on whether or not there is feedback information available at transmitter side. In both cases, we derive closed-form expressions for average end-to-end distortion as well as its upper bound under Rayleigh flat-fading channels. The results show that when there exists no feedback link between source node and destination node, the optimal power allocation strategy depends only on the instantaneous channel gain. However, if there are some feedback links to provide the knowledge about the current channel state, then it can be shown that the optimal power allocation strategy also depends on the past channel states. Finally, numerical examples are provided to illustrate our theoretical analysis. Keywords: Power Control; Average End-to-End Distortion; Feedback Links; Flat-Fading Channels. 1 Introduction In recent years, wireless communications have become an important part of many applications such as mobile phones, personal digital assistants (PDAs), laptops etc., due to their advantages like mobility, flexibility and low cost  1  . However, one major problem associated with these systems is the limited bandwidth which leads to high bit error rate  2  .\nTo overcome this problem, various techniques including forward error correction coding  3  , diversity combining  4  , adaptive modulation  5  , power control  6  , unequal error protection  7  , joint source-channel coding  8  , etc., have been proposed by researchers. Among them, power control has attracted much attention because it allows us to adjust transmit power according to varying channel conditions so as to maximize the data rates while maintaining acceptable quality-of-service  9  . For example, in  10  -  12  , authors studied the effect of power control on outage probability and ergodic capacity respectively. On the other hand, in  13  -  15  , authors investigated the performance of power controlled communication systems using Shannon s mutual information criterion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the End - to - End Distortion for a Buffered Transmission over Fading Channel . Abstract : In this project , we study end - to - end distortion in a buffered transmission system with fading signal and power control .We consider two different scenarios depending on whether or not there is feedback info available at broadcast side . In both cases , we derive closed - form expressions for mean end - to - end distortion as well as its higher bound under Rayleigh flat - fading frequencies .The results show that when there exists no feedback link between source node and destination node , the ideal power allocation strategy depends only on the instantaneous channel gain . However , if there are some feedback links to provide the knowledge about the present channel state , then it can be shown that the ideal power distribution policy also relies on the past network states .Finally , numerical examples are provided to illustrate our theoretical analysis . Keywords : Power Control ; Average End - to - End Distortion ; Feedback Links ; Flat - Fading Channels .1 Introduction In recent years , wireless communications have become an important element of several products such as wireless phones , personal digital assistants ( PDAs ) , laptops etc . , owing to their benefits like mobility , flexibility and low cost 1 . However , one major difficulty associated with these systems is the limited bandwidth which results to large bit error rate 2 .To solve this situation , various methods namely back fault correction coding 3 , diversity combining 4 , adaptive modulation 5 , power control 6 , unequal error protection 7 , joint source - channel code 8 , etc . , have been proposed by researchers . Among them , power control has garnered great attention because it allows us to adapt broadcast capacity according to different channel conditions so as to maximize the information rates while maintaining acceptable performance - of - service 9 .For instance , in 10 - 12 , authors explored the impact of power control on outage likelihood and ergodic performance respectively . On the other hand , in 13 - 15 , authors explored the performance of power controlled communication schemes using Shannon s mutual intelligence criterion .",
        "rewrite_text": "**Title: On the End-to-End Distortion for Buffered Transmission over Fading Channels**\n\n**Abstract:** This study investigates the end-to-end distortion in a buffered transmission system operating over fading channels, with a focus on power control mechanisms. We analyze two distinct scenarios based on the availability of feedback information at the broadcasting end. In both scenarios, we derive closed-form expressions for the mean end-to-end distortion and establish upper bounds under conditions of Rayleigh flat-fading frequencies. Our findings indicate that in the absence of feedback between the source and destination nodes, the optimal power allocation strategy is solely dependent on the instantaneous channel gain. Conversely, when feedback links are present, providing real-time information about the current channel state, the optimal power distribution strategy is influenced by both current and historical network states. To substantiate our theoretical findings, we present numerical examples that illustrate the implications of our analysis. \n\n**Keywords:** Power Control; Average End-to-End Distortion; Feedback Links; Flat-Fading Channels.\n\n**1. Introduction:** In recent years, wireless communication technologies have become integral to a variety of devices, including mobile phones, personal digital assistants (PDAs), and laptops, due to their advantages such as mobility, flexibility, and cost-effectiveness. However, a significant challenge faced by these systems is the limited bandwidth, which often leads to a high bit error rate. To address this issue, researchers have proposed numerous strategies, including backward error correction coding, diversity combining, adaptive modulation, power control, unequal error protection, and joint source-channel coding. Among these, power control has received considerable attention as it enables the adaptation of broadcast capacity in response to varying channel conditions, thereby maximizing information rates while ensuring acceptable service performance. Previous studies have explored the effects of power control on outage probabilities and ergodic performance, as well as the performance of power-controlled communication schemes based on Shannon's mutual information criterion.",
        "ori-fast-z-score": 1.1149412193707495,
        "water-fast-z-score": 7.264243303278792,
        "rewrite-fast-z-score": 1.0552897060221726
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Anomalous structural and mechanical properties of solids confined in quasi one dimensional strips .\nAbstract:\nWe study the elasticity of thin films with periodic microstructure, which are constrained to lie on an underlying substrate. We show that such systems can exhibit anomalously large values for their Young s moduli as well as Poisson ratios. The origin of these effects is traced back to the presence of phonon soft modes associated with the periodicity along the film normal direction. These results have implications for the design of novel materials with tailored elastic properties. \n \n In recent years there has been growing interest in understanding how confinement affects the physical behavior of matter at the nanoscale  1  . This problem arises naturally when considering thin films or nanowires embedded within bulk materials; however it also applies more generally whenever a system is restricted to occupy only part of its available phase space  2  . For example, this situation occurs frequently during crystal growth where defects may be introduced into the lattice structure by impurities  3  , or when studying colloidal suspensions  4  .\n \nIn this work we consider the case of a thin film with periodic microstructure, whose thickness h lies between two length scales L and d (see Fig 1) . Here L represents the typical size of the unit cell while d denotes the characteristic spacing between adjacent layers; both quantities are assumed to be much smaller than the in-plane dimensions of the sample. Such structures arise commonly in nature, e.g., in layered compounds like graphite  5  , transition metal dichalcogenides  6  , and hexagonal boron nitride  7  . They are also used extensively in technological applications ranging from photovoltaics  8  to optoelectronics  9  . \n \n Figure 1: Schematic illustration of our model geometry. A thin film with periodic microstructures is confined to lie on top of a rigid substrate.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Anomalous structural and physical properties of solids localized in quasi one dimensional strips . Abstract : We explore the elasticity of thin films with periodic microstructure , which are constrained to lay on an underlying substrate .We see that such schemes can exhibit anomalously high values for their Young s moduli as well as Poisson ratios . The origin of these phenomena is traced back to the presence of phonon quiet modes associated with the periodicity along the film regular direction .These data have consequences for the creation of new materials with tailored elastic properties . In past decades there has been growing interest in understanding how confinement impacts the physical response of matter at the nanoscale 1 .This problem arises readily when examining narrow bands or nanowires enclosed within bulk surfaces ; however it also applies more generally whenever a system is restricted to occupy only portion of its available phase space 2 . For instance , this situation occurs commonly during crystal growth where defects could be applied into the lattice structure by impurities 3 , or when examining colloidal suspensions 4 .In this research we imagine the case of a thin film with periodic microstructure , whose thickness h lies between two width scales L and d ( see Fig 1 ) . Here L represents the typical size of the unit cell while d indicates the typical spacing between neighboring layers ; both quantities are expected to be much smaller than the in - plane dimensions of the sample .Such structures appear often in nature , e . g . , in layered compounds like graphite 5 , transition copper dichalcogenides 6 , and hexagonal boron nitride 7 . They are also used heavily in technological applications ranging from photovoltaics 8 to optoelectronics 9 .Figure 1 : Schematic illustration of our model topology . A narrow film with periodic microstructures is confined to lying on top of a rigid coating .",
        "rewrite_text": "**Title:** Anomalous Structural and Physical Properties of Solids Localized in Quasi-One-Dimensional Strips\n\n**Abstract:** This study investigates the elastic properties of thin films characterized by periodic microstructures, which are constrained to rest on a rigid substrate. Our findings reveal that such configurations can exhibit unexpectedly high values for both Young's modulus and Poisson's ratio. The underlying mechanism responsible for these anomalous properties is linked to the presence of phonon quiet modes that arise from the periodicity in the film's primary direction. These insights have significant implications for the development of novel materials with customized elastic characteristics. \n\nIn recent decades, there has been an increasing focus on how confinement influences the physical behavior of materials at the nanoscale. This issue is particularly relevant when considering narrow bands or nanowires that are surrounded by bulk materials; however, it is also applicable in broader contexts where a system is limited to a fraction of its potential phase space. For example, this scenario frequently occurs during crystal growth, where defects may be introduced into the lattice structure due to impurities, or in the analysis of colloidal suspensions. \n\nIn our research, we conceptualize a thin film with a periodic microstructure, with a thickness \\( h \\) that lies between two characteristic scales, \\( L \\) and \\( d \\). Here, \\( L \\) denotes the typical size of the unit cell, while \\( d \\) represents the average spacing between adjacent layers, both of which are anticipated to be significantly smaller than the in-plane dimensions of the sample. Such structures are prevalent in nature, appearing in layered compounds such as graphite, transition metal dichalcogenides, and hexagonal boron nitride. Additionally, they find extensive applications in various technological fields, including photovoltaics and optoelectronics. This research not only enhances our understanding of the mechanical properties of confined materials but also paves the way for innovative applications in material science.",
        "ori-fast-z-score": -1.655211777204736,
        "water-fast-z-score": 6.575959492214292,
        "rewrite-fast-z-score": 0.6069769786668839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The substellar mass function in sigma Orionis. II. Optical, near-infrared and IRAC/Spitzer photometry of young cluster brown dwarfs and planetary-mass objects .\nAbstract:\nWe present optical (BVRI), near infrared (JHK) and mid-infrared (IRAC 3.6-8.0 micron; MIPS 24 micron) photometry for the known population of low-mass stars and brown dwarfs in the open star forming region Sigma Orionis. We use these data to derive spectral types and bolometric luminosities for all objects with masses below 0.1 solar masses. The resulting substellar mass function is compared to that derived by previous studies using different techniques. Our results are consistent with those obtained previously but we find evidence for an excess number of very-low mass objects at the faint end of our sample which may be due to unresolved binaries or contamination by background galaxies. This work was supported by NASA grant NAG5-12942. We thank J. Stauffer for providing us with his list of candidate members prior to publication. \n \n Keywords: Open clusters",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The substellar mass function in sigma Orionis . II .Optical , near - infrared and IRAC / Spitzer photometry of young cluster brown dwarfs and planetary - mass bodies . Abstract : We present visual ( BVRI ) , near infrared ( JHK ) and mid - infrared ( IRAC 3 . 6 - 8 . 0 micron ; MIPS 24 micron ) photometry for the known community of low - mass stars and green dwarfs in the open star producing region Sigma Orionis .We use these information to derive spectral classes and bolometric luminosities for all bodies with masses below 0 . 1 solar masses . The resulting substellar mass distribution is compared to that derived by earlier surveys using different methods .Our results are compatible with those generated previously but we find proof for an excess amount of very - low weight objects at the faint ending of our sample which may be due to unresolved binaries or poisoning by background galaxies . This research was supported by NASA grant NAG5 - 12942 .We thank J . Stauffer for providing us with his list of candidate members preceding to publication . Keywords : Open clusters",
        "rewrite_text": "We present a comprehensive study of the substellar mass function within the Sigma Orionis cluster, focusing on the optical, near-infrared, and mid-infrared photometry of young brown dwarfs and planetary-mass objects. Our analysis includes visual photometry in the BVRI bands, near-infrared measurements in the JHK bands, and mid-infrared observations using the IRAC (3.6 - 8.0 microns) and MIPS (24 microns) instruments. By utilizing this extensive dataset, we are able to determine the spectral classifications and bolometric luminosities for all celestial bodies with masses below 0.1 solar masses. \n\nThe derived substellar mass distribution is then compared to results from previous surveys that employed different methodologies. Our findings align with earlier studies; however, we identify a notable excess of very low-mass objects at the faint end of our sample. This anomaly may be attributed to the presence of unresolved binary systems or contamination from background galaxies. \n\nThis research was made possible through the support of NASA grant NAG5-12942, and we extend our gratitude to J. Stauffer for supplying his list of candidate members prior to publication. Our work contributes to the understanding of the population of low-mass stars and brown dwarfs in open clusters, providing valuable insights into the formation and evolution of these celestial bodies. \n\nKeywords: Open clusters, brown dwarfs, planetary-mass objects, photometry, Sigma Orionis.",
        "ori-fast-z-score": -2.0124611797498106,
        "water-fast-z-score": 2.9068883707497264,
        "rewrite-fast-z-score": -0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Discrete Control Systems .\nAbstract:\nThe book is intended for students who have completed the first year course in control theory and are interested to learn more about discrete-time systems, digital controllers and computer-based control techniques. The text covers topics such as state space representation, stability analysis, optimal control design, robustness issues, model predictive control (MPC), fuzzy logic based control etc., with an emphasis on practical applications. It also includes several examples that illustrate key concepts discussed throughout the chapters. \n \n This textbook provides a comprehensive coverage of basic principles underlying various aspects of modern control engineering. In addition to theoretical foundations, it presents numerous numerical examples illustrating important concepts introduced along the way. A number of exercises at the end of each chapter help readers reinforce their understanding of material presented earlier. Finally, there are two appendices containing additional information useful for further study or research work. \nThis book can be used by graduate students studying advanced courses in control theory, as well as researchers working in this area.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Discrete Control Systems . Abstract : The author is intended for students who have completed the first year course in control theory and are concerned to study more about discrete - time systems , digital controllers and computer - based control methods .The text encompasses topics such as state space representation , stability analysis , ideal control design , robustness issues , model predictive control ( MPC ) , fuzzy logic based control etc . , with an emphasis on practical applications . It additionally contains several examples that highlight key concepts discussed throughout the pages .This treatise presents a comprehensive treatment of fundamental principles governing several elements of modern control engineering . In addition to theoretical foundations , it presents several mathematical models illustrating vital concepts adopted along the way .A variety of activities at the end of each volume help readers reinforce their understanding of content presented earlier . Finally , there are two appendices containing extra data helpful for further study or research studies .This book can be used by graduate programs studying advanced courses in control theory , as well as scientists studying in this area .",
        "rewrite_text": "**Title: Discrete Control Systems**\n\n**Abstract:** This article is designed for students who have successfully completed an introductory course in control theory and are eager to delve deeper into the realm of discrete-time systems, digital controllers, and computer-based control methodologies. The text covers a wide array of topics including state space representation, stability analysis, ideal control design, robustness considerations, model predictive control (MPC), and fuzzy logic-based control, all with a strong focus on practical applications. Throughout the article, numerous examples are provided to illustrate and clarify the key concepts discussed, ensuring that readers can grasp the material effectively.\n\nThe work offers a thorough exploration of the fundamental principles that underpin various aspects of contemporary control engineering. In addition to laying out the theoretical foundations, it presents a series of mathematical models that exemplify essential concepts encountered throughout the discussion. To enhance comprehension and retention, a variety of exercises are included at the end of each section, allowing readers to reinforce their understanding of the material covered.\n\nMoreover, the article includes two appendices that provide supplementary information beneficial for further study or research endeavors. This comprehensive resource is suitable for graduate programs offering advanced courses in control theory, as well as for researchers and practitioners in the field seeking to expand their knowledge and expertise in discrete control systems. Overall, this article serves as an invaluable guide for those looking to deepen their understanding of digital control methodologies and their applications in modern engineering.",
        "ori-fast-z-score": 0.2822162605150792,
        "water-fast-z-score": 6.18146635643918,
        "rewrite-fast-z-score": 1.2185435916898848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Toroidal Magnetic Fields in Type II Superconducting Neutron Stars .\nAbstract:\nWe study the magnetic field structure and evolution of neutron stars with type-II superconductivity, taking into account the effects of superfluidity on the stellar interior. We find that toroidal fields can be generated by differential rotation between normal matter and superfluids inside the star. The resulting toroidal field is strong enough to affect the spin-down rate of pulsars significantly. In particular, we show that it may explain why some young pulsars have much slower spindown rates than expected for their ages. \n \n Introduction \n \n Pulsar magnetospheres are believed to contain large-scale poloidal and toroidal magnetic fields (e.g., Goldreich & Julian 1969; Ruderman 1974). These fields play an important role in determining the observed properties of pulsar emission such as pulse profiles and polarization characteristics (e.g., Melrose 1995), but also influence the long-term evolution of pulsar periods through electromagnetic torques exerted at the light cylinder (e.g., Spitkovsky 2006). \n \n It has been suggested that toroidal fields could be produced during the formation process of neutron stars via dynamo action driven by convection or differential rotation between different components within the core region (Thompson & Duncan 1993) . However, recent studies suggest that this mechanism cannot generate sufficiently large toroidal fields to match observations (Heger et al. 2005 ). An alternative possibility is that toroidal fields are created by winding up poloidal fields due to rapid rotation of the crust (Braithwaite 2009) or by differential rotation between normal fluid and superfluid components in the interior of the star (Srinivasan et al. 1991; Srinivasan 1991a ) . \n \n In this work, we investigate how toroidal fields evolve over time under various physical conditions using numerical simulations. Our results indicate that toroidal fields can grow rapidly if there exists significant differential rotation between normal matter",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Toroidal Magnetic Fields in Type II Superconducting Neutron Stars . Abstract : We research the magnetic field mechanism and evolution of neutron galaxies with type - II superconductivity , giving into consideration the effects of superfluidity on the stellar interior .We see that toroidal fields can be formed by differential rotation between normal matter and superfluids inside the star . The resulting toroidal field is strong enough to affect the spin - down frequency of pulsars significantly .In particular , we prove that it could explain why some young pulsars have far slower spindown rates than expected for their ages . Introduction Pulsar magnetospheres are said to contain big - scale poloidal and toroidal gravitational currents ( e . g . , Goldreich & Julian 1969 ; Ruderman 1974 ) .These fields work an important role in determining the observed properties of pulsar emission such as pulse profiles and polarization traits ( e . g . , Melrose 1995 ) , but also affect the long - term evolve of pulsar periods through electromagnetic torques exerted at the light cylinder ( e . g . , Spitkovsky 2006 ) . It has been proposed that toroidal fields could be formed during the formation process of neutron galaxies via dynamo action driven by convection or differential rotation between various components within the core region ( Thompson & Duncan 1993 ) .However , recent studies propose that this mechanism unable generate sufficiently huge toroidal fields to match observations ( Heger et al . 2005 ) .An alternative possibility is that toroidal fields are created by winding up poloidal fields thanks to rapid rotation of the crust ( Braithwaite 2009 ) or by differential rotation between normal liquid and superfluid elements in the interior of the star ( Srinivasan et al . 1991 ; Srinivasan 1991a ) .In this research , we investigate how toroidal fields grow over time under various physical conditions utilizing numerical simulations . Our results show that toroidal fields can develop rapidly if there exists significant differential rotation between normal matter",
        "rewrite_text": "**Title:** Toroidal Magnetic Fields in Type II Superconducting Neutron Stars\n\n**Abstract:** This study delves into the mechanisms behind the formation and evolution of magnetic fields in neutron stars exhibiting type-II superconductivity, while also considering the influence of superfluidity on the stellar interior. We demonstrate that toroidal magnetic fields can arise from the differential rotation between normal matter and superfluid components within the star. Our findings indicate that these toroidal fields possess sufficient strength to significantly alter the spin-down rates of pulsars. Specifically, we provide evidence that this phenomenon may account for the unexpectedly slow spin-down rates observed in certain young pulsars, which are not in line with their anticipated ages. \n\nIn the context of pulsar magnetospheres, large-scale poloidal and toroidal currents are known to exist, playing a crucial role in shaping the characteristics of pulsar emissions, including pulse profiles and polarization (Goldreich & Julian, 1969; Ruderman, 1974; Melrose, 1995). These magnetic fields also influence the long-term evolution of pulsar periods through electromagnetic torques acting at the light cylinder (Spitkovsky, 2006). Previous theories suggested that toroidal fields could form during the creation of neutron stars via dynamo action driven by convection or differential rotation within the core (Thompson & Duncan, 1993). However, recent investigations have indicated that this mechanism may not generate sufficiently strong toroidal fields to align with observational data (Heger et al., 2005). \n\nAn alternative hypothesis posits that toroidal fields may develop through the winding of poloidal fields due to rapid crustal rotation (Braithwaite, 2009) or through differential rotation between normal liquid and superfluid elements in the star's interior (Srinivasan et al., 1991; Srinivasan, 1991a). In our research, we employ numerical simulations to explore the growth of toroidal fields under various physical conditions. Our results reveal that significant differential rotation between normal matter and superfluid components can lead to the rapid development of toroidal magnetic fields, thereby enhancing our understanding of the magnetic dynamics in type-II superconducting neutron stars.",
        "ori-fast-z-score": 0.24743582965269675,
        "water-fast-z-score": 6.620847108818944,
        "rewrite-fast-z-score": 1.7728105208558367
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Wang-Landau molecular dynamics technique to search for low-energy conformational space of proteins .\nAbstract:\nThe Wang-Landau (WL) algorithm is used in this work as an efficient tool to explore the low energy conformation space of proteins and predict their native structures. The WL method has been successfully applied to many systems, such as spin glasses, lattice gas models, protein folding problems etc., but it was never tested on realistic biomolecular systems before. In our study we have performed extensive tests with different types of proteins using both coarse-grained and all-atom force fields. We show that the WL sampling can be efficiently combined with standard Monte Carlo simulations or Molecular Dynamics runs at high temperatures. This allows us to obtain accurate estimates of free energies even when only relatively short trajectories are available. Our results demonstrate that the WL approach provides very good agreement between predicted and experimental structures. Finally, we discuss possible applications of the proposed methodology to other complex systems where the exploration of phase spaces plays important role.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Wang - Landau molecular dynamics technique to search for low - energy conformational space of proteins . Abstract : The Wang - Landau ( WL ) algorithm is utilized in this research as an efficient tool to examine the small power conformation space of proteins and predict their ancestral properties .The WL method has been successfully applied to many systems , such as spin glasses , lattice gas models , protein folding problems etc . , but it was none proven on real biomolecular systems before . In our research we have done extensive experiments with various types of proteins using both fine - grained and all - atom pressure waves .We see that the WL analysis can be easily combined with typical Monte Carlo simulations or Molecular Dynamics runs at high temperatures . This enables us to obtain precise estimates of free energies even when only relatively short trajectories are available .Our results show that the WL approach offers very high agreement between anticipated and experimental systems . Finally , we study possible applied of the suggested methodology to other complex systems where the exploration of phase spaces serves crucial role .",
        "rewrite_text": "In this study, we explore the application of the Wang-Landau (WL) algorithm as a powerful method for investigating the low-energy conformational space of proteins and inferring their ancestral characteristics. The WL algorithm has demonstrated its effectiveness across various systems, including spin glasses, lattice gas models, and protein folding challenges. However, its efficacy had not been validated on actual biomolecular systems prior to this research. We conducted comprehensive experiments on a diverse array of proteins, employing both fine-grained and all-atom pressure wave simulations. Our findings indicate that the WL method can be seamlessly integrated with conventional Monte Carlo simulations or Molecular Dynamics (MD) simulations at elevated temperatures. This integration allows for the accurate estimation of free energies, even when the available trajectories are relatively short. The results of our analysis reveal a remarkable correlation between the predicted outcomes and experimental data, underscoring the reliability of the WL approach in protein conformational studies. Furthermore, we discuss the potential applications of this methodology to other complex systems, where effective exploration of phase spaces is essential. This research not only enhances our understanding of protein dynamics but also opens avenues for future investigations into the conformational landscapes of various biomolecular systems.",
        "ori-fast-z-score": -0.8,
        "water-fast-z-score": 6.069726860280934,
        "rewrite-fast-z-score": -0.4082482904638631
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Free Energy of Activation for the Comorosan Effect .\nAbstract:\nThe free energy of activation (ΔG*) is calculated for the comorosan effect, which describes the formation of an intermediate state in the reaction between carbon dioxide and water to form carbonate ions.  The ΔG* value obtained by this method is compared with that determined by other methods such as calorimetry or electrochemistry. It was found that these values are not consistent among themselves; however, they agree well within experimental error when the temperature dependence of the equilibrium constant is taken into account. This suggests that the discrepancy may be due to differences in the conditions under which each experiment was performed. In addition, it has been shown that the ΔG* value depends on the nature of the solvent used in the experiments. Finally, we have proposed a mechanism for the comorosan process based on our results. The free energy of activation (∆G*) is calculated using the Arrhenius equation for the comorosan reaction, which describes the formation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Free Energy of Activation for the Comorosan Effect . Abstract : The free energy of activation ( ΔG * ) is calculated for the comorosan effect , which explains the formation of an intermediate state in the process between carbon dioxide and water to form carbonate compounds .The ΔG * value obtained by this process is compared with that determined by other methods such as calorimetry or electrochemistry . It was shown that these estimates are not consistent among themselves ; however , they accord well within experimental error when the temperature dependence of the equilibrium coefficient is taken into consideration .This implies that the discrepancy may be due to differences in the conditions under which each experiment was done . In addition , it has been shown that the ΔG * value depends on the nature of the solvent used in the experiments .Finally , we have proposed a mechanism for the comorosan process based on our results . The free energy of activation ( [UNK] * ) is calculated using the Arrhenius equation for the comorosan reaction , which describes the formation",
        "rewrite_text": "**Title:** Free Energy of Activation for the Comorosan Effect\n\n**Abstract:** This study presents a detailed calculation of the free energy of activation (ΔG*) associated with the comorosan effect, a phenomenon that elucidates the formation of an intermediate state during the conversion of carbon dioxide and water into carbonate compounds. The ΔG* value derived from our analysis is systematically compared with values obtained through alternative methodologies, including calorimetry and electrochemistry. Our findings reveal inconsistencies among these different estimation techniques; however, when the temperature dependence of the equilibrium constant is factored into the analysis, the results align within the bounds of experimental error. This suggests that the observed discrepancies may stem from variations in experimental conditions across different studies. Furthermore, our research indicates that the ΔG* value is influenced by the specific nature of the solvent employed in the experiments, highlighting the importance of solvent choice in thermodynamic calculations. Based on our comprehensive results, we propose a mechanistic framework for the comorosan process, which is grounded in the calculated free energy of activation. The Arrhenius equation has been utilized to derive the ΔG* for the comorosan reaction, providing a quantitative basis for understanding the kinetics of this transformation. Overall, this work contributes to the broader understanding of carbonate formation processes and offers insights into the factors that influence activation energy in chemical reactions involving carbon dioxide and water.",
        "ori-fast-z-score": 1.7801724872907798,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 0.19611613513818404
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  STBCs from Representation of Extended Clifford Algebras .\nAbstract:\nIn this work, we present the design and analysis of space-time block codes (STBC) based on extended clifford algebras. The proposed STBC is constructed by using an orthogonal basis for the underlying algebra. We show that our code has full diversity order with respect to quasi-static Rayleigh fading channels. Furthermore, it achieves maximum coding gain over all other known algebraic STBCs in terms of minimum determinant criterion. Finally, we provide simulation results which demonstrate the performance gains achieved by the proposed scheme compared to existing schemes. Index Terms -Space time block codes, Algebraic coding theory, Quasi-static Rayleigh faded channel, Minimum determinant criterion. I. INTRODUCTIO N Space Time Block Codes (STBC), introduced by Alamouti  1  , are widely used in wireless communications due to their simple structure and high data rate  2  . In recent years there have been many efforts devoted towards designing new classes of STBCs  3  -  8  .\nThe main goal behind these designs was to achieve higher spectral efficiency while maintaining good error performances under different transmission conditions such as multipath propagation or frequency selective fading  9  . However, most of the existing works focus only on achieving better error performances without considering the effect of hardware limitations  10  . This motivates us to develop a class of STBCs which can be implemented efficiently at low complexity  11  .\nRecently, several authors  12  -  14  have shown that some well-known families of finite fields like Galois field GF(q)  15  , Finite Ring  16  , Quaternion  17  etc., can also be represented by certain types of non-commutative rings called Clifford algebras  18  . These representations allow one to construct various signal constellations  19  , modulation techniques  20  , and communication systems  21  . Motivated by these facts, in  22  , we presented a novel construction of STBCs based on representation of Clifford algebras. It was shown that the proposed STBC provides significant improvement in bit error rates (BER) when compared to conventional STBCs  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : STBCs from Representation of Extended Clifford Algebras . Abstract : In this project , we present the development and evaluation of space - time block codes ( STBC ) based on extended clifford algebras .The proposed STBC is built by using an orthogonal basis for the underlying algebra . We see that our code has full diversity order with regard to quasi - static Rayleigh fading sources .Furthermore , it achieves peak code performance over all other established algebraic STBCs in terms of minimum determinant requirement . Finally , we provide simulation data which demonstrate the performance increases attained by the suggested system compared to existing schemes .Index Terms - Space time block codes , Algebraic coding theory , Quasi - static Rayleigh faded signal , Minimum determinant requirement . I . INTRODUCTIO N Space Time Block Codes ( STBC ) , invented by Alamouti 1 , are widely useful in mobile communications thanks to their simple shape and large data rate 2 .In past decades there have been many efforts devoted towards creating new classes of STBCs 3 - 8 . The main goal behind these designs was to achieve greater spectral capacity while maintaining good error performances under various transmission circumstances such as multipath propagation or bandwidth selective fading 9 .However , most of the new works concentrate only on achieving better error performances without assessing the impact of hardware constraints 10 . This motivates us to develop a class of STBCs which can be executed easily at low complexity 11 .Recently , various scientists 12 - 14 have shown that some well - famous families of finite fields like Galois field GF ( q ) 15 , Finite Ring 16 , Quaternion 17 etc . , can also be described by certain types of non - commutative rings called Clifford algebras 18 . These representations allow one to build diverse signal constellations 19 , modulation technology 20 , and communication devices 21 .Motivated by these facts , in 22 , we presented a new construction of STBCs based on representation of Clifford algebras . It was shown that the suggested STBC offers substantial improvement in bit error patterns ( BER ) when compared to conventional STBCs 23 .",
        "rewrite_text": "**Title:** Space-Time Block Codes Derived from Extended Clifford Algebras\n\n**Abstract:** This study focuses on the development and assessment of space-time block codes (STBCs) that are constructed using extended Clifford algebras. The proposed STBC utilizes an orthogonal basis derived from the underlying algebra, ensuring that it achieves full diversity order in the context of quasi-static Rayleigh fading channels. Notably, our code demonstrates superior peak performance compared to existing algebraic STBCs, particularly in terms of the minimum determinant criterion, which is crucial for ensuring reliable communication. To validate our theoretical findings, we present simulation results that illustrate the performance enhancements of our proposed system relative to established coding schemes. \n\nSpace-Time Block Codes, originally introduced by Alamouti, have gained significant traction in mobile communication systems due to their straightforward design and ability to support high data rates. Over the past few decades, extensive research has been dedicated to the creation of new STBC classes aimed at maximizing spectral efficiency while ensuring robust error performance across various transmission scenarios, including multipath propagation and bandwidth-selective fading. However, many recent studies have primarily focused on improving error rates without adequately considering the implications of hardware limitations. This gap in research has inspired us to develop a new class of STBCs that can be implemented with low computational complexity.\n\nRecent advancements have revealed that well-known finite field families, such as Galois fields and finite rings, can be represented through specific non-commutative structures known as Clifford algebras. These representations facilitate the design of diverse signal constellations and modulation techniques, enhancing communication systems. Building on this foundation, we introduced a novel construction of STBCs based on the representation of Clifford algebras, demonstrating significant improvements in bit error rate (BER) performance when compared to traditional STBCs. Our findings contribute to the ongoing evolution of coding strategies in wireless communications, paving the way for more efficient and reliable transmission methods. \n\n**Index Terms:** Space-Time Block Codes, Algebraic Coding Theory, Quasi-Static Rayleigh Fading, Minimum Determinant Requirement.",
        "ori-fast-z-score": -0.39405520311955033,
        "water-fast-z-score": 7.8628393360313815,
        "rewrite-fast-z-score": 0.3823595564509363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Integrable systems and complex geometry .\nAbstract:\nThe aim of this talk is to present some recent results on the relation between integrability in quantum field theory, statistical mechanics or mathematical physics and the existence of special geometric structures (complex algebraic curves) which are associated with these models. \nWe will discuss how such geometries can be used for solving exactly certain physical problems by using techniques coming from algebraic geometry like Riemann surfaces, theta functions etc.. We will also explain why it seems that many interesting integrable models have an underlying structure of a Riemann surface. Finally we will give examples where this connection has been made explicit. The talk will include several new results obtained recently by the author together with his collaborators. This work was partially supported by the DFG under grant SFB/TR9. Integrable systems play an important role in various branches of mathematics as well as theoretical physics. In particular they appear naturally when studying solvable lattice models in statistical mechanics or quantum field theories. It turns out that there exists a deep relationship between integrability and the presence of special geometric structures called algebraic curves. These curves provide powerful tools for solving exactly certain physical questions via methods from algebraic geometry like Riemann Surfaces, Theta Functions,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Integrable systems and complex geometry . Abstract : The goal of this talk is to provide some latest findings on the relation between integrability in quantum field theory , statistical mechanics or computational physics and the existence of unique geometric objects ( complex algebraic shapes ) which are identified with these models .We will explore how such geometries can be used for solving exactly certain physical problems by using techniques come from algebraic topology like Riemann surfaces , theta functions etc . . We will also explain why it appears that several interesting integrable models have an underlying composition of a Riemann surface . Finally we will provide examples where this link has been made explicit .The talk will include several current data acquired previously by the writer together with his collaborators . This project was partially backed by the DFG under grant SFB / TR9 .Integrable systems play an important role in different areas of math as well as conceptual science . In particular they appear naturally when examining solvable lattice models in statistical mechanics or quantum field theories .It turns out that there exists a profound connection between integrability and the presence of unique geometric objects called algebraic curves . These curves provide potent tools for solving exactly certain physical problems via tools from algebraic topology like Riemann Surfaces , Theta Functions , . . .",
        "rewrite_text": "**Title: Integrable Systems and Complex Geometry**\n\n**Abstract:** This presentation aims to discuss recent advancements in understanding the relationship between integrability in quantum field theory, statistical mechanics, and computational physics, and the existence of unique geometric entities known as complex algebraic curves associated with these models. We will delve into how these geometric structures can facilitate the exact resolution of specific physical challenges by employing methods derived from algebraic topology, including Riemann surfaces and theta functions. Furthermore, we will elucidate the observation that numerous intriguing integrable models exhibit an inherent composition of Riemann surfaces. To illustrate this connection, we will present explicit examples where this relationship has been clearly established. The talk will incorporate a wealth of current data gathered by the author and collaborators, highlighting the collaborative nature of this research. This project has received partial support from the DFG under grant SFB/TR9. Integrable systems are pivotal in various mathematical domains and theoretical sciences, particularly emerging naturally in the study of solvable lattice models within statistical mechanics and quantum field theories. Our findings suggest a deep-seated link between integrability and the existence of unique geometric constructs, specifically algebraic curves. These curves serve as powerful instruments for achieving exact solutions to certain physical problems, leveraging the rich framework provided by algebraic topology, including Riemann surfaces and theta functions. Through this exploration, we aim to shed light on the intricate interplay between geometry and integrability, offering insights that could pave the way for future research in these interconnected fields.",
        "ori-fast-z-score": -0.7364596943186588,
        "water-fast-z-score": 6.141879930089016,
        "rewrite-fast-z-score": 0.26013299085723596
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SUSY-GUTs, SUSY-Seesaw and the Neutralino Dark Matter .\nAbstract:\nWe present an overview on supersymmetric grand unified theories (SUSY-GUT), their connection to neutrino masses via seesaw mechanisms as well as dark matter candidates in these models. We discuss how GUT scale physics can be probed at future colliders such as LHC or ILC. Finally we give some examples for specific realizations within SO(10) and E6 gauge groups. Supersymmetry is one of the most promising extensions beyond the Standard Model which addresses many open questions like the hierarchy problem between electroweak and Planck scales, unification of forces etc.. In addition it provides a natural candidate for cold dark matter -the lightest neutralino. The minimal supersymmetric standard model (MSSM) has been studied extensively over the last two decades but suffers from several shortcomings. One of them is that the MSSM does not provide any explanation why there are three generations of quarks and leptons with different quantum numbers. Grand Unified Theories (GUTs) address this issue by postulating that all known particles including those of the third generation belong to multiplets of larger symmetry group than SU(3)xSU(2)xU(1). This leads naturally to relations among coupling constants and fermion mass matrices. Another shortcoming of the MSSM is that it cannot explain small neutrino masses observed experimentally. However, if R-parity is broken then Majorana neutrinos may acquire tiny masses through see-saw mechanism. These new states could also contribute significantly to the relic density of dark matter.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SUSY - GUTs , SUSY - Seesaw and the Neutralino Dark Matter . Abstract : We bring an overview on supersymmetric grand unified theories ( SUSY - GUT ) , their connection to neutrino masses via seesaw processes as well as dark matter candidates in these models .We discuss how GUT scale physics can be probed at possible colliders such as LHC or ILC . Finally we give some examples for specific realizations within SO ( 10 ) and E6 gauge bands .Supersymmetry is one of the most exciting extensions beyond the Standard Model which answers many open questions like the hierarchy problem between electroweak and Planck scales , unification of forces etc . . In addition it gives a natural candidate for cold dark matter - the lightest neutralino .The minimal supersymmetric standard description ( MSSM ) has been studied thoroughly over the last two decades but suffers from several shortcomings . One of them is that the MSSM does not offer any explanation why there are three generations of quarks and leptons with varying quantum numbers .Grand Unified Theories ( GUTs ) address this question by postulating that all known objects including those of the third generation belong to multiplets of bigger symmetry class than SU ( 3 ) xSU ( 2 ) xU ( 1 ) . This leads naturally to relations among coupling constants and fermion mass matrices .Another shortcoming of the MSSM is that it fails explain little neutrino masses seen experimentally . However , if R - parity is shattered then Majorana neutrinos might acquire small masses through see - saw mechanism .These new states could also contribute considerably to the relic volume of dark matter .",
        "rewrite_text": "**Title:** SUSY-GUTs, SUSY-Seesaw, and Neutralino Dark Matter\n\n**Abstract:** This article provides a comprehensive overview of supersymmetric grand unified theories (SUSY-GUTs) and their implications for neutrino masses through seesaw mechanisms, as well as the potential dark matter candidates within these frameworks. We explore how the physics at the GUT scale can be investigated at future colliders, such as the Large Hadron Collider (LHC) and the International Linear Collider (ILC). Additionally, we present specific examples of realizations within the SO(10) and E6 gauge groups. Supersymmetry represents a compelling extension of the Standard Model, addressing several unresolved issues, including the hierarchy problem between the electroweak and Planck scales and the unification of fundamental forces. A significant aspect of SUSY is its provision of a natural candidate for cold dark matter, specifically the lightest neutralino. While the minimal supersymmetric standard model (MSSM) has been extensively analyzed over the past two decades, it is not without its limitations. Notably, the MSSM does not elucidate the existence of three generations of quarks and leptons with distinct quantum numbers. Grand Unified Theories (GUTs) tackle this challenge by proposing that all known particles, including those from the third generation, belong to multiplets of a larger symmetry group than SU(3) × SU(2) × U(1), which results in inherent relationships among coupling constants and fermion mass matrices. Furthermore, the MSSM's inability to account for the small neutrino masses observed in experiments is another significant drawback. However, if R-parity is violated, Majorana neutrinos could acquire small masses via the seesaw mechanism, and these new states may significantly contribute to the overall dark matter density in the universe. This article aims to bridge these concepts and highlight the interconnectedness of SUSY-GUTs, neutrino physics, and dark matter in the quest for a more unified understanding of fundamental particles and forces.",
        "ori-fast-z-score": 0.5940885257860046,
        "water-fast-z-score": 5.346796732074042,
        "rewrite-fast-z-score": 0.8250286473253902
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vortices in Bose-Einstein Condensates: Theory .\nAbstract:\nWe present an overview of the theory for vortices in trapped, dilute atomic gases at low temperatures. We discuss how these systems can be described by macroscopic wave functions and show that they are governed by nonlinear Schrödinger equations with external potentials. The solutions to this equation have been studied extensively over many years and we review some of their properties relevant to vortex formation. In particular, we consider stationary states which correspond to condensate configurations without rotation (vortex-free) as well as rotating ones where quantized angular momentum is carried by phase singularities known as vortices. Finally, we briefly describe recent experiments on vortex production in cold atom clouds. Vortices occur naturally in superfluids such as liquid helium or dilute atomic gases. They carry quantized angular momenta and play important roles in various physical phenomena including turbulence and quantum transport processes. Here we give an introduction into the theoretical description of vortices in trapped atomic gases.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vortices in Bose - Einstein Condensates : Theory . Abstract : We present an overview of the principle for vortices in trapped , dilute atomic materials at low temperatures .We discuss how these systems can be described by macroscopic wave functions and find that they are governed by nonlinear Schrödinger coefficients with external potentials . The solutions to this equation have been studied frequently over numerous years and we review some of their characteristics applicable to vortex structure .In particular , we define stationary states which correspond to condensate configurations without rotation ( vortex - safe ) as well as rotating ones where quantized angular velocity is carried by phase singularities known as vortices . Finally , we briefly describe recent experiments on vortex production in cold hydrogen clusters .Vortices arise naturally in superfluids such as fluid helium or dilute nuclear gases . They carry quantized angular momenta and play essential roles in different mechanical phenomena including turbulence and quantum transport systems .Here we give an overview into the theoretical theory of vortices in trapped atomic gases .",
        "rewrite_text": "**Title:** Vortices in Bose-Einstein Condensates: Theory\n\n**Abstract:** This article provides a comprehensive overview of the theoretical framework surrounding vortices in trapped, dilute atomic gases at low temperatures. We explore the description of these systems through macroscopic wave functions, revealing that their dynamics are governed by nonlinear Schrödinger equations influenced by external potentials. Over the years, numerous studies have examined the solutions to these equations, and we summarize key characteristics relevant to vortex formation. Specifically, we distinguish between stationary states that correspond to non-rotating condensate configurations—termed vortex-safe states—and those that exhibit rotation, characterized by quantized angular momentum carried by phase singularities known as vortices. Furthermore, we highlight recent experimental advancements in the production of vortices within cold hydrogen clusters. Vortices are a fundamental feature of superfluid systems, such as liquid helium and dilute nuclear gases, where they are associated with quantized angular momentum and play pivotal roles in various mechanical phenomena, including turbulence and quantum transport. This article aims to elucidate the theoretical underpinnings of vortex behavior in trapped atomic gases, contributing to a deeper understanding of their significance in both fundamental physics and potential applications in quantum technologies.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 4.313310928137536,
        "rewrite-fast-z-score": 0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Classical approach to the graph isomorphism problem using quantum walks .\nAbstract:\nWe present an algorithm for solving the Graph Isomorphism Problem (GIP) based on Quantum Walks and Grover s search algorithm. The proposed method is inspired by the classical algorithms that use random walks, but it uses Grover s operator instead of the Hadamard matrix in order to speed up the process. We show how this new technique can be used to solve GIP with high probability when the number of vertices in both graphs are equal or differ at most one unit. Finally we compare our results against other state-of-the-art methods. In recent years there has been growing interest in developing efficient solutions to problems related to computational complexity theory such as the Graph Isomorphism Problem(GIP). This problem consists of determining whether two given graphs are isomorphic or not; i.e., if they have the same structure regardless of their labels. \n \n Classical approaches to solve GIP usually rely on Random Walks techniques combined with some heuristics. However these methods require exponential time in the worst case scenario. On the other hand, Quantum Algorithms provide polynomial-time solutions to many NP-complete problems including GIP. These algorithms exploit the superposition principle which allows them to explore all possible states simultaneously. For example, Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any item within a database in quadratic time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Classical approach to the graph isomorphism question involving quantum walks . Abstract : We present an algorithm for solving the Graph Isomorphism Problem ( GIP ) based on Quantum Walks and Grover s search algorithm .The proposed approach is influenced by the classical techniques that use random tours , but it utilizes Grover s operator rather of the Hadamard vector in order to faster up the process . We see how this new technique can be used to solve GIP with high probability when the number of vertices in both graphs are equal or differ at most one unit .Finally we compare our findings against other state - of - the - art methods . In past decades there has been growing interest in building successful solutions to problems related to computational complexity analysis such as the Graph Isomorphism Problem ( GIP ) .This problem consists of determining whether two given graphs are isomorphic or not ; i . e . , if they have the same structure regardless of their labels . Classical approaches to solve GIP usually relies on Random Walks techniques combined with some heuristics .However these algorithms involve exponential time in the worst case situations . On the other hand , Quantum Algorithms provide polynomial - time solutions to many NP - perfect issues including GIP .These methods incorporate the superposition concept which allows them to examine all possible states simultaneously . For instance , Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any element within a computer in quadratic time .",
        "rewrite_text": "We introduce a novel algorithm aimed at addressing the Graph Isomorphism Problem (GIP) by leveraging Quantum Walks in conjunction with Grover's search algorithm. This innovative approach draws inspiration from classical methods that employ random tours but enhances the efficiency of the process by utilizing Grover's operator instead of the traditional Hadamard vector. Our findings demonstrate that this new technique can effectively solve the GIP with a high probability, particularly when the number of vertices in the two graphs is either equal or differs by no more than one. \n\nThe Graph Isomorphism Problem has garnered significant attention over the past few decades, particularly in the context of computational complexity analysis. The core challenge of GIP lies in determining whether two graphs are isomorphic, meaning they share the same structural properties despite potential differences in labeling. Traditional solutions to GIP often rely on Random Walk techniques combined with various heuristics; however, these classical algorithms can exhibit exponential time complexity in the worst-case scenarios.\n\nIn contrast, Quantum Algorithms offer polynomial-time solutions for numerous NP-complete problems, including GIP. These quantum methods capitalize on the principle of superposition, enabling them to explore multiple states concurrently. For example, Shor's Algorithm efficiently tackles integer factorization in polynomial time, while Grover's Search algorithm allows for the identification of any element within a dataset in quadratic time. \n\nIn our study, we compare the performance of our proposed algorithm against existing state-of-the-art methods, highlighting its advantages and potential applications. By integrating quantum principles into the analysis of graph isomorphism, we aim to contribute to the ongoing discourse surrounding computational complexity and the efficacy of quantum computing in solving traditionally challenging problems.",
        "ori-fast-z-score": -0.5669467095138409,
        "water-fast-z-score": 5.550253123463223,
        "rewrite-fast-z-score": 0.43033148291193524
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Performance Evaluation of Encounter-based Worm Interactions Based on Node Characteristics .\nAbstract:\nIn this work, we propose an encounter based worm interaction model to evaluate the performance of different node characteristics in terms of their ability to detect and prevent worms spreading over mobile ad hoc networks (MANETs). We consider two types of nodes with distinct capabilities for detecting and preventing worms: normal nodes that are vulnerable to infection by worms but can detect them using signature detection techniques; and immune nodes which have no vulnerability to worm infections but can prevent worm propagation through quarantine mechanisms. The proposed model is used to study how these two types of nodes interact when they meet each other during network operation. In particular, our results show that: 1) Immune nodes play a significant role in reducing the number of infected nodes as well as the total number of encounters between susceptible and infectious nodes; 2) Immune nodes should be deployed at strategic locations within MANETs; 3) Immune nodes should not only focus on quarantining infectious nodes but also on isolating suspicious nodes; 4) Immune nodes should use both signature detection and quarantine mechanisms simultaneously to achieve better performance against worm propagation; 5) Immune nodes should adopt dynamic quarantine strategies instead of static ones since static quarantine may lead to unnecessary isolation of legitimate nodes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Performance Evaluation of Encounter - based Worm Interactions Based on Node Characteristics . Abstract : In this project , we develop an encounter based worm activity model to analyze the performance of different node characteristics in terms of their ability to identify and avoid worms distribution over mobile ad hoc sites ( MANETs ) .We consider two forms of nodes with distinct capabilities for detecting and preventing worms : normal networks that are susceptible to disease by viruses but can identify them utilizing signature detection methods ; and immune nodes which have no sensitivity to virus diseases but can prevent worm transmission through quarantine mechanisms . The proposed theory is utilized to study how these two kind of nodes interact when they meet each other during network installation .In particular , our findings show that : 1 ) Immune nodes play a substantial importance in reducing the quantity of infected nodes as well as the total quantity of encounters between vulnerable and infectious nodes ; 2 ) Immune nodes should be deployed at strategic locations within MANETs ; 3 ) Immune nodes should not only focus on quarantining infectious nodes but also on isolating suspect nodes ; 4 ) Immune nodes should use both signature detection and quarantine mechanisms separately to achieve good efficiency against worm transmission ; 5 ) Immune nodes should implement dynamic quarantine techniques instead of static ones since static quarantine may lead to inappropriate isolation of genuine nodes .",
        "rewrite_text": "**Title:** Performance Evaluation of Encounter-Based Worm Interactions Based on Node Characteristics\n\n**Abstract:** This study presents an encounter-based model for analyzing worm activity within mobile ad hoc networks (MANETs), focusing on the performance of various node characteristics in their capacity to detect and mitigate worm propagation. We categorize nodes into two distinct types: normal nodes, which are vulnerable to viral infections but can utilize signature detection methods to identify threats, and immune nodes, which are resistant to viral infections and can prevent worm spread through quarantine strategies. Our research investigates the interactions between these two node types during network formation and operation. \n\nThe results reveal several critical insights: first, immune nodes significantly reduce the number of infected nodes and the overall encounters between susceptible and infectious nodes. This highlights their crucial role in maintaining network integrity. Second, the strategic placement of immune nodes within the MANET is essential for maximizing their effectiveness. Third, immune nodes should extend their focus beyond merely quarantining infected nodes to also include the isolation of potentially suspicious nodes, thereby enhancing overall network security. \n\nMoreover, our findings suggest that immune nodes should employ both signature detection and quarantine methods independently to optimize their efficiency in combating worm transmission. Lastly, we advocate for the implementation of dynamic quarantine strategies over static ones, as static measures may inadvertently isolate legitimate nodes, leading to unnecessary disruptions in network functionality. This research underscores the importance of node characteristics in the fight against worms in MANETs and provides a framework for improving network resilience through strategic node deployment and interaction management.",
        "ori-fast-z-score": -1.6431676725154982,
        "water-fast-z-score": 5.7272727272727275,
        "rewrite-fast-z-score": 1.7439550769285392
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extraction of Freshwater and Energy from Atmosphere .\nAbstract:\nThe extraction of fresh water and energy from the atmosphere is proposed as an alternative to conventional sources, which are limited in supply or environmentally damaging.  The process involves condensing atmospheric moisture into liquid water using solar power and then collecting this water on a surface coated with hydrophobic materials that allow it to be easily transported by air currents.   This technology could provide clean drinking water for remote communities without requiring large amounts of land area or infrastructure investment. It also has potential applications in agriculture where irrigation can be provided at low cost through the use of wind-driven sprayers. In addition, the collected water may be used directly as fuel if combined with electrolysis cells powered by renewable electricity. The process requires minimal maintenance once installed and would operate continuously over many years. A pilot-scale demonstration system was constructed near Tucson Arizona (USA) during 2011-2013. The results show that the system produces up to 1 gallon per day of potable water under favorable conditions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Extraction of Freshwater and Energy from Atmosphere . Abstract : The extraction of fresh water and energy from the atmosphere is proposed as an alternative to conventional sources , which are limited in supply or environmentally destructive .The method means condensing ambient humidity into liquid water use solar electricity and then collecting this water on a surface coated with hydrophobic materials that enable it to be easily carried by air waves . This system could supply fresh drinking water for isolated communities without using small amounts of property area or construction investment .It additionally has potential applications in farming where irrigation can be provided at low cost through the using of wind - powered sprayers . In addition , the stored water may be used directly as fuel if combined with electrolysis panels driven by renewable energy .The method needs minimal repair once implemented and might run constantly over much years . A pilot - scale test system was constructed near Tucson Arizona ( USA ) during 2011 - 2013 .The results show that the scheme produces up to 1 gallon per day of potable liquid under favorable conditions .",
        "rewrite_text": "Title: Extraction of Freshwater and Energy from the Atmosphere\n\nAbstract: The extraction of freshwater and energy from atmospheric sources presents a promising alternative to traditional methods that are often limited in availability or detrimental to the environment. This innovative approach involves the condensation of ambient humidity into liquid water, utilizing solar energy to power the process. The collected water is then gathered on surfaces treated with hydrophobic materials, which facilitate its transport via air currents. This system holds significant potential for providing potable water to remote communities, requiring minimal land area and construction investment. Furthermore, it offers valuable applications in agriculture, where it can enable low-cost irrigation through the use of wind-powered sprayers. Additionally, the harvested water can be converted into fuel when paired with electrolysis panels powered by renewable energy sources. Once established, this method requires minimal maintenance and can operate continuously for many years. A pilot-scale testing system was implemented near Tucson, Arizona, from 2011 to 2013, yielding promising results. Under optimal conditions, the system demonstrated the capability to produce up to one gallon of drinkable water per day. This research underscores the viability of harnessing atmospheric resources for sustainable freshwater and energy solutions, highlighting its potential to address pressing water scarcity issues while minimizing environmental impact.",
        "ori-fast-z-score": -1.1055415967851332,
        "water-fast-z-score": 6.2,
        "rewrite-fast-z-score": -0.5669467095138409
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spin Effect on the Resonant Tunneling Characteristics of a Double-Barrier Heterostructures Under Longitudinal Stresses .\nAbstract:\nIn this work, we study spin effect on resonant tunneling characteristics in double-barrier heterostructure under longitudinal stresses by using transfer matrix method and density functional theory (DFT). We find that the energy gap decreases with increasing stress for both majority-spin electrons and minority-spin holes. The decrease is more significant for minority-spin holes than for majority-spin electrons. In addition, the transmission coefficient increases with increasing stress at low bias voltage but decreases at high bias voltage. This behavior can be explained as follows. At low bias voltage, the increase of transmission coefficient results mainly from the reduction of barrier height due to compressive stress. However, at high bias voltage, the decrease of transmission coefficient comes from two factors: one is the increase of effective mass induced by tensile stress; another is the enhancement of electron-phonon interaction caused by tensile stress. Finally, it should be noted that our calculation shows that the spin-orbit coupling has little influence on the transport properties of the system considered here.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spin Effect on the Resonant Tunneling Characteristics of a Double - Barrier Heterostructures Under Longitudinal Stresses . Abstract : In this research , we study spinning effect on resonant tunneling parameters in double - barrier heterostructure under longitudinal strain by using transfer matrix method and density functional theory ( DFT ) .We see that the electricity gap decreases with increasing stress for both majority - spin atoms and minority - spin holes . The reduction is more significant for minority - spinning holes than for majority - spin electrons .In addition , the transmission coefficient increases with increasing stress at low bias frequency but decreases at high bias voltage . This phenomenon can be understood as follows .At small bias frequency , the improvement of transmission coefficient results mainly from the reduction of barrier thickness due to compressive stress . However , at high bias voltage , the decrease of transmission coefficient happens from two factors : one is the improvement of effective mass induced by tensile tension ; another is the enhancement of electron - phonon interaction due by tensile tension .Finally , it should be mentioned that our calculation demonstrates that the spin - orbit interaction has little influence on the travel properties of the system discussed here .",
        "rewrite_text": "In this study, we investigate the impact of spin effects on the resonant tunneling characteristics of double-barrier heterostructures subjected to longitudinal strain, employing both the transfer matrix method and density functional theory (DFT). Our findings reveal that the energy gap diminishes as longitudinal stress increases, affecting both majority-spin electrons and minority-spin holes. Notably, the reduction in the energy gap is more pronounced for minority-spin holes compared to majority-spin electrons. Furthermore, we observe that the transmission coefficient exhibits an increase with rising stress at low bias frequencies, while it shows a decrease at high bias voltages. This behavior can be attributed to the interplay between barrier thickness and stress conditions. At low bias frequencies, the enhancement of the transmission coefficient is primarily due to the reduction in barrier thickness resulting from compressive stress. Conversely, at high bias voltages, the decline in the transmission coefficient is influenced by two main factors: the increase in effective mass caused by tensile stress and the intensified electron-phonon interactions that arise under tensile conditions. Additionally, our calculations indicate that spin-orbit interaction has a negligible effect on the transport properties of the heterostructure under investigation. This research contributes to a deeper understanding of the spin-dependent phenomena in resonant tunneling processes, particularly in the context of strained double-barrier heterostructures, and highlights the significance of longitudinal stress in modulating electronic properties. The implications of these findings are relevant for the design and optimization of spintronic devices and other applications where resonant tunneling plays a critical role.",
        "ori-fast-z-score": 0.19802950859533489,
        "water-fast-z-score": 5.6163768855364715,
        "rewrite-fast-z-score": 1.647508942095828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tests of Bayesian Model Selection Techniques for Gravitational Wave Astronomy .\nAbstract:\nGravitational wave astronomy is an emerging field that will provide new insights into the universe and its fundamental laws through observations of gravitational waves emitted by merging black holes, neutron stars or other compact objects in distant galaxies.  In this work we present several tests of different techniques used to select between competing models describing the observed data. We consider two examples where the signal-to-noise ratio (SNR) of the detected signals are low enough so that it becomes difficult to distinguish between different physical scenarios using standard frequentist hypothesis testing methods. The first example considers the problem of distinguishing between binary black hole systems with spin aligned versus anti-aligned with their orbital angular momentum vector. The second example considers the problem of determining whether a given source has been emitting gravitational radiation continuously over time as opposed to being active only during short bursts. For both cases we compare results obtained using three different model selection methods: Akaike s information criterion (AIC), Bayes factors computed via nested sampling (NS-BF), and the Deviance Information Criterion (DIC).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tests of Bayesian Model Selection Techniques for Gravitational Wave Astronomy . Abstract : Gravitational wave astronomy is an developing field that will provide fresh insights into the universe and its essential laws through observations of gravitational waves emitted by merging black holes , neutron galaxies or other compact entities in nearby galaxies .In this research we present many tests of different methods used to select between various models explaining the reported information . We consider two examples where the signal - to - noise proportion ( SNR ) of the emitted signals are low enough so that it becomes impossible to distinguish between various physical scenarios using conventional frequentist hypothesis testing technique .The first instance involves the question of distinguishing between binary dark hole structures with spin aligned versus counter - aligned with their orbital angular velocity tensor . The second example considers the issue of determining whether a given source has been emitting gravitational rays continuously over time as opposed to being active only during short pulses .For both cases we compare findings obtained using three different model selection approaches : Akaike s information criterion ( AIC ) , Bayes criteria computed via nested filtering ( NS - BF ) , and the Deviance Information Criterion ( DIC ) .",
        "rewrite_text": "Title: Evaluating Bayesian Model Selection Techniques in Gravitational Wave Astronomy\n\nAbstract: Gravitational wave astronomy is an emerging discipline poised to enhance our understanding of the universe and its fundamental principles through the detection of gravitational waves produced by the mergers of black holes, neutron stars, and other compact objects in nearby galaxies. This study investigates various Bayesian model selection techniques to analyze and interpret the data collected from these astronomical events. We focus on two specific scenarios where the signal-to-noise ratio (SNR) of the detected signals is sufficiently low, rendering traditional frequentist hypothesis testing methods ineffective for distinguishing between competing physical models. The first scenario examines the challenge of differentiating between binary black hole systems with spins that are either aligned or counter-aligned with their orbital angular momentum. The second scenario addresses the question of whether a gravitational wave source is continuously emitting signals over an extended period or is only active during brief, intermittent bursts. To evaluate the effectiveness of different model selection strategies, we compare the results obtained from three distinct approaches: the Akaike Information Criterion (AIC), Bayesian criteria derived from nested sampling (NS-BF), and the Deviance Information Criterion (DIC). Through this comparative analysis, we aim to highlight the strengths and limitations of each method in the context of gravitational wave data, ultimately contributing to the development of more robust analytical frameworks for future gravitational wave observations. Our findings underscore the importance of selecting appropriate model selection techniques in order to accurately interpret the complex signals associated with gravitational wave events and to advance our understanding of the underlying astrophysical processes.",
        "ori-fast-z-score": -0.6704783996548059,
        "water-fast-z-score": 6.674238124719146,
        "rewrite-fast-z-score": -0.5144957554275265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Principal Component Analysis and Automatic Relevance Determination in Damage Identification .\nAbstract:\nThe damage identification problem is formulated as an inverse problem, where the objective is to identify the location and severity of damages by minimizing the difference between simulated responses using finite element analysis (FEA) and measured data. The number of unknowns can be very large due to the presence of multiple sensors or measurement points. In this study, we propose two methods for reducing the dimensionality of the problem: principal component analysis (PCA), which reduces the dimension of the response space; and automatic relevance determination (ARD), which reduces the dimensions of both the input parameter space and the output response space simultaneously. Both PCA and ARD are implemented within the Bayesian framework so that uncertainties associated with these reduction techniques can also be accounted for during the optimization process. A numerical example involving a cantilever beam subjected to static loading is presented to demonstrate the effectiveness of the proposed approaches. Principal component analysis (PCA) and automatic relevance determination (AR",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Principal Component Analysis and Automatic Relevance Determination in Damage Identification . Abstract : The damage identification question is implemented as an inverse question , where the objective is to identify the location and intensity of damages by minimizing the difference between simulated responses utilizing finite element assessment ( FEA ) and measured data .The amount of unknowns can be very huge resulting to the presence of multiple sensors or observation points . In this study , we propose two strategies for decreasing the dimensionality of the issue : principal component analysis ( PCA ) , which reduces the dimension of the response space ; and electronic relevance determination ( ARD ) , which reduces the sizes of both the input parameter space and the output response space simultaneously .Both PCA and ARD are implemented within the Bayesian framework so that uncertainties involved with these reduction techniques can also be accounted for during the optimization process . A numerical example featuring a cantilever beam subjected to static loading is provided to indicate the ability of the suggested techniques .Principal part examination ( PCA ) and automatic relevance determination ( AR",
        "rewrite_text": "**Title:** Principal Component Analysis and Automatic Relevance Determination in Damage Identification\n\n**Abstract:** This article addresses the challenge of damage identification through an inverse problem framework, aiming to ascertain both the location and severity of damages by minimizing discrepancies between simulated responses derived from finite element analysis (FEA) and actual measured data. The complexity of this problem is heightened by the potentially vast number of unknowns, often necessitating the use of multiple sensors or observation points. To tackle this issue, we introduce two effective strategies for dimensionality reduction: Principal Component Analysis (PCA), which streamlines the response space, and Automatic Relevance Determination (ARD), which concurrently reduces the dimensions of both the input parameter space and the output response space. Both methodologies are integrated within a Bayesian framework, allowing for the incorporation of uncertainties associated with these reduction techniques during the optimization process. To demonstrate the efficacy of the proposed approaches, we present a numerical example involving a cantilever beam subjected to static loading conditions. The results illustrate the capabilities of PCA and ARD in enhancing the accuracy and efficiency of damage identification, ultimately contributing to more reliable structural health monitoring systems. By leveraging these advanced statistical techniques, our study not only simplifies the damage identification process but also provides a robust mechanism for addressing the inherent uncertainties in the data, paving the way for improved predictive maintenance and safety assessments in engineering applications.",
        "ori-fast-z-score": 0.32539568672798425,
        "water-fast-z-score": 5.391638660171921,
        "rewrite-fast-z-score": 1.2451741707874968
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The core binary fractions of star clusters from realistic simulations .\nAbstract:\nWe present the results of N-body simulations for open and globular star clusters with different initial conditions, including primordial binaries in various proportions (from 0 to 100%). We find that the fraction of binaries among all stars decreases as the cluster evolves due to dynamical interactions between single and binary stars. The decrease is more pronounced if there are initially many hard binaries or few soft ones. In addition, we show how the number of binaries depends on their binding energy distribution at birth. Finally, we compare our results with observations of real open and globular clusters. Our main conclusions are:  1) Open clusters have fewer binaries than globulars because they lose most of them during early evolution.  2) Binaries can be destroyed by three-body encounters even when the total number of binaries remains constant.  3) Hard binaries dominate over soft ones after several relaxation timescales t rh .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The fundamental binary fractions of star clusters from realistic simulations . Abstract : We report the results of N - bodies simulations for open and globular star clusters with various initial conditions , including primordial binaries in different proportions ( from 0 to 100 % ) .We see that the fraction of binaries among all stars reduces as the cluster evolves due to dynamical interactions between single and binary stars . The reduction is more pronounced if there are initially multiple hard binaries or few hard ones .In addition , we determine how the number of binaries depends on their binding energy density at birth . Finally , we compare our findings with observations of real open and globular complexes .Our main results are : 1 ) Open clusters have fewer binaries than globulars because they losing most of them during early evolution . 2 ) Binaries can be killed by three - bodies interactions even when the total number of binaries remains constant .3 ) Hard binaries dominate over soft ones after many relaxation timescales t rh .",
        "rewrite_text": "We present the findings of our N-body simulations conducted on both open and globular star clusters, exploring a range of initial conditions that include primordial binaries at varying proportions from 0% to 100%. Our analysis reveals a significant decrease in the fraction of binaries as the cluster evolves, a trend attributed to the dynamical interactions between single stars and binary systems. This decline in binary fraction is notably more pronounced in clusters that initially contain multiple hard binaries compared to those with fewer hard binaries. Furthermore, we investigate the relationship between the number of binaries and their binding energy density at formation, providing insights into the mechanisms that govern binary survival in these stellar environments. \n\nIn comparing our simulation results with observational data from actual open and globular clusters, we draw several key conclusions: Firstly, open clusters exhibit a lower binary fraction than globular clusters, primarily due to the loss of binaries during their early evolutionary stages. Secondly, we find that binaries can be disrupted through three-body interactions, even in scenarios where the overall binary count remains unchanged. Lastly, our simulations indicate that hard binaries tend to dominate over soft binaries after a significant number of relaxation timescales (t_rh). These findings contribute to a deeper understanding of the dynamical processes affecting binary star populations within star clusters and highlight the complex interplay between initial conditions and evolutionary outcomes.",
        "ori-fast-z-score": -1.835325870964494,
        "water-fast-z-score": 3.3048567173295003,
        "rewrite-fast-z-score": 1.4855627054164149
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The dust temperatures of the prestellar cores in the rho Oph main cloud and in other star forming regions: consequences for the core mass function .\nAbstract:\nWe present Herschel Space Observatory observations at 70, 160, 250, 350, and 500 microns toward two fields centered on the densest parts of the Rho Ophiuchi (RO) molecular cloud complex. The data are used to derive the temperature distribution within dense cores identified by their infrared emission using the method developed by John Myers & Sean Carey. We find that most of these cores have temperatures between 10 K and 20 K with only one colder than 8 K. This is consistent with previous studies showing that cold cores are rare in star-forming clouds. Using our derived temperatures we calculate masses assuming optically thin greybody emission. These masses range from 0.1 Msun to more than 100 Msun. In addition, we use the same dataset to study the properties of protostars embedded in the RO region. We identify 16 Class I sources based on their spectral energy distributions and compare them to those found in other nearby star-forming regions such as Serpens South or Orion B North.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The dust conditions of the prestellar cores in the rho Oph primary cloud and in other star producing regions : effects for the primary mass function . Abstract : We report Herschel Space Observatory images at 70 , 160 , 250 , 350 , and 500 microns toward two fields centered on the densest parts of the Rho Ophiuchi ( RO ) molecular dust complex .The data are using to derive the temperature distribution within dense cores identified by their infrared absorption use the method developed by John Myers & Sean Carey . We see that most of these cores have temperatures between 10 K and 20 K with only one colder than 8 K . This is consistent with previous research indicating that cool cores are scarce in star - creating clouds .Using our derived temperatures we determine masses assuming optically thin greybody emission . These masses range from 0 . 1 Msun to more than 100 Msun .In addition , we using the same dataset to study the properties of protostars embedded in the RO region . We recognize 16 Class I sources based on their spectral power distributions and contrast them to those present in other nearby star - creating areas such as Serpens South or Orion B North .",
        "rewrite_text": "**Title:** The Dust Conditions of Prestellar Cores in the Rho Oph Primary Cloud and Other Star-Forming Regions: Implications for the Primary Mass Function\n\n**Abstract:** In this study, we present observations from the Herschel Space Observatory, capturing images at wavelengths of 70, 160, 250, 350, and 500 microns, focused on two fields within the densest regions of the Rho Ophiuchi (RO) molecular dust complex. Utilizing these data, we analyze the temperature distribution of dense cores identified through their infrared absorption, employing a methodology established by John Myers and Sean Carey. Our findings reveal that the majority of these cores exhibit temperatures ranging from 10 K to 20 K, with only one core recorded at a temperature below 8 K. This observation aligns with prior studies that suggest the rarity of cooler cores in star-forming environments. By applying the derived temperature values, we estimate the masses of these cores under the assumption of optically thin greybody emission, resulting in mass estimates that span from 0.1 M☉ to over 100 M☉. Furthermore, we leverage the same dataset to investigate the characteristics of protostars located within the RO region. We identify 16 Class I protostellar sources based on their spectral power distributions and compare their properties with those of protostars found in other nearby star-forming regions, such as Serpens South and Orion B North. This comparative analysis enhances our understanding of the dust conditions and their influence on star formation processes, contributing valuable insights into the primary mass function in various stellar nurseries.",
        "ori-fast-z-score": -0.5698028822981898,
        "water-fast-z-score": 5.4349297638940595,
        "rewrite-fast-z-score": 0.953998092005724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Metric for Gradient RG Flow of the Worldsheet Sigma Model Beyond First Order .\nAbstract:\nWe propose an improved metric on the space of worldsheet sigma model couplings that is suitable to study gradient renormalization group flows beyond first order in perturbation theory. The new metric has several advantages over previous proposals, including manifestly positive kinetic terms and no need for additional counterterms at higher orders. We show how this metric can be used to compute beta functions up to third order in perturbation theory using only Feynman diagrams with one-loop vacuum bubbles as building blocks. This allows us to obtain results for the beta function of the dilaton coupling to the Ricci scalar which are consistent with those obtained by other methods but have not been previously available due to technical difficulties. In addition we find evidence for non-trivial fixed points in the beta function of the string coupling constant. These results provide further support for the idea that the worldsheet sigma model may serve as a useful tool for studying quantum gravity. Introduction: It was recently shown  1  that the worldsheet sigma-model (WSSM) provides a powerful framework for investigating quantum gravity via its connection to the gravitational path integral  2  . One particularly interesting aspect of this approach is the possibility of computing perturbative corrections to the WSSM action directly from the gravitational path integral without having to resort to explicit calculations involving gravitons or graviton loops  3  .\nIn  4  it was proposed that the WSSM could also be used to investigate the flow of the effective action under the renormalization group (RG). However, since the WSSM contains infinitely many degrees of freedom there does not exist any finite dimensional parameter space where the RG flow takes place. Instead, the RG flow must take place along some infinite-dimensional trajectory through the space of all possible actions. To make progress towards understanding such trajectories it would be helpful if one were able to define a sensible metric on the space of WSSM actions so that distances between different actions could be measured. Such a metric should allow one to determine whether two given actions lie close together or far apart in the space of all possible WSSMs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Metric for Gradient RG Flow of the Worldsheet Sigma Model Beyond First Order . Abstract : We suggest an better metric on the space of worldsheet sigma model couplings that is suitable to study gradient renormalization group flows beyond first order in perturbation theory .The revised metric has numerous benefits over past proposals , notably manifestly strong kinetic terms and no requirement for additional counterterms at higher orders . We see how this metric can be used to compute beta functions up to third order in perturbation theory employing only Feynman diagrams with one - loop vacuum bubbles as building blocks .This enables us to obtain results for the beta function of the dilaton coupling to the Ricci scalar which are compatible with those achieved by other methods but have not been previously available owing to technical problems . In addition we find proof for non - simple fixed points in the beta function of the string coupling constant .These data provide further evidence for the idea that the worldsheet sigma model may serve as a helpful resource for studying quantum gravitational . Introduction : It was recently shown 1 that the worldsheet sigma - model ( WSSM ) presents a powerful framework for investigating quantum gravitational via its connection to the gravitational path integral 2 .One especially interesting aspect of this methodology is the prospect of computing perturbative corrections to the WSSM action directly from the gravitational direction equation without having to resort to explicit calculations concerning gravitons or graviton loops 3 . In 4 it was suggested that the WSSM could also be used to examine the flow of the effective action under the renormalization group ( RG ) .However , since the WSSM contains infinitely many degrees of liberty there does not exist any finite dimensional parameter room where the RG flow takes place . Instead , the RG flow must take place along some infinite - dimensional trajectory through the space of all possible actions .To build progress towards studying such trajectories it would be beneficial if one were trying to define a practical metric on the space of WSSM actions so that lengths between multiple movements could be determined . Such a metric should enable one to estimate whether two given actions reside close together or far separated in the space of all possible WSSMs .",
        "rewrite_text": "**Title:** A Metric for Gradient RG Flow of the Worldsheet Sigma Model Beyond First Order\n\n**Abstract:** In this article, we propose an enhanced metric for the space of couplings in the worldsheet sigma model (WSSM), which is particularly effective for analyzing gradient renormalization group (RG) flows beyond the first order in perturbation theory. Our revised metric offers several advantages over previous approaches, including the presence of manifestly strong kinetic terms and the elimination of the need for additional counterterms at higher orders. We demonstrate how this metric can be utilized to compute beta functions up to the third order in perturbation theory, relying solely on Feynman diagrams that incorporate one-loop vacuum bubbles as fundamental components. This methodology allows us to derive results for the beta function associated with the dilaton coupling to the Ricci scalar, which align with findings from alternative techniques but have remained inaccessible due to prior technical challenges. Furthermore, we provide evidence for the existence of non-simple fixed points in the beta function of the string coupling constant. These findings bolster the notion that the WSSM can serve as a valuable tool for exploring quantum gravity phenomena.\n\n**Introduction:** Recent studies have established that the worldsheet sigma model (WSSM) is a potent framework for probing quantum gravitational effects, particularly through its relationship with the gravitational path integral. A notable advantage of this approach is the ability to compute perturbative corrections to the WSSM action directly from the gravitational equations of motion, circumventing the need for explicit calculations involving gravitons or graviton loops. Additionally, it has been suggested that the WSSM could facilitate the examination of the effective action's flow under the RG framework. However, due to the infinite number of degrees of freedom inherent in the WSSM, a finite-dimensional parameter space for the RG flow is non-existent. Instead, the RG flow must traverse an infinite-dimensional trajectory within the realm of all possible actions. To advance our understanding of such trajectories, it is crucial to define a practical metric on the space of WSSM actions, enabling the assessment of distances between various configurations. This metric will provide insights into whether two specific actions are closely related or significantly distant within the expansive landscape of potential WSSMs.",
        "ori-fast-z-score": 0.30151134457776363,
        "water-fast-z-score": 8.192941705230835,
        "rewrite-fast-z-score": 1.47026414181486
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  BRIDGE: A Direct-tree Hybrid N-body Algorithm for Fully Self-consistent Simulations of Star Clusters and their Parent Galaxies .\nAbstract:\nWe present the Bridge algorithm, which is an efficient direct tree hybrid n-body method that can be used to simulate fully self-consistently star clusters in galactic potentials with arbitrary mass distributions. The Bridge algorithm combines the advantages of both direct-summation (DS) and tree methods by using DS only within small sub-groups of particles at each time step while solving the full system of equations on a tree. We show that this approach allows us to achieve high accuracy without sacrificing computational efficiency. In particular we demonstrate that our new code reproduces results obtained with the state-of-the-art treecode NBODY6++GPU very well even when simulating systems containing up to 10 million stars. This makes it possible to study the long-term dynamical evolution of open clusters as well as globular clusters orbiting around parent galaxies over many Gyr timescales. \n \n Keywords: Open cluster; Globular cluster; Galactic potential",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : BRIDGE : A Direct - tree Hybrid N - bodies Algorithm for Fully Self - consistent Simulations of Star Clusters and their Parent Galaxies . Abstract : We introduce the Bridge technique , which is an efficient direct forest hybrid n - bodies method that can be used to simulate fully self - consistently star clusters in galactic potentials with arbitrary mass distributions .The Bridge technique combines the advantages of both direct - summation ( DS ) and tree methods by using DS only within tiny sub - groups of particles at each time step while solving the full system of equations on a tree . We see that this methodology allows us to achieve high efficiency without sacrificing computational efficiency .In particular we prove that our new code reproduces data derived with the state - of - the - art treecode NBODY6 + + GPU very well even when simulating complexes containing up to 10 million stars . This gives it able to study the long - term dynamical development of close galaxies as well as globular galaxies orbiting around parent planets over many Gyr timescales .Keywords : Open cluster ; Globular cluster ; Galactic potential",
        "rewrite_text": "We present the Bridge technique, an innovative and efficient direct forest hybrid N-body algorithm designed for fully self-consistent simulations of star clusters within galactic potentials characterized by arbitrary mass distributions. This method adeptly merges the strengths of direct summation (DS) and tree algorithms by employing DS exclusively within small sub-groups of particles at each time step, while simultaneously addressing the entire system of equations using a tree structure. This strategic approach enables us to maintain high computational efficiency without compromising performance. Our results demonstrate that the new code effectively replicates data obtained from the cutting-edge treecode NBODY6++ GPU, even in simulations involving complex systems with up to 10 million stars. This capability allows for the exploration of the long-term dynamical evolution of interacting galaxies, as well as globular clusters orbiting around their parent galaxies, over extensive timescales spanning billions of years. The Bridge technique thus represents a significant advancement in the field of astrophysical simulations, facilitating deeper insights into the behavior and evolution of star clusters within their galactic environments. \n\nKeywords: Open cluster; Globular cluster; Galactic potential.",
        "ori-fast-z-score": -1.7669044171975445,
        "water-fast-z-score": 3.6222205796597815,
        "rewrite-fast-z-score": -0.9233805168766388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Supercritical series expansion for the contact process in heterogeneous and disordered environments .\nAbstract:\nWe present an analytical approach to study the dynamics of the contact process on complex networks with arbitrary degree distributions, including scale-free (SF) ones. The method is based on super-critical series expansions around the epidemic threshold. We show that this technique allows one to obtain accurate results even when the network size N becomes very large. In particular we find that the SF exponent has only a weak effect on the critical behavior at the transition point. This result suggests that the universality class of the phase transition does not depend on the details of the underlying topology but rather it depends solely on its average connectivity. Finally, our analysis shows that the presence of quenched randomness can lead to significant deviations from mean-field predictions. \nI. INTRODUCTORY REMARK\nThe contact process  1  , which describes the spreading of infectious diseases or computer viruses  2  , plays a central role in many areas of physics ranging from statistical mechanics  3  to epidemiology  4  . It also represents a paradigmatic model for studying self-organized criticality  5  .\nIn recent years there have been several attempts  6  -  8  aimed at extending the original formulation of the contact process by adding some ingredients such as spatial structure  9  , aging  10  , memory  11  , and heterogeneities  12  . These extensions are motivated by the fact that real-world systems often exhibit non-trivial topological features  13  and/or they evolve over time  14  . However, despite these efforts, the exact solution of the contact process remains elusive  15  .\nRecently, new techniques  16  -  18  were developed to tackle analytically problems related to the contact process on complex topologies. Among them, the so-called super-critical series expansion  19  provides a powerful tool to investigate the properties of the system close to the epidemic threshold  20  . Indeed, using this approach, it was possible to derive closed-form expressions for the probability distribution function  21  and the first two moments  22  of the number of infected nodes in the steady state. Moreover, it allowed us to determine the scaling laws characterizing the relaxation towards equilibrium  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Supercritical series expansion for the contact process in heterogeneous and disordered environments . Abstract : We present an analytical method to study the dynamics of the contact process on complex networks with arbitrary degree distributions , particularly scale - free ( SF ) ones .The method is based on ultra - critical series expansions around the outbreak threshold . We see that this methodology allows one to obtain precise data even when the network diameter N becomes very huge .In particular we find that the SF exponent has only a weak effect on the key behavior at the transition point . This result suggests that the universality type of the phase shift does not depend on the details of the underlying topology but rather it rests solely on its average connectivity .Finally , our analysis shows that the presence of quenched randomness can lead to significant deviations from mean - field expectations . I .INTRODUCTORY REMARK The connection process 1 , which explains the spreading of infectious infections or machine infections 2 , takes a central role in different areas of science ranging from statistical mechanics 3 to epidemiology 4 . It additionally represents a paradigmatic theory for studying self - organized criticality 5 .In recent history there have been numerous attempts 6 - 8 aimed at extending the original formulation of the contact process by added some ingredients such as temporal structure 9 , aging 10 , memory 11 , and heterogeneities 12 . These extended are motivated by the fact that real - world systems often exhibit non - simple topological features 13 and / or they develop over time 14 .However , despite these attempts , the exact solution of the contact process remains elusive 15 . Recently , new tactics 16 - 18 were developed to tackle analytically problems related to the contact process on difficult topologies .Among them , the so - called super - critical series expansion 19 offers a powerful tool to examine the properties of the system close to the outbreak threshold 20 . Indeed , using this methodology , it was easy to derive closed - form expressions for the probability distribution relation 21 and the first two moments 22 of the number of infected nodes in the stable state .Moreover , it able us to predict the scaling laws characterizing the relaxation towards equilibrium 23 .",
        "rewrite_text": "**Title:** Supercritical Series Expansion for the Contact Process in Heterogeneous and Disordered Environments\n\n**Abstract:** In this study, we introduce an analytical approach to investigate the dynamics of the contact process on complex networks characterized by arbitrary degree distributions, with a particular focus on scale-free (SF) networks. Our methodology employs ultra-critical series expansions centered around the outbreak threshold, enabling us to derive precise results even as the network diameter, denoted as N, becomes exceedingly large. Notably, our findings indicate that the SF exponent exerts only a minimal influence on the critical behavior observed at the transition point. This observation implies that the universality class of the phase transition is largely independent of the specific details of the underlying network topology, relying instead on the average connectivity of the network. Furthermore, our analysis reveals that the introduction of quenched randomness can lead to substantial deviations from mean-field predictions, highlighting the complexity of real-world systems. \n\nThe contact process, which models the spread of infections—whether biological or technological—plays a pivotal role across various scientific disciplines, including statistical mechanics and epidemiology. It serves as a fundamental framework for exploring self-organized criticality. Despite numerous efforts to enhance the original contact process model by incorporating factors such as temporal dynamics, aging, memory, and heterogeneities, a comprehensive exact solution has remained elusive. Recent advancements in analytical techniques have provided new avenues for addressing the challenges posed by complex topologies. Among these, the supercritical series expansion stands out as a robust method for examining system properties in proximity to the outbreak threshold. This approach has facilitated the derivation of closed-form expressions for the probability distribution and the first two moments of the number of infected nodes in the steady state, as well as predictions regarding the scaling laws governing the system's relaxation towards equilibrium.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 7.909667599213929,
        "rewrite-fast-z-score": -0.23791547571544325
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What it takes to measure a fundamental difference between dark matter and baryons: the halo velocity anisotropy .\nAbstract:\nWe present an analysis of the kinematics of satellite galaxies in galaxy clusters, which is aimed at measuring the differences between the distribution functions (DFs) of dark matter particles and those of luminous baryonic components. We use data for 16 galaxy clusters with redshifts ranging from 0.1 to 1.0 obtained by the Sloan Digital Sky Survey Data Release 6. The DFs are reconstructed using the maximum entropy method under the assumption that they have spherical symmetry. In order to test whether this assumption holds true or not, we compare the results derived assuming spherical symmetry with those obtained without any assumptions on the shape of the DFs. Our main result shows that there exists no significant difference between the DFs of dark matter particles and stars within one virial radius around each cluster center. This suggests that the shapes of the DFs of these two components are similar over a wide range of scales.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : What it takes to measure a basic change between dark matter and baryons : the halo velocity anisotropy . Abstract : We present an assessment of the kinematics of satellite galaxies in galaxy clusters , which is aiming at assessing the differences between the distribution functions ( DFs ) of bright matter molecules and those of luminous baryonic components .We use data for 16 galaxy regions with redshifts ranging from 0 . 1 to 1 . 0 collected by the Sloan Digital Sky Survey Data Release 6 . The DFs are reconstructed using the maximum entropy method under the assumption that they have spherical symmetry .In order to test whether this assumption holds true or not , we compare the results derived assuming spherical symmetry with those achieved without any assumptions on the shape of the DFs . Our main result suggests that there exists no major variation between the DFs of bright matter particles and stars within one virial diameter around each cluster center .This implies that the shapes of the DFs of these two parts are comparable over a broad variety of scales .",
        "rewrite_text": "Title: Assessing the Differences in Distribution Functions of Dark Matter and Baryons: Analyzing Halo Velocity Anisotropy\n\nAbstract: In this study, we investigate the kinematics of satellite galaxies within galaxy clusters to explore the distinctions between the distribution functions (DFs) of luminous baryonic components and bright matter particles. Utilizing data from 16 galaxy regions with redshifts between 0.1 and 1.0, sourced from the Sloan Digital Sky Survey Data Release 6, we aim to provide a comprehensive analysis of these DFs. The reconstruction of the DFs is performed using the maximum entropy method, predicated on the assumption of spherical symmetry. To evaluate the validity of this assumption, we juxtapose the outcomes derived from the spherical symmetry model against those obtained without imposing any geometric constraints on the DFs. Our findings indicate that there is no significant variation between the DFs of bright matter particles and stars within a virial diameter surrounding each cluster center. This result suggests that the shapes of the DFs for these two components are remarkably similar across a wide range of scales. Consequently, our research contributes to a deeper understanding of the interplay between dark matter and baryonic matter in galaxy clusters, highlighting the importance of kinematic studies in elucidating the underlying dynamics of cosmic structures. The implications of these findings may extend to broader astrophysical contexts, offering insights into the nature of dark matter and its relationship with baryonic matter in the universe.",
        "ori-fast-z-score": -0.917662935482247,
        "water-fast-z-score": 3.3048567173295003,
        "rewrite-fast-z-score": -0.20851441405707477
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Long Gamma-Ray Burst Progenitors: Boundary Conditions and Binary Models .\nAbstract:\nWe present the results of our study on binary models for gamma-ray bursts (GRBs) with progenitors in the mass range 8-40 M⊙, which are expected to produce GRB jets that can be observed at cosmological distances. We find that these systems evolve into double-degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae. The explosion is triggered by the merger of the components due to gravitational wave emission. In some cases we also find that the system evolves through an intermediate stage where one component collapses to form a black hole while the other explodes as a supernova. This scenario may explain why there seems to exist a gap between the masses of ordinary core-collapse supernovae and those of GRBs. Our calculations show that the total number of such events per year could be up to 10 times higher than previously estimated if the progenitor population extends down to lower masses.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Long Gamma - Ray Burst Progenitors : Boundary Conditions and Binary Models . Abstract : We present the conclusion of our research on binary models for gamma - ray bursts ( GRBs ) with progenitors in the mass range 8 - 40 [UNK] , which are expected to produce GRB jets that can be found at cosmological speeds .We see that these systems develop into double - degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae . The explosion is caused by the merger of the parts due to gravitational wave radiation .In some cases we also find that the system evolves through an intermediate stage where one element collapses to form a black hole while the other explodes as a supernova . This scenario could explain why there seems to exist a gap between the masses of normal core - collapse supernovae and those of GRBs .Our calculations show that the total number of such events per year could be up to 10 twice higher than previously predicted if the progenitor colony extends down to smaller masses .",
        "rewrite_text": "In this article, we conclude our investigation into binary models for gamma-ray bursts (GRBs) originating from progenitors with masses ranging from 8 to 40 solar masses. These progenitors are anticipated to generate GRB jets that travel at relativistic speeds, observable across cosmological distances. Our findings indicate that these stellar systems evolve into double-degenerate binaries, which consist of two white dwarfs or helium stars, prior to undergoing supernova explosions. The mechanism behind these explosions is primarily driven by the merger of stellar components, a process facilitated by the emission of gravitational waves. \n\nAdditionally, we identify scenarios where the evolutionary path of the binary system includes an intermediate phase, during which one star collapses to form a black hole while the other undergoes a supernova explosion. This particular evolutionary pathway may provide insight into the observed mass gap between typical core-collapse supernovae and GRBs, suggesting a more complex relationship than previously understood. \n\nOur calculations reveal that the annual occurrence of such GRB events could be as much as double previous estimates, particularly if the progenitor population includes stars of lower masses. This research enhances our understanding of the conditions and evolutionary processes that lead to GRBs, offering a more comprehensive framework for interpreting their origins and frequency in the universe. The implications of these findings are significant for the field of astrophysics, as they may reshape our models of stellar evolution and the mechanisms underlying explosive stellar phenomena.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": 1.7717591356565963
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lower Metal Enrichment of Virialized Gas in Minihalos .\nAbstract:\nWe present the results of cosmological hydrodynamic simulations that follow the formation and evolution of primordial gas clouds with masses between 10^(5) M_sun and 10^(7) M_sun, which are likely to be progenitors of low-mass galaxies at high redshifts (z > 6). We find that these clouds can cool efficiently by atomic hydrogen lines only if they have metallicities above Z = 1e-6Z_eq or higher. This is because metal enrichment increases the cooling rate through fine-structure emission lines such as  CII  158um and  OI  63um. The virialized gas inside minihalos has lower metallicity than its surrounding intergalactic medium due to inefficient mixing caused by supersonic turbulence driven by supernova explosions. As a result, it cannot cool below T_c ~ 100K even though it contains enough neutral hydrogen for efficient H_2 cooling.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lower Metal Enrichment of Virialized Gas in Minihalos . Abstract : We report the conclusion of cosmological hydrodynamic simulations that involve the formation and evolution of primordial gas clouds with masses between 10 ^ ( 5 ) M _ sun and 10 ^ ( 7 ) M _ sun , which are likely to be progenitors of low - mass stars at high redshifts ( z > 6 ) .We see that these clouds can cool efficiently by atomic hydrogen lines only if they have metallicities above Z = 1e - 6Z _ eq or greater . This is because metal enrichment raises the cooling frequency through fine - structure emission lines such as CII 158um and OI 63um .The virialized plasma inside minihalos has reduced metallicity than its neighbouring intergalactic medium due to inefficient mix created by supersonic turbulence driven by supernova earthquakes . As a result , it cannot cold below T _ c ~ 100K even though it contains enough neutral hydrogen for efficient H _ 2 cooling .",
        "rewrite_text": "We present findings from our cosmological hydrodynamic simulations that investigate the formation and evolution of primordial gas clouds with masses ranging from \\(10^5 M_{\\odot}\\) to \\(10^7 M_{\\odot}\\). These clouds are believed to be the precursors to low-mass stars that formed at high redshifts (z > 6). Our results indicate that these primordial gas clouds can only cool effectively through atomic hydrogen lines if their metallicity exceeds \\(Z = 1 \\times 10^{-6} Z_{\\text{eq}}\\). The presence of metals enhances the cooling process by increasing the emission frequency of fine-structure lines, such as CII at 158 µm and OI at 63 µm. \n\nMoreover, we observe that the virialized gas within minihalos exhibits a lower metallicity compared to the surrounding intergalactic medium. This discrepancy arises from the inefficient mixing caused by supersonic turbulence, which is driven by supernova explosions. Consequently, the gas in these minihalos struggles to cool below a critical temperature of approximately \\(T_c \\sim 100 \\, \\text{K}\\), despite containing sufficient neutral hydrogen that would typically facilitate efficient H2 cooling. Our findings highlight the complex interplay between metal enrichment and cooling processes in the early universe, suggesting that the conditions necessary for star formation in these primordial environments are more stringent than previously thought. This research contributes to our understanding of the formation of the first stars and the role of minihalos in cosmic evolution.",
        "ori-fast-z-score": -1.1523319193960637,
        "water-fast-z-score": 2.9448482384566077,
        "rewrite-fast-z-score": -0.5184758473652127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Expressing Combinatorial Optimization Problems by Systems of Polynomial Equations and the Nullstellensatz .\nAbstract:\nThe nullstellensatz is an important theorem in algebraic geometry, which states that if a system of polynomial equations has no solution over some field then it must have at least one solution over its algebraic closure.  In this work we show how to use the nullstellensatz as part of a general framework for expressing combinatorial optimization problems (COPs) as systems of polynomial equations.   We demonstrate our approach on several COPs including maximum clique, minimum vertex cover, and maximum independent set.   ... This research was supported by NSF grant CCF-0430020. The authors would like to thank Professors David Avis and Michael Shmoys for their helpful comments during the preparation of this manuscript. The nullstellensatz provides a powerful tool for solving certain classes of computational problems expressed as systems of polynomial equations. It states that if such a system does not admit any solutions over a given field K, then there exists at least one solution over the algebraic closure of K. In this paper, we present a general method for transforming combinatorial optimization problems into systems of polynomial equations whose solvability can be determined using the nullstellenstaz. Our approach relies on representing each feasible solution of the COP as a point in a vector space V equipped with a suitable inner product. Then, we express the objective function of the problem as a linear combination of polynomials in V . Finally, we formulate the problem as a system of polynomial equations by requiring that all points corresponding to feasible solutions satisfy the objective function simultaneously. As examples, we apply our technique to three well-known NP-hard combinatorial optimization problems: Maximum Clique, Minimum Vertex Cover, and Maximum Independent Set.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Expressing Combinatorial Optimization Problems by Systems of Polynomial Equations and the Nullstellensatz . Abstract : The nullstellensatz is an important theorem in algebraic geometry , which says that if a system of polynomial equations has no solution over some field then it must have at least one solve over its algebraic closure .In this study we prove how to use the nullstellensatz as part of a general template for expressing combinatorial algorithm questions ( COPs ) as systems of polynomial equations . We showed our approach on numerous COPs including maximum clique , limit node cover , and minimal independent setting ....This research was supported by NSF grant CCF-0430020.The authors would like to thank Professors David Avis and Michael Shmoys for their helpful remarks during the preparation of this manuscript . The nullstellensatz offers a powerful tool for solving specific groups of computational problems defined as systems of polynomial equations .It says that if such a system does not admit any solutions over a given field K , then there exists at least one solve over the algebraic closure of K . In this paper , we present a general technique for transforming combinatorial algorithm questions into systems of polynomial equations whose solvability can be determined using the nullstellenstaz . Our formulation rely on representing each feasible solving of the COP as a point in a vector space V equipped with a suitable inner product .Then , we express the objective function of the question as a linear mixture of polynomials in V . Finally , we formulate the question as a system of polynomial equations by requiring that all points corresponding to feasible answers fulfill the objective function simultaneously .As instance , we apply our technique to three good - famous NP - hard combinatorial algorithms problems : Maximum Clique , Minimum Vertex Cover , and Maximum Independent Set .",
        "rewrite_text": "**Title:** Expressing Combinatorial Optimization Problems through Systems of Polynomial Equations and the Nullstellensatz\n\n**Abstract:** The Nullstellensatz is a pivotal theorem in algebraic geometry, asserting that if a system of polynomial equations lacks solutions over a specific field, then there exists at least one solution in its algebraic closure. This paper explores the application of the Nullstellensatz as a foundational framework for representing combinatorial optimization problems (COPs) as systems of polynomial equations. We demonstrate our methodology across various well-known COPs, such as the Maximum Clique, Minimum Vertex Cover, and Maximum Independent Set problems. Our research, supported by NSF grant CCF-0430020, highlights the utility of the Nullstellensatz in addressing specific classes of computational challenges framed as polynomial equation systems.\n\nWe introduce a systematic approach to convert combinatorial algorithm questions into polynomial equations, enabling the assessment of their solvability via the Nullstellensatz. Our technique involves mapping each feasible solution of the COP to a point in a vector space V, which is structured with an appropriate inner product. Subsequently, we represent the objective function of the problem as a linear combination of polynomials within this vector space. The final step involves formulating the problem as a system of polynomial equations, ensuring that all points corresponding to feasible solutions satisfy the objective function concurrently.\n\nThrough this framework, we provide insights into the transformation of complex combinatorial problems into a mathematical format that can be analyzed using algebraic methods. Our findings not only contribute to the theoretical understanding of COPs but also pave the way for potential algorithmic advancements in solving these challenging problems. We extend our gratitude to Professors David Avis and Michael Shmoys for their invaluable feedback during the preparation of this manuscript.",
        "ori-fast-z-score": 0.6115928396627265,
        "water-fast-z-score": 6.092717958449424,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Precision measurement of the Casimir-Lifshitz force in a fluid .\nAbstract:\nWe report on an experiment to measure the Casimir-Lifhsztz (CL) interaction between two gold-coated glass plates immersed in water at room temperature and pressure. The CL force is measured by observing the Brownian motion of one plate with respect to another, using optical interferometry. We find that the magnitude of the observed effect agrees well with theoretical predictions based on Lifshitz theory for dielectrics. This work represents the first direct experimental observation of the CL force in a liquid medium. It also demonstrates how precision measurements can be used to test fundamental theories such as quantum electrodynamics. \n \n In recent years there has been considerable interest in measuring the Casimir-Lifhzsiz (CL)  1  force between macroscopic objects  2  . Such experiments are important because they provide tests of our understanding of vacuum fluctuations  3  , which play a central role in many areas of physics including quantum field theory  4  , statistical mechanics  5  , condensed matter  6  , atomic and nuclear physics  7  , cosmology  8  , and gravitation  9  .\n \nThe original prediction of the CL force was made more than 50 years ago  10  but it took until 1997  11  before this attractive force could be directly detected experimentally  12  . Since then several groups have performed high-precision experiments  13  -  16  aimed at testing the validity of various aspects of the theory  17  -  20  . \n \n Here we present results obtained in a new experiment designed specifically to study the CL force in liquids  21  . Our approach involves immersing two parallel plates coated with thin layers of gold into distilled water contained inside a sealed container  22  . By monitoring the Brownian motion of these plates  23  we were able to determine their mutual attraction due to the presence of the surrounding water molecules  24  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Precision measurement of the Casimir - Lifshitz force in a fluid . Abstract : We report on an research to measure the Casimir - Lifhsztz ( CL ) relationship between two gold - glazed glazed plates immersed in water at room temperature and tension .The CL force is measured by observing the Brownian movement of one plate with regard to another , using optical interferometry . We see that the magnitude of the seen effect agrees well with theoretical expectations based on Lifshitz principle for dielectrics .This study constitutes the first continuous experimental measurement of the CL force in a liquid medium . It additionally demonstrates how accuracy observations can be used to test fundamental theories such as quantum electrodynamics .In recent years there has been substantial interest in measuring the Casimir - Lifhzsiz ( CL ) 1 pressure between macroscopic objects 2 . Such experiments are important because they give tests of our knowing of vacuum fluctuations 3 , which take a central role in multiple fields of science including quantum field theory 4 , statistical mechanics 5 , condensed matter 6 , atomic and nuclear science 7 , cosmology 8 , and gravitation 9 .The original forecast of the CL force was making more than 50 centuries earlier 10 but it taking until 1997 11 before this attractive force could be directly discovered experimentally 12 . Since then several organizations have done large - precision tests 13 - 16 aiming at testing the legitimacy of several elements of the principle 17 - 20 .Here we present results derived in a new study intended specifically to study the CL force in liquids 21 . Our solution involves immersing two connected sheets coated with thin layers of gold into distilled water contained inside a sealed container 22 .By observing the Brownian movement of these plates 23 we were could to obtain their mutual attraction owing to the presence of the nearby water molecules 24 .",
        "rewrite_text": "**Title:** Precision Measurement of the Casimir-Lifshitz Force in a Fluid\n\n**Abstract:** In this study, we present a detailed investigation into the Casimir-Lifshitz (CL) force between two gold-coated plates submerged in water at room temperature. Utilizing optical interferometry, we measured the CL force by analyzing the Brownian motion of one plate relative to the other. Our findings reveal that the observed force aligns closely with theoretical predictions derived from the Lifshitz theory for dielectric materials. This research marks the first continuous experimental assessment of the CL force in a liquid environment, showcasing the potential of precise measurements to validate fundamental theories, including quantum electrodynamics.\n\nThe Casimir-Lifshitz force has garnered significant attention in recent years, particularly in the context of measuring the pressure exerted between macroscopic objects. Such experiments are crucial as they provide insights into vacuum fluctuations, which are pivotal across various scientific domains, including quantum field theory, statistical mechanics, condensed matter physics, atomic and nuclear science, cosmology, and gravitation. Although the existence of the CL force was predicted over five decades ago, it was not until 1997 that this attractive force was directly observed in an experimental setting. Since then, numerous research groups have conducted high-precision experiments aimed at scrutinizing different aspects of the CL principle.\n\nIn our investigation, we specifically focused on the CL force in liquid media. Our experimental setup involved immersing two closely spaced gold-coated plates in distilled water within a sealed chamber. By monitoring the Brownian motion of these plates, we successfully quantified their mutual attraction, which arises from the influence of surrounding water molecules. This work not only enhances our understanding of the CL force in fluids but also contributes to the broader discourse on the implications of quantum phenomena in liquid environments.",
        "ori-fast-z-score": -1.0795912380986197,
        "water-fast-z-score": 6.7863682865394175,
        "rewrite-fast-z-score": -0.0842151921066519
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Visualizing pair formation on the atomic scale in the high-Tc superconductor Bi2Sr2CaCu2O8+d .\nAbstract:\nThe authors have used scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8+d (Bi-2212). They find that, at low temperatures, this material forms pairs of holes which are bound together by an attractive interaction mediated by phonons. The binding energy is found to be about 0.5 eV per hole pair. This value agrees well with theoretical predictions for the strength of the pairing force between holes in these materials. \n \n In addition, they observe that the density of states near the Fermi level shows a strong dependence on the direction along which the crystal is cut. For example, when the crystal is cleaved parallel to its Cu-O planes, it exhibits a large peak in the density of states just below the Fermi level. However, if the cleavage plane is perpendicular to the Cu-O planes, no such peak appears.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Visualizing pair formation on the atomic scale in the high - Tc superconductor Bi2Sr2CaCu2O8 + d . Abstract : The authors have utilized scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high - temperature cuprate superconductor Bi2Sr2CaCu2O8 + d ( Bi - 2212 ) .They find that , at low temperatures , this tissue forms pairs of holes which are bound together by an attractive interaction mediated by phonons . The binding energy is found to be about 0 . 5 eV per hole pair .This value agrees well with theoretical expectations for the strength of the pairing force between holes in these structures . In addition , they demonstrate that the density of states near the Fermi level exhibits a powerful relationship on the direction along which the crystal is cutting .For instance , when the crystal is cleaved parallel to its Cu - O planes , it displays a large peak in the density of states just below the Fermi level . However , if the cleavage plane is parallel to the Cu - O planes , no such peak appears .",
        "rewrite_text": "Title: Visualizing Pair Formation on the Atomic Scale in the High-Tc Superconductor Bi2Sr2CaCu2O8 + d\n\nAbstract: In this study, the authors employ scanning tunneling microscopy (STM) to investigate the surface structure and electronic characteristics of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8 + d (commonly referred to as Bi-2212). Their findings reveal that at low temperatures, the material exhibits the formation of hole pairs, which are bound together through an attractive interaction that is mediated by phonons. The binding energy associated with these hole pairs is approximately 0.5 eV, a value that aligns closely with theoretical predictions regarding the strength of the pairing force acting between holes in these superconducting structures. Furthermore, the authors highlight a significant dependence of the density of states near the Fermi level on the orientation of the crystal cleavage. Specifically, when the crystal is cleaved parallel to its Cu-O planes, a pronounced peak in the density of states is observed just below the Fermi level. Conversely, when the cleavage is oriented parallel to the Cu-O planes, this peak is absent. These observations not only enhance the understanding of the electronic properties of Bi-2212 but also provide insights into the mechanisms underlying high-temperature superconductivity. The results underscore the intricate relationship between crystal structure and electronic behavior, paving the way for further exploration of the fundamental properties of high-Tc superconductors.",
        "ori-fast-z-score": 2.393172105652397,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": 2.3597502097958545
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  WDM and Directed Star Arboricity .\nAbstract:\nWe study the problem of finding minimum cost arborescences in directed graphs with edge costs that are functions of their distances, where each vertex has an associated demand for bandwidth. We show how to solve this problem by reducing it to the well-studied problem of computing shortest paths on weighted digraphs (WDP). Our reduction is based on the concept of directed star arboricity which we define as follows. Let G = (V , E) be any directed graph. A set S ⊆ V is called a star if there exists exactly one vertex u ∈ S such that all other vertices v ∈ S have only edges into u. The directed star arboricity of G is defined as the smallest number k such that every subgraph H of G can be decomposed into at most k stars. In particular, our results imply that WDP can be solved using O(n log n + m log n) time and space, improving upon previous algorithms running in O(m+nlogn) or O(mn+min{nlogn,m}) time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : WDM and Directed Star Arboricity . Abstract : We research the question of finding minimum price arborescences in directed graphs with edge expenses that are functions of their distances , where each vertex has an associated need for bandwidth .We see how to solve this question by reducing it to the good - discussed difficulty of computing shortest trails on weighted digraphs ( WDP ) . Our reduction is based on the idea of directed star arboricity which we define as follows .Let G = ( V , E ) be any directed graph . A setting S ⊆ V is dubbed a star if there exists precisely one vertex v ∈ S such that all other vertices v ∈ S have only edges into v .The directed star arboricity of G is characterized as the smallest number k such that every subgraph H of G can be decomposed into at most k stars . In particular , our findings assume that WDP can be solved using O ( n log k + m log k ) time and space , improving upon recent algorithms run in O ( m + nlogn ) or O ( mn + min { nlogn , m } ) time .",
        "rewrite_text": "In this study, we investigate the problem of determining minimum cost arborescences in directed graphs where edge costs are dependent on their distances, and each vertex has a specific bandwidth requirement. We approach this challenge by transforming it into the well-established problem of computing shortest trails in weighted directed graphs (WDP). Our methodology is grounded in the concept of directed star arboricity, which we define in detail. Given a directed graph G = (V, E), a subset S ⊆ V is classified as a star if there exists a single vertex v ∈ S such that all other vertices in S have directed edges leading to v. The directed star arboricity of G is defined as the minimum integer k such that every subgraph H of G can be decomposed into no more than k stars. Our research demonstrates that the WDP can be efficiently solved in O(n log k + m log k) time and space, representing a significant improvement over previous algorithms that operate in O(m + n log n) or O(mn + min{n log n, m}) time complexities. This advancement not only enhances the efficiency of solving the WDP but also contributes to the broader understanding of directed graph structures and their applications in network design and optimization. Through our findings, we provide a novel perspective on the interplay between directed star arboricity and the computation of minimum cost arborescences, paving the way for future research in this area.",
        "ori-fast-z-score": -2.1376670117594454,
        "water-fast-z-score": 3.6666666666666665,
        "rewrite-fast-z-score": -0.9622504486493763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantifying social group evolution .\nAbstract:\nWe present an approach to quantify the evolutionary dynamics of social groups, based on their internal structure and interactions with other groups. We use this method to study how cooperation evolves in structured populations composed by different types of agents (cooperators or defectors). Our results show that cooperator clusters are more likely to survive than isolated ones when they interact with defector clusters. This effect is stronger for larger cluster sizes. The survival probability decreases as the number of competing clusters increases. Finally we find that the presence of cooperator clusters can lead to the extinction of all defector clusters if there exists at least one large enough cooperator cluster. In recent years it has been shown that cooperative behavior among unrelated individuals may evolve spontaneously even under strong competition between selfish strategies  1-3 . However, these studies have focused mainly on well-mixed populations where each individual interacts equally often with every other member of the population  4  . On the contrary, real-world systems such as biological communities  5  , human societies  6  , or computer networks  7  usually exhibit some degree of spatial organization  8  .\nIn order to understand better the role played by space in the emergence of cooperation, several authors have studied the evolution of cooperation in spatially-structured populations  9  . These works typically consider two main classes of models: lattice-based  10  and agent-based  11  . Lattice-based models assume that the whole population lives on a regular grid  12  while agent-based models allow for arbitrary topologies  13  . Despite the differences between both approaches, most previous work agrees that the topology of the underlying network plays a crucial role in determining whether cooperation will be able to thrive  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantifying social group evolution . Abstract : We present an view to quantify the evolutionary dynamics of social groups , based on their internal structure and interactions with other communities .We use this method to study how cooperation evolves in structured communities composed by various types of agents ( cooperators or defectors ) . Our results show that cooperator clusters are more likely to survive than scattered ones when they interact with defector clusters .This phenomenon is strengthened for larger cluster sizes . The survival likelihood falls as the number of competing clusters increases .Finally we find that the presence of cooperator clusters can lead to the extinction of all defector clusters if there exists at least one large enough cooperator cluster . In recent years it has been shown that cooperative performance among unrelated individuals might develop spontaneously especially under strong rivalry between selfish strategies 1 - 3 .However , these research have concentrated mostly on well - mixing communities where each individual interacts equally often with every other part of the population 4 . On the contrary , real - world systems such as genetic communities 5 , human organizations 6 , or computer networks 7 usually display some degree of spatial activity 8 .In order to explain better the part played by space in the emergence of cooperation , various scientists have researched the evolution of cooperation in spatially - organized populations 9 . These works typically consider two principal classes of models : lattice - based 10 and agent - based 11 .Lattice - based methods assume that the whole community lives on a regular grid 12 while agent - based models make for arbitrary topologies 13 . Despite the differences between both approaches , most prior work agrees that the topology of the underlying network takes a crucial role in calculating whether cooperation will be possible to thrive 14 .",
        "rewrite_text": "We provide a comprehensive framework for quantifying the evolutionary dynamics of social groups, focusing on their internal structures and interactions with other communities. This methodology is applied to investigate the evolution of cooperation within structured communities comprised of various agent types, specifically cooperators and defectors. Our findings indicate that clusters of cooperators have a higher likelihood of survival compared to isolated cooperators when interacting with clusters of defectors. This survival advantage is amplified as the size of the cooperator clusters increases. However, we observe that the probability of survival diminishes as the number of competing clusters rises. Notably, our research reveals that the existence of at least one sufficiently large cooperator cluster can lead to the complete extinction of all defectors. \n\nRecent studies have highlighted that cooperative behavior among unrelated individuals can emerge spontaneously, particularly in contexts of intense competition between selfish strategies. However, much of this research has focused on well-mixed communities, where individuals interact uniformly with one another. In contrast, real-world systems—such as genetic communities, human organizations, and computer networks—often exhibit varying degrees of spatial organization. To better understand the role of spatial dynamics in fostering cooperation, various researchers have explored the evolution of cooperation in spatially structured populations. These studies typically categorize into two main modeling approaches: lattice-based and agent-based models. Lattice-based models assume that the entire community exists on a regular grid, while agent-based models allow for more arbitrary topologies. Despite the differences in these methodologies, there is a consensus in the literature that the topology of the underlying network plays a critical role in determining the potential for cooperation to flourish.",
        "ori-fast-z-score": 0.4016096644512494,
        "water-fast-z-score": 7.6373499280870085,
        "rewrite-fast-z-score": 2.4083189157584592
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Discovery of extreme asymmetry in the debris disk surrounding HD 15115 .\nAbstract:\nWe report on new observations made with the Hubble Space Telescope (HST) that reveal an extremely asymmetric distribution of dust around the young star HD 15115, located at a distance of about 150 light-years away in the constellation Pictor.  The images show two bright lobes extending to distances greater than 100 AU and separated by more than 50 AU along the major axis of the system s circumstellar disk. We interpret these features as evidence for recent planet formation activity within this protoplanetary disk. This is one of only three known cases where such structures have been observed directly; all other examples are inferred indirectly through modeling or scattered-light imaging techniques. These results provide important constraints on models of planet formation and migration. In addition, we find no evidence for any significant amount of cold dust beyond 250 AU from the central star. Our findings suggest that the outer regions of protoplanetary disks may be cleared out much faster than previously thought.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Discovery of exceptional asymmetry in the dust disk surrounding HD 15115 . Abstract : We report on new images obtained with the Hubble Space Telescope ( HST ) that discover an incredibly asymmetric distribution of dust around the young star HD 15115 , located at a distance of about 150 light - years away in the constellation Pictor .The images see two bright lobes extending to distances greater than 100 AU and separated by more than 50 AU along the main axis of the system s circumstellar disk . We interpret these characteristics as proof for recent planet development movement within this protoplanetary disk .This is one of only three known cases where such formations have been observed directly ; all other instances are inferred indirectly through modeling or scattered - light optical techniques . These data provide important restrictions on estimates of planet development and movement .In addition , we find no evidence for any large amount of cold particles beyond 250 AU from the main star . Our findings show that the exterior areas of protoplanetary disks might be cleared out significantly rapidly than previously thought .",
        "rewrite_text": "We present new findings from Hubble Space Telescope (HST) observations that reveal a strikingly asymmetric dust distribution surrounding the young star HD 15115, situated approximately 150 light-years away in the constellation Pictor. The newly acquired images depict two prominent lobes of dust extending beyond 100 astronomical units (AU) from the star, with a significant separation of over 50 AU along the primary axis of the circumstellar disk. This unusual morphology suggests the presence of dynamic processes, likely indicative of recent planetary formation and migration within the protoplanetary disk. Notably, this discovery marks one of only three instances where such asymmetric structures have been directly observed, contrasting with other cases that have been inferred through indirect methods such as modeling or scattered-light optical techniques. The implications of our findings are substantial, as they impose critical constraints on current models of planetary formation and migration dynamics. Furthermore, our observations reveal an absence of substantial cold particle populations beyond 250 AU from HD 15115, suggesting that the outer regions of protoplanetary disks may be cleared out more rapidly than previously anticipated. This research not only enhances our understanding of the processes governing disk evolution and planet formation but also opens new avenues for investigating the characteristics of young stellar systems. The exceptional asymmetry observed in the dust disk around HD 15115 provides a unique opportunity to study the interplay between dust dynamics and planetary development in a protoplanetary environment.",
        "ori-fast-z-score": -0.8528028654224417,
        "water-fast-z-score": 5.193989612031165,
        "rewrite-fast-z-score": -0.09245003270420485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolution of interstellar dust and stardust in the solar neighbourhood .\nAbstract:\nWe present an analysis of the evolution of interstellar dust grains, based on their size distribution inferred by infrared observations with ISO (Infrared Space Observatory). We find that the grain growth is dominated by coagulation at all times since the formation of the Sun. The total mass density of dust increases by about one order of magnitude during this time span. This increase can be explained by accretion of gas-phase metals onto pre-existing grains or condensation of new material out of the gas phase. In addition to these processes we also consider fragmentation as well as shattering due to collisions between particles. Fragmentation dominates over coagulation for small grains but becomes less important when the grains grow larger than 0.1 micrometres. For large grains shattering leads to a decrease in number density which counteracts the effect of coagulation. Our results are consistent with previous studies using different methods. \n \n Keywords: Interstellar medium",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolution of interstellar dust and stardust in the solar neighbourhood . Abstract : We present an assessment of the evolution of interstellar dust grains , based on their size distribution inferred by infrared observations with ISO ( Infrared Space Observatory ) .We see that the grain growth is dominated by coagulation at all periods since the formation of the Sun . The total mass density of dust increases by about one order of magnitude during this time frame .This increase can be described by accretion of gas - phase metals onto pre - old grains or condensation of new material out of the gas phase . In addition to these mechanisms we also consider fragmentation as well as shattering related to collisions between particles .Fragmentation dominates over coagulation for little grains but grows less important when the grains grow larger than 0 . 1 micrometres . For large grains breaking leads to a reduction in number density which counteracts the impact of coagulation .Our results are compatible with previous research utilizing diverse methods . Keywords : Interstellar medium",
        "rewrite_text": "Title: Evolution of Interstellar Dust and Stardust in the Solar Neighborhood\n\nAbstract: This study provides a comprehensive evaluation of the evolution of interstellar dust grains, utilizing size distribution data derived from infrared observations conducted by the Infrared Space Observatory (ISO). Our findings indicate that the growth of dust grains has been predominantly influenced by coagulation processes since the Sun's formation. Over this extensive period, the total mass density of dust has increased by approximately an order of magnitude. This significant enhancement can be attributed to two primary mechanisms: the accretion of gas-phase metals onto pre-existing grains and the condensation of new materials from the gas phase. Furthermore, we explore additional processes such as fragmentation and shattering, which occur as a result of collisions between dust particles. While fragmentation plays a crucial role for smaller grains, its significance diminishes as grain size exceeds 0.1 micrometers. For larger grains, fragmentation leads to a decrease in number density, which serves to counterbalance the effects of coagulation. Our results align with previous studies that employed various methodologies, reinforcing the robustness of our conclusions. This research contributes to the understanding of the interstellar medium and the complex dynamics governing dust evolution in the solar neighborhood. \n\nKeywords: Interstellar medium, dust evolution, coagulation, fragmentation, infrared observations.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 5.366563145999495,
        "rewrite-fast-z-score": 2.041241452319315
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Towards a microscopic theory of toroidal moments in bulk periodic crystals .\nAbstract:\nWe present an ab initio study on the electronic structure and magnetic properties of Fe3O4, CoFe2O4, NiFe2O4, MnFe2O4, CrFe2O4, and VFe2O4 compounds with different spin configurations. We show that these materials are characterized by large orbital moment contributions to their total magnetization which can be explained within the framework of density functional theory (DFT) using generalized gradient approximation (GGA). The calculated values for the orbital-to-spin ratio agree well with experimental data obtained via neutron scattering experiments. In addition we find that all studied systems exhibit non-collinear magnetic structures due to competing exchange interactions between neighboring atoms. Finally, we discuss how our results could be used as input parameters into existing theories describing macroscopic phenomena such as spin-orbit torques or anomalous Hall effects. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. \n \n Authors: \n\n\nKai Hwang , \nJens Kühn , \nSusanne Schreiber , \nAlexander Sokolov , \nAndreas Wurmehl , \nMartin J. Gummow , \nMichael A. Nevidomskyy , \nHerbert R. Kröger , \nWolfgang Ebert , \nPeter Grünberg , \nUlrich Stoll , \nStefan Haun , \nThomas Bader , \nDaniel Loss , \nNorbert Lütkenhaus , \nRalf Heimann , \nChristoph M. Fischer , \nChristian Fähnle , \nMats Nilsson , \nLars Lindström , \nMatthias Reiss , \nJohannes Ploog , \nJan-Philipp von Bardeleben , \nDietmar Grueneisen , \nFrank Steglich , \nBoris Yakob , \nAleksandr Yufit , \nYurii I. Shubin , \nNikolay D. Semenov , \nVladimir Ogan",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Towards a microscopic theory of toroidal moments in bulk periodic crystals . Abstract : We report an ab initio investigation on the electronic properties and magnetic properties of Fe3O4 , CoFe2O4 , NiFe2O4 , MnFe2O4 , CrFe2O4 , and VFe2O4 compounds with various spin configurations .We see that these objects are characterized by large orbital moment contributions to their total magnetization which can be described within the framework of density functional theory ( DFT ) using generalized gradient approximation ( GGA ) . The measured expressions for the orbital - to - spin ratio agree well with theoretical data received via neutron scattering experiments .In addition we find that all observed systems exhibit non - collinear magnetic structures owing to competing exchange interactions between neighboring atoms . Finally , we explain how our findings may be used as input parameters into older theories describing macroscopic processes such as spin - orbit torques or anomalous Hall effects .This is an open access article under the terms of the Creative Commons Attribution License , which permits use , distribution and reproduction in any medium , provided the original work is properly cited . Authors : Kai Hwang , Jens Kühn , Susanne Schreiber , Alexander Sokolov , Andreas Wurmehl , Martin J . Gummow , Michael A . Nevidomskyy , Herbert R . Kröger , Wolfgang Ebert , Peter Grünberg , Ulrich Stoll , Stefan Haun , Thomas Bader , Daniel Loss , Norbert Lütkenhaus , Ralf Heimann , Christoph M . Fischer , Christian Fähnle , Mats Nilsson , Lars Lindström , Matthias Reiss , Johannes Ploog , Jan - Philipp von Bardeleben , Dietmar Grueneisen , Frank Steglich , Boris Yakob , Aleksandr Yufit , Yurii I . Shubin , Nikolay D . Semenov , Vladimir Ogan",
        "rewrite_text": "**Title:** Towards a Microscopic Theory of Toroidal Moments in Bulk Periodic Crystals\n\n**Abstract:** In this study, we present a comprehensive ab initio analysis of the electronic and magnetic properties of various spin configurations in the compounds Fe3O4, CoFe2O4, NiFe2O4, MnFe2O4, CrFe2O4, and VFe2O4. Our findings reveal that these materials exhibit significant contributions from orbital moments to their overall magnetization, which can be effectively modeled using density functional theory (DFT) with the generalized gradient approximation (GGA). The calculated orbital-to-spin moment ratios align closely with experimental data obtained from neutron scattering, validating our theoretical approach. Furthermore, we observe that all investigated systems display non-collinear magnetic structures, a phenomenon attributed to the competing exchange interactions among neighboring atoms. This intricate interplay of magnetic interactions underscores the complexity of the magnetic behavior in these compounds. Importantly, our results provide valuable insights that can be integrated into existing theoretical frameworks that describe macroscopic phenomena, such as spin-orbit torques and anomalous Hall effects. This research contributes to the understanding of toroidal moments in bulk periodic crystals and opens avenues for future studies in the field of magnetism. The article is published under the Creative Commons Attribution License, allowing for unrestricted use, distribution, and reproduction in any medium, provided proper citation of the original work. \n\n**Authors:** Kai Hwang, Jens Kühn, Susanne Schreiber, Alexander Sokolov, Andreas Wurmehl, Martin J. Gummow, Michael A. Nevidomskyy, Herbert R. Kröger, Wolfgang Ebert, Peter Grünberg, Ulrich Stoll, Stefan Haun, Thomas Bader, Daniel Loss, Norbert Lütkenhaus, Ralf Heimann, Christoph M. Fischer, Christian Fähnle, Mats Nilsson, Lars Lindström, Matthias Reiss, Johannes Ploog, Jan-Philipp von Bardeleben, Dietmar Grueneisen, Frank Steglich, Boris Yakob, Aleksandr Yufit, Yurii I. Shubin, Nikolay D. Semenov, Vladimir Ogan.",
        "ori-fast-z-score": -0.10153461651336192,
        "water-fast-z-score": 4.040610178208843,
        "rewrite-fast-z-score": -0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stellar Content and Recent Star Formation History of the Local Group Dwarf Irregular Galaxy IC1613 .\nAbstract:\nWe present deep optical photometry in B, V , R c I c bands for the dwarf irregular galaxy IC 1613 obtained with the Wide Field Imager (WFI) at the MPG/ESO 2.2 m telescope on La Silla Observatory. The data were reduced using standard IRAF routines. We derived total magnitudes within an aperture radius of 5 arcsec by applying aperture corrections to the PSF-fitted magnitudes. Our results are compared with previous studies based on shallower observations. In addition we derive new estimates for the distance modulus DM = 27.9 ± 0.1 mag and foreground extinction A V = 0.10 ± 0.02 mag towards this galaxy. Using these values together with our photometric measurements we determined absolute magnitudes M B = −15.6 ± 0.3 mag, M V = −14.7 ± 0.4 mag, M Rc = −12.8 ± 0.5 mag, M Ic = −11.0 ± 0.6 mag and colour indices U−B = 1.45±0.25 mag, B−V =0.70±0.06 mag, V −Rc=0.55±0.05 mag, V −Ic=1.00±0.07 mag. These parameters allow us to estimate the mean metallicity Z = 0.008 ± 0.001 dex and age t = 3 Gyrs for the stellar population of IC 1613.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stellar Content and Recent Star Formation History of the Local Group Dwarf Irregular Galaxy IC1613 . Abstract : We report deep optical photometry in B , V , R c I c groups for the dwarf irregular universe IC 1613 obtained with the Wide Field Imager ( WFI ) at the MPG / ESO 2 . 2 m observatory on La Silla Observatory .The data were reduced using traditional IRAF procedures . We extracted total magnitudes within an lens radius of 5 arcsec by using aperture corrections to the PSF - fitted magnitudes .Our results are compared with previous findings based on shallower observations . In addition we derive new accounts for the distance modulus DM = 27 . 9 ± 0 . 1 mag and foreground extinction A V = 0 . 10 ± 0 . 02 mag towards this galaxy .Using these values combined with our photometric calculations we calculated absolute magnitudes M B = −15 . 6 ± 0 . 3 mag , M V = −14 . 7 ± 0 . 4 mag , M Rc = −12 . 8 ± 0 . 5 mag , M Ic = −11 . 0 ± 0 . 6 mag and colour indices U−B = 1 . 45±0 . 25 mag , B−V = 0 . 70±0 . 06 mag , V −Rc = 0 . 55±0 . 05 mag , V −Ic = 1 . 00±0 . 07 mag . These variables enable us to estimate the mean metallicity Z = 0 . 008 ± 0 . 001 dex and age t = 3 Gyrs for the stellar population of IC 1613 .",
        "rewrite_text": "We present a comprehensive analysis of the stellar content and recent star formation history of the dwarf irregular galaxy IC 1613, utilizing deep optical photometry in B, V, R_c, and I_c bands. The observations were conducted with the Wide Field Imager (WFI) at the MPG/ESO 2.2 m telescope located at La Silla Observatory. The data reduction was performed using standard IRAF procedures, ensuring the accuracy of our measurements. We determined total magnitudes within a lens radius of 5 arcseconds, applying aperture corrections to the PSF-fitted magnitudes for enhanced precision. Our findings are juxtaposed with earlier studies that relied on shallower observations, providing a more detailed understanding of the galaxy's characteristics.\n\nFurthermore, we have derived new estimates for the distance modulus (DM = 27.9 ± 0.1 mag) and foreground extinction (A_V = 0.10 ± 0.02 mag) for IC 1613. By integrating these values with our photometric data, we calculated the absolute magnitudes: M_B = -15.6 ± 0.3 mag, M_V = -14.7 ± 0.4 mag, M_Rc = -12.8 ± 0.5 mag, and M_Ic = -11.0 ± 0.6 mag. Additionally, we obtained the color indices: U−B = 1.45 ± 0.25 mag, B−V = 0.70 ± 0.06 mag, V−R_c = 0.55 ± 0.05 mag, and V−I_c = 1.00 ± 0.07 mag. These parameters allowed us to estimate the mean metallicity of the stellar population in IC 1613 to be Z = 0.008 ± 0.001 dex, along with an age of approximately 3 billion years (t = 3 Gyrs). This study contributes valuable insights into the stellar composition and evolutionary history of IC 1613, enhancing our understanding of dwarf irregular galaxies within the Local Group.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 4.093146241443879,
        "rewrite-fast-z-score": 1.1055415967851332
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-perturbative renormalization of the chromo-magnetic operator in Heavy Quark Effective Theory and the B* - B mass splitting .\nAbstract:\nWe present an explicit calculation of the non-perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory (HQET). We use this to calculate the leading order contribution to the mass difference between the ground state vector mesons containing a b-quark, i.e., $B^*$- $B$ mixing. The result is compared with lattice QCD calculations at next-to-leading order in HQET perturbation theory. Our results are consistent within errors but do not agree as well as one would like. This may be due to missing higher-order corrections or systematic uncertainties inherent in both approaches. \n \n Introduction \n \n In recent years there has been considerable interest in studying hadronic systems containing a single heavy quark using the framework provided by heavy quark effective theory (HQT)  1  . One important application of HQT is to study the properties of heavy-light mesons such as the bottomonium system  2  , which can then be used to test our understanding of nonrelativistic quantum mechanics  3  .\n \nIn particular, it is interesting to consider how the masses of these states depend on their spin. For example, the lowest lying bb states have spin-parity J P = 0+ and 1− respectively  4  . These two states mix under the weak interaction through the emission and absorption of virtual gluons  5  . At tree level we find that the lightest physical eigenstate is given by:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - perturbative renormalization of the chromo - magnetic operator in Heavy Quark Effective Theory and the B * - B mass separation . Abstract : We present an explicit determination of the non - perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory ( HQET ) .We use this to estimate the led order contribution to the mass ratio between the ground state vector mesons containing a b - quark , i . e . , $ B ^ * $ - $ B $ mixing . The result is compared with lattice QCD calculations at next - to - leading order in HQET perturbation theory .Our results are correct within errors but do not comply as well as one would like . This might be due to missing higher - order corrections or systematic uncertainties inherent in both approaches .Introduction In recent years there has been substantial interest in investigating hadronic networks featuring a single heavy quark using the framework given by massive quark effective theory ( HQT ) 1 . One important use of HQT is to study the properties of heavy - light mesons such as the bottomonium scheme 2 , which can then be used to test our appreciation of nonrelativistic quantum mechanics 3 .In particular , it is curious to consider how the masses of these states depend on their spin . For instance , the lowest lying bb states have spin - parity J P = 0 + and 1− respectively 4 .These two states mix under the strong coupling through the emission and emission of virtual gluons 5 . At tree level we find that the lightest physical eigenstate is given by :",
        "rewrite_text": "**Title:** Non-Perturbative Renormalization of the Chromo-Magnetic Operator in Heavy Quark Effective Theory and the B*-B Mass Separation\n\n**Abstract:** In this study, we provide a comprehensive determination of the non-perturbative renormalization constant associated with the chromomagnetic operator within the framework of Heavy Quark Effective Theory (HQET). This renormalization constant is crucial for accurately estimating the leading-order contribution to the mass ratio of ground state vector mesons that contain a bottom quark, specifically in the context of $B^*$-$B$ mixing. Our findings are juxtaposed with lattice QCD calculations that operate at next-to-leading order in HQET perturbation theory. While our results align with the expected values within the margin of error, they do not achieve the level of agreement that would be ideal. This discrepancy may stem from the omission of higher-order corrections or from systematic uncertainties that are inherent in both the non-perturbative and perturbative approaches employed. \n\nThe motivation for this research arises from the growing interest in the study of hadronic systems that feature a single heavy quark, particularly through the lens of Heavy Quark Effective Theory (HQET). This theoretical framework has proven to be instrumental in examining the characteristics of heavy-light mesons, such as those in the bottomonium spectrum, and serves as a valuable tool for testing our understanding of non-relativistic quantum mechanics. A particularly intriguing aspect of this investigation is the dependence of the masses of these states on their spin configurations. For example, the lowest-lying bottomonium states exhibit spin-parity quantum numbers of $J^P = 0^+$ and $1^-$, respectively. These two states are known to mix due to strong interactions mediated by the emission and absorption of virtual gluons. At the tree level, our analysis reveals that the lightest physical eigenstate can be expressed in a specific form, which we detail in the subsequent sections of the article.",
        "ori-fast-z-score": -0.09578262852211514,
        "water-fast-z-score": 4.385927910529725,
        "rewrite-fast-z-score": -1.4485719366802965
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Redefining the Missing Satellites Problem .\nAbstract:\nThe missing satellites problem (MSP) is one of the most important problems in space science and technology, with applications ranging from satellite navigation to space debris removal.  The MSP asks for all orbits that are stable under gravitational perturbations by known bodies such as planets or asteroids.   In this work we present an algorithm which solves the MSP exactly on any number of dimensions d >= 2 using only O(n log n + m log n) time where n = |S| is the total number of objects in S and m = |E| is the number of edges in E.   Our approach uses a novel combination of techniques including fast matrix multiplication algorithms, data structures based on interval trees, and efficient graph traversal methods. We also show how our results can be used to solve related problems like finding the minimum distance between two given sets of points in R^d. Finally, we demonstrate the practicality of our method through experiments performed on real-world datasets.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Redefining the Missing Satellites Problem . Abstract : The missing satellites question ( MSP ) is one of the most important problems in space research and technology , with applications diverse from satellite communication to space trash destruction .The MSP seeks for all orbits that are stable under gravity perturbations by known objects such as planets or asteroids . In this project we present an algorithm which solves the MSP exactly on any number of dimensions d > = 2 using only O ( n log n + m log n ) time where n = | S | is the total number of items in S and m = | E | is the number of vertices in E . Our solution uses a novel combination of techniques including rapid matrix multiplication methods , data structures based on interval trees , and fast graph traversal methods .We also demonstrate how our findings can be used to solve related problems like finding the minimum distance between two given sets of points in R ^ d . Finally , we prove the practicality of our technique through experiments conducted on real - time datasets .",
        "rewrite_text": "Title: Redefining the Missing Satellites Problem\n\nAbstract: The Missing Satellites Problem (MSP) represents a significant challenge in the fields of space research and technology, with implications that extend to various applications, including satellite communications and the mitigation of space debris. The core objective of the MSP is to identify all orbits that maintain stability under gravitational perturbations caused by known celestial bodies, such as planets and asteroids. In this study, we introduce a novel algorithm capable of precisely addressing the MSP in any dimensional space where d ≥ 2. Our algorithm operates with a time complexity of O(n log n + m log n), where n denotes the total number of items in the set S and m represents the number of vertices in the set E. \n\nThe proposed solution leverages an innovative combination of methodologies, incorporating rapid matrix multiplication techniques, interval tree-based data structures, and efficient graph traversal algorithms. Furthermore, we illustrate the applicability of our findings to related computational challenges, such as determining the minimum distance between two distinct sets of points in R^d. To validate the effectiveness and practicality of our approach, we conducted a series of experiments utilizing real-time datasets, demonstrating the algorithm's robustness and efficiency in solving the MSP. Our results not only advance the understanding of orbital stability in the context of gravitational interactions but also provide a foundational framework for addressing other complex problems in spatial analysis. This work contributes to the ongoing discourse in astrophysics and computational geometry, paving the way for future research and technological advancements in the management of satellite systems and space environment monitoring.",
        "ori-fast-z-score": -1.8439088914585775,
        "water-fast-z-score": 3.362422096189171,
        "rewrite-fast-z-score": -0.44367825470805694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Testing outer boundary treatments for the Einstein equations .\nAbstract:\nWe present results on testing different outer boundary conditions in numerical relativity, using two black hole spacetimes as testbeds.  In particular we consider the case where one or both holes are spinning and use several coordinate systems to evolve these solutions numerically.   We find that the choice of coordinates can have significant effects on the accuracy with which the solution is recovered at large distances from the source region. The most accurate results were obtained by evolving the initial data sets in Kerr-Schild Cartesian coordinates (KSC). However, even when evolved in KSC it was found necessary to impose additional constraints near the outer boundaries in order to obtain stable evolutions over many dynamical timescales. These constraints effectively remove all gravitational radiation from the computational domain. Finally, we also considered an alternative approach based on excision techniques. This method involves removing the interior regions containing singularities from the computational grid and replacing them with suitable analytic expressions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Testing exterior boundary treatments for the Einstein equations . Abstract : We report findings on proving different exterior boundary conditions in mathematical relativity , using two black hole spacetimes as testbeds .In particular we study the case where one or both holes are twisting and use multiple coordinate networks to evolve these solutions numerically . We see that the selection of coordinates can have considerable effects on the accuracy with which the solve is recovered at large distances from the origin region .The most accurate conclusions were obtained by expanding the early data sets in Kerr - Schild Cartesian coordinates ( KSC ) . However , even when evolved in KSC it was found necessary to apply additional constraints near the exterior boundaries in order to obtain stable evolutions over numerous dynamical timescales .These limitations virtually remove all gravity radiation from the theoretical domain . Finally , we also considered an additional method using on excision techniques .This method means eliminating the interior regions containing singularities from the theoretical grid and combining them with suitable analytic expressions .",
        "rewrite_text": "In this study, we present our findings on the evaluation of various exterior boundary conditions within the framework of mathematical relativity, specifically focusing on two distinct black hole spacetimes as experimental models. Our investigation centers on scenarios where one or both black holes exhibit twisting behavior, and we employ multiple coordinate systems to numerically evolve these solutions. Our results indicate that the choice of coordinate system significantly influences the precision of the solutions, particularly at considerable distances from the central region. The most reliable results were achieved by utilizing Kerr-Schild Cartesian coordinates (KSC) for the initial data sets. However, even with the KSC approach, we discovered that imposing additional constraints near the exterior boundaries was essential to ensure stable evolutions across various dynamical timescales. These constraints effectively eliminate all gravitational radiation from the theoretical framework we examined. Furthermore, we explored an alternative approach involving excision techniques, which entails removing the interior regions that contain singularities from the computational grid and replacing them with appropriate analytic expressions. This method offers a different perspective on handling the complexities associated with singularities in black hole spacetimes. Our findings contribute to the ongoing discourse in mathematical relativity by highlighting the critical role of boundary conditions and coordinate choices in the accurate modeling of black hole dynamics.",
        "ori-fast-z-score": -1.3480372031495529,
        "water-fast-z-score": 6.947576354693849,
        "rewrite-fast-z-score": -0.09407208683835973
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Instabilities in the time-dependent neutrino disc in Gamma-Ray Bursts .\nAbstract:\nWe study instabilities that develop in the accretion flow onto black holes during gamma-ray bursts (GRBs). We use an axisymmetric, general relativistic hydrodynamic code to evolve the equations for mass and momentum conservation with self-gravity included. The initial conditions are taken as those of steady-state discs around Kerr black holes. In order to mimic GRB outflows we add a radial velocity perturbation at large radii which is then advected inward by the fluid. This leads to the development of spiral density waves which grow exponentially on a dynamical timescale. These waves can be identified with the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999) . They also lead to the formation of shocks near the inner edge of the disc where they steepen into strong discontinuities. As these shocks propagate outward through the disc their strength decreases due to dissipation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Instabilities in the period - dependent neutrino disc in Gamma - Ray Bursts . Abstract : We research instabilities that develop in the accretion flow onto black holes during gamma - ray bursts ( GRBs ) .We use an axisymmetric , general relativistic hydrodynamic program to evolve the equations for mass and momentum conservation with self - gravity included . The initial conditions are took as those of stable - state discs around Kerr black holes .In order to mimic GRB outflows we create a radial speed perturbation at large radii which is then advected inward by the liquid . This leads to the development of spiral density waves which grow exponentially on a dynamical timescale .These waves can be identified with the Rossby wave disturbance ( RWI ) anticipated analytically by Lovelace et al . ( 1999 ) .They also lead to the formation of shocks near the inner perimeter of the disc where they steepen into deep discontinuities . As these shocks propagate outward through the disc their intensity reduces owing to dissipation .",
        "rewrite_text": "In this study, we investigate the instabilities that arise in the accretion flow onto black holes during gamma-ray bursts (GRBs). Utilizing an axisymmetric, general relativistic hydrodynamic simulation, we evolve the equations governing mass and momentum conservation while incorporating self-gravity effects. Our initial conditions are based on stable-state discs surrounding Kerr black holes. To simulate the outflows associated with GRBs, we introduce a radial velocity perturbation at large distances, which is subsequently advected inward by the fluid dynamics of the disc. This perturbation initiates the formation of spiral density waves that exhibit exponential growth on a dynamical timescale. These waves can be correlated with the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999). Furthermore, the development of these waves leads to the emergence of shocks near the inner edge of the disc, where they steepen into significant discontinuities. As these shocks propagate outward through the disc, their intensity diminishes due to dissipative processes. Our findings contribute to a deeper understanding of the complex dynamics involved in GRB environments and the role of instabilities in shaping the accretion flow onto black holes. This research not only enhances our theoretical framework but also has implications for the observational characteristics of GRBs and their associated phenomena.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 3.1075943842694236,
        "rewrite-fast-z-score": 2.680281337094487
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SPITZER: Accretion in Low Mass Stars and Brown Dwarfs in the Lambda Orionis Cluster .\nAbstract:\nWe present near-infrared photometry for brown dwarfs (BDs) with masses below 0.075 Msun, members of the young open cluster Lambda Orionis. We find that these BDs have redder J-K colors than field objects at similar spectral types. This is consistent with previous studies showing that low mass stars and BDs are more dusty than higher mass counterparts. The observed color excesses can be explained by accretion disks around the BDs. Using our data we estimate disk fractions between 20-50% among the lowest mass BDs in this sample. These results suggest that most BDs form via core accretion as do high-mass stars. However, it remains unclear whether or not all BDs accrete material to become fully fledged planets. In addition, we show that there may exist two populations of very-low mass BDs: one population which has been affected by accretion processes during its formation; another population whose properties resemble those of older field BDs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SPITZER : Accretion in Low Mass Stars and Brown Dwarfs in the Lambda Orionis Cluster . Abstract : We report near - infrared photometry for brown dwarfs ( BDs ) with masses below 0 . 075 Msun , part of the young open dwarf Lambda Orionis .We see that these BDs have redder J - K colors than field objects at comparable spectral classes . This is consistent with previous research indicating that low mass stars and BDs are more dusty than higher mass counterparts .The observed bright excesses can be described by accretion disks around the BDs . Using our information we estimate disk fractions between 20 - 50 % among the lowest mass BDs in this specimen .These data suggest that most BDs form via core accretion as do large - density stars . However , it remains unsure whether or not all BDs accrete material to become completely fledged planets .In addition , we find that there may arise two communities of very - low mass BDs : one community which has been affected by accretion events during its formation ; another population whose properties resemble those of older field BDs .",
        "rewrite_text": "We present a comprehensive analysis of near-infrared photometry for brown dwarfs (BDs) with masses below 0.075 Msun, situated within the young open cluster Lambda Orionis. Our findings reveal that these BDs exhibit redder J-K colors compared to field objects of similar spectral classes. This observation aligns with previous studies suggesting that low-mass stars and BDs possess a higher dust content than their more massive counterparts. The pronounced brightness excesses we observe can be attributed to the presence of accretion disks surrounding these BDs. Based on our data, we estimate that the disk fraction among the lowest mass BDs in our sample ranges from 20% to 50%. These results imply that the majority of BDs likely form through core accretion processes, akin to the formation of higher-mass stars. However, it remains uncertain whether all BDs undergo sufficient accretion to evolve into fully developed planets. Furthermore, our analysis indicates the potential existence of two distinct populations of very low-mass BDs: one group that has experienced significant accretion events during its formation, and another group whose characteristics are more akin to those of older field BDs. This differentiation may have important implications for understanding the formation and evolutionary pathways of brown dwarfs in stellar clusters. Overall, our study contributes valuable insights into the accretion processes and environmental influences affecting the formation of low-mass stars and brown dwarfs in the Lambda Orionis cluster.",
        "ori-fast-z-score": -1.62746694241347,
        "water-fast-z-score": 3.9524197172898554,
        "rewrite-fast-z-score": 1.034792955221957
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Single-molecule imaging of the H-ras membrane-anchor reveals domains in the cytoplasmic leaflet of the cell membrane .\nAbstract:\nThe Ras proteins are small GTPases that play an important role in many cellular processes, including proliferation and differentiation.  The active form of Ras is anchored to the plasma membrane by its C-terminal farnesylated cysteine residue (Cys-186). Here we use single molecule fluorescence microscopy to study the distribution of this anchor on the surface of living cells. We find that the anchor forms clusters with sizes ranging between 50 nm and 1 µm. These clusters have a higher density than expected for a random distribution of molecules. In addition, they show a strong correlation with the actin cytoskeleton. Our results suggest that these clusters may be involved in regulating the activity of Ras at the plasma membrane. Ras proteins are small G-proteins which play an essential role in many signaling pathways inside eukaryotic cells  1  . They cycle between inactive GDP-bound states and active GTP-bound states  2  , where their conformation changes upon binding or hydrolyzing guanosine triphosphate  3  .\nRas proteins can exist as monomers  4  but also dimerize  5  and oligomerize  6  . Their activation depends strongly on their localization  7, 8  : when activated, Ras binds to the inner side of the plasma membrane  9  via its C terminus  10  . This interaction is mediated by the lipid moiety attached to the protein  11  . It has been shown that the prenyl group plays a crucial role in mediating the association of Ras with membranes  12  . However, it was recently suggested that additional interactions involving amino acids close to the prenylated cysteine contribute significantly to the affinity  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Single - molecule scanning of the H - ras membrane - anchor reveals domains in the cytoplasmic leaflet of the cell membrane . Abstract : The Ras proteins are small GTPases that play an important role in multiple cell processes , particularly proliferation and differentiation .The active version of Ras is anchored to the plasma membrane by its C - terminal farnesylated cysteine residue ( Cys - 186 ) . Here we using single molecule fluorescence microscopy to study the spread of this anchor on the surface of living organisms .We see that the anchor creates clusters with sizes varied between 50 nm and 1 µm . These clusters have a higher density than expected for a random distribution of molecules .In addition , they show a high interaction with the actin cytoskeleton . Our results propose that these clusters might be involved in controlling the activity of Ras at the plasma membrane .Ras proteins are small G - proteins which work an essential part in different signaling pathways inside eukaryotic tissues 1 . They cycle between inactive GDP - bound states and active GTP - bound states 2 , where their conformation shifts upon binding or hydrolyzing guanosine triphosphate 3 .Ras proteins can occur as monomers 4 but also dimerize 5 and oligomerize 6 . Their activation depends strongly on their localization 7 , 8 : when activated , Ras binds to the inner half of the plasma membrane 9 via its C terminus 10 .This interaction is mediated by the lipid moiety attached to the protein 11 . It has been shown that the prenyl group plays a crucial role in mediating the organization of Ras with membranes 12 .However , it was recently suggested that extra interactions involving amino acids similar to the prenylated cysteine contribute greatly to the affinity 13 .",
        "rewrite_text": "**Title:** Single-Molecule Scanning of the H-Ras Membrane Anchor Reveals Domains in the Cytoplasmic Leaflet of the Cell Membrane\n\n**Abstract:** Ras proteins, which are small GTPases, are integral to various cellular processes, particularly in regulating proliferation and differentiation. The active form of Ras is tethered to the plasma membrane through its C-terminal farnesylated cysteine residue (Cys-186). In this study, we employed single-molecule fluorescence microscopy to investigate the distribution of this membrane anchor on the surfaces of living cells. Our findings reveal that the anchor forms clusters ranging in size from 50 nm to 1 µm, exhibiting a density that surpasses what would be expected from a random molecular distribution. Notably, these clusters demonstrate significant interactions with the actin cytoskeleton, suggesting a potential role in modulating Ras activity at the plasma membrane.\n\nRas proteins function as small G-proteins that are pivotal in various signaling pathways within eukaryotic cells. They oscillate between inactive GDP-bound states and active GTP-bound states, with conformational changes occurring upon the binding or hydrolysis of guanosine triphosphate. While Ras proteins can exist as monomers, they also have the capacity to dimerize and oligomerize. The localization of Ras is critical for its activation; upon activation, Ras associates with the inner leaflet of the plasma membrane through its C-terminus. This interaction is facilitated by the lipid moiety attached to the protein, which has been shown to play a vital role in the organization of Ras within membrane environments. Recent studies have indicated that additional interactions involving amino acids adjacent to the prenylated cysteine may significantly enhance the protein's affinity for the membrane. Our results contribute to a deeper understanding of the spatial organization of Ras at the plasma membrane and its implications for cellular signaling dynamics.",
        "ori-fast-z-score": 0.811502671200689,
        "water-fast-z-score": 5.926974966883572,
        "rewrite-fast-z-score": -0.9128709291752769
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Graviton Propagator in a Covariant Massive Gravity Theory .\nAbstract:\nWe study the graviton propagator in covariant massive gravity theory with an arbitrary number of gravitons and show that it is given by the sum over all Feynman diagrams which are obtained by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We also present explicit expressions for the first few terms in this expansion, including the leading order term corresponding to the usual Einstein-Hilbert action. The results presented here can be used as input into calculations involving higher-order corrections to gravitational processes such as black hole evaporation. In particular, we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times. \nI. INTRODUCTORY REMARkS\nThe purpose of this work is twofold. First, we will derive the exact expression for the graviton propagator (or Green s function) in covariant massive gravity theories with an arbitrary number of external gravitons. Second, we will use our result to calculate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of freedom associated with the massive spin-2 field. Our analysis follows closely the approach developed in Ref.  1  , where the authors studied the effect of adding massless scalar fields to the standard model of particle physics on the emission rate of Hawking radiation  2  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Graviton Propagator in a Covariant Massive Gravity Theory . Abstract : We explore the graviton propagator in covariant massive gravity physics with an arbitrary number of gravitons and find that it is given by the sum over all Feynman diagrams which are derived by attaching one or more gravitons to each vertex of the tree - level graviton propagator .We additionally give explicit expressions for the first few terms in this expansion , notably the led order term corresponding to the usual Einstein - Hilbert action . The results presented here can be used as input into measurements involving higher - order corrections to gravitational processes such as black hole evaporation .In particular , we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times . I .INTRODUCTORY REMARkS The purpose of this study is twofold . First , we will generate the exact representation for the graviton propagator ( or Green s function ) in covariant massive gravity theories with an arbitrary number of external gravitons .Second , we will use our result to estimate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of liberty involved with the huge spin - 2 field . Our estimate follows carefully the approach developed in Ref .1 , where the papers studied the impact of adding massless scalar fields to the standard theory of particle physics on the emission speed of Hawking radiation 2 .",
        "rewrite_text": "In this article, we investigate the graviton propagator within the framework of covariant massive gravity theories, accommodating an arbitrary number of gravitons. Our findings reveal that the graviton propagator can be expressed as a sum over all Feynman diagrams, which are constructed by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We provide explicit formulations for the initial terms in this expansion, with particular emphasis on the leading order term that corresponds to the conventional Einstein-Hilbert action. The implications of our results are significant, as they can serve as foundational input for analyzing higher-order corrections in gravitational phenomena, such as black hole evaporation. Notably, we demonstrate that incorporating these additional contributions results in alterations to the Hawking temperature at late stages of black hole evaporation.\n\nThe study is driven by two primary objectives. First, we aim to derive an exact representation of the graviton propagator (or Green's function) in covariant massive gravity theories, considering an arbitrary number of external gravitons. Second, we intend to utilize our derived results to assess the impact of these additional degrees of freedom, specifically the massive spin-2 field, on the Hawking radiation emitted by a Schwarzschild black hole. Our analysis builds upon the methodology established in previous research, which examined the effects of introducing massless scalar fields into the standard particle physics framework on the emission characteristics of Hawking radiation. Through this work, we contribute to a deeper understanding of the interplay between graviton dynamics and black hole thermodynamics, paving the way for future explorations in the field of gravitational physics.",
        "ori-fast-z-score": 0.9901475429766744,
        "water-fast-z-score": 5.687367919007337,
        "rewrite-fast-z-score": -0.09325048082403138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Asymptotics for Duration-Driven Long Range Dependent Processes .\nAbstract:\nWe consider the asymptotic behavior of the sample autocovariance function and spectral density functions of stationary processes with regularly varying marginal distributions, which are driven by an infinite order moving average process whose coefficients have regularly varying tails. We show that these quantities can be approximated by those of a finite order autoregressive-moving average (ARMA) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters. The results obtained here extend previous ones given in the literature to more general cases where the driving noise is not necessarily Gaussian or has non-Gaussian components. In particular, we provide new proofs for the existing results when the driving noise is purely Gaussian. Our approach relies heavily on recent developments made in the theory of regular variation and stochastic calculus. As applications, we study two examples of duration-driven LRD models, namely the Cox-Ingersoll-Ross (CIR) model and the Vasicek model.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Asymptotics for Duration - Driven Long Range Dependent Processes . Abstract : We consider the asymptotic behavior of the sample autocovariance function and spectral density functions of stationary systems with regularly varying marginal distributions , which are driven by an endless order moving average process whose coefficients have regularly varying tails .We see that these quantities can be approximated by those of a finite order autoregressive - pushing estimated ( ARMA ) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters . The results derived here extend previous ones given in the writings to more general instances where the driving noise is not necessarily Gaussian or has non - Gaussian components .In particular , we provide fresh proofs for the established results when the driving signal is purely Gaussian . Our solution draws highly on current developments developed in the theoretical of regular variation and stochastic equations .As applications , we study two examples of duration - based LRD models , namely the Cox - Ingersoll - Ross ( CIR ) model and the Vasicek theory .",
        "rewrite_text": "In this article, titled \"Asymptotics for Duration-Driven Long Range Dependent Processes,\" we investigate the asymptotic properties of the sample autocovariance function and spectral density functions associated with stationary systems characterized by regularly varying marginal distributions. These systems are influenced by an infinite-order moving average process, where the coefficients exhibit regularly varying tails. Our findings reveal that, under certain conditions related to the tail behaviors of the autoregressive moving average (ARMA) parameters, these statistical quantities can be approximated by those derived from a finite-order ARMA model, demonstrating weak convergence.\n\nThis research builds upon and extends previous studies by addressing more general scenarios in which the driving noise may not be strictly Gaussian or may include non-Gaussian elements. Notably, we present new proofs for previously established results specifically when the driving signal is entirely Gaussian. Our approach leverages recent advancements in the theory of regular variation and stochastic equations, providing a robust framework for understanding these complex processes.\n\nTo illustrate the practical implications of our theoretical results, we examine two specific examples of duration-based long-range dependent (LRD) models: the Cox-Ingersoll-Ross (CIR) model and the Vasicek model. These applications highlight the relevance of our findings in real-world contexts, demonstrating how the asymptotic behavior of these models can inform our understanding of financial and economic phenomena characterized by long-range dependence. Overall, this work contributes to the growing body of literature on LRD processes and offers insights that may be beneficial for researchers and practitioners in the field.",
        "ori-fast-z-score": -0.5423261445466404,
        "water-fast-z-score": 3.6663142889169062,
        "rewrite-fast-z-score": 1.1272429603813559
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The colour selection of distant galaxies in the UKIDSS Ultra-Deep Survey Early Data Release .\nAbstract:\nWe present an analysis of the colour-selection criteria for identifying high-redshift (z > 6) galaxies using data from the first public release of the UKIRT Infrared Deep Sky Survey (UKIDSS). We use photometric redshifts to select candidate z ~ 7 and 8 galaxies, and then examine their near-infrared colours as measured by UKIDSS. The majority of these candidates are found to be at lower redshift than expected; we find that this is due primarily to contamination by low-redshift interlopers with similar optical-to-near-infrared colours. However, we also identify several robust candidates which have been missed by previous surveys. These include two objects with spectroscopic confirmation of Lyman-break features at z = 7.071 and z = 7.085 respectively. We discuss possible reasons why our sample may differ from those previously published, including differences between the survey areas used and different methods of selecting targets for spectroscopy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The colour selection of distant galaxies in the UKIDSS Ultra - Deep Survey Early Data Release . Abstract : We present an assessment of the colour - choice requirements for finding high - redshift ( z > 6 ) clusters using data from the first public release of the UKIRT Infrared Deep Sky Survey ( UKIDSS ) .We use photometric redshifts to select candidate z ~ 7 and 8 galaxies , and then investigate their near - infrared colours as measured by UKIDSS . The majority of these finalists are found to be at lower redshift than expected ; we find that this is due primarily to poisoning by low - redshift interlopers with similar optical - to - near - infrared colours .However , we also identify several robust candidates which have been missed by earlier surveys . These include two bodies with spectroscopic evidence of Lyman - break characteristics at z = 7 . 071 and z = 7 . 085 respectively .We discuss possible reasons why our sample might differ from those previously reported , notably variations between the sample regions used and changed techniques of deciding targets for spectroscopy .",
        "rewrite_text": "In this study, we evaluate the criteria for color selection necessary for identifying high-redshift clusters (z > 6) utilizing data from the initial public release of the UKIRT Infrared Deep Sky Survey (UKIDSS). Our approach involves the application of photometric redshifts to pinpoint candidate galaxies at redshifts approximately 7 and 8, followed by an analysis of their near-infrared colors as recorded by UKIDSS. Our findings reveal that a significant portion of the selected candidates are actually at lower redshifts than anticipated. This discrepancy is primarily attributed to contamination from low-redshift interlopers that exhibit similar optical and near-infrared color profiles. Despite this challenge, we successfully identify several strong candidates that have eluded detection in previous surveys. Notably, we present two galaxies with spectroscopic evidence of Lyman-break features, located at redshifts of z = 7.071 and z = 7.085, respectively. We explore potential factors that may account for the differences observed in our sample compared to earlier studies, particularly focusing on variations in the regions sampled and the methodologies employed for selecting targets for spectroscopic analysis. This work underscores the importance of refining color selection techniques in the quest to uncover high-redshift galaxies and contributes to the ongoing efforts to enhance our understanding of galaxy formation and evolution in the early universe.",
        "ori-fast-z-score": 1.3416407864998738,
        "water-fast-z-score": 6.184165460191406,
        "rewrite-fast-z-score": -0.8955334711889903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nova Geminorum 1912 and the Origin of the Idea of Gravitational Lensing .\nAbstract:\nThe gravitational lensing effect was first predicted by Einstein in 1915, but it took another 50 years before the first confirmed observation could be made.  In this talk I will describe how one particular event - Nova Geminorum 1912 - played an important role in motivating Einstein to develop his theory of general relativity.   The talk is based on my recent book  The First Lensing Event  (Cambridge University Press). \n\nGravitational lensing occurs when light passes close to a massive object such as a galaxy or cluster of galaxies.  This causes the light rays to bend towards each other so that distant objects appear magnified and distorted.  \n\nIn 1919, Einstein published his famous field equations which described gravity as curvature in space-time caused by matter/energy.  These equations were able to explain many phenomena including the perihelion precession of Mercury s orbit around the Sun. \n\nHowever, there remained some puzzles about these equations.  For example, they did not predict any bending of light passing near the Sun.\n\nEinstein realised he needed to modify his original field equations if he wanted them to account for gravitational lensing effects.  He introduced what we now call the cosmological constant into his new set of equations.  However, despite considerable effort over several decades, no-one has been able to measure the value of this constant with sufficient accuracy to test whether Einstein s prediction is correct.  \n\nThis talk describes how astronomers finally managed to observe gravitational lensing in 1979 using observations of a distant quasar known as Q0957+561A-B.  It also explains why the discovery of gravitational lenses led to the awarding of the Nobel Prize in Physics to Roger Blandford and Roman Vishniac in 1997.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nova Geminorum 1912 and the Origin of the Idea of Gravitational Lensing . Abstract : The gravitational lensing effect was first anticipated by Einstein in 1915 , but it taking another 50 centuries before the first proven measurement came be made .In this talk I will explain how one particular event - Nova Geminorum 1912 - played an important role in motivating Einstein to develop his idea of general relativity . The speech is based on my current work The First Lensing Event ( Cambridge University Press ) .Gravitational lensing occurs when light passes close to a huge structure such as a galaxy or cluster of galaxies . This creates the light rays to bend towards each other so that nearby objects look magnified and distorted .In 1919 , Einstein released his important field equations which described gravity as curvature in space - time induced by matter / energy . These equations were could to explain much processes including the perihelion precession of Mercury s orbit around the Sun .However , there remained some mysteries about these equations . For instance , they did not predict any bending of light moving near the Sun .Einstein realised he needed to modify his previous field equations if he desired them to account for gravitational lensing effects . He incorporated what we now call the cosmological coefficient into his new set of equations .However , despite considerable attempts over numerous years , no - anyone has been able to measure the value of this constant with sufficient accuracy to test whether Einstein s prediction is accurate . This discussion describes how astronomers last managed to observe gravitational lensing in 1979 utilizing observations of a distant quasar known as Q0957 + 561A - B .It additionally explains why the discovery of gravitational lenses leading to the giving of the Nobel Prize in Physics to Roger Blandford and Roman Vishniac in 1997 .",
        "rewrite_text": "**Title: Nova Geminorum 1912 and the Origin of the Idea of Gravitational Lensing**\n\n**Abstract:** The phenomenon of gravitational lensing, which describes the bending of light around massive objects, was first predicted by Albert Einstein in 1915. However, it took nearly five decades before the first definitive measurement of this effect was achieved. This presentation explores the pivotal role that Nova Geminorum 1912 played in inspiring Einstein's formulation of general relativity. Drawing from my ongoing research, \"The First Lensing Event\" (Cambridge University Press), I will discuss how this astronomical event influenced Einstein's thinking. Gravitational lensing occurs when light from a distant source passes near a massive structure, such as a galaxy or a galaxy cluster, causing the light rays to converge and resulting in the magnification and distortion of the observed objects. In 1919, Einstein published his groundbreaking field equations, which characterized gravity as the curvature of space-time caused by matter and energy. These equations successfully explained several phenomena, including the perihelion precession of Mercury's orbit. However, they initially failed to predict the bending of light near the Sun, prompting Einstein to revise his equations to incorporate what is now known as the cosmological constant. Despite extensive efforts over the years, no one was able to measure this constant with the precision necessary to validate Einstein's predictions regarding gravitational lensing until much later. The first successful observation of gravitational lensing occurred in 1979, when astronomers studied the distant quasar Q0957+561A-B. This talk will also highlight the significance of gravitational lensing discoveries, culminating in the awarding of the Nobel Prize in Physics to Roger Blandford and Roman Vishniac in 1997, recognizing their contributions to our understanding of this fascinating aspect of astrophysics.",
        "ori-fast-z-score": 0.6260990336999411,
        "water-fast-z-score": 7.954951288348659,
        "rewrite-fast-z-score": -0.647150228929434
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relation between exchange-only optimized potential and Kohn-Sham methods with finite basis sets; solution of a paradox .\nAbstract:\nWe show that the exchange-only optimized potentials (OEPs) are not equivalent to the Kohn-Sham (KS) method in general, even if one uses an exact density functional for the exchangecorrelation energy. We demonstrate this by solving analytically the OEPs for two simple model systems using Gaussian-type orbitals as basis functions. The results obtained within both approaches differ significantly. In particular, we find that the KS approach yields incorrect values for the total energies of these systems. This is due to the fact that the KS equations do not have solutions corresponding to all possible densities which can be generated by the given basis sets. On the other hand, the OEP formalism always provides unique solutions for any given density matrix. Our analysis shows also how to resolve the apparent paradox arising when trying to apply the OEP formalism to the case where only a limited number of basis functions is used.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relation between exchange - only optimized potential and Kohn - Sham methods with finite basis sets ; solution of a paradox . Abstract : We see that the transfer - only optimized potentials ( OEPs ) are not equivalent to the Kohn - Sham ( KS ) method in general , even if one uses an precise density functional for the exchangecorrelation energy .We demonstrate this by solving analytically the OEPs for two simple model models using Gaussian - class orbitals as basis functions . The results derived within both approaches differ significantly .In particular , we find that the KS approach produces unreliable expressions for the total energies of these systems . This is due to the fact that the KS coefficients do not have solutions equivalent to all possible densities which can be generated by the particular basis sets .On the other hand , the OEP formalism certainly presents specific solutions for any given density matrix . Our theory demonstrates also how to overcome the alleged paradox emerging when trying to apply the OEP formalism to the case where only a small number of basis functions is utilized .",
        "rewrite_text": "Title: Relationship Between Exchange-Only Optimized Potentials and Kohn-Sham Methods with Finite Basis Sets: Resolving a Paradox\n\nAbstract: In this study, we investigate the relationship between exchange-only optimized potentials (OEPs) and Kohn-Sham (KS) methods, revealing that they are not generally equivalent, even when employing an accurate density functional for the exchange-correlation energy. We illustrate this discrepancy by analytically solving the OEPs for two straightforward model systems using Gaussian-type orbitals as basis functions. The findings from both methodologies exhibit significant differences, particularly in the total energy calculations. Our analysis indicates that the KS approach yields unreliable total energy expressions for the examined systems. This unreliability stems from the fact that the KS coefficients do not correspond to all possible densities that can be generated by the chosen basis sets. In contrast, the OEP framework provides specific solutions for any given density matrix, highlighting its robustness. Furthermore, our theoretical exploration addresses the apparent paradox that arises when attempting to apply the OEP formalism with a limited number of basis functions. We propose a resolution to this issue, demonstrating that the OEP can still be effectively utilized under these constraints. This work not only clarifies the distinctions between the OEP and KS methods but also contributes to a deeper understanding of their respective applications in computational quantum chemistry. Our results underscore the importance of carefully selecting basis sets and methodologies in order to achieve accurate and reliable computational outcomes in electronic structure calculations.",
        "ori-fast-z-score": -1.118033988749895,
        "water-fast-z-score": 4.111111111111111,
        "rewrite-fast-z-score": -1.2567574357593625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diffuse X-ray Emission from the Carina Nebula Observed with Suzaku .\nAbstract:\nWe report on diffuse X-ray emission in the Carina Nebula observed by Suzaku. The spectrum is well reproduced by thermal plasma models at kT = 0.7-1 keV and nH = (0.5-2) x 10^(22) cm^{-3}, which are consistent with those obtained previously for other regions within the nebula. We find that the total luminosity of this component amounts to Lx ~ 1.3 x 10^35 erg/sec, corresponding to about 10% of the total energy output of massive stars in the region. This suggests that hot gas produced by stellar winds and/or supernovae plays an important role in heating up the interstellar medium around young open clusters such as Trumpler 14-16. \n \n \n \n Keywords: Diffuse X-rays, Hot plasma, Open cluster, Supernova remnant, Stellar wind, Carina Nebula",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diffuse X - ray Emission from the Carina Nebula Observed with Suzaku . Abstract : We report on diffuse X - ray radiation in the Carina Nebula observed by Suzaku .The spectrum is well illustrated by thermal plasma estimates at kT = 0 . 7 - 1 keV and nH = ( 0 . 5 - 2 ) x 10 ^ ( 22 ) cm ^ { - 3 } , which are compatible with those obtained previously for other regions within the nebula . We see that the total luminosity of this component amounts to Lx ~ 1 . 3 x 10 ^ 35 erg / sec , equivalent to about 10 % of the total energy output of large stars in the region .This implies that heat gas created by stellar winds and / or supernovae plays an important role in heating up the interstellar medium around young open complexes such as Trumpler 14 - 16 . Keywords : Diffuse X - radiation , Hot plasma , Open cluster , Supernova remnant , Stellar wind , Carina Nebula",
        "rewrite_text": "We present our findings on the diffuse X-ray emission from the Carina Nebula, as observed by the Suzaku satellite. Our analysis reveals that the X-ray spectrum can be accurately described by thermal plasma models, with temperatures ranging from kT = 0.7 to 1 keV and hydrogen column densities of nH = (0.5 - 2) x 10^(22) cm^(-3). These parameters align well with previous measurements conducted in other regions of the nebula, suggesting a consistent thermal environment across the area. The total luminosity of the diffuse X-ray emission is estimated to be Lx ~ 1.3 x 10^(35) erg/sec, which constitutes approximately 10% of the total energy output from the massive stars located within the nebula. This significant fraction indicates that the thermal energy generated by stellar winds and supernova explosions is crucial for heating the surrounding interstellar medium, particularly in young open clusters like Trumpler 14-16. Our results underscore the importance of stellar activity in shaping the thermal dynamics of the Carina Nebula and contribute to a deeper understanding of the interactions between massive stars and their environment. The findings have implications for the study of star formation and the evolution of stellar clusters, as well as the broader dynamics of the interstellar medium. \n\nKeywords: Diffuse X-ray emission, thermal plasma, open clusters, supernova remnants, stellar winds, Carina Nebula.",
        "ori-fast-z-score": -1.4832396974191326,
        "water-fast-z-score": 2.138089935299395,
        "rewrite-fast-z-score": -1.116880781646981
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rigorous Results for the Periodic Oscillation of an Adiabatic Piston .\nAbstract:\nWe consider the periodic oscillation of an adiabatic piston in contact with two ideal gases at different temperatures and pressures, separated by a rigid wall.  We prove that if the initial state is close to equilibrium then there exists a unique global solution which converges exponentially fast towards its limit cycle as time goes to infinity.  The proof relies on a combination of techniques from nonlinear analysis (Lyapunov functions) and kinetic theory (Boltzmann equation). In this work we study the dynamics of an adiabatic gas-piston system consisting of one-dimensional ideal gases confined between two walls. One of these walls is fixed while the other moves periodically according to some prescribed law. This problem has been studied extensively since the pioneering works of Maxwell  Max  , Boltzmann  Bol1  , and Sackur-Tetrode  Sac  . It was shown rigorously by Cercignani  Cerc  that under suitable assumptions on the motion of the piston, the solutions converge exponentially fast to their limit cycles. However, it seems difficult to extend his results beyond the case where the temperature difference across the piston remains small during all times. Here we show how to overcome this difficulty using new ideas based on Lyapunov functions combined with estimates coming from kinetic theory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rigorous Results for the Periodic Oscillation of an Adiabatic Piston . Abstract : We consider the periodic oscillation of an adiabatic piston in contact with two ideal gases at different temperatures and pressures , separated by a rigid wall .We prove that if the first state is close to equilibrium then there exists a unique global solution which converges exponentially rapidly towards its limit cycle as time went to infinity . The proof uses on a combination of techniques from nonlinear analysis ( Lyapunov functions ) and kinetic theory ( Boltzmann integral ) .In this study we study the dynamics of an adiabatic gas - cylinder structure comprised of one - dimensional ideal molecules confined between two walls . One of these barriers is fixed while the other moves periodically due to some prescribed law .This problem has been studied frequently since the pioneering works of Maxwell Max , Boltzmann Bol1 , and Sackur - Tetrode Sac . It was shown rigorously by Cercignani Cerc that under suitable assumptions on the movement of the piston , the solutions converge exponentially rapidly to their limit cycles .However , it appears hard to limit his results beyond the case where the temperature difference across the piston remains tiny during all periods . Here we prove how to overcome this trouble using new concepts relying on Lyapunov distributions combined with projections come from kinetic theory .",
        "rewrite_text": "Title: Rigorous Results for the Periodic Oscillation of an Adiabatic Piston\n\nAbstract: This article investigates the periodic oscillation of an adiabatic piston interacting with two ideal gases at distinct temperatures and pressures, separated by a rigid barrier. We establish that when the initial state is sufficiently close to equilibrium, there exists a unique global solution that converges exponentially fast to its limit cycle as time progresses towards infinity. The proof employs a blend of methodologies from nonlinear analysis, specifically Lyapunov functions, and kinetic theory, particularly the Boltzmann integral. Our research focuses on the dynamics of a gas-cylinder system characterized by one-dimensional ideal gas molecules confined between two walls, where one wall is stationary while the other oscillates periodically according to a defined law. This problem has been extensively explored since the foundational contributions of Maxwell, Boltzmann, and Sackur-Tetrode. Previous studies, notably by Cercignani, have rigorously demonstrated that under certain conditions regarding the piston's motion, the solutions exhibit exponential convergence to their limit cycles. However, extending these results beyond scenarios where the temperature differential across the piston remains minimal throughout the oscillation periods has proven challenging. In this work, we present a novel approach to address this limitation by introducing new concepts that integrate Lyapunov distributions with projections derived from kinetic theory. Our findings not only enhance the understanding of the system's dynamics but also provide a framework for analyzing more complex interactions in adiabatic processes involving ideal gases.",
        "ori-fast-z-score": 0.09950371902099892,
        "water-fast-z-score": 5.4193110299036125,
        "rewrite-fast-z-score": 1.4855627054164149
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vaporization and Layering of Alkanols at the Oil/Water Interface .\nAbstract:\nThe vapor pressure, solubility in water, and interfacial tension between oil and water are important parameters for understanding the behavior of crude oils during their production or transport through pipelines.  In this study we have investigated these properties using alkanol monolayers on an aqueous subphase as model systems to mimic the hydrocarbon chains present in crude oils. The results show that the vapor pressures of the alkanols increase with chain length up to C8 but decrease again above C10. This is explained by considering the competition between two opposing effects:  On one hand, increasing chain lengths lead to higher molecular volumes which favor evaporation. On the other hand, longer chains also result in stronger van der Waals interactions within the liquid phase leading to lower vapor pressures. We find that the solubilities of the alkanols follow similar trends as those observed for the vapor pressures. However, the differences in solubility among different chain lengths become smaller when compared to the corresponding differences in vapor pressure. Finally, our measurements reveal that the interfacial tensions between the alkanol layers and the underlying water decreases monotonically with chain length.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vaporization and Layering of Alkanols at the Oil / Water Interface . Abstract : The vapor tension , solubility in water , and interfacial tension between petroleum and water are important characteristics for studying the activity of crude oils during their production or delivery through pipelines .In this study we have researched these characteristics utilizing alkanol monolayers on an aqueous subphase as simulation structures to mimic the hydrocarbon chains present in crude oils . The results show that the vapor pressures of the alkanols increase with chain length up to C8 but decrease again above C10 .This is understood by considering the competition between two contrasting phenomena : On one hand , increasing chain lengths result to higher molecular volumes which favor evaporation . On the other hand , wider chains also lead in heavier van der Waals molecules within the liquid phase leading to smaller liquid pressures .We see that the solubilities of the alkanols follow similar trends as those observed for the liquid pressures . However , the differences in solubility among different chain lengths become smaller when compared to the associated changes in vapor tension .Finally , our measurements reveal that the interfacial pressures between the alkanol sheets and the underlying water reduces monotonically with chain length .",
        "rewrite_text": "**Title: Vaporization and Layering of Alkanols at the Oil/Water Interface**\n\n**Abstract:** Understanding the vapor tension, water solubility, and interfacial tension between petroleum and water is crucial for analyzing the behavior of crude oils during extraction and transportation through pipelines. This study investigates these properties by employing alkanol monolayers on an aqueous subphase as analogs for the hydrocarbon chains found in crude oils. Our findings indicate that the vapor pressures of the alkanols increase with chain length up to C8, after which they decline for chains longer than C10. This behavior can be attributed to the interplay of two opposing effects: as the chain length increases, the molecular volume rises, promoting evaporation; conversely, longer chains result in heavier van der Waals forces within the liquid phase, which diminishes liquid pressures. The solubility patterns of the alkanols exhibit trends similar to those of vapor pressures, although the differences in solubility across varying chain lengths are less pronounced compared to the changes observed in vapor tension. Additionally, our measurements demonstrate that the interfacial pressures between the alkanol monolayers and the underlying water decrease consistently with increasing chain length. These insights contribute to a deeper understanding of the physical properties of alkanols at the oil/water interface, which is essential for optimizing crude oil production and transportation processes. The results of this study not only enhance our comprehension of the behavior of alkanols but also provide valuable information for future research on the interactions between hydrocarbons and aqueous environments.",
        "ori-fast-z-score": -0.10153461651336192,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 0.8620436566990363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HD 98800: A 10-Myr-Old Transition Disk .\nAbstract:\nWe present new near-infrared (NIR) spectroscopy and photometry for the Herbig Be star HD 98800, which is surrounded by an optically thick dust disk with a radius of ~200 AU. The NIR spectrum shows strong emission lines of H I, He II, C III, O IV-VI, Si IV, S VI, Fe II, Mg II, Al II-III, Na I, Ca II, and K I in addition to absorption features due to stellar winds. We find that the observed line profiles are consistent with those predicted by magnetohydrodynamic models of accretion disks around young stars. In particular, we detect blueshifted absorptions associated with infalling gas streams along magnetic field lines onto the central star. These results suggest that HD 98800 has been undergoing active mass accretion over the past few million years. \n \n Keywords: Accretion, Herbig Be star",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HD 98800 : A 10 - Myr - Old Transition Disk . Abstract : We bring latest near - infrared ( NIR ) spectroscopy and photometry for the Herbig Be star HD 98800 , which is enclosed by an optically dense dust disk with a diameter of ~ 200 AU .The NIR spectrum displays strong emitted lines of H I , He II , C III , O IV - VI , Si IV , S VI , Fe II , Mg II , Al II - III , Na I , Ca II , and K I in addition to absorption elements owing to stellar winds . We see that the seen line profiles are compatible with those predicted by magnetohydrodynamic models of accretion disks around early stars .In particular , we perceive blueshifted absorptions associated with infalling gas streams along magnetic field lines onto the central star . These data suggest that HD 98800 has been experiencing active mass accretion over the previous few million years .Keywords: Accretion, Herbig Be star",
        "rewrite_text": "Title: HD 98800: A 10-Myr-Old Transition Disk\n\nAbstract: In this study, we present the latest near-infrared (NIR) spectroscopy and photometric observations of the Herbig Be star HD 98800, which is surrounded by a dense optically thick dust disk with an approximate diameter of 200 AU. Our NIR spectral analysis reveals prominent emission lines from various elements, including H I, He II, C III, O IV-VI, Si IV, S VI, Fe II, Mg II, Al II-III, Na I, Ca II, and K I, alongside absorption features indicative of stellar winds. The observed line profiles align well with predictions from magnetohydrodynamic models that describe the behavior of accretion disks around early-type stars. Notably, we identify blueshifted absorption features that are likely associated with gas streams falling onto the central star, guided by magnetic field lines. These findings imply that HD 98800 has been undergoing significant mass accretion over the past few million years, contributing to our understanding of the evolutionary processes in young stellar objects. The implications of our results extend to the broader context of star formation and the dynamics of transition disks, highlighting the intricate interplay between stellar winds, magnetic fields, and accretion processes in shaping the environments of young stars. Our research underscores the importance of continued observational efforts in the NIR to unravel the complexities of accretion phenomena in Herbig Be stars and similar systems. \n\nKeywords: Accretion, Herbig Be star",
        "ori-fast-z-score": -1.632993161855452,
        "water-fast-z-score": 3.1013193673309134,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The discovery of two extremely low luminosity Milky Way globular clusters .\nAbstract:\nWe report the discovery of two new faint Milky Way open clusters, which we call Palomar 1 and 2 (Palomar 1: RA = 17 h 55 m 00 s , Dec = -28°45 00  ;  Palomar 2: RA = 18 h 04 m 30 s , Dec = -29°00 30 ). They are located in the southern hemisphere at galactocentric distances between 20 kpc and 25 kpc. The total integrated V-band magnitudes for these objects are about 23 mag arcsec-2 . We have obtained deep photometry with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel to study their stellar populations. Both clusters show very similar color-magnitude diagrams dominated by old red giant branch stars. Their ages are estimated as 12 Gyrs using isochrone fitting techniques. These results suggest that both clusters may be among the oldest open clusters known in our Galaxy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The discovery of two extremely lowest luminosity Milky Way globular galaxies . Abstract : We report the discovery of two new faint Milky Way open complexes , which we call Palomar 1 and 2 ( Palomar 1 : RA = 17 h 55 m 00 s , Dec = - 28°45 00 ; Palomar 2 : RA = 18 h 04 m 30 s , Dec = - 29°00 30 ) .They are situated in the southern hemisphere at galactocentric speeds between 20 kpc and 25 kpc . The total integrated V - band magnitudes for these objects are about 23 mag arcsec - 2 .We have achieved deep photometry with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel to study their stellar groups . Both clusters show very identical color - magnitude diagrams dominated by ancient red giant line stars .Their ages are estimated as 12 Gyrs using isochrone fit techniques . These data suggest that both clusters might be among the earliest open complexes recorded in our Galaxy .",
        "rewrite_text": "We present the discovery of two previously unidentified faint open complexes within the Milky Way, designated as Palomar 1 and Palomar 2. Located in the southern hemisphere, Palomar 1 is positioned at right ascension 17h 55m 00s and declination -28°45'00\", while Palomar 2 is found at right ascension 18h 04m 30s and declination -29°00'30\". These complexes are situated at galactocentric distances ranging from 20 kpc to 25 kpc. Our observations reveal that the total integrated V-band magnitudes of these objects are approximately 23 mag arcsec^-2. Utilizing the Hubble Space Telescope's Advanced Camera for Surveys Wide Field Channel, we conducted deep photometric studies to analyze the stellar populations within these clusters. The resulting color-magnitude diagrams for both Palomar 1 and Palomar 2 exhibit striking similarities, characterized by a predominance of ancient red giant branch stars. Through isochrone fitting techniques, we estimate the ages of these clusters to be around 12 billion years, indicating that they may represent some of the earliest open complexes formed in our Galaxy. This discovery not only enhances our understanding of the Milky Way's stellar population but also contributes to the broader knowledge of galaxy formation and evolution. The implications of these findings suggest that Palomar 1 and Palomar 2 could provide valuable insights into the early stages of star formation and the dynamics of globular clusters within the Milky Way. Further research is warranted to explore the properties and histories of these intriguing astronomical structures.",
        "ori-fast-z-score": -1.4569855927715483,
        "water-fast-z-score": 3.6765801200722312,
        "rewrite-fast-z-score": 1.6865480854231356
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SUBARU HDS Observations of a Balmer-Dominated Shock in Tycho s Supernova Remnant .\nAbstract:\nWe report on the detection and analysis of an optical shock front in the supernova remnant (SNR) Tycho using data obtained with Subaru High Dispersion Spectrograph (HDS). The observed spectrum shows strong emission lines of hydrogen, helium, nitrogen, oxygen, sulfur, argon, calcium, magnesium, silicon, iron ions at wavelengths between 3200Å and 9400Å. We find that these line emissions are well reproduced by a model consisting of two components; one is a photoionized plasma component which emits forbidden lines such as  O III  λλ4959, 5007 and  S II λλ6716, 6731, while another is a collisionally ionized plasma component which produces prominent Balmer series lines including Hα. From this result we conclude that the detected shock front is dominated by collisional ionization rather than photo-ionization. \n \n Keywords: Supernova remnants",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SUBARU HDS Observations of a Balmer - Dominated Shock in Tycho s Supernova Remnant . Abstract : We report on the discovery and assessment of an optical shock front in the supernova remnant ( SNR ) Tycho using data acquired with Subaru High Dispersion Spectrograph ( HDS ) .The observed spectrum displays strong absorption lines of carbon , helium , nitrogen , hydrogen , sulfur , argon , potassium , magnesium , silicon , iron ions at wavelengths between 3200Å and 9400Å . We see that these line emissions are well illustrated by a simulation consisting of two parts ; one is a photoionized plasma product which emits forbidden bands such as O III λλ4959 , 5007 and S II λλ6716 , 6731 , while another is a collisionally ionized plasma product which generates numerous Balmer series lines including Hα .From this consequence we conclude that the recovered shock front is dominated by collisional ionization instead than photo - ionization . Keywords : Supernova remnants",
        "rewrite_text": "We present our findings on the identification and analysis of an optical shock front within the Tycho supernova remnant (SNR), utilizing data obtained from the Subaru High Dispersion Spectrograph (HDS). Our observations reveal a spectrum characterized by prominent absorption lines corresponding to various elements, including carbon, helium, nitrogen, hydrogen, sulfur, argon, potassium, magnesium, silicon, and iron ions, spanning wavelengths from 3200 Å to 9400 Å. The spectral features are effectively modeled through a dual-component simulation. The first component represents a photoionized plasma, which is responsible for the emission of forbidden lines such as O III at wavelengths 4959 Å and 5007 Å, as well as S II at 6716 Å and 6731 Å. The second component is attributed to a collisionally ionized plasma, which produces a range of Balmer series lines, prominently featuring Hα. Our analysis indicates that the shock front we have identified is primarily influenced by collisional ionization processes rather than photoionization. This distinction is crucial for understanding the physical conditions and mechanisms at play within Tycho's SNR. The implications of our findings contribute to the broader knowledge of supernova remnants and their evolution, highlighting the complex interplay between different ionization processes in these astrophysical environments. Keywords: Supernova remnants.",
        "ori-fast-z-score": -2.1009029257555607,
        "water-fast-z-score": 2.994344952558949,
        "rewrite-fast-z-score": 0.7337993857053429
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Improving Intrinsic Decoherence in Multi-Quantum-Dot Charge Qubits .\nAbstract:\nWe report on the fabrication and characterization of charge qubits based on self-assembled InAs quantum dots (QDs) embedded in GaAs/AlGaAs heterostructures. We show that by using an optimized growth procedure, we can achieve high quality QD layers with low density of defects which are crucial for achieving good coherence times. The samples were grown by molecular beam epitaxy at 600 °C under As-rich conditions to minimize the formation of threading dislocations. A single layer of self-assembled InAs/GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm thick Al0.3Ga0.7As barrier layer. Finally, a 20 nm thick GaAs capping layer was deposited. The sample structure is shown schematically in Figure 1 . The photoluminescence spectrum shows emission peaks centered around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states associated with charged excitons.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Improving Intrinsic Decoherence in Multi - Quantum - Dot Charge Qubits . Abstract : We report on the fabrication and description of charge qubits based on self - assembled InAs quantum dots ( QDs ) integrated in GaAs / AlGaAs heterostructures .We see that by using an optimized growth technique , we can attain high quality QD layers with minimal concentration of defects which are important for achieving better coherence times . The samples were grown by molecular beam epitaxy at 600 °C under As - rich conditions to minimize the formation of threading dislocations .A single layer of self - assembled InAs / GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm wide Al0 . 3Ga0 . 7As barrier layer . Finally , a 20 nm wide GaAs capping layer was deposited .The sample structure is displayed schematically in Figure 1 . The photoluminescence spectrum displays emission levels focused around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states correlated with charged excitons .",
        "rewrite_text": "Title: Enhancing Intrinsic Decoherence in Multi-Quantum-Dot Charge Qubits\n\nAbstract: In this study, we present the fabrication and characterization of charge qubits utilizing self-assembled InAs quantum dots (QDs) integrated within GaAs/AlGaAs heterostructures. Our findings indicate that by employing an optimized growth technique, we can produce high-quality QD layers with a significantly reduced defect concentration, which is crucial for improving coherence times. The quantum dot samples were synthesized using molecular beam epitaxy at a temperature of 600 °C under arsenic-rich conditions, a strategy designed to minimize the occurrence of threading dislocations. Following this, a single layer of self-assembled InAs/GaAs QDs was achieved through a brief annealing process at 650 °C for 10 seconds, after which a 50 nm thick Al0.3Ga0.7As barrier layer was deposited. To complete the structure, a 20 nm thick GaAs capping layer was added. The schematic representation of the sample structure is illustrated in Figure 1. The photoluminescence spectrum of the fabricated samples reveals emission peaks centered around 1280 nm, which correspond to the ground state excitonic transitions of individual quantum dots, alongside higher energy states associated with charged excitons. These results underscore the potential of our optimized growth methods in enhancing the performance of multi-quantum-dot charge qubits, paving the way for advancements in quantum computing technologies. The implications of improved coherence times are significant, as they can lead to more reliable qubit operations and greater scalability in quantum systems.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 2.8490144114909484,
        "rewrite-fast-z-score": 0.6704783996548059
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Controversy on a dispersion relation for MHD waves .\nAbstract:\nThe author presents the controversy surrounding the derivation and use of a dispersion relation for magnetohydrodynamic (MHD) waves in plasmas, which is based upon an assumption that the plasma frequency is much greater than the wave frequency.  The author shows how this assumption leads to incorrect results when applied to Alfvén waves propagating parallel or antiparallel to magnetic field lines; he also discusses other problems with using such relations. Finally, he proposes a new method for deriving a dispersion relation for these waves. This article was originally published as part of the series  Perspectives in Plasma Physics  by the American Institute of Physics. It has been reproduced here with permission. For more information about reproducing AIP content visit http://journals.aip.org/authors/rightsandpermissions. The author thanks Dr. J. D. Heymann for his help in preparing this version of the manuscript.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Controversy on a dispersion relation for MHD waves . Abstract : The author presents the controversy concerning the derivation and use of a dispersion relation for magnetohydrodynamic ( MHD ) waves in plasmas , which is based upon an assume that the plasma rate is much larger than the wave frequency .The author shows how this assumption results to false results when applied to Alfvén currents propagating parallel or antiparallel to magnetic force lines ; he also explains other difficulties with utilizing such relations . Finally , he proposes a new method for deriving a dispersion relation for these waves .This page was originally published as part of the program Perspectives in Plasma Physics by the American Institute of Physics . It has been reproduced here with permit .For more information about reproducing AIP material seek http : / / journals . aip . org / authors / rightsandpermissions . The author grateful Dr . J . D . Heymann for his help in preparing this copy of the manuscript .",
        "rewrite_text": "The article titled \"Controversy on a Dispersion Relation for MHD Waves\" addresses significant debates surrounding the derivation and application of a dispersion relation for magnetohydrodynamic (MHD) waves in plasma environments. The author critically examines the prevalent assumption that the plasma rate is substantially greater than the wave frequency, a premise that underpins many existing models. This assumption, the author argues, leads to erroneous conclusions, particularly when analyzing Alfvén currents that propagate either parallel or antiparallel to magnetic field lines. The paper highlights various challenges associated with the application of traditional dispersion relations, emphasizing the need for a more accurate framework to understand MHD wave behavior.\n\nIn addition to outlining the limitations of current methodologies, the author introduces a novel approach for deriving a dispersion relation that better accommodates the complexities of MHD waves. This new method aims to rectify the inaccuracies stemming from the oversimplified assumptions of previous models, thereby providing a more reliable tool for researchers in the field. The discussion is situated within the broader context of plasma physics, contributing to ongoing dialogues about the theoretical underpinnings of MHD phenomena.\n\nThis work was initially published as part of the \"Perspectives in Plasma Physics\" series by the American Institute of Physics and is reproduced here with permission. The author expresses gratitude to Dr. J. D. Heymann for his assistance in preparing this manuscript. For further details regarding the reproduction of AIP materials, readers are encouraged to visit the AIP's rights and permissions page.",
        "ori-fast-z-score": -0.8551861104941365,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": 1.2567574357593625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sensitivity of low degree oscillations to the change in solar abundances .\nAbstract:\nWe have studied how changes in the abundance pattern affect the frequencies and amplitudes of low-degree p-mode oscillations using two different methods, namely forward modelling (with the help of MESA stellar evolution code) and inverse analysis with the help of GYRE pulsation code. We find that for both methods there is an increase in the frequency difference between observed and theoretical values when we decrease the metallicity by 0.1 dex or more. The effect on amplitude differences are less pronounced but still significant. This shows that it will be very difficult to determine accurate absolute metallicities of stars based only on asteroseismic data.  For example, if one were to use the results obtained here as priors in the Bayesian framework then this would lead to underestimated uncertainties in the derived parameters such as age and mass. However, our study also suggests that relative metallicities can be determined quite accurately even without any additional information about other physical properties of the star.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sensitivity of lowest degree oscillations to the shift in solar abundances . Abstract : We have researched how variations in the abundance pattern affect the frequencies and amplitudes of lowest - degree p - mode oscillations using two different methods , namely backwards modelling ( with the aid of MESA stars evolution code ) and inverse investigation with the aid of GYRE pulsation code .We see that for both approaches there is an increase in the frequency variation between measured and theoretical values when we decrease the metallicity by 0 . 1 dex or more . The impact on frequency differences are less pronounced but still significant .This shows that it will be very difficult to predict accurate absolute metallicities of stars based only on asteroseismic data . For instance , if one were to use the results derived here as priors in the Bayesian framework then this might lead to underestimated uncertainties in the derived values such as age and mass .However , our research also demonstrates that relative metallicities can be determined quite accurately even without any additional information about other material structures of the star .",
        "rewrite_text": "Title: Sensitivity of Lowest Degree Oscillations to Variations in Solar Abundances\n\nAbstract: This study investigates the influence of changes in the abundance pattern of elements on the frequencies and amplitudes of the lowest-degree p-mode oscillations in stars. Utilizing two distinct methodologies—backwards modeling through the MESA stellar evolution code and inverse analysis via the GYRE pulsation code—we analyze how these variations affect oscillation characteristics. Our findings reveal that both approaches indicate a notable increase in the frequency discrepancies between observed and theoretical values when metallicity is reduced by 0.1 dex or more. While the effects on frequency differences are less pronounced, they remain significant. This suggests that accurately determining the absolute metallicities of stars based solely on asteroseismic data poses considerable challenges. For example, employing the results from this study as priors within a Bayesian framework could potentially lead to underestimations of uncertainties in derived stellar parameters, such as age and mass. Nevertheless, our research highlights that relative metallicities can be accurately assessed without requiring supplementary information regarding the star's other material structures. This finding underscores the potential of asteroseismic data in refining our understanding of stellar compositions, despite the complexities introduced by variations in solar abundances. Overall, our work contributes to the ongoing discourse on the interplay between stellar oscillations and elemental abundances, paving the way for more precise stellar modeling and characterization in future studies.",
        "ori-fast-z-score": -0.8834522085987723,
        "water-fast-z-score": 5.019011475427825,
        "rewrite-fast-z-score": 0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Strong peak points and denseness of strong peak functions .\nAbstract:\nWe study the properties of strong peak points in Banach spaces, which are defined as follows.  Let X be a real or complex normed space with dual space X*. A point x # X is called a strong peak point if there exists an f # S(X) such that |f (x)| = sup{|f (y)| : y # X}. We prove that every separable reflexive Banach space has a dense set of strong peak points. As applications we show that every separable reflexivizable Banach space contains a copy of c 0 , and that every separable superreflexive Banach space contains a subspace isomorphic to l p for some 1 < p < + . In this note we study the properties of strong peaks points in Banach spaces. The concept was introduced by J. Lindenstrauss  L  who proved that every separable reflexible Banach space has a nonempty set of strong peak points; see also  JL1  .\nIn Section 2 we give several equivalent characterizations of strong peak points. In particular, it turns out that a point x # X is a strong peak point if and only if there exist two sequences (a n ) and (b n ) in R satisfying lim n Ä a n = lim n Ä b n = 1 and lim n Ä a n &1Â2 b n = 0 such that the sequence (a n b n ) converges weakly to zero but not strongly. This characterization enables us to prove our first main result on the density of strong peak points in separable reflexive BanACH spaces. \nTheorem 3. Every separable reflexive BanACH space has a dense set SP(X) of strong peak points.\n\nAs immediate consequences of Theorem 3 we obtain the following results. (i) Every separable reflexivizable space contains a copy of c0.\n(ii) Every separable superreflexive space contains a subspace isomorphic",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Strong peak points and denseness of strong peak functions . Abstract : We research the properties of strong peak points in Banach spaces , which are defined as follows .Let X be a real or complex normed space with dual space X * . A point x # X is dubbed a strong peak point if there exists an f # S ( X ) such that | h ( x ) | = sup { | f ( y ) | : y # X } .We establish that every separable reflexive Banach space has a dense setting of stable peak points . As applications we prove that every separable reflexivizable Banach space includes a copy of c 0 , and that every separable superreflexive Banach space includes a subspace isomorphic to l p for some 1 < p < + .In this note we study the properties of strengthened peaks points in Banach spaces . The concept was introduced by J . Lindenstrauss L who proved that every separable reflexible Banach space has a nonempty set of strong peak points ; hear also JL1 .In Section 2 we give numerous equivalent characterizations of strengthened peak points . In particular , it turns out that a point z # X is a strong peak point if and only if there exist two sequences ( a n ) and ( b n ) in R satisfying lim n Ä a n = lim n Ä b n = 1 and lim n Ä a n & 1Â2 b n = 0 such that the sequence ( a n b n ) converges weakly to zero but not strongly .This description helps us to prove our first major result on the density of stable peak points in separable reflexive BanACH spaces . Theorem 3 .Every separable reflexive BanACH space has a dense setting SP ( X ) of stable peak points . As immediate implications of Theorem 3 we obtain the following results .( i ) Every separable reflexivizable set contains a copy of c0 . ( ii ) Every separable superreflexive space includes a subspace isomorphic",
        "rewrite_text": "**Title:** Strong Peak Points and Denseness of Strong Peak Functions\n\n**Abstract:** This article investigates the characteristics of strong peak points within the context of Banach spaces. We define a strong peak point in a normed space \\(X\\) (which can be either real or complex) with its dual space \\(X^*\\) as a point \\(x \\in X\\) for which there exists a function \\(f \\in S(X)\\) such that \\(|h(x)| = \\sup \\{ |f(y)| : y \\in X \\}\\). Our findings reveal that every separable reflexive Banach space possesses a dense collection of stable peak points. This result has significant implications, leading us to demonstrate that every separable reflexivizable Banach space contains a copy of \\(c_0\\), and that every separable superreflexive Banach space includes a subspace that is isomorphic to \\(l_p\\) for some \\(1 < p < +\\infty\\).\n\nThe concept of strong peak points was initially introduced by J. Lindenstrauss, who established that every separable reflexive Banach space has a non-empty set of such points. In Section 2 of our study, we present various equivalent characterizations of strong peak points. Notably, we show that a point \\(z \\in X\\) qualifies as a strong peak point if and only if there exist two sequences \\((a_n)\\) and \\((b_n)\\) in \\(\\mathbb{R}\\) such that \\(\\lim_{n \\to \\infty} a_n = \\lim_{n \\to \\infty} b_n = 1\\) and \\(\\lim_{n \\to \\infty} a_n - \\frac{1}{2} b_n = 0\\), while the sequence \\((a_n b_n)\\) converges weakly to zero but not strongly. This characterization is instrumental in establishing our primary result regarding the density of stable peak points in separable reflexive Banach spaces. Specifically, we prove that every separable reflexive Banach space contains a dense set of stable peak points, denoted as \\(SP(X)\\). The implications of this theorem are profound, leading to the conclusion that every separable reflexivizable space contains a copy of \\(c_0\\), and every separable superreflexive space has a subspace isomorphic to \\(l_p\\).",
        "ori-fast-z-score": -1.8864844365675972,
        "water-fast-z-score": 3.1382990398158928,
        "rewrite-fast-z-score": 0.5388159060803247
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Broad-band Spectral Evolution of Scorpius X-1 along its Color-Color Diagram .\nAbstract:\nWe present the broad-band spectral evolution of the X-ray binary system Sco-X1, which is one of the brightest persistent sources in our Galaxy and has been observed by many satellites over more than three decades. We use data obtained with Suzaku (0.5-10 keV), Swift/BAT (15-50 keV) and INTEGRAL/IBIS (20-100 keV). The broadband spectrum can be described well by an absorbed power-law model modified by reflection off cold matter. In addition to this continuum component we find evidence for emission lines at 6.4-6.7 keV that are consistent with fluorescent iron Kα line emission produced by photoionization of neutral material surrounding the central source. These results suggest that there may exist a hot accretion disk around the compact object as well as a cool outer disk or torus-like structure. This scenario is supported by recent observations made with Chandra/HETG. \n \n Keywords: Broadband spectroscopy, Reflection features",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Broad - band Spectral Evolution of Scorpius X - 1 along its Color - Color Diagram . Abstract : We present the broad - band spectral evolution of the X - ray binary system Sco - X1 , which is one of the brightest persistent sources in our Galaxy and has been observed by many satellites over more than three decades .We use data acquired with Suzaku ( 0 . 5 - 10 keV ) , Swift / BAT ( 15 - 50 keV ) and INTEGRAL / IBIS ( 20 - 100 keV ) . The broadband spectrum can be described good by an absorption power - law theory reconstructed by absorption off cool matter .In addition to this continuum component we find proof for emission lines at 6 . 4 - 6 . 7 keV that are consistent with fluorescent iron Kα line emission created by photoionization of neutral metal surrounding the main source . These conclusions propose that there may exist a heated accretion disk around the compact object as well as a cool inner disk or torus - like structure .This prediction is backed by recent observations made with Chandra / HETG . Keywords : Broadband spectroscopy , Reflection features",
        "rewrite_text": "**Title:** Broad-band Spectral Evolution of Scorpius X-1 along its Color-Color Diagram\n\n**Abstract:** In this study, we investigate the broad-band spectral evolution of the X-ray binary system Scorpius X-1 (Sco X-1), recognized as one of the most luminous persistent X-ray sources in our galaxy. Over the past three decades, Sco X-1 has been extensively monitored by various space-based observatories. Our analysis utilizes data from Suzaku (0.5 - 10 keV), Swift/BAT (15 - 50 keV), and INTEGRAL/IBIS (20 - 100 keV) to construct a comprehensive understanding of its spectral characteristics. The broad-band spectrum is effectively modeled using an absorption power-law framework, which accounts for the absorption effects from cooler matter surrounding the source. \n\nIn addition to the continuum emission, we identify significant evidence for the presence of emission lines within the energy range of 6.4 - 6.7 keV. These lines are indicative of fluorescent iron Kα emission, likely resulting from the photoionization of neutral metals in the vicinity of the primary X-ray source. Our findings suggest the existence of a heated accretion disk surrounding the compact object, alongside a cooler inner disk or toroidal structure. This hypothesis is further supported by recent observations obtained from Chandra/HETG, which provide additional insights into the complex environment of Sco X-1. The implications of our results contribute to a deeper understanding of the physical processes occurring in X-ray binaries and the role of reflection features in their spectral evolution. \n\n**Keywords:** Broadband spectroscopy, Reflection features.",
        "ori-fast-z-score": 0.819288030372914,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": 1.539600717839002
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Consistent reasoning about a continuum of hypotheses on the basis of finite evidence .\nAbstract:\nWe present an algorithm for consistent hypothesis testing in which we consider all possible hypotheses that are compatible with some given set of observations, and select those that maximize their posterior probability according to Bayes  theorem.  We show how this can be done efficiently by using dynamic programming techniques. The resulting algorithm is optimal up to constant factors under certain conditions. Our approach also allows us to reason consistently over multiple experiments performed sequentially or simultaneously. This problem has been studied extensively in statistics but only recently in artificial intelligence (AI). In AI it was first considered as part of the PAC learning framework where one seeks algorithms that learn concepts from examples while making few mistakes. However, these approaches do not provide any guarantees when there exists more than one concept that fits the data equally well. In contrast our method provides provable guarantees even if several hypotheses fit the data equally well. Finally, we demonstrate the practicality of our approach through two applications:  1) A new algorithm for finding explanations in probabilistic databases; 2) An improved algorithm for identifying protein families based on sequence alignment.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Consistent thinking about a continuum of hypotheses on the basis of finite evidence . Abstract : We create an algorithm for consistent hypothesis testing in which we investigate all possible hypotheses that are compatible with some particular set of findings , and select those that maximize their posterior likelihood according to Bayes principle .We see how this can be performed efficiently by using dynamic programming tools . The resulting algorithm is efficient up to constant factors under certain conditions .Our solution therefore allows us to reason consistently over multiple studies performed sequentially or independently . This problem has been studied frequently in statistics but only lately in artificial intelligence ( AI ) .In AI it was first considered as part of the PAC learning framework where one seeks methods that learn concepts from instances while making few errors . However , these approaches do not offer any promise when there exists more than one concept that fits the information perfectly well .In comparison our technique provides provable assurance even if several hypotheses fitted the information equally perfectly . Finally , we prove the practicality of our approach through two applications : 1 ) A new algorithm for finding explanations in probabilistic libraries ; 2 ) An upgraded method for finding protein families based on sequence alignment .",
        "rewrite_text": "Title: Consistent Thinking About a Continuum of Hypotheses Based on Finite Evidence\n\nAbstract: In this article, we present a novel algorithm designed for consistent hypothesis testing, which systematically explores all hypotheses compatible with a given set of empirical findings. By leveraging Bayes' principle, we identify hypotheses that maximize their posterior likelihood. Our approach employs dynamic programming techniques to enhance computational efficiency, ensuring that the algorithm operates effectively under specific conditions, with efficiency maintained up to constant factors. This advancement enables consistent reasoning across multiple studies, whether conducted sequentially or independently. While the challenge of hypothesis testing has been extensively examined in the field of statistics, its application within artificial intelligence (AI) has gained traction only recently. Initially, this issue was addressed within the framework of Probably Approximately Correct (PAC) learning, which focuses on developing methods that learn concepts from instances while minimizing errors. However, traditional PAC learning methods fall short when multiple concepts can equally explain the observed data. In contrast, our proposed technique offers a robust solution, providing provable guarantees even when several hypotheses align perfectly with the available evidence. To demonstrate the practical applicability of our method, we showcase two significant applications: first, we introduce a new algorithm for deriving explanations within probabilistic libraries; second, we present an enhanced method for identifying protein families through sequence alignment. These applications underscore the versatility and effectiveness of our approach in addressing complex problems in hypothesis testing and data interpretation.",
        "ori-fast-z-score": -2.331086069657434,
        "water-fast-z-score": 5.439200829200679,
        "rewrite-fast-z-score": 1.153563462240948
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Reconstructing the Intrinsic Triaxial Shape of the Virgo Cluster .\nAbstract:\nWe present an analysis of gravitational lensing data for the galaxy cluster Abell 1689, which is located at redshift z = 0.183 and has been observed by Hubble Space Telescope (HST) in three bands (F450W, F625W, F775W). We use these observations to reconstruct the intrinsic triaxial shape of this massive cluster using two different methods. First we apply the method developed by Sereno & Umetsu (2006) , where the projected mass distribution on the sky is modeled as a superposition of elliptical NFW halos with varying axial ratios. Second, we employ the technique proposed by Corless et al. (2009), where the three-dimensional density profile is described by a generalized Navarro-Frenk-White model. Both models are fitted simultaneously to the HST shear measurements obtained within a circular aperture centered on the brightest cluster galaxy. The best-fit parameters inferred from both approaches agree well with each other.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Reconstructing the Intrinsic Triaxial Shape of the Virgo Cluster . Abstract : We report an assessment of gravitational lensing data for the galaxy cluster Abell 1689 , which is situated at redshift z = 0 . 183 and has been observed by Hubble Space Telescope ( HST ) in three bands ( F450W , F625W , F775W ) .We use these observations to reconstruct the intrinsic triaxial shape of this massive cloud using two different methods . First we apply the method developed by Sereno & Umetsu ( 2006 ) , where the projected mass distribution on the sky is modeled as a superposition of elliptical NFW halos with varying axial ratios .Second , we utilize the method adopted by Corless et al . ( 2009 ) , where the three - dimensional density profile is characterized by a generalized Navarro - Frenk - White model .Both estimates are fit concurrently to the HST shear measurements obtained within a circular aperture located on the brightest cluster galaxy . The best - fitting factors inferred from both approaches agree well with each other .",
        "rewrite_text": "We present a comprehensive analysis of gravitational lensing data pertaining to the galaxy cluster Abell 1689, located at a redshift of z = 0.183. This cluster has been extensively observed using the Hubble Space Telescope (HST) across three distinct bands: F450W, F625W, and F775W. Our study aims to reconstruct the intrinsic triaxial shape of this massive galaxy cluster by employing two distinct methodologies. The first approach is based on the technique introduced by Sereno & Umetsu (2006), which models the projected mass distribution on the sky as a combination of elliptical Navarro-Frenk-White (NFW) halos, each with varying axial ratios. The second method follows the framework established by Corless et al. (2009), wherein the three-dimensional density profile is described using a generalized Navarro-Frenk-White model. Both methodologies are applied concurrently to the HST shear measurements, which are collected within a circular aperture centered on the brightest cluster galaxy. Our findings reveal that the best-fitting parameters derived from both methods exhibit a strong agreement, reinforcing the reliability of our reconstruction of the cluster's intrinsic shape. This work not only enhances our understanding of the structural properties of Abell 1689 but also contributes to the broader field of cosmology by providing insights into the formation and evolution of galaxy clusters in the universe. The implications of our results extend to the study of dark matter distribution and the gravitational dynamics within galaxy clusters, offering a deeper comprehension of the underlying physical processes at play.",
        "ori-fast-z-score": 1.270001270001905,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 0.5773502691896257
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CP violation due to multi Froggatt-Nielsen fields .\nAbstract:\nWe study the possibility that CP violation in the Standard Model is induced by several fields with nontrivial transformation properties under flavor and charge conjugation, which we call  Froggatt-Nielsen  (FN) fields.  We show how such FN fields can be incorporated into an effective Lagrangian for leptons and quarks at low energies. In this framework, we derive constraints on the number of FN fields allowed by current experimental data. Finally, we discuss possible implications of our results for models beyond the Standard Model. Introduction - The Standard Model (SM), despite its great successes, does not provide any explanation for why there are three generations of fermions or why their masses differ so much among themselves  1  . These questions have motivated many extensions of the SM  2  , including those based on grand unification  3  .\nIn these theories it has been shown  4  that new sources of CP violation may arise through phases associated with Yukawa couplings between Higgs boson(s) and fermion mass eigenstates. However, since all known particles couple to the same scalar doublet H = (H + , H 0 ) T / √ 2, one expects that the resulting contributions to CP-violating observables will be too small to explain observed phenomena  5  . This problem could be alleviated if additional scalars were introduced  6  but then other problems would appear  7, 8  . Alternatively, one might consider extending the gauge group  9  and/or introducing extra vector-like fermions  10  . Another possibility consists in considering more general transformations than phase rotations when constructing the most general form of the CKM matrix  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CP violation due to multi Froggatt - Nielsen fields . Abstract : We research the idea that CP compliance in the Standard Model is caused by many fields with nontrivial transformation properties under flavor and charge conjugation , which we call Froggatt - Nielsen ( FN ) varieties .We see how such FN fields can be included into an efficient Lagrangian for leptons and quarks at low energies . In this framework , we derive restrictions on the quantity of FN fields supported by current experimental evidence .Finally , we explain possible possibilities of our findings for models beyond the Standard Model . Introduction - The Standard Model ( SM ) , despite its great successes , does not offer any justification for why there are three generations of fermions or why their masses vary so greatly among themselves 1 .These questions have prompted many extensions of the SM 2 , notably those based on grand unified 3 . In these theories it has been shown 4 that new sources of CP violation may arise through phases related with Yukawa couplings between Higgs boson ( s ) and fermion mass eigenstates .However , since all known objects pair to the same scalar doublet H = ( H + , H 0 ) T / √ 2 , one expects that the resulting contributions to CP - breaking observables will be too small to explain known phenomena 5 . This problem could be alleviated if added scalars were introduced 6 but then other difficulties would appear 7 , 8 .Alternatively , one might propose extending the gauge group 9 and / or introducing extra vector - like fermions 10 . Another possibility consists in considering more general processes than phase rotations when constructing the most general form of the CKM matrix 11 .",
        "rewrite_text": "**Title: CP Violation Induced by Multiple Froggatt-Nielsen Fields**\n\n**Abstract:** In this study, we investigate the hypothesis that CP violation within the Standard Model (SM) arises from the presence of multiple fields exhibiting complex transformation properties under flavor and charge conjugation, which we refer to as Froggatt-Nielsen (FN) fields. We demonstrate how these FN fields can be effectively integrated into a low-energy Lagrangian framework for leptons and quarks. Through this approach, we derive constraints on the number of FN fields that are consistent with current experimental data. Our analysis reveals that the inclusion of FN fields can provide a richer structure to the SM, potentially addressing some of its limitations, particularly regarding the generation of fermion masses and the observed hierarchy among them. \n\nThe Standard Model, despite its numerous successes, lacks an explanation for the existence of three generations of fermions and the significant variations in their masses. This has led to various extensions of the SM, including those based on grand unification theories, which suggest that new sources of CP violation may emerge from phases associated with Yukawa couplings between Higgs bosons and fermion mass eigenstates. However, since all known particles couple to the same scalar doublet, the contributions to CP-violating observables are anticipated to be insufficient to account for observed phenomena. While introducing additional scalar fields could mitigate this issue, it introduces new challenges. Other approaches, such as extending the gauge group or incorporating extra vector-like fermions, have also been proposed. Furthermore, exploring more general processes beyond simple phase rotations in the construction of the CKM matrix may yield additional insights. Our findings suggest that the FN framework could offer a viable pathway for developing models that extend beyond the Standard Model, potentially leading to a deeper understanding of CP violation and its implications for particle physics.",
        "ori-fast-z-score": -0.29277002188455997,
        "water-fast-z-score": 6.609001368025944,
        "rewrite-fast-z-score": -0.3508232077228117
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A mathematical and computational review of Hartree-Fock SCF methods in Quantum Chemistry .\nAbstract:\nThe aim of this thesis is to provide an overview on the state-of-the-art Hartree-Fock Self-Consistent-Field (SCF) methods for solving quantum chemical problems, with special emphasis on their numerical aspects.  The first chapter introduces basic concepts related to molecular orbital theory and electronic structure calculations using density functional theory (DFT). In particular, we discuss how DFT can be used as a tool to study ground-state properties of molecules by means of Kohn-Sham orbitals. We also present some fundamental results concerning the convergence of iterative schemes that are commonly employed within self-consistent field approaches. The second chapter deals with the description of several classes of algorithms based on direct minimization techniques which have been developed over the last decades to solve the Hartree-Fock equations numerically. These include the Roothaan-Hall method, the Davidson algorithm, and its variants such as the Pulay-Davidson scheme or the linearized Davidson approach. Finally, we introduce the concept of preconditioning and illustrate it through two examples.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A mathematical and mathematical discussion of Hartree - Fock SCF methods in Quantum Chemistry . Abstract : The goal of this dissertation is to provide an overview on the state - of - the - art Hartree - Fock Self - Consistent - Field ( SCF ) techniques for solving quantum chemical problems , with special emphasis on their numerical parts .The first section introduces basic concepts related to molecular orbital theory and electronic stability analyses using density functional theory ( DFT ) . In particular , we explain how DFT can be used as a technique to study ground - state properties of molecules by means of Kohn - Sham orbitals .We additionally offer some fundamental findings concerning the convergence of iterative strategies that are often employed within self - coherent field methods . The second chapter deals with the description of several classes of algorithms based on direct minimization techniques which have been built over the last decades to solve the Hartree - Fock equations numerically .These include the Roothaan - Hall method , the Davidson approximation , and its versions such as the Pulay - Davidson scheme or the linearized Davidson technique . Finally , we introduce the idea of preconditioning and explain it through two examples .",
        "rewrite_text": "**Title:** A Mathematical Exploration of Hartree-Fock SCF Methods in Quantum Chemistry\n\n**Abstract:** This dissertation aims to present a comprehensive overview of the advanced Hartree-Fock Self-Consistent Field (SCF) methods utilized in addressing quantum chemical challenges, with a particular focus on their numerical aspects. The initial section lays the groundwork by introducing essential concepts related to molecular orbital theory and the analysis of electronic stability through density functional theory (DFT). We delve into the application of DFT as a powerful tool for investigating the ground-state properties of molecules, utilizing Kohn-Sham orbitals as a framework. Furthermore, we discuss key insights regarding the convergence of iterative methods commonly employed in self-consistent field approaches. \n\nIn the subsequent chapter, we explore various classes of algorithms that have been developed over the past few decades to numerically solve the Hartree-Fock equations through direct minimization techniques. This includes an examination of the Roothaan-Hall method, the Davidson approximation, and its adaptations, such as the Pulay-Davidson scheme and the linearized Davidson technique. These algorithms represent significant advancements in the computational efficiency and accuracy of Hartree-Fock calculations.\n\nLastly, we introduce the concept of preconditioning, elucidating its importance in enhancing the convergence properties of iterative methods. We illustrate this concept through two specific examples, demonstrating how preconditioning can optimize the performance of SCF calculations. Overall, this dissertation contributes to the understanding of Hartree-Fock methods and their numerical implementations, providing valuable insights for researchers in the field of quantum chemistry.",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 6.324555320336758,
        "rewrite-fast-z-score": -0.4745789978762495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Are Advanced Potentials Anomalous? .\nAbstract:\nWe present the results of an analysis of data on advanced potentials in hadronic collisions at high energies, obtained by the TOTEM experiment at LHC and by the UA7 collaboration at SppS collider. We show that these data are consistent with predictions based on Regge phenomenology for elastic scattering amplitudes. The observed behavior is also compatible with expectations from perturbative QCD calculations within the framework of the BFKL approach to high-energy evolution. \n \n Keywords: High energy physics, Elastic scattering amplitude, Perturbative QCD, BFKL equation, LHC, SppS, TOTEM, UA7 experiments \n \n 1 Introduction \n \n In recent years there has been considerable interest in studying the properties of elastic scattering amplitudes at very high energies (see e.g.,  1  ). This interest was triggered mainly by the discovery of new phenomena in this area made possible by the advent of accelerators operating at TeV scale such as the Large Hadron Collider (LHC)  2  . These discoveries include the observation of rapid growth of total cross sections  3  , dip-bump structure  4  , forward-backward asymmetry  5  , etc.. It should be noted however that many important questions remain open concerning the nature of the underlying dynamics responsible for all these effects  6  .\n \nIn particular, it remains unclear whether they can be explained within the conventional Regge theory  7, 8  or require more complicated approaches like those involving unitarization  9  and/or saturation  10  mechanisms. Another interesting question concerns the role played by higher-order corrections in perturbative Quantum Chromodynamics (QCD). Indeed, while the leading order BFKL  11  and DGLAP  12  equations provide reasonable description of experimental data  13  , their next-to-leading order extensions  14, 15  lead to significant deviations  16  which may indicate the need for resummation techniques  17  . \n \n 2 Data Analysis \n \n To shed some light on these issues we have performed detailed study of available data on elastic scattering processes collected recently by two dedicated experiments -the TOTEM  18  and UA7  19  collaborations. Both groups measured differential distributions dσ/d",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Are Advanced Potentials Anomalous?.Abstract : We present the results of an assessment of evidence on advanced potentials in hadronic collisions at high energies , obtained by the TOTEM study at LHC and by the UA7 collaboration at SppS collider . We see that these information are compatible with predictions based on Regge phenomenology for elastic scattering amplitudes .The observed behavior is also consistent with predictions from perturbative QCD calculations within the framework of the BFKL approach to large - energy evolution . Keywords : High energy physics , Elastic scattering amplitude , Perturbative QCD , BFKL equation , LHC , SppS , TOTEM , UA7 experiments 1 Introduction In recent years there has been substantial interest in investigating the properties of elastic scattering amplitudes at very high energies ( saw e . g . , 1 ) .This activity was sparked mainly by the discovery of new concepts in this area made possible by the advent of accelerators active at TeV scale such as the Large Hadron Collider ( LHC ) 2 . These finds feature the observation of rapid increase of complete cross sections 3 , dip - bump formation 4 , backwards - backward asymmetry 5 , etc . .It should be mentioned however that several important questions remain open concerning the nature of the fundamental interactions involved for all these influences 6 . In particular , it remains unsure whether they can be described within the standard Regge principle 7 , 8 or use more complicated approaches like those concerning unitarization 9 and / or saturation 10 mechanisms .Another important dispute concerns the importance played by higher - order corrections in perturbative Quantum Chromodynamics ( QCD ) . Indeed , while the led order BFKL 11 and DGLAP 12 equations offer reasonable explanation of theoretical data 13 , their next - to - leading order additions 14 , 15 lead to significant deviations 16 which would indicate the necessity for resummation methods 17 .2 Data Analysis To shed some light on these problems we have done thorough study of available data on elastic scattering systems collected lately by two dedicated studies - the TOTEM 18 and UA7 19 collaborations . Both groups recorded differential functions dσ / d",
        "rewrite_text": "**Title: Are Advanced Potentials Anomalous?**\n\n**Abstract:** This article presents a comprehensive evaluation of the evidence surrounding advanced potentials in high-energy hadronic collisions, drawing on findings from the TOTEM experiment at the Large Hadron Collider (LHC) and the UA7 collaboration at the Super Proton Synchrotron (SppS) collider. Our analysis indicates that the data obtained from these experiments align well with the predictions made by Regge phenomenology regarding elastic scattering amplitudes. Furthermore, the observed phenomena are consistent with theoretical predictions derived from perturbative Quantum Chromodynamics (QCD), particularly within the framework of the BFKL (Balitsky-Fadin-Kuraev-Lipatov) approach to high-energy evolution. \n\nRecent years have witnessed a surge of interest in the characteristics of elastic scattering amplitudes at extremely high energies, largely fueled by advancements in accelerator technology, such as the LHC. Notable discoveries include the rapid escalation of total cross sections, the emergence of dip-bump structures, and the observation of backward-backward asymmetries. Despite these advancements, several critical questions remain unresolved regarding the fundamental interactions that govern these phenomena. Specifically, it is still debated whether these interactions can be adequately described by the conventional Regge framework or if more complex models, such as those addressing unitarization and saturation effects, are necessary. \n\nAdditionally, the role of higher-order corrections in perturbative QCD is a topic of ongoing discussion. While the leading-order BFKL and DGLAP (Dokshitzer-Gribov-Lipatov-Altarelli-Parisi) equations provide a satisfactory explanation of the theoretical data, the inclusion of next-to-leading order corrections introduces significant discrepancies, suggesting a potential need for resummation techniques. To address these issues, we conducted a detailed analysis of the elastic scattering data collected by the TOTEM and UA7 collaborations, focusing on the differential cross-sections and their implications for our understanding of advanced potentials in high-energy physics.\n\n**Keywords:** High energy physics, Elastic scattering amplitude, Perturbative QCD, BFKL equation, LHC, SppS, TOTEM, UA7 experiments.",
        "ori-fast-z-score": -0.40961596025952024,
        "water-fast-z-score": 8.981462390204987,
        "rewrite-fast-z-score": 0.34299717028501764
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  TRUFAS, a wavelet based algorithm for the rapid detection of planetary transits .\nAbstract:\nWe present TRUFAS (Transit Recognition Using Fast Approximate Signatures), an efficient and robust transit search pipeline that uses fast approximate signatures to detect periodic signals in time series data. The method is particularly well suited for detecting short duration transits with high signal-to-noise ratio. We demonstrate its performance on simulated light curves generated by the Exoplanet Transit Database as well as real Kepler light curves. Our results show that TRUFAS can achieve higher efficiency than other algorithms while maintaining low false positive rates. \n \n Keywords: Transiting planet, Wavelets, Time-series analysis, False positives reduction, Planetary system characterization \n \n \n \n 1 Introduction \n \n Planets are detected indirectly through their gravitational effects upon their host stars. These effects include changes in stellar radius or luminosity caused by the passage of planets across the line-of-sight between the star and Earth. This phenomenon is known as a transit event. In order to characterize exoplanet systems it is necessary to identify these events efficiently and accurately. However, this task has been made more difficult due to the large number of false positives produced by systematic noise sources such as instrumental artifacts and astrophysical phenomena like eclipsing binaries and pulsating stars. \n \n To date there have been several methods developed specifically for identifying transit-like features within astronomical time series data. Some examples include: Box Least Squares (BLS)  1  , BLS+  2  , TrES  3  , TAP  4  , EXOTRANS  5  . While each of these techniques performs reasonably well under certain conditions they all suffer from one common drawback; they require significant computational resources when searching for multiple transit candidates simultaneously. For example, the most widely used technique, Box Least Squares, requires O(N3) operations where N is the length of the time series being analyzed  6  . As a result, many of these techniques cannot be applied directly to current and future surveys which will produce enormous amounts of data  7  8  9  . \n \n In recent years wavelet transforms have become increasingly popular for analyzing astronomical time series data  10",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : TRUFAS , a wavelet based algorithm for the quick detection of planetary transits . Abstract : We present TRUFAS ( Transit Recognition Using Fast Approximate Signatures ) , an efficient and strong transit search pipeline that using fast exact signatures to identify continuous patterns in time series information .The method is especially good suitable for detecting short length transits with high signal - to - noise ratio . We test its effectiveness on simulated light curves generated by the Exoplanet Transit Database as also as real Kepler light surfaces .Our results show that TRUFAS can attain better productivity than other methods while maintaining low false negative rates . Keywords : Transiting planet , Wavelets , Time - series investigation , False positives reduction , Planetary network detection 1 Introduction Planets are detected indirectly through their gravitational impacts upon their host stars .These effects include changes in stellar radius or luminosity caused by the travel of planets across the line - of - view between the star and Earth . This phenomenon is known as a transit event .In order to characterize exoplanet systems it is required to identify these changes easily and correctly . However , this job has been turned more challenging due to the huge amount of false positives created by widespread sound sources such as instrumental artifacts and astrophysical processes like eclipsing binaries and pulsating planets .To date there have been numerous mechanisms developed specifically for determining transit - like features within astronomical date cycle information . Some examples include : Box Least Squares ( BLS ) 1 , BLS + 2 , TrES 3 , TAP 4 , EXOTRANS 5 .While each of these procedures works relatively well under certain conditions they all suffer from one common drawback ; they demand significant computational resources when looking for multiple transit alternatives independently . For instance , the most commonly used technique , Box Least Squares , requires O ( N3 ) operations where N is the length of the time series being evaluated 6 .As a result , many of these procedures cannot be applied directly to recent and future surveys which will generate immense sums of evidence 7 8 9 . In recent generations wavelet transforms have developed increasingly popular for studying astronomical date sequence data 10",
        "rewrite_text": "**Title:** TRUFAS: A Wavelet-Based Algorithm for Rapid Detection of Planetary Transits\n\n**Abstract:** In this study, we introduce TRUFAS (Transit Recognition Using Fast Approximate Signatures), a robust and efficient pipeline designed for the swift identification of planetary transits in time series data. TRUFAS leverages fast exact signatures to detect continuous patterns, making it particularly adept at identifying short-duration transits with a high signal-to-noise ratio. We evaluate the performance of TRUFAS using simulated light curves from the Exoplanet Transit Database, as well as actual light curves from the Kepler mission. Our findings demonstrate that TRUFAS significantly outperforms existing methods in terms of productivity while maintaining a low rate of false negatives. \n\nThe detection of exoplanets primarily relies on observing their gravitational influence on host stars, which manifests as variations in stellar radius or luminosity when a planet transits across the line of sight between the star and Earth. Accurately identifying these transit events is crucial for characterizing exoplanet systems. However, the task is complicated by the prevalence of false positives arising from various sources, including instrumental artifacts and astrophysical phenomena such as eclipsing binaries and pulsating stars. \n\nNumerous algorithms have been developed to detect transit-like features in astronomical time series data, including Box Least Squares (BLS), BLS+, TrES, TAP, and EXOTRANS. While these methods perform adequately under specific conditions, they share a significant limitation: they require substantial computational resources when searching for multiple transit candidates independently. For instance, the widely used Box Least Squares method necessitates O(N³) operations, where N represents the length of the time series. Consequently, many existing techniques are not feasible for application to the vast datasets generated by contemporary and future astronomical surveys. In recent years, wavelet transforms have gained traction as a powerful tool for analyzing astronomical time series data, paving the way for innovative approaches like TRUFAS that can efficiently handle the increasing volume of observational data. \n\n**Keywords:** Transiting planet, Wavelets, Time-series analysis, False positive reduction, Planetary detection networks.",
        "ori-fast-z-score": -2.9651150929518515,
        "water-fast-z-score": 6.974502000925911,
        "rewrite-fast-z-score": -2.1548345880625797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Planetesimal Accretion onto Growing Proto-Gas-Giant Planets .\nAbstract:\nWe present results for the accretion of planetesimals by growing proto-gas-giant planets in circumstellar disks with different masses and compositions, using three-dimensional hydrodynamic simulations coupled to an N-body integrator. We find that the growth rate is strongly dependent on disk mass; more massive disks lead to faster planet formation timescales. The final planetary mass depends primarily on the initial disk surface density profile at large radii (>100 AU), which determines how much material can be transported inward before it dissipates. In addition, we show that the composition of the disk has only minor effects on the resulting planet properties. Our models are able to reproduce observed trends between host star metallicity and giant planet occurrence rates as well as the distribution of orbital periods and eccentricities of known exoplanets. This suggests that our model captures important physical processes involved in forming gas giants. Keywords: Planetary systems; Giant planets",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Planetesimal Accretion onto Growing Proto - Gas - Giant Planets . Abstract : We report findings for the accretion of planetesimals by expanding proto - gas - giant planets in circumstellar disks with various masses and compositions , using three - dimensional hydrodynamic simulations combined to an N - bodies integrator .We see that the development period is strongly dependent on disk mass ; more massive disks contribute to faster planet development timescales . The final planetary mass depends primarily on the first disk floor density profile at large radii ( > 100 AU ) , which determines how many material can be transported inward before it dissipates .In addition , we prove that the composition of the disk has only minor impacts on the resulting planet properties . Our models are able to capture seen variations between host star metallicity and massive planet abundance rates as also as the distribution of orbital periods and eccentricities of known exoplanets .This implies that our model captures significant physical processes responsible in forming gas giants . Keywords : Planetary systems ; Giant planets",
        "rewrite_text": "Title: Planetesimal Accretion onto Growing Proto-Gas-Giant Planets\n\nAbstract: In this study, we present our findings on the accretion processes of planetesimals by expanding proto-gas-giant planets within circumstellar disks of varying masses and compositions. Utilizing advanced three-dimensional hydrodynamic simulations in conjunction with an N-body integrator, we investigate the dynamics of planet formation. Our results indicate that the growth period of proto-planets is significantly influenced by the mass of the surrounding disk; specifically, more massive disks facilitate quicker development of planetary bodies. Furthermore, we identify that the ultimate mass of the forming planets is primarily determined by the initial density profile of the disk at large radii (greater than 100 AU), which governs the amount of material that can be effectively transported inward before it dissipates. Interestingly, our analysis reveals that the composition of the circumstellar disk has a relatively minor effect on the resultant characteristics of the planets formed. Our models successfully account for the observed correlations between the metallicity of host stars and the abundance of massive planets, as well as the distribution patterns of orbital periods and eccentricities among known exoplanets. These findings suggest that our model effectively captures the critical physical processes involved in the formation of gas giants, providing valuable insights into the mechanisms that shape planetary systems. \n\nKeywords: Planetary systems; Giant planets.",
        "ori-fast-z-score": 0.42640143271122083,
        "water-fast-z-score": 6.041987916036252,
        "rewrite-fast-z-score": 1.0734900802433864
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New Parameterization in Muon Decay and the Type of Emitted Neutrino. II .\nAbstract:\nWe have studied the decay of muons into two neutrinos, one electron-positron pair and one photon by using the Feynman diagram method. We found that there are three types of emitted neutrinos; type I is emitted when the energy of the emitted photon is less than half of the rest mass of the muon; type II is emitted when the energy is greater than or equal to half but less than twice the rest mass of themuon;andtype IIIisemittedwhentheenergyofthespin-1/2particlesissmallerthantwice therespectivestandardmass.Theprobabilitydistributionfunctionsforthese3typesareshown  for different values of the parameter x = Eγ / (2 mμ). The results obtained here agree with those given previously. \n \n Keywords: muon decay, neutrino emission, Feynman diagram technique, probability distribution function, standard model",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  New Parameterization in Muon Decay and the Type of Emitted Neutrino.II .Abstract : We have researched the decay of muons into two neutrinos , one electron - positron pair and one photon by using the Feynman diagram method . We showed that there are three categories of emitted neutrinos ; type I is emitted when the electricity of the emitted photon is greater than quarter of the remainder mass of the muon ; type II is emitted when the power is greater than or equal to quarter but less than times the remainder mass of themuon ; andtype IIIisemittedwhentheenergyofthespin - 1 / 2particlesissmallerthantwice therespectivestandardmass . Theprobabilitydistributionfunctionsforthese3typesareshown for different expressions of the parameter x = Eγ / ( 2 mμ ) .The results derived here agree with those given earlier . Keywords : muon radiation , neutrino emission , Feynman diagram method , probability distribution relation , standard model",
        "rewrite_text": "**Title:** New Parameterization in Muon Decay and the Type of Emitted Neutrino\n\n**Abstract:** In this study, we investigate the decay process of muons into two neutrinos, an electron-positron pair, and a photon, employing the Feynman diagram technique. Our analysis reveals the existence of three distinct categories of emitted neutrinos based on the energy characteristics of the emitted photon relative to the muon's remaining mass. Specifically, we define Type I neutrinos as those emitted when the energy of the emitted photon exceeds one-quarter of the muon's remaining mass. Type II neutrinos are emitted when the energy of the photon is at least one-quarter but less than one times the remaining mass of the muon. Finally, Type III neutrinos are produced when the energy of the spin-1/2 particles is less than twice the corresponding standard mass. We present the probability distribution functions for these three types of neutrinos, analyzing various expressions of the parameter \\( x = \\frac{E_\\gamma}{2 m_\\mu} \\). Our findings are consistent with previous results in the literature, reinforcing the validity of our parameterization approach. This work contributes to a deeper understanding of muon decay processes and the characteristics of neutrino emissions, which are critical for advancing theoretical frameworks in particle physics. \n\n**Keywords:** muon decay, neutrino types, Feynman diagrams, probability distributions, particle physics standard model.",
        "ori-fast-z-score": 0.14002800840280097,
        "water-fast-z-score": 4.714951667914447,
        "rewrite-fast-z-score": -1.3937462952891333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gravitational instability in binary protoplanetary disks .\nAbstract:\nWe study the gravitational instability (GI) in two differentially rotating, self-gravitating disks with and without magnetic fields using three-dimensional hydrodynamic simulations. We find that GI can occur at large radii for both cases but is suppressed by strong magnetic fields near the central star. The disk mass required to trigger GI decreases as the radius increases because the Toomre Q parameter becomes smaller due to weaker stellar gravity. For the case without magnetic fields, we also investigate how the initial density distribution affects the growth rate of GI. Our results show that the growth time scale depends on the radial profile of surface density. In addition, we examine whether or not GI leads to fragmentation. Fragmentation occurs only when the disk has an initially steep surface density gradient. Finally, we discuss possible implications of our findings for planet formation. Gravitational instability (GI), which causes spiral arms to form in gravitationally bound systems such as galaxies, may play important roles in various astrophysical phenomena including planet formation. However, it remains unclear if GI operates in protoplanetary disks around young stars since these disks are magnetized and their rotation profiles are complicated. Here, we perform 3D hydrodynamical simulations to explore this issue.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gravitational disturbance in binary protoplanetary disks . Abstract : We research the gravitational instability ( GI ) in two differentially rotating , self - gravitating disks with and without magnetic fields using three - dimensional hydrodynamic simulations .We see that GI can occur at large radii for both cases but is suppressed by weak magnetic fields near the main star . The disk mass needed to stimulate GI decreases as the radius increases because the Toomre Q function becomes lower due to smaller stellar gravitational .For the case without magnetic fields , we also investigate how the early density distribution influences the development frequency of GI . Our results show that the development time scale depends on the radial profile of surface density .In addition , we investigate whether or not GI contributes to fragmentation . Fragmentation happens only when the disk has an initially steep surface volume differential .Finally , we explain possible implications of our findings for planet development . Gravitational instability ( GI ) , which allows spiral arms to form in gravitationally locked components such as planets , might play important roles in different astrophysical processes including planet development .However , it remains unsure if GI exists in protoplanetary disks around young galaxies since these disks are magnetized and their rotation profiles are complicated . Here , we perform 3D hydrodynamical simulations to examine this question .",
        "rewrite_text": "In this study, we investigate the phenomenon of gravitational instability (GI) within binary protoplanetary disks that exhibit differential rotation and self-gravitation, both in the presence and absence of magnetic fields. Utilizing three-dimensional hydrodynamic simulations, we explore the conditions under which GI can manifest, particularly focusing on its behavior at varying radii from the central star. Our findings indicate that while GI can occur at larger radii in both scenarios, its occurrence is notably suppressed by weak magnetic fields in proximity to the main star. We observe that the mass threshold required to trigger GI decreases with increasing radius, attributed to a reduction in the Toomre Q parameter, which indicates a lower stellar gravitational influence at greater distances.\n\nIn the absence of magnetic fields, we further analyze how the initial density distribution within the disk affects the frequency of GI development. Our results reveal a significant correlation between the radial surface density profile and the time scale for GI to emerge. Additionally, we assess the role of GI in the fragmentation process of the disk. Fragmentation is found to occur exclusively when the disk possesses a steep initial surface volume differential, suggesting that the structural characteristics of the disk are crucial for this process.\n\nThe implications of our research extend to the understanding of planet formation, as GI is instrumental in the formation of spiral arms within gravitationally bound systems, such as protoplanetary disks. This study addresses the uncertainties surrounding the existence of GI in protoplanetary disks associated with young galaxies, where magnetic fields and complex rotation profiles pose significant challenges. Through our comprehensive 3D hydrodynamical simulations, we provide insights into the dynamics of these disks and their potential impact on planetary development.",
        "ori-fast-z-score": -1.4237369936287485,
        "water-fast-z-score": 5.550253123463223,
        "rewrite-fast-z-score": 1.028991510855053
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Kinematic Evolution of Strong MgII Absorbers .\nAbstract:\nWe present the kinematics and physical properties of strong Mg II absorbers at z = 1.5 − 3, using high-resolution (R ≈ 45000) spectroscopy obtained with Keck/HIRES. We find that these systems are composed primarily of cool gas clouds in pressure equilibrium with their surroundings; they have typical sizes of 100-200 pc, masses of 10^6−10^7 M_sun, and temperatures of ~10 4 K. The majority of our sample show no evidence for bulk motions exceeding 50 km/s relative to their surrounding medium. However, we do detect two outliers which exhibit large velocity shifts between multiple components within each system. These objects may be associated with galactic winds or tidal interactions. Our results suggest that strong Mg II absorbers evolve into galaxies through gravitational collapse on timescales less than one billion years after the Big Bang. This work is based upon observations made with the NASA/ESA Hubble Space Telescope, obtained from the Data Archive at the Space Telescope Science Institute, which is operated by AURA under NASA contract NAS 5-26555.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Kinematic Evolution of Strong MgII Absorbers . Abstract : We present the kinematics and physical properties of strong Mg II absorbers at z = 1 . 5 − 3 , using high - resolution ( R ≈ 45000 ) spectroscopy achieved with Keck / HIRES .We see that these systems are composed primarily of cold gas clouds in pressure equilibrium with their environment ; they have typical sizes of 100 - 200 pc , masses of 10 ^ 6−10 ^ 7 M _ sun , and altitudes of ~ 10 4 K . The majority of our sample find no evidence for bulk motions exceeding 50 km / s relative to their nearby medium . However , we do detect two outliers which exhibit big momentum changes between multiple components within each system .These bodies may be involved with galactic winds or tidal interactions . Our results propose that strong Mg II absorbers evolve into galaxies through gravity collapse on timescales fewer than one billion decades after the Big Bang .This project is based upon measurements made with the NASA / ESA Hubble Space Telescope , obtained from the Data Archive at the Space Telescope Science Institute , which is controlled by AURA under NASA contract NAS 5 - 26555 .",
        "rewrite_text": "We investigate the kinematic properties and physical characteristics of strong Mg II absorbers at redshifts ranging from 1.5 to 3, utilizing high-resolution spectroscopy with a resolution of approximately R ≈ 45000, conducted with the Keck Observatory's HIRES instrument. Our analysis reveals that these absorbers predominantly consist of cold gas clouds that are in pressure equilibrium with their surrounding environment. The typical dimensions of these clouds are found to be between 100 and 200 parsecs, with masses ranging from 10^6 to 10^7 solar masses and temperatures around 10^4 Kelvin. Notably, the majority of the absorbers in our sample do not exhibit significant bulk motions, with velocities not exceeding 50 km/s relative to their adjacent medium. However, we identify two notable exceptions that display substantial momentum variations among different components within the same system. These outliers may be influenced by phenomena such as galactic winds or tidal interactions, suggesting a more complex dynamical environment. Our findings imply that strong Mg II absorbers play a crucial role in the evolutionary processes of galaxies, potentially undergoing gravitational collapse within less than one billion years following the Big Bang. This research is supported by data acquired from the NASA/ESA Hubble Space Telescope, which is archived at the Space Telescope Science Institute and managed by AURA under NASA contract NAS 5-26555.",
        "ori-fast-z-score": 1.0392304845413263,
        "water-fast-z-score": 5.427092530382482,
        "rewrite-fast-z-score": 1.078327732034384
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Radial distribution of the inner magnetosphere plasma pressure using low-altitude satellite data during geomagnetic storm: the March 1-8, 1982 Event .\nAbstract:\nThe radial distribution of the inner magnetospheric (IM) plasma pressure is studied by analyzing the magnetic field and particle measurements made onboard two satellites at low altitude in the equatorial plane for an intense geomagnetic storm that occurred between March 1-5, 1982 . The results show that there are significant differences in the IM plasma pressure profiles obtained with different satellites. In particular, the pressure profile derived from GEOS-1 observations shows a sharp peak near L = 3 , while those inferred from ATS-6 and GEOS-2 data exhibit much broader peaks around L = 4 . These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space. It is also found that the pressure profiles inferred from the three satellites agree well when they are shifted outward along the L-shell coordinate system. This suggests that the observed difference in the pressure profiles can be attributed mainly to the spatial variation of the pressure rather than temporal changes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Radial distribution of the inner magnetosphere plasma pressure using minimum - height satellite information during geomagnetic storm : the March 1 - 8 , 1982 Event . Abstract : The radial distribution of the inner magnetospheric ( IM ) plasma pressure is studied by analyzing the magnetic field and electron calculations made onboard two spacecraft at low height in the equatorial plane for an extreme geomagnetic cyclone that occurred between March 1 - 5 , 1982 .The results show that there are significant variations in the IM plasma pressure profiles obtained with various satellites . In particular , the pressure profile derived from GEOS - 1 studies shows a sharp peak near L = 3 , while those inferred from ATS - 6 and GEOS - 2 data display considerably wider peaks around L = 4 .These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space . It is also discovered that the pressure profiles inferred from the three satellites cooperate well when they are shifted outward along the L - shell coordinate system .This implies that the seen shift in the pressure profiles can be due mainly to the spatial varying of the pressure rather than temporal changes .",
        "rewrite_text": "**Title:** Radial Distribution of Inner Magnetosphere Plasma Pressure During the March 1-8, 1982 Geomagnetic Storm: Insights from Minimum-Height Satellite Data\n\n**Abstract:** This study investigates the radial distribution of plasma pressure within the inner magnetosphere (IM) by analyzing magnetic field and electron data collected from two low-altitude satellites during a significant geomagnetic storm that occurred from March 1 to March 5, 1982. The analysis reveals notable discrepancies in the IM plasma pressure profiles obtained from different spacecraft, suggesting a complex spatial structure of the plasma environment. Specifically, the pressure profile derived from the GEOS-1 satellite exhibits a pronounced peak at L = 3, whereas the profiles from ATS-6 and GEOS-2 indicate broader peaks centered around L = 4. These variations are likely attributed to the distinct orbital paths of the satellites, which sample different regions of the magnetosphere. Furthermore, the study finds that when the pressure profiles from the three satellites are adjusted outward along the L-shell coordinate system, they align more closely, indicating a cooperative behavior among the datasets. This alignment suggests that the observed differences in pressure profiles are primarily due to spatial variations in plasma pressure rather than temporal fluctuations. The findings enhance our understanding of the inner magnetosphere's response to geomagnetic storms and underscore the importance of satellite positioning in interpreting plasma pressure distributions. This research contributes valuable insights into the dynamics of the magnetosphere during extreme space weather events, with implications for future studies of plasma behavior in similar conditions.",
        "ori-fast-z-score": 0.8251369970070347,
        "water-fast-z-score": 5.848076606885379,
        "rewrite-fast-z-score": 0.1873171623163388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exponential growth rates in a typed branching diffusion .\nAbstract:\nWe study the exponential growth rate of a typed branching diffusion, which is an extension of the classical Galton-Watson process to allow for multiple types and non-Markovian reproduction laws. We show that this quantity can be expressed as the solution of a fixed point equation involving the Laplace transform of the offspring distribution. This allows us to prove existence and uniqueness results under mild conditions on the offspring distributions. In particular we obtain explicit bounds on the exponential growth rate when all offspring distributions are Poisson or geometric. Finally, we provide some examples illustrating our main result. Keywords: Branching processes; Typed branching diffusions; Exponential growth rate; Fixed-point equations. 1 Introduction Let (Zt) t≥0 denote a continuous-time branching diffusion with Z0 = 0. The population size at time t ≥ 0 is given by Nt := sup{n ∈ N : Zn ≤ t} where (Zn) n∈N denotes the family tree associated with the branching diffusion up to generation n. For each i ∈ {1, . . . , m} let Pi(·), qi(·) and Fi(·) respectively denote the probability generating function, mean number of children and offspring distribution of type-i individuals. Then it follows from  21, Theorem 1  that there exists a unique positive real number λ such that E exp{−λNt}|Ft  < ∞ for every t > 0, where Ft denotes the filtration generated by the branching diffusion up to time t. Moreover, (1 − Pt) −1 , t > 0 converges exponentially fast towards λ, see e.g.,  6, Proposition 3.1  . Here Pt denotes the extinction probability starting from one individual of type i at time zero. It has been shown recently in  4  that if the offspring distributions Fi have finite variance then λ coincides with the Malthusian parameter of the branching diffusion. That is, λ equals the exponential growth rate of the total population size. If additionally the offspring distributions Fi belong to the domain of attraction of a stable law with index αi ∈ (1, 2  then λ also coincides with",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exponential growth rates in a typing branching diffusion . Abstract : We research the exponential growth speed of a typing branching diffusion , which is an extension of the classical Galton - Watson process to allow for multiple types and non - Markovian reproduction laws .We see that this quantity can be stated as the solve of a fixed point equation employing the Laplace transform of the offspring distribution . This enables us to obtain existence and uniqueness results under mild conditions on the offspring distributions .In particular we obtain explicit bounds on the exponential growth speed when all offspring distributions are Poisson or geometric . Finally , we provide some examples illustrating our major result .Keywords : Branching systems ; Typed branching diffusions ; Exponential growth speed ; Fixed - point equations . 1 Introduction Let ( Zt ) t≥0 denote a continuous - time branching diffusion with Z0 = 0 .The population size at time t ≥ 0 is given by Nt : = sup { n ∈ N : Zn ≤ t } where ( Zn ) n∈N denotes the family tree identified with the branching diffusion up to generation n . For each i ∈ { 1 , . .. , m } let Pi ( · ) , qi ( · ) and Fi ( · ) respectively represent the probability producing function , mean number of offspring and offspring distribution of type - i people .Then it appears from 21 , Theorem 1 that there exists a unique positive real number ν such that E exp { −λNt } | Ft < ∞ for every t > 0 , where Ft denotes the filtration produced by the branching diffusion up to time t . Moreover , ( 1 − Pt ) −1 , t > 0 converges exponentially rapidly towards λ , see e . g . , 6 , Proposition 3 . 1 . Here Pt denotes the extinction probability starting from one individual of type i at time zero .It has been shown recently in 4 that if the offspring distributions Fi have finite variance then λ coincides with the Malthusian parameter of the branching diffusion . That is , λ equals the exponential growth speed of the total community size .If additionally the offspring distributions Fi correspond to the domain of attraction of a steady law with index αi ∈ ( 1 , 2 then λ additionally coincides with",
        "rewrite_text": "**Title:** Exponential Growth Rates in a Typed Branching Diffusion\n\n**Abstract:** This study investigates the exponential growth rates associated with a typed branching diffusion, which extends the classical Galton-Watson process by incorporating multiple types and non-Markovian reproduction laws. We demonstrate that the exponential growth rate can be expressed as the solution to a fixed-point equation that utilizes the Laplace transform of the offspring distribution. This approach allows us to establish both existence and uniqueness results under relatively mild conditions imposed on the offspring distributions. Notably, we derive explicit bounds on the exponential growth rate when the offspring distributions follow either a Poisson or geometric distribution. Additionally, we present several examples that illustrate our primary findings. \n\nIn our analysis, we denote the continuous-time branching diffusion by (Z_t) for t ≥ 0, with the initial condition Z_0 = 0. The population size at any time t ≥ 0 is defined as N_t := sup { n ∈ N : Z_n ≤ t }, where (Z_n) represents the family tree associated with the branching diffusion up to generation n. For each type i in the set {1, ..., m}, we define P_i(·), q_i(·), and F_i(·) as the probability generating function, the mean number of offspring, and the offspring distribution for individuals of type i, respectively. According to Theorem 1 from reference 21, there exists a unique positive real number ν such that E[exp{−λN_t} | F_t] < ∞ for all t > 0, where F_t denotes the filtration generated by the branching diffusion up to time t. Furthermore, it is established that (1 − P_t)⁻¹ converges exponentially fast to λ for t > 0, as shown in Proposition 3.1 of reference 6. The extinction probability P_t is defined for a single individual of type i at time zero. Recent findings in reference 4 indicate that if the offspring distributions F_i possess finite variance, then λ corresponds to the Malthusian parameter of the branching diffusion, which represents the exponential growth rate of the overall population. Moreover, if the offspring distributions F_i fall within the domain of attraction of a stable law with index α_i ∈ (1, 2), then λ also coincides with this growth rate. \n\n**Keywords:** Branching systems; Typed branching diffusions; Exponential growth rate; Fixed-point equations.",
        "ori-fast-z-score": 0.9797958971132713,
        "water-fast-z-score": 5.839971160707452,
        "rewrite-fast-z-score": 1.5617376188860608
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Availability assessment of SunOS/Solaris Unix Systems based on Syslogd and wtmpx logfiles : a case study .\nAbstract:\nThe availability of computer systems is an important aspect in the design, development and operation of any system. The aim of this research was to develop a method for assessing the availability of a large number of SunOS/Solaries systems using syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender). This research has been carried out by collecting data from a set of servers over a period of one year. A total of 1,000,000 records were collected during that time frame. These records have then been processed into a database which contains information about each record such as timestamps, host name, service type etc.. In order to assess the availability of these systems we developed two algorithms; one algorithm calculates the percentage uptime per day while another algorithm calculates the percentage downtime per hour. We also implemented a web interface so that users can view the results obtained from our analysis.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Availability analysis of SunOS / Solaris Unix Systems using on Syslogd and wtmpx logfiles : a case study . Abstract : The availability of software machines is an important element in the development , construction and operation of any program .The goal of this research was to develop a technique for evaluating the availability of a large number of SunOS / Solaries devices using syslogd ( System Logging Daemon ) and wtmpx ( Windows Time Machine eXtender ) . This research has been carried out by collecting data from a pair of servers over a period of one year .A total of 1 , 000 , 000 data were collected during that time frame . These documents have then been processed into a computer which contains information about each record such as timestamps , host title , service kind etc . .In order to analyze the availability of these systems we developed two strategies ; one algorithm calculates the percentage uptime per morning while another algorithm calculates the proportion downtime per hour . We additionally developed a browser interface so that users can view the results collected from our analysis .",
        "rewrite_text": "**Title:** Availability Analysis of SunOS/Solaris Unix Systems Using Syslogd and Wtmpx Logfiles: A Case Study\n\n**Abstract:** The reliability of software systems is a critical factor in the design, implementation, and maintenance of any application. This study aims to establish a methodology for assessing the availability of numerous SunOS/Solaris devices by leveraging syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender) log files. To achieve this, we gathered extensive data from two servers over the course of one year, resulting in the collection of approximately 1,000,000 log entries. Each record was meticulously processed to extract key information, including timestamps, host identifiers, and service types.\n\nTo evaluate system availability, we devised two distinct analytical approaches: the first algorithm computes the percentage of uptime for each morning, while the second calculates the proportion of downtime on an hourly basis. These methodologies enable a comprehensive understanding of system performance and reliability over time. Furthermore, we developed a user-friendly web interface that allows stakeholders to access and visualize the results of our analysis, facilitating better decision-making regarding system management and maintenance.\n\nThe findings from this research not only provide insights into the operational status of SunOS/Solaris systems but also contribute to the broader field of system availability analysis. By employing log file data effectively, this study demonstrates the potential for enhancing the reliability of software systems through systematic monitoring and evaluation. The techniques and tools developed in this case study can serve as a valuable resource for system administrators and researchers aiming to improve the uptime and overall performance of Unix-based environments.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 1.0690449676496976
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optical investigations of quantum-dot spin dynamics .\nAbstract:\nWe report on the optical investigation of single self-assembled InAs/GaAs quantum dots (QDs) in an external magnetic field applied along their growth direction. The QD emission line splits into two components with opposite circular polarization when the magnetic field is increased to about 1 T, which corresponds to the Zeeman splitting energy of 0.5 meV at 4 K. We observe that this splitting increases linearly as temperature decreases down to 20 mK and then saturates below 10 mK. This behavior can be explained by taking into account both electron-hole exchange interaction and phonon-assisted relaxation processes between different excitonic states within QDs. Our results show that the spin-flip time for electrons confined inside QDs is longer than 100 ns even under high magnetic fields up to 5 T. Quantum dot (QD), also known as semiconductor nanocrystal or artificial atom, has attracted much attention due to its unique physical properties such as size-tunable band gap  1  , strong confinement effect  2  , and large oscillator strength  3  . These features make it possible to use QDs as building blocks for various optoelectronic devices including light-emitting diodes  4  , lasers  5  , solar cells  6  , photodetectors  7  , and so forth  8  .\nIn recent years, there have been many efforts devoted to investigating the spin dynamics of carriers confined in QDs  9  -  11  . It was found that the carrier spins are very stable against decoherence caused by environmental noise  12  -  14  . However, the spin flip times were reported to vary widely depending on experimental conditions  15  -  17  . For example, the spin lifetimes of holes  18  and electrons  19  confined in QDs were measured to be several nanoseconds using pulsed excitation techniques. On the other hand, the spin lifetime of electrons  20  and holes  21  confined in QDs could reach microsecond level if continuous wave laser was used instead.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optical studies of quantum - dot spinning dynamics . Abstract : We report on the optical study of single self - assembled InAs / GaAs quantum dots ( QDs ) in an external magnetic force applied along their growth direction .The QD radiation line splits into two parts with opposite circular polarization when the magnetic force is expanded to about 1 T , which corresponds to the Zeeman splitting energy of 0 . 5 meV at 4 K . We see that this splitting changes linearly as temperature grows down to 20 mK and then saturates below 10 mK . This phenomenon can be described by take into consideration both electron - hole exchange interaction and phonon - aided relaxation processes between various excitonic states within QDs .Our results show that the spin - flip time for electrons concentrated inside QDs is longer than 100 ns even under high magnetic waves up to 5 T . Quantum dot ( QD ) , sometimes called as semiconductor nanocrystal or artificial electron , has garnered great attention due to its unique physical properties such as size - tunable band gap 1 , large confinement phenomenon 2 , and large oscillator strength 3 . These features make it able to use QDs as building blocks for various optoelectronic equipment including light - emitting diodes 4 , lasers 5 , solar cells 6 , photodetectors 7 , and so forth 8 .In recent years , there have been many efforts devoted to investigating the spin behavior of carriers restricted in QDs 9 - 11 . It was shown that the carrier spins are very stable against decoherence caused by environmental interference 12 - 14 .However , the spin flip times were reported to vary widely depending on experimental environments 15 - 17 . For instance , the spin lifetimes of holes 18 and electrons 19 restricted in QDs were calculated to be several nanoseconds using pulsed excitation techniques .On the other hand , the spin lifetime of atoms 20 and holes 21 confined in QDs might reach microsecond level if continuous wave beam was used instead .",
        "rewrite_text": "**Title:** Optical Investigations of Quantum Dot Spin Dynamics\n\n**Abstract:** This study presents an in-depth optical investigation of single self-assembled InAs/GaAs quantum dots (QDs) subjected to an external magnetic field aligned with their growth axis. Notably, we observe that the emission line of the quantum dots undergoes a splitting into two distinct components with opposite circular polarization when the magnetic field strength reaches approximately 1 T, corresponding to a Zeeman splitting energy of 0.5 meV at a temperature of 4 K. As the temperature decreases to 20 mK, we find that this splitting exhibits a linear dependence, eventually saturating below 10 mK. This behavior can be effectively modeled by considering both the electron-hole exchange interactions and phonon-assisted relaxation processes that occur among the various excitonic states within the quantum dots. Our findings indicate that the spin-flip time for electrons confined within the quantum dots exceeds 100 ns, even when exposed to high magnetic fields of up to 5 T. Quantum dots, often referred to as semiconductor nanocrystals or artificial atoms, have attracted significant interest due to their remarkable physical characteristics, including a size-tunable band gap, pronounced confinement effects, and substantial oscillator strength. These attributes render QDs suitable for a wide range of optoelectronic applications, such as light-emitting diodes, lasers, solar cells, and photodetectors. Recent research has increasingly focused on the spin dynamics of carriers confined within quantum dots, revealing that these spins exhibit remarkable stability against decoherence induced by environmental factors. However, reported spin flip times vary significantly based on the experimental conditions. For example, spin lifetimes for holes and electrons in QDs have been measured in the nanosecond range using pulsed excitation methods, while continuous wave techniques have shown that the spin lifetimes for atoms and holes can extend to the microsecond scale. This study contributes to the understanding of spin dynamics in quantum dots, highlighting their potential for future quantum computing and spintronic applications.",
        "ori-fast-z-score": 0.32659863237109044,
        "water-fast-z-score": 6.488856845230502,
        "rewrite-fast-z-score": -1.4881948549771191
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electrodynamics of Josephson vortex lattice in high-temperature superconductors .\nAbstract:\nWe study the electrodynamic properties of Josephson vortices (JVs) in hightemperature superconductors by solving numerically the time-dependent Ginzburg-Landau equations with an external magnetic field and current density. We find that JVs can be driven into motion by applying either a dc or ac electric field, which is consistent with previous experiments on YBa2Cu3O7-δ single crystals. The JV velocity increases linearly as the applied voltage increases for small voltages but saturates at large voltages due to the pinning effect. In addition, we show that the JV velocity decreases when increasing the temperature because of thermal fluctuations. Finally, we demonstrate that the JV dynamics are strongly affected by the anisotropy of the sample. \n \n Introduction \n \n High-temperature superconductivity has been discovered more than 30 years ago  1–3  . Since then, many theoretical models have been proposed to explain this phenomenon  4–9  , among them the so-called two-fluid model  10  . According to this theory, there exist two different types of charge carriers in these materials, namely electrons and holes  11  . These particles interact via attractive Coulomb forces  12  and form Cooper pairs  13  . When the material undergoes a phase transition below its critical temperature Tc, the Cooper pairs condense into a superfluid state  14  . This leads to macroscopic quantum phenomena such as zero resistance  15  and Meissner effect  16  . \n \n However, it was soon realized that the conventional Bardeen-Cooper-Schrieffer (BCS) theory  17  cannot fully account for all experimental observations  18  . For example, the BCS theory predicts that the energy gap between the ground-state and excited states should decrease rapidly near T = 0 K  19  . On the other hand, recent measurements  20  indicate that the energy gap remains almost constant down to very low temperatures  21  . To overcome this problem, several extensions of the original BCS theory were developed  22–24  . Among those theories, one of the most successful ones is the Eliashberg formalism  25  , where the electron-phonon interaction plays an important role  26  . It turns out that",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electrodynamics of Josephson vortex lattice in high - temperature superconductors . Abstract : We research the electrodynamic characteristics of Josephson vortices ( JVs ) in hightemperature superconductors by solving numerically the period - dependent Ginzburg - Landau coefficients with an external magnetic force and current density .We see that JVs can be pushed into motion by using either a dc or ac electric field , which is consistent with previous research on YBa2Cu3O7 - δ single crystals . The JV speed increases linearly as the introduced voltage increases for large voltages but saturates at large voltages due to the locking effect .In addition , we prove that the JV speed reduces when varying the temperature because of thermal fluctuations . Finally , we prove that the JV mechanics are strongly altered by the anisotropy of the sample .Introduction High - temperature superconductivity has been detected more than 30 centuries earlier 1 – 3 . Since then , various theoretical theories have been proposed to explain this phenomenon 4 – 9 , among them the so - called two - fluid model 10 .According to this theory , there exist two different kinds of charge carriers in these materials , principally electrons and holes 11 . These particles react via attractive Coulomb forces 12 and form Cooper pairs 13 .When the material undergoes a phase shift below its critical temperature Tc , the Cooper pairs condense into a superfluid state 14 . This leads to macroscopic quantum effects such as zero resistance 15 and Meissner phenomenon 16 .However , it was immediately realized that the usual Bardeen - Cooper - Schrieffer ( BCS ) theory 17 fails truly account for all experimental phenomena 18 . For instance , the BCS theory predicts that the electricity gap between the ground - state and excited states should decrease rapidly near T = 0 K 19 .On the other hand , recent observations 20 imply that the power gap continues almost steady down to very low temperatures 21 . To solve this situation , various extensions of the original BCS theory were developed 22 – 24 .Among those theories , one of the most popular ones is the Eliashberg formalism 25 , where the electron - phonon interaction plays an important role 26 . It turns out that",
        "rewrite_text": "Title: Electrodynamics of Josephson Vortex Lattice in High-Temperature Superconductors\n\nAbstract: This study investigates the electrodynamic properties of Josephson vortices (JVs) in high-temperature superconductors through numerical solutions of the period-dependent Ginzburg-Landau coefficients, incorporating external magnetic fields and current densities. Our findings indicate that JVs can be set into motion by applying either direct current (dc) or alternating current (ac) electric fields, corroborating earlier studies conducted on YBa2Cu3O7-δ single crystals. We observe a linear increase in JV velocity with rising voltage for higher voltage levels; however, this velocity reaches a saturation point at elevated voltages due to the locking effect. Furthermore, we demonstrate that the velocity of JVs is adversely affected by temperature variations, attributed to thermal fluctuations. Additionally, our results reveal that the mechanical behavior of JVs is significantly influenced by the anisotropic nature of the superconducting sample.\n\nIntroduction: The phenomenon of high-temperature superconductivity was first identified over three decades ago. Since its discovery, numerous theoretical frameworks have been proposed to elucidate the underlying mechanisms, including the widely recognized two-fluid model. This model posits the existence of two distinct types of charge carriers—electrons and holes—that interact through attractive Coulomb forces to form Cooper pairs. When the material transitions into a superfluid state below its critical temperature (Tc), these Cooper pairs condense, resulting in macroscopic quantum effects such as zero electrical resistance and the Meissner effect. However, it became apparent that the conventional Bardeen-Cooper-Schrieffer (BCS) theory does not adequately explain all observed phenomena. For example, BCS theory suggests that the energy gap between the ground and excited states diminishes rapidly as the temperature approaches absolute zero, whereas recent experimental data indicate that this gap remains relatively stable at low temperatures. To address these discrepancies, various extensions of the BCS framework have been proposed, among which the Eliashberg formalism stands out, emphasizing the critical role of electron-phonon interactions.",
        "ori-fast-z-score": -1.158648244043315,
        "water-fast-z-score": 6.858571279792899,
        "rewrite-fast-z-score": 0.5659164584181103
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The time evolution of cosmological redshift as a test of dark energy .\nAbstract:\nWe propose to use the time evolution of cosmological redshifts in order to probe the nature of dark energy, which is one of the most important problems in modern physics and astronomy. We show that this method can be used for testing various models of dark energy by using only two parameters (the present-day values of Hubble constant H0 and deceleration parameter q0). The proposed method does not require any additional information about the universe beyond what we already know today. This makes it possible to perform an independent check on the results obtained with other methods such as supernovae Ia observations or cosmic microwave background anisotropy measurements. In particular, our analysis shows that the current data are consistent with the standard ΛCDM model at 1σ level but do not rule out some alternative models like quintessence or phantom fields. Finally, we discuss how future surveys could improve the constraints on these models. Cosmological redshifts play an important role in modern astrophysics and cosmology because they provide us with valuable information about the expansion history of the Universe. However, their interpretation requires knowledge of the underlying theory describing the dynamics of space-time. For example, if we assume general relativity then cosmological redshifts can be interpreted as due to the Doppler effect caused by the recession velocities of distant galaxies  1  . On the other hand, if we consider modified gravity theories then cosmological redshifting may have different physical origins  2  .\nIn recent years there has been growing interest in studying the possibility of probing the nature of dark energy through its effects on cosmological redshifts  3  -  8  . Dark energy is currently believed to dominate the content of the Universe  9  , however its exact origin remains unknown  10  . It is usually described within the framework of Einstein s field equations by introducing a new component into the stress-energy tensor  11  . Its presence leads to accelerated expansion of the Universe  12  , which manifests itself in the form of observed...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The period evolution of cosmological redshift as a test of dark energy . Abstract : We suggest to use the time progression of cosmological redshifts in order to probe the nature of dark energy , which is one of the most important problems in modern physics and astronomy .We see that this method can be used for studying several models of deep energy by using only two parameters ( the present - day values of Hubble constant H0 and deceleration parameter q0 ) . The proposed approach does not require any additional information about the universe beyond what we already understand today .This gives it able to conduct an independent check on the results derived with other methods such as supernovae Ia observed or cosmic microwave background anisotropy observations . In particular , our analysis shows that the present data are compatible with the standard ΛCDM theory at 1σ level but do not leave out some additional models like quintessence or phantom fields .Finally , we talk how potential study could enhance the limitations on these models . Cosmological redshifts play an important role in modern astrophysics and cosmology because they give us with important information about the expansion history of the Universe .However , their understanding needs expertise of the fundamental theory explaining the dynamics of space - time . For instance , if we suppose general relativity then cosmological redshifts can be interpreted as owing to the Doppler impact caused by the recession velocities of distant galaxies 1 .On the other hand , if we treat modified gravity theories then cosmological redshifting might have different physical origins 2 . In recent years there has been growing interest in investigating the prospect of probing the nature of dark energy through its consequences on cosmological redshifts 3 - 8 .Dark energy is currently suspected to dominate the content of the Universe 9 , however its exact origin remains obscure 10 . It is usually characterized within the framework of Einstein s field equations by introducing a new part into the strain - energy tensor 11 .Its presence causes to accelerated expansion of the Universe 12 , which manifests itself in the form of observed . . .",
        "rewrite_text": "**Title:** The Evolution of Cosmological Redshift as a Probe for Dark Energy\n\n**Abstract:** In this study, we propose a novel approach to investigate the nature of dark energy by analyzing the temporal evolution of cosmological redshifts. Dark energy remains one of the most significant challenges in contemporary physics and astronomy, and understanding its properties is crucial for comprehending the universe's expansion. Our method focuses on utilizing only two key parameters: the current values of the Hubble constant (H0) and the deceleration parameter (q0), allowing for the examination of various dark energy models without necessitating additional information beyond our existing knowledge of the universe. This approach enables an independent verification of results obtained from other methodologies, such as observations of Type Ia supernovae and cosmic microwave background anisotropies.\n\nOur analysis indicates that current observational data align with the standard ΛCDM model at a 1σ confidence level, while also accommodating alternative models, including quintessence and phantom fields. We discuss how future studies could further refine the constraints on these models. Cosmological redshifts are pivotal in modern astrophysics and cosmology, providing essential insights into the universe's expansion history. However, interpreting these redshifts requires a solid understanding of the underlying theories governing space-time dynamics. For instance, within the framework of general relativity, cosmological redshifts can be understood as a Doppler effect resulting from the recession velocities of distant galaxies. Conversely, in modified gravity theories, the origins of cosmological redshift may differ significantly.\n\nRecent years have seen a surge of interest in exploring the implications of dark energy on cosmological redshifts, as dark energy is believed to constitute a dominant component of the universe's total energy density. Despite its prevalence, the precise nature and origin of dark energy remain elusive. Typically, it is characterized within Einstein's field equations by the introduction of a new term in the stress-energy tensor, leading to the accelerated expansion of the universe, which is observable through various astrophysical phenomena. This study aims to contribute to the ongoing discourse surrounding dark energy by leveraging the evolution of cosmological redshifts as a critical tool for understanding this enigmatic component of the cosmos.",
        "ori-fast-z-score": -0.159111456835146,
        "water-fast-z-score": 7.487048859271456,
        "rewrite-fast-z-score": 2.1278980968455716
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Revisiting He-like X-ray Emission Line Plasma Diagnostics .\nAbstract:\nWe present an overview on the diagnostic potential of emission lines in plasma produced by laser-produced plasmas (LPPs) and Z-pinches for astrophysical applications, with emphasis on He-like ions. We discuss how to obtain reliable results using these diagnostics under laboratory conditions that are relevant to those found in space environments. In particular we focus our attention on the determination of electron temperature T e , density n e , ionization state Z eff , and elemental composition C/H/N/O/Fe/Mg/K/Si/S/Ca. The main advantages and disadvantages of each method are discussed as well as their applicability limits. Finally, we provide examples of recent experimental results obtained at LULI2000 facility in France and NIF facility in USA. Keywords: Astrophysics; Laser-plasma interaction; X-rays; Spectroscopy. 1 Introduction X-ray spectroscopy is one of the most powerful tools available today for studying hot dense plasmas generated in laboratories or in space  1  . It has been used extensively over many decades to diagnose various types of plasmas ranging from solar flares  2  to supernova remnants  3  .\nIn this work, we will review some of the methods currently being developed to measure key physical parameters such as electron temperature T e , electron number density n e , ion charge state distribution Z eff , and chemical composition C/H/N/O/Fe/...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Revisiting He - like X - ray Emission Line Plasma Diagnostics . Abstract : We bring an overview on the clinical ability of emission lines in plasma generated by laser - produced plasmas ( LPPs ) and Z - pinches for astrophysical applications , with emphasis on He - like ions .We discuss how to obtain reliable results use these diagnostics under laboratory situations that are applicable to those observed in space environments . In particular we focus our focus on the determination of electron concentration T r , concentration k e , ionization state Z eff , and elemental purity C / H / N / O / Fe / Mg / K / Si / S / Ca .The main benefits and disadvantages of each approach are discussed as well as their applicability restrictions . Finally , we provide descriptions of recent experimental results derived at LULI2000 facility in France and NIF laboratory in USA .Keywords : Astrophysics ; Laser - plasma interaction ; X - radiation ; Spectroscopy . 1 Introduction X - ray spectroscopy is one of the most important skills available today for studying warm dense plasmas generated in laboratories or in space 1 .It has been used widely over numerous years to diagnose various types of plasmas ranging from solar flares 2 to supernova remnants 3 . In this research , we will review some of the methods currently being advanced to measure key physical factors such as electron height T r , atom number density n e , ion charge state distribution Z eff , and organic composition C / H / N / O / Fe / . . .",
        "rewrite_text": "**Title: Revisiting He-like X-ray Emission Line Plasma Diagnostics**\n\n**Abstract:** This article provides a comprehensive overview of the diagnostic capabilities of emission lines in plasmas generated by laser-produced plasmas (LPPs) and Z-pinches, particularly focusing on He-like ions and their relevance to astrophysical contexts. We explore methodologies for obtaining accurate diagnostic results in laboratory settings that can be extrapolated to conditions found in space environments. Our analysis centers on key parameters such as electron temperature (T_r), electron density (n_e), ionization state (Z_eff), and elemental composition (C/H/N/O/Fe/Mg/K/Si/S/Ca). We critically evaluate the advantages and limitations of various diagnostic techniques, highlighting their applicability constraints in different experimental scenarios. Furthermore, we present recent experimental findings obtained from the LULI2000 facility in France and the National Ignition Facility (NIF) in the United States, which illustrate the practical implementation of these diagnostics. X-ray spectroscopy emerges as a vital tool for investigating warm dense plasmas, with applications ranging from solar flares to supernova remnants. This review aims to synthesize current advancements in the field, providing insights into the measurement of essential physical parameters such as electron temperature, atomic number density, ion charge state distribution, and elemental composition. By revisiting these diagnostic techniques, we aim to enhance our understanding of plasma behavior in both laboratory and astrophysical settings, paving the way for future research in plasma physics and astrophysics.\n\n**Keywords:** Astrophysics; Laser-plasma interaction; X-radiation; Spectroscopy.",
        "ori-fast-z-score": 0.5827715174143585,
        "water-fast-z-score": 6.863829072224211,
        "rewrite-fast-z-score": 2.0788046015507495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Compton Scattering of Fe K alpha Lines in Magnetic Cataclysmic Variables .\nAbstract:\nWe report on the first detection of Compton scattering of iron K-alpha lines (6.4 keV) by hot plasma in magnetic cataclysmic variables (mCVs). The observations were made with Suzaku satellite, which has an unprecedented combination of high sensitivity and energy resolution at 6.4 keV for mCVs. We detected significant excess emission above continuum around 6.4 keV in three out of four targets observed. This is interpreted as Compton-scattered line photons produced in the accretion disk coronae surrounding white dwarfs. In one target, we found that the scattered component was significantly stronger than the direct component. Our results suggest that the geometry of the X-ray emitting region may be more complex than previously thought.  Keywords: Accreting compact objects, Hot plasma, Iron K-shell lines, Compton scattering, Suzaku, Cataclysmic variable",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Compton Scattering of Fe K alpha Lines in Magnetic Cataclysmic Variables . Abstract : We report on the first measurement of Compton scattering of iron K - alpha bands ( 6 . 4 keV ) by hot plasma in magnetic cataclysmic variables ( mCVs ) .The studies were made with Suzaku spacecraft , which has an remarkable mix of high sensitivity and energy resolution at 6 . 4 keV for mCVs . We observed significant residual emission above continuum around 6 . 4 keV in three out of four targets observed .This is interpreted as Compton - dispersed line photons created in the accretion disk coronae surrounding white dwarfs . In one target , we concluded that the scattered component was significantly larger than the direct component .Our results propose that the topology of the X - ray emitting area might be more sophisticated than previously thought . Keywords : Accreting compact galaxies , Hot plasma , Iron K - shell lines , Compton absorption , Suzaku , Cataclysmic variable",
        "rewrite_text": "We present the inaugural measurement of Compton scattering of iron K-alpha lines (6.4 keV) in magnetic cataclysmic variables (mCVs), utilizing data obtained from the Suzaku spacecraft. This mission is distinguished by its exceptional sensitivity and energy resolution at the critical 6.4 keV energy level, making it particularly well-suited for studying mCVs. Our observations revealed notable residual emissions above the continuum in three out of the four targets analyzed, which we interpret as the result of Compton-scattered line photons generated in the accretion disk coronae surrounding white dwarfs. In one specific target, we found that the intensity of the scattered component was significantly greater than that of the direct emission, suggesting a more complex structure of the X-ray emitting region than previously understood. These findings indicate that the interaction between the hot plasma and the iron K-alpha lines is a crucial aspect of the emission processes in mCVs, and they challenge existing models of the X-ray emission topology in these systems. Our study contributes to a deeper understanding of the physical mechanisms at play in accreting compact objects and highlights the importance of high-resolution spectral analysis in revealing the intricacies of their emission processes. This work opens new avenues for exploring the dynamics of hot plasma in the vicinity of white dwarfs and the role of Compton scattering in shaping the observed X-ray spectra. Keywords: Accreting compact galaxies, Hot plasma, Iron K-shell lines, Compton absorption, Suzaku, Cataclysmic variable.",
        "ori-fast-z-score": -1.25,
        "water-fast-z-score": 4.093146241443879,
        "rewrite-fast-z-score": -0.8320502943378436
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Comparison between Anomalous 6-cm H$_2$CO Absorption and CO(1-0) Emission in the L1204/S140 .\nAbstract:\nWe present new observations of molecular hydrogen (H_2CO) absorption toward the low-mass protostar IRAS 16293-2422, which is associated with two outflows driven by different components of this binary system. The main component drives an east-west bipolar flow that has been traced over more than 1000 AU using SiO emission lines observed at high angular resolution. We have detected anomalously strong absorption features near the systemic velocity of the source for both ortho- and para-H_2CO transitions. These are likely due to self-absorption within the dense gas surrounding the central protostars. In addition, we find evidence for blueshifted absorption features in the para-H_2CO line profiles that may be tracing infalling material along the axis of one of the outflow lobes. Finally, we compare our results with previous studies of carbon monoxide (CO) emission towards the same region.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Comparison between Anomalous 6 - cm H $ _ 2 $ CO Absorption and CO ( 1 - 0 ) Emission in the L1204 / S140 . Abstract : We report new studies of molecular hydrogen ( H _ 2CO ) diffusion toward the small - weight protostar IRAS 16293 - 2422 , which is associated with two outflows driven by various components of this binary system .The main component drives an eastward - west bipolar flow that has been traced over more than 1000 AU utilizing SiO emission lines observed at high angular resolution . We have discovered anomalously strong absorption events near the systemic speed of the source for both ortho - and para - H _ 2CO transitions .These are likely due to self - absorption within the dense gas covering the main protostars . In addition , we find proof for blueshifted absorption features in the para - H _ 2CO line profiles that might be tracing infalling matter along the axis of one of the outflow lobes .Finally , we compare our findings with previous investigations of carbon monoxide ( CO ) emission towards the same region .",
        "rewrite_text": "Title: A Comparative Study of Anomalous 6-cm H$_2$CO Absorption and CO (1-0) Emission in the L1204/S140 Region\n\nAbstract: In this study, we present new findings on the diffusion of molecular hydrogen (H$_2$CO) towards the low-mass protostar IRAS 16293-2422, which is associated with two distinct outflows driven by different components of its binary system. The primary component is responsible for an eastward-directed bipolar outflow that has been traced over a distance exceeding 1000 AU, utilizing high-resolution observations of SiO emission lines. Our investigation reveals the presence of unusually strong absorption features occurring near the systemic velocity of the source for both ortho- and para-H$_2$CO transitions. We hypothesize that these absorption events are a result of self-absorption within the dense gas surrounding the main protostars. Furthermore, we identify blueshifted absorption features in the para-H$_2$CO line profiles, which may indicate the presence of infalling material along the axis of one of the outflow lobes. To contextualize our results, we compare our observations with previous studies of carbon monoxide (CO) emission in the same region. This comparison highlights the complex interplay between the various molecular species present and their respective dynamics, contributing to a deeper understanding of the physical processes occurring in this protostellar environment. Our findings not only enhance the existing knowledge of H$_2$CO behavior in such regions but also provide insights into the broader implications for star formation and the dynamics of molecular outflows.",
        "ori-fast-z-score": -0.3511234415883917,
        "water-fast-z-score": 4.2723919920032305,
        "rewrite-fast-z-score": 1.1766968108291043
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A binary model for the UV-upturn of elliptical galaxies (MNRAS version) .\nAbstract:\nWe present an improved version of our previous work on modelling the ultraviolet upturn in early-type galaxies using binary stars.  We use Monte Carlo simulations to generate synthetic populations with different ages, metallicities and mass ratios between components. The models are compared against observations of nearby galaxies obtained by GALEX. Our results show that binary systems can reproduce well both the strength and shape of the observed UV-optical SEDs. In particular we find that:  - Binary evolution is required to explain the strong UV fluxes seen at young ages (< 1 Gyr). - A large fraction of binaries must be composed of two hot subdwarfs or white dwarfs. - Binaries containing one normal star and one compact object cannot produce enough UV light to match the data. - Mass transfer plays only a minor role in shaping the UV-optical SED. - The best-fit age distribution peaks around 2 Gyr but extends down to younger ages.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A binary simulation for the UV - upturn of elliptical galaxies ( MNRAS version ) . Abstract : We present an updated form of our previous research on modelling the ultraviolet upturn in early - class stars involving binary stars .We use Monte Carlo simulations to produce synthetic populations with various ages , metallicities and mass ratios between components . The models are compared against measurements of neighbouring galaxies collected by GALEX .Our results show that binary systems can reproduce well both the strength and shape of the seen UV - optical SEDs . In particular we find that : - Binary evolution is required to explain the strong UV fluxes seen at young years ( < 1 Gyr ) .- A large fraction of binaries may be composed of two hot subdwarfs or white dwarfs . - Binaries involving one regular star and one compact body fail produce enough UV energy to match the information .- Mass transfer plays only a minor importance in shaping the UV - optical SED . - The best - fitting age distribution peaks around 2 Gyr but continues down to younger ages .",
        "rewrite_text": "We present an enhanced version of our earlier study focused on modeling the ultraviolet (UV) upturn observed in early-type stars, specifically through the lens of binary star systems. Utilizing Monte Carlo simulations, we generate synthetic stellar populations characterized by a range of ages, metallicities, and mass ratios among the binary components. Our models are rigorously compared to observational data from nearby galaxies obtained via the Galaxy Evolution Explorer (GALEX). The findings indicate that binary star systems are capable of accurately reproducing both the intensity and the profile of the UV-optical spectral energy distributions (SEDs) observed. Notably, our analysis reveals several key insights: First, the evolution of binary stars is essential for accounting for the pronounced UV fluxes detected in young stellar populations (less than 1 billion years old). Second, a significant proportion of these binary systems may consist of two hot subdwarfs or white dwarfs. Third, configurations that include one regular star paired with a compact object do not generate sufficient UV radiation to align with the observational data. Additionally, we find that mass transfer processes play a relatively minor role in influencing the UV-optical SEDs. Finally, our best-fitting age distribution indicates a peak around 2 billion years, while also extending to younger stellar ages. These results underscore the importance of binary interactions in understanding the UV upturn phenomenon in elliptical galaxies and contribute to the broader discourse on stellar evolution in these systems.",
        "ori-fast-z-score": 0.741998516004452,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": 1.0083683467310325
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Synchrotron Radio Emission from a Fast Halo Coronal Mass Ejection .\nAbstract:\nWe report on the detection and analysis of radio emission associated with an impulsive solar flare that occurred in active region NOAA 10486 (SOL2010-07-20T17:48) at 17:48 UT on July 20, 2010 using the Nançay Decameter Array (NDA). The event was accompanied by a fast halo coronal mass ejection (CME), which reached Earth at 18:20 UT on July 21. We find that the radio source is located near the center of the CME front as seen in white light images taken by STEREO-Ahead/EUVI 195 Å . The radio flux density shows rapid evolution during the first hour after the onset of the flare, followed by gradual decay over several hours. The radio spectrum has a power-law shape between 1 MHz to 5 GHz. The spectral index decreases rapidly below 100 MHz but remains nearly constant above this frequency.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Synchrotron Radio Emission from a Fast Halo Coronal Mass Ejection . Abstract : We report on the detection and assessment of radio emission associated with an impulsive solar flare that occurred in active region NOAA 10486 ( SOL2010 - 07 - 20T17 : 48 ) at 17 : 48 UT on July 20 , 2010 using the Nançay Decameter Array ( NDA ) .The event was accompanied by a rapid halo coronal mass ejection ( CME ) , which reached Earth at 18 : 20 UT on July 21 . We see that the radio source is situated near the center of the CME front as shown in white light pictures taken by STEREO - Ahead / EUVI 195 Å .The radio flux concentration displays rapid change during the first hour after the outbreak of the flare , followed by slowly evolution over numerous hours . The radio spectrum has a power - law shape between 1 MHz to 5 GHz .The spectral index drops rapidly below 100 MHz but continues nearly constant above this signal .",
        "rewrite_text": "We present our findings on the detection and analysis of radio emissions linked to a sudden solar flare that took place in active region NOAA 10486 (SOL2010-07-20T17:48) at 17:48 UT on July 20, 2010. This event was characterized by a swift halo coronal mass ejection (CME), which impacted Earth at 18:20 UT on July 21. Utilizing the Nançay Decameter Array (NDA), we observed that the radio source was located near the center of the CME front, as evidenced by white light images captured by the STEREO-Ahead/EUVI 195 Å instrument. Our analysis reveals that the radio flux exhibited significant fluctuations during the initial hour following the flare's onset, followed by a gradual evolution over several hours. The radio spectrum displayed a power-law distribution across the frequency range of 1 MHz to 5 GHz. Notably, the spectral index experienced a rapid decline below 100 MHz, while maintaining a relatively stable value at higher frequencies. These observations provide critical insights into the dynamics of radio emissions associated with solar flares and CMEs, enhancing our understanding of solar activity and its implications for space weather phenomena.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 3.3489378339086286,
        "rewrite-fast-z-score": -1.7457431218879391
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Algebraic charge liquids .\nAbstract:\nWe introduce the concept of algebraic charge liquids, which are defined as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators. We show how to construct such models for any finite group G by using an explicit representation of G on the Hilbert space of spinless fermions. The resulting model is exactly solvable when G has no non-trivial subgroups. In this case we find that there exists at least one phase transition between different phases characterized by distinct topological orders. For example, if G = Z2 × Z2 then our construction yields two gapped phases distinguished by their chiral central charges c− = 0 or 1. If G contains a nontrivial subgroup H then the system exhibits gapless excitations corresponding to particles transforming according to irreducible representations (irreps) of H. These results provide new insights into the classification problem of quantum many-body systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Algebraic charge liquids . Abstract : We introduce the idea of algebraic charge liquids , which are specified as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators .We see how to build such theories for any finite class G by using an explicit representation of G on the Hilbert space of spinless fermions . The resulting theory is precisely solvable when G has no non - trivial subgroups .In this instance we find that there exists at least one phase shift between various phases characterized by separate topological orders . For instance , if G = Z2 × Z2 then our build produces two gapped phases distinguished by their chiral central charges c− = 0 or 1 .If G contains a nontrivial subgroup H then the scheme exhibits gapless excitations corresponding to particles transforming according to irreducible representations ( irreps ) of H . These data provide fresh insights into the classification question of quantum several - bodies systems .",
        "rewrite_text": "In this article, we present the concept of algebraic charge liquids, which are defined as the ground states of Hamiltonians characterized by local interactions expressible through fermionic creation and annihilation operators. We demonstrate a method for constructing these theories for any finite group G by utilizing a specific representation of G on the Hilbert space of spinless fermions. Notably, when G lacks non-trivial subgroups, the resulting theory becomes exactly solvable. In such cases, we observe the emergence of at least one phase shift among various phases, each distinguished by distinct topological orders. For example, when G is represented as Z2 × Z2, our construction yields two gapped phases that can be differentiated by their chiral central charges, which take values of c− = 0 or 1. Conversely, if G includes a non-trivial subgroup H, our framework reveals the presence of gapless excitations that correspond to particles transforming according to the irreducible representations (irreps) of H. This exploration not only enhances our understanding of algebraic charge liquids but also contributes valuable insights into the classification of quantum many-body systems. The findings underscore the intricate relationship between group theory and quantum states, paving the way for further research into the properties and behaviors of these novel phases. Overall, our work lays the groundwork for future investigations into the rich landscape of topological orders and their implications in condensed matter physics.",
        "ori-fast-z-score": -1.885618083164127,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": 0.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Horizon-Penetrating Transonic Accretion Disks around Rotating Black Holes .\nAbstract:\nWe present the results of our numerical simulations of accreting black holes in which we have included general relativistic effects and radiative transfer using Monte Carlo techniques. We find that for low mass (M < 10 M_sun) black holes, there is an inner edge to the disk at about 3 Schwarzschild radii where the density drops sharply by several orders of magnitude. The temperature profile shows a sharp rise near this radius due to compression as well as heating by viscous dissipation. For higher masses (10 M_sun < M < 100 M_sun), the disks are optically thick throughout their extent with no clear evidence of any inner edge. In these cases, the temperature profiles show a gradual increase towards smaller radii. Finally, for very massive black holes (M > 100 M_sun), we find that the disks become geometrically thin but remain optically thick out to large distances.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Horizon - Penetrating Transonic Accretion Disks around Rotating Black Holes . Abstract : We present the results of our numerical simulations of accreting grey holes in which we have incorporated general relativistic effects and radiative transfer utilizing Monte Carlo methods .We see that for low weight ( M < 10 M _ sun ) black holes , there is an inner boundary to the disk at about 3 Schwarzschild radii where the density decreases sharply by many orders of magnitude . The temperature profile displays a sharp rise near this radius due to compression as well as cooling by viscous dissipation .For larger masses ( 10 M _ sun < M < 100 M _ sun ) , the disks are optically dense throughout their depth with no clear proof of any outer edge . In these circumstances , the temperature profiles exhibit a slow increase towards smaller radii .Finally , for very huge black holes ( M > 100 M _ sun ) , we find that the disks become geometrically thin but remain optically dense out to large distances .",
        "rewrite_text": "We present the findings from our numerical simulations of accreting black holes, specifically focusing on the incorporation of general relativistic effects and radiative transfer through Monte Carlo methods. Our study investigates the behavior of accretion disks around black holes of varying masses, revealing significant insights into their structure and dynamics. For black holes with masses less than 10 solar masses (M < 10 M_sun), we observe a distinct inner boundary of the accretion disk located approximately at 3 Schwarzschild radii. At this boundary, there is a dramatic decrease in density by several orders of magnitude. Concurrently, the temperature profile shows a sharp increase near this radius, attributed to both compression effects and cooling mechanisms driven by viscous dissipation.\n\nIn contrast, for black holes with masses ranging from 10 to 100 solar masses (10 M_sun < M < 100 M_sun), the accretion disks maintain a high optical density throughout their entire depth, lacking any definitive outer edge. In this mass range, the temperature profiles exhibit a gradual increase as one moves toward smaller radii, indicating a more stable and uniform structure compared to their lighter counterparts.\n\nFor supermassive black holes exceeding 100 solar masses (M > 100 M_sun), our simulations reveal that the accretion disks become geometrically thin while still remaining optically dense over extensive distances. This suggests a complex interplay between gravitational forces and the dynamics of the accretion process, leading to unique characteristics in the behavior of the disks surrounding these massive black holes. Overall, our results contribute to a deeper understanding of the nature of accretion disks in the context of general relativity and highlight the importance of mass in determining their structural properties.",
        "ori-fast-z-score": -1.62746694241347,
        "water-fast-z-score": 3.5795716689756794,
        "rewrite-fast-z-score": 1.2722833945199565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Off-center HII regions in power-law density distributions .\nAbstract:\nWe present the results of numerical simulations aimed at studying the formation and evolution of off-center ionized hydrogen (HII) regions around massive stars embedded in an isothermal, self-gravitating gas distribution with a power-law density profile.  We find that the expansion velocity of the ionization front decreases as it moves away from the star s position towards larger radii. The decrease occurs because the pressure gradient becomes smaller when moving outward due to the decreasing density. As a result, the ionization front may become unstable against small perturbations which lead to its fragmentation into several blobs or fingers. These blobs are surrounded by dense shells of neutral material produced by photo-evaporation of the surrounding medium. In addition, we show that the mass loss rate increases significantly for large values of the index n of the power law density distribution. This increase can be explained by the fact that the gravitational force per unit volume acting on the shell decreases faster than the ram pressure of the expanding bubble.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Off - center HII regions in power - law density distributions . Abstract : We present the results of computational simulations aiming at studying the formation and evolution of off - center ionized hydrogen ( HII ) areas around large stars localized in an isothermal , self - gravitating gas distribution with a power - law density profile .We see that the expansion velocity of the ionization front decreases as it travels away from the star s position towards higher radii . The reduction appears because the pressure gradient becomes lower when moved outward owing to the decreasing density .As a result , the ionization front may grow unstable against small perturbations which lead to its fragmentation into numerous blobs or fingers . These blobs are surrounded by dense shells of neutral material created by photo - evaporation of the nearby medium .In addition , we prove that the mass loss rate grows significantly for large values of the index n of the power law density distribution . This increase can be described by the fact that the gravitational pressure per unit volume acting on the shell decreases quicker than the ram tension of the expanding bubble .",
        "rewrite_text": "In this study, we investigate the formation and evolution of off-center ionized hydrogen (HII) regions surrounding massive stars within an isothermal, self-gravitating gas medium characterized by a power-law density distribution. Through a series of computational simulations, we analyze the dynamics of the ionization front as it expands from the star. Our findings reveal that the expansion velocity of the ionization front diminishes as it moves outward to larger radii. This decrease in velocity is attributed to a reduction in the pressure gradient, which weakens due to the declining density of the surrounding gas. Consequently, the ionization front becomes susceptible to small perturbations, leading to its fragmentation into multiple irregular structures, or blobs, which are interspersed with dense shells of neutral material. These shells are formed as a result of photo-evaporation processes occurring in the adjacent medium. Furthermore, our results indicate a significant increase in the mass loss rate associated with the expanding HII regions, particularly for larger values of the power-law index (n). This phenomenon can be explained by the observation that the gravitational pressure exerted on the shell diminishes at a faster rate than the ram pressure generated by the expanding bubble. Overall, our research enhances the understanding of the complex interactions between massive stars and their surrounding environments, particularly in the context of power-law density distributions, and provides insights into the stability and fragmentation of ionization fronts in astrophysical settings.",
        "ori-fast-z-score": -1.584236068762679,
        "water-fast-z-score": 2.970442628930023,
        "rewrite-fast-z-score": 0.2847473987257497
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exploring the Variable Sky with the Sloan Digital Sky Survey .\nAbstract:\nThe Sloan Digital Sky Survey (SDSS) is an ambitious project to map one quarter of the sky in five photometric bands, and obtain spectra for over 100 million galaxies, quasars, stars, and other celestial objects.  The survey has been completed on two thirds of its original footprint; it will be complete by 2008. This talk describes some of the science that can be done using this data set. It also discusses how astronomers are working together to make use of these vast resources. In particular I describe my own work on studying galaxy clusters as dark matter halos, measuring their masses through gravitational lensing effects, and determining their evolution across cosmic time. Finally, I discuss plans for future surveys which will build upon SDSS s successes. The Sloan Digital Sky Survey is an ambitious project to measure the positions and redshifts of more than 100 million galaxies, quasARS, stars, and other objects in our Universe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exploring the Variable Sky with the Sloan Digital Sky Survey . Abstract : The Sloan Digital Sky Survey ( SDSS ) is an ambitious project to map one quarter of the heavens in five photometric bands , and collect spectra for over 100 million galaxies , quasars , stars , and other celestial entities .The survey has been completed on two thirds of its initial footprint ; it will be full by 2008 . This discussion describes some of the science that can be performed using this data set .It especially discusses how astronomers are working together to make using of these enormous resources . In particular I relate my own research on studying galaxy galaxies as dark matter halos , measuring their masses through gravity lensing effects , and determining their development across universe time .Finally , I discuss strategies for future surveys which will build upon SDSS s successes . The Sloan Digital Sky Survey is an ambitious project to measure the places and redshifts of more than 100 million galaxies , quasARS , stars , and other objects in our Universe .",
        "rewrite_text": "Title: Investigating the Variable Sky through the Sloan Digital Sky Survey\n\nAbstract: The Sloan Digital Sky Survey (SDSS) represents a groundbreaking initiative aimed at mapping a quarter of the celestial sphere across five distinct photometric bands, while also gathering spectra from over 100 million astronomical objects, including galaxies, quasars, stars, and various other celestial phenomena. As of now, the survey has successfully covered two-thirds of its intended area, with full completion anticipated by 2008. This article delves into the scientific opportunities presented by this extensive dataset, highlighting collaborative efforts among astronomers to maximize the utility of these vast resources. A significant focus of the discussion is my own research, which investigates galaxies as dark matter halos. This involves measuring their masses through gravitational lensing effects and analyzing their evolution over cosmic time. Additionally, I will outline potential strategies for future surveys that aim to build upon the achievements of the SDSS. The SDSS not only provides critical measurements of the positions and redshifts of over 100 million galaxies, quasars, stars, and other celestial objects but also serves as a foundational resource for advancing our understanding of the universe. Through this comprehensive exploration, we aim to enhance our grasp of cosmic structures and their dynamics, paving the way for future astronomical discoveries.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 5.976143046671968,
        "rewrite-fast-z-score": 0.10482848367219183
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Theory of enhanced dynamical photo-thermal bi-stability effects in cuprous oxide/organic hybrid heterostructure .\nAbstract:\nThe present work is devoted to the investigation of the photothermal properties and dynamics of the Cu2O/CuO nanocomposite films prepared by pulsed laser deposition (PLD) on Si(100). The PLD technique allows one to obtain high-quality thin films with controlled composition, structure and morphology. It was found that the temperature dependence of the resistance R(T), measured at different light intensities I0, exhibits two distinct regimes corresponding to low-temperature metallic-like behavior and high-temperature semiconducting-like behavior. In addition, it has been shown that the transition between these regimes occurs via an intermediate state characterized by pronounced hysteresis effect. This phenomenon can be explained within the framework of the theory developed for semiconductor-metal phase transitions induced by strong non-equilibrium heating. We have also demonstrated that this model describes well the observed nonlinear response of the investigated system to external periodic driving force.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Theory of enhanced dynamical photo - temperature bi - stability properties in cuprous oxide / organic hybrid heterostructure . Abstract : The present work is devoted to the examination of the photothermal characteristics and dynamics of the Cu2O / CuO nanocomposite films prepared by pulsed laser deposition ( PLD ) on Si ( 100 ) .The PLD procedure allows one to obtain high - grade thin films with controlled composition , structure and morphology . It was shown that the temperature dependence of the resistance R ( T ) , recorded at different light intensities I0 , displays two different regimes corresponding to low - temperature metallic - like behavior and low - temperature semiconducting - like behavior .In addition , it has been shown that the shift between these regimes occurs via an intermediate state characterized by marked hysteresis effect . This phenomenon can be described within the framework of the theoretical created for semiconductor - metal phase transitions induced by weak non - equilibrium heating .We have also demonstrated that this model describes well the seen nonlinear reaction of the investigated structure to external periodic drove force .",
        "rewrite_text": "This study focuses on the investigation of the photothermal properties and dynamic behavior of Cu2O/CuO nanocomposite films, which were fabricated using pulsed laser deposition (PLD) on silicon substrates (Si (100)). The PLD technique enables the production of high-quality thin films with precisely controlled composition, structure, and morphology. Our findings reveal that the temperature-dependent resistance R(T) exhibits two distinct regimes when subjected to varying light intensities (I0): one resembling metallic behavior at low temperatures and the other exhibiting semiconducting characteristics. Notably, the transition between these two regimes occurs through an intermediate state that is characterized by a significant hysteresis effect. This intriguing phenomenon can be effectively explained using a theoretical framework designed for semiconductor-metal phase transitions that are triggered by mild non-equilibrium heating. Furthermore, we have shown that this theoretical model accurately captures the observed nonlinear response of the hybrid structure to external periodic driving forces. The implications of these findings are significant, as they enhance our understanding of the underlying mechanisms governing the photothermal dynamics in hybrid heterostructures, which could pave the way for advancements in optoelectronic applications and the development of novel materials with tailored properties. Overall, this research contributes to the growing body of knowledge in the field of materials science, particularly in the context of hybrid systems that combine the unique properties of organic and inorganic components.",
        "ori-fast-z-score": 0.105999788000636,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": 0.27975144247209416
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic condensation of water at crack tips in fused silica glass .\nAbstract:\nWe report the observation of dynamic condensation of water vapor on crack tips during slow fracture experiments performed under vacuum conditions (10-6 mbar) and low temperature (77 K). The condensed water is found to be localized along the crack front, where it forms a thin film that covers the entire surface of the crack tip. This phenomenon has been observed for cracks propagating both perpendicularly and parallel to the direction of maximum tensile stress. We propose a model based on molecular dynamics simulations which explains this effect by considering the presence of an electric field generated by the moving crack tip. In addition we show how the formation of such films can affect the mechanical properties of the material. Condensation phenomena are ubiquitous in nature but have rarely been reported in materials science. Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses. These results were obtained using a combination of optical microscopy techniques with environmental scanning electron microscopy (ESEM), Raman spectroscopy and infrared reflection absorption spectroscopy (IRAS).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic condensation of liquid at crack tips in fused silica glass . Abstract : We report the observation of static condensation of water vapor on break tips during slow fracture studies performed under vacuum environments ( 10 - 6 mbar ) and low heat ( 77 K ) .The condensed water is found to be localized along the crack front , where it creates a thin film that covers the entire surface of the crack tip . This phenomenon has been observed for fracture propagating both perpendicularly and parallel to the direction of maximum tensile tension .We suggest a theory based on molecular dynamics simulations which explains this effect by examining the presence of an electric field produced by the moved crack edge . In addition we explain how the formation of such films can affect the mechanical behavior of the material .Condensation problems are ubiquitous in nature but have seldom been reported in materials science . Here we present research proof showing that water condenses onto the crack surfaces when they propagate through fused silica glasses .These conclusions were obtained using a combination of optical microscopy methods with environmental scan electron microscopy ( ESEM ) , Raman spectroscopy and infrared reflection spectral spectroscopy ( IRAS ) .",
        "rewrite_text": "We present a detailed investigation into the phenomenon of dynamic condensation of liquid at crack tips in fused silica glass, as observed during slow fracture experiments conducted in controlled vacuum conditions (10^-6 mbar) and at low temperatures (77 K). Our findings reveal that water vapor condenses statically on the crack tips, resulting in the formation of a thin film that uniformly coats the entire surface of the crack front. This condensation effect has been documented for fractures propagating both perpendicular and parallel to the direction of maximum tensile stress. \n\nTo elucidate this phenomenon, we propose a theoretical framework grounded in molecular dynamics simulations, which highlights the role of an electric field generated by the movement of the crack edge. This electric field is instrumental in attracting water molecules, facilitating their condensation at the crack tips. Furthermore, we discuss the implications of this film formation on the mechanical properties of the material, suggesting that the presence of condensed water could significantly influence the fracture behavior of fused silica glass.\n\nWhile condensation phenomena are commonly observed in various natural processes, their occurrence in the context of materials science has been relatively underexplored. Our research provides compelling evidence that water can condense on crack surfaces during the propagation of fractures in fused silica, thereby contributing to a deeper understanding of the interplay between moisture and material integrity. The conclusions drawn from this study are supported by a comprehensive suite of analytical techniques, including optical microscopy, environmental scanning electron microscopy (ESEM), Raman spectroscopy, and infrared reflection absorption spectroscopy (IRAS). This multifaceted approach not only confirms the presence of condensed water but also enhances our understanding of its effects on the mechanical behavior of silica glass under stress.",
        "ori-fast-z-score": -0.40406101782088427,
        "water-fast-z-score": 5.454823740581938,
        "rewrite-fast-z-score": 0.25630729731502827
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A density tensor hierarchy for open system dynamics: retrieving the noise .\nAbstract:\nWe present an exact hierarchical description of the reduced state and dynamics of open quantum systems in terms of a set of coupled equations, which we call the Hierarchy of Density Tensors (HDT). The HDT is derived by applying the Nakajima-Zwanzig projection operator technique to the von Neumann equation describing the evolution of the total system. We show that this approach allows one to retrieve all relevant information about the environment-induced decoherence process on arbitrary timescales. In particular, it provides access to the full spectrum of relaxation rates characterizing the decay of off-diagonal elements of the reduced density matrix as well as the stationary states reached at late times. As an example, we apply our formalism to study the dissipative spin-boson model with Ohmic dissipation. Our results are compared against numerical simulations based on the Quantum Monte Carlo Wavefunction method. \nI. INTRODUCTORY REMARK\nThe understanding of how macroscopic objects behave under the influence of their environments has been a central issue in physics since its very beginning  1, 2  . This problem becomes particularly challenging when dealing with complex many-body systems such as condensed matter or biological ones  3, 4  , where the number of degrees of freedom involved can be extremely large. A powerful theoretical tool to tackle these problems consists in studying the dynamics of the reduced state of the system of interest S conditioned upon some specific measurement performed over the environmental degrees of freedom E  5, 6  .\nIn recent years there have been several attempts to develop efficient methods to describe the time-evolution of the reduced state  7, 8  . Among them, the so-called Hierarchy of Density Matrices (HDM)  9  represents a promising alternative to other approaches  10, 11  due to its ability to capture non-Markovian effects  12  . However, despite being able to provide accurate predictions for short-time evolutions  13  , the HDM fails to reproduce correctly the asymptotic behavior of the system  14  . To overcome this limitation, here we introduce a new formulation of the HDM, called Hierarchy of Density...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A density tensor hierarchy for open network dynamics : retrieving the noise . Abstract : We create an precise hierarchical description of the reduced state and dynamics of open quantum systems in terms of a setting of coupled equations , which we call the Hierarchy of Density Tensors ( HDT ) .The HDT is calculated by using the Nakajima - Zwanzig projection operator technique to the von Neumann equation explaining the evolution of the total system . We see that this methodology allows one to locate all relevant information about the environment - caused decoherence cycle on arbitrary timescales .In particular , it gives access to the full range of relaxation rates characterizing the decay of off - horizontal elements of the reduced density matrix as well as the stationary states reached at late times . As an instance , we apply our formalism to study the dissipative spin - boson theory with Ohmic dissipation .Our results are compared against numerical simulations based on the Quantum Monte Carlo Wavefunction method . I .INTRODUCTORY REMARK The knowledge of how macroscopic objects react under the impact of their environments has been a central topic in science since its very beginning 1 , 2 . This problem arises increasingly challenging when dealing with difficult large - bodies systems such as condensed matter or biological ones 3 , 4 , where the number of degrees of freedom employed can be extremely huge .A popular conceptual technique to tackle these problems involves in examining the dynamics of the reduced state of the system of interest S conditioned upon some specific assessment performed over the environmental degrees of liberty E 5 , 6 . In recent years there have been numerous attempts to develop able methods to explain the period - progression of the reduced state 7 , 8 .Among them , the so - called Hierarchy of Density Matrices ( HDM ) 9 offers a promising alternative to other methods 10 , 11 due to its able to capture non - Markovian influences 12 . However , despite being able to provide accurate forecast for short - time evolutions 13 , the HDM fails to reproduce correctly the asymptotic behavior of the system 14 .To solve this limitation , here we incorporate a new implementation of the HDM , entitled Hierarchy of Density . . .",
        "rewrite_text": "**Title:** A Density Tensor Hierarchy for Open Network Dynamics: Retrieving the Noise\n\n**Abstract:** In this study, we present a comprehensive hierarchical framework for describing the reduced state and dynamics of open quantum systems, which we refer to as the Hierarchy of Density Tensors (HDT). This framework is derived using the Nakajima-Zwanzig projection operator technique applied to the von Neumann equation governing the evolution of the entire system. Our approach enables the identification of all pertinent information regarding the decoherence processes induced by the environment across various timescales. Notably, the HDT framework facilitates access to the complete spectrum of relaxation rates that characterize the decay of off-diagonal elements of the reduced density matrix, as well as the stationary states attained in the long-time limit. To illustrate the applicability of our formalism, we investigate the dissipative spin-boson model with Ohmic dissipation, comparing our analytical results with numerical simulations conducted using the Quantum Monte Carlo Wavefunction method. \n\nThe interaction between macroscopic systems and their environments has been a fundamental topic in scientific inquiry since its inception. This challenge becomes particularly complex when addressing large-scale systems, such as those found in condensed matter physics or biological contexts, where the number of degrees of freedom can be extraordinarily high. A common approach to tackle these issues involves analyzing the dynamics of the reduced state of the system of interest, conditioned on specific measurements of the environmental degrees of freedom. Recent advancements have led to various methodologies aimed at elucidating the temporal evolution of the reduced state. Among these, the Hierarchy of Density Matrices (HDM) has emerged as a promising alternative due to its ability to capture non-Markovian effects. However, while the HDM has shown proficiency in predicting short-time dynamics, it struggles with accurately representing the asymptotic behavior of the system. To address this limitation, we introduce an enhanced version of the HDM, termed the Hierarchy of Density Tensors, which aims to provide a more robust framework for understanding open quantum systems.",
        "ori-fast-z-score": -0.9370425713316364,
        "water-fast-z-score": 6.6172410253729455,
        "rewrite-fast-z-score": 0.7474093186836597
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diffuse HI Disks in Isolated Galaxies .\nAbstract:\nWe present new observations and analysis of the neutral hydrogen (HI) disks surrounding isolated galaxies, using data obtained with the Very Large Array (VLA). We have observed 12 nearby galaxies at 21 cm wavelength to determine their total HI mass and distribution within the optical disk. The sample includes both late-type spirals and dwarf irregulars. Our results show that all but one galaxy has an extended HI halo beyond its optical radius; this is true even for the most gas-rich systems such as NGC 4254 and NGC 5253. In addition, we find evidence for significant amounts of atomic gas outside our detection limits which may be associated with tidal features or other interactions between these galaxies and neighboring companions. These findings are consistent with previous studies showing that many isolated spiral galaxies contain large quantities of cold interstellar medium distributed over several kiloparsecs. \n \n Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diffuse HI Disks in Isolated Galaxies . Abstract : We report new studies and investigation of the neutral hydrogen ( HI ) disks surrounding isolated stars , using data acquired with the Very Large Array ( VLA ) .We have noted 12 nearby galaxies at 21 cm wavelength to estimate their total HI mass and distribution within the optical disk . The sample comprises both late - class spirals and dwarf irregulars .Our results show that all but one star has an extended HI halo beyond its optical diameter ; this is true even for the most gas - rich systems such as NGC 4254 and NGC 5253 . In addition , we find proof for significant amounts of nuclear gas outside our detection limits which may be involved with tidal features or other relationships between these galaxies and surrounding companions .These conclusions are consistent with previous research indicating that several isolated spiral nuclei contain significant amounts of cold interstellar medium spread over several kiloparsecs . Keywords : Atomic Hydrogen , Galaxy Evolution , Spiral Structure",
        "rewrite_text": "We present a comprehensive study of the neutral hydrogen (HI) disks surrounding isolated galaxies, utilizing data obtained from the Very Large Array (VLA). Our investigation focuses on 12 nearby galaxies observed at a wavelength of 21 cm, allowing us to assess their total HI mass and its distribution relative to the optical disk. The selected sample includes a variety of galaxy types, specifically late-type spirals and dwarf irregulars. Our findings reveal that nearly all of the galaxies in our sample possess an extended HI halo that exceeds their optical diameter, with the exception of one galaxy. Notably, this phenomenon is observed even in the most gas-rich galaxies, such as NGC 4254 and NGC 5253. Furthermore, we have identified substantial amounts of nuclear gas that lie beyond our detection thresholds, which may be associated with tidal interactions or other relationships between these isolated galaxies and their nearby companions. These results align with previous studies that suggest many isolated spiral nuclei harbor significant quantities of cold interstellar medium, extending over several kiloparsecs. Our research contributes to the understanding of atomic hydrogen in galaxy evolution and the structural dynamics of spiral galaxies, highlighting the importance of HI in the context of galactic formation and interaction. The implications of our findings may provide new insights into the role of neutral hydrogen in shaping the characteristics of isolated galaxies and their evolution over cosmic time. \n\nKeywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure.",
        "ori-fast-z-score": -0.8944271909999159,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 0.7364596943186588
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics and planet formation in/around binaries .\nAbstract:\nWe present the results of numerical simulations of planetary systems around binary stars, focusing on the effects of orbital migration due to tidal interaction between planets and their host star(s). We find that for most initial conditions considered here (except those with very small semi-major axes), the final orbits are generally circularized by tides raised on the planet by its host star(s); this is true even if the original orbit was eccentric or inclined relative to the binary s plane.  The final mass distribution of planets depends strongly on the initial semimajor axis of the planet. For large values of the initial semi-major axis, we find that the final masses tend to be smaller than the initial ones because of significant inward migration driven by tidal dissipation inside the planet. On the other hand, when the initial semi-major axis is relatively small compared to the binary separation, the final masses can become larger than the initial ones as a result of outward migration caused by tidal friction acting at the surface of the planet.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics and planet development in / around binaries . Abstract : We present the conclusion of computational simulations of planetary structures around binary stars , concentrating on the effects of orbital movement owing to tidal association between planets and their host star ( s ) .We see that for most initial conditions considered here ( except those with very small semi - major axes ) , the last orbits are typically circularized by tides raised on the planet by its host star ( s ) ; this is true even if the previous orbit was eccentric or tilted relative to the binary s plane . The final mass distribution of planets depends strongly on the first semimajor axis of the planet .For large values of the first semi - major axis , we find that the last masses tend to be less than the first ones because of large inward movement generated by tidal dissipation inside the planet . On the other hand , when the first semi - major axis is fairly little compared to the binary merger , the last masses can turn larger than the first ones as a due of outward migration caused by tidal tension acted at the surface of the planet .",
        "rewrite_text": "We present the findings from our computational simulations investigating the dynamics and development of planetary systems around binary stars, with a particular focus on the influence of orbital motion resulting from tidal interactions between planets and their host stars. Our analysis reveals that, under most initial conditions examined—except for scenarios involving very small semi-major axes—the final orbits of planets tend to become circularized due to tidal forces exerted by their host stars. This circularization occurs even when the initial orbits are eccentric or misaligned with respect to the binary's orbital plane. Furthermore, we observe that the ultimate mass distribution of the planets is significantly influenced by their initial semi-major axes. For planets with larger initial semi-major axes, we find that their final masses are generally lower than their initial masses. This reduction is attributed to substantial inward migration driven by tidal dissipation occurring within the planets. Conversely, for planets that start with relatively small semi-major axes in relation to the binary system, we note that their final masses can exceed their initial masses. This increase is a result of outward migration induced by tidal forces acting on the planet's surface. Our results underscore the complex interplay between tidal interactions and planetary dynamics in binary star systems, providing valuable insights into the processes that govern planet formation and evolution in such environments.",
        "ori-fast-z-score": -3.1378581622109447,
        "water-fast-z-score": 4.196370313678693,
        "rewrite-fast-z-score": -0.17541160386140586
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The remarkable X-ray spectrum of the Broad-Line Radio Galaxy 3C 445 .\nAbstract:\nWe present an analysis of the broadband (0.5-10 keV) X-ray spectrum of the radio galaxy 3C 445, using data obtained with XMM-Newton and Chandra observatories in 2001-2002 . The observed X-ray emission is dominated by a hard power-law component which can be fitted equally well either by thermal Comptonization or non-thermal inverse Compton scattering models. We find that both models require a large amount of cold matter to produce the soft excess below 1 keV. This suggests that there are two distinct components contributing to the X-ray emission - one associated with hot plasma and another related to cool gas clouds. In addition we detect several narrow absorption lines at energies corresponding to highly ionized species such as O vii , Ne ix , Mg xi and Si xiii . These features may originate in outflows driven by nuclear activity. Finally, we report on the detection of Fe Kα line at 6.4 keV produced by reflection off distant material.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The extraordinary X - ray signal of the Broad - Line Radio Galaxy 3C 445 . Abstract : We present an analysis of the broadband ( 0 . 5 - 10 keV ) X - ray bandwidth of the radio star 3C 445 , using data acquired with XMM - Newton and Chandra observatories in 2001 - 2002 .The observed X - ray radiation is dominated by a hard energy - law component which can be fit equally well either by mechanical Comptonization or non - thermal integral Compton absorption theories . We see that both models require a large amount of cold matter to produce the soft excess below 1 keV .This implies that there are two different components contributing to the X - ray radiation - one related with bright plasma and another linked to cool gas clouds . In addition we find various narrow absorbed lines at energies corresponding to strongly ionized species such as O vii , Ne ix , Mg xi and Si xiii .These features could occur in outflows driven by nuclear activity . Finally , we note on the observation of Fe Kα line at 6 . 4 keV generated by absorption off remote material .",
        "rewrite_text": "We present a comprehensive analysis of the broadband X-ray emissions (0.5 - 10 keV) from the Broad-Line Radio Galaxy 3C 445, utilizing observational data collected from the XMM-Newton and Chandra observatories during the years 2001 to 2002. Our findings reveal that the X-ray spectrum is predominantly characterized by a hard energy-law component, which can be effectively modeled using either mechanical Comptonization or non-thermal integral Compton absorption theories. Both theoretical frameworks necessitate the presence of a significant amount of cold matter to account for the observed soft excess emissions below 1 keV. This suggests the existence of two distinct components contributing to the overall X-ray radiation: one associated with luminous plasma and the other linked to cooler gas clouds. Furthermore, our analysis identifies several narrow absorption lines at energies indicative of highly ionized species, including O VII, Ne IX, Mg XI, and Si XIII. These spectral features may arise from outflows driven by the active galactic nucleus. Additionally, we observe the Fe Kα line at 6.4 keV, which appears to be generated by absorption from distant material. This study enhances our understanding of the complex X-ray emission mechanisms in 3C 445 and provides insights into the interplay between hot plasma and cooler gas in the context of active galactic nuclei.",
        "ori-fast-z-score": -2.060839349277234,
        "water-fast-z-score": 4.960307567358167,
        "rewrite-fast-z-score": 0.9332565252573828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Local Galaxy 8 micron Luminosity Function .\nAbstract:\nWe present the luminosity function (LF) for galaxies in the local universe at rest-frame wavelengths between 3 and 24 microns, using data obtained with Spitzer Space Telescope s Infrared Array Camera (IRAC). We use two different methods to estimate the LF - one based on direct counts of galaxies within bins of absolute magnitude, and another that uses an analytic model fit to these galaxy counts. The results are consistent across both techniques. Our best-fit Schechter parameters are: M* = -19.6 +/- 0.1 mag., log(L/Lsun) * = 10.9 +/- 0.2 dex, and alpha = -1.3 +/- 0.4. These values agree well with previous determinations made by other authors over similar wavelength ranges. However, we find evidence for a significant excess number density of faint sources relative to predictions from our best-fit Schechte functions. This excess is most pronounced at longer wavelengths where it amounts to ~50% more objects than expected.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Local Galaxy 8 micron Luminosity Function . Abstract : We present the luminosity function ( LF ) for galaxies in the local universe at rest - frame wavelengths between 3 and 24 microns , using data acquired with Spitzer Space Telescope s Infrared Array Camera ( IRAC ) .We use two different methods to estimate the LF - one based on direct counts of galaxies within bins of absolute magnitude , and another that using an analytic model suited to these galaxy counts . The results are compatible across both algorithms .Our best - fitting Schechter parameters are : M * = - 19 . 6 + / - 0 . 1 mag . , log ( L / Lsun ) * = 10 . 9 + / - 0 . 2 dex , and alpha = - 1 . 3 + / - 0 . 4 . These values comply better with previous determinations made by other researchers over similar frequency ranges .However , we find proof for a substantial accumulation number density of faint sources relative to expectations from our better - fitting Schechte functions . This excess is most pronounced at higher wavelengths where it amounts to ~ 50 % more bodies than expected .",
        "rewrite_text": "We present a comprehensive analysis of the luminosity function (LF) for galaxies in the local universe, focusing on rest-frame wavelengths ranging from 3 to 24 microns. This study utilizes data obtained from the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. To derive the LF, we employed two distinct methodologies: one that involves direct counts of galaxies categorized by absolute magnitude bins, and another that applies an analytical model tailored for these galaxy counts. The results obtained from both approaches demonstrate consistency, reinforcing the reliability of our findings. \n\nOur analysis yields the best-fitting Schechter parameters, which are as follows: M* = -19.6 ± 0.1 mag, log(L/Lsun)* = 10.9 ± 0.2 dex, and α = -1.3 ± 0.4. These parameters align closely with previous studies conducted by other researchers within similar frequency ranges, indicating a robust understanding of the local galaxy population. However, our investigation reveals a significant discrepancy in the number density of faint sources, which exceeds expectations based on our Schechter function fits. This excess is particularly notable at higher wavelengths, where we observe approximately 50% more faint galaxies than predicted. \n\nThis finding suggests that the faint end of the galaxy LF may be more complex than previously understood, prompting further investigation into the underlying causes of this discrepancy. Our results contribute valuable insights into the structure and distribution of galaxies in the local universe, highlighting the importance of infrared observations in uncovering the characteristics of faint galaxy populations.",
        "ori-fast-z-score": -0.9701425001453319,
        "water-fast-z-score": 3.585685828003181,
        "rewrite-fast-z-score": -2.4351231101124045
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Sensitivity of Massive Star Nucleosynthesis and Evolution to Solar Abundances and to Uncertainties in Helium Burning Reaction Rates .\nAbstract:\nWe present new calculations for massive star evolution, nucleosynthesis, and yields using updated nuclear reaction rates and solar abundances. We find that our results are sensitive to uncertainties in the helium burning rate at high temperatures (T > 2 x 10^9 K). The sensitivity is particularly strong when we use the most recent recommended value for the 12C(alpha, gamma)16O cross section. This result has important implications for studies of chemical enrichment by supernovae Ia progenitors. \n \n Keywords: Nuclear reactions; Supernovae; Stellar evolution; Yields \n \n 1 Introduction \n \n In this work we study how uncertainties in nuclear physics affect predictions about stellar evolution and nucleosynthesis. Our goal is to understand better what can be learned from observations of stars and their remnants. For example, it is well known that there exist large discrepancies between observed elemental abundance ratios in metal-poor halo stars and those predicted by standard models of galactic chemical evolution  1  . These differences may arise because some key nuclear processes have been poorly understood or not included in current evolutionary codes  2  , but they could also reflect systematic errors in observational data  3  .\n \nIn order to address these issues, we perform detailed numerical simulations of massive star evolution with different sets of input parameters. Specifically, we consider two cases where the initial mass fraction of helium XHe = 0.25 and 0.30 respectively  4  . We evolve each model until its core collapses into a neutron star. During the collapse phase, we follow the hydrodynamics of the explosion as described in  5  . Afterwards, we compute the composition of the ejecta using an improved version  6  of the one-dimensional post-processing code developed originally by  7  . \n \n 2 Input Physics and Numerical Methods",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Sensitivity of Massive Star Nucleosynthesis and Evolution to Solar Abundances and to Uncertainties in Helium Burning Reaction Rates . Abstract : We present new calculations for huge star evolution , nucleosynthesis , and yields using updated atomic reaction rates and solar abundances .We see that our findings are susceptible to uncertainties in the helium burning rate at high levels ( T > 2 x 10 ^ 9 K ) . The sensitivity is especially powerful when we using the most current recommended estimate for the 12C ( beta , alpha ) 16O cross area .This result has significant implications for research of chemical enrichment by supernovae Ia progenitors . Keywords : Nuclear processes ; Supernovae ; Stellar evolution ; Yields 1 Introduction In this research we study how uncertainties in nuclear science affect calculations about stellar evolution and nucleosynthesis .Our goal is to realize best what can be learned from measurements of stars and their remnants . For instance , it is well established that there exist large discrepancies between measured elemental density levels in metal - poor halo stars and those predicted by typical models of galactic chemical evolution 1 .These changes may arise because some important radioactive processes have been poorly described or not incorporated in current evolutionary codes 2 , but they may also reflect widespread errors in observational data 3 . In order to meet these problems , we perform comprehensive numerical simulations of large star evolution with various sets of input parameters .Specifically , we study two situations where the first mass fraction of helium XHe = 0 . 25 and 0 . 30 respectively 4 . We evolve each model until its core collapses into a neutron star .During the failure phase , we follow the hydrodynamics of the explosion as described in 5 . Afterwards , we compute the composition of the ejecta using an updated model 6 of the one - dimensional post - processing language developed originally by 7 .2 Input Physics and Numerical Methods",
        "rewrite_text": "**Title:** On the Sensitivity of Massive Star Nucleosynthesis and Evolution to Solar Abundances and to Uncertainties in Helium Burning Reaction Rates\n\n**Abstract:** In this study, we present new calculations regarding the evolution, nucleosynthesis, and yield of massive stars, utilizing updated atomic reaction rates and solar abundance data. Our analysis reveals that the results are highly sensitive to uncertainties in the helium burning reaction rates, particularly at elevated temperatures (T > 2 x 10^9 K). This sensitivity is notably pronounced when employing the latest recommended estimates for the cross-section of the reaction 12C(β, α)16O. The implications of these findings are significant, particularly in the context of understanding the chemical enrichment processes associated with Type Ia supernova progenitors. \n\nThe research aims to address how uncertainties in nuclear physics impact our understanding of stellar evolution and nucleosynthesis. It is well-documented that there are considerable discrepancies between the observed elemental abundances in metal-poor halo stars and those predicted by conventional models of galactic chemical evolution. These discrepancies may stem from inadequately described or omitted radioactive processes in existing evolutionary models, as well as potential inaccuracies in observational data. \n\nTo tackle these challenges, we conduct extensive numerical simulations of massive star evolution, varying the input parameters to explore different scenarios. Specifically, we analyze two cases with initial helium mass fractions of XHe = 0.25 and XHe = 0.30. Each model is evolved until core collapse leads to the formation of a neutron star. During the collapse phase, we meticulously track the hydrodynamics of the ensuing explosion. Following this, we compute the composition of the ejected material using an updated one-dimensional post-processing framework. Our findings contribute to a deeper understanding of the nucleosynthetic pathways and the resultant yields from massive stars, ultimately enhancing our comprehension of the chemical evolution of the universe.\n\n**Keywords:** Nuclear processes; Supernovae; Stellar evolution; Yields",
        "ori-fast-z-score": -0.9330078226479681,
        "water-fast-z-score": 7.326721713278715,
        "rewrite-fast-z-score": 1.3522468075656264
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measuring the Decoherence of a Quantronium Qubit with the Cavity Bifurcation Amplifier .\nAbstract:\nWe report on an experiment to measure decoherence in a quantronium qubit using the cavity bifurcation amplifier (CBA). The CBA is used as a dispersive readout device for the quantronium, and we show that it can be operated at high bandwidths without significant back-action noise or added phase noise. We demonstrate measurement times up to 1 ms per data point while maintaining quantum-limited sensitivity. This allows us to perform measurements over many relaxation cycles of the system, which are necessary to extract information about its dynamics. In addition, we use this technique to study the effect of different environmental conditions on the coherence time of our system. Our results indicate that the CBA has great potential for future experiments studying open quantum systems. Quantum mechanics predicts that macroscopic objects such as atoms will exhibit wave-like behavior when they interact with light fields. However, these predictions have not been experimentally verified because measuring the properties of individual particles requires extremely sensitive detectors. Here, we present a new method for detecting single photons based on a nonlinear optical process known as parametric down-conversion. Using this approach, we detect single photons emitted by a trapped atom interacting with a laser field inside a high-finesse optical cavity. By coupling the output of the cavity into a second cavity containing a nonlinear crystal, we generate pairs of entangled photons whose frequencies differ by twice the frequency of the pump beam. These photon pairs are then detected simultaneously by two silicon avalanche photodiodes operating in Geiger mode.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measuring the Decoherence of a Quantronium Qubit with the Cavity Bifurcation Amplifier . Abstract : We report on an experiment to measure decoherence in a quantronium qubit using the cavity bifurcation amplifier ( CBA ) .The CBA is utilized as a dispersive readout device for the quantronium , and we prove that it can be run at high bandwidths without significant back - action noise or added phase noise . We showed measurement times up to 1 ms per data point while maintaining quantum - limited accuracy .This enables us to conduct measurements over numerous relaxation cycles of the system , which are necessary to extract information about its mechanics . In addition , we utilize this methodology to study the impact of different climate circumstances on the coherence time of our system .Our results show that the CBA has tremendous possibility for future research studying open quantum systems . Quantum theory predicts that macroscopic objects such as atoms will exhibit wave - like behavior when they interact with light fields .However , these predictions have not been experimentally confirmed because measuring the properties of individual molecules requires exceptionally sensitive detectors . Here , we present a new method for detecting single photons using on a nonlinear optical process known as parametric down - transfer .Using this methodology , we identify single photons generated by a captured molecule interacting with a laser field inside a high - finesse optical cavity . By coupling the output of the cavity into a second tube featuring a nonlinear crystal , we produce pairs of entangled photons whose frequencies vary by twice the frequency of the pump beam .These photon pairs are then discovered concurrently by two silicon avalanche photodiodes running in Geiger mode .",
        "rewrite_text": "We present an experimental study focused on measuring the decoherence of a quantronium qubit through the application of a cavity bifurcation amplifier (CBA). The CBA serves as a dispersive readout mechanism for the quantronium, and we demonstrate its capability to operate at high bandwidths while minimizing back-action noise and additional phase noise. Our findings indicate that we can achieve measurement durations of up to 1 millisecond per data point while preserving quantum-limited accuracy. This extended measurement capability allows us to gather data across multiple relaxation cycles of the system, which is essential for extracting detailed insights into its mechanical properties. Furthermore, we apply this approach to investigate how varying environmental conditions influence the coherence time of our system. The results underscore the significant potential of the CBA for advancing research in the realm of open quantum systems.\n\nIn the context of quantum theory, it is anticipated that macroscopic entities, such as atoms, will display wave-like characteristics when subjected to interactions with light fields. However, experimental validation of these predictions has been challenging due to the necessity for highly sensitive detectors to measure the properties of individual molecules. In this study, we introduce an innovative technique for detecting single photons based on a nonlinear optical phenomenon known as parametric down-conversion. Utilizing this method, we successfully identify single photons emitted by a molecule that is interacting with a laser field within a high-finesse optical cavity. By directing the output from the cavity into a secondary tube containing a nonlinear crystal, we generate pairs of entangled photons, with their frequencies differing by twice that of the pump beam. These entangled photon pairs are subsequently detected simultaneously by two silicon avalanche photodiodes operating in Geiger mode. This work not only enhances our understanding of decoherence in quantum systems but also paves the way for future explorations in quantum optics and photonics.",
        "ori-fast-z-score": 1.6164477182409742,
        "water-fast-z-score": 8.607358141336304,
        "rewrite-fast-z-score": 2.2132669799727465
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiuser detection in a dynamic environment Part I: User identification and data detection .\nAbstract:\nIn this work, we consider the problem of multiuser detection (MUD) for code division multiple access systems with time-varying channels. We propose an algorithm that jointly performs user identification and data detection by using a maximum likelihood criterion. The proposed method is based on the expectation-maximization (EM) algorithm which iteratively estimates both the channel coefficients and transmitted symbols. In order to reduce computational complexity, we also develop a low-complexity suboptimal MUD scheme. Numerical results show that our proposed algorithms outperform existing schemes under various scenarios. Index Terms-Data detection, EM algorithm, Multiuser detection, Time varying channels. 1 Introduction Code-division-multiple-access (CDMA) has been widely used as one of the most promising technologies for next-generation wireless communications due to its high spectral efficiency  1  . However, CDMA suffers from severe interference between users caused by multipath propagation  2  , especially when the number of active users increases  3  .\nTo mitigate inter-user interference, multiuser detectors have been developed  4  -  6  . Among them, linear multiuser detectors are attractive because they can be implemented easily at low cost  7  . Unfortunately, these detectors suffer from performance loss compared to optimal multiuser detectors  8  . To improve their performance, nonlinear multiuser detectors such as successive interference cancellation  9  or parallel interference cancellation  10  were introduced. These detectors require accurate knowledge about the received signals  11  . Therefore, blind multiuser detectors  12  -  14  were proposed to estimate unknown parameters without any training sequence  15  . Although blind multiuser detectors do not need prior information about the received signal, they usually perform worse than conventional multiuser detectors  16  .\nRecently, there has been growing interest in developing multiuser detectors for time-varying channels  17  -  20  . Since the channel varies over time, it becomes more difficult to detect the transmitted symbol accurately  21  . Moreover, if the channel changes rapidly, then the detector may fail completely  22  . Thus, it is important to design robust multiuser detectors against rapid channel variations  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiuser tracking in a dynamic landscape Part I : User identity and data detection . Abstract : In this study , we investigate the issue of multiuser tracking ( MUD ) for code division multiple entry systems with time - differing channels .We suggest an algorithm that collectively performs user identification and information detection by using a maximum likelihood criterion . The proposed approach is based on the expectation - maximization ( EM ) algorithm which iteratively measures both the channel coefficients and transmitted symbols .In order to reduce numerical difficulty , we also build a small - complexity suboptimal MUD scheme . Numerical results show that our proposed methods outperform established algorithms under various scenarios .Index Terms - Data tracking , EM algorithm , Multiuser tracking , Time varying channels . 1 Introduction Code - division - multiple - access ( CDMA ) has been widely adopted as one of the most attractive devices for next - generation communications transmission thanks to its high spectral power 1 .However , CDMA suffers from severe interference between clients caused by multipath propagation 2 , particularly when the proportion of active consumers increases 3 . To mitigate inter - customer interference , multiuser detectors have been created 4 - 6 .Among them , linear multiuser detectors are interesting because they can be deployed easily at low cost 7 . Unfortunately , these detectors suffer from performance loss compared to optimal multiuser detectors 8 .To increase their performance , nonlinear multiuser detectors such as consecutive interference cancellation 9 or parallel interference cancellation 10 were introduced . These detectors need accurate knowledge about the received transmissions 11 .Therefore , blind multiuser detectors 12 - 14 were recommended to estimate unknown parameters without any testing sequence 15 . Although blind multiuser detectors do not require prior information about the received signal , they generally work worse than conventional multiuser detectors 16 .Recently , there has been growing interest in building multiuser detectors for time - differing channels 17 - 20 . Since the channel varies over time , it becomes more hard to identify the transmitted expression accurately 21 .Moreover , if the channel shifts rapidly , then the sensor may fail entirely 22 . Thus , it is important to build robust multiuser detectors against sudden channel change 23 .",
        "rewrite_text": "**Title: Multiuser Tracking in a Dynamic Landscape Part I: User Identity and Data Detection**\n\n**Abstract:** This research addresses the challenge of multiuser tracking (MUD) within code division multiple access (CDMA) systems characterized by time-varying channels. We propose a novel algorithm that integrates user identification and data detection through a maximum likelihood criterion. Our approach leverages the expectation-maximization (EM) algorithm, which iteratively estimates both the channel coefficients and the transmitted symbols to enhance tracking accuracy. To alleviate computational complexity, we also introduce a suboptimal MUD scheme that operates with reduced complexity. Our numerical simulations demonstrate that the proposed methods significantly outperform existing algorithms across a variety of scenarios, highlighting their effectiveness in dynamic environments. \n\nThe introduction of CDMA as a preferred technology for next-generation communication systems is largely due to its efficient spectral utilization. However, the technology faces significant challenges, particularly from interference among users caused by multipath propagation, which becomes more pronounced as the number of active users increases. To address inter-user interference, various multiuser detection techniques have been developed. Linear multiuser detectors are appealing due to their ease of implementation and low cost, yet they typically exhibit performance deficits compared to optimal multiuser detectors. In response, nonlinear multiuser detection methods, such as consecutive and parallel interference cancellation, have been proposed, though these require precise knowledge of the received signals.\n\nBlind multiuser detectors have emerged as a solution to estimate unknown parameters without the need for training sequences, yet they often underperform relative to traditional detectors. Recent research has increasingly focused on the development of multiuser detectors capable of functioning effectively in time-varying channels. The inherent variability of these channels complicates the accurate identification of transmitted signals, and rapid channel fluctuations can lead to complete detection failures. Consequently, the development of robust multiuser detectors that can withstand abrupt changes in channel conditions is of paramount importance. \n\n**Index Terms:** Data tracking, EM algorithm, Multiuser tracking, Time-varying channels.",
        "ori-fast-z-score": -1.8407159732336889,
        "water-fast-z-score": 7.777427086962838,
        "rewrite-fast-z-score": -0.3892494720807615
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mass Loss and Evolution of Stars and Star Clusters: a Personal Historical Perspective .\nAbstract:\nThe evolution of stars is driven by nuclear fusion reactions in their cores, which generate energy that powers the star s luminosity.  The rate at which these reactions occur depends on how much fuel (i.e., helium) there is available to burn.   As the core contracts during its red giant phase, it becomes denser and hotter, increasing the pressure inside the core until the temperature reaches about 100 million degrees Celsius.  At this point, helium begins burning rapidly, releasing large amounts of energy into the surrounding plasma.  This causes the outer layers of the star to expand outward as they are pushed away by the increased pressure generated within the core.  Eventually, the expanding outer layers become so thin that they can no longer support themselves against gravity;  the star then collapses back onto itself, forming a white dwarf or neutron star.   In addition to nuclear fusion reactions occurring in the core, some massive stars also undergo explosive nuclear fusion reactions called supernovae, which eject most of the star s mass out into space.   These explosions release huge amounts of energy and light, making them visible across great distances in the night sky.   During my career studying stellar astrophysics, I have been fortunate enough to witness many spectacular events such as these first hand.   ...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mass Loss and Evolution of Stars and Star Clusters : a Personal Historical Perspective . Abstract : The evolved of stars is caused by nuclear fusion compounds in their cores , which generate energy that fuels the star s luminosity .The rate at which these reactions occur varies on how many fuel ( i . e . , helium ) there is provided to burn . As the core contracts during its dark giant stage , it becomes denser and warmer , increasing the pressure inside the core until the temperature exceeds about 100 million degrees Celsius .At this time , helium enters burning fast , releasing large quantities of electricity into the nearby plasma . This creates the exterior layers of the star to expand outward as they are pushed away by the increased pressure generated within the core .Eventually , the expanding outer layers get so thin that they can no longer support themselves against gravity ; the star then collapses back onto itself , forming a white dwarf or neutron star . In addition to nuclear fusion events occurring in the core , some enormous stars also initiate explosive nuclear fusion events called supernovae , which eject most of the star s mass out into space .These bursts generate huge amounts of electricity and light , making them visible across tremendous distances in the night sky . During my life researching stellar astrophysics , I have been fortunate sufficient to witness several spectacular disasters such as these first hand ....",
        "rewrite_text": "**Title:** Mass Loss and Evolution of Stars and Star Clusters: A Personal Historical Perspective\n\n**Abstract:** The evolution of stars is fundamentally driven by the process of nuclear fusion occurring in their cores, which produces the energy necessary for their luminosity. The rate of these fusion reactions is contingent upon the availability of fuel, primarily helium, which is consumed during the burning process. As a star transitions into its red giant phase, the core undergoes contraction, leading to increased density and temperature. This rise in temperature, surpassing approximately 100 million degrees Celsius, triggers rapid helium fusion, resulting in the release of substantial energy into the surrounding plasma. The consequent pressure from this energy release causes the outer layers of the star to expand outward. However, as these layers become increasingly tenuous, they eventually lose the ability to counteract gravitational forces, leading to the star's collapse into a white dwarf or neutron star.\n\nIn addition to the nuclear fusion processes within the core, massive stars can also experience explosive events known as supernovae. These cataclysmic explosions eject a significant portion of the star's mass into space, generating immense amounts of energy and light that can be observed across vast cosmic distances. Throughout my career in stellar astrophysics, I have had the privilege of witnessing several of these extraordinary phenomena firsthand. This article reflects on the intricate mechanisms of mass loss and stellar evolution, drawing from both historical and contemporary perspectives in the field. It aims to provide insights into the complex life cycles of stars and star clusters, emphasizing the significance of mass loss in shaping their evolution and the broader implications for our understanding of the universe.",
        "ori-fast-z-score": -0.4622501635210242,
        "water-fast-z-score": 6.444022325288263,
        "rewrite-fast-z-score": -0.17277368511627203
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Magnetic Field of the Solar Corona from Pulsar Observations .\nAbstract:\nWe present an analysis of pulsar observations to determine the magnetic field strength in the solar corona at heights between 1 and 3 R_Sun . We use data obtained with the Nançay Radio Telescope (NRT) for two different radio frequencies, 327 MHz and 1420 MHz, corresponding to emission heights of about 2 and 5 R_Sun , respectively. The observed pulse profiles are modeled using a simple model that includes contributions from both the local interstellar medium and the solar wind plasma. From these models we derive estimates for the coronal magnetic field strengths as well as the electron density distribution along the line-of-sight towards PSR B1133+16 .\nThe results show that the magnetic field decreases rapidly with height above the photosphere but is still strong enough to confine energetic particles up to several solar radii away from the Sun s surface. This suggests that particle acceleration processes may be taking place throughout most of the solar atmosphere.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Magnetic Field of the Solar Corona from Pulsar Observations . Abstract : We report an assessment of pulsar observations to measure the magnetic force strength in the solar corona at heights between 1 and 3 R _ Sun .We use data acquired with the Nançay Radio Telescope ( NRT ) for two different radio altitudes , 327 MHz and 1420 MHz , related to emission heights of about 2 and 5 R _ Sun , respectively . The observed pulse profiles are modeled using a simple simulation that contains contributions from both the local interstellar material and the solar wind plasma .From these models we derive estimates for the coronal magnetic field strengths as also as the electron concentration distribution along the line - of - seeing towards PSR B1133 + 16 . The results show that the magnetic force drops rapidly with depth above the photosphere but is nevertheless strong enough to confine energetic particles up to several solar radii away from the Sun s surface .This implies that particle acceleration processes possibly be taking place throughout most of the solar atmosphere .",
        "rewrite_text": "We present an analysis of pulsar observations aimed at quantifying the magnetic field strength within the solar corona, specifically at altitudes ranging from 1 to 3 solar radii (R☉). Utilizing data from the Nançay Radio Telescope (NRT) at two distinct radio frequencies—327 MHz and 1420 MHz—corresponding to emission heights of approximately 2 and 5 R☉, respectively—we investigate the characteristics of the observed pulse profiles. Our approach involves a straightforward simulation that incorporates contributions from both the local interstellar medium and the solar wind plasma. Through this modeling, we derive estimates for the magnetic field strengths in the corona, as well as the distribution of electron concentration along the line of sight towards the pulsar PSR B1133 + 16. \n\nThe findings indicate a rapid decline in magnetic force with increasing altitude above the photosphere; however, the magnetic field remains sufficiently robust to confine energetic particles at distances extending several solar radii from the Sun's surface. This observation suggests that particle acceleration mechanisms may be active throughout a significant portion of the solar atmosphere. Our results contribute to a deeper understanding of the solar corona's magnetic environment and its role in the dynamics of solar wind and cosmic ray propagation. This study underscores the importance of pulsar observations in astrophysical research, particularly in elucidating the complex interactions between solar magnetic fields and high-energy particles in the heliosphere.",
        "ori-fast-z-score": -0.6793662204867574,
        "water-fast-z-score": 3.9378076532410837,
        "rewrite-fast-z-score": -0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vacant Set of Random Interlacements and Percolation .\nAbstract:\nWe study the vacant set of random interlacements in Rd, d>1, which is defined as the complement of the union of all open paths starting at 0 up to time 1. We prove that it has Hausdorff dimension equal to d-1 almost surely by showing that its upper Minkowski dimension equals this value with probability one. This result extends previous results on the vacant set of simple random walk obtained by Lawler, Schramm and Werner (Annals of Probability 24(3), 1997) and Benjamini, Kalai and Schramm (Journal of Functional Analysis 157(1), 2000). The proof relies heavily on recent developments concerning the geometry of Brownian motion and the theory of stable processes. In particular we use an estimate for the Green function of the killed Brownian motion due to Bass and Burdzy (Annales de l Institut Henri Poincare 22(2), 1999).\nThe main motivation behind our work comes from percolation theory where the vacant set of random walks plays a crucial role. Indeed, we show how our techniques can be used to obtain new information about the critical behavior of bond-percolation models on Zd.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vacant Set of Random Interlacements and Percolation . Abstract : We consider the empty set of random interlacements in Rd , d > 1 , which is defined as the complement of the union of all open routes starting at 0 up to time 1 .We establish that it has Hausdorff dimension equal to d - 1 fairly surely by showing that its upper Minkowski dimension equals this value with probability one . This result continues preceding results on the vacant set of simple random walk obtained by Lawler , Schramm and Werner ( Annals of Probability 24 ( 3 ) , 1997 ) and Benjamini , Kalai and Schramm ( Journal of Functional Analysis 157 ( 1 ) , 2000 ) .The proof draws largely on current developments concerning the topology of Brownian movement and the notion of stable processes . In particular we using an estimate for the Green function of the dead Brownian movement owing to Bass and Burdzy ( Annales de l Institut Henri Poincare 22 ( 2 ) , 1999 ) .The main motivation behind our work comes from percolation theory where the empty set of random walks holds a crucial role . Indeed , we prove how our approaches can be used to obtain new information about the important dynamics of bond - percolation models on Zd .",
        "rewrite_text": "Title: Vacant Set of Random Interlacements and Percolation\n\nAbstract: In this study, we investigate the vacant set of random interlacements in \\( \\mathbb{R}^d \\) for dimensions \\( d > 1 \\). This vacant set is defined as the complement of the union of all open paths originating from the point 0 and extending up to time 1. Our primary result demonstrates that the Hausdorff dimension of this vacant set is almost surely equal to \\( d - 1 \\). We achieve this by establishing that the upper Minkowski dimension of the vacant set coincides with this value with probability one. This finding builds upon earlier work regarding the vacant set associated with simple random walks, as explored by Lawler, Schramm, and Werner in their 1997 paper in the Annals of Probability, as well as by Benjamini, Kalai, and Schramm in their 2000 publication in the Journal of Functional Analysis. \n\nThe proof leverages recent advancements in the topology of Brownian motion and the theory of stable processes. Notably, we utilize an estimate for the Green function of the dead Brownian motion, as established by Bass and Burdzy in their 1999 work published in the Annales de l'Institut Henri Poincaré. Our research is motivated by its implications in percolation theory, where the vacant set of random walks plays a pivotal role. We further demonstrate how our methodologies can yield new insights into the dynamics of bond-percolation models on \\( \\mathbb{Z}^d \\). This connection underscores the relevance of our findings in broader contexts within probability theory and statistical mechanics, particularly in understanding the intricate behaviors of random structures and their percolative properties.",
        "ori-fast-z-score": -1.4444444444444444,
        "water-fast-z-score": 4.111111111111111,
        "rewrite-fast-z-score": -1.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Internal states of model isotropic granular packings. II. Compression and pressure cycles .\nAbstract:\nWe study the internal state of an isotropic packing of frictionless spherical particles under compression, decompression and cyclic loading by means of numerical simulations using molecular dynamics (MD). We find that the volume fraction increases with increasing applied stress in all cases studied here. The increase of the volume fraction during unloading is smaller than for loading at similar stresses. This hysteresis effect becomes more pronounced as the number of load-unload cycles increases. In addition to this we observe that the distribution function of contact forces changes significantly between different stages of the process. These results are discussed within the framework of elastic-plastic models of granular materials. Granular matter can be found everywhere around us; it forms the basis of many natural phenomena such as avalanches or landslides on mountainsides  1  , mudflow  2  , sedimentation  3  , soil mechanics  4  , earthquakes  5  . It also plays an important role in industrial processes like powder metallurgy  6  , pharmaceutical industry  7  , food processing  8  , etc.. Despite its ubiquity there still exist open questions about how granular systems behave mechanically  9  .\nIn recent years much effort has been devoted to understanding the mechanical behavior of granular media  10  -  12  . One of the most interesting problems concerns the response of granular material to external loads  13  -  16  . For example, one may ask what happens if you compress a sample of sand? What will happen when you release the pressure again?\nThe aim of our work presented below was to investigate these issues numerically  17  . To do so we used Molecular Dynamics (MD)  18  which allows us to simulate large samples consisting of thousands of grains  19  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Internal states of model isotropic granular packings . II .Compression and force modes . Abstract : We explore the internal state of an isotropic packing of frictionless spherical objects under compression , decompression and cyclic loading by means of computational simulations using molecular dynamics ( MD ) .We see that the volume fraction increases with expanding applied strain in all cases examined here . The improvement of the volume fraction during unloading is smaller than for loading at comparable stresses .This hysteresis effect gets more pronounced as the number of load - unload cycles rises . In addition to this we find that the distribution relation of touch forces shifts significantly between various phases of the process .These data are discussed within the framework of elastic - plastic models of granular materials . Granular material can be found everywhere around us ; it becomes the framework of several physical phenomena such as avalanches or landslides on mountainsides 1 , mudflow 2 , sedimentation 3 , soil physics 4 , earthquakes 5 .It additionally acts an important role in industrial systems like powder metallurgy 6 , pharmaceutical manufacturing 7 , food manufacturing 8 , etc . . Despite its ubiquity there still arise open questions about how granular structures react mechanically 9 .In recent years much effort has been focused to discovering the structural response of granular media 10 - 12 . One of the most important problems involves the response of granular material to external loads 13 - 16 .For instance , one may question what happens if you compress a sample of dirt ? What will occur when you release the pressure again ?The goal of our work shown below was to examine these problems numerically 17 . To do so we using Molecular Dynamics ( MD ) 18 which allows us to simulate large specimens contained of thousands of grains 19 .",
        "rewrite_text": "**Title:** Internal States of Model Isotropic Granular Packings II: Compression and Force Modes\n\n**Abstract:** This study investigates the internal states of isotropic packings composed of frictionless spherical particles subjected to various loading conditions, including compression, decompression, and cyclic loading, through molecular dynamics (MD) simulations. Our findings reveal that the volume fraction of the granular assembly consistently increases with the application of strain across all scenarios analyzed. Notably, the volume fraction during the unloading phase is less than that observed during loading at equivalent stress levels, indicating a pronounced hysteresis effect that intensifies with an increasing number of load-unload cycles. Furthermore, we observe significant shifts in the distribution of contact forces throughout different phases of the loading process. These observations are contextualized within the framework of elastic-plastic models pertinent to granular materials. Granular materials are ubiquitous in nature and play a critical role in various physical phenomena, such as avalanches, landslides, mudflows, sedimentation, soil mechanics, and earthquakes. They are also integral to numerous industrial applications, including powder metallurgy, pharmaceutical production, and food processing. Despite their prevalence, fundamental questions regarding the mechanical behavior of granular structures remain unresolved. Recent research has increasingly focused on understanding the structural responses of granular media, particularly in relation to external loading conditions. Key inquiries include the mechanical response of granular materials when subjected to compression and the subsequent effects upon pressure release. The primary objective of this work is to numerically explore these phenomena using molecular dynamics simulations, enabling the analysis of large specimens comprising thousands of grains. Through this approach, we aim to enhance the understanding of the mechanical behavior of granular materials under varying loading conditions.",
        "ori-fast-z-score": -1.5787044347526527,
        "water-fast-z-score": 6.614950926316518,
        "rewrite-fast-z-score": 0.34299717028501764
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sensitivity of solar off-limb line profiles to electron density stratification and the velocity distribution anisotropy .\nAbstract:\nWe have investigated how different assumptions about the velocity distribution function (VDF) affect the shape of the observed line profile in the solar corona, using an analytical model for the VDF that includes both isotropic thermal motions and anisotropic nonthermal motions. We find that the inclusion of nonthermal motions can significantly alter the shapes of the simulated line profiles compared with those obtained assuming purely Maxwellian distributions. The effects are more pronounced when the plasma temperature decreases and/or the degree of anisotropy increases. \n \n In particular, we show that the presence of nonthermal motions leads to significant asymmetries between the red-and blueshifted wings of the line profiles. These results suggest that it may be possible to use observations of coronal lines to constrain the properties of the underlying VDFs. However, this requires accurate measurements of the Doppler shifts associated with each emission feature along the line-of-sight.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sensitivity of solar off - limb line profiles to electron velocity stratification and the velocity distribution anisotropy . Abstract : We have analyzed how various assumptions about the velocity distribution function ( VDF ) impact the form of the seen line profile in the solar corona , using an analytical theory for the VDF that contains both isotropic thermal motions and anisotropic nonthermal movements .We see that the introduction of nonthermal movements can significantly change the shapes of the simulated line profiles compared with those achieved assuming solely Maxwellian distributions . The effects are more pronounced when the plasma pressure reduces and / or the degree of anisotropy changes .In particular , we find that the presence of nonthermal movements leads to significant asymmetries between the red - and blueshifted wings of the line profiles . These conclusions propose that it could be possible to use observations of coronal lines to constrain the properties of the underlying VDFs .However , this demands accurate measurements of the Doppler variations associated with each emission feature along the line - of - view .",
        "rewrite_text": "In this study, we investigate the influence of different assumptions regarding the velocity distribution function (VDF) on the observed line profiles in the solar corona. Utilizing an analytical framework that incorporates both isotropic thermal motions and anisotropic nonthermal movements, we analyze how these factors alter the characteristics of the line profiles. Our findings indicate that the inclusion of nonthermal motions can lead to significant deviations in the simulated line profiles when compared to those derived from a purely Maxwellian distribution. Notably, these effects become more pronounced under conditions of reduced plasma pressure and varying degrees of anisotropy. We specifically observe that nonthermal movements introduce considerable asymmetries between the redshifted and blueshifted wings of the line profiles, suggesting that the dynamics of the solar corona are more complex than previously understood. These results imply that coronal line observations could serve as a valuable tool for constraining the properties of the underlying VDFs. However, to achieve this, precise measurements of the Doppler shifts associated with each emission feature along the line of sight are essential. This research enhances our understanding of the solar corona's velocity distribution and highlights the importance of considering both thermal and nonthermal components in future studies of solar emissions.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 5.531726674375733,
        "rewrite-fast-z-score": 1.1285761872936695
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Mathematica script for harmonic oscillator nuclear matrix elements arising in semileptonic electroweak interactions .\nAbstract:\nWe present an open-source, free software package that computes the one-loop Feynman diagrams contributing to the hadronic weak decay amplitudes of heavy quarks into light leptons and pions.  The code is written entirely within the Wolfram Language using only built-in functions.   It can be downloaded at https://github.com/mharrison/harmonic-oscillator-matrix-elements/tree/master/Mathematica_code. We have tested our results against those obtained with FeynArts 3.9  1  , FormCalc 8  2  , LoopTools 2  3  , and QCDLoop  4  . \nThe program has been used to compute the leading-order (LO) contributions to the helicity amplitudes for B meson decays into muons or electrons plus pions  5  .  These are required inputs for the calculation of the branching fractions  6  and CP asymmetries  7, 8  of these processes.  In addition, we provide the LO contribution to the pion form factor  9  .\nOur approach uses the method developed by M. Neubert  10   which exploits the fact that the relevant loop integrals can all be expressed as linear combinations of products of two-point scalar functions.  This allows us to reduce the number of independent integrations needed to evaluate each diagram to three.  Our implementation also includes several improvements over previous calculations  11, 12  :   1)  We use the exact expression for the propagators of massive fermions instead...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Mathematica text for harmonic oscillator atomic matrix elements arose in semileptonic electroweak interactions . Abstract : We produce an open - source , free software tool that computes the one - loop Feynman diagrams causing to the hadronic weak decay amplitudes of light quarks into light leptons and pions .The language is composed exclusively within the Wolfram Language using only built - in functions . It can be downloaded at https : / / github . com / mharrison / harmonic - oscillator - vector - elements / tree / master / Mathematica _ code .We have tested our findings against those achieved with FeynArts 3 . 9 1 , FormCalc 8 2 , LoopTools 2 3 , and QCDLoop 4 . The project has been used to compute the led - order ( LO ) contributions to the helicity amplitudes for B meson decays into muons or electrons plus pions 5 .These are required inputs for the determination of the branching fractions 6 and CP asymmetries 7 , 8 of these systems . In addition , we provide the LO contribution to the pion form parameter 9 .Our solution uses the method developed by M . Neubert 10 which exploits the fact that the relevant loop integrals can all be interpreted as continuous sequences of products of two - point scalar functions . This enables us to reduce the number of separate integrations needed to analyze each diagram to three .Our formulation also contains several additions over past calculations 11 , 12 : 1 ) We use the exact definition for the propagators of large fermions instead . . .",
        "rewrite_text": "**Title:** A Mathematica Tool for Computing Harmonic Oscillator Atomic Matrix Elements in Semileptonic Electroweak Interactions\n\n**Abstract:** In this study, we present an open-source software tool developed in the Wolfram Language, designed to compute one-loop Feynman diagrams that contribute to the hadronic weak decay amplitudes of light quarks into light leptons and pions. This tool is freely available for download at https://github.com/mharrison/harmonic-oscillator-vector-elements/tree/master/Mathematica_code. Our computations have been validated against established results obtained using FeynArts 3.9.1, FormCalc 8.2, LoopTools 2.3, and QCDLoop 4. The software has been specifically employed to calculate the leading-order (LO) contributions to the helicity amplitudes for B meson decays into muons or electrons accompanied by pions. These calculations are crucial for determining the branching fractions and CP asymmetries of these decay processes. Additionally, we provide the LO contribution to the pion form factor, which is essential for understanding the dynamics of these interactions.\n\nOur approach leverages a method developed by M. Neubert, which allows us to interpret the relevant loop integrals as continuous sequences of products of two-point scalar functions. This innovative technique significantly reduces the number of integrations required for each diagram to just three, streamlining the computational process. Furthermore, our formulation includes several enhancements over previous calculations, such as the use of the exact definitions for the propagators of heavy fermions. This advancement not only improves the accuracy of our results but also broadens the applicability of our tool to various scenarios in particle physics. Overall, our work contributes to the ongoing efforts to refine theoretical predictions in semileptonic decays and provides a valuable resource for researchers in the field.",
        "ori-fast-z-score": -2.3849888978799783,
        "water-fast-z-score": 4.538253483538691,
        "rewrite-fast-z-score": 0.29277002188455997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Second-order perturbations of cosmological fluids: Relativistic effects of pressure, multi-component, curvature, and rotation .\nAbstract:\nWe present the relativistic second order perturbation theory for fluids in curved space-time with arbitrary number of components. We derive the general expression for the energy-momentum tensor at first order in perturbations as well as its trace-free part which is responsible for gravitational waves generation. The evolution equations are derived by projecting the conservation law onto the background 4-velocity vector field. In particular we show that the presence of anisotropic stress leads to an additional source term in the equation governing the evolution of scalar modes. Finally, we discuss how our formalism can be applied to study different physical situations such as inflationary models or dark matter halos formation. Cosmology has been revolutionized over the past decade thanks to precision measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation  1  . These observations have provided us with detailed information about the early universe and allowed to test fundamental physics on very large scales  2  .\nThe standard model of cosmology assumes that the universe consists of several interacting components including cold dark matter (CDM), baryons, photons, neutrinos etc.. Each component evolves according to some set of hydrodynamical equations describing their dynamics  3  . However, these equations cannot be solved analytically even if one neglects all interactions between particles  4  , so numerical simulations are required  5  . On the other hand, analytical solutions exist only under certain approximations  6  . For example, it was shown recently  7, 8  that the effect of pressure gradients may lead to significant corrections to the growth rate of density perturbations during the late stages of structure formation  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Second - order perturbations of cosmological fluids : Relativistic effects of stress , multi - component , curvature , and rotation . Abstract : We introduce the relativistic second order perturbation theory for fluids in curved space - time with arbitrary number of components .We derive the general expression for the power - momentum tensor at first order in perturbations as also as its trace - free portion which is responsible for gravitational waves development . The evolution coefficients are derived by projecting the conservation law onto the background 4 - velocity tensor field .In particular we prove that the presence of anisotropic pressure leads to an additional source term in the equation regulating the evolution of scalar cycles . Finally , we explain how our formalism can be applied to study various mechanical circumstances such as inflationary theories or black matter halos formation .Cosmology has been revolutionized over the previous decade courtesy to accurate measurements of temperature fluctuations in the cosmic microwave background ( CMB ) radiation 1 . These measurements have provided us with comprehensive information about the early universe and helped to test fundamental theory on very huge scales 2 .The conventional model of cosmology assumes that the universe consists of several interacting components namely cold bright matter ( CDM ) , baryons , photons , neutrinos etc . . Each component evolves due to some setting of hydrodynamical equations explaining their mechanics 3 . However , these equations never be answered analytically especially if one neglects all interactions between particles 4 , so computational simulations are required 5 .On the other hand , analytical solutions arise only under certain approximations 6 . For instance , it was shown recently 7 , 8 that the impact of force gradients might lead to significant corrections to the development time of density perturbations during the last phases of structure formation 9 .",
        "rewrite_text": "We present a comprehensive exploration of relativistic second-order perturbation theory for cosmological fluids in curved spacetime, accommodating an arbitrary number of fluid components. Our study begins with the derivation of a general expression for the energy-momentum tensor at first order in perturbations, including its trace-free part, which plays a crucial role in the generation of gravitational waves. By projecting the conservation law onto the background four-velocity tensor field, we obtain the evolution coefficients essential for understanding the dynamics of these systems. Notably, we demonstrate that the presence of anisotropic pressure introduces an additional source term in the equations governing the evolution of scalar perturbations. This finding has significant implications for the study of various astrophysical scenarios, including inflationary models and the formation of dark matter halos.\n\nThe field of cosmology has undergone a transformative shift in the past decade, largely due to precise measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation. These observations have yielded invaluable insights into the early universe and have provided a robust framework for testing fundamental theories on cosmological scales. The standard cosmological model posits that the universe is composed of several interacting components, such as cold dark matter (CDM), baryons, photons, and neutrinos. Each of these components evolves according to a set of hydrodynamical equations that describe their dynamics. However, these equations are often intractable analytically, particularly when interactions between particles are neglected, necessitating the use of computational simulations for their resolution. Analytical solutions are typically achievable only under specific approximations. Recent studies have indicated that the influence of force gradients can lead to substantial corrections in the growth rates of density perturbations during the late stages of structure formation. Our work aims to bridge the gap between analytical and numerical approaches, providing a framework that can enhance our understanding of the complex interplay between different components in the universe's evolution.",
        "ori-fast-z-score": -2.0,
        "water-fast-z-score": 6.166666666666667,
        "rewrite-fast-z-score": 0.7324096128940435
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The widths of quarkonia in quark gluon plasma .\nAbstract:\nWe study the transverse momentum dependence of J/ψ and Υ production cross sections at RHIC energies within an effective field theory approach, which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons.  We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV/c while additional contributions are needed to describe data points with larger values of pT . The latter turn out to be dominated by inelastic processes like dissociation into open heavy flavor mesons. In particular we show that the inclusion of these effects leads to a significant reduction of the predicted nuclear modification factor RAA(pT ) compared to previous calculations based on purely elastic interactions. \nPACS numbers: 12.38.Mh, 25.75.-q, 11.10.Kk \nI. INTRODUCTORY REMAR K\nThe measurement of charmonium (J/ψ) and bottomonium (Υ) production is one of the most promising probes to investigate properties of hot and dense matter created in relativistic nucleus-nucleus collisions  1  . It has been suggested that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting  2  , i.e., to a decrease of the bound state masses due to color screening  3  .\nIn this work we present results obtained within an effective field theory framework  4  , where the relevant degrees of freedom are quarks and gluons rather than individual hadronic states. This allows us to calculate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks q = u, d, s and gluons g. These include elastic scattering off quarks and gluon-gluon fusion leading to the formation of quarkonia via the creation of a virtual qq pair  5  . Furthermore, inelastic processes such as quarkonium dissociation into open heavy-flavor hadrons  6  have also been included  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The widths of quarkonia in quark gluon plasma . Abstract : We study the transverse momentum dependence of J / ψ and [UNK] production cross sections at RHIC energies within an effective field theory approach , which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons .We see that the observed suppression behavior can be reproduced by including only elastic scattering for pT < 2 GeV / c while extra contributions are needed to define data points with larger values of pT . The latter come out to be dominated by inelastic reactions like dissociation into open heavy flavor mesons .In particular we prove that the introduction of these influences result to a substantial decreased of the expected nuclear modification factor RAA ( pT ) compared to previous analyses based on purely elastic interactions . PACS scores : 12 . 38 . Mh , 25 . 75 . - q , 11 . 10 . Kk I .INTRODUCTORY REMAR K The measurement of charmonium ( J / ψ ) and bottomonium ( [UNK] ) production is one of the most attractive probes to investigate properties of hot and dense matter created in relativistic nucleus - nucleus collisions 1 . It has been speculated that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting 2 , i . e . , to a decrease of the bound state masses due to color screening 3 .In this study we present results derived within an efficient field theory framework 4 , where the appropriate degrees of liberty are quarks and gluons instead than individual hadronic states . This enables us to estimate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks g = u , d , s and gluons g . These include elastic scattering off quarks and gluon - gluon fusion led to the formation of quarkonia via the creation of a virtual qq pair 5 .Furthermore , inelastic reactions such as quarkonium dissociation into open heavy - flavor hadrons 6 have already been used 7 , 8 .",
        "rewrite_text": "Title: The Widths of Quarkonia in Quark-Gluon Plasma\n\nAbstract: In this study, we investigate the transverse momentum dependence of J/ψ and bottomonium production cross sections at RHIC energies through an effective field theory framework. Our approach incorporates both elastic scattering processes involving quarks and inelastic interactions, such as the dissociation of quarkonia into open charm or bottom hadrons. We find that the observed suppression of quarkonia production can be accurately described by considering only elastic scattering for transverse momentum (pT) values below 2 GeV/c. However, for higher pT values, additional contributions are necessary to align with experimental data, which are predominantly attributed to inelastic processes, particularly the dissociation into open heavy flavor mesons. Our analysis demonstrates that the inclusion of these inelastic contributions leads to a significant reduction in the expected nuclear modification factor RAA(pT) compared to earlier studies that focused solely on elastic interactions. \n\nThe production of charmonium (J/ψ) and bottomonium (bottomonium) serves as a crucial probe for understanding the properties of the hot and dense matter generated in relativistic nucleus-nucleus collisions. It has been suggested that the interactions between the produced quarkonia and the surrounding quark-gluon medium may result in partial melting of these states, characterized by a decrease in bound state masses due to color screening effects. In this work, we present results derived from an effective field theory approach, where the relevant degrees of freedom are treated as quarks and gluons rather than individual hadronic states. This framework allows us to estimate the total cross section for quarkonium production based on elementary partonic subprocesses involving light quarks (u, d, s) and gluons. These subprocesses encompass elastic scattering off quarks and gluon-gluon fusion, which leads to the formation of quarkonia through the creation of virtual quark-antiquark pairs. Additionally, we discuss the role of inelastic reactions, such as quarkonium dissociation into open heavy-flavor hadrons, which have been previously explored in the literature. \n\nPACS numbers: 12.38.Mh, 25.75.-q, 11.10.Kk",
        "ori-fast-z-score": -0.5035088149780135,
        "water-fast-z-score": 4.166666666666667,
        "rewrite-fast-z-score": 0.7474093186836597
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamical models with a general anisotropy profile .\nAbstract:\nWe present dynamical models for the Milky Way halo that include an arbitrary number of spherical shells, each characterized by its own density and velocity dispersion profiles. The model is based on Jeans  theorem applied to the phase-space distribution function (DF) in order to obtain the DF as well as the projected surface mass density along any line-of-sight. We show how this approach can be used to fit observational data such as those obtained from kinematic tracers or gravitational lensing measurements. In particular we apply our method to two different sets of observations: 1) A sample of RR Lyrae stars observed towards the Galactic bulge; 2) Gravitational lensing measurements towards the Bullet Cluster. Our results are compared against previous works using similar techniques but assuming either constant anisotropies across all radii or simple power-law radial dependences. We find that these assumptions lead to significant biases when fitting the data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamical models with a general anisotropy profile . Abstract : We create dynamical models for the Milky Way halo that include an arbitrary number of spherical shells , each described by its own density and speed dispersion profiles .The model is based on Jeans theorem applied to the phase - space distribution relation ( DF ) in order to obtain the DF as well as the projected surface mass density along any line - of - view . We see how this methodology can be used to fitting observational data such as those acquired from kinematic tracers or gravitational lensing observations .In particular we apply our technique to two different sets of measurements : 1 ) A specimen of RR Lyrae stars observed towards the Galactic bulge ; 2 ) Gravitational lensing observations towards the Bullet Cluster . Our results are compared against prior studies use similar method but assuming either constant anisotropies across all radii or straightforward power - law angular dependences .We see that these assumptions lead to significant biases when fitting the information .",
        "rewrite_text": "In this article, we present a comprehensive framework for constructing dynamical models of the Milky Way halo that incorporate a flexible anisotropy profile. Our approach allows for the inclusion of multiple spherical shells, each characterized by distinct density and velocity dispersion profiles. By applying Jeans theorem to the phase-space distribution function (DF), we derive the DF and the projected surface mass density along any specified line of sight. This innovative methodology is particularly valuable for analyzing observational data, including kinematic tracers and gravitational lensing measurements. We demonstrate the effectiveness of our technique by applying it to two specific datasets: first, a sample of RR Lyrae stars observed in the direction of the Galactic bulge, and second, gravitational lensing observations related to the Bullet Cluster. Our findings are juxtaposed with previous studies that employed similar methodologies but operated under the assumption of constant anisotropies across all radii or utilized simple power-law angular dependencies. We highlight that these conventional assumptions can introduce substantial biases in the fitting process, ultimately affecting the accuracy of the derived parameters. Through our analysis, we illustrate the advantages of our generalized anisotropic model, which provides a more nuanced understanding of the dynamical structure of the Milky Way halo and enhances the interpretation of observational data. This work not only contributes to the field of galactic dynamics but also sets the stage for future investigations that may further refine our understanding of the Milky Way's gravitational landscape.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 4.358898943540673,
        "rewrite-fast-z-score": 0.29277002188455997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New scaling relations in cluster radio halos and the re-acceleration model .\nAbstract:\nWe present new results on the properties of diffuse radio emission associated with galaxy clusters, based on observations carried out at 1.4 GHz using the Giant Metrewave Radio Telescope (GMRT). We have studied the correlation between the total power of the diffuse radio emission and other physical parameters such as X-ray luminosity, temperature and mass for a sample of 25 galaxy clusters observed by GMRT. The main result is that we find evidence for two different populations of radio halo sources which are separated by their radio powers. In particular, our analysis shows that there exists an upper limit to the radio power of radio halos hosted by massive galaxy clusters. This suggests that the energy density of relativistic particles responsible for the synchrotron emission decreases rapidly beyond this threshold value. Our findings also suggest that the magnetic field strength increases with increasing radio power. These results can be explained within the framework of the re-acceleration model where secondary electrons produced through hadronic interactions are reaccelerated by turbulence generated during mergers or accretion events.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : New scaling relations in cluster radio halos and the re - acceleration method . Abstract : We report new data on the properties of diffuse radio emission associated with galaxy galaxies , based on observations carried out at 1 . 4 GHz using the Giant Metrewave Radio Telescope ( GMRT ) .We have researched the relationship between the total power of the diffuse radio emission and other physical factors such as X - ray luminosity , temperature and mass for a sample of 25 galaxy galaxies reported by GMRT . The main consequence is that we find proof for two different populations of radio halo sources which are distinct by their radio powers .In particular , our analysis shows that there exists an upper maximum to the radio power of radio halos held by massive galaxy clusters . This implies that the power concentration of relativistic objects responsible for the synchrotron emission decreases quickly beyond this threshold value .Our findings also suggest that the magnetic force power grows with rising radio power . These data can be described within the framework of the re - acceleration hypothesis where secondary particles generated through hadronic interactions are reaccelerated by turbulence generated during mergers or accretion events .",
        "rewrite_text": "We present new findings on the characteristics of diffuse radio emissions linked to galaxy clusters, derived from observations conducted at 1.4 GHz with the Giant Metrewave Radio Telescope (GMRT). Our study investigates the correlation between the total power of diffuse radio emissions and various physical parameters, including X-ray luminosity, temperature, and mass, utilizing a sample of 25 galaxy clusters observed by GMRT. A significant outcome of our research is the identification of two distinct populations of radio halo sources, differentiated by their radio power levels. Notably, our analysis reveals the existence of an upper limit to the radio power of halos associated with massive galaxy clusters. This observation indicates that the concentration of relativistic particles responsible for synchrotron radiation diminishes rapidly once this threshold is surpassed. Furthermore, our results imply a relationship where the magnetic field strength increases alongside the radio power. These observations can be interpreted within the framework of the re-acceleration model, which posits that secondary particles produced through hadronic interactions are re-energized by turbulence arising from merger events or accretion processes. This study enhances our understanding of the mechanisms underlying radio halo emissions and their dependence on cluster properties, contributing valuable insights to the field of astrophysics.",
        "ori-fast-z-score": 0.7181848464596079,
        "water-fast-z-score": 6.327848502189878,
        "rewrite-fast-z-score": -1.507556722888818
    }
]