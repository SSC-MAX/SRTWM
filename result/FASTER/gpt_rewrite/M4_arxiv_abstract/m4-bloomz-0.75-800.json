[
    {
        "original_text": "We study vacuum energy in quantum field theory with boundary conditions that break conformal invariance, such as Dirichlet or Neumann boundary conditions. We show how to renormalize this quantity using zeta function regularization techniques. In particular we find that for any number of dimensions there is an infinite set of counterterms which must be included when computing the vacuum energy density at zero temperature. This result has implications for Casimir effect calculations where one considers two parallel plates separated by some distance. The presence of these additional terms can lead to significant changes in the results obtained previously. Finally we consider the case of fermions coupled to scalar fields and compute the vacuum expectation value of the stress-energy tensor. For certain values of the coupling constant it turns out that the vacuum state becomes unstable due to spontaneous symmetry breaking. Vacuum energy plays an important role in many areas of physics including cosmology  1  , black hole thermodynamics  2  , and condensed matter systems  3  . It also appears in various contexts within string theory  4  .\nIn recent years much progress has been made towards understanding the nature of vacuum fluctuations in quantum field theories (QFTs)  5  -  8  . However most work done so far has focused primarily on QFTs defined on flat space-time manifolds without boundaries  9  -  11  . Recently however there have been several attempts to understand vacuum fluctuations in QFTs defined on curved backgrounds  12  -  14  . Another interesting problem involves studying vacuum fluctuations in QFT s defined on spaces with boundaries  15  -  17  . Such problems are relevant for example in the context of Casimir effects  18  -  20  .",
        "watermark_text": "We research vacuum energy in quantum field theory with boundary constraints that broke conformal invariance , such as Dirichlet or Neumann border conditions . We see how to renormalize this quantity use zeta function regularization techniques .In particular we find that for any number of dimensions there is an endless system of counterterms which requires be included when computing the vacuum energy density at zero temperature . This result has implications for Casimir effect calculations where one considers two adjacent plates connected by some distance .The presence of these additional terms can lead to significant changes in the results derived earlier . Finally we study the case of fermions connected to scalar fields and compute the vacuum expectation value of the strain - energy tensor .For specific values of the interaction factor it turns out that the vacuum state grows unstable due to spontaneous symmetry breaking . Vacuum energy serves an important role in multiple fields of science specifically cosmology 1 , white hole thermodynamics 2 , and condensed matter structures 3 .It additionally occurs in different settings within string theory 4 . In recent years much work has been achieved towards studying the nature of vacuum fluctuations in quantum field theories ( QFTs ) 5 - 8 .However most work done so far has concentrated mostly on QFTs defined on straight space - time manifolds without boundaries 9 - 11 . Recently however there have been numerous attempts to explain vacuum fluctuations in QFTs defined on curved backgrounds 12 - 14 .Another important task means analyzing vacuum fluctuations in QFT s characterized on spaces with borders 15 - 17 . Such issues are applicable for example in the context of Casimir interactions 18 - 20 .",
        "rewrite_text": "We investigate vacuum energy in quantum field theory under boundary conditions that break conformal invariance, such as Dirichlet and Neumann conditions. Our approach involves renormalizing this quantity using zeta function regularization techniques. Notably, we discover that in any dimensional setting, there exists an infinite series of counterterms that must be included when calculating the vacuum energy density at zero temperature. This finding has significant implications for Casimir effect calculations, particularly in scenarios involving two parallel plates separated by a distance. The introduction of these additional terms can lead to substantial modifications in previously obtained results. Additionally, we examine the scenario where fermions are coupled to scalar fields, computing the vacuum expectation value of the strain-energy tensor. We find that for certain values of the interaction parameter, the vacuum state may become unstable due to spontaneous symmetry breaking. Vacuum energy plays a crucial role in various scientific domains, including cosmology, white hole thermodynamics, and condensed matter physics. It also arises in various contexts within string theory. In recent years, substantial progress has been made in understanding the nature of vacuum fluctuations in quantum field theories (QFTs). However, much of this work has primarily focused on QFTs defined on flat spacetime manifolds without boundaries. Recently, there has been an increasing number of efforts aimed at understanding vacuum fluctuations in QFTs set in curved backgrounds. Another important area of study involves analyzing vacuum fluctuations in QFTs defined on spaces with boundaries, which has direct relevance to phenomena such as Casimir interactions.",
        "ori-fast-z-score": -1.0366421106976322,
        "water-fast-z-score": 7.0837210897671525,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "We present the results of an optical spectroscopic survey of radio galaxies in clusters at redshifts z=0.4-0.8, carried out with the William Herschel Telescope (WHT). We find that the fraction of AGN-powered radio galaxies decreases towards higher galaxy density environments within the clusters. This is consistent with previous studies which have found evidence for environmental quenching of star formation activity among massive galaxies. However we also find that there are many examples where powerful radio sources reside in dense regions without any obvious signs of being environmentally suppressed. These objects may be undergoing rapid evolution or they could represent a population of recently accreted field galaxies whose properties are still evolving to resemble those of their local counterparts. The sample consists of 20 radio galaxies selected from the VLA-COSMOS 3 GHz Large Project catalogue (Smolčić et al., 2009) using the following criteria: 1) They lie in one of four X-ray luminous clusters at 0.4 < z < 0.8; 2) Their radio luminosity lies above L(3GHz) = 10 25 W Hz-1; 3) They do not show strong emission lines indicative of ongoing nuclear activity; 4) They were observed during our WHT run on 2010 May 24-25.",
        "watermark_text": "We present the conclusion of an optical spectroscopic study of radio stars in clusters at redshifts z = 0 . 4 - 0 . 8 , conducted out with the William Herschel Telescope ( WHT ) . We see that the fraction of AGN - powered radio stars reduces towards higher galaxy concentration environments within the clusters .This is consistent with previous research which have shown evidence for environmental quenching of galaxy formation activity among huge objects . However we also find that there are many instance where powerful radio sources exist in dense areas without any obvious signs of being environmentally suppressed .These bodies may be experiencing rapid evolution or they may contain a population of newly accreted field galaxies whose characteristics are still changing to reflect those of their nearby rivals . The sample consists of 20 radio objects chose from the VLA - COSMOS 3 GHz Large Project catalogue ( Smolčić et al . , 2009 ) applying the following criteria : 1 ) They sit in one of four X - ray luminous clusters at 0 . 4 < z < 0 . 8 ; 2 ) Their radio luminosity sits above L ( 3GHz ) = 10 25 W Hz - 1 ; 3 ) They do not show weak absorption tracks indicative of ongoing nuclear activity ; 4 ) They were detected during our WHT running on 2010 May 24 - 25 .",
        "rewrite_text": "We present the findings from an optical spectroscopic study of radio stars in clusters at redshifts z = 0.4 to 0.8, conducted using the William Herschel Telescope (WHT). Our analysis reveals that the proportion of AGN-powered radio stars decreases in environments with higher galaxy concentration within the clusters. This observation is in line with previous studies that have indicated environmental quenching of galaxy formation activity in large structures. However, we also observe numerous cases where powerful radio sources are present in densely populated regions without obvious signs of environmental suppression. These sources may be undergoing rapid evolution or may host a population of newly accreted field galaxies that are still adapting to reflect the characteristics of their nearby counterparts. The sample includes 20 radio objects selected from the VLA-COSMOS 3 GHz Large Project catalogue (Smolčić et al., 2009), based on the following criteria: 1) they reside within one of four X-ray luminous clusters at 0.4 < z < 0.8; 2) their radio luminosity exceeds L (3 GHz) = 10^25 W Hz^-1; 3) they do not exhibit weak absorption features indicative of ongoing nuclear activity; 4) they were detected during our WHT observations on May 24-25, 2010.",
        "ori-fast-z-score": -1.1793237883215741,
        "water-fast-z-score": 6.685631938613182,
        "rewrite-fast-z-score": 2.4379951240146283
    },
    {
        "original_text": "We propose to use thermal noise as the information carrier for totally secure communications, zero power consumption devices, and computing systems. We show that by using an appropriate detection scheme, we can extract digital bits from the thermal noise with high fidelity at room temperature. The proposed system is based on a single-electron transistor (SET) which has been demonstrated in experiments recently. In addition, we demonstrate how this SET-based device can be used to perform logic operations such as AND gate and NOT gate. Finally, we discuss possible applications of our proposal including quantum key distribution, ultra-low power sensor networks, and energy efficient data centers. This work was supported by NSF under Grant No. ECCS-0926491. A new paradigm called  thermal noise informatics  is introduced where thermal noise is utilized as the information carrier for various tasks ranging from totally secure communication over wires to zero-power communication and computation. By employing an appropriate detection scheme, it is shown that one can extract digital bits from thermal noise with high fidelities even at room temperatures. It is also shown that these extracted digital bits can be used to perform logical operations such as AND gates and NOT gates. Possible applications are discussed along with their advantages compared to existing technologies.",
        "watermark_text": "We suggest to use thermal noise as the information carrier for totally secure services , zero power consumption devices , and computing systems . We see that by using an appropriate detection system , we can extract digital bits from the thermal noise with high fidelity at room temperature .The proposed system is based on a single - ion transistor ( SET ) which has been shown in experiments recently . In addition , we prove how this SET - based device can be used to conduct logic events such as AND gate and NOT gate .Finally , we review possible users of our proposal including molecular key transmission , ultra - low power detector networks , and energy effective data systems . This project was supported by NSF under Grant No .ECCS - 0926491 . A different technology called temperature noise informatics is adopted where thermal noise is utilized as the information carrier for various jobs ranging from completely secure transmission over wires to zero - energy transmission and computation .By using an appropriate detection system , it is demonstrated that one can extract digital bits from temperature noise with high fidelities even at room temperatures . It is also shown that these obtained digital frames can be used to conduct logical functions such as AND doors and NOT gates .Possible products are discussed along with their benefits compared to existing technologies .",
        "rewrite_text": "We propose utilizing thermal noise as the information carrier for completely secure services, devices with zero power consumption, and computing systems. With the aid of a suitable detection system, we can extract digital bits from thermal noise with high fidelity at room temperature. Our proposed system is based on a single-electron transistor (SET), which has been validated through recent experiments. Moreover, we demonstrate how this SET-based device can perform logical operations, such as AND and NOT gates. Finally, we explore potential applications for our proposal, including molecular key transmission, ultra-low power detection networks, and energy-efficient data systems. This research is supported by the NSF under Grant No. ECCS-0926491. Additionally, we discuss a different approach known as temperature noise informatics, where thermal noise serves as the information carrier across various tasks, from completely secure transmission over wires to zero-energy transmission and computation. Our findings show that digital bits can be extracted from temperature noise with high fidelity, even at room temperatures, and these digital frames can facilitate logical functions like AND and NOT gates. Possible applications and their advantages over existing technologies are also reviewed.",
        "ori-fast-z-score": 1.3987572123604708,
        "water-fast-z-score": 8.541985556144386,
        "rewrite-fast-z-score": 3.333974297349129
    },
    {
        "original_text": "We present new constructions for locally decodable codes (LDCs) based on nice subsets of finite fields, prime factors of Mersenne numbers, and the Chinese remainder theorem. Our first construction is an explicit family of LDCs with optimal query complexity that are derived from nice subsets of finite fields. We then show how to use these results in conjunction with the Chinese remainder theorem to obtain families of LDCs whose parameters can be tuned by varying the number of primes used in their construction. Finally we give a lower bound on the length of any LDC over a binary alphabet which uses only one-sided queries. The proofs of our main theorems appear at the end of this extended abstract. \nIntroduction\n\nLocally decodable codes (abbreviated as LCDs)\nare error correcting codes where each codeword has associated with it some decoding algorithm that allows efficient recovery of individual bits or symbols when queried about them. In particular, if the codeword is corrupted by up to t errors, then there exists a deterministic polynomial time algorithm that recovers the original uncorrupted word using O(t) queries per symbol. This property makes such codes useful in applications like data storage systems, peer-to-peer networks, and streaming media distribution services. \n \n A large body of work has been devoted towards understanding the trade-off between the rate of the code, its distance, and the query complexity of the decoder. For example, Guruswami et al. (2005), Chan et al. (2006) , and Kopparty & Saraf (2007)  studied the problem of constructing codes with low query complexity while maintaining high rates and distances. Recently, several works have focused on improving the known bounds on the minimum possible query complexity required to decode a single bit given a certain amount of corruption. These include the recent breakthrough result of Dvir et al. (2010a) (which improved upon the previous best-known upper bound due to Sudan et al., 2005 ) and the subsequent improvements made by Dvir et al. (2011 ), Feldman et al. (2012 , and Kopparty et al. (2013) . \n \n Another important",
        "watermark_text": "We introduce novel constructions for locally decodable codes ( LDCs ) based on pleasant subsets of finite fields , prime elements of Mersenne numbers , and the Chinese remainder theorem . Our first build is an explicit class of LDCs with optimal query complexity that are derived from good subsets of finite fields .We then show how to use these results in partnership with the Chinese remainder theorem to obtain families of LDCs whose parameters can be tuned by varying the number of primes used in their design . Finally we give a smaller bound on the length of any LDC over a binary alphabet which uses only one - sided queries .The proofs of our major theorems emerge at the end of this enlarged abstract . Introduction Locally decodable codes ( denoted as LCDs ) are mistake correcting codes where each codeword has associated with it some decoding algorithm that enables efficient returning of individual characters or characters when queried about them .In particular , if the codeword is corrupted by up to t errors , then there exists a deterministic polynomial period algorithm that recovers the actual uncorrupted phrase using O ( t ) queries per symbol . This property gives such codes helpful in applications like data storage systems , peer - to - peer systems , and streaming media distribution services .A vast body of research has been focused towards studying the trade - off between the frequency of the code , its location , and the query complexity of the decoder . For instance , Guruswami et al .( 2005 ) , Chan et al . ( 2006 ) , and Kopparty & Saraf ( 2007 ) studied the question of constructing coding with little query complexity while maintaining high frequencies and distances .Recently , various efforts have concentrated on improving the known bounds on the minimum possible query complexity required to decode a single bit given a certain quantity of corruption . These include the recent breakthrough result of Dvir et al .( 2010a ) ( which updated upon the previous good - used upper bound due to Sudan et al . , 2005 ) and the subsequent improvements done by Dvir et al . ( 2011 ) , Feldman et al .( 2012 , and Kopparty et al . ( 2013 ) .Another important",
        "rewrite_text": "We present innovative constructions for locally decodable codes (LDCs) utilizing pleasant subsets of finite fields, prime elements of Mersenne numbers, and the Chinese remainder theorem. Our initial construction yields an explicit class of LDCs with optimal query complexity, derived from well-structured subsets of finite fields. We then demonstrate how these findings, in conjunction with the Chinese remainder theorem, can be used to create families of LDCs with adjustable parameters based on the number of primes incorporated into their design. Additionally, we establish a tighter bound on the length of any LDC defined over a binary alphabet that relies on one-sided queries. The proofs of our primary theorems can be found at the conclusion of this expanded abstract. \n\nIntroduction: Locally decodable codes (LDCs) are error-correcting codes designed such that each codeword is linked to a decoding algorithm, allowing for the efficient retrieval of individual characters upon request. Specifically, in scenarios where a codeword is affected by up to t errors, a deterministic polynomial-time algorithm can restore the original uncorrupted message, requiring O(t) queries per symbol. This capability makes LDCs valuable for applications including data storage systems, peer-to-peer networks, and media streaming services. Extensive research has focused on the trade-offs between code frequency, location, and decoder query complexity. Notable studies by Guruswami et al. (2005), Chan et al. (2006), and Kopparty & Saraf (2007) have explored how to construct codes with minimal query complexity while maintaining high frequency and distance attributes. Recently, efforts have intensified to refine the known limits on the minimum query complexity necessary for decoding a single bit amid a specified level of corruption. This includes the recent breakthrough by Dvir et al. (2010a), which surpassed earlier bounds established by Sudan et al. (2005), alongside further enhancements by Dvir et al. (2011), Feldman et al. (2012), and Kopparty et al. (2013). Another crucial area of research...",
        "ori-fast-z-score": -0.40689422938557973,
        "water-fast-z-score": 7.021870595978444,
        "rewrite-fast-z-score": 0.8333333333333334
    },
    {
        "original_text": "We present an analysis of the relativistic Riemann problem for ideal fluids in two space dimensions, with emphasis on the role played by vortex sheets. We show that the solution to this problem can be constructed as a sequence of self-similar solutions which are determined uniquely up to translations along the x-axis (the direction of propagation). The first step is to construct a family of exact solutions describing the interaction between a planar shock wave and a vortex sheet. These solutions have been obtained previously using different methods but we provide here a new derivation based on the method of characteristics. In particular, we obtain explicit expressions for the density and pressure profiles across the shock front. Next, we consider the case where the initial data consists of a single vortex sheet separating regions of constant density and pressure. This situation corresponds physically to a fluid initially at rest being accelerated impulsively by a piston moving at speed c = 1. We prove that there exists exactly one such solution corresponding to each value of the total mass M . Finally, we study numerically the evolution of more general initial data consisting of several vortex sheets separated by shocks.",
        "watermark_text": "We present an assessment of the relativistic Riemann problem for perfect fluids in two space dimensions , with emphasis on the part played by vortex sheets . We see that the solve to this question can be synthesized as a sequence of self - similar solutions which are decided uniquely up to translations along the x - axis ( the direction of propagation ) .The first step is to build a family of precise solutions governing the interaction between a planar blast flow and a vortex sheet . These solutions have been constructed previously using separate methods but we provide here a new derivation based on the method of characteristics .In particular , we obtain precise expressions for the density and tension characteristics across the shock front . Next , we find the case where the first data contains of a single vortex sheet separating areas of constant density and pressure .This problem corresponds physically to a fluid initially at rest being advanced impulsively by a cylinder moved at speed c = 1 . We prove that there exists precisely one such solution corresponding to each value of the total mass M .Finally , we study numerically the evolution of more general initial evidence consisting of several vortex strands divided by shocks .",
        "rewrite_text": "We provide an analysis of the relativistic Riemann problem for perfect fluids in two spatial dimensions, focusing on the role of vortex sheets. Our findings indicate that the solution can be expressed as a series of self-similar solutions, distinguished uniquely by translations along the x-axis, which is the direction of propagation. The initial step involves constructing a family of exact solutions that describe the interaction between a planar blast flow and a vortex sheet. Although these solutions have been derived through various methods previously, we offer a new derivation utilizing the method of characteristics. In particular, we derive precise expressions for the density and tension characteristics at the shock front. We then consider the scenario where the initial data consists of a single vortex sheet that separates regions of constant density and pressure. This scenario physically represents a fluid at rest being impulsively set in motion by a cylinder moving at speed \\(c = 1\\). We demonstrate that there is exactly one solution corresponding to each total mass \\(M\\). Finally, we conduct a numerical investigation into the evolution of more complex initial data that includes multiple vortex strands separated by shock waves.",
        "ori-fast-z-score": -0.7107423155935334,
        "water-fast-z-score": 5.858884758402822,
        "rewrite-fast-z-score": 0.10153461651336192
    },
    {
        "original_text": "We propose a new string derived model with stable proton in which the lightest supersymmetric particle (LSP) is not neutralino but gravitino. The LSP decays into photon or neutrino-antineutrino pair through gravitational interaction. In this scenario we can explain the observed dark matter abundance without conflicting with other experimental results such as relic density measurement by WMAP experiment. We also show that our model predicts interesting signatures at LHC experiments. Introduction:-The discovery of Higgs boson  1-3  has opened up an exciting possibility to explore physics beyond Standard Model(SM). Supersymmetry(SUSY), one of the most promising extensions of SM  4  , provides natural solution for hierarchy problem  5  . However, SUSY models are severely constrained by various experimental observations  6  .\nIn order to solve these problems, several authors have proposed different mechanisms  7-9 . One of them is introducing additional gauge symmetries  10  . Another way is adding extra dimensions  11  . Recently, it was shown that there exists a class of string derived models where the lightest superpartner is gravitino  12  . Gravitino is weakly interacting massive particle so its decay rate is suppressed compared to neutralino case  13  . This feature makes gravitino a good candidate for cold dark matter  14  . Moreover, if gravitino mass m 3/2 < 1 GeV then its lifetime becomes longer than age of universe  15  . Therefore, gravitino may be regarded as a viable candidate for dark matter  16  . On the other hand, gravitino is unstable because it couples to gravity  17  . It decays into photon or lepton-neutrino pairs  18  . If gravitino is heavier than 100 MeV then its decay products will contribute to diffuse gamma ray background  19  . Thus, gravitino should satisfy following conditions  20  :",
        "watermark_text": "We suggest a new string derived model with stable proton in which the lightest supersymmetric object ( LSP ) is not neutralino but gravitino . The LSP decays into photon or neutrino - antineutrino bond through gravity interaction .In this situation we can describe the seen dark matter density without conflicting with other experimental outcome such as relic volume observation by WMAP study . We additionally prove that our model predicts exciting signatures at LHC experiments .Introduction : - The observation of Higgs boson 1 - 3 has opened up an exciting possibility to study physics beyond Standard Model ( SM ) . Supersymmetry ( SUSY ) , one of the most attractive extensions of SM 4 , offers natural solution for hierarchy problem 5 .However , SUSY models are severely constrained by various experimental studies 6 . In try to solve these problems , various scientists have proposed different processes 7 - 9 .One of them is adding additional gauge symmetries 10 . Another means is adding extra dimensions 11 .Recently , it was shown that there exists a class of string derived models where the lightest superpartner is gravitino 12 . Gravitino is weakly interacting massive object so its degradation rate is suppressed compared to neutralino case 13 .This characteristic makes gravitino a better contender for cold gray material 14 . Moreover , if gravitino mass m 3 / 2 < 1 GeV then its duration remains longer than age of universe 15 .Therefore , gravitino might be regarded as a viable contender for black matter 16 . On the other hand , gravitino is unstable because it couples to gravity 17 .It decays into photon or lepton - neutrino pairs 18 . If gravitino is heavier than 100 MeV then its degradation elements will contribute to diffuse gamma ray background 19 .Thus , gravitino should satisfy following conditions 20 :",
        "rewrite_text": "We propose a new model derived from string theory that features a stable proton, in which the lightest supersymmetric particle (LSP) is the gravitino instead of the neutralino. In this framework, the LSP decays into a photon or a neutrino-antineutrino pair via gravitational interactions. This approach allows us to account for the observed dark matter density without contradicting other experimental results, such as the relic abundance measurements from the WMAP study. Furthermore, we demonstrate that our model predicts intriguing signatures that could be detected in LHC experiments.\n\nIntroduction: The discovery of the Higgs boson has opened up exciting new avenues for exploring physics beyond the Standard Model (SM). Supersymmetry (SUSY), one of the most compelling extensions of the SM, provides a natural solution to the hierarchy problem. However, SUSY models face significant constraints from various experimental findings. In an effort to address these challenges, numerous researchers have proposed alternative approaches, such as introducing additional gauge symmetries or incorporating extra dimensions. Recently, it has been shown that there exists a class of string-derived models in which the lightest superpartner is the gravitino. As a weakly interacting massive particle, the decay rate of the gravitino is suppressed compared to that of the neutralino, making it a more suitable candidate for cold dark matter. Moreover, if the gravitino mass is less than 1 GeV, its lifetime exceeds the age of the universe, positioning it as a viable candidate for dark matter. However, gravitational coupling renders the gravitino unstable, leading to its decay into photons or lepton-neutrino pairs. If the gravitino mass exceeds 100 MeV, its decay products will contribute to the diffuse gamma-ray background. Therefore, the gravitino must satisfy certain conditions:",
        "ori-fast-z-score": -0.19069251784911848,
        "water-fast-z-score": 6.620784138506228,
        "rewrite-fast-z-score": 2.108406543164886
    },
    {
        "original_text": "We report on the detection and characterization of a massive galaxy cluster, Abell S1063 (z = 0.90), using data obtained with the Wide Field Camera 3 (WFC3) aboard Hubble Space Telescope (HST). The cluster was discovered as part of an ongoing survey for distant clusters carried out by our team within the framework of the UKIRT Infrared Deep Sky Survey Data Release 8 (UKIDSS DR8). We use photometric redshifts to select galaxies that are likely members of this structure over a large area around its center. Using these candidates we identify two brightest cluster galaxies (BCGs) separated by about 1 arcmin along the line-of-sight. These BCGs have magnitudes mF160B = 20.6 ± 0.1 mag and mF140W = 21.0 ± 0.2 mag respectively. Their colors suggest they are both early-type galaxies.",
        "watermark_text": "We report on the discovery and description of a huge galaxy cluster , Abell S1063 ( z = 0 . 90 ) , using data acquired with the Wide Field Camera 3 ( WFC3 ) aboard Hubble Space Telescope ( HST ) . The cluster was discovered as part of an continuing survey for distant galaxies carried out by our team within the framework of the UKIRT Infrared Deep Sky Survey Data Release 8 ( UKIDSS DR8 ) .We use photometric redshifts to select clusters that are likely elements of this formation over a large area around its core . Using these candidates we identify two brightest cluster clusters ( BCGs ) connected by about 1 arcmin along the line - of - view .These BCGs have magnitudes mF160B = 20 . 6 ± 0 . 1 mag and mF140W = 21 . 0 ± 0 . 2 mag respectively . Their colors indicate they are both earliest - class objects .",
        "rewrite_text": "We report the discovery and characterization of a massive galaxy cluster, Abell S1063 (z = 0.90), using data obtained from the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST). This cluster was identified as part of an ongoing survey for distant galaxies conducted by our team in conjunction with the UKIRT Infrared Deep Sky Survey Data Release 8 (UKIDSS DR8). We utilized photometric redshifts to select clusters likely to be components of this structure across a broad area surrounding its core. From these candidates, we identified two brightest cluster galaxies (BCGs) located approximately 1 arcminute apart along the line of sight. These BCGs have magnitudes of mF160B = 20.6 ± 0.1 mag and mF140W = 21.0 ± 0.2 mag, respectively, and their colors suggest that they are both among the earliest-class objects.",
        "ori-fast-z-score": -0.6030226891555273,
        "water-fast-z-score": 4.824181513244218,
        "rewrite-fast-z-score": 0.8846517369293828
    },
    {
        "original_text": "We present an analysis of the linear stability of epitaxially self-assembled quantum dots (QDs) on semiconductor surfaces, which are grown by molecular beam epitaxy under conditions where QDs form spontaneously and in regular arrays. We show that the QD ordering is determined by two competing mechanisms: surface diffusion and strain relaxation. The former tends to smooth out the QD density profile while the latter leads to its steepening. In particular we find that for small values of the QD size dispersion there exists a critical value of the growth rate above which ordered QD arrays cannot be formed. This result explains why it has been so difficult to grow ordered QD arrays with large QD sizes using conventional techniques. \n \n Keywords: Ordering, Strain Relaxation, Surface Diffusion, Quantum Dot Arrays, Stability, Growth Rate, Molecular Beam Epitaxy \n \n 1 Introduction \n \n Semiconductor nanocrystals or quantum dots (QDs), also known as colloidal quantum dots, have attracted considerable attention due to their unique optical properties  1  . They can be used in optoelectronic devices such as light-emitting diodes  2  , lasers  3  , solar cells  4  , photodetectors  5  , etc., and they may even play important roles in biological systems  6  .\n \nThe most common method for growing QDs is based on the so-called Stranski-Krastanov process  7, 8  . It involves depositing a thin layer of material onto a substrate at high temperatures followed by annealing at lower temperatures. Under these conditions islands nucleate randomly over the entire sample area but then evolve into ordered arrays through Ostwald ripening  9  . However, this technique does not allow one to control the position of individual QDs within each array  10  . Recently developed methods  11, 12  enable us to produce highly ordered QD arrays; however, they require very precise temperature control during deposition  13  . \n \n 2 Model Description \n \n Here we consider a model describing the formation of QDs on a two-dimensional lattice. Our starting point is the continuum equation proposed by Tersoff et al.  14  :",
        "watermark_text": "We present an assessment of the linear stability of epitaxially self - assembled quantum dots ( QDs ) on semiconductor surfaces , which are grown by molecular wave epitaxy under environments where QDs form spontaneously and in regular arrays . We see that the QD ordering is chosen by two different processes : material absorption and strain relaxation .The first helps to smooth out the QD density profile while the second contributes to its steepening . In particular we find that for low values of the QD width dispersion there exists a critical value of the growth rate above which organized QD arrays cannot be formed .This result explains why it has been so difficult to develop ordered QD arrays with large QD sizes using conventional methods . Keywords : Ordering , Strain Relaxation , Surface Diffusion , Quantum Dot Arrays , Stability , Growth Rate , Molecular Beam Epitaxy 1 Introduction Semiconductor nanocrystals or quantum dots ( QDs ) , sometimes called as colloidal quantum dots , have garnered considerable scrutiny due to their extraordinary optical properties 1 .They can be used in optoelectronic equipment such as light - emitting diodes 2 , lasers 3 , solar cells 4 , photodetectors 5 , etc . , and they may even hold important roles in biological environments 6 . The most common method for growing QDs is based on the so - called Stranski - Krastanov process 7 , 8 .It involves depositing a thin layer of material onto a substrate at high temperatures followed by annealing at lower temperatures . Under these conditions islands nucleate randomly over the entire sample region but then evolve into organized arrays through Ostwald ripening 9 .However , this methodology does not enable one to affect the placement of individual QDs within each array 10 . Recently developed methods 11 , 12 enable us to produce fully ordered QD arrays ; however , they demand very exact heat control during deposition 13 .2 Model Description Here we imagine a description explaining the formation of QDs on a two - dimensional crystal . Our opening point is the continuum equation proposed by Tersoff et al .14  :",
        "rewrite_text": "We provide an analysis of the linear stability of epitaxially self-assembled quantum dots (QDs) on semiconductor surfaces, which are created through molecular beam epitaxy in environments conducive to spontaneous formation and orderly arrangement of QDs. Our findings indicate that the ordering of QDs results from two distinct mechanisms: material absorption, which tends to smooth the density profile of the QDs, and strain relaxation, which enhances its steepness. Notably, we discover that for small variations in QD width, there exists a critical growth rate beyond which organized QD arrays cannot be achieved. This finding sheds light on the challenges faced in developing ordered QD arrays with larger sizes using traditional techniques. \n\n**Keywords:** Ordering, Strain Relaxation, Surface Diffusion, Quantum Dot Arrays, Stability, Growth Rate, Molecular Beam Epitaxy.\n\n**1 Introduction** Semiconductor nanocrystals, or quantum dots (QDs), often referred to as colloidal quantum dots, have attracted significant attention due to their remarkable optical characteristics. They have potential applications in various optoelectronic devices such as light-emitting diodes, lasers, solar cells, and photodetectors, and may also play critical roles in biological settings. The predominant method for fabricating QDs is the Stranski-Krastanov process, which involves depositing a thin layer of material onto a substrate at elevated temperatures, followed by annealing at reduced temperatures. In these conditions, islands nucleate randomly across the sample but later organize into structured arrays through a process known as Ostwald ripening. However, this traditional technique does not allow for precise control over the placement of individual QDs within the arrays. Recent advancements have introduced new methods that can produce fully ordered QD arrays, but they require precise thermal control during deposition. \n\n**2 Model Description** In this section, we propose a framework for understanding the formation of QDs on a two-dimensional crystal, starting with the continuum equation put forth by Tersoff et al.",
        "ori-fast-z-score": -0.6069769786668839,
        "water-fast-z-score": 6.219852664185793,
        "rewrite-fast-z-score": 1.5666989036012806
    },
    {
        "original_text": "We present an experimental investigation into complementarity between position and momentum measurements on single photons using a modified version of the original Einstein-Bohr photon box experiment.  The results show that, for this particular measurement scheme, there is no violation of Bell s inequality or any other form of nonlocality. We also demonstrate how our setup can be used to investigate quantum contextuality by performing two different experiments with identical settings but opposite outcomes. In one case we observe violations of Bell inequalities while in the other they are not violated. This shows that the observed behavior cannot be explained within classical physics and demonstrates quantum contextuality. Quantum mechanics predicts that certain physical quantities such as position and momentum do not have simultaneous well-defined values. Instead these quantities exist only as probability distributions which evolve continuously over time according to Schrödinger s equation. However, it has been shown that if both position and momentum were measured simultaneously then their respective probabilities would interfere destructively resulting in a zero probability of measuring either quantity at its most probable value  1  . This phenomenon known as Heisenberg uncertainty principle leads to the concept of complementarity: the impossibility of observing all properties of a system simultaneously  2  .\nIn 1964 John Bell showed that local hidden variable theories could not explain some predictions made by quantum mechanics  3  , leading to the formulation of Bell s theorem  4  . Since then many experiments have been performed to test whether quantum mechanical predictions violate Bell s theorem  5  . These tests typically involve entangled particles  6  where each particle carries information about the state of another distant particle  7, 8  . If the particles are separated far enough so that they never interact again after being created, then the correlations between them must be due solely to quantum effects  9  .",
        "watermark_text": "We present an experimental inquiry into complementarity between position and momentum estimates on single photons using a altered version of the original Einstein - Bohr photon box observation . The results show that , for this special measurement scheme , there is no violation of Bell s inequality or any other form of nonlocality .We additionally prove how our setup can be used to examine quantum contextuality by performing two different trials with identical settings but different outcomes . In one instance we exhibit abuses of Bell inequalities while in the other they are not upheld .This shows that the seen behavior cannot be described within classical physics and demonstrates quantum contextuality . Quantum theory predicts that particular physical components such as position and momentum do not have simultaneous well - defined parameters .Instead these quantities occur only as probability distributions which evolution continuously over time according to Schrödinger s equation . However , it has been shown that if both position and momentum were calculated separately then their different probabilities would interfere destructively resulting in a zero probability of assessing either quantity at its most likely value 1 .This phenomenon known as Heisenberg uncertainty theory gives to the notion of complementarity : the impossibility of experiencing all characteristics of a system simultaneously 2 . In 1964 John Bell demonstrated that nearby secret variable theories cannot not understand some observations made by quantum mechanics 3 , leading to the realization of Bell s theorem 4 .Since then many tests have been performed to test whether quantum mechanical predictions violate Bell s theorem 5 . These studies typically involve entangled particles 6 where each particle contains information about the state of another distant particle 7 , 8 .If the molecules are apart farther enough so that they cannot interact again after being created , then the correlations between them need be due exclusively to quantum effects 9 .",
        "rewrite_text": "We conduct an experimental exploration of the complementarity between position and momentum estimates of single photons, utilizing a modified version of the original Einstein-Bohr photon box experiment. Our findings indicate that, within this specific measurement framework, there is no infringement of Bell's inequality or any form of nonlocality. Furthermore, we demonstrate how our experimental setup can be employed to investigate quantum contextuality by conducting two distinct trials with identical parameters yet yielding different outcomes. In one trial, we observe violations of Bell's inequalities, while in the other, they are upheld. This discrepancy illustrates that the observed behavior cannot be explained through classical physics and serves as evidence for quantum contextuality. Quantum theory posits that certain physical properties, such as position and momentum, cannot be precisely determined at the same time. Instead, these quantities are represented as probability distributions that evolve over time according to Schrödinger's equation. It has been shown that if position and momentum are assessed separately, their respective probabilities can interfere destructively, resulting in a negligible probability of determining either property at its most probable value. This phenomenon, known as the Heisenberg uncertainty principle, supports the concept of complementarity: the impossibility of fully capturing all characteristics of a system simultaneously. In 1964, John Bell demonstrated that nearby hidden variable theories cannot adequately account for certain quantum mechanical observations, leading to the establishment of Bell's theorem. Since then, numerous tests have been conducted to investigate whether quantum mechanical predictions violate Bell's theorem. These experiments often involve entangled particles, where each particle contains information regarding the state of another distant particle. If the particles are sufficiently separated to prevent any subsequent interaction after their creation, their correlations must arise solely from quantum effects.",
        "ori-fast-z-score": -0.17149858514250882,
        "water-fast-z-score": 8.231932086840423,
        "rewrite-fast-z-score": 0.17025130615174972
    },
    {
        "original_text": "We report on the discovery of an X-ray pulsar, PSR J1814-1744, associated with the unidentified TeV gamma-ray source HESS J1813-178 in the Galactic plane at l = 18.6 deg., b = -0.1 deg.. The pulsar has spin period Psr = 2.16 s and is located within 0.2 arcmin (0.01 pc) of the centroid position determined by the High Energy Stereoscopic System (HESS). We also discovered a faint extended nebula around this pulsar using Chandra observations. This nebula shows a shell-like structure with radius Rneb ~ 1.5 arcmin (0.06 pc), which is consistent with that expected for a wind nebula powered by the pulsar s relativistic winds. The observed flux density of the nebula at radio wavelengths is Fν(3 GHz) ~ 3 mJy beam-1 , while its luminosity at X-rays is Lx ~ 1033 erg s-1 . These values are comparable to those seen in other young pulsars.",
        "watermark_text": "We report on the discovery of an X - ray pulsar , PSR J1814 - 1744 , associated with the unidentified TeV gamma - ray source HESS J1813 - 178 in the Galactic jet at l = 18 . 6 deg . , b = - 0 . 1 deg . . The pulsar has spin date Psr = 2 . 16 s and is situated within 0 . 2 arcmin ( 0 . 01 pc ) of the centroid point determined by the High Energy Stereoscopic System ( HESS ) .We additionally discovered a faint extended nebula around this pulsar utilizing Chandra measurements . This nebula shows a shell - like structure with diameter Rneb ~ 1 . 5 arcmin ( 0 . 06 pc ) , which is compatible with that expected for a wind nebula powered by the pulsar s relativistic winds .The observed flux concentration of the nebula at radio wavelengths is Fν ( 3 GHz ) ~ 3 mJy beam - 1 , while its luminosity at X - radiation is Lx ~ 1033 erg s - 1 . These figures are comparable to those observed in other young pulsars .",
        "rewrite_text": "We have discovered an X-ray pulsar, PSR J1814-1744, linked to the unidentified TeV gamma-ray source HESS J1813-178, located in the Galactic jet at coordinates l = 18.6°, b = -0.1°. This pulsar has a spin period of Psr = 2.16 s and lies within 0.2 arcmin (0.01 pc) of the centroid identified by the High Energy Stereoscopic System (HESS). Additionally, we found a faint extended nebula surrounding the pulsar through Chandra observations. This nebula exhibits a shell-like structure with a diameter of Rneb ~ 1.5 arcmin (0.06 pc), consistent with expectations for a wind nebula driven by the pulsar's relativistic winds. The nebula's flux density at radio wavelengths is approximately Fν (3 GHz) ~ 3 mJy beam^-1, while its X-ray luminosity is Lx ~ 10^33 erg s^-1. These measurements are similar to those found in other young pulsars.",
        "ori-fast-z-score": 0.16012815380508713,
        "water-fast-z-score": 4.643716460347527,
        "rewrite-fast-z-score": 0.48038446141526137
    },
    {
        "original_text": "We report on observations made by the High Energy Stereoscopic System (H.E.S. S.) telescope array in Namibia, which detected very-high-energy (VHE) gamma rays from the distant blazar  1ES1102-232 at redshift z = 0.186. The source was observed for more than 50 hours between September 2005 and March 2006 using data taken simultaneously with four telescopes. A total excess of 12 events above background were found within an energy range of 400 GeV to 20 TeV. No significant variability is seen during this period. We present results from spectral analysis performed over different time intervals as well as broadband modelling of the multi-wavelength spectrum including radio through X-ray measurements. This work demonstrates that H.E.S.  S. can detect sources beyond redshifts previously accessible only to ground-based Cherenkov telescopes. It also shows how such observations are important for understanding the physics of these extreme objects.",
        "watermark_text": "We report on observations made by the High Energy Stereoscopic System ( H . E . S . S . ) telescope array in Namibia , which detected very - large - energy ( VHE ) gamma radiation from the distant blazar 1ES1102 - 232 at redshift z = 0 . 186 .The source was seen for more than 50 hours between September 2005 and March 2006 using data taken concurrently with four telescopes . A total excess of 12 events above background were found within an energy range of 400 GeV to 20 TeV .No considerable variability is seen during this time . We see results from spectral study performed over different time periods as well as broadband analysis of the multi - wavelength spectrum including radio through X - ray observations .This research proves that H . E . S . S . can identify sources beyond redshifts previously accessible only to surface - based Cherenkov telescopes .It additionally shows how such observations are important for studying the physics of these extreme objects .",
        "rewrite_text": "We present findings from the High Energy Stereoscopic System (H.E.S.S.) telescope array in Namibia, which detected very high-energy (VHE) gamma radiation from the distant blazar 1ES1102-232 at a redshift of z = 0.186. The observations, made over more than 50 hours between September 2005 and March 2006, utilized data collected simultaneously from four telescopes. A total of 12 excess events above the background were recorded within an energy range of 400 GeV to 20 TeV, with no significant variability observed during this period. Our results include spectral analyses conducted over various time frames as well as a comprehensive broadband examination of the multi-wavelength spectrum, encompassing radio to X-ray observations. This study demonstrates that H.E.S.S. can detect sources at redshifts that were previously only accessible to ground-based Cherenkov telescopes, highlighting the significance of such observations for understanding the physics of these extreme astrophysical objects.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 5.505585837114527,
        "rewrite-fast-z-score": 0.508000508000762
    },
    {
        "original_text": "We present results for the evolution of magnetized protostellar accretion discs in which we have included both Ohmic and ambipolar diffusion, as well as radiative transfer effects using flux-limited diffusion (FLD). We find that the inclusion of these additional physical processes has important consequences for disc structure and evolution.  In particular, we show that the presence of an initial magnetic field can significantly affect the mass distribution within the disc at early times by suppressing fragmentation near the central star. This leads to more massive discs than those found previously with purely hydrodynamic simulations. The resulting discs are also less flared due to the increased pressure support provided by the magnetic field. As time progresses, however, the magnetic field is dissipated through ohmic dissipation and turbulence driven by gravitational instabilities. Once this happens, the disc becomes thinner and more flared compared to non-magnetic models.",
        "watermark_text": "We see results for the evolution of magnetized protostellar accretion balls in which we have incorporated both Ohmic and ambipolar diffusion , as well as radiative transfer effects utilizing flux - limited absorption ( FLD ) . We see that the introduction of these additional material processes has crucial consequences for disc composition and evolution .In particular , we find that the presence of an initial magnetic force can significantly affect the mass distribution within the disc at early periods by suppressing fragmentation near the main star . This leads to more massive discs than those identified previously with solely hydrodynamic simulations .The produced discs are also less flared due to the increased pressure support offered by the magnetic force . As time progresses , however , the magnetic force is dissipated through ohmic dissipation and turbulence driven by gravitational instabilities .Once this happens , the disc becomes thinner and more flared relative to non - magnetic models .",
        "rewrite_text": "We present findings on the evolution of magnetized protostellar accretion disks, incorporating both Ohmic and ambipolar diffusion, along with radiative transfer effects modeled through flux-limited absorption (FLD). The inclusion of these processes has significant implications for the composition and development of the disks. Notably, we discover that an initial magnetic force markedly influences the mass distribution within the disk during its early phases, suppressing fragmentation near the central star. This results in more massive disks compared to those identified in previous hydrodynamic simulations. Furthermore, the disks produced are less flared due to the enhanced pressure support from the magnetic force. However, as time advances, the magnetic force dissipates through ohmic dissipation and turbulence generated by gravitational instabilities. Consequently, the disk becomes thinner and exhibits a greater flare compared to non-magnetic models.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 2.75
    },
    {
        "original_text": "We present the results of our investigation on semiclassical scalar propagator in curved space-time, which is based on the WKB approximation to the wave function. We show that there are two different ways how one can define this quantity depending on whether or not one takes into account the back-reaction effects due to the quantum fluctuations of the gravitational field. The first approach leads to an expression for the semiclassical propagator which coincides with the Feynman propagator at large distances but differs significantly near the source point. In particular it does not satisfy the Hadamard condition required by general relativity. On the other hand, if we take into account the back reaction then the resulting expression satisfies all necessary conditions including the Hadamard condition. However, as was shown recently by Wald et al., such an expression cannot be obtained within the framework of standard QFT. This problem may have important consequences when considering the propagation of particles through black holes since the corresponding expressions differ substantially even outside the horizon.",
        "watermark_text": "We present the conclusion of our analysis on semiclassical scalar propagator in curved space - time , which is based on the WKB approximation to the wave function . We see that there are two different ways how one can define this quantity based on whether or not one takes into consideration the back - reaction effects due to the quantum fluctuations of the gravitational field .The first method results to an definition for the semiclassical propagator which coincides with the Feynman propagator at large distances but varies dramatically near the origin point . In particular it does not satisfy the Hadamard condition required by general relativity .On the other hand , if we took into consideration the back response then the resulting expression satisfies all necessary circumstances including the Hadamard condition . However , as was shown lately by Wald et al . , such an form cannot be obtained within the framework of standard QFT .This problem could have important implications when examining the propagation of particles through black holes since the equivalent definitions differ substantially even outside the horizon .",
        "rewrite_text": "In this analysis, we present our conclusions regarding the semiclassical scalar propagator in curved spacetime, which employs the WKB approximation to the wave function. We have identified two distinct methods for defining this quantity, contingent upon whether back-reaction effects from quantum fluctuations in the gravitational field are considered. The first approach yields a semiclassical propagator that aligns with the Feynman propagator at large distances but exhibits significant variation near the origin, notably failing to meet the Hadamard condition mandated by general relativity. Conversely, if back-reaction is taken into account, the resulting expression fulfills all necessary criteria, including adherence to the Hadamard condition. However, recent findings by Wald et al. indicate that this form cannot be derived within the standard framework of quantum field theory (QFT). This discrepancy may have critical implications for studying particle propagation near black holes, as the alternative definitions diverge considerably even outside the event horizon.",
        "ori-fast-z-score": 0.5698028822981898,
        "water-fast-z-score": 5.128225940683707,
        "rewrite-fast-z-score": 0.917662935482247
    },
    {
        "original_text": "We present highly resolved numerical simulations of the incompressible Navier-Stokes equations with the LANS-alpha model, which is known to produce good results for wall-bounded flows at low Reynolds numbers. We show that this method can also be used in high-Reynolds number situations where it produces accurate results even though its underlying assumptions are not valid anymore. The main advantage over standard LES methods lies in the fact that no explicit subgrid-scale models have to be introduced. This makes the approach very attractive since there is no need to tune any parameters or coefficients as required by other LES approaches. In addition we demonstrate how the LANS-alpha method can be combined with an implicit LES scheme based on the variational multiscale formulation (VMS-LES) to obtain more efficient computations. Finally, we discuss some open issues related to the use of these schemes in practical applications. Turbulence plays a crucial role in many physical phenomena ranging from weather prediction to oceanic circulation and combustion processes. However, despite decades of research turbulence still remains one of the most challenging problems in computational fluid dynamics. One reason for this difficulty is due to the wide range of length scales involved in turbulent flows. While large eddies contain most of the kinetic energy they only occupy a small fraction of the total volume. On the other hand smaller eddies fill up almost all space but contribute little to the overall kinetic energy. Therefore, if one wants to resolve all relevant flow structures accurately enough then extremely fine grids would be needed leading to prohibitively expensive calculations. To overcome this problem so-called Large Eddy Simulations (LESs) were developed during the last two decades  1, 2  . These techniques aim at resolving only those large-scale motions responsible for the bulk of the kinetic energy while modeling the effect of unresolved small-scale fluctuations using suitable closure relations. Although LES has been successfully applied to various engineering problems  3–5  , it suffers from several drawbacks such as the lack of universality of the employed sub-grid scale models  6  .\nIn recent years new classes of LES-like methods have emerged  7–10  . They are based",
        "watermark_text": "We create highly resolved numerical simulations of the incompressible Navier - Stokes equations with the LANS - alpha simulation , which is known to produce excellent results for wall - defined currents at low Reynolds numbers . We see that this method can also be used in high - Reynolds number circumstances where it generates accurate conclusions albeit though its core assumptions are not valid anymore .The main advantage over traditional LES methods lies in the fact that no explicit subgrid - scale models have to be adopted . This makes the approach very appealing since there is no necessary to tune any coefficients or coefficients as required by other LES approaches .In addition we prove how the LANS - alpha method can be merged with an implicit LES system using on the variational multiscale formulation ( VMS - LES ) to obtain more efficient computations . Finally , we explain some open concerns relevant to the using of these schemes in practical applications .Turbulence plays a crucial role in many natural phenomena ranging from weather prediction to oceanic circulation and combustion cycles . However , despite decades of research turbulence nonetheless continues one of the most challenging difficulties in computational liquid mechanics .One reason for this difficulty is due to the broad variety of length scales implicated in turbulent streams . While big eddies contain most of the kinetic power they only comprise a small fraction of the total quantity .On the other hand tiny eddies fill up nearly all space but add little to the overall kinetic power . Therefore , if one wants to resolve all relevant stream dynamics correctly sufficiently then extremely good grids might be needed leading to prohibitively expensive calculations .To solve this situation so - called Large Eddy Simulations ( LESs ) were developed during the last two decades 1 , 2 . These methods aim at resolving only those huge - scale motions responsible for the majority of the kinetic power while solving the impact of unresolved small - scale fluctuations using appropriate completion relations .Although LES has been successfully applied to numerous technical problems 3 – 5 , it suffers from several drawbacks such as the lack of universality of the involved sub - grid size models 6 . In recent years new classes of LES - like methods have developed 7 – 10 .They are based",
        "rewrite_text": "We develop highly detailed numerical simulations of the incompressible Navier-Stokes equations using the LANS-alpha method, which is recognized for delivering excellent results for wall-defined flows at low Reynolds numbers. Our findings indicate that this method can also be applied in high Reynolds number scenarios, consistently yielding accurate results, despite the fact that its fundamental assumptions may no longer hold. A key advantage of the LANS-alpha approach over traditional Large Eddy Simulation (LES) methods is that it does not require explicit subgrid-scale models. This aspect greatly enhances its appeal, as there is no need to fine-tune any coefficients, unlike other LES techniques. Furthermore, we demonstrate how the LANS-alpha method can be integrated with an implicit LES framework based on the variational multiscale formulation (VMS-LES) to achieve more efficient computations. Finally, we address some open concerns related to the practical implementation of these schemes. Turbulence is a critical factor in numerous natural processes, including weather forecasting, ocean circulation, and combustion cycles. However, despite decades of study, turbulence remains one of the most significant challenges in computational fluid dynamics. This challenge stems largely from the wide range of length scales present in turbulent flows. While large eddies account for most of the kinetic energy, they represent only a small fraction of the total number of eddies. Conversely, small eddies occupy almost all available space but contribute little to the overall kinetic energy. Consequently, adequately resolving all significant flow dynamics may require extremely fine grids, resulting in prohibitively high computational costs. To address this issue, Large Eddy Simulations (LES) have been developed over the past two decades. These methods focus on resolving the large-scale motions that contribute most significantly to the kinetic energy while modeling the effects of unresolved small-scale fluctuations through suitable closure relations. Although LES has been effectively utilized in a variety of engineering problems, it has several limitations, including the non-universality of the subgrid-scale models used. In recent years, new classes of LES-like methods have emerged.",
        "ori-fast-z-score": -1.4122588778696161,
        "water-fast-z-score": 7.345410552159442,
        "rewrite-fast-z-score": -1.1470786693528088
    },
    {
        "original_text": "We present new observations and analysis of the Balmer-dominated shocks driven by supernova remnants (SNRs) into dense molecular clouds, which are known as  molecular cloud shocks  or  Balmer-dominated shocks . We find that these shocks have an intermediate temperature between those of typical J-type and C-type shocks. The observed emission lines show prominent P-Cygni profiles with blueshifted absorption features indicating high velocities up to 100 km s-1 . These results suggest that the transition zone is located at the interface between the shocked gas and unshocked ambient medium. In addition, we found that the widths of the Hα line profiles increase toward the center of SNR W28. This indicates that the density structure of the surrounding environment may be more complicated than previously thought. Our study suggests that Balmer-dominated molecular cloud shocks can provide important information on the physical conditions of the interstellar medium around young SNRs.",
        "watermark_text": "We report new studies and investigation of the Balmer - dominated shocks driven by supernova remnants ( SNRs ) into thick molecular clouds , which are known as atom cluster shocks or Balmer - dominated shocks . We see that these shocks have an intermediate heat between those of typical J - class and C - class shocks .The observed emission lines show marked P - Cygni profiles with blueshifted emission elements suggesting high velocities up to 100 km s - 1 . These data suggest that the shift area is situated at the interface between the excited gas and unshocked ambient material .In addition , we recovered that the widths of the Hα line profiles increase toward the center of SNR W28 . This implies that the density structure of the nearby region might be more complicated than previously thought .Our study implies that Balmer - dominated molecular storm shocks can provide important information on the physical conditions of the interstellar medium around early SNRs .",
        "rewrite_text": "We present new research on Balmer-dominated shocks produced by supernova remnants (SNRs) interacting with dense molecular clouds, often referred to as atom cluster shocks. Our findings indicate that these shocks exhibit thermal properties that fall between those of typical J-class and C-class shocks. The emission lines we observed feature prominent P-Cygni profiles, with blueshifted emission components implying high velocities of up to 100 km/s. This data suggests that the emission is originating from the interface between the excited gas and the surrounding unshocked material. Furthermore, we found that the widths of the Hα line profiles increase towards the center of SNR W28, indicating that the density structure in the surrounding area may be more complex than previously recognized. Our study demonstrates that Balmer-dominated shocks in molecular clouds can yield valuable insights into the physical conditions of the interstellar medium in the vicinity of early SNRs.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.861993625888845,
        "rewrite-fast-z-score": 2.1514114968019085
    },
    {
        "original_text": "We present new U BVRI photometric observations for the barred galaxy NGC 3367, obtained with the 1 m telescope at Cerro Tololo Inter-American Observatory (CTIO). The main goal is to study the stellar structures in this galaxy and their relation to its nuclear activity. We find that there are two bright knots along the major axis of the galaxy which may be associated with star formation regions. These knots have colors similar to those found in HII regions. In addition we detect several other faint knots on both sides of the nucleus. Their color indices suggest that they could also be related to recent star formation events. Finally, we identify an extended structure towards south-east direction whose nature remains unclear. This work was supported by CONACyT grant 36586-E. We thank J. M. Alloin for his help during our observing run at CTIO. Keywords: Starburst galaxies; Nuclear activity",
        "watermark_text": "We report new U BVRI photometric surveys for the barred star NGC 3367 , obtained with the 1 m observatory at Cerro Tololo Inter - American Observatory ( CTIO ) . The main goal is to study the stars formations in this galaxy and their connection to its nuclear activity .We see that there are two bright knots along the main axis of the galaxy which may be identified with star formation regions . These threads have colors similar to those present in HII centers .In addition we find several other slight knots on both sides of the nucleus . Their color indices indicate that they may also be connected to recent star formation changes .Finally , we identify an extended structure towards south - eastward direction whose nature remains unsure . This project was supported by CONACyT grant 36586 - E . We praise J . M . Alloin for his help during our observing run at CTIO .Keywords : Starburst galaxies ; Nuclear activity",
        "rewrite_text": "We present new U BVRI photometric surveys of the barred star NGC 3367, conducted at the 1-meter facility of the Cerro Tololo Inter-American Observatory (CTIO). Our primary objective is to investigate star formation within this galaxy and its relationship to its nuclear activity. We observe two prominent knots along the galaxy's main axis, which are likely associated with regions of star formation and exhibit colors similar to those found in HII centers. Additionally, we detect several faint knots flanking the nucleus, with color indices suggesting potential ties to recent star formation events. Lastly, we identify an extended structure in the south-east direction, though its exact nature remains uncertain. This project was funded by CONACyT grant 36586-E. We would like to express our gratitude to J.M. Alloin for his assistance during our observation period at CTIO. \n\nKeywords: Starburst galaxies; Nuclear activity.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": 0.8307471607356973
    },
    {
        "original_text": "We present new near-infrared integral field spectroscopy (IFS) data for the brightest galaxy in the cluster Abell 2218, which is known to be interacting with its nearest neighbor, the radio-quiet quasar I Zw 1 at z = 0.0625. We find that this galaxy has an extended low-surface-brightness component surrounding it, extending out to about 10 kpc on both sides along the major axis. This feature shows no evidence of rotation but does show some velocity structure consistent with infalling gas or tidal debris. In addition we detect two compact objects within 5 kpc of the center of the galaxy. One of these appears to have a very high surface brightness and may represent a nuclear starburst; however, the other one displays much lower surface brightness and could possibly be associated with a supermassive black hole binary system. These results are discussed in terms of possible evolutionary scenarios for this interacting pair.",
        "watermark_text": "We present new near - infrared integral field spectroscopy ( IFS ) statistics for the brightest galaxy in the cluster Abell 2218 , which is known to be interacting with its closest neighbor , the radio - quiet quasar I Zw 1 at z = 0 . 0625 . We see that this galaxy has an extended low - exterior - brightness core covering it , extending out to about 10 kpc on both sides along the main axis .This structure exhibits no evidence of rotation but does display some velocity pattern correlated with infalling dust or tidal debris . In addition we find two compact entities within 5 kpc of the center of the galaxy .One of these seems to have a very high surface brightness and may indicate a nuclear starburst ; however , the other one exhibits far lower surface brightness and could possibly be identified with a supermassive black hole binary system . These conclusions are discussed in terms of possible evolved situations for this interacting pair .",
        "rewrite_text": "We present new near-infrared integral field spectroscopy (IFS) data for the brightest galaxy in the Abell 2218 cluster, which is known to be interacting with its nearby neighbor, the radio-quiet quasar I Zw 1 at z = 0.0625. Our observations reveal that this galaxy has a low-exterior-brightness core that extends approximately 10 kpc on either side along its main axis. This structure shows no signs of rotation but does exhibit a velocity pattern that may be related to infalling dust or tidal debris. Additionally, we have identified two compact entities within 5 kpc of the galaxy's center. One of these displays a very high surface brightness, which may suggest a nuclear starburst; the other, however, has a much lower surface brightness and could potentially correspond to a supermassive black hole binary system. We explore these findings in the context of possible evolutionary scenarios for this interacting system.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 1.649915822768611
    },
    {
        "original_text": "We study the nonlinear perturbations of general relativity (GR) and other metric theories of gravity, focusing on their effects on conserved quantities such as energy-momentum tensors. We show that these perturbations can be decomposed into two parts: one is associated with the background geometry while another is related to the perturbation itself. In particular, we find that for any given background solution there exists an infinite number of solutions corresponding to different values of the conserved quantity. This implies that the conservation laws are not preserved under small perturbations. Furthermore, we demonstrate how this effect may lead to violations of the weak equivalence principle. Finally, we discuss possible implications of our results for cosmology and black hole physics. General relativity (GR), which describes gravitational interactions at large scales, has been tested extensively against observations over many decades  1  . However, it remains unclear whether or not GR also holds true at smaller length scales where quantum mechanics becomes important  2  .\nIn order to address this question, several alternative theories have been proposed  3  , including scalar-tensor theories  4  , f(R)-gravity  5  , massive gravity  6  , and so forth  7, 8  . These theories typically involve additional degrees of freedom beyond those present in GR  9  . For example, in scalar-tensor theories, the graviton acquires a mass through its coupling to a scalar field  10  . Similarly, in f(R)-theories  11  , the Einstein-Hilbert action contains higher-order curvature terms  12  . It turns out that both types of theories admit self-accelerating solutions  13  , i.e., de Sitter-like solutions without requiring dark energy  14  .",
        "watermark_text": "We research the nonlinear perturbations of general relativity ( GR ) and other metric explanations of gravitational , concentrating on their impacts on conserved parameters such as energy - momentum tensors . We see that these perturbations can be decomposed into two parts : one is associated with the background geometry while another is related to the perturbation itself .In particular , we find that for any given background solution there exists an endless number of solutions associated to different values of the conserved quantity . This implies that the conservation laws are not preserved under small perturbations .Furthermore , we explain how this effect could lead to infringement of the weak equivalence principle . Finally , we explain possible possibilities of our findings for cosmology and dark hole physics .General relativity ( GR ) , which explains gravitational interactions at large scales , has been tested extensively against measurements over numerous centuries 1 . However , it remains unsure whether or not GR still holds true at greater size scales where quantum mechanics becomes crucial 2 .In order to meet this question , various alternative theories have been proposed 3 , notably scalar - vector models 4 , f ( R ) - gravity 5 , giant gravity 6 , and so forth 7 , 8 . These explanations typically involve additional degrees of autonomy beyond those present in GR 9 .For instance , in scalar - vector theories , the graviton acquires a mass through its interaction to a scalar field 10 . Similarly , in g ( R ) - fields 11 , the Einstein - Hilbert action contains upper - order curvature terms 12 .It turns out that both types of theories admit self - accelerating problems 13 , i . e . , de Sitter - like solutions without using dark energy 14 .",
        "rewrite_text": "We investigate the nonlinear perturbations in general relativity (GR) and other metric theories of gravitation, focusing on their effects on conserved quantities such as energy-momentum tensors. Our findings reveal that these perturbations can be divided into two components: one tied to the background geometry and the other linked to the perturbations themselves. Notably, we discover that for any chosen background solution, there is an infinite number of solutions connected to varying values of the conserved quantities. This suggests that conservation laws may not hold under small perturbations. Furthermore, we discuss how this phenomenon could potentially violate the weak equivalence principle. We also explore the implications of our results for cosmology and black hole physics. \n\nGeneral relativity, which describes gravitational interactions on large scales, has been rigorously tested against empirical measurements over many centuries. However, it remains unclear whether GR remains valid on larger scales, particularly where quantum mechanics becomes significant. In response to this question, various alternative theories have been proposed, including scalar-vector models, f(R) gravity, and massive gravity, among others. These theories often introduce additional degrees of freedom that are absent in GR. For example, in scalar-vector models, gravitons acquire mass through their interaction with a scalar field. Similarly, in f(R) theories, the Einstein-Hilbert action includes higher-order curvature terms. Notably, both classes of theories can exhibit self-accelerating solutions, resembling de Sitter solutions without the need for dark energy.",
        "ori-fast-z-score": -1.4485719366802965,
        "water-fast-z-score": 6.454545454545454,
        "rewrite-fast-z-score": -1.4605934866804429
    },
    {
        "original_text": "We report on the electronic structure and magnetic properties of zigzag graphene nanoribbons (ZGNRs) with different edge structures, including hydrogenated ZGNR (H-ZGNR), fluorinated ZGNR (F-ZGNR), oxygenated ZGNR (O-ZGNR), and nitrogen-doped O-ZGNR (N-ZGNR). We find that all these ZGNRs are half-metals except for H-ZGNR which is metallic. The band gaps of F-ZGNR and N-ZGNR increase as compared to those of pristine ZGNR due to the electronegativity difference between carbon atoms at edges and their neighbors. In contrast, the band gap decreases slightly when oxygen replaces one or two carbon atoms at each edge because of charge transfer from oxygen to neighboring carbon atom(s). Our results show that the spin polarization can be enhanced by introducing oxygen into the edges of ZGNRs.",
        "watermark_text": "We report on the electronic structure and magnetic properties of zigzag graphene nanoribbons ( ZGNRs ) with various edge structures , notably hydrogenated ZGNR ( H - ZGNR ) , fluorinated ZGNR ( F - ZGNR ) , oxygenated ZGNR ( O - ZGNR ) , and nitrogen - doped O - ZGNR ( N - ZGNR ) . We see that all these ZGNRs are half - metals except for H - ZGNR which is metallic .The band holes of F - ZGNR and N - ZGNR increase as compared to those of pristine ZGNR due to the electronegativity difference between carbon atoms at corners and their relatives . In comparison , the band gap falls slightly when oxygen replaces one or two carbon atoms at each edge because of charge transfer from nitrogen to neighboring carbon molecule ( s ) .Our results show that the spin polarization can be enhanced by bringing oxygen into the edges of ZGNRs .",
        "rewrite_text": "We investigate the electronic structure and magnetic properties of zigzag graphene nanoribbons (ZGNRs) featuring different edge configurations, specifically hydrogenated ZGNR (H-ZGNR), fluorinated ZGNR (F-ZGNR), oxygenated ZGNR (O-ZGNR), and nitrogen-doped O-ZGNR (N-ZGNR). Our findings reveal that, with the exception of H-ZGNR, which exhibits metallic behavior, all other ZGNRs act as half-metals. The band gaps of F-ZGNR and N-ZGNR increase compared to those of pristine ZGNR, attributed to the electronegativity differences between the corner carbon atoms and their counterparts. In contrast, the band gap decreases slightly when oxygen substitutes for one or two carbon atoms at each edge due to charge transfer from nitrogen to adjacent carbon atoms. Overall, our results indicate that incorporating oxygen at the edges of ZGNRs can enhance spin polarization.",
        "ori-fast-z-score": 0.5897678246195885,
        "water-fast-z-score": 3.8334908600273256,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results of an analysis of all available data on cometary dust tails, including those observed by spacecraft and ground-based telescopes in recent years. We find that most of these objects are associated with Jupiter family comets (JFCs), which have orbital periods less than 20 yr. The JFCs produce dust tails that can be traced for up to several thousand AU along their orbits. These tails appear as narrow streams of material extending outward at high speed from the parent bodies. In some cases they show evidence of being disrupted into multiple fragments or branches. Most of the tail structures we observe are consistent with models where particles are released continuously over time scales ranging from months to thousands of years. However, there is growing observational evidence suggesting that many of these tails may also contain significant amounts of freshly produced dust grains ejected during more recent outburst events. This suggests that the production rate of dust particles in these systems varies significantly both spatially and temporally. \n \n Keywords: Comet",
        "watermark_text": "We publish the conclusion of an assessment of all available data on cometary dust tails , particularly those observed by satellites and land - based telescopes in recent seasons . We see that most of these objects are identified with Jupiter class comets ( JFCs ) , which have orbital periods fewer than 20 yr .The JFCs produce dust tails that can be traced for up to several thousand AU along their orbits . These tails occur as short streams of debris extending outward at high velocity from the parent bodies .In some cases they show proof of being disrupted into multiple pieces or limbs . Most of the tail structures we study are compatible with models where objects are released constantly over time ranges varied from months to thousands of years .However , there is growing observational evidence indicating that several of these tails might additionally carry significant amounts of newly released powder grains ejected during more recent outburst events . This implies that the production frequency of dust particles in these systems vary significantly both spatially and temporally .Keywords: Comet",
        "rewrite_text": "We present our findings from an assessment of all available data on cometary dust tails, especially those observed by both satellites and ground-based telescopes in recent seasons. Our analysis reveals that a majority of these objects are associated with Jupiter-family comets (JFCs), which have orbital periods of less than 20 years. These JFCs generate dust tails that can extend for thousands of astronomical units along their orbits. The tails manifest as short streams of debris that move outward rapidly from the parent comets. In certain instances, there is evidence of fragmentation into multiple pieces or limbs. Most of the tail structures we investigate align with models suggesting that debris is continuously released over time spans ranging from months to thousands of years. However, increasing observational data indicates that many of these tails may also contain considerable amounts of newly released powder grains that are expelled during more recent outburst events. This suggests that the rate of dust particle production in these systems varies significantly both in space and time. Keywords: Comet",
        "ori-fast-z-score": 0.10846522890932808,
        "water-fast-z-score": 7.979625217054442,
        "rewrite-fast-z-score": 2.6499947000159003
    },
    {
        "original_text": "We present an effective model to study the properties of dense quark matter in the presence of large strange quarks masses, which are relevant for compact stars with high central density. The model is based on the Nambu-Jona-Lasinio (NJL) Lagrangian extended by including vector mesons as explicit degrees of freedom. We show that this extension allows us to reproduce simultaneously several features observed experimentally or theoretically in nuclear physics such as: i) saturation of nuclear matter; ii) appearance of hyperon resonances; iii) restoration of chiral symmetry at finite baryonic densities; iv) existence of a critical endpoint in the phase diagram of strongly interacting matter. In addition we find that our results agree well with recent lattice simulations performed within the framework of the PNJL model. Finally, we discuss how these findings can be used to explore new regions of the parameter space of the NJL model.",
        "watermark_text": "We present an efficient model to study the properties of dense quark matter in the presence of large odd quarks masses , which are important for compact stellar with high central density . The model is based on the Nambu - Jona - Lasinio ( NJL ) Lagrangian extended by including tensor mesons as explicit degrees of liberty .We see that this extension permits us to capture concurrently many features detected experimentally or theoretically in nuclear science such as : i ) saturation of nuclear material ; ii ) presence of hyperon resonances ; iii ) restoration of chiral symmetry at finite baryonic densities ; iv ) creation of a critical endpoint in the phase diagram of highly correlated matter . In addition we find that our findings agree well with recent lattice simulations conducted within the framework of the PNJL theory .Finally , we talk how these results can be used to study new regions of the parameter space of the NJL model .",
        "rewrite_text": "We introduce an effective model to investigate the characteristics of dense quark matter, particularly in the context of significant odd quark masses, which are crucial for understanding compact stars with high central densities. This model is built upon the Nambu-Jona-Lasinio (NJL) Lagrangian, which we enhance by incorporating tensor mesons as explicit degrees of freedom. This extension allows us to simultaneously address several phenomena observed both experimentally and theoretically in nuclear physics, including: i) the saturation of nuclear matter; ii) the existence of hyperon resonances; iii) the restoration of chiral symmetry at finite baryonic densities; and iv) the emergence of a critical endpoint in the phase diagram of strongly correlated matter. Additionally, our results align well with recent lattice simulations conducted within the framework of the PNJL theory. Finally, we discuss how these findings can be utilized to explore new areas of the NJL model's parameter space.",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 5.898906801202691,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the first results on differential rotation for an evolved star, based on observations with the Microvariability and Oscillations of STars (MOST) satellite. The target is the F-type main-sequence star kappa 1 Cet (HD 128898), which has been observed in two consecutive runs during 2005-2006. We find that the surface shear rate decreases towards lower latitudes, but increases again at mid-latitudes. This behaviour can be explained if we assume that there are two differentially rotating regions on opposite sides of the equator. In addition to this large-scale pattern, we also detect smaller-scale features such as spots and plages. These structures appear to have lifetimes between one week and several months. Finally, we show how our results compare with theoretical predictions made using stellar evolution models. Keywords: Differential rotation; Stellar activity; Spots; Plages; MOST",
        "watermark_text": "We report the first findings on differential rotation for an evolved star , based on observations with the Microvariability and Oscillations of STars ( MOST ) satellite . The target is the F - class major - sequence star kappa 1 Cet ( HD 128898 ) , which has been observed in two consecutive ran during 2005 - 2006 .We see that the surface shear rate decreases nearer lower latitudes , but grows again at mid - latitudes . This behaviour can be described if we suppose that there are two differentially rotating zones on opposite sides of the equator .In addition to this big - scale signature , we also observe lower - scale characteristics such as spots and plages . These structures appear to have lifetimes between one month and many months .Finally , we tell how our findings compare with theoretical estimates made using stellar evolution models . Keywords : Differential rotation ; Stellar activity ; Spots ; Plages ; MOST",
        "rewrite_text": "We present the initial findings on differential rotation in an evolved star, derived from observations made with the Microvariability and Oscillations of STars (MOST) satellite. Our target is the F-class main-sequence star kappa 1 Cet (HD 128898), which was observed over two consecutive runs during 2005-2006. Our analysis reveals that the surface shear rate diminishes at lower latitudes but increases again at mid-latitudes. This pattern suggests the presence of two differentially rotating zones on opposite sides of the equator. In addition to this large-scale feature, we also observe small-scale structures such as spots and plages, which seem to have lifetimes ranging from one month to several months. Lastly, we discuss how our results align with theoretical predictions derived from stellar evolution models. Keywords: Differential rotation; Stellar activity; Spots; Plages; MOST.",
        "ori-fast-z-score": -0.7745966692414834,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": -0.7878385971583353
    },
    {
        "original_text": "We report the discovery of three planets orbiting an intermediate-mass subgiant star, HD 168625 (M = 1.5 Msun), using high-precision radial velocity measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile. The planet masses are 0.7 MJup , 2.1 MJup , and 3.2 MJup . We find that all three planets have orbital periods shorter than 100 days; two of them are close to their host stars (P orb < 10 d). All three planets are located within the habitable zone around their host star. This is one of only four systems known so far where more than one planet has been found inside this region.  These results demonstrate that it should be possible to detect terrestrial planets in the habitable zones of evolved stars by means of Doppler spectroscopy. Keywords: exoplanet, planetary system",
        "watermark_text": "We report the discovery of three planets orbiting an intermediate - weight subgiant star , HD 168625 ( M = 1 . 5 Msun ) , using high - precision radial speed measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile . The planet masses are 0 . 7 MJup , 2 . 1 MJup , and 3 . 2 MJup .We see that all three worlds have orbital periods shorter than 100 hours ; two of them are close to their host stars ( P orb < 10 d ) . All three planets are situated within the habitable zone around their host star .This is one of only four systems available so far where more than one planet has been seen inside this area . These data demonstrate that it should be possible to identify terrestrial worlds in the habitable zones of evolved planets by means of Doppler spectroscopy .Keywords: exoplanet, planetary system",
        "rewrite_text": "We announce the discovery of three planets orbiting the intermediate-mass subgiant star HD 168625 (M = 1.5 Msun), based on high-precision radial velocity measurements taken with the HARPS spectrograph at La Silla Observatory in Chile. The planets have masses of 0.7 MJup, 2.1 MJup, and 3.2 MJup, and all exhibit orbital periods of less than 100 hours, with two of them being in close proximity to their host star (Porb < 10 d). Remarkably, all three planets lie within the habitable zone of their star, making this one of only four known systems with multiple planets located in this region. These findings suggest that it is feasible to discover terrestrial planets within the habitable zones of evolved stars using Doppler spectroscopy. Keywords: exoplanet, planetary system.",
        "ori-fast-z-score": -0.29488391230979427,
        "water-fast-z-score": 3.5386069477175313,
        "rewrite-fast-z-score": -1.3127849234810511
    },
    {
        "original_text": "We study pair production of doubly-charged scalars in association with two jets, which can be induced by both weak gauge bosons (W or Z) and photons via loops involving heavy fermions such as top quarks. We derive constraints on the masses of these particles using current experimental data for W+jets and Z+jets processes collected by ATLAS and CMS experiments at the Large Hadron Collider (LHC). In addition to the standard model backgrounds, we also consider contributions from other new physics models that may have similar signatures. The results are presented in terms of exclusion limits on the mass parameters of various new physics scenarios. Finally, we discuss possible signals of this process at future runs of the LHC. PACS numbers: 12.60.Jv, 13 .85.Rm, 14.80.Ly \nI. INTRODUCTIO N\nThe discovery of neutrinos has opened up an exciting possibility of probing beyond Standard Model (SM), especially its Majorana nature  1  , through their lepton number violating interactions  2  . One interesting scenario is the seesaw mechanism  3  where SM singlet right-handed neutrinos acquire large Majorana masses after electroweak symmetry breaking  4  .\nIn order to test whether the observed light neutrinos are indeed Majorana particles, one needs to look for lepton-number-violating processes mediated by virtual heavy neutrinos  5  . These include neutrinoless double beta decay  6  , tritium beta decay  7  , and charged-current quasielastic scattering  8  . However, it turns out that all these processes suffer from severe astrophysical and/or nuclear matrix element uncertainties  9  . On the other hand, colliders provide clean environments to probe lepton number violation directly  10  . For example, searches for same-sign dileptons  11  and trileptons  12  at hadronic colliders could lead to important information about Majorana neutrinos  13  . Another promising channel is the production of doubly-charge scalar particles  14  , which can occur either through s-channel exchange of neutral gauge bosons  15  or t-channel exchange of heavy ferm",
        "watermark_text": "We research pair production of doubly - charged scalars in association with two jets , which can be induced by both weak gauge bosons ( W or Z ) and photons via loops involving heavy fermions such as top quarks . We derive restrictions on the masses of these ions using current experimental evidence for W + jets and Z + jets interactions collected by ATLAS and CMS observations at the Large Hadron Collider ( LHC ) .In addition to the standard description backgrounds , we also consider contributions from other recent physics systems that might have related signatures . The results are presented in terms of exclusion limits on the mass parameters of several novel physics scenarios .Finally , we explain possible transmissions of this process at next ran of the LHC . PACS scores : 12 . 60 . Jv , 13 . 85 . Rm , 14 . 80 . Ly I . INTRODUCTIO N The observation of neutrinos has opened up an exciting possibility of probing beyond Standard Model ( SM ) , particularly its Majorana nature 1 , through their lepton size violating interactions 2 .One interesting scenario is the seesaw mechanism 3 where SM singlet right - handed neutrinos gain big Majorana masses after electroweak symmetry breaking 4 . In order to test whether the seen light neutrinos are indeed Majorana objects , one needs to search for lepton - number - violating reactions mediated by virtual heavy neutrinos 5 .These include neutrinoless double alpha emission 6 , tritium alpha emission 7 , and charged - current quasielastic emission 8 . However , it turns out that all these mechanisms suffer from severe astrophysical and / or radioactive matrix element uncertainties 9 .On the other hand , colliders provide clean environments to probe lepton total violation directly 10 . For instance , searches for same - sign dileptons 11 and trileptons 12 at hadronic colliders may yield to key information about Majorana neutrinos 13 .Another promising channel is the production of doubly - charge scalar particles 14 , which can occur either through s - channel exchange of neutral gauge bosons 15 or t - channel exchange of heavy ferm",
        "rewrite_text": "We investigate the pair production of doubly-charged scalar particles in association with two jets, which can be triggered by both weak gauge bosons (W or Z) and photons via loop diagrams involving heavy fermions, such as top quarks. Using current experimental data on W + jets and Z + jets interactions obtained from ATLAS and CMS at the Large Hadron Collider (LHC), we establish constraints on the masses of these ions. In addition to the standard background descriptions, we also account for contributions from other recent physics phenomena that could exhibit similar signatures. Our findings are presented as exclusion limits on the mass parameters of various new physics scenarios. Furthermore, we discuss potential implications of this process for the next run of the LHC. \n\nThe observation of neutrinos has unveiled exciting opportunities to explore physics beyond the Standard Model (SM), particularly their Majorana nature, through lepton number violating interactions. One intriguing framework is the seesaw mechanism, wherein SM singlet right-handed neutrinos acquire substantial Majorana masses following electroweak symmetry breaking. To determine if the observed light neutrinos are indeed Majorana particles, it is essential to investigate lepton number violating processes mediated by virtual heavy neutrinos. These processes include neutrinoless double alpha decay, tritium alpha decay, and charged-current quasielastic scattering. However, each of these mechanisms faces significant astrophysical and/or radioactive matrix element uncertainties. In contrast, colliders provide a clean environment to directly examine lepton number violation. For example, searches for same-sign dileptons and trileptons at hadronic colliders can yield valuable insights into the properties of Majorana neutrinos. Another promising avenue is the production of doubly-charged scalar particles, which may occur via s-channel exchanges of neutral gauge bosons or t-channel exchanges of heavy fermions.",
        "ori-fast-z-score": 1.1188618555710317,
        "water-fast-z-score": 7.315635209502899,
        "rewrite-fast-z-score": 0.0854357657716761
    },
    {
        "original_text": "The stopping power for uranium ions was measured at the National Superconducting Cyclotrons Laboratory (NSCL) using a thick target method and an ion chamber placed downstream of the target.  The results are compared to calculations based on the TRIM code, which is used extensively by nuclear physicists studying heavy-ion reactions. A good agreement between experiment and theory has been found over most of the range studied here. However, there appears to be some discrepancy near the end of the range where the experimental data show less stopping than predicted by the TRIM calculation. This may indicate that the TRIM model underestimates the contribution of electronic excitation processes to the total stopping cross section. These measurements provide useful information about the behavior of uranium ions as they slow down through matter. They can also serve as input parameters into models describing the transport of energetic particles in matter. Stopping powers were determined experimentally for uranium ions incident upon thin targets of carbon, copper, gold, silver, lead, tin, aluminum, iron, nickel, titanium, molybdenum, tungsten, tantalum, niobium, zirconium, hafnium, ytterbium, and uranium metal. Measurements were made at NSCL s Heavy Ion Accelerator Facility using a thick-target method and an ion chamber located downstream of the target. Results are presented along with comparisons to theoretical predictions obtained from the TRIM computer program.",
        "watermark_text": "The stopping power for nuclear atoms was measured at the National Superconducting Cyclotrons Laboratory ( NSCL ) using a thick target technique and an ion chamber placed downstream of the target . The results are compared to calculations based on the TRIM code , which is utilized heavily by nuclear physicists studying heavy - ion reactions .A good agreement between experiment and theory has been seen over most of the range studied here . However , there seems to be some discrepancy near the end of the range where the empirical data demonstrate fewer stopping than expected by the TRIM calculation .This might suggest that the TRIM theory underestimates the impact of electronic excitation systems to the total stopping cross area . These measurements give valuable info about the activity of nuclear ions as they slow down through matter .They can also help as input parameters into experiments describing the travel of energetic particles in matter . Stopping powers were determined experimentally for nuclear atoms incident upon thin targets of carbon , copper , gold , platinum , lead , tin , iron , iron , nickel , titanium , molybdenum , tungsten , tantalum , niobium , zirconium , hafnium , ytterbium , and uranium metal .Measurements were made at NSCL s Heavy Ion Accelerator Facility using a thick - target technique and an ion chamber situated downstream of the target . Results are presented along with comparisons to theoretical estimates obtained from the TRIM computer program .",
        "rewrite_text": "The stopping power of nuclear atoms was investigated at the National Superconducting Cyclotron Laboratory (NSCL) using a thick target method and an ion chamber positioned downstream of the target. The findings were compared to calculations derived from the TRIM code, commonly used by nuclear physicists for studies of heavy-ion reactions. Overall, a strong correlation between experimental results and theoretical predictions was observed across most of the examined range. However, discrepancies were noted towards the end of the range, where empirical data indicated lower stopping power than anticipated by the TRIM calculations. This suggests that the TRIM model may not fully account for the influence of electronic excitation on the total stopping cross-section. These measurements provide valuable insight into the behavior of nuclear ions as they traverse matter and can serve as input parameters for experiments focused on the movement of energetic particles through materials. Stopping powers were measured for nuclear atoms interacting with thin targets of various elements, including carbon, copper, gold, platinum, lead, tin, iron, nickel, titanium, molybdenum, tungsten, tantalum, niobium, zirconium, hafnium, ytterbium, and uranium. The experiments were conducted at the NSCL's Heavy Ion Accelerator Facility, employing a thick-target approach and an ion chamber situated downstream of the target. The results are presented alongside comparisons to theoretical estimates generated by the TRIM program.",
        "ori-fast-z-score": 0.2873478855663454,
        "water-fast-z-score": 6.735753140545634,
        "rewrite-fast-z-score": 0.8466487815452375
    },
    {
        "original_text": "The dystrophin-glycoprotein complex (DGC) is an essential component of the muscle cell membrane, which provides structural support to skeletal muscles during contraction.  The DGC consists of several proteins including dystrophin, sarcoglycans, syntrophins, dystrobrevins, and utrophin.   In Duchenne muscular dystrophy (DMD), mutations in the gene encoding for dystrophin lead to loss of this protein from the sarcolemma resulting in severe muscle wasting.  However, it has been shown that expression of utrophin can compensate for the absence of dystrophin by binding to other components of the DGC such as alpha-dystroglycan (alpha-DG).  This study investigated whether processing of beta-dystroglycan affects its ability to bind with utrophin or its homologous counterpart, dp116, using normal mouse Schwann cells (MSC) and mdx mice lacking functional dystrophin.  Beta-dystroglycan was found to be processed into two different forms in both MSCs and mdx sciatic nerves.  One form had a molecular weight similar to that observed in normal mouse brain tissue while another form showed higher mobility when compared to the first one.  Both forms were able to interact with utrophin but only the lower molecular weight form could also bind to dp116.  These results suggest that processing of betadystroglycan may affect its interaction with utrophin and/or dp116.",
        "watermark_text": "The dystrophin - glycoprotein compound ( DGC ) is an essential component of the muscle cellular membrane , which offers functional strength to skeletal muscles during contraction . The DGC contains of several proteins namely dystrophin , sarcoglycans , syntrophins , dystrobrevins , and utrophin .In Duchenne muscular dystrophy ( DMD ) , defects in the gene encoding for dystrophin lead to lack of this protein from the sarcolemma culminating in severe muscle wasting . However , it has been shown that expression of utrophin can alleviate for the absence of dystrophin by binding to other components of the DGC such as alpha - dystroglycan ( beta - DG ) .This study investigated whether processing of beta - dystroglycan affects its capacity to interact with utrophin or its homologous counterpart , dp116 , using normal mice Schwann cells ( MSC ) and mdx mice lacking functional dystrophin . Beta - dystroglycan was shown to be processed into two different versions in both MSCs and mdx sciatic nerves .One version had a molecular weight similar to that detected in regular mouse skull tissue while another form showed higher mobility when compared to the first one . Both types were ability to interact with utrophin but only the smaller molecular weight version could also bind to dp116 .These data suggest that processing of betadystroglycan might impact its interaction with utrophin and / or dp116 .",
        "rewrite_text": "The dystrophin-glycoprotein complex (DGC) is a crucial part of the muscle cell membrane, providing functional support to skeletal muscles during contraction. The DGC consists of several proteins, including dystrophin, sarcoglycans, syntrophins, dystrobrevins, and utrophin. In Duchenne muscular dystrophy (DMD), mutations in the dystrophin gene result in the absence of this protein from the sarcolemma, leading to significant muscle degeneration. However, research has indicated that utrophin expression can compensate for the lack of dystrophin by interacting with other components of the DGC, such as alpha-dystroglycan (beta-DG). This study explored whether the processing of beta-dystroglycan influences its ability to interact with utrophin or its analogue, dp116, using normal mice Schwann cells (MSC) and mdx mice that lack functional dystrophin. It was found that beta-dystroglycan is processed into two distinct forms in both MSCs and the sciatic nerves of mdx mice. One form had a molecular weight comparable to what is found in normal mouse skull tissue, while the other exhibited higher mobility. Both forms interacted with utrophin; however, only the lower molecular weight variant also bound to dp116. These findings suggest that the processing of beta-dystroglycan may affect its interactions with utrophin and/or dp116.",
        "ori-fast-z-score": 1.0660035817780522,
        "water-fast-z-score": 6.901297485020058,
        "rewrite-fast-z-score": 2.429493573646624
    },
    {
        "original_text": "Der Spinecho ist ein technisches Hilfsmittel, das bei der Messung von magnetischen Resonanzspektren verwendet wird.  Dieser Artikel beschreibt die Theorie eines longitudinalen Atomstrahl-Spinechos (LASE), der einen neuen Ansatz für den Einfluß einer Paritätsverlängung auf die Spin-Resonanzfrequenz enthält. Die Berechnungen zeigen, dass eine LASE-Berry-Phasenfunktion mit einem Winkel θ = π/4 gegenüber dem Zeitstrahl existiert. Der Einfluss dieser Phase auf die Frequenzen der Spin-Resonanz ist analysiert. Es ergibt sich daraus, dass die Anzahl der Spin-Resonanten im Bereich der Frequenzen  ω0 - Δω/2, ω0 + Δω/2  um 2θΔω/π erhöht werden kann.",
        "watermark_text": "Der Spinecho ist ein technisches Hilfsmittel , das bei der Messung von magnetischen Resonanzspektren verwendet wird . Dieser Artikel beschreibt die Theorie eines longitudinalen Atomstrahl - Spinechos ( LASE ) , der einen neuen Ansatz mit den Einfluß einer Paritätsverlängung auf die Spin - Resonanzfrequenz enthält .Die Berechnungen zeigen , dass eine LASE - Berry - Phasenfunktion mit einem Winkel θ = π / 4 gegenüber dem Zeitstrahl existiert . Der Einfluss dieser Phase auf die Frequenzen die Spin - Resonanz ist analysiert .Es ergibt sich daraus , dass der Anzahl die Spin - Resonanten um Bereich der Frequenzen ω0 - Δω / 2 , ω0 + Δω / 2 um 2θΔω / π erhöht werden kann .",
        "rewrite_text": "Der Spinecho ist ein technisches Instrument, das zur Messung von magnetischen Resonanzspektren eingesetzt wird. In diesem Artikel wird die Theorie eines longitudinalen Atomstrahl-Spinechos (LASE) behandelt, der einen innovativen Ansatz verfolgt, welcher den Einfluss einer Paritätsverlängerung auf die Spin-Resonanzfrequenz berücksichtigt. Die Berechnungen zeigen, dass eine LASE-Berry-Phasenfunktion mit einem Winkel von θ = π / 4 im Verhältnis zur Zeitachse existiert. Der Einfluss dieser Phase auf die Spin-Resonanzfrequenzen wird analysiert. Es zeigt sich, dass die Anzahl der Spin-Resonanzen im Frequenzbereich ω0 - Δω / 2 bis ω0 + Δω / 2 um 2θΔω / π erhöht werden kann.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.6726124191242437,
        "rewrite-fast-z-score": -0.2773500981126146
    },
    {
        "original_text": "We present the Bridge algorithm, which is an efficient direct tree hybrid n-body method that can be used to simulate fully self-consistently star clusters in galactic potentials with arbitrary mass distributions. The Bridge algorithm combines the advantages of both direct-summation (DS) and tree methods by using DS only within small sub-groups of particles at each time step while solving the full system of equations on a tree. We show that this approach allows us to achieve high accuracy without sacrificing computational efficiency. In particular we demonstrate that our new code reproduces results obtained with the state-of-the-art treecode NBODY6++GPU very well even when simulating systems containing up to 10 million stars. This makes it possible to study the long-term dynamical evolution of open clusters as well as globular clusters orbiting around parent galaxies over many Gyr timescales. \n \n Keywords: Open cluster; Globular cluster; Galactic potential",
        "watermark_text": "We introduce the Bridge algorithm , which is an efficient direct tree hybrid n - bodies method that can be used to simulate fully self - consistently star clusters in galactic potentials with arbitrary mass distributions . The Bridge technique combines the advantages of both direct - summation ( DS ) and tree methods by using DS only within tiny sub - families of atoms at each time step while solving the full system of equations on a tree .We suggest that this methodology allows us to achieve high efficiency without sacrificing computational efficiency . In particular we prove that our new code reproduces data derived with the state - of - the - art treecode NBODY6 + + GPU very best even when simulating complexes containing up to 10 million stars .This gives it able to study the long - term dynamical development of open complexes as well as globular complexes orbiting around parent objects over many Gyr timescales . Keywords : Open cluster ; Globular cluster ; Galactic potential",
        "rewrite_text": "We present the Bridge algorithm, an efficient direct tree hybrid method for simulating fully self-consistent star clusters within galactic potentials characterized by arbitrary mass distributions. The Bridge technique leverages the strengths of both direct summation (DS) and tree methods by applying direct summation exclusively within small sub-families of particles at each time step, while simultaneously solving the entire system of equations using a tree structure. This approach enables us to achieve high efficiency without compromising computational performance. Notably, we demonstrate that our new code accurately reproduces results obtained from the advanced treecode NBODY6++ GPU, even for simulations involving complex environments with up to 10 million stars. This capability allows for the exploration of the long-term dynamical evolution of both open clusters and globular clusters orbiting parent objects over timescales spanning several billion years. Keywords: Open cluster; Globular cluster; Galactic potential.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 3.771236166328254,
        "rewrite-fast-z-score": -1.0392304845413263
    },
    {
        "original_text": "We present an extension to the standard model that includes gravity, based on Feynman s ideas about quantum gravity. The extended standard model is formulated in terms of gauge fields for all known interactions (including gravity) and fermions with spin 1/2 or 1. We show how this theory can be derived from first principles using Feynman diagrams. In addition we discuss some phenomenological consequences such as neutrino masses and dark matter candidates. Finally we comment briefly on possible experimental tests of our proposal. This work was supported by NSF grant PHY-0456747. A theory of everything should include gravity along with other fundamental forces. Here we propose one such theory which extends the standard model including gravitational effects. Our approach follows closely Feynman s original idea of formulating quantum gravity in terms of gauge fields coupled to fermions. Using Feynman diagrams we derive the extended standard model from first principles. Some phenomenological consequences are discussed, e.g., neutrino mass generation via seesaw mechanisms and dark matter candidates. Possible experiments testing our proposal are also mentioned.",
        "watermark_text": "We introduce an addition to the standard theory that encompasses gravity , built on Feynman s ideas about particle gravity . The extended standard theory is implemented in terms of gauge fields for all known interactions ( including gravity ) and fermions with spin 1 / 2 or 1 .We see how this theory can be derived from first principles utilizing Feynman diagrams . In addition we explain some phenomenological consequences such as neutrino masses and dark matter candidates .Finally we comment briefly on potential experimental tests of our proposal . This project was supported by NSF grant PHY - 0456747 .A description of things should involve gravity along with other fundamental forces . Here we undertake one such idea which extends the standard theory including gravitational interactions .Our formulation follows carefully Feynman s earlier notion of formulating quantum gravitational in terms of gauge fields coupled to fermions . Using Feynman diagrams we derive the extended standard description from first principles .Some phenomenological consequences are discussed , e . g . , neutrino mass development via seesaw processes and dark matter candidates . Possible experiments testing our proposal are also discussed .",
        "rewrite_text": "We present an enhancement to the standard theory that includes gravity, building on Feynman's concepts regarding particle gravity. This extended standard theory employs gauge fields to account for all known interactions, including gravity, as well as fermions with spin 1/2 or 1. We demonstrate how this theory can be derived from foundational principles using Feynman diagrams. Additionally, we discuss several phenomenological implications, such as the emergence of neutrino masses and possible candidates for dark matter. Finally, we offer brief commentary on potential experimental tests of our approach. This research was made possible by NSF grant PHY - 0456747. Our work aims to incorporate gravity along with other fundamental forces, extending the standard framework to include gravitational interactions. Our formulation is consistent with Feynman’s original idea of describing quantum gravity through gauge fields that couple to fermions. We derive the extended standard model from first principles using Feynman diagrams and explore various phenomenological outcomes, including neutrino mass generation through seesaw mechanisms and dark matter candidates. We also consider possible experiments to test our framework.",
        "ori-fast-z-score": 1.1531133203941102,
        "water-fast-z-score": 8.07179324275877,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We have investigated whether pulsar timing observations can be used to detect gravitational waves by observing the Shapiro effect, which is caused by the passage of gravitational waves through space-time around the Earth and its companion star (the Sun). We find that this method will not work with current technology because it requires very accurate measurements of pulse arrival times over many years. However, we show how future radio telescopes could make such measurements if they are equipped with new technologies like phased-array feeds or digital backends. In addition, we discuss other possible methods using pulsar timing data to search for gravitational waves. Gravitational waves cause time delays between pulses emitted at different points on the surface of a rotating neutron star. These time delays depend on the distance between these points as well as their position relative to the line-of-sight towards the observer. The most prominent effects occur when the wave passes close to the Earth and its companion stars. This causes an additional delay known as the Shapiro effect  Shapiro 1964  . If one knows the positions of all pulsars within a globular cluster, then one can use the observed pulse arrival times to measure the distances between them. By comparing these measured distances with those predicted by general relativity, one can test whether Einstein s theory correctly describes gravity on large scales.",
        "watermark_text": "We have researched whether pulsar timing observations can be used to identify gravitational waves by observing the Shapiro phenomenon , which is caused by the travel of gravitational waves through space - time around the Earth and its companion star ( the Sun ) . We see that this process will not work with current technology because it takes very accurate measurements of pulse onset times over numerous years .However , we explain how new radio telescopes might making such measurements if they are installed with modern innovations like phased - array feeds or digital backends . In addition , we explain other possible methods using pulsar timing statistics to search for gravitational waves .Gravitational waves cause period delays between pulses emitted at different places on the surface of a rotating neutron star . These time delays rely on the distance between these points as also as their placement relative to the line - of - view towards the observer .The most notable effects happen when the wave passes close to the Earth and its companion stars . This creates an additional delay known as the Shapiro phenomenon Shapiro 1964 .If one understands the places of all pulsars within a globular cluster , then one can using the known pulse onset times to measure the distances between them . By matching these measured distances with those predicted by general relativity , one can test whether Einstein s concept correctly describes gravitational on huge scales .",
        "rewrite_text": "We investigated the potential for using pulsar timing observations to detect gravitational waves through the Shapiro phenomenon, which occurs when gravitational waves propagate through the space-time surrounding Earth and its companion star, the Sun. Our findings indicate that current technology is not sufficient for this method, as it requires highly precise measurements of pulse onset times collected over many years. However, we discuss how advancements in new radio telescopes, particularly with implementations like phased-array feeds or digital backends, could enable such measurements in the future. Additionally, we explore other possible approaches that utilize pulsar timing statistics to search for gravitational waves. Gravitational waves result in time delays between pulses that are emitted from different locations on the surface of a rotating neutron star, with these delays dependent on the distances between emission points and their orientation relative to the observer. The most significant effects occur when the wave is in close proximity to Earth and its companion stars, resulting in what is known as the Shapiro delay (Shapiro, 1964). By accurately determining the locations of all pulsars within a globular cluster, we can utilize known pulse onset times to calculate the distances between them. By comparing these measured distances with predictions made by general relativity, we can assess the validity of Einstein’s theories on a cosmic scale.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.619839033907137,
        "rewrite-fast-z-score": 1.0536089137432665
    },
    {
        "original_text": "We present Herschel Space Observatory observations at 70, 160, 250, 350, and 500 microns toward two fields centered on the densest parts of the Rho Ophiuchi (RO) molecular cloud complex. The data are used to derive the temperature distribution within dense cores identified by their infrared emission using the method developed by John Myers & Sean Carey. We find that most of these cores have temperatures between 10 K and 20 K with only one colder than 8 K. This is consistent with previous studies showing that cold cores are rare in star-forming clouds. Using our derived temperatures we calculate masses assuming optically thin greybody emission. These masses range from 0.1 Msun to more than 100 Msun. In addition, we use the same dataset to study the properties of protostars embedded in the RO region. We identify 16 Class I sources based on their spectral energy distributions and compare them to those found in other nearby star-forming regions such as Serpens South or Orion B North.",
        "watermark_text": "We present Herschel Space Observatory images at 70 , 160 , 250 , 350 , and 500 microns toward two fields centered on the densest parts of the Rho Ophiuchi ( RO ) molecular mist complex . The data are using to derive the temperature balance within dense cores identified by their infrared emission utilizing the method developed by John Myers & Sean Carey .We see that most of these cores have temperatures between 10 K and 20 K with only one colder than 8 K . This is consistent with previous research indicating that cool cores are scarce in star - creating clouds . Using our derived temperatures we estimate masses assuming optically thin greybody emission .These masses range from 0 . 1 Msun to more than 100 Msun . In addition , we utilize the same dataset to study the properties of protostars embedded in the RO region .We distinguish 16 Class I sources based on their spectral power distributions and contrast them to those present in other nearby star - creating areas such as Serpens South or Orion B North .",
        "rewrite_text": "We present images from the Herschel Space Observatory at wavelengths of 70, 160, 250, 350, and 500 microns, focusing on two areas centered around the densest regions of the Rho Ophiuchi (RO) molecular cloud complex. These images are analyzed to determine the temperature equilibrium within dense cores identified by their infrared emissions, using the approach developed by John Myers and Sean Carey. Our results show that most cores have temperatures ranging from 10 K to 20 K, with only one core exhibiting a temperature lower than 8 K. This finding aligns with previous studies suggesting that cool cores are rare in regions of star formation. Based on the temperatures obtained, we estimate the masses of these cores under the assumption of optically thin greybody emission, with values spanning from 0.1 M☉ to over 100 M☉. Additionally, we analyze the same dataset to investigate the characteristics of protostars located within the RO region. We identify 16 Class I sources based on their spectral energy distributions and compare them to sources found in other nearby star-forming regions, such as Serpens South and Orion B North.",
        "ori-fast-z-score": 0.5163977794943222,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an approach to temporal runtime verification based on monadic difference logic (MDL). MDL is a decidable fragment of first-order logic with the ability to express properties about sequences of states, such as those produced by model checkers or execution traces. We show how to translate formulas in this logic into automata and use these automata for monitoring executions against specifications expressed in MDL. Our translation preserves satisfiability and allows us to monitor infinite state systems. The resulting monitors are efficient enough that they can be used online during program execution. In addition we provide a method for reducing the size of the generated automata which makes them suitable for offline analysis. Finally, we demonstrate our technique on several examples including a Java implementation of the dining philosophers problem. This work was supported by NSF grant CCF-0430018. Keywords: Runtime verification, Automata theory, Model checking",
        "watermark_text": "We present an perspective to temporal runtime verification relying on monadic difference reasoning ( MDL ) . MDL is a decidable fragment of first - order calculus with the ability to define properties about strings of states , such as those generated by model checkers or execution traces .We see how to translate formulas in this reasoning into automata and use these automata for monitoring executions against specifications expressed in MDL . Our translation retains satisfiability and allows us to observe infinite state systems .The produced screens are powerful sufficient that they can be used internet during program processing . In addition we provide a technique for decreasing the size of the produced automata which makes them useful for offline analysis .Finally , we demonstrate our technique on several examples including a Java implementation of the dining philosophers problem . This work was supported by NSF grant CCF - 0430018 .Keywords : Runtime verification , Automata analysis , Model checking",
        "rewrite_text": "We introduce a perspective on temporal runtime verification that utilizes monadic difference reasoning (MDL). MDL serves as a decidable subset of first-order calculus, allowing us to define properties related to sequences of states, which can be produced by model checkers or execution traces. We demonstrate how to convert formulas from this reasoning framework into automata, enabling the monitoring of executions against specifications articulated in MDL. Our translation preserves satisfiability and provides the capability to observe infinite state systems. The resulting automata are sufficiently powerful for use on the internet during program execution. Additionally, we present a method for reducing the size of these automata, enhancing their applicability for offline analysis. Finally, we illustrate our approach through various examples, including a Java implementation of the dining philosophers problem. This research was supported by NSF grant CCF-0430018. \n\nKeywords: Runtime verification, Automata analysis, Model checking.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 4.093146241443879,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We report on non-adiabatic effects in dissociative oxygen adsorption and desorption processes occurring at low temperatures (<100 K). The experiments were performed using an ultrahigh vacuum scanning tunneling microscope equipped with a molecular beam source for dosing O 2 molecules onto clean, well-ordered Al(111) surfaces held at different sample temperatures between 10 and 100 K. We find that the sticking probability decreases strongly when increasing the surface temperature due to thermal activation of vibrational modes which lead to non-collinearity of electronic states involved in the reaction process. This effect is also observed during the subsequent desorption of atomic oxygen from the surface. In addition we observe a pronounced dependence of the sticking coefficient on the kinetic energy of incident oxygen molecules: At high energies (>500 meV), where the molecule-surface interaction time becomes comparable or even shorter than typical vibrational periods, the sticking probability increases again as compared to lower kinetic energies.",
        "watermark_text": "We report on non - adiabatic effects in dissociative oxygen adsorption and desorption processes occurring at low temperatures ( < 100 K ) . The experiments were performed using an ultrahigh vacuum scanning tunneling microscope equipped with a molecular beam source for dosing O 2 molecules onto clean , well - ordered Al ( 111 ) surfaces held at different sample temperatures between 10 and 100 K . We find that the sticking probability decreases strongly when increasing the surface temperature due to thermal activation of vibrational modes which lead to non - collinearity of electronic states involved in the reaction process .This phenomenon is also observed during the subsequent desorption of atomic oxygen from the surface . In addition we study a noticeable dependence of the sticking coefficient on the kinetic power of incident oxygen molecules : At high energies ( > 500 meV ) , where the molecule - surface interaction rate gets comparable or especially shorter than typical vibrational intervals , the sticking likelihood grows again as compared to higher kinetic energies .",
        "rewrite_text": "We investigate non-adiabatic effects in the processes of oxygen adsorption and desorption that occur at low temperatures (below 100 K). Our experiments utilized an ultrahigh vacuum scanning tunneling microscope combined with a molecular beam source to introduce O2 molecules onto clean, well-ordered Al (111) surfaces, which were maintained at various temperatures ranging from 10 to 100 K. Our findings reveal that as the surface temperature increases, the sticking probability declines significantly due to the thermal activation of vibrational modes, leading to the non-collinearity of the electronic states involved in the reaction. This behavior is also evident during the subsequent desorption of atomic oxygen from the surface. Furthermore, we observe a marked dependence of the sticking coefficient on the kinetic energy of the incoming oxygen molecules. At elevated energies (above 500 meV), where the rate of molecule-surface interactions is comparable to or shorter than typical vibrational intervals, the likelihood of sticking actually increases relative to that observed at higher kinetic energies.",
        "ori-fast-z-score": 0.22086305214969307,
        "water-fast-z-score": 3.092082730095703,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We present the theory behind relativistic fluctuation theorems, which are exact relations between entropy production in nonequilibrium processes and fluctuations in equilibrium states. We show that these results can be derived using only standard statistical mechanics techniques applied to systems with time-reversal symmetry breaking interactions. In particular we derive an expression for the entropy production rate in terms of correlation functions at thermal equilibrium. This result is used to calculate the entropy production rates associated with several simple models including Brownian motion, Langevin dynamics, and driven harmonic oscillators. Finally, we discuss how our approach may be extended beyond classical physics. Relativistic fluctuation theorems provide exact relations between entropy production during non-equilibrium processes and fluctuations in corresponding equilibrium states. These results have been obtained by applying standard statistical mechanics methods to systems with broken timereversal invariance. Here we use this formalism to obtain expressions for the entropy production rate as well as other quantities such as heat currents in terms of correlation functions evaluated at thermal equilibrium. As concrete applications we consider several simple models including Browninan motion, Langevin dynamics and driven harmonic oscillators. \n \n 1 Introduction \n \n Entropy production plays a central role in many areas of science ranging from biology  1  , chemistry  2  , geophysics  3  , and neuroscience  4  . It has also become increasingly important in quantum information processing  5  where it provides a measure of irreversibility  6  . Despite its importance there remains no general method for calculating entropy production rates except in very special cases  7–9  . Recently, however, new theoretical tools based on fluctuation theorems  10–12  have emerged which allow one to relate entropy production directly to measurable properties of physical systems  13–18  . For example, in recent years there has been considerable interest in developing experimental schemes  19–21  capable of measuring entropy production rates in small isolated quantum systems  22  . Such experiments would enable direct tests of fundamental thermodynamic principles  23  and could potentially lead to practical devices for extracting work from heat baths  24  . \n \n 2 Classical fluctuation theorems \n \n Perhaps the most famous fluctuation theorem was first proposed by Jarzynski  10 ",
        "watermark_text": "We introduce the principle behind relativistic fluctuation theorems , which are exact relations between entropy production in nonequilibrium systems and fluctuations in equilibrium states . We see that these results can be derived using only typical statistical mechanics algorithms used to systems with time - reversal symmetry breaking processes .In particular we derive an definition for the entropy production rate in terms of correlation functions at heat equilibrium . This result is utilized to estimate the entropy production rates associated with many simple theories including Brownian movement , Langevin mechanics , and driven harmonic oscillators .Finally , we explain how our approach may be generalized beyond classical physics . Relativistic fluctuation theorems allow exact relations between entropy production during non - equilibrium processes and fluctuations in associated equilibrium states .These conclusions have been achieved by using conventional statistical mechanics models to systems with broken timereversal invariance . Here we utilize this formalism to obtain definitions for the entropy production speed as well as other quantities such as heat currents in terms of correlation functions evaluated at heat equilibrium .As concrete uses we treat various simple theories including Browninan movement , Langevin mechanics and driven harmonic oscillators . 1 Introduction Entropy production plays a central role in multiple fields of science diverse from biology 1 , chemistry 2 , geophysics 3 , and neuroscience 4 .It has additionally grown increasingly important in quantum information processing 5 where it gives a estimate of irreversibility 6 . Despite its significance there survives no general technique for calculating entropy production rates except in very exceptional cases 7 – 9 .Recently , however , new theoretical tools based on fluctuation theorems 10 – 12 have arose which allow one to relate entropy production immediately to measurable properties of physical structures 13 – 18 . For instance , in recent months there has been substantial interest in establishing experimental schemes 19 – 21 capable of calculating entropy production rates in small scattered quantum systems 22 .Such experiments would enable direct tests of fundamental thermodynamic principles 23 and could potentially lead to practical devices for extracting work from heat baths 24 . 2 Classical fluctuation theorems Perhaps the most famous fluctuation theorem was first proposed by Jarzynski 10",
        "rewrite_text": "We present the foundational concepts behind relativistic fluctuation theorems, which reveal exact relationships between entropy production in non-equilibrium systems and fluctuations in equilibrium states. These findings can be derived using typical statistical mechanics techniques applicable to systems exhibiting time-reversal symmetry breaking. Specifically, we establish a definition for the entropy production rate using correlation functions at thermal equilibrium. This formulation is then applied to estimate entropy production rates in a variety of simple theories, including Brownian motion, Langevin dynamics, and driven harmonic oscillators. Furthermore, we discuss the potential for extending our methodology beyond classical physics. Relativistic fluctuation theorems provide precise connections between entropy production during non-equilibrium processes and the fluctuations present in related equilibrium states. This work has utilized standard statistical mechanics models for systems with broken time-reversal invariance to derive definitions for the entropy production rate along with quantities such as heat currents, expressed in terms of correlation functions at thermal equilibrium. As practical examples, we analyze several straightforward theories, such as Brownian motion, Langevin mechanics, and driven harmonic oscillators.\n\n1. Introduction: Entropy production is a crucial aspect across various scientific fields, including biology, chemistry, geophysics, and neuroscience. Its relevance has also surged in the realm of quantum information processing, where it serves as a measure of irreversibility. Despite its importance, there remains no widely applicable method for calculating entropy production rates, except in a few exceptional scenarios. Recently, however, novel theoretical frameworks based on fluctuation theorems have emerged, enabling a direct connection between entropy production and measurable characteristics of physical systems. Notably, there has been significant interest in developing experimental setups capable of measuring entropy production rates in small, isolated quantum systems. Such experiments could provide direct tests of fundamental thermodynamic principles and potentially pave the way for practical devices designed to extract work from thermal baths.\n\n2. Classical Fluctuation Theorems: Perhaps the most renowned fluctuation theorem was initially proposed by Jarzynski.",
        "ori-fast-z-score": 0.37371754637596794,
        "water-fast-z-score": 9.391485505499116,
        "rewrite-fast-z-score": 0.3823595564509363
    },
    {
        "original_text": "We present the results on the metallicity distribution functions (MDFs) for stars at different latitudes and distances from the Galactic plane, based on spectroscopic data obtained by the Sloan Digital Sky Survey (SDSS). We find that MDFs are similar to each other within errors except those at |b| > 30° where there is an excess of metal-poor stars compared to the disk population. The fraction of metal-poor stars increases towards higher latitude. This suggests that the halo component becomes more dominant as one goes farther away from the Galactic plane. In addition we also found that the mean metallicities decrease slightly toward larger distance from the Galactic center. These findings suggest that the outer part of our Galaxy has been formed through accretion processes. \n \n Keywords: Metallicity Distribution Function; Halo; Disk; High Latitude Stars; Sloan Digital Sky Survey \n \n 1 Introduction \n \n It is well known that the Milky Way consists of three main components -the thin disk, thick disk and halo. However, it remains unclear how these components were assembled during its formation history. To understand this process, it is important to study their chemical compositions separately because they may have experienced different evolutionary histories. For example, the age-metallicity relation shows that the halo was formed earlier than the disk(e.g., Twarog 1980), while the abundance ratios such as  Fe/H  show that the halo contains many old low-mass stars which should be destroyed by supernova explosions if the halo had been formed recently like the disk(e. g., Nissen & Schuster 1997). \n \n Many studies have investigated the properties of the halo using various samples of distant halo stars selected mainly from proper motion surveys or photometric parallax measurements. Recently, large spectroscopic surveys such as the Sloan Digital Sky Surveys (SDSS) (York et al. 2000) , RAVE survey (Steinmetz 2003 )and SEGUE survey (Yanny et al. 2009 )have provided us with much better information about the chemical composition of the halo. Using",
        "watermark_text": "We present the conclusion on the metallicity distribution functions ( MDFs ) for stars at different latitudes and distances from the Galactic plane , using on spectroscopic data derived by the Sloan Digital Sky Survey ( SDSS ) . We see that MDFs are comparable to each other within errors except those at | b | > 30° where there is an accumulation of steel - weak stars compared to the disk community .The percentage of metal - low stars increases towards higher latitude . This implies that the halo element increases more prevalent as one goes deeper away from the Galactic jet .In addition we also discovered that the mean metallicities reduce slightly toward larger distance from the Galactic center . These studies imply that the exterior part of our Galaxy has been formed through accretion cycles .Keywords : Metallicity Distribution Function ; Halo ; Disk ; High Latitude Stars ; Sloan Digital Sky Survey 1 Introduction It is well established that the Milky Way consists of three principal components - the narrow disk , thick disk and halo . However , it remains unsure how these constituents were assembled during its formation history .To understand this process , it is important to study their chemical compositions separately because they may have experienced distinct evolutionary histories . For instance , the age - metallicity relation shows that the halo was formed earlier than the disk ( e . g . , Twarog 1980 ) , while the density proportions such as Fe / H indicate that the halo contains much young high - density stars which should be killed by supernova explosions if the halo had been formed recently like the disk ( e . g . , Nissen & Schuster 1997 ) .Many experiments have explored the properties of the halo utilizing diverse samples of distant halo stars selected mainly from proper motion surveys or photometric parallax observations . Recently , large spectroscopic studies such as the Sloan Digital Sky Surveys ( SDSS ) ( York et al .2000 ) , RAVE study ( Steinmetz 2003 ) and SEGUE study ( Yanny et al . 2009 ) have provided us with far better details about the chemical composition of the halo .Using",
        "rewrite_text": "We present our findings on the metallicity distribution functions (MDFs) of stars at varying latitudes and distances from the Galactic plane, utilizing spectroscopic data sourced from the Sloan Digital Sky Survey (SDSS). Our analysis reveals that the MDFs are largely consistent within their error margins, with the exception of those at |b| > 30°, where there is a notable increase in metal-poor stars compared to the disk population. Additionally, we observe that the proportion of metal-poor stars rises with increasing latitude, suggesting that halo stars become more common as one moves farther from the Galactic plane. We also found that mean metallicities tend to decrease slightly with greater distance from the Galactic center. These observations indicate that the outer regions of our Galaxy likely formed through various accretion processes.\n\nKeywords: Metallicity Distribution Function; Halo; Disk; High Latitude Stars; Sloan Digital Sky Survey\n\n1. Introduction  \nIt is widely recognized that the Milky Way comprises three main components: the thin disk, the thick disk, and the halo. However, the exact processes that led to their formation remain uncertain. To unravel this mystery, it is crucial to examine the chemical compositions of these components, as they may have undergone different evolutionary paths. For example, the age-metallicity relation suggests that the halo formed earlier than the disk (e.g., Twarog 1980), while measurements such as the iron-to-hydrogen ratio (Fe/H) imply that the halo contains young, high-density stars that would likely have been eliminated by supernovae had the halo formed more recently, like the disk (e.g., Nissen & Schuster 1997). Numerous studies have investigated the properties of the halo using various samples of distant halo stars, primarily selected through proper motion surveys or photometric parallax methods. Recently, large-scale spectroscopic efforts, including the Sloan Digital Sky Survey (SDSS) (York et al. 2000), the RAVE survey (Steinmetz 2003), and the SEGUE study (Yanny et al. 2009), have significantly enhanced our understanding of the halo's chemical composition.",
        "ori-fast-z-score": -0.26013299085723596,
        "water-fast-z-score": 8.419756985347481,
        "rewrite-fast-z-score": -0.5035088149780135
    },
    {
        "original_text": "We present the results of N-body simulations for open and globular star clusters with different initial conditions, including primordial binaries in various proportions (from 0 to 100%). We find that the fraction of binaries among all stars decreases as the cluster evolves due to dynamical interactions between single and binary stars. The decrease is more pronounced if there are initially many hard binaries or few soft ones. In addition, we show how the number of binaries depends on their binding energy distribution at birth. Finally, we compare our results with observations of real open and globular clusters. Our main conclusions are:  1) Open clusters have fewer binaries than globulars because they lose most of them during early evolution.  2) Binaries can be destroyed by three-body encounters even when the total number of binaries remains constant.  3) Hard binaries dominate over soft ones after several relaxation timescales t rh .",
        "watermark_text": "We present the conclusion of N - bodies simulations for open and globular star clusters with various initial conditions , covering primordial binaries in different proportions ( from 0 to 100 % ) . We see that the fraction of binaries among all stars reduces as the cluster evolves due to dynamical interactions between single and binary galaxies .The decrease is more pronounced if there are initially multiple tough binaries or few hard ones . In addition , we study how the number of binaries depends on their binding energy flow at birth .Finally , we compare our findings with observations of real open and globular nuclei . Our main results are : 1 ) Open clusters have fewer binaries than globulars because they losing most of them during early evolution .2 ) Binaries can be damaged by three - bodies interactions even when the total number of binaries remains constant . 3 ) Hard binaries dominate over soft ones after many relaxation timescales t rh .",
        "rewrite_text": "We present our findings from N-body simulations of open and globular star clusters with varying initial conditions, including different proportions of primordial binaries ranging from 0% to 100%. Our results indicate that the fraction of binaries among the stars decreases as the cluster evolves, primarily due to dynamical interactions between single stars and binary systems. This reduction is particularly significant when there are initially many hard binaries or few tough ones. Furthermore, we explore the dependence of binary numbers on their binding energy at formation. Lastly, we compare our results with observational data from actual open and globular clusters. Our key conclusions are: 1) Open clusters contain fewer binaries than globular clusters, as they lose a significant number during their early evolution; 2) Binaries can be disrupted by three-body interactions even if the overall number of binaries remains unchanged; and 3) After multiple relaxation timescales (t_rh), hard binaries become more prevalent than soft ones.",
        "ori-fast-z-score": -1.6378460497066512,
        "water-fast-z-score": 4.0,
        "rewrite-fast-z-score": -0.11547005383792514
    },
    {
        "original_text": "In this paper, we propose an autonomous distributed admission control scheme to improve the performance and fairness in wireless local area networks (WLANs). The proposed scheme is based on the concept that each station maintains its own queue length information by using the packet inter-arrival time at the physical layer. In addition, it uses the number of active stations as well as their transmission rates to determine whether or not new connections are admitted into the network. We show through simulation results that our scheme can achieve better throughput than existing schemes while maintaining good fairness among competing stations. Keywords: Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement. 1 Introduction With the rapid development of mobile computing devices such as laptops, PDAs, smart phones etc., there has been growing interest in providing high quality services over wireless local area networks (WLANS)  1  . However, due to limited bandwidth resources available in WLANs, efficient resource management becomes crucially important  2  .\nThe most widely used medium access control protocol in current commercial WLAN products is the IEEE 802.11 Distributed Coordination Function (DCF), which provides both contention-based channel access mechanism called Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA)  3  , and contention-free service via Point Coordinated Function (PCF)  4  . Although CSMA/CA allows multiple stations to share the same radio channel simultaneously without any centralized coordination, it suffers from poor system performance when the traffic load increases  5  . This problem is mainly caused by the hidden terminal effect  6  where two nodes may transmit packets to one another simultaneously causing collisions. To alleviate these problems, several approaches have been proposed  7 -10  . Among them, the authors in  8  introduced a simple but effective method known as Virtual Reservation Channel (VRC) to reduce the probability of collision between data frames transmitted by different stations. They also presented a modified version of VRC  9  to further enhance the performance of CSMA/CA under heavy loads. However, all these works assume that the number of active stations within the",
        "watermark_text": "In this paper , we propose an autonomous spread entry system scheme to ensure the performance and fairness in mobile regional region systems ( WLANs ) . The proposed system is based on the idea that each broadcaster keeps its own queue length information by using the message inter - arrival rate at the physical layer .In addition , it utilizes the quantity of active stations as well as their transmission rates to estimate whether or not fresh connections are admitted into the channel . We see through simulation data that our scheme can attain better throughput than existing schemes while maintaining good fairness among competing networks .Keywords : Wireless Local Area Networks , Packet Inter - Arrival Time , Fairness , Throughput Improvement . 1 Introduction With the fast development of mobile computing devices such as laptops , PDAs , smart phones etc . , there has been growing interest in providing superior grade services over wireless regional area networks ( WLANS ) 1 .However , owing to limited bandwidth assets available in WLANs , efficient resource control becomes crucially vital 2 . The most commonly used medium access control protocol in current consumer WLAN applications is the IEEE 802 . 11 Distributed Coordination Function ( DCF ) , which offers both contention - based network entry method called Carrier Sense Multiple Access with Collision Avoidance ( CSMA / CA ) 3 , and contention - free service via Point Coordinated Function ( PCF ) 4 .Although CSMA / CA enables many affiliates to share the same radio channel simultaneously without any centralized coordination , it suffers from poor system performance when the traffic burden rises 5 . This problem is mainly caused by the concealed terminal effect 6 where two nodes may transmit streams to one another simultaneously creating collisions .To reduce these problems , various approaches have been proposed 7 - 10 . Among them , the published in 8 developed a simple but effective technology called as Virtual Reservation Channel ( VRC ) to reduce the probability of collision between information frames transmitted by separate stations .They even presented a altered version of VRC 9 to further enhance the performance of CSMA / CA under heavy loads . However , all these works assume that the number of active stations within the",
        "rewrite_text": "In this paper, we introduce an autonomous spread entry system designed to enhance performance and fairness in mobile regional systems, specifically in Wireless Local Area Networks (WLANs). Our proposed approach relies on individual broadcasters maintaining their own queue length information, which is derived from the message inter-arrival rates at the physical layer. Additionally, it takes into account the number of active stations and their transmission rates to determine whether to allow new connections into the channel. Simulation results indicate that our system outperforms existing methods in terms of throughput while ensuring fair access among competing networks.\n\n**Keywords:** Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement. \n\n**1. Introduction**   \nWith the rapid advancement of mobile computing devices such as laptops, PDAs, and smartphones, there is an increasing demand for high-quality services over wireless local area networks (WLANs). However, due to the limited bandwidth resources available in WLANs, efficient resource management has become critically important. The most prevalent medium access control protocol utilized in current consumer WLAN applications is the IEEE 802.11 Distributed Coordination Function (DCF), which provides both a contention-based access method known as Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) and a contention-free service through Point Coordination Function (PCF). While CSMA/CA allows multiple users to access the same radio channel without centralized control, it struggles with performance issues when traffic loads increase significantly. This degradation in performance is primarily attributed to the hidden terminal problem, where two nodes may transmit data simultaneously, leading to collisions. Various strategies have been proposed to mitigate these challenges. Notably, one such method introduced the Virtual Reservation Channel (VRC) to reduce the likelihood of collisions among frames transmitted from different stations, and an improved version of VRC was also suggested to enhance CSMA/CA performance under heavy load conditions. However, all these studies assume a fixed number of active stations within the system.",
        "ori-fast-z-score": -1.0999438818457405,
        "water-fast-z-score": 8.485281374238571,
        "rewrite-fast-z-score": 0.8892972917998876
    },
    {
        "original_text": "We present the results on the super quantum mechanics (SQM) in dimer models with free fermion boundary conditions. We show that SQM is equivalent to the usual quantum mechanics for periodic boundary condition case. In addition we find that there are two different types of supersymmetric ground states depending on whether or not the number of particles N is even. For odd N , the ground state has zero energy but it does not have any degeneracy. On the other hand, when N is even, the ground state becomes doubly degenerate. The degeneracy can be understood by using the concept of parity symmetry breaking. Finally we discuss how our result fits into the general framework of supersymmetry. PACS: 11.10.Wx, 05.45.Mt, 11.15.Tk \nI. INTRODUCTORY REMARK\nIn this work we study the super quantum mechanics ( S Q M ) i n d im e r m odels w ith f ree fermi b oundary c o ndition s . T hese models were first introduced by Rokhsar et al  1  as an exactly solvable model which describes spin-1/2 Heisenberg antiferromagnet on a square lattice. They showed that these models exhibit many interesting properties such as spontaneous dimerization at low temperatures  2  .\nThe main purpose of this work is to investigate the effect of boundary conditions on the supersymmetric structure of the system. It turns out that the supersymmetric structure depends crucially on the boundary conditions imposed on the system. As will become clear later, the supersymmetric structure changes drastically if one switches between periodic and free-fermion boundary conditions.",
        "watermark_text": "We present the results on the super quantum mechanics ( SQM ) in dimer models with free fermion boundary rules . We see that SQM is analogous to the usual quantum mechanics for periodic border condition case .In addition we find that there are two different kinds of supersymmetric ground laws varying on whether or not the quantity of particles N is even . For odd N , the ground state has zero energy but it does not have any degeneracy .On the other hand , when N is even , the ground state remains doubly degenerate . The degeneracy can be understood by using the notion of parity symmetry breaking .Finally we talk how our consequence fits into the broad context of supersymmetry . PACS : 11 . 10 . Wx , 05 . 45 . Mt , 11 . 15 . Tk I .INTRODUCTORY REMARK In this research we study the super quantum mechanics ( S Q M ) i n d im e r m odels v ith f ree fermi b oundary c o ndition s . T hese models were first described by Rokhsar et al 1 as an exactly solvable theory which presents spin - 1 / 2 Heisenberg antiferromagnet on a square lattice .They showed that these models exhibit several interesting properties such as spontaneous dimerization at low temperatures 2 . The main aim of this research is to examine the impact of boundary rules on the supersymmetric composition of the system .It happens out that the supersymmetric structure depends crucially on the boundary rules imposed on the system . As will become clear afterwards , the supersymmetric geometry changes significantly if one switches between periodic and free - fermion boundary conditions .",
        "rewrite_text": "In this study, we report our findings on super quantum mechanics (SQM) within dimer models that employ free fermion boundary conditions. We observe that SQM behaves similarly to standard quantum mechanics in systems with periodic boundary conditions. Additionally, we identify two distinct types of supersymmetric ground states, which depend on the parity of the particle number, N. For odd N, the ground state possesses zero energy and lacks degeneracy. Conversely, when N is even, the ground state remains doubly degenerate. This degeneracy can be explained through the concept of parity symmetry breaking. Finally, we discuss how our findings relate to the broader field of supersymmetry.\n\n### I. INTRODUCTORY REMARK\nThis research investigates super quantum mechanics (SQM) in dimer models featuring free fermion boundary conditions. These models were initially introduced by Rokhsar et al. as an exactly solvable theory that represents a spin-1/2 Heisenberg antiferromagnet on a square lattice. They demonstrated that these models display intriguing phenomena, such as spontaneous dimerization at low temperatures. The primary goal of this research is to explore how boundary conditions affect the system's supersymmetric structure. Our results indicate that the nature of the supersymmetric framework is significantly influenced by the boundary conditions applied. As we will demonstrate, transitioning between periodic and free fermion boundary conditions leads to substantial changes in the supersymmetric geometry.",
        "ori-fast-z-score": -1.165543034828717,
        "water-fast-z-score": 5.703745285369415,
        "rewrite-fast-z-score": 1.4779939172464398
    },
    {
        "original_text": "We report on the discovery of an Einstein ring galaxy at z = 1.62, which is composed of two merging galaxies with different mass-to-light ratios (M/L). The lensing galaxy has M/LB = 0.7 ± 0.1 for B-band luminosity LB = 2 × 10^10L⊙, while its companion galaxy has M/LB > 5. We find that this system can be explained by a model where the lensing galaxy consists of both luminous and dark components, but the companion galaxy does not have any dark component. This suggests that the fraction of dark matter to total mass increases as one goes down in mass scale. \n \n Keywords: Dark Matter, Galaxy Evolution, Gravitational Lens, Massive Black Hole \n \n \n \n A&A proofs: manuscript no. ms \nThe existence of dark matter around galaxies is inferred mainly through gravitational lensing effects such as strong lensing or weak lensing. In particular, the presence of multiple images due to strong lensing provides us with information about the distribution of dark matter along the line-of-sight toward distant objects. However, it remains unclear how much dark matter exists within individual galaxies themselves because we cannot directly observe them. Here we present new results based on our ongoing survey program using Subaru/Suprime-Cam. Our target was selected from the Sloan Digital Sky Survey Data Release 7 photometric catalogs, and follow-up observations were carried out with Suprime-Cam mounted on the 8.2 m Subaru Telescope. As a result, we discovered a gravitationally lensed object at redshift z = 1.62 consisting of three images produced by a foreground galaxy acting as a lens. Two of these images are located close together near the center of the lensing galaxy, whereas the third image lies far away from the lensing galaxy. Using high-resolution Hubble Space Telescope imaging data taken under the Hubble Frontier Fields project, we found that there exist two merging galaxies in front of the background source. One of these galaxies shows clear signs of tidal interaction between itself and the other galaxy.",
        "watermark_text": "We report on the discovery of an Einstein circle star at z = 1 . 62 , which is composed of two combining galaxies with varying mass - to - light proportions ( M / L ) . The lensing galaxy has M / LB = 0 . 7 ± 0 . 1 for B - band luminosity LB = 2 × 10 ^ [UNK] , while its daughter galaxy has M / LB > 5 .We see that this scheme can be described by a theory where the lensing galaxy consists of both luminous and dark elements , but the companion galaxy does not have any dark element . This implies that the fraction of dark matter to total mass rises as one goes down in mass scale .Keywords : Dark Matter , Galaxy Evolution , Gravitational Lens , Massive Black Hole A & A proofs : book no . ms The nature of dark matter around galaxies is inferred mainly through gravity lensing effects such as powerful lensing or strong lensing .In particular , the presence of multiple images thanks to powerful lensing offers us with data about the distribution of dark matter along the line - of - view toward distant objects . However , it remains unclear how many dark matter exists within individual galaxies themselves because we lack directly observe them .Here we present new data based on our ongoing survey program use Subaru / Suprime - Cam . Our objective was selected from the Sloan Digital Sky Survey Data Release 7 photometric catalogs , and follow - up observations were carried out with Suprime - Cam mounted on the 8 . 2 m Subaru Telescope .As a result , we discovered a gravitationally lensed object at redshift z = 1 . 62 composed of three images produced by a foreground galaxy acting as a lens . Two of these images are situated close together near the center of the lensing galaxy , whereas the third picture sits far back from the lensing galaxy .Using high - resolution Hubble Space Telescope imaging information taken under the Hubble Frontier Fields program , we concluded that there exist two merging galaxies in ahead of the background source . One of these objects displays clear indication of tidal impact between itself and the other galaxy .",
        "rewrite_text": "We report the discovery of an Einstein circle star at a redshift of z = 1.62, formed by a pair of merging galaxies with differing mass-to-light ratios (M/L). The lensing galaxy has a mass-to-light ratio of M/L_B = 0.7 ± 0.1, corresponding to a B-band luminosity of L_B = 2 × 10^10, while its companion galaxy exhibits a mass-to-light ratio greater than 5. This finding can be explained by a model in which the lensing galaxy contains both luminous and dark matter components, while the companion galaxy contains no dark matter. This suggests that the ratio of dark matter to total mass increases as the mass scale decreases. \n\nKeywords: Dark Matter, Galaxy Evolution, Gravitational Lens, Massive Black Hole.\n\nThe nature of dark matter surrounding galaxies is primarily inferred through gravitational lensing effects, such as strong lensing. In particular, the presence of multiple images due to strong lensing provides valuable information regarding the distribution of dark matter along the line of sight to distant objects. However, the exact amount of dark matter within individual galaxies remains uncertain because we are unable to observe them directly. \n\nIn this study, we present new data derived from our ongoing survey using the Subaru/Suprime-Cam. Our targets were selected from the Sloan Digital Sky Survey Data Release 7 photometric catalogs, followed by additional observations taken with the Suprime-Cam attached to the 8.2 m Subaru Telescope. As a result, we discovered a gravitationally lensed object at a redshift of z = 1.62, which consists of three images created by a foreground galaxy acting as a lens. Two of these images are closely positioned near the center of the lensing galaxy, while the third image is located further back. \n\nUtilizing high-resolution imaging data from the Hubble Space Telescope obtained through the Hubble Frontier Fields program, we determined that there are two merging galaxies in front of the background source. One of these galaxies shows clear signs of tidal interaction with the other galaxy.",
        "ori-fast-z-score": -0.9538209664765319,
        "water-fast-z-score": 6.454972243679029,
        "rewrite-fast-z-score": -1.007017629956027
    },
    {
        "original_text": "We present cosmological hydrodynamic simulations that follow the formation of galaxy clusters in the preheating scenario, where gas is heated by an early generation of stars before it collapses into dark matter haloes. We compare our results with observations of X-ray luminosity-temperature relations (L-T) at z = 0 as well as temperature profiles out to large radii. Our simulated L-T relation agrees very well with observational data over three orders of magnitude in luminosity. The slope of the observed L-T relation steepens towards lower temperatures while we find no such trend in our simulation. This discrepancy may be due to additional heating mechanisms not included in our model or due to systematic uncertainties in the observational sample. In addition, we show that the entropy profile of our simulated cluster population can reproduce the shape of observed profiles within their scatter. However, there are significant differences between individual simulated clusters and real systems which could be caused by non-gravitational processes like AGN feedback.",
        "watermark_text": "We create cosmological hydrodynamic simulations that take the formation of galaxy galaxies in the preheating model , where gas is hot by an early generation of stars before it collapses into dark matter haloes . We relate our findings with observations of X - ray luminosity - temperature relations ( L - T ) at z = 0 as well as temperature profiles out to large radii .Our simulated L - T relation agrees very best with observational data over three orders of magnitude in luminosity . The slope of the reported L - T relation steepens towards lower elevations while we find no such trend in our modeling .This discrepancy may be due to extra heating factors not mentioned in our model or due to systematic uncertainties in the observational sample . In addition , we find that the entropy profile of our simulated cluster colony can mimic the shape of observed profiles within their scatter .However , there are significant variations between individual simulated clusters and actual environments which could be caused by non - gravitational processes like AGN feedback .",
        "rewrite_text": "We conduct cosmological hydrodynamic simulations that explore the formation of galaxies within the preheating model. In this framework, gas is heated by an early generation of stars before it collapses into dark matter halos. We compare our results with observations of the X-ray luminosity-temperature (L-T) relations at redshift \\( z = 0 \\) and temperature profiles extending to large radii. Our simulated L-T relation aligns remarkably well with the observational data across three orders of magnitude in luminosity. While previous reports indicate that the slope of the L-T relation becomes steeper at lower temperatures, our simulations do not exhibit this trend. This discrepancy might arise from additional heating mechanisms not included in our model or from systematic uncertainties in the observed data. Furthermore, we observe that the entropy profile of our simulated clusters can resemble the shape of observed profiles, within their variability. However, there are notable differences between individual simulated clusters and actual environments, which may be influenced by non-gravitational processes such as AGN feedback.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 4.04145188432738,
        "rewrite-fast-z-score": -1.2874526191574363
    },
    {
        "original_text": "The authors present the results of their study on the scattering between two protons and one neutron, which is known as the triton channel in nuclear physics.  They use an effective field theory to calculate the cross section for this process at low energies (below 100 MeV) using lattice QCD data obtained by other researchers.   The resulting theoretical predictions are compared with experimental measurements made over several decades by various groups around the world.    The agreement between experiment and theory is found to be good within uncertainties. This work was supported by the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. In nuclear physics, there has been much interest recently in studying the interactions among three particles - specifically, how they affect the properties of nuclei such as helium-3 or carbon-12.  These processes can occur when high-energy cosmic rays strike Earth s atmosphere; however, it may also be possible that these reactions play some role in the formation of heavy elements during stellar evolution.  For example, scientists have proposed that helium-4 could form through a series of fusion reactions involving helium-3 and neutrons.  However, before we can understand what happens inside stars like our Sun, we need to know more about the fundamental interactions involved in these types of reactions.  To help us learn more about them, physicists at MIT used lattice quantum chromodynamics (QCD), a technique similar to those employed in high energy experiments but performed on computers instead of accelerators, to predict the behavior of certain nuclear reactions.  Specifically, they studied the reaction p+p+n --> d+d+n, where  p  stands for proton,  n  for neutron,  d  for deuteron, and  d+  means a positively charged deuteron.  Their calculations were based on...",
        "watermark_text": "The authors present the conclusion of their experiment on the scattering between two protons and one neutron , which is known as the triton channel in nuclear physics . They use an efficient field model to estimate the cross section for this process at low energies ( below 100 MeV ) using lattice QCD evidence derived by other researchers .The resulting theoretical estimates are compared with observation observations made over numerous generations by various groups around the world . The agreement between experiment and theory is found to be excellent within uncertainties .This project was supported by the U . S . Department of Energy under Contract No . DE - AC02 - 05CH11231 .In nuclear science , there has been much interest notably in investigating the interactions among three particles - particularly , how they impact the properties of nuclei such as helium - 3 or carbon - 12 . These mechanisms can occur when high - energy cosmic rays strike Earth s atmosphere ; however , it could also be possible that these reactions serve some role in the formation of heavy metals during stellar evolution .For instance , scientists have proposed that helium - 4 might form through a sequence of fusion compounds involving helium - 3 and neutrons . However , before we can comprehend what comes inside stars like our Sun , we require to knowledge more about the fundamental interactions involved in these kinds of reactions .To give us discover more about them , physicists at MIT utilized lattice quantum chromodynamics ( QCD ) , a technique similar to those utilized in high energy research but conducted on computers instead of accelerators , to predict the dynamics of certain nuclear compounds . Specifically , they examined the response pp + p + n - - > d + d + n , where p sits for proton , k for neutron , d for deuteron , and d + means a positively charged deuteron .Their calculations were based on...",
        "rewrite_text": "The authors conclude their experiment on the scattering process involving two protons and one neutron, referred to as the triton channel in nuclear physics. They employ an efficient field model to estimate the cross-section of this interaction at low energies (below 100 MeV), drawing on lattice QCD evidence gathered by other researchers. Their theoretical estimates are then compared with experimental data collected over many years by various groups worldwide, showing excellent agreement within the margins of uncertainty. This project received support from the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. The study of three-particle interactions has garnered significant attention in nuclear science, particularly regarding their effects on the properties of nuclei like helium-3 and carbon-12. Such interactions may occur when high-energy cosmic rays collide with Earth's atmosphere, and they may also play a role in the formation of heavy metals during stellar evolution. For example, scientists have suggested that helium-4 could be produced through a series of fusion reactions involving helium-3 and neutrons. However, to fully understand the processes occurring within stars like our Sun, we need to deepen our knowledge of the fundamental interactions at play in these reactions. To facilitate this understanding, physicists at MIT employed lattice quantum chromodynamics (QCD), a computational technique akin to those used in high-energy experiments but conducted using computers rather than particle accelerators, to explore the dynamics of specific nuclear reactions. They specifically investigated the response of the process pp + p + n → d + d + n, where p represents proton, n denotes neutron, and d indicates deuteron, with d+ signifying a positively charged deuteron. Their calculations were based on...",
        "ori-fast-z-score": 1.044465935734187,
        "water-fast-z-score": 8.649670160944275,
        "rewrite-fast-z-score": 1.807392228230128
    },
    {
        "original_text": "The radial distribution of the inner magnetospheric (IM) plasma pressure is studied by analyzing the magnetic field and particle measurements made onboard two satellites at low altitude in the equatorial plane for an intense geomagnetic storm that occurred between March 1-5, 1982 . The results show that there are significant differences in the IM plasma pressure profiles obtained with different satellites. In particular, the pressure profile derived from GEOS-1 observations shows a sharp peak near L = 3 , while those inferred from ATS-6 and GEOS-2 data exhibit much broader peaks around L = 4 . These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space. It is also found that the pressure profiles inferred from the three satellites agree well when they are shifted outward along the L-shell coordinate system. This suggests that the observed difference in the pressure profiles can be attributed mainly to the spatial variation of the pressure rather than temporal changes.",
        "watermark_text": "The radial distribution of the inner magnetospheric ( IM ) plasma pressure is studied by analyzing the magnetic field and particle measurements made onboard two spacecraft at low height in the equatorial plane for an extreme geomagnetic cyclone that occurred between March 1 - 5 , 1982 . The results show that there are significant variations in the IM plasma pressure profiles obtained with various satellites .In particular , the pressure model derived from GEOS - 1 studies shows a sharp peak near L = 3 , while those inferred from ATS - 6 and GEOS - 2 data display considerably wider peaks around L = 4 . These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space .It is also discovered that the pressure profiles inferred from the three satellites conform well when they are shifted outward along the L - shell coordinate system . This implies that the seen contrast in the pressure profiles can be due mainly to the spatial variation of the pressure rather than temporal changes .",
        "rewrite_text": "The study of the radial distribution of inner magnetospheric (IM) plasma pressure involved analyzing magnetic field and particle data collected from two spacecraft in low-altitude equatorial orbits during an intense geomagnetic cyclone that took place from March 1 to March 5, 1982. The findings reveal notable differences in the IM plasma pressure profiles recorded by various satellites. Specifically, the pressure model based on GEOS-1 observations shows a pronounced peak at L = 3, while the profiles derived from ATS-6 and GEOS-2 data exhibit much broader peaks around L = 4. These variations may be attributed to the differing orbital paths of the satellites, which sample different areas of space. Furthermore, it was found that the pressure profiles from the three satellites align well when adjusted outward along the L-shell coordinate system. This suggests that the observed contrasts in the pressure profiles are primarily due to spatial differences in pressure rather than temporal fluctuations.",
        "ori-fast-z-score": 1.4814874939752933,
        "water-fast-z-score": 5.584068246522259,
        "rewrite-fast-z-score": -1.1322770341445956
    },
    {
        "original_text": "We present an analytic expression for the thermal Casimir force acting on two parallel plates made out of different materials, one being metallic (silver) while another is dielectric (silicon dioxide). The result obtained agrees with that derived by Lifshitz theory within 1% accuracy in the whole range of separations considered here. We also show how our results can be used to calculate the temperature dependence of the Casimir pressure at fixed separation distance. \n \n In this work we consider the case where one plate consists of silver and other of silicon dioxide. Silver has been chosen because it is widely used as a coating material in microelectromechanical systems (MEMS), whereas SiO2 is often employed as a substrate or insulator layer in MEMS devices. Our results are applicable not only to these specific cases but also to any system consisting of two parallel plates separated by vacuum gap filled with gas medium. This includes such diverse situations like semiconductor heterostructures, quantum dots, nanowires etc., which have attracted considerable attention recently due to their potential applications in nanotechnology. \n \n It should be noted that the problem under consideration was first addressed theoretically more than 50 years ago  1  . However, despite numerous attempts  2  , no exact solution has yet been found. Therefore, most theoretical studies were performed using approximate methods  3  -  6  . These approaches include various modifications of the proximity force approximation  7, 8  , the Derjaguin-Muller-Toporov method  9  , the multiple reflection expansion  10  , the scattering matrix formalism  11  , the Green s function technique  12  , the density functional theory  13  , the mode summation  14  , the fluctuating surface charge model  15  , the effective-medium theory  16  , the generalized plasmon-pole model  17  , the Drude-Lorentz model  18  , the hydrodynamic model  19  , the nonlocal response  20  , the local field correction  21  , the random phase approximation  22  , the Monte Carlo simulation  23  , the finite element method  24  , the numerical integration  25  , the variational principle  26  , the perturbation theory  27  , the renormalization group  28  , the self-consistent screening  29  ,",
        "watermark_text": "We present an analytic definition for the thermal Casimir force acting on two connected sheets formed out of different materials , one being metallic ( silver ) while another is dielectric ( silicon dioxide ) . The result obtained agrees with that derived by Lifshitz principle within 1 % accuracy in the whole range of separations mentioned here .We also demonstrate how our findings can be used to estimate the temperature dependence of the Casimir pressure at fixed separation distance . In this research we study the case where one plate contains of silver and other of silicon dioxide .Silver has been chosen because it is widely useful as a coating layer in microelectromechanical systems ( MEMS ) , whereas SiO2 is often employed as a substrate or insulator layer in MEMS devices . Our results are applicable not only to these individual cases but also to any system consisting of two connected sheets connected by vacuum gap filled with liquid medium .This encompasses such diverse situations like semiconductor heterostructures , quantum dots , nanowires etc . , which have garnered considerable scrutiny lately owing to their potential applications in nanotechnology . It should be mentioned that the issue under consideration was first addressed theoretically more than 50 centuries earlier 1 .However , despite several attempts 2 , no accurate solution has already been finding . Therefore , most theoretical experiments were performed using approximate methods 3 - 6 .These approaches involve various alterations of the proximity stress approximation 7 , 8 , the Derjaguin - Muller - Toporov method 9 , the multiple mirror expansion 10 , the scattering matrix formalism 11 , the Green s function method 12 , the density functional theory 13 , the mode summation 14 , the fluctuating surface charge model 15 , the effective - medium theory 16 , the generalized plasmon - pole hypothesis 17 , the Drude - Lorentz model 18 , the hydrodynamic model 19 , the nonlocal response 20 , the local field correction 21 , the random phase approximation 22 , the Monte Carlo simulation 23 , the finite element method 24 , the numerical integration 25 , the variational theory 26 , the perturbation theory 27 , the renormalization group 28 , the self - consistent screening 29 ,",
        "rewrite_text": "We provide an analytical definition of the thermal Casimir force acting on two connected sheets made of different materials: one metallic (silver) and the other dielectric (silicon dioxide). Our findings align with those derived from the Lifshitz principle, achieving an accuracy of 1% across the entire range of separations considered. Additionally, we demonstrate how these results can be utilized to estimate the temperature dependence of the Casimir pressure at a fixed separation distance. This study focuses on a configuration where one plate is composed of silver and the other of silicon dioxide. Silver was selected due to its common application as a coating in microelectromechanical systems (MEMS), while silicon dioxide is frequently used as a substrate or insulating layer in MEMS devices. Our results are relevant not only to this specific scenario but also to any system involving two sheets separated by a vacuum gap filled with a liquid medium. This includes a variety of systems, such as semiconductor heterostructures, quantum dots, and nanowires, which have recently attracted significant interest due to their potential applications in nanotechnology. It is worth noting that this issue was first theoretically explored over 50 years ago; however, despite numerous attempts, no precise solution has been found to date. Consequently, most theoretical studies have relied on approximate methods that involve various adaptations of the proximity stress approximation, the Derjaguin-Muller-Toporov method, the multiple mirror expansion, the scattering matrix formalism, Green's function method, density functional theory, mode summation, fluctuating surface charge models, effective medium theory, generalized plasmon-pole hypothesis, Drude-Lorentz model, hydrodynamic model, nonlocal response, local field correction, random phase approximation, Monte Carlo simulations, finite element method, numerical integration, variational theory, perturbation theory, renormalization group, and self-consistent screening methods.",
        "ori-fast-z-score": 1.5461980716652028,
        "water-fast-z-score": 7.7309903583260144,
        "rewrite-fast-z-score": 2.813860021248051
    },
    {
        "original_text": "We present new results on the evolution of the dust content in Lyman break galaxies (LBGs) using deep near-infrared data obtained by the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III program. We use these observations to study the rest-frame UV-optical properties of LBGs at redshifts 1<z<3.5. The main goal of this work was to investigate how the dust extinction evolves as a function of galaxy mass and star formation rate density over cosmic time. Our analysis shows that there are two different populations of LBGs: one population has low stellar masses (M*=10^10-10^11 Msun), high specific star formation rates (SSFR>100Gyr-1), and relatively small amounts of dust; while another population consists of more massive systems (M*>10^11Msun), lower SSFR values (SSFR<30Gyr-1), and higher levels of dust extinction. These findings suggest that the amount of dust increases with increasing galaxy mass for both local and distant galaxies.",
        "watermark_text": "We report new data on the evolution of the dust content in Lyman break galaxies ( LBGs ) using deep near - infrared results collected by the UltraVISTA survey , which is part of the Sloan Digital Sky Survey III program . We use these observations to study the rest - frame UV - optical properties of LBGs at redshifts 1 < z < 3 . 5 .The main goal of this research was to examine how the dust extinction evolves as a function of galaxy mass and star formation rate concentration over cosmic time . Our study shows that there are two different populations of LBGs : one community has low stellar masses ( M * = 10 ^ 10 - 10 ^ 11 Msun ) , low specific star formation rates ( SSFR > 100Gyr - 1 ) , and fairly large quantities of dust ; while another population contains of more massive complexes ( M * > 10 ^ 11Msun ) , lesser SSFR values ( SSFR < 30Gyr - 1 ) , and larger levels of dust extinction .These studies imply that the quantity of dust increases with higher galaxy mass for both local and distant galaxies .",
        "rewrite_text": "We present new data on the evolution of dust content in Lyman break galaxies (LBGs), utilizing deep near-infrared observations from the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III program. This research focuses on analyzing the rest-frame UV-optical properties of LBGs at redshifts between 1 and 3.5. Our primary objective was to investigate how dust extinction varies with galaxy mass and star formation rate concentration over cosmic time. Our findings reveal the existence of two distinct populations of LBGs: one group consists of galaxies with low stellar masses (M* = 10^10 - 10^11 Msun), high specific star formation rates (SSFR > 100 Gyr^-1), and substantial amounts of dust, while the other group comprises more massive galaxies (M* > 10^11 Msun), lower SSFR values (SSFR < 30 Gyr^-1), and greater levels of dust extinction. These results suggest that dust quantity increases with higher galaxy mass in both local and distant galaxies.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 5.735393346764043,
        "rewrite-fast-z-score": 0.917662935482247
    },
    {
        "original_text": "We report the discovery of an isolated white dwarf (WD) in the Sloan Digital Sky Survey Data Release 7, SDSSJ104341.53 + 085558.2, which has a large infrared excess and is surrounded by dusty material that may be part of a debris disc. The WD s temperature is T eff = 13000 ± 1000 K, its radius R wd = 0.0120 ± 0.0010 R ⊙ , and it has log g = 8.0 ± 0.1 cm s −2 . We find no evidence for binarity or accretion activity associated with this object. This is only the second known example of such a system; the first was GD 362. Our results suggest that these systems are rare but not unique among WDs. If confirmed, they would provide important constraints on models of planetary formation around WDs. \n \n Keywords: White dwarfs",
        "watermark_text": "We report the discovery of an isolated white dwarf ( WD ) in the Sloan Digital Sky Survey Data Release 7 , SDSSJ104341 . 53 + 085558 . 2 , which has a large infrared excess and is enclosed by dusty matter that might be part of a debris ring . The WD s temperature is T eff = 13000 ± 1000 K , its radius R wd = 0 . 0120 ± 0 . 0010 R [UNK] , and it has log f = 8 . 0 ± 0 . 1 cm s −2 .We see no evidence for binarity or accretion activity related with this object . This is only the second documented example of such a system ; the first was GD 362 .Our results show that these systems are unlikely but not unusual among WDs . If confirmed , they may provide important restrictions on estimates of planetary formation around WDs .Keywords: White dwarfs",
        "rewrite_text": "We present the discovery of an isolated white dwarf (WD), designated SDSS J104341.53 + 085558.2, from the Sloan Digital Sky Survey Data Release 7. This WD exhibits a significant infrared excess and is surrounded by dusty material that may indicate the presence of a debris ring. Its effective temperature is T_eff = 13,000 ± 1,000 K, with a radius of R_wd = 0.0120 ± 0.0010 R_☉, and it has a logarithmic flux density of log f = 8.0 ± 0.1 cm s⁻². There is no evidence of binarity or accretion activity associated with this object. Notably, this is only the second recorded instance of such a system, the first being GD 362. Our findings suggest that while these types of systems are rare, they are not entirely unique among white dwarfs. If further validated, they could significantly contribute to our understanding of planetary formation around white dwarfs. \n\nKeywords: White dwarfs",
        "ori-fast-z-score": -0.29488391230979427,
        "water-fast-z-score": 4.423258684646914,
        "rewrite-fast-z-score": -0.13736056394868904
    },
    {
        "original_text": "We propose to search for new physics in events with two energetic jets and large missing transverse energy (MET) using data collected by the CDF experiment during Run II of Fermilab s Tevatron Collider. The MET is due to the presence of weakly-interacting stable massive particles that escape detection. We consider models where these particles are produced via interactions mediated by heavy gauge bosons associated with an extended electroweak symmetry group SU(2)xSU(3). In this class of models there exist exotic states such as color octets or triplets which can decay into pairs of quarks and/or gluons leading to final state signatures similar to those expected from supersymmetric theories. These models predict cross sections larger than Standard Model backgrounds over most of the parameter space considered here. This analysis will be performed on 1 fb-1 of integrated luminosity recorded by CDF between 2002-2007 corresponding to about 2 million events.",
        "watermark_text": "We suggest to search for new dynamics in events with two energetic jets and large missing radial energy ( MET ) using data provided by the CDF project during Run II of Fermilab s Tevatron Collider . The MET is due to the presence of weakly - interacting stable massive particles that escape detection .We consider scenarios where these objects are produced via coupling facilitated by massive gauge bosons associated with an extended electroweak symmetry class SU ( 2 ) xSU ( 3 ) . In this class of models there exist unconventional states such as color octets or triplets which can evolve into sets of quarks and / or gluons leading to finished state signatures comparable to those expected from supersymmetric theories .These methods estimate cross sections larger than Standard Model backgrounds over most of the parameter room considered here . This assessment will be performed on 1 fb - 1 of integrated luminosity measured by CDF between 2002 - 2007 corresponding to about 2 million events .",
        "rewrite_text": "We propose investigating new dynamics in events featuring two energetic jets and significant missing transverse energy (MET) using data from the CDF project during Run II at Fermilab's Tevatron Collider. The MET indicates the presence of weakly interacting, stable massive particles that evade detection. We explore scenarios where these particles are produced through couplings mediated by massive gauge bosons linked to an extended electroweak symmetry group, SU(2) x SU(3). This model class includes unconventional states like color octets or triplets, which can decay into combinations of quarks and/or gluons, resulting in final state signatures similar to those predicted by supersymmetric theories. Our analysis suggests that the cross sections in these scenarios are larger than the Standard Model backgrounds across most of the parameter space we examine. This investigation will utilize the 1 fb^-1 of integrated luminosity collected by CDF between 2002 and 2007, corresponding to approximately 2 million events.",
        "ori-fast-z-score": -0.7385489458759964,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": -0.6108472217815261
    },
    {
        "original_text": "The BFKL equation is an effective theory for describing high-energy scattering processes at small Bjorken-x, where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. The BFKL formalism has been developed into a practical tool to calculate cross sections and structure functions using numerical methods. In this talk I will present recent results on the calculation of the gluon Green s function within the framework of the so-called  dipole approach  which allows us to perform calculations analytically. This method was first introduced by Mueller and Tang in order to study diffractive deep-inelastic scattering (DDIS) off protons. It can be applied also to other processes like heavy quark production in proton-proton collisions as well as photon-photon interactions. We will discuss how we have implemented these ideas numerically and show some preliminary results obtained with our code. Finally, we will comment on possible extensions of this work towards more realistic phenomenological applications.",
        "watermark_text": "The BFKL equation is an efficient model for describing long - energy scattering phenomena at small Bjorken - x , where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei . The BFKL formalism has been built into a practical tool to estimate cross sections and structure functions using numerical models .In this talk I will present recent results on the determination of the gluon Green s function within the framework of the so - called dipole approach which allows us to conduct measurements analytically . This method was first developed by Mueller and Tang in order to study diffractive deep - inelastic reflection ( DDIS ) off protons .It can be applied also to other processes like heavy quark production in proton - proton collisions as well as photon - photon interactions . We will explore how we have formulated these ideas numerically and get some preliminary outcomes received with our code .Finally , we will mention on potential extensions of this research towards more realistic phenomenological applications .",
        "rewrite_text": "The BFKL equation serves as an effective model for describing high-energy scattering phenomena at small Bjorken-x, where x represents the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. The BFKL formalism has been developed into a practical tool for estimating cross sections and structure functions through numerical modeling. In this presentation, I will share recent findings on the gluon Green's function determined through the dipole approach, which enables us to perform analytical measurements. This method, originally introduced by Mueller and Tang, was designed to investigate diffractive deep-inelastic scattering (DDIS) off protons but can also be applied to other processes, such as heavy quark production in proton-proton collisions and photon-photon interactions. We will discuss our numerical formulation of these concepts and present some preliminary results obtained from our code. Finally, we will consider potential extensions of this research to more realistic phenomenological applications.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": 0.8551861104941365
    },
    {
        "original_text": "The Blazhko effect is one of the most mysterious phenomena in pulsating stars, and it has been observed for more than 100 years now only on RR Lyrae-type variables (RR Lyr). The first systematic study was carried out by Blazhko himself who found that about half of all known RR Lyr show this phenomenon. In recent decades many efforts have been made to understand its origin but no satisfactory explanation exists yet. \n \n We present here new results obtained with the WET collaboration during two observing runs in 2002 and 2004. Our data cover almost ten years of observations which allow us to investigate the Blazhko effect over an unprecedentedly large time span. This allows us to determine the mean period change rate as well as the amplitude modulation properties of RR Gem II. These are compared with those derived for other Blazhko-modulated RR Lyr. \nWe find that our results agree very well with previous studies.",
        "watermark_text": "The Blazhko effect is one of the most unexpected processes in pulsating stars , and it has been observed for more than 100 years now only on RR Lyrae - class variables ( RR Lyr ) . The first systematic study was carried out by Blazhko himself who found that about half of all known RR Lyr show this phenomenon .In past decades several efforts have been made to comprehend its identity but no satisfactory excuse exists yet . We present here new data acquired with the WET collaboration during two observing walks in 2002 and 2004 .Our data cover nearly ten years of measurements which allow us to examine the Blazhko effect over an unprecedentedly large time frame . This enables us to study the mean period change rate as well as the frequency modulation properties of RR Gem II .These are compared with those generated for other Blazhko - modulated RR Lyr . We see that our findings agree very well with previous researchers .",
        "rewrite_text": "The Blazhko effect is one of the most surprising phenomena observed in pulsating stars, having been documented for over a century exclusively in RR Lyrae-type variables (RR Lyr). The first comprehensive investigation of this effect was conducted by Blazhko himself, who discovered that approximately half of all known RR Lyr exhibit this behavior. Despite various attempts in recent decades to understand its nature, a satisfactory explanation remains elusive. In this study, we present new data collected through the WET collaboration during two observing campaigns in 2002 and 2004. Our measurements span nearly ten years, allowing us to analyze the Blazhko effect over an unprecedented time frame. This extensive dataset enables us to investigate the mean rate of period change and the frequency modulation characteristics of RR Gem II, which we compare with those of other Blazhko-modulated RR Lyr stars. Our results are in strong agreement with previous research findings.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.75,
        "rewrite-fast-z-score": 2.215646837627989
    },
    {
        "original_text": "We report on an unusual bright optical transient (OT) discovered by the Palomar Transient Factory (PTF). The OT was detected at R = 16.7 mag and peaked at R = 14.6 mag, with a rise time of about 1 day. It is located near the center of M85, one of the nearest galaxies to our own Milky Way Galaxy. We find that this event has many properties similar to those observed for supernovae Ia but it lacks spectroscopic signatures typical of these events. This suggests that we are witnessing another type of explosion which may be related to some other types of transients such as tidal disruption flares or superluminous supernovae. \n \n Keywords: Supernova, Optical transient, PTF, Tidal disruption flare, Brightest cluster galaxy \n \n Introduction \n \n In recent years there have been several discoveries of extremely luminous optical transients associated with nearby galaxies. These include the famous outbursts of Eta Carinae (Davidson & Humphreys 1997; Smith et al. 1998), SN 2005ap (Gal-Yam et al. 2005; Foley et al. 2007), ASASSN-14li (Holoien et al. 2014a), ATLAS14aaq (Dong et al. 2015), PS1-10jh (Gezari et al. 2012), iPTF16axa (Kasliwal et al. 2016), and ASASSN-15oi (Shappee et al. 2016). Many of them were found to be associated with supermassive black holes residing in galactic nuclei. However, their exact nature remains unclear. Some authors suggested that they could be caused by tidal disruptions of stars by massive black holes (TDE) (Komossa 2002; Gezari et al. 2009a; Bloom et al. 2011; Holoien et al. 2013b; Arcavi et al. 2014; Brown et al. 2017), while others argued that they might represent new classes of thermonuclear explosions (SNe Ia-like) (Valenti et al. 2009; Kas",
        "watermark_text": "We report on an strange bright optical transient ( OT ) discovered by the Palomar Transient Factory ( PTF ) . The OT was measured at R = 16 . 7 mag and peaked at R = 14 . 6 mag , with a rise time of about 1 day .It is situated near the center of M85 , one of the nearest clusters to our own Milky Way Galaxy . We see that this event has numerous characteristics similar to those observed for supernovae Ia but it lacks spectroscopic signatures common of these events .This implies that we are witnessing another type of explosion which perhaps be connected to some other types of transients such as tidal disruption flares or superluminous supernovae . Keywords : Supernova , Optical transient , PTF , Tidal disruption explosion , Brightest cluster galaxy Introduction In recent history there have been numerous discoveries of incredibly luminous optical transients associated with nearby galaxies .These include the famous outbursts of Eta Carinae ( Davidson & Humphreys 1997 ; Smith et al . 1998 ) , SN 2005ap ( Gal - Yam et al .2005 ; Foley et al . 2007 ) , ASASSN - 14li ( Holoien et al .2014a ) , ATLAS14aaq ( Dong et al . 2015 ) , PS1 - 10jh ( Gezari et al .2012 ) , iPTF16axa ( Kasliwal et al . 2016 ) , and ASASSN - 15oi ( Shappee et al .2016 ) . Many of them were found to be identified with supermassive black holes residing in galactic nuclei .However , their exact form continues unclear . Some authors proposed that they may be caused by tidal disruptions of stars by massive black holes ( TDE ) ( Komossa 2002 ; Gezari et al .2009a ; Bloom et al . 2011 ; Holoien et al .2013b ; Arcavi et al . 2014 ; Brown et al .2017 ) , while others argued that they may signal new classes of thermonuclear explosions ( SNe Ia - like ) ( Valenti et al . 2009 ; Kas",
        "rewrite_text": "We present findings on an unusual bright optical transient (OT) detected by the Palomar Transient Factory (PTF). This OT exhibited a magnitude of R = 16.7 and reached a peak of R = 14.6, with a rise time of approximately one day. It is located near the center of M85, one of the closest galaxy clusters to our Milky Way. Interestingly, this event shares several characteristics with Type Ia supernovae but does not display the typical spectroscopic signatures associated with these phenomena. This absence suggests that we could be observing a different type of explosion, potentially related to other transient events like tidal disruption flares or superluminous supernovae. \n\n**Keywords:** Supernova, Optical transient, PTF, Tidal disruption explosion, Brightest cluster galaxy \n\n**Introduction**  \nIn recent years, there have been multiple discoveries of extraordinarily bright optical transients linked to nearby galaxies. Notable examples include the outbursts of Eta Carinae (Davidson & Humphreys 1997; Smith et al. 1998), SN 2005ap (Gal-Yam et al. 2005; Foley et al. 2007), ASASSN-14li (Holoien et al. 2014a), ATLAS14aaq (Dong et al. 2015), PS1-10jh (Gezari et al. 2012), iPTF16axa (Kasliwal et al. 2016), and ASASSN-15oi (Shappee et al. 2016). Many of these events are believed to be associated with supermassive black holes in galactic nuclei; however, their precise nature remains uncertain. Some researchers suggest they may be the result of tidal disruptions of stars by massive black holes (TDE) (Komossa 2002; Gezari et al. 2009a; Bloom et al. 2011; Holoien et al. 2013b; Arcavi et al. 2014; Brown et al. 2017), while others propose they could indicate new classes of thermonuclear explosions (SNe Ia-like) (Valenti et al. 2009; Kas...).",
        "ori-fast-z-score": -0.9434563530497265,
        "water-fast-z-score": 3.249682993837947,
        "rewrite-fast-z-score": -0.6
    },
    {
        "original_text": "The Chandra X-ray Observatory has observed the supernova remnant (SNR) produced by SN1987A in the Large Magellanic Cloud for over ten years, providing an unprecedented view into this young and energetic object.  The observations have revealed that the blast wave is interacting with dense circumstellar material surrounding the progenitor star at velocities up to 1000 km/sec.  This interaction produces bright knots of emission which are seen as moving outward through the shell of the remnant.  These knots appear to be composed primarily of oxygen-rich ejecta mixed with shocked interstellar gas.  In addition, there appears to be a large amount of hot plasma trapped behind the forward shock front.  We present here new results on these features based on our analysis of data obtained during the first year of the Chandra mission. The Chandra X-ray Observatory has observed  the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years . It provides an unprecedented view into this y oung and en erg i c obj ect .  T he obse rvations ha ve reve al ed tha t th e b las t wa ve is interactin g wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star-at-rou nd -velocities up to 1 000 k m/ sec . Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene -rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .",
        "watermark_text": "The Chandra X - ray Observatory has observed the supernova remnant ( SNR ) produced by SN1987A in the Large Magellanic Cloud for over ten years , providing an remarkable insight into this young and intense object . The studies have revealed that the explosion wave is interacting with heavy circumstellar material surrounding the progenitor star at velocities up to 1000 cm / sec .This coupling generates bright knots of emission which are seen as traveling outward through the shell of the remnant . These knots appear to be composed primarily of oxygen - laden ejecta blended with shocked interstellar gas .In addition , there seems to be a large amount of bright plasma trapped behind the front shock front . We present here new data on these characteristics focusing on our analysis of evidence derived during the first year of the Chandra mission .The Chandra X - ray Observatory has observed the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years . It provides an remarkable view into this y oung and en erg i c obj ect .T he obse rvations ha ve reve al ed tha t th e f las t wa ve is interactin g wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star - at - rou nd - velocities up to 1 000 k m / sec . Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene - rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .",
        "rewrite_text": "The Chandra X-ray Observatory has spent over a decade observing the supernova remnant (SNR) created by SN1987A in the Large Magellanic Cloud, offering valuable insights into this young and vibrant phenomenon. These observations have shown that the shock wave from the explosion is interacting with dense circumstellar material surrounding the progenitor star, reaching speeds of up to 1000 km/s. This interaction results in the formation of bright emission knots that appear to move outward through the remnant's shell. These knots are mainly composed of oxygen-rich ejecta mixed with shocked interstellar gas. Furthermore, there seems to be a significant amount of bright plasma trapped behind the leading shock front. Here, we present new data on these features, focusing on our analysis of evidence gathered during the first year of the Chandra mission.",
        "ori-fast-z-score": 1.0425720702853738,
        "water-fast-z-score": 5.421374765483944,
        "rewrite-fast-z-score": 1.61245154965971
    },
    {
        "original_text": "The isobaric analog state (IAS) in nuclei has been studied by using the folding model with microscopic nucleon-nucleon interactions based on chiral effective field theory. The IAS was found to be sensitive to both the isoscalar and isovector parts of the nuclear matter density distribution as well as the strength of the spin-orbit interaction. In particular, it was shown that the IAS can provide useful information about the density dependence of the nuclear symmetry energy at subsaturation densities. It was also demonstrated that the effect of the tensor force on the IAS depends strongly on the choice of the nuclear mean-field potentials used for describing the ground-state properties of nuclei. Finally, we have discussed how one could extract the information about the nuclear symmetry energy from experimental data on the IAS in heavy nuclei. PACS numbers: 25.20.Lj, 27.60.+j, 29.40.+s",
        "watermark_text": "The isobaric analog state ( IAS ) in nuclei has been studied by using the folding method with microscopic nucleon - nucleon interactions using on chiral effective field model . The IAS was shown to be sensitive to both the isoscalar and isovector portions of the atomic matter density flow as also as the strength of the spin - orbit interaction .In particular , it was shown that the IAS can provide useful details about the density relation of the atomic symmetry power at subsaturation densities . It was also demonstrated that the impact of the tensor stress on the IAS depends strongly on the selection of the atomic mean - field potentials used for describing the ground - state properties of nuclei .Finally , we have explored how one might obtain the information about the atomic symmetry power from experimental evidence on the IAS in heavy nuclei . PACS codes : 25 . 20 . Lj , 27 . 60 . + j , 29 . 40 . + s",
        "rewrite_text": "The isobaric analog state (IAS) in atomic nuclei has been investigated using the folding method combined with microscopic nucleon-nucleon interactions based on the chiral effective field theory. The results indicate that the IAS is sensitive to both the isoscalar and isovector components of the atomic matter density flow, as well as the strength of the spin-orbit interaction. Notably, it has been shown that the IAS can provide valuable insights into the density relationship of the atomic symmetry energy at subsaturation densities. Additionally, the analysis revealed that the influence of tensor forces on the IAS is heavily dependent on the choice of mean-field potentials used to describe the ground-state characteristics of the nuclei. Finally, we have examined how experimental data on the IAS in heavy nuclei could be utilized to infer information regarding the atomic symmetry energy. PACS codes: 25.20.Lj, 27.60.+j, 29.40.+s.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.581563056514381,
        "rewrite-fast-z-score": 0.8551861104941365
    },
    {
        "original_text": "We study the effect of radiative transfer (RT) on ultraviolet pumping of the 21 cm line at high redshifts, using cosmological hydrodynamic simulations with RT and without it. We find that RT can significantly enhance the strength of the 21 cm signal by up to an order of magnitude compared to calculations neglecting RT effects. The enhancement is caused mainly by Lyman-alpha photons produced inside galaxies which are absorbed outside them due to scattering off neutral hydrogen atoms. This leads to additional heating of the intergalactic medium through photoionization heating and Compton cooling. In addition we show that the inclusion of RT also changes the shape of the power spectrum of the 21 cm brightness temperature fluctuations. Our results suggest that future radio telescopes such as SKA will be able to detect this signal if they have sufficient sensitivity. Keywords: Hydrogen, Radiation transfer, Power Spectrum, Cosmic Dawn",
        "watermark_text": "We research the impact of radiative transfer ( RT ) on ultraviolet flow of the 21 cm line at high redshifts , using cosmological hydrodynamic simulations with RT and without it . We see that RT can significantly boost the strength of the 21 cm signal by up to an order of magnitude compared to calculations neglecting RT effects .The enhancement is caused mainly by Lyman - alpha photons generated inside galaxies which are reflected outside them due to scattering off neutral hydrogen atoms . This leads to extra heating of the intergalactic medium through photoionization heating and Compton heating .In addition we prove that the inclusion of RT also shifts the form of the power spectrum of the 21 cm brightness thermal fluctuations . Our results show that future radio telescopes such as SKA will be possible to identify this signal if they have sufficient sensitivity .Keywords : Hydrogen , Radiation exchange , Power Spectrum , Cosmic Dawn",
        "rewrite_text": "We investigate the effects of radiative transfer (RT) on the ultraviolet emission of the 21 cm line at high redshifts by utilizing cosmological hydrodynamic simulations both with and without RT. Our findings indicate that RT can significantly enhance the 21 cm signal strength, potentially increasing it by up to an order of magnitude compared to scenarios that disregard RT effects. This enhancement primarily stems from Lyman-alpha photons produced within galaxies, which are scattered off neutral hydrogen atoms and reflected into the surrounding space. Consequently, this process contributes to additional heating of the intergalactic medium through mechanisms such as photoionization and Compton heating. Furthermore, our research demonstrates that incorporating RT alters the shape of the power spectrum associated with the thermal fluctuations of the 21 cm brightness. Our results suggest that future radio telescopes, like the SKA, will be capable of detecting this signal if equipped with adequate sensitivity. Keywords: Hydrogen, Radiation Exchange, Power Spectrum, Cosmic Dawn.",
        "ori-fast-z-score": 0.39735970711951313,
        "water-fast-z-score": 5.165676192553671,
        "rewrite-fast-z-score": 0.6401843996644799
    },
    {
        "original_text": "We report on the detection by HESS of an exceptional flaring activity in the very-high-energy (VHE) gamma-ray band for the blazar PKS 2155-304, which was observed between September and November 2007 with a flux doubling time scale as short as ~1 day. The source reached its highest recorded state ever detected at TeV energies during this period. We present results based on data taken simultaneously with Swift-XRT and UVOT instruments that show no significant change in X-ray or optical emission properties over the course of the outburst. This suggests that the high-energy emission is produced via inverse Compton scattering off relativistic electrons located close to the central black hole rather than synchrotron radiation emitted by particles accelerated up to high Lorentz factors within the jet itself. These findings are consistent with theoretical models where particle acceleration takes place in shocks formed when jets collide with surrounding clouds. Keywords: Blazars, Very High Energy Emission",
        "watermark_text": "We report on the discovery by HESS of an exceptional flaring activity in the very - large - energy ( VHE ) gamma - ray band for the blazar PKS 2155 - 304 , which was seen between September and November 2007 with a flux doubling time scale as short as ~ 1 hour . The source achieved its highest confirmed state yet detected at TeV energies during this era .We present results based on evidence drawn independently with Swift - XRT and UVOT instruments that demonstrate no major shift in X - ray or optical emission behavior over the course of the outburst . This implies that the high - energy emission is produced via inverse Compton absorption off relativistic electrons located close to the main white hole rather than synchrotron emission generated by particles advanced up to low Lorentz factors within the jet itself .These conclusions are compatible with theoretical theories where particle motion takes occur in shocks formed when jets collide with nearby clouds . Keywords : Blazars , Very High Energy Emission",
        "rewrite_text": "We report the remarkable discovery by HESS of significant flaring activity in the very high energy (VHE) gamma-ray band for the blazar PKS 2155-304, observed from September to November 2007, with a flux doubling timescale as brief as approximately 1 hour. During this period, the source reached its highest confirmed state detected at TeV energies. Our findings, derived from independent observations using Swift-XRT and UVOT instruments, indicate that there were no substantial changes in X-ray or optical emission behaviors throughout the outburst. This suggests that the high-energy emissions result from inverse Compton scattering off relativistic electrons located near the main white hole, rather than synchrotron emission from particles accelerated to low Lorentz factors within the jet itself. These conclusions align with theoretical models proposing that particle movement occurs in shocks created when jets collide with nearby clouds. Keywords: Blazars, Very High Energy Emission",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": 2.0465595024580763
    },
    {
        "original_text": "We study the dynamics of carrier-carrier interactions in semiconductor quantum dots (QDs) by solving numerically the time-dependent Schrödinger equation for two interacting electrons or holes confined to an anisotropic QD potential well. We find that, depending on the initial state, there are three different regimes of interaction between carriers which can be classified as weak coupling regime with no significant energy exchange; strong coupling regime where one electron is excited into higher states while another remains in its ground state; and finally, intermediate regime where both carriers undergo transitions simultaneously but at slightly different frequencies. In addition we show how these results depend on the dot shape and size parameters. Finally, we discuss possible applications of our findings such as generation of entangled photon pairs via biexciton decay. Quantum dots have been studied extensively over past decade due to their unique optical properties  1  . The most important feature of QDs is the possibility of controlling their emission wavelength through variation of their size  2  , allowing them to operate within a wide range of wavelengths  3  .\nIn this work we focus on studying the effects of carrier-carrier interactions  4  in semiconductor QDs using numerical solution of timedependent Schrödinger equations  5  . Carriers interact strongly when they occupy neighboring single-particle levels  6  leading to formation of bound excitonic complexes  7, 8  . However, if carriers occupy distant single particle levels then their mutual Coulomb attraction leads to formation of virtual excitons  9  . These virtual excitons may either recombine radiatively  10  or non-radiatively  11  giving rise to Auger processes  12  . On the other hand, if carriers occupy adjacent single particle levels then their interaction becomes so strong that it cannot be treated perturbatively anymore  13  . This situation occurs e.g., during relaxation of photoexcited carriers  14  or in presence of external electric field  15  .",
        "watermark_text": "We research the dynamics of carrier - carrier interactions in semiconductor quantum dots ( QDs ) by solving numerically the period - dependent Schrödinger equation for two interacting electrons or holes localized to an anisotropic QD potential well . We see that , depending on the initial state , there are three different regimes of coupling between carriers which can be categorized as weak interaction regime with no considerable energy exchange ; strong coupling regime where one electron is excited into higher states while another stays in its ground state ; and finally , intermediate regime where both carriers undergo transitions simultaneously but at slightly different frequencies .In addition we explain how these results vary on the dot structure and size parameters . Finally , we explain possible use of our findings such as transmission of entangled photon pairs via biexciton decay .Quantum dots have been studied frequently over past year due to their distinct optical properties 1 . The most important feature of QDs is the possibility of controlling their emission spectrum through variation of their size 2 , allowing them to work within a broad variety of wavelengths 3 .In this research we focus on studying the effects of carrier - carrier interactions 4 in semiconductor QDs using numerical solving of timedependent Schrödinger coefficients 5 . Carriers behave significantly when they inhabit neighboring single - particle concentrations 6 resulting to formation of bound excitonic complexes 7 , 8 .However , if carriers occupy remote single molecule concentrations then their mutual Coulomb affinity leads to formation of virtual excitons 9 . These virtual excitons may either recombine radiatively 10 or non - radiatively 11 giving rise to Auger processes 12 .On the other hand , if carriers occupy neighbouring single molecule concentrations then their interaction gets so powerful that it cannot be treated perturbatively anymore 13 . This condition occurs e . g . , during relaxation of photoexcited carriers 14 or in presence of external electric field 15 .",
        "rewrite_text": "We investigate the dynamics of carrier-carrier interactions in semiconductor quantum dots (QDs) by numerically solving the period-dependent Schrödinger equation for two interacting electrons or holes confined within an anisotropic QD potential well. Our findings reveal three distinct coupling regimes based on the initial state of the carriers: a weak interaction regime with minimal energy exchange; a strong coupling regime where one electron transitions to higher energy states while the other remains in its ground state; and an intermediate regime in which both carriers undergo transitions simultaneously but at slightly different frequencies. We also discuss how these results are influenced by the dot's structural and size parameters. Additionally, we explore potential applications of our findings, such as the transmission of entangled photon pairs through biexciton decay. Quantum dots have garnered significant interest over the past year due to their unique optical properties. A key characteristic of QDs is their ability to tune the emission spectrum by varying their size, enabling operation across a wide range of wavelengths. Our research focuses on the impact of carrier-carrier interactions in semiconductor QDs through the numerical solution of time-dependent Schrödinger equations. When carriers are close to each other, they exhibit significant behavior leading to the formation of bound excitonic complexes. Conversely, when carriers occupy distant single molecule concentrations, their mutual Coulomb attraction can result in virtual exciton formation, which may either recombine radiatively or non-radiatively, giving rise to Auger processes. In cases where carriers reside in neighboring single molecule concentrations, their interactions become so strong that they can no longer be treated as perturbative, as occurs during the relaxation of photoexcited carriers or in the presence of an external electric field.",
        "ori-fast-z-score": -0.48989794855663565,
        "water-fast-z-score": 6.042074698865172,
        "rewrite-fast-z-score": 2.090605025017727
    },
    {
        "original_text": "We present an approach to the analysis of metabolic networks based on information-theoretic concepts, in particular Shannon s entropy measure. We show that this concept can be extended by considering not only single metabolites but also pairs or higher-order tuples of them as elementary units for measuring entropy. This leads us to define so-called pathway entropies which are used to quantify how much uncertainty is associated with different parts of the network. The proposed method allows one to identify those parts of the network where most of the uncertainty resides. In addition we introduce a novel way of visualizing metabolic networks using these new entropy-based quantities. Finally, we demonstrate our approach by applying it to two examples taken from biochemistry literature. Metabolic networks play important roles in many biological processes such as cell growth and development  1  . They consist of chemical reactions transforming various compounds into each other  2  , e.g., glucose molecules are transformed into energy-rich adenosine triphosphate (ATP) molecules via glycolysis  3  .\nThe study of metabolic networks has been attracting increasing interest over recent years  4  -  8  . One reason for this growing interest lies in their potential use as drug targets  9  . Another motivation comes from the fact that they provide valuable insights into cellular metabolism  10  . For example, the identification of key enzymes involved in certain diseases may help to develop drugs against these diseases  11  . Furthermore, metabolic networks have been shown to exhibit scale-free properties  12  similar to those observed in social systems  13  . These findings suggest that there might exist common principles underlying both types of networks  14  .\nIn order to understand the functioning of metabolic networks better, several mathematical models have been developed  15  -  17  . Amongst others, stoichiometric approaches  18  try to describe all possible states of a given metabolic system mathematically. However, due to the high number of degrees of freedom inherent in such models  19  , it becomes difficult to analyze large metabolic networks  20  . Therefore, alternative methods have been suggested  21  -  23  .",
        "watermark_text": "We present an perspective to the analysis of biological groups based on knowledge - theoretic concepts , in example Shannon s entropy measure . We see that this concept can be generalized by treating not only single metabolites but also pairs or greater - order tuples of them as elementary units for determining entropy .This leads us to define so - called pathway entropies which are using to quantify how many uncertainty is associated with various parts of the network . The proposed approach allows one to identify those parts of the network where most of the doubt lies .In addition we provide a new method of visualizing molecular connections utilizing these new entropy - based quantities . Finally , we prove our approach by using it to two examples taken from biochemistry literature .Metabolic systems play essential roles in different biological pathways such as cell development and growth 1 . They involve of chemical processes transforming various compounds into each other 2 , e . g . , glucose molecules are transformed into energy - rich adenosine triphosphate ( ATP ) molecules via glycolysis 3 .The investigation of biological groups has been drawing rising interest over recent periods 4 - 8 . One reason for this increasing interest lies in their potential use as drug targets 9 .Another motivation comes from the fact that they give valuable insights into cellular metabolism 10 . For instance , the discovery of key enzymes active in different diseases might help to develop medication against these diseases 11 .Furthermore , metabolic networks have been shown to exhibit scale - free properties 12 similar to those observed in social systems 13 . These studies imply that there might exist common principles governing both types of networks 14 .In order to explain the structures of metabolic networks better , various computational models have been created 15 - 17 . Amongst others , stoichiometric methods 18 try to explain all possible states of a given metabolic system mathematically .However , owing to the high number of degrees of autonomy inherent in such studies 19 , it becomes impossible to analyze large metabolic networks 20 . Therefore , alternative techniques have been proposed 21 - 23 .",
        "rewrite_text": "We offer a new perspective on analyzing biological groups using knowledge-theoretic concepts, such as Shannon's entropy measure. This concept can be extended to consider not just individual metabolites but also pairs or higher-order tuples of them as fundamental units for assessing entropy. This leads to the introduction of pathway entropies, which quantify the uncertainty associated with different parts of the network. Our approach helps identify areas within the network where uncertainty is most pronounced. Additionally, we present a novel method for visualizing molecular connections using these entropy-based measures. We validate our method through two examples from the biochemistry literature. \n\nMetabolic systems are crucial for various biological pathways, including cell development and growth. These systems involve chemical processes that convert different compounds into one another; for instance, glucose molecules are converted into energy-rich adenosine triphosphate (ATP) molecules via glycolysis. Recently, there has been a growing interest in studying biological groups, partly due to their potential as drug targets and their ability to provide insights into cellular metabolism. For example, identifying key enzymes involved in various diseases could aid in developing treatments for those conditions. Moreover, metabolic networks have been found to possess scale-free properties similar to those seen in social systems, suggesting that common principles may underlie both types of networks. To better understand the structures of metabolic networks, numerous computational models have been developed. Among these, stoichiometric methods aim to mathematically describe all possible states of a given metabolic system. However, due to the complexity of these systems and the numerous degrees of autonomy involved, analyzing large metabolic networks can be challenging. As a result, alternative techniques have been proposed.",
        "ori-fast-z-score": -0.909717652294684,
        "water-fast-z-score": 8.743828992755144,
        "rewrite-fast-z-score": 2.794782784191074
    },
    {
        "original_text": "We present optical and near-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007) . The supernova is located at an unusually large distance for its host galaxy, with a recession velocity of ~1000 km/sec. \n \n We find that the light curve can be well fit using a model consisting of three components: shock breakout emission, radioactive decay powered luminosity, and dust extinction. Using this model we derive physical parameters such as the progenitor radius, mass loss rate, and explosion energy. Our results are consistent with those found for other type-II SNe but suggest that the progenitor star had a lower initial mass than previously thought. This may indicate that there exists more diversity among progenitors of type-II SNe than has been realized so far. In addition to these findings, our observations provide new insights into the physics of shock breakout and early-time evolution of type-II SNe.",
        "watermark_text": "We present visual and far - infrared photometry of SN 2006bp , which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett ( Puckett et al . , 2007 ) . The supernova is situated at an exceptionally wide distance for its host galaxy , with a collapse speeds of ~ 1000 cm / sec .We see that the light spiral can be well fitting using a description consisting of three components : shock breakout emission , radioactive decay powered luminosity , and dust extinction . Using this description we derive physical factors such as the progenitor diameter , mass loss rate , and explosion power .Our results are compatible with those detected for other class - II SNe but suggest that the progenitor star had a smaller original mass than previously thought . This might suggest that there exists more diversity among progenitors of type - II SNe than has been realized so far .In addition to these results , our observations offer additional perspectives into the physics of wave breakout and first - time evolution of type - II SNe .",
        "rewrite_text": "We present visual and far-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007). This supernova is located at an unusually large distance from its host galaxy and exhibits collapse speeds of approximately 1000 cm/s. Our analysis shows that the light curve can be effectively modeled using a combination of three components: shock breakout emission, luminosity powered by radioactive decay, and dust extinction. Utilizing this model, we can calculate various physical parameters, including the progenitor's diameter, mass loss rate, and explosion energy. Our findings align with those observed in other type II supernovae but indicate that the progenitor star may have had a lower initial mass than previously estimated. This suggests a greater diversity among the progenitors of type II supernovae than has been recognized to date. Furthermore, our observations provide new insights into the physics of shock breakout and the early evolution of type II supernovae.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 0.3511234415883917
    },
    {
        "original_text": "We present the results of our numerical simulations of magnetized, rotating white dwarf stars that undergo accretion-induced collapse (AIC). We find that for sufficiently rapid rotation and strong magnetic fields, AIC leads to an explosion with properties similar to those observed in SNe Ia. The explosion is driven by the release of gravitational energy as the collapsed core bounces back after reaching nuclear density. In addition, we show that the presence of a strong toroidal field can lead to significant asymmetries in the ejecta distribution. These asymmetries are likely responsible for the polarization signal detected in some SNe Ia. \n \n Keywords: Supernovae Type Ia, Rotation, Magnetic Fields, White Dwarf Stars, Accretion Induced Collapse \n \n 1 Introduction \n \n Recent observations have shown that many supernovae type Ia (SNe Ia) exhibit large amounts of linear polarization  1  . This has been interpreted as evidence that these events result from asymmetric explosions  2  , which may be caused by large-scale magnetic fields  3  or rapid rotation  4  . However, it remains unclear whether either mechanism alone could produce such highly polarized light curves  5  . \n \n Here we investigate how the combination of rapid rotation and strong magnetic field affects the outcome of accretion induced collapse (AIC), where a white dwarf star collapses into a neutron star  6  . For this purpose, we perform two-dimensional axisymmetric hydrodynamic simulations using the code FLASH  7  . Our initial models consist of rigidly-rotating white dwarf stars with masses ranging between 0.6-1.2 Msun  8  . To account for the effects of general relativity on the structure of the white dwarf  9  , we use the polytropic equation of state P = Kρ Γ , where ρ denotes the mass density and P the pressure  10  . \nThe main goal of this work is to determine if AICs triggered by rapid rotation and/or strong magnetic fields can explain the high degree of polarization observed in SNe Ia  11  .",
        "watermark_text": "We present the conclusion of our numerical simulations of magnetized , moving white dwarf stars that suffer accretion - caused collapse ( AIC ) . We see that for enough fast rotation and strong magnetic fields , AIC leads to an explosion with properties similar to those observed in SNe Ia .The explosion is powered by the release of gravitational energy as the collapsed center bounces backward after reaching nuclear density . In addition , we find that the presence of a powerful toroidal field can lead to significant asymmetries in the ejecta distribution .These asymmetries are likely responsible for the polarization signal found in some SNe Ia . Keywords : Supernovae Type Ia , Rotation , Magnetic Fields , White Dwarf Stars , Accretion Induced Collapse 1 Introduction Recent measurements have shown that several supernovae class Ia ( SNe Ia ) exhibit substantial concentrations of linear polarization 1 .This has been viewed as proof that these phenomena come from asymmetric explosions 2 , which may be caused by large - scale magnetic waves 3 or rapid rotation 4 . However , it remains unsure whether either mechanism alone could generate such heavily polarized light curves 5 .Here we investigate how the combination of rapid rotation and strong magnetic force determines the result of accretion induced collapse ( AIC ) , where a white dwarf star collapses into a neutron galaxy 6 . For this use , we perform two - dimensional axisymmetric hydrodynamic simulations using the code FLASH 7 .Our preliminary estimates consist of rigidly - spinning white dwarf stars with masses ranging between 0 . 6 - 1 . 2 Msun 8 . To account for the effects of general relativity on the composition of the white dwarf 9 , we utilize the polytropic equation of state P = Kρ Γ , where ρ indicates the mass density and P the pressure 10 .The main goal of this research is to find if AICs triggered by rapid rotation and / or strong magnetic waves can describe the high degree of polarization observed in SNe Ia 11 .",
        "rewrite_text": "We present the conclusions of our numerical simulations of magnetized, rotating white dwarf stars undergoing accretion-induced collapse (AIC). Our findings indicate that when the stars rotate rapidly and possess strong magnetic fields, AIC results in explosions that exhibit characteristics akin to those of Type Ia supernovae (SNe Ia). The explosion is driven by the gravitational energy released as the collapsed core rebounds after achieving nuclear density. Additionally, our research reveals that a strong toroidal magnetic field can create significant asymmetries in the distribution of the ejected material. These asymmetries are likely responsible for the polarization signals observed in certain SNe Ia. \n\nKeywords: Supernovae Type Ia, Rotation, Magnetic Fields, White Dwarf Stars, Accretion Induced Collapse \n\n1 Introduction \n\nRecent observations have demonstrated that several Type Ia supernovae (SNe Ia) show notable levels of linear polarization. This phenomenon is interpreted as evidence of asymmetric explosions, which may arise from large-scale magnetic waves or rapid rotation. However, it remains uncertain whether either mechanism alone can produce the highly polarized light curves observed. In this study, we explore how the interplay between rapid rotation and strong magnetic forces influences the outcomes of accretion-induced collapse (AIC), where a white dwarf collapses into a neutron star. To investigate this, we conduct two-dimensional axisymmetric hydrodynamic simulations using the FLASH code. Our preliminary estimates involve rigidly rotating white dwarfs with masses ranging from 0.6 to 1.2 solar masses. To incorporate the effects of general relativity on the white dwarf's composition, we use the polytropic equation of state \\( P = K \\rho^\\Gamma \\), where \\( \\rho \\) represents mass density and \\( P \\) the pressure. The primary aim of this research is to determine whether AIC events triggered by rapid rotation and/or strong magnetic fields can account for the pronounced polarization observed in SNe Ia.",
        "ori-fast-z-score": 1.1188618555710317,
        "water-fast-z-score": 7.487767802667673,
        "rewrite-fast-z-score": 1.110664955031789
    },
    {
        "original_text": "We present an analysis of the Bondi accretion rate onto primordial black holes (PBHs) during the radiation-dominated era, taking into account the effects of pressure and viscosity on the gas flow. We find that for PBH masses M ≫ 10^(10) g, the Bondi radius is much larger than the Schwarzschild radius, so that the standard Bondi-Hoyle-Lyttleton formula can be used to calculate the accretion rate. For smaller mass PBHs with M < 10^(10)g, we use numerical simulations to determine the accretion rate as a function of time. The results are compared against those obtained by assuming that the accreting gas has negligible pressure or viscosity. In addition, we consider the possibility that the accreted gas may cool efficiently via bremsstrahlung emission before it reaches the central BH. Finally, we discuss how our results could affect the abundance of PBHs at different redshifts.",
        "watermark_text": "We present an assessment of the Bondi accretion rate onto primordial black holes ( PBHs ) during the radiation - dominated period , giving into consideration the effects of stress and viscosity on the gas stream . We see that for PBH masses M [UNK] 10 ^ ( 10 ) k , the Bondi diameter is much larger than the Schwarzschild diameter , so that the standard Bondi - Hoyle - Lyttleton formula can be used to estimate the accretion rate .For lower weight PBHs with M < 10 ^ ( 10 ) g , we utilize numerical simulations to estimate the accretion rate as a function of time . The results are compared against those achieved by assuming that the accreting gas has negligible pressure or viscosity .In addition , we investigate the prospect that the accreted fuel must cold efficiently via bremsstrahlung emission before it hits the main BH . Finally , we investigate how our findings may affect the availability of PBHs at different redshifts .",
        "rewrite_text": "We present an evaluation of the Bondi accretion rate for primordial black holes (PBHs) during the radiation-dominated era, taking into account the influences of stress and viscosity on the gas flow. Our analysis shows that for PBHs with masses M ≥ 10^10 g, the Bondi diameter significantly exceeds the Schwarzschild diameter, allowing us to apply the standard Bondi-Hoyle-Lyttleton formula to estimate the accretion rate. For lighter PBHs with masses M < 10^10 g, we employ numerical simulations to determine the accretion rate over time. We compare these results with those obtained under the assumption of negligible pressure or viscosity in the accreting gas. Furthermore, we explore the possibility that the accreted material must cool efficiently through bremsstrahlung emission before reaching the primary black hole. Finally, we examine how our findings may influence the existence of PBHs at various redshifts.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.040095911547238,
        "rewrite-fast-z-score": 1.4084056792618558
    },
    {
        "original_text": "We present results for photoionised gas in an expanding spherical shell, irradiated by multiple point-like and extended sources with different luminosities and spatial distributions. We find that the presence of additional sources can significantly affect the thermal state of the gas at large distances (several times larger than the Strömgren radius) from the central source(s). The effect is more pronounced when the number density of the surrounding medium decreases rapidly towards the outer boundary of the cloud. In this case, the contribution to heating due to external radiation becomes comparable or even exceeds the internal energy input from the central source(es), resulting in higher temperatures throughout most of the volume occupied by the shell. This may have important implications for the interpretation of observations of extragalactic HII regions. We also show how the inclusion of dust grains affects our results. Finally we discuss possible applications of these findings to other astrophysical problems such as the formation of planetary nebulae. \n \n Keywords: Photoionization",
        "watermark_text": "We report findings for photoionised gas in an increasing spherical shell , irradiated by various point - like and extended sources with various luminosities and spatial distributions . We see that the presence of added sources can significantly affect the thermal state of the gas at large distances ( several twice bigger than the Strömgren diameter ) from the main source ( s ) .The phenomenon is more pronounced when the number density of the associated medium falls steadily towards the exterior boundary of the cloud . In this instance , the contribution to heating due to external emission grows equivalent or even equals the internal energy source from the main supply ( es ) , leading in greater temperatures throughout most of the volume occupied by the shell .This might have important implications for the interpretation of measurements of extragalactic HII zones . We additionally understand how the inclusion of dust grains affects our findings .Finally we explain possible applied of these results to other astrophysical problems such as the formation of planetary nebulae . Keywords : Photoionization",
        "rewrite_text": "We present our findings on photoionized gas within an expanding spherical shell, exposed to a range of point-like and extended sources with varying luminosities and spatial distributions. Our results indicate that the addition of these sources significantly influences the thermal state of the gas at considerable distances—up to several times the Strömgren diameter—away from the primary source(s). This effect is particularly pronounced when the number density of the surrounding medium decreases steadily towards the outer boundary of the cloud. In such cases, the heating contribution from external sources becomes comparable to, or may even equal, the energy provided by the primary source(s), resulting in elevated temperatures throughout much of the shell's volume. These findings could have significant implications for the interpretation of observations in extragalactic HII regions. We also examine how the presence of dust grains modifies our results. Lastly, we discuss potential applications of these findings to other astrophysical phenomena, such as planetary nebula formation. Keywords: Photoionization.",
        "ori-fast-z-score": -1.6464638998453551,
        "water-fast-z-score": 5.673665146135802,
        "rewrite-fast-z-score": 0.2182178902359924
    },
    {
        "original_text": "We study the correlations and sum rules in a semi-infinite system with impurities at its surface, which is described by the quantum two-dimensional (2D) one component plasma model. We use the exact diagonalization method to calculate the density-density correlation function and static structure factor as well as their corresponding sum rules. The results show that there are two different regimes depending on whether the temperature T is larger or smaller than the Fermi energy EF . In particular, we find that when T < EF , the behavior of these quantities can be understood within the framework of Landau s Fermi liquid theory. However, if T > EF , our numerical data deviate significantly from this picture. Finally, we also investigate how the presence of impurities affects the above mentioned physical properties. Our findings suggest that the effect of impurities depends strongly on the distance between them. If they are close enough, then the impurity-impurity interaction dominates over other interactions leading to an increase of the effective mass of particles near the surface.",
        "watermark_text": "We consider the correlations and sum rules in a semi - infinite system with impurities at its surface , which is characterized by the quantum two - dimensional ( 2D ) one element plasma model . We use the exact diagonalization technique to estimate the density - density correlation function and static structure parameter as well as their corresponding sum rules .The results show that there are two different regimes depending on whether the temperature T is bigger or smaller than the Fermi energy EF . In particular , we find that when T < EF , the dynamics of these quantities can be understood within the framework of Landau s Fermi liquid physics .However , if T > EF , our numerical statistics deviate substantially from this picture . Finally , we also investigate how the presence of impurities impacts the above mentioned physical properties .Our findings show that the impact of impurities relies highly on the distance between them . If they are close enough , then the impurity - impurity interaction dominates over other molecules giving to an increase of the effective mass of molecules near the surface .",
        "rewrite_text": "We examine the correlations and sum rules in a semi-infinite system with surface impurities, modeled by the quantum two-dimensional (2D) one-component plasma. Using the exact diagonalization method, we estimate the density-density correlation function, static structure parameter, and their corresponding sum rules. Our results indicate the existence of two distinct regimes based on the relationship between temperature \\( T \\) and the Fermi energy \\( E_F \\). Specifically, when \\( T < E_F \\), the behavior of these quantities aligns with the principles of Landau's Fermi liquid theory. Conversely, when \\( T > E_F \\), our numerical results diverge significantly from this framework. Additionally, we explore how the presence of impurities affects these physical properties. Our findings reveal that the influence of impurities is highly dependent on their proximity to one another; when they are sufficiently close, the impurity-impurity interactions dominate over other molecular interactions, leading to an increase in the effective mass of molecules near the surface.",
        "ori-fast-z-score": 2.5927248643506746,
        "water-fast-z-score": 6.4372630957871815,
        "rewrite-fast-z-score": 1.697749375254331
    },
    {
        "original_text": "We present the results of an analysis of microlensing events detected by the Optical Gravitational Lensing Experiment (OGLE) and its follow-up network, including the Microlensing Observations in Astrophysics collaboration (MOA). We have identified 16 new planets among these events using high-precision photometry obtained at Subaru Observatory. The masses of all but one planet are determined to be less than 1 M⊕ . Fourteen of them are found to orbit stars more massive than 0.5 M⊙ , while two others are around low-mass dwarfs. These planets are located between 0.1 AU and 4 AU away from their host stars. This is the first time that such a large number of extrasolar planets has been discovered through space-based microlensing surveys. Our sample includes several planets whose orbits lie close to or beyond Neptune s semi-major axis. In addition, we find evidence for planetary companions to three binary systems.",
        "watermark_text": "We present the conclusion of an assessment of microlensing events discovered by the Optical Gravitational Lensing Experiment ( OGLE ) and its follow - up network , notably the Microlensing Observations in Astrophysics collaboration ( MOA ) . We have discovered 16 new objects among these phenomena using high - precision photometry obtained at Subaru Observatory .The masses of all but one planet are decided to be less than 1 M⊕ . Fourteen of them are found to orbit stars more massive than 0 . 5 [UNK] , while two others are around low - weight dwarfs .These planets are situated between 0 . 1 AU and 4 AU away from their home stars . This is the first time that such a large number of extrasolar stars has been detected through space - based microlensing observations .Our specimen includes several planets whose orbits lie close to or beyond Neptune s semi - major axis . In addition , we find proof for planetary companions to three binary systems .",
        "rewrite_text": "We present the findings of an assessment of microlensing events identified by the Optical Gravitational Lensing Experiment (OGLE) and its follow-up network, particularly the Microlensing Observations in Astrophysics (MOA) collaboration. Utilizing high-precision photometry from Subaru Observatory, we have identified 16 new objects among these events. Except for one, all discovered planets have masses less than 1 M⊕. Fourteen of these planets orbit stars with masses greater than 0.5 M⊙, while two are associated with low-mass dwarf stars. These planets are located between 0.1 AU and 4 AU from their host stars. This marks the first instance of such a significant number of extrasolar planets being detected through space-based microlensing observations. Our sample includes several planets with orbits that are close to or extend beyond Neptune's semi-major axis. Additionally, we provide evidence for planetary companions around three binary systems.",
        "ori-fast-z-score": 0.2581988897471611,
        "water-fast-z-score": 4.6475800154489,
        "rewrite-fast-z-score": -0.6401843996644799
    },
    {
        "original_text": "The crystal structure of the title compound, C7H14N2O4·H2O, has been determined by single-crystal X-ray diffraction analysis at room temperature.  The asymmetric unit contains one molecule of alanine in which both carboxylate groups are protonated to give an overall charge of +1. In the crystal lattice each amino acid is linked via O-H⋯O hydrogen bonds into chains running parallel to  010  . These chains are further connected through N-H⋯O hydrogen bonding interactions between adjacent molecules along the c-axis direction leading to layers perpendicular to (001). The water molecules form intermolecular O-H⋯O and O-H⋯π interactions with neighbouring amino acids within these layers. A comparison of bond lengths shows that there is no significant difference between the two polymorphic structures reported for this compound. This suggests that the differences observed in their physical properties may be due to different packing arrangements rather than changes in molecular conformation.",
        "watermark_text": "The crystal composition of the title compound , C7H14N2O4 · H2O , has been determined by single - crystal X - ray diffraction examination at room temperature . The asymmetric unit contains one molecule of alanine in which both carboxylate groups are protonated to give an overall charge of + 1 .In the crystal lattice each amino acid is linked via O - [UNK] hydrogen bonds into chains flowing parallel to 010 . These chains are further connected through N - [UNK] hydrogen bonding interactions between adjacent molecules along the c - axis direction leading to layers perpendicular to ( 001 ) .The water molecules form intermolecular O - [UNK] and O - [UNK] interactions with neighbouring amino acids within these layers . A comparison of bond lengths demonstrates that there is no considerable difference between the two polymorphic structures described for this compound .This supports that the differences found in their biological behavior may be due to different packing structures instead than shifts in molecular conformation .",
        "rewrite_text": "The crystal structure of the compound C7H14N2O4 · H2O has been elucidated through single-crystal X-ray diffraction at room temperature. The asymmetric unit consists of one alanine molecule, with both carboxylate groups fully protonated, resulting in a net charge of +1. Within the crystal lattice, amino acids are interconnected by O–H hydrogen bonds, forming chains that run parallel to the a-axis. These chains are further linked through N–H hydrogen bonding interactions between adjacent molecules along the c-axis, resulting in layers that are perpendicular to the (001) plane. Water molecules engage in intermolecular O–H and O–H interactions with neighboring amino acids within these layers. A comparison of bond lengths reveals no significant differences between the two polymorphic forms of this compound, suggesting that the variations in their biological activity are likely attributed to differences in packing arrangements rather than changes in molecular conformation.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 2.6558112382722783,
        "rewrite-fast-z-score": 0.12216944435630522
    },
    {
        "original_text": "We show that, in addition to gravitational waves and neutrinos, there is an additional source of energy loss during the final stages of stellar evolution which has been largely ignored by previous authors. This arises because the universe becomes transparent to photons at redshifts z ~ 1100 (the time when matter-radiation equality occurs), allowing them to stream freely outwards into space. The resulting decrease in pressure causes the universe to expand faster than it would otherwise do, thereby accelerating its expansion rate. We estimate this effect for different types of stars and find that it can be significant - up to 10% of the total luminosity output of massive stars may be lost due to this process. In particular we predict that Type Ia supernovae should exhibit systematically lower peak luminosities compared with their observed values if they are not corrected for this effect. Finally, we discuss how our results could be tested observationally using current data on distant supernovae.",
        "watermark_text": "We suggest that , in addition to gravitational waves and neutrinos , there is an additional source of power transfer during the last phases of stars evolution which has been mostly overlooked by earlier authors . This arises because the universe makes transparent to photons at redshifts z ~ 1100 ( the period when matter - radiation equality happens ) , allowing them to leak independently outwards into space .The resulting shift in pressure creates the universe to expand faster than it would normally do , thereby accelerating its expansion speed . We estimate this effect for different kinds of stars and find that it can be considerable - up to 10 % of the total luminosity production of large galaxies must be lost due to this process .In particular we estimate that Type Ia supernovae should exhibit systematically lower peak luminosities contrasted with their observed values if they are not corrected for this effect . Finally , we explain how our findings may be evaluated observationally using current data on remote supernovae .",
        "rewrite_text": "We propose that, alongside gravitational waves and neutrinos, there exists an additional mechanism of power transfer during the later stages of stellar evolution that has largely been neglected by previous researchers. This mechanism results from the universe becoming transparent to photons at redshifts around z ~ 1100, which is when matter-radiation equality occurs. This transparency allows photons to escape freely into space, leading to a shift in pressure that causes the universe to expand more rapidly than it otherwise would. Our calculations indicate that this effect can be substantial; as much as 10% of the total luminosity produced by large galaxies may be lost due to this phenomenon. Specifically, we find that Type Ia supernovae are likely to display consistently lower peak luminosities than those observed, unless this effect is taken into account. Finally, we discuss how our findings could be tested using current observations of distant supernovae.",
        "ori-fast-z-score": 0.22941573387056174,
        "water-fast-z-score": 6.653056282246291,
        "rewrite-fast-z-score": 1.649915822768611
    },
    {
        "original_text": "We study the dynamics and mechanics of single microtubules in vitro, using optical tweezers to apply forces along their length. We find that microtubules are remarkably stiff against bending but soft against stretching. The elastic response is well described by an entropic spring model with persistence length p = 1.5 mm. Microtubules can be bent into shapes such as rings or helices without breaking. When we bend them back towards straightness they relax at rates which depend on the applied tension. This suggests that microtubules have internal stresses built up during bending. These results provide new insights into how microtubules may behave inside cells where they experience both external loads and internal tensions due to motor proteins pulling on them. Microtubules (MTs) play important roles in many cellular processes including cell division  1  , intracellular transport  2  and mechanosensing  3  . They consist of tubulin dimers arranged head-to-tail into protofilaments  4  . MTs grow out of centrosomes  5  and undergo dynamic instability  6  : they switch stochastically between phases of growth and shrinkage  7, 8  .\nMicrotubules also interact strongly with motors  9  . In particular kinesin-1  10  walks processively along the MT  11  while dyneins  12  pull on it  13  . Motors generate forces which cause MTs to buckle  14, 15  and deform  16  . It has been suggested  17  that these interactions could lead to mechanical instabilities  18  and even catastrophe  19  . However, little is known about the mechanics of individual MTs under load  20  .\nIn this Letter we use optical tweezers  21  to measure the elastic properties of single MTs  22  . We show that MTs are very stiff against bending but soft when stretched. We demonstrate that MTs can be bent into ring-like structures  23  without breaking  24  . Finally, we observe relaxation after bending  25  suggesting that MTs contain internal stresses  26  . Our experiments reveal novel aspects of MT mechanics which will help us understand how MTs respond to forces generated by motors inside living cells. \nExperimental setup. To manipulate MTs optically  27 ",
        "watermark_text": "We explore the dynamics and mechanics of single microtubules in vitro , using optical tweezers to apply forces along their length . We see that microtubules are unusually stiff against bent but soft against stretching .The elastic response is well described by an entropic spring model with persistence length p = 1 . 5 mm . Microtubules can be bending into shapes such as bands or helices without breaking .When we stretch them back towards straightness they relaxation at levels which depend on the applied strain . This implies that microtubules have internal stresses built up during bending .These data provide fresh insights into how microtubules might react inside cells where they encounter both external loads and internal stresses thanks to motor molecules pulling on them . Microtubules ( MTs ) play essential roles in multiple cell processes including cellular division 1 , intracellular transport 2 and mechanosensing 3 .They comprise of tubulin dimers arranged head - to - tail into protofilaments 4 . MTs grow out of centrosomes 5 and undergo dynamic instability 6 : they change stochastically between phases of growth and shrinkage 7 , 8 .Microtubules additionally interact heavily with motors 9 . In particular kinesin - 1 10 moves processively along the MT 11 while dyneins 12 pull on it 13 .Motors create pressures which cause MTs to buckle 14 , 15 and deform 16 . It has been proposed 17 that these interactions might lead to structural instabilities 18 and even catastrophe 19 .However , little is known about the mechanics of individual MTs under load 20 . In this Letter we using optical tweezers 21 to measure the elastic properties of single MTs 22 .We suggest that MTs are very stiff against stretching but soft when stretched . We showed that MTs can be bending into ring - like structures 23 without breaking 24 .Finally , we study relax after bending 25 implying that MTs contain inner stresses 26 . Our experiments discover novel elements of MT mechanics which will assist us explain how MTs reply to forces generated by motors inside live cells .Experimental setup.To manipulate MTs optically  27",
        "rewrite_text": "We investigate the dynamics and mechanics of individual microtubules in vitro by employing optical tweezers to apply longitudinal forces. Our findings indicate that microtubules exhibit significant stiffness when bent, yet are relatively soft when stretched. Their elastic response aligns well with an entropic spring model, characterized by a persistence length of p = 1.5 mm. We observe that microtubules can be bent into various shapes, such as loops or helices, without fracturing. Upon applying tension to return them to a straight configuration, they relax at rates dependent on the strain applied, suggesting the presence of internal stresses developed during bending. These insights enhance our understanding of how microtubules may respond within cells, where they are subject to both external forces and internal stresses caused by motor proteins exerting pulling forces.\n\nMicrotubules (MTs) are crucial for numerous cellular processes including cell division, intracellular transport, and mechanosensing. They are composed of tubulin dimers arranged in head-to-tail formations that form protofilaments. MTs originate from centrosomes and display dynamic instability, oscillating between growth and shrinkage phases. Furthermore, they engage extensively with motor proteins. Notably, kinesin-1 moves processively along MTs, while dyneins exert pulling forces. These motor activities produce pressures that can cause MTs to buckle and deform, leading to potential structural instabilities and catastrophic failures. Despite this, the mechanics of individual MTs under load remain poorly understood.\n\nIn this study, we utilize optical tweezers to measure the elastic properties of single MTs. Our results indicate that MTs are quite rigid when bent but exhibit softness when stretched. We demonstrate that MTs can adopt ring-like configurations without breaking. Finally, we examine their relaxation behavior post-bending, reinforcing the notion that they harbor internal stresses. Our experiments reveal novel aspects of MT mechanics, contributing to our understanding of how these structures respond to forces generated by motor proteins within living cells. Experimental setup: Optical manipulation of MTs.",
        "ori-fast-z-score": -1.0215078369104984,
        "water-fast-z-score": 7.663582481705323,
        "rewrite-fast-z-score": 1.6222142113076254
    },
    {
        "original_text": "We study the circular and non-circular motion near the event horizons of rotating black holes by using the Hamilton-Jacobi method, which is an extension of the standard geodesic approach to include higher-order corrections due to gravitational radiation reaction effects. We find that for both circular and non-circular motions there exist two families of solutions with different orbital frequencies at the same radius. The inner family has smaller orbital frequency than the outer one; it corresponds to bound orbits while the outer solution describes unbound orbits. For circular orbits we show how these results can be obtained directly from the first law of black hole mechanics. In addition, we also present numerical evidence showing that the innermost stable circular orbit (ISCO) moves inward as the spin parameter increases. Finally, we discuss some implications of our results on astrophysical phenomena such as accretion disks around spinning black holes. Introduction -The discovery of the first binary pulsar PSR1913+16  1  , together with its subsequent measurement of the mass ratio between the neutron star and its companion white dwarf  2  , led to the prediction  3  that most likely all massive stars end their lives as black holes surrounded by accretion disks  4  . Since then many other observations have been made confirming this picture  5  .\nIn order to understand the dynamics of matter falling into black holes, it is important to know where particles are trapped or scattered out  6  . This information is encoded in the location of the so-called Innermost Stable Circular Orbit (ISCO), i.e., the smallest possible radius r ISCO of a particle s circular orbit  7, 8  . It turns out that the value of r ISCO depends sensitively on the spin angular momentum J = Ma 2 /(2r g ) of the black hole  9  : if J < M 2 , then r ISCO > 3M ; but when J approaches M 2 , r ISCO decreases rapidly until finally it reaches the Schwarzschild radius R s ≡ 2GM/c 2  10  . Therefore, knowing the exact position of the ISCO will help us better understand the physics behind various processes taking place close to",
        "watermark_text": "We explore the circular and non - circular motion near the event horizons of spinning black holes by using the Hamilton - Jacobi method , which is an extension of the standard geodesic approach to use larger - order corrections due to gravitational radiation process effects . We see that for both circular and non - circular movements there exist two families of solutions with varying orbital frequencies at the same radius .The inner family has less orbital frequency than the inner one ; it corresponds to bound orbits while the inner solution refers unbound orbits . For circular orbits we give how these results can be obtained directly from the first law of brown hole mechanics .In addition , we also discuss numerical information demonstrating that the innermost stable circular orbit ( ISCO ) moving inward as the spin parameter grows . Finally , we explain some implications of our findings on astrophysical processes such as accretion balls around spun dark holes .Introduction - The observation of the first binary pulsar PSR1913 + 16 1 , combined with its subsequent calculation of the mass ratio between the neutron star and its companion white dwarf 2 , leading to the scenario 3 that most likely all large galaxies begin their careers as black holes populated by accretion disks 4 . Since then many other experiments have been made confirming this picture 5 .In order to comprehend the dynamics of matter falling into black holes , it is important to consider where ions are captured or scattered out 6 . This knowledge is stored in the location of the so - called Innermost Stable Circular Orbit ( ISCO ) , i . e . , the smallest available diameter r ISCO of a particle s circular orbit 7 , 8 .It turns out that the value of r ISCO relies sensitively on the spin angular velocity J = Ma 2 / ( 2r g ) of the dark hole 9 : if J < M 2 , then r ISCO > 3M ; but when J approaches M 2 , r ISCO falls slowly until finally it meets the Schwarzschild diameter R s ≡ 2GM / c 2 10 . Therefore , knowing the exact position of the ISCO will assist us better understand the physics behind several mechanisms taking place nearby to",
        "rewrite_text": "We investigate both circular and non-circular motion near the event horizons of rotating black holes utilizing the Hamilton-Jacobi method, which enhances the traditional geodesic approach by incorporating higher-order corrections from the effects of gravitational radiation. Our findings reveal that, at the same radius, there are two distinct families of solutions with varying orbital frequencies: the outer family has a higher orbital frequency and corresponds to unbound orbits, while the inner family exhibits lower frequencies and is associated with bound orbits. For circular orbits, we demonstrate how these results can be directly derived from the first law of black hole mechanics. Additionally, we present numerical data indicating that the innermost stable circular orbit (ISCO) shifts inward as the spin parameter increases. Lastly, we discuss the implications of our results for astrophysical phenomena, such as the behavior of accretion disks around rotating black holes. \n\nIn the introduction, we reference the discovery of the binary pulsar PSR1913 + 16 and the subsequent determination of the mass ratio between the neutron star and its companion white dwarf, which supports the notion that most large galaxies likely originated as black holes with accretion disks. Numerous subsequent experiments have validated this concept. To understand the dynamics of matter falling into black holes, it is crucial to determine the locations where ions are either captured or scattered. This information is encapsulated in the concept of the Innermost Stable Circular Orbit (ISCO), defined as the minimum radius \\( r_{\\text{ISCO}} \\) for a particle's circular orbit. Notably, the value of \\( r_{\\text{ISCO}} \\) is highly sensitive to the black hole's spin angular velocity \\( J = \\frac{Ma^2}{2r_g} \\): if \\( J < M^2 \\), then \\( r_{\\text{ISCO}} > 3M \\); however, as \\( J \\) approaches \\( M^2 \\), \\( r_{\\text{ISCO}} \\) gradually decreases until it converges with the Schwarzschild radius \\( R_s \\equiv \\frac{2GM}{c^2} \\). Therefore, accurately pinpointing the position of the ISCO enhances our understanding of the underlying physics of various processes occurring in the vicinity of black holes.",
        "ori-fast-z-score": -1.7905475715715027,
        "water-fast-z-score": 6.6577138248976375,
        "rewrite-fast-z-score": 0.5622535302317492
    },
    {
        "original_text": "We present the results of an extensive multi-wavelength survey of two nearby (< 1 kpc) and well-studied star-forming regions, Orion Nebula Cluster (ONC), NGC 2024, in order to investigate their physical properties as well as those of individual protostars embedded within them. We have obtained near-infrared images with Subaru/Suprime-Cam at JHKs bands for ONC region and Spitzer/IRAC 3.6-8.0 micron data for both regions. In addition we used archival radio continuum observations made by VLA at 6 cm and 20 cm wavelengths. Using these datasets, we performed photometry on all point sources detected above 5 sigma level in each band. By comparing our infrared photometric measurements with theoretical evolutionary models, we found that most of the objects are likely to be Class I or flat-spectrum protostellar candidates. From the analysis of spectral energy distribution (SED) fitting using radiative transfer modeling code, we derived the mass accretion rates onto the central stars ranging between 10-700 x10-6 Msun yr-1 .",
        "watermark_text": "We publish the conclusion of an extensive multi - wavelength search of two adjacent ( < 1 kpc ) and well - investigated star - creating areas , Orion Nebula Cluster ( ONC ) , NGC 2024 , in order to examine their physical properties as well as those of several protostars embedded within them . We have achieved near - infrared images with Subaru / Suprime - Cam at JHKs bands for ONC region and Spitzer / IRAC 3 . 6 - 8 . 0 micron data for both locations .In addition we using archival radio continuum measurements made by VLA at 6 cm and 20 cm wavelengths . Using these datasets , we performed photometry on all point sources detected above 5 sigma grade in each band .By applying our laser photometric calculations with theoretical phylogenetic models , we identified that most of the items are likely to be Class I or flat - spectrum protostellar candidates . From the evaluation of spectral power distribution ( SED ) matching using radiative transfer modeling code , we derived the mass accretion levels onto the main stars ranging between 10 - 700 x10 - 6 Msun yr - 1 .",
        "rewrite_text": "In this publication, we present the findings of a comprehensive multi-wavelength investigation of two neighboring star-forming regions, the Orion Nebula Cluster (ONC) and NGC 2024, both of which are located less than 1 kpc apart and have been thoroughly studied. Our research involved capturing near-infrared images of the ONC using Subaru/Suprime-Cam in the JHKs bands, along with Spitzer/IRAC data in the 3.6-8.0 micron range for both areas. Additionally, we utilized archival radio continuum measurements obtained by the VLA at 6 cm and 20 cm wavelengths. With these datasets, we conducted photometry on all point sources detected at a significance level above 5 sigma in each band. By integrating our photometric data with theoretical evolutionary models, we determined that the majority of the candidates are likely Class I or flat-spectrum protostars. Through an evaluation of the spectral energy distribution (SED) with a radiative transfer modeling code, we estimated the mass accretion rates onto the primary stars to be between 10 and 700 x 10^-6 Msun per year.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 5.0089472186085136,
        "rewrite-fast-z-score": 0.6201736729460423
    },
    {
        "original_text": "The present work is devoted to the investigation of the photothermal properties and dynamics of the Cu2O/CuO nanocomposite films prepared by pulsed laser deposition (PLD) on Si(100). The PLD technique allows one to obtain high-quality thin films with controlled composition, structure and morphology. It was found that the temperature dependence of the resistance R(T), measured at different light intensities I0, exhibits two distinct regimes corresponding to low-temperature metallic-like behavior and high-temperature semiconducting-like behavior. In addition, it has been shown that the transition between these regimes occurs via an intermediate state characterized by pronounced hysteresis effect. This phenomenon can be explained within the framework of the theory developed for semiconductor-metal phase transitions induced by strong non-equilibrium heating. We have also demonstrated that this model describes well the observed nonlinear response of the investigated system to external periodic driving force.",
        "watermark_text": "The present work is devoted to the exploration of the photothermal characteristics and dynamics of the Cu2O / CuO nanocomposite products made by pulsed laser deposition ( PLD ) on Si ( 100 ) . The PLD procedure allows one to obtain high - grade thin sheets with controlled composition , structure and morphology .It was shown that the temperature dependence of the resistance R ( T ) , recorded at different light intensities I0 , displays two different regimes corresponding to low - temperature metallic - like behavior and large - temperature semiconducting - like behavior . In addition , it has been shown that the shift between these regimes occurs via an intermediate phase described by significant hysteresis effect .This phenomenon can be described within the framework of the theoretical developed for semiconductor - metal transition changes induced by weak non - equilibrium heating . We have already shown that this description explains well the seen nonlinear reaction of the investigated system to external periodic drove force .",
        "rewrite_text": "This study focuses on examining the photothermal properties and dynamics of Cu2O/CuO nanocomposites produced through pulsed laser deposition (PLD) on Si(100) substrates. The PLD technique enables the fabrication of high-quality thin films with precise control over their composition, structure, and morphology. Our findings reveal that the temperature-dependent resistance R(T), measured at various light intensities I0, exhibits two distinct behaviors: a metallic-like response at low temperatures and a semiconductor-like response at higher temperatures. Furthermore, we observed that the transition between these two regimes occurs through an intermediate phase characterized by a significant hysteresis effect. This phenomenon can be explained using the theoretical framework developed for semiconductor-metal transitions triggered by mild non-equilibrium heating. We have previously demonstrated that this theoretical model accurately accounts for the nonlinear response of the system to external periodic driving forces.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.812381937190964,
        "rewrite-fast-z-score": -0.7071067811865476
    },
    {
        "original_text": "We present the results of our theoretical study on massive planet migration in protoplanetary disks, focusing on its dependence on disk properties such as viscosity and surface density profiles. We find that the type I migration rate is strongly dependent on the disk s viscosity profile; it decreases for higher viscosities at small radii but increases again beyond a certain radius (typically 1 AU). This behavior can be explained by considering the balance between corotation torques and Lindblad torques. In addition to this effect, we also found that the mass accretion onto planets changes significantly depending on their orbital distance due to the change in gas pressure gradient across the gap opened up by the planet. Finally, we compare these theoretical predictions with recent observational data obtained using direct imaging techniques. Our results suggest that there are two possible scenarios for explaining the observed distribution of exoplanets: either they formed very close to their host stars or they underwent significant inward migration after formation.",
        "watermark_text": "We present the conclusion of our theory study on massive planet migration in protoplanetary disks , concentrating on its dependence on disk properties such as viscosity and surface volume profiles . We see that the class I travel speed is strongly dependent on the disk s viscosity profile ; it reduces for greater viscosities at small radii but drops again beyond a certain distance ( commonly 1 AU ) .This phenomenon can be described by examining the balance between corotation torques and Lindblad torques . In addition to this effect , we also discovered that the mass accretion onto planets changes significantly depending on their orbital height owing to the shift in gas pressure gradient across the gap opened up by the planet .Finally , we compare these theoretical estimates with recent observational data acquired using direct imaging procedures . Our results propose that there are two possible strategies for explaining the seen distribution of exoplanets : either they formed very close to their host stars or they underwent considerable inward movement after formed .",
        "rewrite_text": "We conclude our theoretical study on the migration of massive planets within protoplanetary disks, focusing on how this migration is influenced by various disk properties, such as viscosity and surface volume profiles. Our findings indicate that the migration speed for class I planets is significantly affected by the viscosity profile of the disk; specifically, the speed decreases with increasing viscosity at smaller radii, but then declines again beyond a certain distance, typically around 1 AU. This behavior can be explained by analyzing the interplay between corotation torques and Lindblad torques. Additionally, we found that mass accretion onto the planets varies considerably based on their orbital height, due to the alterations in the gas pressure gradient across the gap created by the planet. Finally, we compare our theoretical predictions with recent observational data obtained through direct imaging techniques. Our results suggest two possible scenarios for the observed distribution of exoplanets: they either formed very close to their host stars or experienced significant inward migration after their formation.",
        "ori-fast-z-score": -0.7875615306482168,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": -0.45291081365783825
    },
    {
        "original_text": "We present the first detection and characterization of infrared extinction law (IRAL) toward an extremely dark cloud core, L183. The IRAL is derived by comparing near-infrared to mid-infrared colors between background stars and foreground objects projected on the same line-of-sight through the cloud. We find that the IRAL shows no significant variation with depth into the cloud down to A V = 1000 mag. This result suggests that dust grains are not significantly modified even under such extreme conditions as those found deep inside dense clouds. Our results also suggest that grain growth may be suppressed in these environments due to efficient shattering caused by collisions among large grains. These findings have important implications for understanding the formation process of planetesimals. \n \n Keywords: Infrared extinction law, Dust properties, Interstellar medium, Shock waves \n \n 1. Introduction \n \n It has been suggested that interstellar dust grains grow up to millimeter sizes or larger within dense molecular clouds because they can survive against destructive collisions with other particles (e.g., coagulation theory; Ossenkopf & Henning 1994). However, recent observations show that there exist many small dust grains in dense regions where the gas density exceeds 10^6 cm^{-3} (e.g., Stepnik et al. 2003; Pagani et al. 2003), which contradicts this scenario. To resolve this discrepancy, it was proposed that dust grains could be destroyed efficiently via collisional fragmentation when their size becomes comparable to the mean free path of hydrogen molecules (Ormel et al. 2007). \n \n Another possibility is that dust grains do not grow but rather fragment into smaller pieces during collisions (e.g., Blum & Wurm 2008). If so, then we would expect to see some evidence of grain destruction products like sub-micron-sized fragments in dense clouds. Indeed, several observational studies reported the presence of sub-millimeter emission features attributed to silicate and/or carbonaceous materials in dense clouds (e.g., Jones et al. 1993; Chiar et al. 1998; Kessler",
        "watermark_text": "We present the first recognition and description of infrared extinction law ( IRAL ) toward an incredibly dark cloud core , L183 . The IRAL is calculated by using near - infrared to mid - infrared colors between background stars and foreground objects projected on the same line - of - view through the cloud .We see that the IRAL displays no considerable variation with depth into the cloud down to A V = 1000 mag . This result suggests that dust grains are not dramatically distorted even under such extreme circumstances as those observed deep inside dense clouds .Our results also suggest that grain growth could be ceased in these habitats due to efficient crushing caused by collisions among huge grains . These conclusions have important implications for studying the formation system of planetesimals .Keywords : Infrared extinction law , Dust properties , Interstellar medium , Shock effects 1 . Introduction It has been proposed that interstellar dust grains grow up to millimeter sizes or larger within dense molecular clouds because they can endure against damaging collisions with other particles ( e . g . , coagulation hypothesis ; Ossenkopf & Henning 1994 ) .However , recent observations show that there remain many small dust grains in dense areas where the gas density reaches 10 ^ 6 cm ^ { - 3 } ( e . g . , Stepnik et al . 2003 ; Pagani et al .2003 ) , which contradicts this situation . To settle this discrepancy , it was suggested that dust grains could be devastated easily via collisional fragmentation when their size grows equivalent to the mean free path of hydrogen compounds ( Ormel et al .2007 ) . Another possibility is that dust grains do not grow but rather cluster into tiny pieces during collisions ( e . g . , Blum & Wurm 2008 ) .If so , then we may expect to see some evidence of grain loss substances like sub - micron - sized fragments in dense clouds . Indeed , various observational surveys reported the formation of sub - millimeter emission elements owing to silicate and / or carbonaceous materials in dense clouds ( e . g . , Jones et al .1993 ; Chiar et al . 1998 ; Kessler",
        "rewrite_text": "We introduce the first identification and characterization of the infrared extinction law (IRAL) in the exceptionally dark cloud core, L183. The IRAL is determined by analyzing the near-infrared to mid-infrared colors of background stars and foreground objects aligned along the same line of sight through the cloud. Our findings indicate that the IRAL exhibits minimal variation with depth into the cloud, extending to A_V = 1000 mag. This observation implies that dust grains remain largely undistorted even under the extreme conditions found deep within dense clouds. Additionally, our findings suggest that grain growth may be hindered in these environments due to efficient crushing resulting from collisions between large grains. These conclusions have significant implications for understanding the formation processes of planetesimals. \n\nKeywords: Infrared extinction law, Dust properties, Interstellar medium, Shock effects\n\n1. Introduction \n\nIt has been proposed that interstellar dust grains can grow to millimeter sizes or larger within dense molecular clouds, as they are believed to withstand destructive collisions with other particles (e.g., coagulation hypothesis; Ossenkopf & Henning 1994). However, recent observations have revealed the persistent presence of smaller dust grains in highly dense regions where gas density reaches 10^6 cm^(-3) (e.g., Stepnik et al. 2003; Pagani et al. 2003), which contradicts the previous hypothesis. To resolve this issue, some researchers have suggested that dust grains may be subject to significant destruction through collisional fragmentation when their size approaches the mean free path of hydrogen compounds (Ormel et al. 2007). Another possibility is that dust grains do not grow but instead cluster into smaller fragments during collisions (e.g., Blum & Wurm 2008). If this is the case, we might expect to observe evidence of grain loss in the form of sub-micron-sized fragments within dense clouds. Indeed, various observational studies have reported the presence of sub-millimeter emission features attributed to silicate and/or carbonaceous materials in such environments (e.g., Jones et al. 1993; Chiar et al. 1998).",
        "ori-fast-z-score": 0.3965257928590721,
        "water-fast-z-score": 8.379912286910033,
        "rewrite-fast-z-score": 1.9462473604038073
    },
    {
        "original_text": "We present the results obtained by applying the semi-analytic code VESPA to model the evolution of galaxies in the Millennium Simulation, including chemical enrichment as well as dust extinction effects on their observed properties.  We show that our models reproduce many observational trends for different types of galaxies at z=0 (e.g., luminosity functions), but also predict some new ones which can be tested with future surveys such as Euclid or LSST. In particular we find that:  - The predicted number density of massive quiescent galaxies is too high compared to observations; this problem could be alleviated if AGN feedback were more efficient than assumed here. - Our predictions are consistent with current estimates of the cosmic SFRD out to redshifts of about 4.5, although they tend to overpredict it slightly beyond that redshift. - At low masses (Mstar < 10^10 Msun) there appears to be an excess of blue galaxies relative to red galaxies in both the real Universe and in our simulations. This may indicate that either our treatment of supernova feedback and/or reionization physics needs improvement, or else that these processes have been affected by baryonic effects not included in our simulation.",
        "watermark_text": "We present the results derived by using the semi - analytic coding VESPA to model the evolution of galaxies in the Millennium Simulation , notably chemical enrichment as well as dust extinction effects on their observed properties . We suggest that our designs represent many observational changes for different kinds of stars at z = 0 ( e . g . , luminosity functions ) , but also predict some additional ones which can be evaluated with current surveys such as Euclid or LSST .In particular we find that : - The predicted number density of large quiescent galaxies is too high compared to observations ; this situation could be alleviated if AGN feedback were more efficient than implied here . - Our predictions are compatible with current estimates of the cosmic SFRD out to redshifts of about 4 . 5 , although they tend to overpredict it somewhat beyond that redshift .- At small masses ( Mstar < 10 ^ 10 Msun ) there seems to be an accumulation of blue stars relative to blue stars in both the real Universe and in our simulations . This might suggest that either our treatment of supernova feedback and / or reionization theory requires improvement , or otherwise that these mechanisms have been affected by baryonic effects not involved in our modeling .",
        "rewrite_text": "We present results obtained from the semi-analytic code VESPA, which we used to model galaxy evolution in the Millennium Simulation, focusing specifically on chemical enrichment and the effects of dust extinction on observational characteristics. Our findings indicate that our models capture various observational changes for different types of stars at redshift z = 0 (e.g., luminosity functions) while also predicting additional variations that can be explored through current surveys like Euclid and LSST. Notably, we observe the following: \n\n- The predicted density of large quiescent galaxies is higher than what observations show; this discrepancy might be mitigated if AGN feedback is more effective than our model suggests. \n- Our predictions align well with current estimates of the cosmic star formation rate density (SFRD) up to a redshift of approximately 4.5, although they tend to overestimate it at greater redshifts. \n- For smaller masses (Mstar < 10^10 Msun), there is an observed excess of blue stars in our simulations compared to the real universe, which may indicate that either our supernova feedback model and/or our understanding of reionization need refinement, or that these processes have been influenced by baryonic effects not accounted for in our framework.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 5.888888888888889,
        "rewrite-fast-z-score": 0.4472135954999579
    },
    {
        "original_text": "We present here the rigorous mathematical formulation and complete proof of our previous results on the equivalence between Ising spin glasses (ISGs) and Ising models (IMs). We show that, in the thermodynamic limit N → ∞ with fixed ratio J / T , ISG partition functions can be mapped to IM ones by means of a suitable transformation which preserves all relevant physical quantities such as free energy density f = −k B T ln Z/N . The main idea is to introduce a new set of variables {s} ≡ {s1, ..., sN } representing the local magnetizations of each site i ∈ {1, .., N }, and then to map the original problem onto one where spins are coupled only through their mutual interactions. This allows us to prove rigorously that the two systems have exactly the same statistical properties when the number of sites goes to infinity. \nThe manuscript is organized as follows. In Sec. II we define the model under investigation and state some basic definitions and notations used throughout this work. In Sec. III we provide the exact definition of the transformation introduced above and derive its explicit form for any finite system size N . Then, in Sec. IV, we discuss how it can be extended to infinite-size lattices. Finally, in Sec. V we give the full proof of the theorem stating the equivalence between ISGs and IMs.",
        "watermark_text": "We present here the thorough mathematical formulation and complete proving of our previous findings on the equivalence between Ising spin glasses ( ISGs ) and Ising models ( IMs ) . We see that , in the thermodynamic limit N → ∞ with constant ratio J / T , ISG partition functions can be mapped to IM ones by means of a suitable decomposition which preserves all relevant physical components such as free energy density g = −k B T ln Z / N .The main idea is to introduce a new collection of variables { s } ≡ { s1 , . . . , sN } representing the local magnetizations of each site i ∈ { 1 , . . , N } , and then to map the previous problem onto one where spins are coupled only through their mutual interactions . This enables us to prove rigorously that the two systems have exactly the same statistical characteristics when the number of places goes to infinity .The manuscript is organized as follows.In Sec.II we define the model under inquiry and explain some fundamental definitions and notations used throughout this study . In Sec .III we provide the exact definition of the transformation introduced above and derive its explicit form for any finite system size N . Then , in Sec .IV , we explain how it can be generalized to infinite - length lattices . Finally , in Sec .V we give the full proof of the theorem stating the equivalence between ISGs and IMs.",
        "rewrite_text": "In this work, we provide a detailed mathematical formulation along with a comprehensive proof of our earlier results regarding the equivalence of Ising spin glasses (ISGs) and Ising models (IMs). We demonstrate that, in the thermodynamic limit as N approaches infinity with a constant ratio of J/T, the partition functions of ISGs can be transformed into those of IMs through a suitable decomposition that retains all essential physical aspects, such as the free energy density defined as g = -k_B T ln Z / N. The core concept involves introducing a new set of variables {s} ≡ {s₁, ..., sₙ}, which represent the local magnetizations at each site i ∈ {1, ..., N}, allowing us to reformulate the original problem to one in which spins interact solely through their mutual connections. This approach enables us to rigorously establish that the two systems exhibit identical statistical properties in the limit of an infinite number of sites. The structure of the manuscript is as follows: In Section II, we outline the model under consideration and present essential definitions and notation employed throughout this study. In Section III, we give a precise definition of the transformation mentioned earlier and derive its explicit formulation for any finite system size N. Section IV discusses how this transformation can be extended to infinite-length lattices. Finally, in Section V, we present the complete proof of the theorem asserting the equivalence between ISGs and IMs.",
        "ori-fast-z-score": 1.801996396010812,
        "water-fast-z-score": 5.7564193416014815,
        "rewrite-fast-z-score": 0.502518907629606
    },
    {
        "original_text": "We consider the possibility that dark matter is made up of bosonic particles, which can condense into a superfluid state at low temperatures. We show how this scenario could explain several puzzling observations in astrophysics and cosmology. In particular we argue that: (i) The observed flat rotation curves of spiral galaxies are explained by the presence of a halo of cold dark matter surrounding each galaxy. (ii) The formation of large-scale structures such as clusters of galaxies proceeds through gravitational collapse of overdensities in the primordial density field seeded by quantum fluctuations during inflation. (iii) Dark energy may arise naturally if the universe contains a large number of weakly interacting massive particles with masses around $10^{22}$ GeV. This article is part of a series on Quantum Matter. For more information see http://arxiv.org/abs/quant-ph/0604070 . \nIntroduction:  Many theories beyond the Standard Model predict new types of elementary particles whose existence has yet to be confirmed experimentally. One particularly interesting class of models involves so-called WIMPZILLAs  1  , i.e., stable relic particles with masses around $10^9$ GeV or higher  2  . These particles would have been produced thermally in the early Universe but their abundance today should still be determined by their annihilation cross section  3  .\nIn this Letter we propose an alternative explanation for the origin of dark matter based on the idea that it consists of self-gravitating bosons  4  . Boson stars  5  are gravitationally bound states of scalar fields  6  predicted by many extensions of the Standard Model  7, 8  . They were first studied in the context of supersymmetric grand unified theories  9  where they play the role of solitonic solutions  10  . More recently, boson stars have also been considered within the framework of string theory  11  . If these objects exist then they will form a population of compact remnants  12  that might constitute all or some fraction of the dark matter  13  .",
        "watermark_text": "We consider the prospect that dark matter is made up of bosonic particles , which can condense into a superfluid state at low temperatures . We see how this situation could explain several puzzling discoveries in astrophysics and cosmology .In particular we claim that : ( i ) The observed flat rotation curves of spiral nuclei are explained by the presence of a halo of cold dark matter surrounding each galaxy . ( ii ) The formation of large - scale structures such as clusters of stars happens through gravity collapse of overdensities in the primordial density field seeded by quantum fluctuations during inflation .( iii ) Dark energy may arise naturally if the universe consists a large number of mildly interacting massive particles with masses around $ 10 ^ { 22 } $ GeV . This page is part of a trilogy on Quantum Matter .For more information see www : / / arxiv . org / abs / quant - ph / 0604070 . Introduction : Many theories beyond the Standard Model predict new types of primary objects whose existence has yet to be verified experimentally .One especially interesting class of models involves so - called WIMPZILLAs 1 , i . e . , stable relic objects with masses around $ 10 ^ 9 $ GeV or greater 2 . These particles might have been created thermally in the early Universe but their density today should still be determined by their annihilation cross section 3 .In this Letter we propose an additional argument for the origin of dark matter based on the idea that it consists of self - gravitating bosons 4 . Boson galaxies 5 are gravitationally bound states of scalar fields 6 expected by many extensions of the Standard Model 7 , 8 .They were first investigated in the context of supersymmetric grand unified fields 9 where they hold the part of solitonic answers 10 . More recently , boson stars have also been discussed within the framework of string theory 11 .If these objects exist then they will form a population of compact remnants 12 that might constitute all or some fraction of the dark matter 13 .",
        "rewrite_text": "We explore the possibility that dark matter comprises bosonic particles that can transition into a superfluid state at low temperatures. This scenario may help clarify several intriguing findings in astrophysics and cosmology. Specifically, we assert that: (i) the flat rotation curves observed in spiral galaxies can be accounted for by a halo of cold dark matter enveloping each galaxy; (ii) the emergence of large-scale structures, such as star clusters, occurs due to the gravitational collapse of overdensities in the primordial density field, which were initiated by quantum fluctuations during the inflationary period; and (iii) dark energy might arise naturally if the universe contains a vast number of mildly interacting massive particles with masses around \\(10^{22}\\) GeV. This document is part of a trilogy on Quantum Matter. For further details, visit www: / / arxiv . org / abs / quant - ph / 0604070. \n\nIntroduction: Numerous theories beyond the Standard Model predict the existence of new fundamental objects that await experimental validation. A particularly intriguing category of models includes so-called WIMPZILLAs, which are stable relics with masses around \\(10^9\\) GeV or higher. These particles may have been thermally produced in the early Universe, but their present density should still be influenced by their annihilation cross-section. In this Letter, we present an additional argument for the origin of dark matter, proposing that it consists of self-gravitating bosons. Boson galaxies are gravitationally bound states of scalar fields anticipated by various extensions of the Standard Model. Initially explored in the context of supersymmetric grand unified theories, where they represent solitonic solutions, boson stars have recently also been examined within the framework of string theory. If such objects exist, they could form a population of compact remnants that might account for all or a portion of dark matter.",
        "ori-fast-z-score": 1.6783627165933783,
        "water-fast-z-score": 7.495152097492019,
        "rewrite-fast-z-score": 2.6680551940539985
    },
    {
        "original_text": "We present the demographics and properties of transition objects in SDSS DR7, which are defined as galaxies with both emission lines (ELGs) and absorption features (AGNs). We find that there is an excess number of ELG-AGN pairs at small separations compared to random distributions. The fraction of AGNs among all ELGs increases towards lower luminosities. There appears to be no significant difference between the fractions of AGNs found within different types of ELGs. These results suggest that some ELGs may harbor hidden AGNs. This work was supported by NASA grant NNX10AD65G. We thank the anonymous referee for helpful comments on this manuscript. In recent years, it has been shown that many active galactic nuclei (AGNs), especially those with low luminosity or obscured by dusty torii, have strong emission line components (see e.g., Ho et al. (1997) , Hao et al. (2005) ), making them appear like normal star-forming galaxies when observed through optical spectroscopic surveys such as Sloan Digital Sky Survey (SDSS; York et al. (2000) ) .\nIn order to identify these  transition objects , we use two criteria based on their spectral energy distribution (SED): 1) they must show both emission lines (ELGs; see Section 2.1 below) and absorption features (Section 2.2) simultaneously; and 2) they should not be classified as quasars according to the BPT diagram (Baldwin et al. 1981 , Kewley et al. 2001 . By applying these selection criteria to the entire sample of galaxies in the seventh data release (DR7; Abazajian et al. 2009 ) of the SDSS, we obtain a total of 16,082 transition objects out of a parent sample of 3,962,843 galaxies.",
        "watermark_text": "We present the demographics and features of transfer objects in SDSS DR7 , which are specified as galaxies with both emission lines ( ELGs ) and emission elements ( AGNs ) . We see that there is an excess amount of ELG - AGN pairs at small separations compared to random distributions .The percentage of AGNs among all ELGs increases towards less luminosities . There seems to be no major variation between the fractions of AGNs observed within various types of ELGs .These data suggest that some ELGs might harbor hidden AGNs . This research was supported by NASA grant NNX10AD65G .We thank the anonymous referee for useful comments on this manuscript . In recent years , it has been shown that several active galactic nuclei ( AGNs ) , particularly those with poor luminosity or obscured by dusty torii , have strong emitted path constituents ( saw e . g . , Ho et al .( 1997 ) , Hao et al . ( 2005 ) ) , making them seem like usual star - creating galaxies when observed through optical spectroscopic studies such as Sloan Digital Sky Survey ( SDSS ; York et al .( 2000 ) ) . In order to identify these transition objects , we using two requirements according on their spectral power distribution ( SED ) : 1 ) they must show both emission lines ( ELGs ; seeing Section 2 . 1 below ) and emission elements ( Section 2 . 2 ) simultaneously ; and 2 ) they should not be categorized as quasars according to the BPT chart ( Baldwin et al .1981 , Kewley et al . 2001 .By applying these selection criteria to the entire sample of galaxies in the seventh data release ( DR7 ; Abazajian et al . 2009 ) of the SDSS , we obtain a total of 16 , 082 transition objects out of a parent sample of 3 , 962 , 843 galaxies .",
        "rewrite_text": "We provide an overview of the demographics and characteristics of transfer objects identified in SDSS DR7, specifically those classified as galaxies exhibiting both emission lines (ELGs) and emission elements (AGNs). Our analysis reveals a notable surplus of ELG-AGN pairs at close separations when compared to random distributions. Additionally, the proportion of AGNs among all ELGs increases for those with lower luminosities. There appears to be minimal variation in the AGN fractions across different types of ELGs, suggesting that some ELGs may conceal hidden AGNs. This study was funded by NASA grant NNX10AD65G, and we appreciate the insightful comments of the anonymous referee on our manuscript. Recent studies have indicated that many active galactic nuclei (AGNs), particularly those with low luminosity or those obscured by dusty toroidal structures, possess strong emitted path constituents, leading them to resemble typical star-forming galaxies in optical spectroscopic surveys like the Sloan Digital Sky Survey (SDSS; York et al. 2000) (see for instance Ho et al. 1997, Hao et al. 2005). To identify these transition objects, we apply two criteria based on their spectral energy distribution (SED): (1) they must display both emission lines (ELGs; see Section 2.1) and emission elements (see Section 2.2) simultaneously, and (2) they should not be classified as quasars according to the BPT diagram (Baldwin et al. 1981, Kewley et al. 2001). When these criteria are applied to the complete galaxy sample from the seventh data release (DR7; Abazajian et al. 2009) of SDSS, we identify a total of 16,082 transition objects from an initial sample of 3,962,843 galaxies.",
        "ori-fast-z-score": -3.104378865665871,
        "water-fast-z-score": 4.308294733275792,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "We present an argument for why quantum mechanics is nonlocal, based on the fact that it allows one to predict with certainty whether or not a measurement will be made in any given experiment. We show how this can lead to superluminal communication between two parties sharing entangled particles by using only local operations and classical communication (LOCC). Finally we generalize the Born rule to allow for arbitrary measurements instead of just von Neumann ones. The usual formulation of quantum mechanics assumes that all experiments are performed under ideal conditions where no errors occur during the preparation of states or the execution of measurements. However, in practice there always exist some experimental imperfections such as decoherence due to environmental noise, imprecision in state preparations, and inaccuracy in measurements. In order to account for these effects, several approaches have been proposed including stochastic Schrödinger equations  1  , open systems  2  , and generalized probabilistic theories  3  . Here we consider another approach known as Quantum Bayesianism  4  .\nIn Quantum Bayesianism, the wave function is regarded as representing our knowledge about the system rather than describing its physical properties. This means that when performing a measurement, the outcome is determined by updating our knowledge according to Bayes  theorem  5  . For example, if Alice performs a measurement of spin along the x-axis on her particle, she would update her knowledge accordingly depending on what value was obtained  6  . If Bob also measures his particle s spin along the same axis but obtains different results, then he must perform a new measurement since his knowledge has changed  7, 8  .",
        "watermark_text": "We present an argument for why quantum mechanics is nonlocal , based on the fact that it allows one to predict with confidence whether or not a measurement will be made in any certain study . We see how this can lead to superluminal transmission between two sides sharing entangled particles by using only local operations and classical communication ( LOCC ) .Finally we generalize the Born rule to allow for arbitrary measurements rather of just von Neumann ones . The typical interpretation of quantum mechanics implies that all experiments are performed under ideal circumstances where no errors occur during the preparation of states or the execution of measurements .However , in practice there always arise some experimental imperfections such as decoherence caused to environmental pollution , imprecision in state preparations , and inaccuracy in measurements . In order to explain for these phenomena , various approaches have been proposed namely stochastic Schrödinger equations 1 , open systems 2 , and generalized probabilistic models 3 .Here we define another methodology called as Quantum Bayesianism 4 . In Quantum Bayesianism , the wave function is regarded as representing our information about the system instead than representing its physical properties .This implies that when performing a measurement , the result is chosen by updating our information according to Bayes principle 5 . For instance , if Alice performs a measurement of spin along the x - axis on her particle , she would update her knowledge appropriately depending on what quantity was obtained 6 .If Bob also measures his particle s spin along the same axis but obtains changed conclusions , then he must perform a new measurement since his knowledge has altered 7 , 8 .",
        "rewrite_text": "We propose an argument for the nonlocal nature of quantum mechanics, emphasizing its capacity to reliably predict whether a measurement will occur in a given study. This ability can facilitate superluminal communication between two parties sharing entangled particles, utilizing only local operations and classical communication (LOCC). Additionally, we extend the Born rule to accommodate arbitrary measurements, beyond just von Neumann measurements. The conventional interpretation of quantum mechanics often assumes that experiments are conducted under ideal circumstances, free from errors in state preparation or measurement execution. However, in reality, experimental imperfections frequently emerge, such as decoherence due to environmental influences, inaccuracies in preparing states, and measurement errors. To account for these issues, several methodologies have been proposed, including stochastic Schrödinger equations, open systems, and generalized probabilistic models. Here, we introduce a new approach called Quantum Bayesianism. In this framework, the wave function is interpreted as a representation of our information about the system rather than a depiction of its physical properties. Consequently, when a measurement is made, the outcome is determined by updating our information according to Bayes' theorem. For example, if Alice measures the spin of her particle along the x-axis, she will revise her understanding based on the result obtained. Similarly, if Bob measures the spin of his particle along the same axis but arrives at a different conclusion, he must conduct an additional measurement, as his information has changed.",
        "ori-fast-z-score": -0.27975144247209416,
        "water-fast-z-score": 6.444022325288263,
        "rewrite-fast-z-score": 0.6644105970267493
    },
    {
        "original_text": "We present results of our numerical simulations of the interaction between the fast stellar winds and the slow dense shell ejected during previous eruption in the recurrent nova RS Oph (T Sco). We find that the observed X-ray light curve can be reproduced by assuming an initial mass loss rate of ~10-6 Msun/yr for the red giant component, which is consistent with theoretical predictions.  The predicted temperature structure of the shocked region agrees well with the observationally inferred one. Our model also predicts that the optical depth to X-rays should increase as time goes on because of the increasing density of the ejecta. This prediction seems to be supported by recent Swift/XRT observations. In addition we show that the observed UV fluxes are not explained by the standard steady state photoionization model but require additional heating source such as shocks or magnetic reconnection. Finally we discuss possible scenarios for future evolution of this system based on our numerical results.",
        "watermark_text": "We present results of our numerical simulations of the interaction between the fast stellar winds and the slow dense shell ejected during earlier eruption in the recurrent nova RS Oph ( T Sco ) . We see that the emitted X - ray light curve can be reproduced by assuming an initial mass loss rate of ~ 10 - 6 Msun / yr for the red dwarf component , which is compatible with theoretical estimates .The predicted temperature structure of the shocked areas agrees well with the observationally inferred one . Our model also predicts that the optical height to X - radiation should expand as time go on because of the increasing volume of the ejecta .This prediction seems to be supported by recent Swift / XRT studies . In addition we prove that the seen UV fluxes are not explained by the standard steady state photoionization theory but need extra heating source such as shocks or mechanical reconnection .Finally we explain possible strategies for future evolution of this scheme depending on our numerical findings .",
        "rewrite_text": "We present the results of our numerical simulations examining the interaction between the fast stellar winds and the slow, dense shell ejected during earlier eruptions of the recurrent nova RS Oph (T Sco). Our simulations indicate that the observed X-ray light curve can be accurately reproduced by assuming an initial mass loss rate of approximately \\(10^{-6} M_{\\odot} \\, \\text{yr}^{-1}\\) for the red dwarf component, which aligns with theoretical estimates. Furthermore, the predicted temperature structure of the shocked regions closely matches observational data. Our model also suggests that the ratio of optical height to X-ray emissions should increase over time due to the expanding volume of the ejecta, a prediction that recent Swift/XRT observations appear to support. Additionally, we demonstrate that the observed UV fluxes cannot be accounted for by the conventional steady-state photoionization theory; instead, they require an additional heating source, such as shocks or mechanical reconnection. Finally, we discuss potential strategies for the future development of this model based on our numerical findings.",
        "ori-fast-z-score": 0.11547005383792514,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 1.6464638998453551
    },
    {
        "original_text": "We study the efficiency of cosmic ray (CR) acceleration by relativistic shocks using Monte Carlo simulations and analytical calculations. We find that, for strong shocks with Mach number M = 10 - 100, only about 1% CRs can be accelerated to ultra-high energy (UHE). This is because most particles are scattered back upstream before they gain enough energy to cross the shock front again. The low efficiency of UHE particle production leads to an upper limit on the maximum proton energy as well as the total CR luminosity produced by such shocks. Our results suggest that the observed fluxes of UHE protons cannot be explained solely by diffusive shock acceleration mechanism operating at cosmological shocks. However, our findings do not rule out other mechanisms proposed recently to explain the origin of UHE cosmic rays. \n \n Keywords: Cosmic Ray Acceleration, Diffusive Shock Acceleration, Relativistic Shocks",
        "watermark_text": "We research the performance of cosmic ray ( CR ) displacement by relativistic shocks using Monte Carlo simulations and mathematical calculations . We see that , for strong shocks with Mach number M = 10 - 100 , only about 1 % CRs can be enhanced to ultra - large energy ( UHE ) .This is because most grains are scattered backwards upstream before they get enough energy to pass the shock front again . The poor efficiency of UHE particle production gives to an upper limitation on the maximum proton power as well as the total CR luminosity generated by such shocks .Our results show that the known fluxes of UHE protons impossible be described solely by diffusive shock velocity process working at cosmological shocks . However , our findings do not leave out other mechanisms proposed lately to explain the origin of UHE cosmic rays .Keywords: Cosmic Ray Acceleration, Diffusive Shock Acceleration, Relativistic Shocks",
        "rewrite_text": "We investigate the performance of cosmic ray (CR) displacement by relativistic shocks through Monte Carlo simulations and mathematical calculations. Our findings indicate that for strong shocks with a Mach number ranging from M = 10 to 100, only about 1% of cosmic rays can be accelerated to ultra-high energies (UHE). This limited acceleration occurs because most particles are scattered back upstream before they gain sufficient energy to cross the shock front again. Consequently, the low efficiency of UHE particle production imposes an upper limit on both the maximum proton power and the total CR luminosity generated by these shocks. Our results suggest that the known fluxes of UHE protons cannot be solely attributed to the diffusive shock acceleration process occurring at cosmological shocks. Nonetheless, our research does not rule out other recently proposed mechanisms that could explain the origins of UHE cosmic rays.  \nKeywords: Cosmic Ray Acceleration, Diffusive Shock Acceleration, Relativistic Shocks.",
        "ori-fast-z-score": -1.721892064184557,
        "water-fast-z-score": 5.430582663966679,
        "rewrite-fast-z-score": -0.8819171036881969
    },
    {
        "original_text": "The author presents an alternative approach to teaching special relativity by using the concept of transformation groups and their generators, which are used in physics for describing symmetry properties of physical systems.  The main idea is that students can learn about the foundations of special relativity without having to deal with complicated mathematical concepts such as tensors or spinor fields. This article describes how this method works on examples related to the Lorentz transformations. It also discusses some possible applications of these ideas in other areas of physics. In particular, it shows how one can use them to explain the origin of gauge symmetries in quantum field theory. Special relativity (SR) has been taught at many universities since its discovery in 1905  1  . However, despite numerous attempts  2  , there still exists no generally accepted way of introducing SR into undergraduate courses  3  .\nIn recent years, several authors have proposed new approaches to teaching SR  4  -  8  . These methods usually involve presenting the basic principles of SR through simple experiments performed in different reference frames  9  -  11  . They often require only minimal knowledge of mathematics  12  -  14  . Some of these proposals were inspired by Feynman s lectures  15  . Other authors tried to develop similar techniques based on modern computer technology  16  -  18  .",
        "watermark_text": "The author presents an different approach to teaching regular gravity by using the idea of transformation groups and their generators , which are needed in science for describing symmetry properties of physical structures . The main idea is that students can know about the foundations of regular gravity without having to deal with difficult numerical notions such as tensors or spinor fields .This page describes how this process uses on examples related to the Lorentz transformations . It additionally outlines some possible applied of these ideas in other areas of physics .In particular , it demonstrates how one can using them to explain the origin of gauge symmetries in quantum field theory . Special relativity ( SR ) has been taught at many universities since its discovery in 1905 1 .However , despite several efforts 2 , there still exists no usually agreed way of introducing SR into undergraduate courses 3 . In recent years , various authors have proposed additional strategies to teaching SR 4 - 8 .These methods usually include presenting the fundamental principles of SR through ordinary observations performed in different reference spaces 9 - 11 . They often use only limited knowledge of math 12 - 14 .Some of these proposals were inspired by Feynman s lectures 15 . Other scholars try to develop similar method using on contemporary computer technology 16 - 18 .",
        "rewrite_text": "The author introduces a novel approach to teaching regular gravity by utilizing the concept of transformation groups and their generators, which are essential for describing the symmetry properties of physical structures in science. The core idea is that students can learn the fundamentals of regular gravity without the complexities of advanced numerical concepts such as tensors or spinor fields. This page illustrates this educational process using examples related to Lorentz transformations and also discusses potential applications of these ideas in other fields of physics. Specifically, it shows how these concepts can help elucidate the origin of gauge symmetries in quantum field theory. Since its discovery in 1905, special relativity (SR) has been taught at numerous universities. However, despite various attempts, a universally accepted method for introducing SR in undergraduate courses has yet to be established. In recent years, several authors have proposed alternative strategies for teaching SR. These approaches typically emphasize presenting the fundamental principles of SR through everyday observations in different reference frames, often requiring only a basic understanding of mathematics. Some proposals have drawn inspiration from Feynman’s lectures, while other researchers aim to develop similar methods incorporating contemporary computer technology.",
        "ori-fast-z-score": 0.769800358919501,
        "water-fast-z-score": 7.120653320005384,
        "rewrite-fast-z-score": 1.165543034828717
    },
    {
        "original_text": "We present new observations with Hubble Space Telescope (HST) and Spitzer Space Telescope to study the galaxy population in the cluster Abell 2744 at z = 0.308. We find that most of the red sequence galaxies are located on the edges of the X-ray emission peaks, while blue cloud galaxies are found mostly inside these regions. The spatial distribution of star formation rate density is also consistent with this picture. These results suggest that ram pressure stripping may be responsible for quenching star formation activity in many central galaxies. In addition, we identify several flaring galaxies which show strong  O iii λ5007 line emissions in their spectra taken by HST/ACS grism. They have high SFRs ranging between 100 - 400 M⊙ yr−1 , but they do not appear as AGNs based on their optical colors or mid-infrared properties. Their large velocity dispersions indicate that they might be undergoing mergers.",
        "watermark_text": "We present new experiments with Hubble Space Telescope ( HST ) and Spitzer Space Telescope to study the galaxy community in the cluster Abell 2744 at z = 0 . 308 . We see that most of the red sequence galaxies are situated on the edges of the X - ray radiation peaks , while dark cloud galaxies are found primarily inside these regions .The spatial distribution of galaxy formation rate concentration is also consistent with this picture . These conclusions propose that ram pressure stripping may be responsible for quenching star formation activity in many central galaxies .In addition , we identify several flaring stars which show intense O iii λ5007 line emissions in their spectra made by HST / ACS grism . They have high SFRs varied between 100 - 400 [UNK] yr−1 , but they do not appear as AGNs based on their optical colors or mid - infrared properties .Their large velocity dispersions suggested that they may be experiencing mergers .",
        "rewrite_text": "We introduce new experiments using the Hubble Space Telescope (HST) and the Spitzer Space Telescope to investigate the galaxy population in the Abell 2744 cluster at a redshift of z = 0.308. Our findings indicate that most of the red sequence galaxies are located at the outskirts of the X-ray emission peaks, while galaxies with dark clouds are primarily situated within these areas. The spatial distribution of the concentration of galaxy formation rates supports this observation. These results suggest that ram pressure stripping could be causing the suppression of star formation in many central galaxies. Additionally, we have identified several flaring stars that exhibit strong O III λ5007 line emissions in their spectra obtained from HST/ACS grism. These stars demonstrate high star formation rates ranging from 100 to 400 [UNK] yr−1, and they do not classify as AGNs based on their optical colors or mid-infrared characteristics. Their significant velocity dispersions imply that they may be undergoing mergers.",
        "ori-fast-z-score": -0.7385489458759964,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": -0.9561828874675149
    },
    {
        "original_text": "We present an analysis of the distribution of planetary orbital parameters and masses for all known extrasolar planets with measured mass (N=73). We find that this sample can be divided into three distinct groups, which we call super-Earths, mini-Neptunes, and hot Jupiters. The first two are characterized by low eccentricities and high mutual inclinations between their orbits; in contrast, hot Jupiters have small mutual inclinations but large eccentricities. Super-Earths and mini-Neptunes also appear to form separate sequences on the radius-mass diagram. These results suggest that these three populations may represent different formation mechanisms or evolutionary histories. In particular, our findings support previous suggestions that super-Earths formed via core accretion followed by significant inward migration while mini-Neptunes were likely assembled through gravitational instability. Finally, we show how these results can be used as priors when searching for additional exoplanets using transit surveys such as Kepler.",
        "watermark_text": "We present an assessment of the distribution of planetary orbital characteristics and masses for all known extrasolar planets with calculated mass ( N = 73 ) . We see that this specimen can be grouped into three different categories , which we call super - Earths , mini - Neptunes , and hot Jupiters .The first two are marked by low eccentricities and large mutual inclinations between their orbits ; in comparison , hard Jupiters have small mutual inclinations but large eccentricities . Super - Earths and mini - Neptunes additionally appear to form different series on the radius - mass graph .These conclusions show that these three communities may contain different formation factors or evolutionary histories . In particular , our findings support previous suggestions that super - Earths formed via nucleus accretion followed by significant eastward migration while mini - Neptunes were likely formed through gravity instability .Finally , we show how these results can be used as priors when looking for additional exoplanets using transit surveys such as Kepler .",
        "rewrite_text": "We provide an analysis of the distribution of orbital characteristics and masses of all identified extrasolar planets with measured mass (N = 73). Our findings categorize these planets into three distinct groups: super-Earths, mini-Neptunes, and hot Jupiters. The first two groups are characterized by low eccentricities and significant mutual inclinations in their orbits, while hot Jupiters exhibit small mutual inclinations but high eccentricities. Additionally, super-Earths and mini-Neptunes seem to align along different trends on the radius-mass graph. These observations suggest that the three categories may experience varying formation processes or evolutionary paths. Specifically, our results reinforce earlier hypotheses that super-Earths likely formed through core accretion followed by considerable eastward migration, whereas mini-Neptunes are thought to have formed via gravitational instability. Lastly, we discuss how these insights can inform future searches for exoplanets through transit surveys, such as those conducted by Kepler.",
        "ori-fast-z-score": -0.7071067811865476,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 1.1952286093343936
    },
    {
        "original_text": "We present the results of our study on the possible existence and stability of terrestrial planets around the star Gliese 581, which is located at about 20 light-years away from Earth. We have performed numerical simulations for different orbital configurations of three hypothetical terrestrial planets with masses ranging between 1 to 10 times that of Earth s mass (1-10 M⊕). Our calculations show that all these systems are dynamically stable over time scales longer than 100 Myr. The most massive planet has an eccentric orbit with e=0.2 and its periastron distance ranges between 0.05 AU and 0.15 AU depending on the initial conditions used. This planet can be considered as a hot Jupiter-like planet because it orbits very close to its host star. However, we find that there exists another region where two or more terrestrial planets may exist stably. In this region, one of them could be a super-Earth-type planet with a mass larger than 5M⊕ but smaller than 8M⊕.",
        "watermark_text": "We present the conclusion of our research on the possible existence and dynamics of terrestrial worlds around the star Gliese 581 , which is situated at about 20 light - years distance from Earth . We have done mathematical simulations for different orbital arrangements of three hypothetical terrestrial worlds with masses differing between 1 to 10 twice that of Earth s mass ( 1 - 10 M⊕ ) .Our calculations show that all these systems are dynamically stable over time ranges longer than 100 Myr . The most large planet has an eccentric orbit with e = 0 . 2 and its periastron speed ranges between 0 . 05 AU and 0 . 15 AU depending on the early conditions utilized .This planet can be regarded as a bright Jupiter - like planet because it orbits very close to its host star . However , we find that there exists another region where two or more terrestrial worlds may arise stably .In this area , one of them could be a super - Earth - class planet with a mass greater than 5M⊕ but smaller than 8M⊕ .",
        "rewrite_text": "We present the findings of our research regarding the potential existence and dynamics of terrestrial planets orbiting the star Gliese 581, located approximately 20 light-years from Earth. Our mathematical simulations explored various orbital configurations for three hypothetical terrestrial planets, with masses ranging from 1 to 10 times that of Earth's (1 - 10 M⊕). Our results indicate that all these planetary systems remain dynamically stable over time spans exceeding 100 million years. The largest planet features an eccentric orbit with an eccentricity of e = 0.2, and its periastron speed varies between 0.05 AU and 0.15 AU, contingent upon the initial conditions used. This planet can be likened to a prominent Jupiter-like entity due to its close proximity to its host star. Additionally, we identified another region where two or more terrestrial planets could stably coexist. Within this area, one such planet could be classified as a super-Earth, with a mass exceeding 5M⊕ but less than 8M⊕.",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": -1.1952286093343936
    },
    {
        "original_text": "We present the results of numerical simulations of two-dimensional N = (2, 2)\nsuper-Yang-Mills theory with gauge group SU(N). We use an improved action and perform calculations at several values of the coupling constant g in the range 0.1 < g < 1.0. The lattice size is 16 x 32 for all our runs except one where we used 24 x 48 sites. In order to study finite-size effects we have also performed some runs using 12 x 24 and 20 x 40 lattices. \nThe main goal of this work was to check whether the phase transition between confinement and deconfinement phases observed previously by other authors persists when the continuum limit is approached. \n \n Our data show that there are no significant differences between the results obtained on different sizes of lattices within statistical errors. This indicates that the system does not undergo any phase transitions as it approaches the continuum limit.",
        "watermark_text": "We present the conclusion of statistical simulations of two - dimensional N = ( 2 , 2 ) super - Yang - Mills theory with gauge group SU ( N ) . We use an modified action and conduct measurements at several values of the interaction factor g in the range 0 . 1 < g < 1 . 0 .The lattice size is 16 x 32 for all our runs except one where we using 24 x 48 locations . In order to study discrete - length effects we have already completed some runs use 12 x 24 and 20 x 40 lattices .The main goal of this research was to test whether the phase shift between confinement and deconfinement cycles observed previously by other researchers persists when the continuum limit is neared . Our data demonstrate that there are no considerable variations between the results derived on various sizes of lattices within statistical errors .This implies that the system does not undergo any phase transitions as it approaches the continuum limit .",
        "rewrite_text": "We present the results of statistical simulations of two-dimensional N = (2, 2) super-Yang-Mills theory with a gauge group of SU(N). Utilizing a modified action, we conducted measurements at several interaction factor values, ranging from 0.1 to 1.0. The lattice size used in all our simulations was 16 x 32, except for one case where a 24 x 48 lattice was employed. To investigate discrete-length effects, we also completed runs using 12 x 24 and 20 x 40 lattices. The primary objective of this research was to determine whether the phase shift between confinement and deconfinement cycles, previously observed by other researchers, persists as we approach the continuum limit. Our data indicate that there are no significant variations in results across different lattice sizes within the bounds of statistical error. This suggests that the system does not experience any phase transitions as it nears the continuum limit.",
        "ori-fast-z-score": -1.5650160901149996,
        "water-fast-z-score": 5.176591682688076,
        "rewrite-fast-z-score": 1.2309149097933272
    },
    {
        "original_text": "We present spatially-resolved spectroscopic observations for the central regions (r < 1 kpc) of six nearby, massive early-type galaxies in clusters or groups with Mvir > 1013M⊙. The data were obtained using the Gemini Multi-Object Spectrograph on Gemini North telescope as part of our ongoing program to study the formation histories of these systems. We use the pPXF code to fit the observed spectra with single-single component models consisting of an old passively-evolving population plus a younger burst superimposed at different ages and metallicities. Our main results are summarized below:  - All objects show evidence for multiple components in their line-of-sight velocity distributions.  - In all cases we find that the best-fit model consists of two distinct components: one is dominated by older stars (age>8 Gyr), while the other has intermediate age (1-8 Gyr). - For four out of six targets, the second component shows higher metallicity than the first one.",
        "watermark_text": "We generate spatially - resolved spectroscopic observations for the central regions ( r < 1 kpc ) of six nearby , massive early - type galaxies in clusters or groups with Mvir > [UNK] . The data were obtained using the Gemini Multi - Object Spectrograph on Gemini North telescope as part of our ongoing program to study the formation histories of these systems .We use the pPXF code to pack the known spectra with single - single component versions comprised of an old passively - expanding population plus a later burst superimposed at different ages and metallicities . Our main results are presented below : - All bodies exhibit data for multiple components in their line - of - seeing velocity distributions .- In all situations we find that the best - fitting model consists of two separate phases : one is dominated by older stars ( age > 8 Gyr ) , while the other has intermediate older ( 1 - 8 Gyr ) . - For four out of six targets , the second component displays higher metallicity than the first one .",
        "rewrite_text": "We obtain spatially-resolved spectroscopic observations of the central regions (r < 1 kpc) of six nearby, massive early-type galaxies located in clusters or groups with Mvir > [UNK]. These data were collected using the Gemini Multi-Object Spectrograph on the Gemini North telescope as part of our ongoing effort to investigate the formation histories of these galaxies. To analyze the data, we employ the pPXF code, which allows us to model the known spectra with single-component versions that consist of an old, passively evolving population along with a later burst, varying in ages and metallicities. Our main findings are as follows: \n\n- All galaxies show evidence of multiple components in their line-of-sight velocity distributions. \n- In every case, we find that the optimal model comprises two distinct phases: one predominantly features older stars (age > 8 Gyr), while the other involves intermediate-aged stars (1 - 8 Gyr). \n- For four out of the six galaxies studied, the second component exhibits higher metallicity compared to the first component.",
        "ori-fast-z-score": 0.11704114719613057,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 1.5652475842498528
    },
    {
        "original_text": "We study the relic density of dark matter (DM) produced by thermal freeze-out and freezein processes in Universal Extra Dimension (UED) model, where we include effects due to Majorana mass terms for the right handed neutrino fields. We find that the DM can be either fermionic or bosonic depending on the values of the parameters involved. In particular, if the UED scale is close to 1 TeV then the DM candidate could be a scalar particle which has been recently proposed as a possible explanation for the PAMELA positron excess observed at high energies. The results are presented in the context of LHC searches for new physics beyond Standard Model. Introduction:-The discovery of the Higgs-like resonance  1-3  at Large Hadron Collider (LHC), alongwith other experimental evidences  4  , have established the existence of a fundamental scalar field responsible for breaking electroweak symmetry. However, there still remain many open questions about the nature of this scalar sector such as: What is its spin? Is it CP-even or odd? Does it couple only to gauge bosons or also to fermions? Are there any additional scalars present in Nature ? These issues will be addressed once more data becomes available from ongoing experiments like ATLAS  5  and CMS  6  . On the theoretical front, one of the most interesting possibilities is to consider extensions of the Standard Model (SM). One possibility is to extend SM into higher dimensions  7-9 , thereby introducing Kaluza-Klein excitations of all particles  10  .\nIn recent years, several authors  11-13  studied the phenomenology of these theories in detail. It was shown that the lightest KaluzaKlein excitation of the graviton may act as cold Dark Matter (CDM)  14-16 . This scenario is particularly appealing since CDM constitutes around 23%  17  of the energy content of our universe  18  . Moreover, the presence of an extra spatial dimension opens up the possibility of producing Kaluza-Klein states through various production mechanisms  19-21  including decay  22  and annihilation  23  . Recently, it has been pointed out  24 ",
        "watermark_text": "We explore the relic thickness of dark matter ( DM ) produced by temperature freeze - out and freezein cycles in Universal Extra Dimension ( UED ) model , where we include effects due to Majorana mass terms for the right handed neutrino fields . We see that the DM can be either fermionic or bosonic varying on the values of the variables required .In particular , if the UED scale is close to 1 TeV then the DM candidate could be a scalar object which has been lately considered as a possible mechanism for the PAMELA positron excess observed at high energies . The results are presented in the context of LHC searches for future physics beyond Standard Model .Introduction : - The observation of the Higgs - like resonance 1 - 3 at Large Hadron Collider ( LHC ) , alongwith other experimental evidences 4 , have confirmed the existence of a basic scalar field involved for breaking electroweak symmetry . However , there still continue several open questions about the nature of this scalar sector such as : What is its spin ?Is it CP - even or odd ? Does it couple only to gauge bosons or also to fermions ?Are there any additional scalars present in Nature ? These issues will be addressed once more data becomes available from continuing studies like ATLAS 5 and CMS 6 .On the theoretical front , one of the most exciting possibilities is to consider extensions of the Standard Model ( SM ) . One possibility is to expanded SM into larger dimensions 7 - 9 , thereby introducing Kaluza - Klein excitations of all particles 10 .In past times , various scientists 11 - 13 examined the phenomenology of these theories in detail . It was shown that the lightest KaluzaKlein excitation of the graviton could act as chill Dark Matter ( CDM ) 14 - 16 .This scenario is especially appealing since CDM constitutes around 23 % 17 of the power content of our universe 18 . Moreover , the presence of an additional spatial dimension opens up the prospect of creating Kaluza - Klein states through several production mechanisms 19 - 21 notably emission 22 and annihilation 23 .Recently, it has been pointed out  24",
        "rewrite_text": "We investigate the relic density of dark matter (DM) generated through temperature freeze-out and freeze-in processes within the framework of the Universal Extra Dimension (UED) model, incorporating the effects of Majorana mass terms for right-handed neutrino fields. Our analysis reveals that the DM can manifest as either fermionic or bosonic, depending on the parameter values involved. Notably, if the UED scale is approximately 1 TeV, the dark matter candidate may be a scalar particle, which has recently been proposed as a potential explanation for the PAMELA positron excess observed at high energies. These findings are contextualized within the ongoing searches at the LHC for physics beyond the Standard Model.\n\nIntroduction: The discovery of a Higgs-like resonance at the Large Hadron Collider (LHC), along with various other experimental findings, has validated the existence of a fundamental scalar field responsible for electroweak symmetry breaking. Nonetheless, several questions remain unresolved regarding the characteristics of this scalar sector, such as its spin, whether it is CP-even or CP-odd, its coupling nature to gauge bosons and fermions, and the potential existence of additional scalar particles in nature. These questions are expected to be addressed as more data emerges from ongoing experiments, including those conducted by ATLAS and CMS.\n\nFrom a theoretical perspective, one of the most intriguing avenues to explore is the extension of the Standard Model (SM) into higher dimensions, introducing Kaluza-Klein excitations for all particle types. Previous research has thoroughly analyzed the phenomenology of these theories, showing that the lightest Kaluza-Klein excitation of the graviton could serve as a cold dark matter (CDM) candidate. This scenario is particularly compelling given that CDM is estimated to comprise around 23% of the total energy content of our universe. Furthermore, the existence of an additional spatial dimension raises the possibility of producing Kaluza-Klein states via various mechanisms, including emission and annihilation. Recently, it has been highlighted that...",
        "ori-fast-z-score": 0.9918365981341755,
        "water-fast-z-score": 7.902633289178096,
        "rewrite-fast-z-score": 0.35355339059327373
    },
    {
        "original_text": "We present new results on the distribution of loop widths in active regions observed by TRACE at 171 Å, using data obtained during solar maximum (May-June 2001). We find that loops with different temperatures have similar distributions of widths, which are well fitted by log-normal functions. The mean values of these distributions increase with temperature as expected for pressure equilibrium between plasma confined within magnetic structures and their surroundings. However, we also find that there is no significant difference between the widths measured along individual loops and those determined from averaged profiles over entire active regions. This suggests that the apparent constancy of loop widths may be due to averaging effects rather than being intrinsic properties of coronal structures. In addition, we show that the widths derived from observations made under different viewing angles do not depend significantly on the position angle of the line-of-sight relative to the direction perpendicular to the local magnetic field vector.",
        "watermark_text": "We report new data on the distribution of loop widths in active regions observed by TRACE at 171 Å , using data acquired during thermal maximum ( May - June 2001 ) . We see that loops with varying temperatures have similar distributions of widths , which are better fitted by log - normal functions .The mean estimates of these distributions increase with temperature as anticipated for pressure equilibrium between plasma confined within magnetic structures and their environment . However , we also find that there is no considerable difference between the widths calculated along individual loops and those estimated from averaged profiles over whole active regions .This implies that the apparent constancy of loop widths might be due to averaging influences rather than being intrinsic characteristics of coronal structures . In addition , we prove that the widths generated from measurements made under distinct observation angles do not depend greatly on the orientation angle of the line - of - view relative to the direction perpendicular to the local magnetic force vector .",
        "rewrite_text": "We present new findings on the distribution of loop widths in active regions as observed by TRACE at 171 Å, utilizing data collected during the thermal maximum period from May to June 2001. Our analysis reveals that loops with different temperature ranges exhibit similar width distributions, which can be effectively described by log-normal functions. As expected, the mean values of these distributions increase with temperature, consistent with the pressure equilibrium between plasma contained within magnetic structures and their surrounding environment. Notably, we also found no significant difference between the widths calculated along individual loops and those derived from averaged profiles across entire active regions. This suggests that the apparent consistency of loop widths may be influenced by averaging effects rather than representing inherent properties of coronal structures. Furthermore, we demonstrate that the widths obtained from measurements taken at various observation angles do not show a significant dependence on the orientation angle of the line of sight relative to the direction perpendicular to the local magnetic force vector.",
        "ori-fast-z-score": -0.4364357804719848,
        "water-fast-z-score": 4.939391699536065,
        "rewrite-fast-z-score": 0.10369516947304253
    },
    {
        "original_text": "The Sloan Digital Sky Survey (SDSS) is an ongoing project to map the distribution and motion of galaxies, quasars, stars, and other celestial objects in space. The fourth data release was made public on September 30th 2003. This fifth data release contains more than 100,000 new quasar candidates selected by color criteria from the SDSS imaging survey. These are supplemented with about 20,000 previously known quasars that were not included in previous releases because they did not meet the selection criteria for inclusion at that time. \n \n In addition to these newly discovered quasars, this catalog also includes all quasars found during the first four years of the survey as well as those found since then but which have yet to be released publicly. A total of over 250,000 quasars are now available through this catalog. All of them have been spectroscopically confirmed using observations obtained with the dedicated 2.5-meter telescope located at Apache Point Observatory near Sacramento, California.",
        "watermark_text": "The Sloan Digital Sky Survey ( SDSS ) is an continuing work to map the distribution and motion of galaxies , quasars , stars , and other celestial entities in space . The fourth information publication was making public on September 30th 2003 .This fifth information update contains more than 100 , 000 new quasar finalists chosen by color categories from the SDSS imaging survey . These are supplemented with about 20 , 000 former recognized quasars that were not included in earlier versions because they did not meet the selection standards for inclusion at that point .In addition to these newly discovered quasars , this list also contains all quasars discovered during the first four seasons of the census as well as those identified since then but which have never to be available openly . A total of over 250 , 000 quasars are now released through this catalog .All of them have been spectroscopically confirmed using observations collected with the reserved 2 . 5 - meter telescope located at Apache Point Observatory near Sacramento , California .",
        "rewrite_text": "The Sloan Digital Sky Survey (SDSS) is an ongoing project aimed at mapping the distribution and movement of galaxies, quasars, stars, and other celestial objects in the universe. The fourth data release took place on September 30, 2003. The fifth data update features over 100,000 new quasar candidates categorized by color from the SDSS imaging survey. This update also includes approximately 20,000 previously identified quasars that were not part of earlier releases due to not meeting the selection criteria at the time. Alongside the newly discovered quasars, this release contains all quasars identified during the first four years of the survey, as well as those discovered later that had not been made publicly available. In total, over 250,000 quasars are now included in this catalog, all of which have been spectroscopically confirmed using observations obtained with the 2.5-meter telescope at Apache Point Observatory near Sacramento, California.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 5.25,
        "rewrite-fast-z-score": -0.629940788348712
    },
    {
        "original_text": "The Fermi Large Area Telescope (LAT) has revolutionized our understanding of blazar jets by providing unprecedented sensitivity to gamma rays above 100 MeV. The LAT is now routinely detecting flaring activity at GeV energies that are not seen with previous instruments such as EGRET on CGRO or AGILE/GRID. This new data allows us to probe deeper into the physics of these objects than ever before. \n \n In this talk I will discuss how we can use the upcoming generation of space-based observatories - including GLAST and EXIST - to study blazars over an even broader energy range. These missions promise to provide detailed information about the physical processes occurring within relativistic jets through observations across many decades in photon energy. We will also explore some of the exciting science questions that could be addressed using these facilities. Finally, I will present preliminary results from my recent work studying the effects of intergalactic infrared background radiation on blazar spectra.",
        "watermark_text": "The Fermi Large Area Telescope ( LAT ) has revolutionized our knowing of blazar jets by offering extraordinary exposure to gamma radiation above 100 MeV . The LAT is now regularly detecting flaring activity at GeV values that are not seen with previous instruments such as EGRET on CGRO or AGILE / GRID .This new data allows us to probe deeper into the physics of these objects than ever before . In this talk I will explain how we can using the latest generation of space - based observatories - including GLAST and EXIST - to study blazars over an much broader energy range .These expeditions promise to provide comprehensive information about the natural reactions evolving within relativistic jets through observations across many years in photon energy . We will also investigate some of the exciting science problems that might be addressed using these facilities .Finally , I will present preliminary results from my current work studying the effects of intergalactic infrared background radiation on blazar spectra .",
        "rewrite_text": "The Fermi Large Area Telescope (LAT) has transformed our understanding of blazar jets by providing unprecedented observations of gamma radiation above 100 MeV. The LAT is now consistently detecting flaring activity at GeV energies that were not observable with previous instruments such as EGRET on the Compton Gamma Ray Observatory (CGRO) or AGILE/GRID. This new data enables us to delve deeper into the physics of these objects than ever before. In this presentation, I will discuss how we can utilize the latest generation of space-based observatories, including GLAST and EXIST, to investigate blazars across a much wider range of energies. These missions promise to yield extensive insights into the physical processes occurring within relativistic jets through long-term observations of photon energies. Additionally, we will explore some intriguing scientific questions that these facilities may help answer. Finally, I will share preliminary results from my ongoing research on the impact of intergalactic infrared background radiation on blazar spectra.",
        "ori-fast-z-score": 1.4770978917519928,
        "water-fast-z-score": 6.474980550884177,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "The effect of adding small amounts (0.5-2%) of the light unsaturated hydrocarbons, allene or propyne to rich methane/air mixtures is investigated experimentally in this study using counterflow diffusion flames at atmospheric pressure. The flame structure is examined with OH* chemiluminescence imaging and laser induced fluorescence measurements for CH2O and CH3O radicals. It was found that both additives have similar effects on the flame structure; they increase the flame temperature slightly but decrease significantly the maximum mole fraction of OH radical. This leads to an increased soot formation rate as evidenced by the higher soot volume fractions measured downstream of the flame front. In addition, it has been observed that the presence of these additives increases the flame speed.  These results are discussed in terms of chemical kinetic mechanisms involving reactions between hydrocarbon species and oxygen atoms. Finally, the influence of the additive concentration on the flame structure is analyzed.",
        "watermark_text": "The impact of adding small concentrations ( 0 . 5 - 2 % ) of the light unsaturated hydrocarbons , allene or propyne to rich methane / atmosphere mixtures is investigated experimentally in this study using counterflow diffusion flames at atmospheric pressure . The flame composition is investigated with OH * chemiluminescence scanning and infrared triggered fluorescence measurements for CH2O and CH3O radicals .It was shown that both additives have similar influences on the burn structure ; they change the burning heat slightly but decrease greatly the maximum mole proportion of OH radical . This leads to an elevated soot accumulation frequency as demonstrated by the higher soot mass fractions calculated downstream of the burning front .In addition , it has been observed that the presence of these additives increases the burning speed . These data are discussed in terms of chemical kinetic mechanisms governing reactions between hydrocarbon species and oxygen atoms .Finally , the impact of the additive concentration on the flame composition is evaluated .",
        "rewrite_text": "This study experimentally examines the effects of introducing small concentrations (0.5-2%) of light unsaturated hydrocarbons, namely allene or propyne, into rich methane/air mixtures using counterflow diffusion flames at atmospheric pressure. The flame composition is analyzed using OH* chemiluminescence scanning and infrared-triggered fluorescence measurements for CH2O and CH3O radicals. The results indicate that both additives produce similar effects on the flame structure; they slightly alter the heat of combustion while significantly reducing the maximum mole fraction of the OH radical. This reduction contributes to an increased frequency of soot accumulation, evidenced by higher soot mass fractions observed downstream of the flame front. Moreover, the presence of these additives is found to enhance the burning rate. The findings are discussed in relation to the chemical kinetic mechanisms that underlie the reactions between hydrocarbon species and oxygen atoms. Finally, the influence of the concentration of the additives on flame composition is also assessed.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "The physical characteristics, spectral energy distributions (SEDs), and atmospheric properties of late-type dwarfs are reviewed in this chapter.  The SEDs for these objects have been measured by many authors using ground-based telescopes as well as space observatories such as ISO, Spitzer Space Telescope, and AKARI.  These observations show that the infrared flux increases with decreasing effective temperature.  In addition to their red colors, they also exhibit strong water vapor absorption bands at wavelengths longer than 2 microns.   Their near-infrared spectra reveal features due to methane gas which is abundant in their atmospheres.  They also display prominent atomic lines of potassium and sodium in their optical spectra.  Finally, we discuss how the observed photometric data can be used to determine the fundamental parameters of these objects including mass, radius, luminosity, surface gravity, age, metallicity, and distance.   ... This chapter reviews the physical characteristics, spectral energy distribution (SED) measurements, and atmospheric properties of late type dwarfs.",
        "watermark_text": "The physical qualities , spectral power distributions ( SEDs ) , and atmospheric characteristics of late - class dwarfs are reviewed in this page . The SEDs for these objects have been measured by many scientists using ground - based telescopes as well as space observatories such as ISO , Spitzer Space Telescope , and AKARI .These measurements show that the infrared flux changes with decreasing effective heat . In addition to their red colors , they even exhibit strong water vapor absorption groups at wavelengths greater than 2 microns .Their near - infrared spectra demonstrate characteristics owing to methane gas which is abundant in their atmospheres . They even exhibit prominent atomic patterns of potassium and sodium in their infrared spectra .Finally , we explain how the observed photometric data can be used to determine the fundamental variables of these objects including mass , diameter , luminosity , surface gravity , age , metallicity , and distance . . . .This section reviews the physical qualities , spectral power distribution ( SED ) observations , and atmospheric characteristics of late type dwarfs .",
        "rewrite_text": "This section provides an overview of the physical properties, spectral energy distributions (SEDs), and atmospheric features of late-type dwarfs. Numerous scientists have measured the SEDs of these objects using both ground-based telescopes and space observatories, including the ISO, Spitzer Space Telescope, and AKARI. These observations reveal that the infrared flux decreases as the effective temperature drops. In addition to their reddish appearances, late-type dwarfs show significant absorption features due to water vapor at wavelengths exceeding 2 microns. Their near-infrared spectra further indicate the presence of abundant methane gas in their atmospheres, along with notable atomic signatures of potassium and sodium. Finally, we discuss how the available photometric data can be utilized to ascertain key parameters for these objects, such as mass, diameter, luminosity, surface gravity, age, metallicity, and distance.",
        "ori-fast-z-score": 0.22360679774997896,
        "water-fast-z-score": 6.555555555555555,
        "rewrite-fast-z-score": -0.24618298195866545
    },
    {
        "original_text": "The magnetic properties of carbon nanotubes and fullerenes have been studied extensively, but the effect on their electronic structure is still not well understood. In this work we use density functional theory to investigate how the presence of an external magnetic field affects the electronic structure of single-wall zigzag (n = 3) and armchair (n = 4) nanographenes with different sizes. We find that for small n=3 or n=4 nanographenes there are no significant changes in the band gap when applying a magnetic field up to 10 T along the tube axis. However, as the size increases, the energy gaps decrease significantly under applied fields due to the formation of Landau levels. The results show that the magnetic response depends strongly on the chirality of the nanographene. For example, while the zigzag tubes exhibit diamagnetic behavior at low fields, they become paramagnetic above 1 T. On the other hand, the armchair tubes always display paramagnetic behavior even at very weak fields.",
        "watermark_text": "The magnetic properties of carbon nanotubes and fullerenes have been studied thoroughly , but the impact on their electronic structure is also not well understood . In this research we utilize density functional theory to examine how the presence of an external magnetic force influence the electronic structure of double - wall zigzag ( n = 3 ) and armchair ( n = 4 ) nanographenes with various sizes .We see that for little n = 3 or n = 4 nanographenes there are no major changes in the band gap when applying a magnetic force up to 10 T along the tunnel axis . However , as the height rises , the electricity gaps decline considerably under applied fields thanks to the formation of Landau concentrations .The results show that the magnetic response depends strongly on the chirality of the nanographene . For instance , while the zigzag tubes undergo diamagnetic activity at low fields , they become paramagnetic above 1 T . On the other hand , the armchair devices always display paramagnetic activity even at very low fields .",
        "rewrite_text": "The magnetic properties of carbon nanotubes and fullerenes have been extensively investigated, yet their effects on electronic structure remain poorly understood. In this study, we employ density functional theory to explore how external magnetic fields affect the electronic structure of double-wall zigzag (n = 3) and armchair (n = 4) nanographenes of varying sizes. Our findings indicate that for smaller nanographenes, specifically n = 3 and n = 4, there are no significant alterations in the band gap when a magnetic field of up to 10 T is applied along the tunnel axis. However, as the height of the structures increases, the band gaps tend to decrease significantly under such fields due to the emergence of Landau levels. The results reveal a strong dependence of the magnetic response on the chirality of the nanographenes; for example, zigzag configurations exhibit diamagnetic behavior at low fields but transition to paramagnetic behavior above 1 T. In contrast, armchair structures consistently display paramagnetic behavior even at minimal field strengths.",
        "ori-fast-z-score": -1.6059101370939322,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": -1.7888543819998317
    },
    {
        "original_text": "The electronic structure and transport properties of intrinsic ripples on single-layered graphene are investigated by using the tight-binding model with first-principles hopping parameters, which is based on density-functional theory (DFT). The results show that the ripple-induced local strain can lead to an energy gap opening at Dirac point for both armchair and zigzag directions. In addition, it is found that the electron mobility decreases as the amplitude increases due to the increase of scattering centers induced by the ripple structures. These findings may be useful for understanding the physical mechanism behind the experimental observations of rippling effects on the electrical performance of graphene devices. Graphene has attracted great attention because its unique two-dimensional honeycomb lattice structure leads to many novel phenomena such as high carrier mobility  1  , Klein tunneling  2  , valley polarization  3  , etc.. However, recent experiments have shown that the pristine flat monolayer graphene sheets tend to form ripples spontaneously  4  . It was also reported that these ripples could affect the electrical performance of graphene-based devices  5  .\nIn this work we investigate the effect of ripples on the electronic structure and transport properties within the framework of the tight-binding model  6  . We find that the ripple-induced strains can open up an energy gap around the Fermi level for both armchair andzigzag directions. Moreover, the electron mobility decreases as increasing the amplitude of ripples since more scattering centers are introduced into the system.",
        "watermark_text": "The mechanical composition and transport properties of intrinsic ripples on single - layered graphene are examined by using the fast - binding model with first - principles hopping characteristics , which is based on density - functional theory ( DFT ) . The results show that the ripple - caused local tension can lead to an energy gap opening at Dirac position for both armchair and zigzag directions .In addition , it is found that the electron mobility decreases as the frequency rises due to the increase of absorption centers caused by the ripple structures . These conclusions could be valuable for studying the physical mechanism behind the empirical observations of rippling influence on the electrical performance of graphene devices .Graphene has garnered great popularity because its unique two - dimensional honeycomb structure form gives to many novel processes such as long carrier density 1 , Klein tunneling 2 , valley polarization 3 , etc . . However , recent experiments have shown that the pristine rolled monolayer graphene strands tend to form ripples spontaneously 4 .It was also reported that these ripples could affect the electrical performance of graphene - based products 5 . In this research we investigate the impact of ripples on the electronic structure and transport properties within the framework of the tight - binding model 6 .We see that the ripple - mediated strains can offer up an energy gap around the Fermi level for both armchair andzigzag directions . Moreover , the electron mobility decreases as increasing the frequency of ripples since more scattering centers are introduced into the system .",
        "rewrite_text": "This study investigates the mechanical composition and transport properties of intrinsic ripples in single-layer graphene using a fast-binding model with first-principles hopping characteristics derived from density-functional theory (DFT). The findings indicate that the local tension induced by ripples can open an energy gap at the Dirac point in both armchair and zigzag orientations. Additionally, it is observed that electron mobility decreases with increasing ripple frequency due to the heightened number of scattering centers created by the ripple structures. These insights could be crucial for understanding the underlying physical mechanisms that explain the observed effects of rippling on the electrical performance of graphene devices. Graphene has gained significant attention because its distinctive two-dimensional honeycomb structure enables various novel phenomena, such as long carrier lifetimes, Klein tunneling, and valley polarization. However, recent experiments reveal that pristine rolled monolayer graphene tends to form ripples spontaneously. It has also been reported that these ripples can influence the electrical characteristics of graphene-based devices. In this research, we explore the ramifications of ripples on the electronic structure and transport properties within the tight-binding model framework. Our results demonstrate that strain induced by ripples can create an energy gap near the Fermi level in both armchair and zigzag configurations, while increasing ripple frequency leads to a decrease in electron mobility due to the introduction of additional scattering centers in the system.",
        "ori-fast-z-score": 0.3746343246326776,
        "water-fast-z-score": 6.80336051416609,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results for hadron production in semiinclusive DIS off nuclei at large Bjorken x and low Q^2, obtained with the HERMES experiment using data taken between 1997 and 2002. The analysis is performed within the framework of collinear factorisation and the modified perturbative approach to nuclear shadowing developed by Frankfurt et al.. We find that the observed suppression of leading neutron production relative to deuterium can be explained by nuclear effects alone without invoking any additional mechanism such as intrinsic charm or gluon saturation. In addition we observe an enhancement of strange particle production which cannot be described by conventional partonic models but may be attributed to the presence of intrinsic strangeness in the proton wave function. \n \n 1 Introduction \n \n Semi-inclusive deep-inelastic lepton-nucleus scattering (SIDIS) has been studied extensively over many years both experimentally  1 - 6  and theoretically  7  8  9  . This process provides information about the quark structure of the target nucleus through measurements of final state particles produced in association with the scattered lepton. At high values of Bjorken-x, where the struck quarks are highly virtual, SIDIS probes the transition region between the non-perturbative regime governed by confinement physics and the perturbative domain dominated by short-distance interactions  10  . \nIn this kinematic range it becomes possible to study the properties of bound-state systems directly via their interaction with hard probe photons  11  , thereby providing insight into the dynamics underlying the formation of composite states  12  -  14  .\nTheoretical studies have shown that the cross section for SIDIS depends strongly on the transverse momentum k_T of the outgoing hadrons  15  -  17  . It was found that the dependence of the cross sections on k_T could be used to discriminate among different theoretical approaches  18  -  20  . For example, calculations based on the standard DGLAP formalism  21  predict a strong increase of the cross section with increasing k_T  22  while those employing the CCFM evolution equations  23  lead to much weaker dependences  24  . \n \n 2 Experimentally measured quantities",
        "watermark_text": "We present the results for hadron emission in semiinclusive DIS off hydrogen at large Bjorken x and low Q ^ 2 , obtained with the HERMES experiment using data taken between 1997 and 2002 . The investigation is conducted within the framework of collinear factorisation and the modified perturbative methodology to nuclear shadowing developed by Frankfurt et al . . We see that the seen suppression of leading neutron production relative to deuterium can be described by nuclear effects alone without invoking any additional process such as intrinsic charm or gluon saturation .In addition we study an enhancement of odd particle production which cannot be described by traditional partonic theories but might be due to the presence of intrinsic strangeness in the proton wave function . 1 Introduction Semi - inclusive shallow - inelastic lepton - nucleus scattering ( SIDIS ) has been studied thoroughly over numerous years both experimentally 1 - 6 and theoretically 7 8 9 .This process provides knowledge about the quark configuration of the target nucleus through measurements of last state particles generated in association with the scattered lepton . At high values of Bjorken - x , where the strikes quarks are extremely virtual , SIDIS probes the transfer region between the non - perturbative period controlled by confinement physics and the perturbative domain dominated by short - distance interactions 10 .In this kinematic range it becomes possible to study the properties of bound - state systems fully via their association with hard probe photons 11 , thereby providing information into the dynamics underlying the formation of composite states 12 - 14 . Theoretical experiments have shown that the cross section for SIDIS relies highly on the transverse momentum n _ T of the outgoing hadrons 15 - 17 .It was shown that the dependence of the cross sections on k _ T might be used to discriminate among different conceptual approaches 18 - 20 . For instance , analyses based on the standard DGLAP formalism 21 predict a positive increase of the cross section with expanding k _ T 22 while those adopting the CCFM evolution equations 23 lead to considerably weaker dependences 24 .2 Experimentally recorded quantities",
        "rewrite_text": "We report on our findings regarding hadron emission in semi-inclusive deep inelastic scattering (DIS) off hydrogen at high Bjorken x and low Q², derived from data collected by the HERMES experiment between 1997 and 2002. Our exploration is framed within the context of collinear factorization and the modified perturbative approach to nuclear shadowing established by Frankfurt et al. Our results indicate that the observed suppression of leading neutron production relative to deuterium can be attributed solely to nuclear effects, without the need to invoke additional processes such as intrinsic charm or gluon saturation. Furthermore, we investigate the increased production of odd particles, which traditional partonic theories fail to explain, suggesting that this phenomenon may arise from the presence of intrinsic strangeness in the proton's wave function. \n\n1 Introduction \n\nSemi-inclusive shallow-inelastic lepton-nucleus scattering (SIDIS) has been extensively investigated both experimentally and theoretically over the years. This process provides insights into the quark structure of the target nucleus by analyzing the final state particles produced alongside the scattered lepton. At elevated Bjorken x values, where the struck quarks have significant virtuality, SIDIS serves as a probe into the transition region bridging non-perturbative confinement physics and perturbative short-distance interactions. In this kinematic regime, it becomes feasible to comprehensively study the characteristics of bound-state systems through their interactions with hard probe photons, thereby illuminating the dynamics involved in the formation of composite states. Theoretical analyses indicate that the cross-section for SIDIS is highly sensitive to the transverse momentum (k_T) of the outgoing hadrons, with variations in k_T facilitating differentiation among various theoretical frameworks. For example, analyses utilizing the standard DGLAP formalism predict a positive correlation between the cross-section and increasing k_T, while those employing CCFM evolution equations suggest weaker dependences. \n\n2 Experimentally Recorded Quantities",
        "ori-fast-z-score": -0.7474093186836597,
        "water-fast-z-score": 6.560592908445457,
        "rewrite-fast-z-score": 1.8593393604027364
    },
    {
        "original_text": "We present the discovery of two gravitationally lensed quasars, QSO B1608+656A and QSO B1608+655B (hereafter A and B), which are separated by 1.5 arcsec in projection on to the sky but have very different redshifts z = 2.7 and z = 3.6 respectively. The lensing galaxy is identified with an elliptical galaxy at redshift 0.33 located between the quasar images. We find that this system has properties similar to those found for other known gravitational lenses produced by massive clusters of galaxies. However, we show that it can be explained as being due to the lensing effect of only one cluster member -the central globular cluster of the lensing galaxy-if its mass-to-light ratio M/LV ~ 100M⊙ /L⊙ V . This value agrees well with previous estimates based on observations of Galactic globular clusters.",
        "watermark_text": "We report the discovery of two gravitationally lensed quasars , QSO B1608 + 656A and QSO B1608 + 655B ( hereafter A and B ) , which are apart by 1 . 5 arcsec in projection on to the heavens but have very different redshifts z = 2 . 7 and z = 3 . 6 respectively . The lensing galaxy is identified with an elliptical galaxy at redshift 0 . 33 located between the quasar images .We see that this scheme has properties similar to those noted for other known gravitational lenses generated by massive clusters of clusters . However , we find that it can be understood as being owing to the lensing effect of only one cluster member - the main globular cluster of the lensing galaxy - if its mass - to - light density M / LV ~ [UNK] / [UNK] V .This value agrees well with previous estimates based on observations of Galactic globular regions .",
        "rewrite_text": "We announce the discovery of two gravitationally lensed quasars, QSO B1608 + 656A and QSO B1608 + 655B (referred to as A and B), which are separated by 1.5 arcseconds in projection in the sky but possess significantly different redshifts of z = 2.7 and z = 3.6, respectively. The lensing galaxy has been identified as an elliptical galaxy at a redshift of 0.33, situated between the images of the quasars. Our observations indicate that this configuration exhibits similarities to other known gravitational lenses caused by massive galaxy clusters. However, we conclude that this phenomenon can be attributed solely to the lensing effect of a single cluster member—the primary globular cluster of the lensing galaxy—assuming a mass-to-light density ratio of M/LV ~ [UNK]/[UNK] V. This estimate aligns well with prior calculations based on observations of Galactic globular regions.",
        "ori-fast-z-score": 0.14907119849998599,
        "water-fast-z-score": 4.221158824088691,
        "rewrite-fast-z-score": 0.7001400420140048
    },
    {
        "original_text": "We present results on semi-inclusive deep-inelastic scattering (SIDIS), ep -> e  p X , with an emphasis on final state interaction effects for both proton and neutron targets. We use a model-independent approach to extract information about the transverse momentum dependence of partonic distributions by studying azimuthal correlations between hadrons produced in SIDIS events. The data were collected using the HERMES experiment at DESY during 1997-2000. Final state interactions are studied through their effect on the cross section as well as on various asymmetries. For example we show that the observed cos2#h correlation is consistent with rescattering contributions involving quarks carrying only a small fraction of the target nucleon s longitudinal momentum. This result indicates that the quark sea may be more asymmetric than previously thought. Furthermore, our analysis shows that the magnitude of the rescattering contribution depends strongly on the kinematics chosen. Finally, we study the influence of FSI on the extraction of transversity distribution functions.",
        "watermark_text": "We report findings on quasi - inclusive deep - inelastic scattering ( SIDIS ) , ep - > e p X , with an emphasis on final state interaction effects for both proton and neutron targets . We use a theory - independent approach to extract information about the transverse momentum dependence of partonic distributions by examining azimuthal correlations between hadrons observed in SIDIS events .The data were collected using the HERMES experiment at DESY during 1997 - 2000 . Final state effects are studied through their effect on the cross section as well as on various asymmetries .For instance we find that the reported cos2 # h relationship is compatible with rescattering contributions using quarks carrying only a small fraction of the target nucleon s longitudinal momentum . This result suggests that the quark ocean must be more asymmetric than previously thought .Furthermore , our analysis shows that the extent of the rescattering contribution varies strongly on the kinematics selected . Finally , we study the impact of FSI on the extraction of transversity distribution functions .",
        "rewrite_text": "We present our findings on quasi-inclusive deep-inelastic scattering (SIDIS), specifically the process ep → e p X, focusing on final state interaction effects for both proton and neutron targets. Employing a theory-independent method, we extract insights into the transverse momentum dependence of parton distributions by analyzing azimuthal correlations between hadrons detected in SIDIS events. The data were gathered from the HERMES experiment at DESY during the years 1997 to 2000. We investigate final state interactions by examining their influence on the cross section and various asymmetries. Notably, we observe that the reported cos2 φh relationship aligns with rescattering contributions where quarks carry only a minimal fraction of the longitudinal momentum of the target nucleon. This finding implies that the quark sea may be more asymmetric than previously realized. Additionally, our analysis demonstrates that the degree of rescattering contributions significantly depends on the selected kinematics. Lastly, we explore how final state interactions affect the extraction of transversity distribution functions.",
        "ori-fast-z-score": 0.11704114719613057,
        "water-fast-z-score": 4.242640687119286,
        "rewrite-fast-z-score": -0.3511234415883917
    },
    {
        "original_text": "We study the dynamics of string cosmologies with nontrivial dilaton potentials, focusing on their chaotic behavior. We show that for certain classes of potentials there are regions where trajectories can be trapped by unstable fixed points or periodic orbits. In these cases we find that the system is not ergodic but has an infinite number of attractors which correspond to different values of the Hubble parameter H(t). The existence of such attractor solutions may have important consequences for the evolution of our universe. For example, it could explain why the present value of H(t) differs so much from its initial value at t = 0. It also provides a possible explanation for the observed flatness problem since the volume V (t) grows exponentially fast during inflation while the energy density decreases as 1/V (t).\nThe results presented here were obtained using numerical methods based on the fourth-order Runge-Kutta algorithm combined with Newton s method for finding roots.",
        "watermark_text": "We research the dynamics of string cosmologies with nontrivial dilaton potentials , concentrating on their chaotic dynamics . We see that for particular categories of potentials there are areas where trajectories can be trapped by unstable fixed points or periodic orbits .In these circumstances we find that the system is not ergodic but has an endless number of attractors which belong to different values of the Hubble parameter H ( t ) . The existence of such attractor solutions may have important implications for the evolution of our universe .For instance , it could explain why the present value of H ( t ) changes so greatly from its initial value at t = 0 . It additionally offers a possible reason for the seen flatness problem since the volume V ( t ) rises exponentially rapidly during inflation while the electricity capacity reduces as 1 / V ( t ) .The results presented here were obtained using numerical technique based on the fourth - order Runge - Kutta algorithm coupled with Newton s method for finding roots .",
        "rewrite_text": "We investigate the dynamics of string cosmologies that feature nontrivial dilaton potentials, with a particular focus on their chaotic behaviors. Our findings reveal that for specific types of potentials, there exist regions where trajectories can become trapped by unstable fixed points or periodic orbits. In such scenarios, the system does not exhibit ergodicity; instead, it possesses an infinite number of attractors corresponding to various values of the Hubble parameter H(t). The presence of these attractor solutions could have significant implications for the evolution of our universe. For example, they might elucidate why the current value of H(t) differs so markedly from its initial value at t = 0. Additionally, this framework may provide insight into the observed flatness problem, as the volume V(t) expands exponentially during inflation while the energy density decreases proportionately to 1/V(t). The results discussed here were derived using numerical methods based on the fourth-order Runge-Kutta algorithm combined with Newton's method for root-finding.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 4.85071250072666,
        "rewrite-fast-z-score": -1.7556172079419585
    },
    {
        "original_text": "We present integral formulas for the steady-state distribution and current in the asymmetric simple exclusion process (ASEP) with open boundaries, which is one of the most important models to describe non-equilibrium phenomena such as traffic flow on highways or biochemical reactions at molecular motors.  We derive these results by using an exact mapping between ASEP and the totally asymmetric zero-range process (TAZRP), which can be solved exactly via matrix product ansatz. The obtained formulae are expressed only in terms of elementary functions and thus provide explicit expressions for physical quantities that have been studied so far mainly numerically. In particular, we show that our result reproduces known results for the case where particles enter and exit at both ends of the system with equal rates. Furthermore, we obtain new results for the cases where particles enter and/or exit at either end of the system with unequal rates. \nI. INTRODUCTIO N\n\nThe asymmetric simple exclusion process (AS EP)\nis one of the most fundamental models describing nonequilibrium phenomena  1  . It describes the dynamics of interacting particles hopping along a chain of L sites under the following rules: each site i = 1, ..., L contains at most one particle; if there is no particle at site i , then it hops rightward with rate p ; otherwise, it stays still. If there is already another particle at site i , however, this particle cannot move until the first particle moves away. This model has attracted much attention because its stationary state exhibits various interesting properties depending on boundary conditions  2  .\nIn recent years, several studies have focused on the so-called open-boundary condition  3  -  8  : Particles enter into the leftmost site of the chain with probability α per unit time and leave from the rightmost site with probability β per unit time. For example, when α = β = 1/2, the stationary state becomes uniform regardless of the initial configuration  9  . On the other hand, when α > β , the stationary state shows phase separation  10  . Moreover, when α < β , the stationary state displays shock profiles  11  . These features make the AS EP a powerful tool to",
        "watermark_text": "We present integral formulas for the stable - state distribution and current in the asymmetric simple exclusion cycle ( ASEP ) with open boundaries , which is one of the most important models to explain non - equilibrium phenomena such as traffic flow on highways or biochemical reactions at molecular motors . We derive these results by using an precise mapping between ASEP and the completely asymmetric zero - range method ( TAZRP ) , which can be solved exactly via matrix product ansatz .The achieved formulae are written only in terms of elementary functions and therefore offer explicit expressions for mechanical parameters that have been studied so far primarily numerically . In particular , we prove that our consequence reproduces known data for the case where objects enter and exit at both sides of the system with equal rates .Furthermore , we obtain new data for the situations where objects enter and / or enter at either end of the process with unequal levels . I . INTRODUCTIO N The asymmetric simple elimination mechanism ( AS EP ) is one of the most important models explaining nonequilibrium phenomena 1 .It studies the dynamics of interacting particles hopping along a network of L locations under the following laws : each site i = 1 , . . . , L includes at most one particle ; if there is no particle at site i , then it hops rightward with speed r ; otherwise , it remains still . If there is already another molecule at site i , however , this particle cannot move until the first particle moves away .This theory has garnered considerable scrutiny because its stationary state exhibits several interesting properties depending on boundary constraints 2 . In recent years , various studies have concentrated on the so - called open - boundary relation 3 - 8 : Particles enter into the leftmost site of the chain with probability α per unit time and leave from the rightmost site with probability β per unit time .For instance , when α = β = 1 / 2 , the stationary state remains regular regardless of the first configuration 9 . On the other hand , when α > β , the stationary state shows phase separation 10 .Moreover , when α < β , the stationary state presents shock profiles 11 . These features make the AS EP a powerful tool to",
        "rewrite_text": "We introduce integral formulas for the steady-state distribution and current within the asymmetric simple exclusion process (ASEP) under open boundary conditions, a key model for understanding non-equilibrium phenomena such as traffic dynamics on highways and biochemical reactions in molecular motors. Our derivation involves a precise mapping between ASEP and the completely asymmetric zero-range process (TAZRP), which can be solved exactly using a matrix product ansatz. The formulas we obtain are expressed solely in terms of elementary functions, providing explicit expressions for mechanical parameters that have primarily been studied through numerical methods. Notably, we demonstrate that our results reproduce established data for scenarios where particles enter and exit the system at both ends at equal rates. Additionally, we present new findings for conditions where particles enter and/or exit at different rates at each end. \n\nI. INTRODUCTION\n\nThe asymmetric simple exclusion process (ASEP) is a crucial model for understanding non-equilibrium phenomena. It examines the dynamics of interacting particles moving along a lattice of L sites, governed by specific rules: each site i (where i = 1, ..., L) can host at most one particle; if site i is unoccupied, a particle can hop to the right with speed r; if a particle is present, it cannot move until the other particle vacates the site. This theory has received significant attention due to its stationary state, which exhibits various intriguing properties influenced by boundary conditions. In recent years, research has focused on the open boundary condition, where particles enter the leftmost site with probability α per unit time and exit from the rightmost site with probability β per unit time. For example, when α = β = 1/2, the steady state remains regular irrespective of the initial configuration. Conversely, when α > β, the steady state exhibits phase separation, and when α < β, shock profiles are observed. These characteristics highlight the ASEP's utility as a powerful analytical tool for studying non-equilibrium systems.",
        "ori-fast-z-score": 0.6135719910778963,
        "water-fast-z-score": 8.743288094601613,
        "rewrite-fast-z-score": 1.3000224919331833
    },
    {
        "original_text": "We have recently shown that the one-range addition theorems derived in our previous work are valid not only for the Coulomb interaction potential but also its derivatives, such as the nuclear attraction potential or the exchange potential. \n \n In this comment we show how these results can be used to derive new addition theorems for the nuclear attraction potential and the exchange potential. These new addition theorems are useful when calculating matrix elements between atomic orbitals with different angular momenta. We illustrate their application using examples involving hydrogenic wave functions. Finally, we discuss some possible extensions of these results. DOI: 10.1063/1.2055316 \n \n This is an extended version of a comment published in ChemPhysChem. DOI: 10.1002/cphc.201500420 \n \n \n \n One-range addition theorems play important roles in many areas of physics including quantum chemistry  1-3 , molecular physics  4 , condensed matter physics  5 , etc.. They provide simple expressions for evaluating matrix elements of various potentials between two arbitrary wavefunctions. For example, they allow us to calculate matrix elements of the Coulomb interaction potential between any pair of atomic orbital basis sets without having to perform complicated numerical integrations  6 . Recently, we showed that the same approach could be applied to other types of potentials  7-9 .",
        "watermark_text": "We have recently shown that the one - range addition theorems generated in our previous study are applicable not only for the Coulomb interaction potential but also its derivatives , such as the atomic attraction potential or the exchange potential . In this comment we give how these results can be used to derive new addition theorems for the atomic attraction potential and the transfer potential .These new addition theorems are helpful when calculating matrix elements between atomic orbitals with various angular momenta . We illustrate their application using examples involving hydrogenic wave systems .Finally , we explain some possible extensions of these results . DOI : 10 . 1063 / 1 . 2055316 This is an extended version of a note published in ChemPhysChem .DOI : 10 . 1002 / cphc . 201500420 One - range addition theorems play important roles in many areas of physics including quantum chemistry 1 - 3 , molecular physics 4 , condensed matter physics 5 , etc . . They provide simple expressions for evaluating matrix elements of various potentials between two arbitrary wavefunctions .For instance , they allow us to estimate matrix elements of the Coulomb interaction potential between any pair of atomic orbital basis sets without having to conduct complicated mathematical integrations 6 . Recently , we demonstrated that the same method could be applied to other types of potentials 7 - 9 .",
        "rewrite_text": "We have recently demonstrated that the one-range addition theorems developed in our earlier work are relevant not only for the Coulomb interaction potential but also for its derivatives, including the atomic attraction potential and the exchange potential. In this commentary, we outline how these findings can be utilized to formulate new addition theorems for the atomic attraction potential and the transfer potential. These newly established theorems are beneficial for calculating matrix elements between atomic orbitals of varying angular momenta. We provide examples of their application using hydrogenic wave systems. Additionally, we discuss potential extensions of these results. The DOI for this work is 10.1063/1.2055316, and it serves as an extended version of a note published in ChemPhysChem, DOI: 10.1002/cphc.201500420. One-range addition theorems are crucial in numerous fields of physics, including quantum chemistry, molecular physics, and condensed matter physics. They offer straightforward expressions for evaluating matrix elements of different potentials between any two wavefunctions. For example, they enable us to compute matrix elements of the Coulomb interaction potential between any atomic orbital basis sets without requiring complex mathematical integrations. Recently, we showed that this technique can also be applied to other types of potentials.",
        "ori-fast-z-score": -0.5076730825668095,
        "water-fast-z-score": 3.7567808109943908,
        "rewrite-fast-z-score": 0.10369516947304253
    },
    {
        "original_text": "We have studied the dependence of growth rates for collisionless magnetic instabilities (CMIs) in nonrelativistic electron-ion plasmas with Maxwellian velocity distributions, using linear kinetic theory. We found that the growth rate is strongly dependent upon the shape of the distribution function at high velocities. In particular, we find that the fastest growing mode has its maximum growth rate when the distribution function peaks near the speed of light. This result suggests that CMIs may be excited more easily than previously thought under certain conditions. \n \n The effect of solitary waves on the growth rates was also investigated numerically. It was shown that the presence of solitary waves can significantly enhance or suppress the growth rates depending on their amplitudes relative to those of background fluctuations. These results are important because they show how nonlinear effects such as solitary wave generation affect the stability properties of plasma systems. They should therefore provide useful information about the evolution of unstable plasma systems.",
        "watermark_text": "We have researched the dependence of growth rates for collisionless magnetic instabilities ( CMIs ) in nonrelativistic electron - ion plasmas with Maxwellian velocity distributions , using linear kinetic theory . We showed that the development frequency is strongly dependent upon the morphology of the distribution function at high velocities .In particular , we find that the fastest growing mode has its highest growth speed when the distribution parameter peaks near the speed of light . This result suggests that CMIs might be excited more easily than previously thought under certain conditions .The impact of solitary waves on the development rates was also examined numerically . It was shown that the presence of solitary waves can significantly affect or resist the development rates depending on their amplitudes compared to those of background fluctuations .These conclusions are important because they show how nonlinear effects such as solitary wave generation affect the stability properties of plasma networks . They should therefore offer useful details about the evolution of unstable plasma networks .",
        "rewrite_text": "We conducted a study on the growth rates of collisionless magnetic instabilities (CMIs) in nonrelativistic electron-ion plasmas characterized by Maxwellian velocity distributions, utilizing linear kinetic theory. Our findings indicate that the development frequency is highly influenced by the structure of the distribution function at elevated velocities. Notably, we discovered that the fastest-growing mode achieves its maximum growth rate when the distribution parameter peaks close to the speed of light. This suggests that CMIs may be more readily excited under certain conditions than previously anticipated. Additionally, we numerically examined the effect of solitary waves on growth rates and found that their presence can significantly modify or inhibit the rates based on their amplitudes in relation to background fluctuations. These findings are significant as they illuminate how nonlinear phenomena, such as solitary wave generation, impact the stability characteristics of plasma systems, providing valuable insights into the dynamics of unstable plasma networks.",
        "ori-fast-z-score": 1.3251783128981585,
        "water-fast-z-score": 5.963302408041713,
        "rewrite-fast-z-score": 0.22941573387056174
    },
    {
        "original_text": "We study spin effects on the lattice QCD using recurrence lattices (RL) with multi-site exchanges, which are constructed by applying the RL transformation to the original fermion action. We show that the spin dependence is suppressed for large quark masses but not completely removed even at mq = 5 GeV. The residual spin dependence can be reduced further if we use larger number of sites in the exchange term. In this work, we adopt Ns = 4 as an example. We also find that the spin dependent part of the effective potential has no imaginary part up to O(a^4). This implies that there exists no spontaneous breaking of chiral symmetry due to spin effects within our framework. Finally, we discuss possible extensions of our method. PACS numbers: 11.15.Ha, 12.38.Gc, 13 .25.Hw \nI. INTRODUCTORY REMARK\nIn recent years, it was found that the standard Wilson-type fermions suffer from severe problems such as the so-called species doubling problem  1  , the Nielsen-Ninomiya theorem  2  , and the Gribov copy problem  3  . These difficulties have been overcome by introducing new types of fermionic actions  4  -  8  .\nThe most popular one among them is probably the overlap-Dirac operator  9  , whose eigenfunctions satisfy the Ginsparg-Wilson relation  10  . However, its numerical cost grows rapidly when the lattice volume becomes large because the inverse of the Dirac operator must be calculated exactly. To reduce the computational costs, several approximate methods were proposed  11  -  13  . Among these approaches, the Neuberger overlap operator  14  seems to be the best choice so far  15  .\nAnother promising approach is based on the idea of the exact renormalization group  16  . It was shown  17  that the fermion determinant detD(μ), where D(μ) denotes the fermion matrix defined through the fermion action Sf  U  ≡ ∑x Tr γμD(μ)Ux , satisfies the following flow equation:",
        "watermark_text": "We research spin effects on the lattice QCD utilizing recurrence lattices ( RL ) with multi - location exchanges , which are built by using the RL shift to the previous fermion action . We see that the spin dependence is suppressed for large quark masses but not totally eliminated even at mq = 5 GeV .The residual spin effect can be reduced further if we using larger number of places in the transfer term . In this study , we follow Ns = 4 as an instance .We additionally find that the spin dependent part of the effective potential has no imaginary part up to O ( a ^ 4 ) . This implies that there exists no premature breaking of chiral symmetry due to spinning factors within our framework .Finally , we investigate possible extensions of our technique . PACS numbers : 11 . 15 . Ha , 12 . 38 . Gc , 13 . 25 . Hw I .INTRODUCTORY REMARK In recent years , it was shown that the standard Wilson - class fermions suffer from severe challenges such as the so - called genus doubling question 1 , the Nielsen - Ninomiya conjecture 2 , and the Gribov copies problem 3 . These difficulties have been overcome by introducing novel forms of fermionic operations 4 - 8 .The most popular one among them is probably the overlap - Dirac operator 9 , whose eigenfunctions satisfy the Ginsparg - Wilson relation 10 . However , its numerical cost rises steadily when the lattice volume becomes large because the inverse of the Dirac operator must be determined exactly .To reduce the theoretical costs , various approximate approaches were recommended 11 - 13 . Among these method , the Neuberger overlap operator 14 says to be the best choice so far 15 .Another promising alternative is based on the idea of the exact renormalization group 16 . It was shown 17 that the fermion determinant detD ( μ ) , where D ( μ ) denotes the fermion matrix established through the fermion action Sf U ≡ [UNK] Tr γμD ( μ ) Ux , satisfies the following fluid equation :",
        "rewrite_text": "We investigate spin effects within lattice QCD using recurrence lattices (RL) that feature multi-location exchanges, constructed by applying the RL shift to the previous fermion action. Our findings indicate that while the spin dependence diminishes for larger quark masses, it is not completely absent even at mq = 5 GeV. The residual spin effect can be further minimized by employing a greater number of locations in the transfer term; in this study, we specifically consider Ns = 4. Additionally, we observe that the spin-dependent component of the effective potential remains free of imaginary parts up to O(a^4), suggesting that our framework prevents premature chiral symmetry breaking due to spin factors. Lastly, we explore potential extensions of our methodology. \n\nPACS numbers: 11.15.Ha, 12.38.Gc, 13.25.Hw.\n\n**I. INTRODUCTORY REMARKS**  \nIn recent years, it has been demonstrated that standard Wilson-class fermions face significant challenges, including the genus doubling issue, as well as the Nielsen-Ninomiya conjecture and the problem of Gribov copies. These challenges have been addressed through innovative forms of fermionic operations. Among these, the overlap-Dirac operator is perhaps the most well-known, with its eigenfunctions conforming to the Ginsparg-Wilson relation. However, its numerical demands increase substantially with larger lattice volumes, due to the need for precise calculations of the Dirac operator's inverse. To alleviate theoretical costs, several approximate methods have been proposed. Of these, the Neuberger overlap operator is regarded as the most reliable option to date. Another promising alternative involves the concept of exact renormalization group techniques. It has been established that the fermion determinant detD(μ), where D(μ) indicates the fermion matrix constructed from the fermion action Sf U ≡ Tr γμD(μ) Ux, adheres to a fluid equation.",
        "ori-fast-z-score": -0.8838834764831843,
        "water-fast-z-score": 7.424621202458749,
        "rewrite-fast-z-score": 0.4472135954999579
    },
    {
        "original_text": "We present new results on outflow properties in quasars based on observations with Chandra, XMM-Newton, Spitzer Space Telescope (SST), Hubble Space Telescope (HST) and ground-based optical telescopes. We find that quasar winds are ubiquitous at all luminosities probed by our sample; they have high velocities (up to 0.26c) and mass loss rates (10-100 Msun/yr). The wind kinetic power is comparable or exceeds the bolometric luminosity for most objects studied here. Winds appear to be launched close to the central black hole but can extend up to several hundred parsecs away. They show evidence for both thermal and non-thermal emission components. In addition we report on the discovery of an X-ray absorber associated with one of these winds which has been previously detected only in absorption lines seen in UV spectra. This absorber shows significant variability over timescales as short as days.",
        "watermark_text": "We report new data on outflow properties in quasars based on observations with Chandra , XMM - Newton , Spitzer Space Telescope ( SST ) , Hubble Space Telescope ( HST ) and land - based optical telescopes . We see that quasar clouds are ubiquitous at all luminosities probed by our sample ; they have high velocities ( up to 0 . 26c ) and mass loss patterns ( 10 - 100 Msun / yr ) .The wind kinetic power is equal or exceeds the bolometric luminosity for most objects discussed here . Winds appear to be launched close to the central black hole but can extend up to several hundred parsecs apart .They show proof for both thermal and non - thermal emitted elements . In addition we publish on the discovery of an X - ray absorber associated with one of these winds which has been previously observed only in absorption patterns shown in UV spectra .This absorber indicates considerable variability over timescales as short as days .",
        "rewrite_text": "We present new findings on the outflow properties of quasars derived from observations with Chandra, XMM-Newton, the Spitzer Space Telescope (SST), the Hubble Space Telescope (HST), and various ground-based optical telescopes. Our analysis reveals that quasar clouds are prevalent across all luminosity ranges studied; they exhibit high velocities of up to 0.26c and demonstrate mass loss rates between 10 and 100 M⊙ per year. The kinetic energy of these winds is comparable to or surpasses the bolometric luminosity for the majority of the quasars examined. These winds appear to originate near the central black hole and can extend several hundred parsecs away. Evidence for both thermal and non-thermal emitted elements is present. Additionally, we report the discovery of an X-ray absorber linked to one of these winds, which had previously only been detected through absorption features in UV spectra. This absorber shows significant variability over timescales as brief as days.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.532898610306738,
        "rewrite-fast-z-score": -0.9113223768657671
    },
    {
        "original_text": "We have recently shown that the primordial spectrum of density fluctuations can be obtained by solving an initial value problem for a massless scalar field in deSitter space-time, which is known as the cosmological slingshot effect (CSE). In this work we show how to match the CSE predictions with those of the Wilkinson Microwave Anisotropy Probe 3-year data release (WMAP3) and compare them against other models. We find that our model fits well within 1-sigma error bars on all parameters except n_s, where it lies just outside 2-sigma limits. The best-fit values are given by:  H_0 = 72.6 +/- 0.9 km/s/Mpc,  Omega_m = 0.26 +/- 0.01,   Omega_Lambda = 0.74 +/- 0.02,    n_s = 0.96 +/- 0.06.   These results agree very well with recent measurements made using Type Ia supernovae.  Our analysis shows that the CSE provides a viable alternative explanation for the origin of cosmic structure formation.",
        "watermark_text": "We have recently shown that the primordial range of density fluctuations can be obtained by solving an initial value problem for a massless scalar field in deSitter space - time , which is known as the cosmological slingshot phenomenon ( CSE ) . In this research we find how to tie the CSE predictions with those of the Wilkinson Microwave Anisotropy Probe 3 - month data release ( WMAP3 ) and compare them against other models .We see that our model fits well within 1 - sigma error bars on all parameters except k _ s , where it lies just outside 2 - sigma limits . The best - fitting values are given by : H _ 0 = 72 . 6 + / - 0 . 9 km / s / Mpc , Omega _ m = 0 . 26 + / - 0 . 01 , Omega _ Lambda = 0 . 74 + / - 0 . 02 , n _ s = 0 . 96 + / - 0 . 06 .These conclusions agree very best with recent observations made using Type Ia supernovae . Our study shows that the CSE provides a viable alternative theory for the origin of universe mechanism formation .",
        "rewrite_text": "We have recently demonstrated that the primordial density fluctuation range can be obtained by addressing an initial value problem involving a massless scalar field in de Sitter space-time, a process referred to as the cosmological slingshot phenomenon (CSE). In this study, we establish a connection between the predictions of the CSE and those derived from the Wilkinson Microwave Anisotropy Probe's three-month data release (WMAP3), while also comparing our findings with other models. Our model aligns well within 1-sigma error bars for all parameters, except for k_s, which falls slightly outside the 2-sigma limits. The optimal values we determined are: H_0 = 72.6 ± 0.9 km/s/Mpc, Omega_m = 0.26 ± 0.01, Omega_Lambda = 0.74 ± 0.02, and n_s = 0.96 ± 0.06. These results are in strong agreement with recent observations from Type Ia supernovae. Our research suggests that the CSE offers a credible alternative theory for the mechanisms behind the formation of the universe.",
        "ori-fast-z-score": -0.9271726499455306,
        "water-fast-z-score": 3.841143835488627,
        "rewrite-fast-z-score": 2.32379000772445
    },
    {
        "original_text": "We study evolutionary games played by agents that are randomly distributed over the nodes of an unstructured network, and interact only with their immediate neighbors in this network. We show how to compute Nash equilibria for such games using local computations at each node. Our approach is based on computing best responses locally at every agent s neighborhood, which we then use as input to a global algorithm that computes all Nash equilibria. The main technical challenge lies in showing that our local algorithms terminate after polynomially many rounds (in terms of the number of players), even though they may be executed concurrently across different parts of the network. This result holds under mild assumptions about the underlying interaction graph. In particular, it applies to any undirected connected graph where there exists some constant upper bound on the degree of its vertices. As a consequence, we obtain polynomial-time algorithms for finding Nash equilibria in several classes of games including congestion games, matching pennies games, and coordination games.",
        "watermark_text": "We research evolutionary matches played by agents that are randomly distributed over the nodes of an unstructured system , and interact only with their immediate relatives in this network . We see how to compute Nash equilibria for such players using local computations at each node .Our strategy is based on solving best responses locally at every agent s neighborhood , which we then use as input to a global algorithm that computes all Nash equilibria . The main engineering obstacle lies in showing that our regional algorithms end after polynomially many tournaments ( in terms of the number of teams ) , even though they may be executed concurrently across different components of the organization .This result holds under mild assumptions about the underlying interaction graph . In particular , it applies to any undirected connected graph where there exists some constant upper bound on the degree of its vertices .As a consequence , we obtain polynomial - time computers for finding Nash equilibria in multiple groups of games including congestion games , matching pennies games , and coordination games .",
        "rewrite_text": "We investigate evolutionary matches conducted by agents that are randomly spread across the nodes of an unstructured system, interacting solely with their immediate neighbors within the network. Our approach for computing Nash equilibria involves executing local calculations at each node. Specifically, we determine best responses within the vicinity of each agent, which then serve as input for a global algorithm designed to calculate all Nash equilibria. A significant engineering challenge is proving that our local algorithms conclude after a polynomial number of tournaments (relative to the number of teams), despite potentially running simultaneously across different parts of the organization. This finding is valid under mild assumptions about the underlying interaction graph, particularly for any undirected, connected graph with a constant upper limit on vertex degree. As a result, we can develop polynomial-time algorithms for identifying Nash equilibria in various game scenarios, including congestion games, matching pennies games, and coordination games.",
        "ori-fast-z-score": 1.2074068598865937,
        "water-fast-z-score": 4.939391699536065,
        "rewrite-fast-z-score": -0.8834522085987723
    },
    {
        "original_text": "In this work, we present an algorithm for the analysis and comparison of profile Hidden Markov Models (HMMs). The proposed method is based on the concept of Structural Similarity Index Measurement (SSIM) which has been widely used in image processing to measure similarity between two images. We use SSIM as a distance metric to compare HMMs by measuring their similarities at different levels of granularity. In addition, our approach allows us to identify regions that are responsible for differences among models. Our experiments show that the proposed method can be effectively applied to analyze and compare HMMs generated using various algorithms such as PSI-BLAST or SAM-T2K. This research was supported by NSERC Discovery Grant RGPIN-2014-04567 awarded to Dr. Yi-Chun Wu. Profile hidden markov model(HMM),Structural similarity index measurement(SSIM),Similarity score matrix(SCM),PSI-BLAST",
        "watermark_text": "In this research , we present an algorithm for the evaluation and contrast of model Hidden Markov Models ( HMMs ) . The proposed approach is based on the idea of Structural Similarity Index Measurement ( SSIM ) which has been widely useful in image processing to measure similarity between two images .We use SSIM as a distance metric to identify HMMs by assessing their similarities at different levels of granularity . In addition , our approach allows us to identify areas that are responsible for variations among models .Our experiments indicate that the suggested method can be effectively used to analyze and compare HMMs generated using numerous algorithms such as PSI - BLAST or SAM - T2K . This research was supported by NSERC Discovery Grant RGPIN - 2014 - 04567 granted to Dr . Yi - Chun Wu .Profile hidden markov model ( HMM ) , Structural similarity index monitoring ( SSIM ) , Similarity rating matrix ( SCM ) , PSI - BLAST",
        "rewrite_text": "In this study, we introduce a novel algorithm for evaluating and contrasting Hidden Markov Models (HMMs). Our approach leverages the Structural Similarity Index Measurement (SSIM), a metric widely used in image processing to assess the similarity between two images. By employing SSIM as a distance metric, we can analyze HMMs in terms of their similarities across various levels of granularity. Additionally, our method facilitates the identification of factors contributing to the differences among models. Experimental results demonstrate that our proposed technique is effective in analyzing and comparing HMMs generated by various algorithms, including PSI-BLAST and SAM-T2K. This research was supported by an NSERC Discovery Grant (RGPIN - 2014 - 04567) awarded to Dr. Yi-Chun Wu. Key terms include Hidden Markov Model (HMM), Structural Similarity Index Measurement (SSIM), Similarity Comparison Matrix (SCM), and PSI-BLAST.",
        "ori-fast-z-score": -0.40451991747794525,
        "water-fast-z-score": 5.258758927213289,
        "rewrite-fast-z-score": -1.7693034738587656
    },
    {
        "original_text": "We report on high-resolution spectroscopy of the lithium-like ions C VI, N VII, O VIII, Ne IX, Mg XI, Si XIII, S XV, Ar XVII, Ca XIX, Fe XXIII, Ni XXIX observed with Chandra HETGS during an outburst of the black hole candidate Cen X-4 (Nova Muscae 1991). The measured line fluxes are used to determine the abundance ratios between different elements as well as their relative abundances compared to solar values. We find that the Li-Be-B element abundances are enhanced by factors up to 100 times solar for some lines. This is consistent with previous results obtained using ASCA data taken at lower spectral resolution. In addition we detect strong emission features due to highly ionized iron which have not been seen before in this source. These new observations allow us to study the chemical composition of the accretion disk around the compact object more accurately than previously possible.",
        "watermark_text": "We report on wide - resolution spectroscopy of the lithium - like ions C VI , N VII , O VIII , Ne IX , Mg XI , Si XIII , S XV , Ar XVII , Ca XIX , Fe XXIII , Ni XXIX found with Chandra HETGS during an outburst of the dark hole contender Cen X - 4 ( Nova Muscae 1991 ) . The measured line fluxes are using to estimate the availability proportions between various objects as well as their relative abundances compared to solar values .We see that the Li - Be - B element abundances are increased by factors up to 100 times solar for some lines . This is compatible with previous findings obtained using ASCA information taken at lower spectral resolution .In addition we perceive strong radiation elements owing to strongly ionized iron which have not been seen before in this source . These new studies permit us to study the chemical composition of the accretion disk around the compact body more accurately than previously possible .",
        "rewrite_text": "We present our findings from high-resolution spectroscopy of lithium-like ions, including C VI, N VII, O VIII, Ne IX, Mg XI, Si XIII, S XV, Ar XVII, Ca XIX, Fe XXIII, and Ni XXIX, observed with the Chandra HETGS during an outburst of the black hole candidate Cen X-4 (Nova Muscae 1991). The measured line fluxes are employed to estimate the relative proportions of various elements and their abundances in comparison to solar values. Our analysis reveals that the abundances of Li, Be, and B elements have increased by factors of up to 100 times the solar quantities for certain lines, consistent with previous results obtained from lower spectral resolution data from ASCA. Additionally, we have observed strong radiation from highly ionized iron, which had not been detected previously in this source. These new insights allow us to examine the chemical composition of the accretion disk surrounding the compact object with greater accuracy than was previously achievable.",
        "ori-fast-z-score": -1.2909944487358056,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": -0.3841106397986879
    },
    {
        "original_text": "We present an improved factorization formula for the fragmentation functions (FFs) of hadrons containing one heavy quark, which is valid in both leading order and next-to-leading order QCD perturbation theory. The new formula takes into account all possible contributions to the FFs at each perturbative order. We show that our results are consistent with those obtained by using other approaches such as the operator product expansion method or the renormalization group equation approach. Finally we give numerical predictions on some important quantities related to the charm-quark FFs. PACS numbers: 12.38.Qk, 13.25.Gv, 11.15.Tk \nI. INTRODUCTORY REMARK\nThe fragmentation function D(z), where z = Phadron/Pquark , describes how quarks fragment into hadrons when they are produced in hard processes like deep-inelastic scattering  1  . It plays an essential role in understanding many phenomena observed experimentally  2  .\nIn this work, we will study the fragmentation functions of hadronic states containing only one heavy quark. In particular, we consider the case of charmed-meson production in e + e-annihilation processes:",
        "watermark_text": "We introduce an updated factorization formula for the fragmentation functions ( FFs ) of hadrons containing one heavy quark , which is valid in both leading order and last - to - leading order QCD perturbation theory . The revised formula takes into consideration all possible contributions to the FFs at each perturbative order .We see that our findings are compatible with those achieved by using other methods such as the operator product expansion method or the renormalization group function method . Finally we give quantitative predictions on some important quantities related to the charm - quark FFs .PACS codes : 12 . 38 . Qk , 13 . 25 . Gv , 11 . 15 . Tk I . INTRODUCTORY REMARK The fragmentation relation D ( z ) , where z = Phadron / Pquark , explains how quarks cluster into hadrons when they are produced in hard processes like shallow - inelastic scattering 1 .It plays an essential part in understanding several phenomena observed experimentally 2 . In this research , we will research the fragmentation processes of hadronic states involving only one heavy quark .In particular , we treat the case of charmed - meson production in e + e - annihilation processes :",
        "rewrite_text": "We present a revised factorization formula for the fragmentation functions (FFs) of hadrons that include a single heavy quark, applicable in both leading order and next-to-leading order QCD perturbation theory. This updated formula accounts for all potential contributions to the FFs at each order of perturbation. Our results align with findings obtained through alternative approaches, such as the operator product expansion and the renormalization group function methods. Additionally, we provide quantitative predictions for several significant quantities related to charm-quark FFs. \n\nPACS codes: 12.38.Qk, 13.25.Gv, 11.15.Tk\n\nI. INTRODUCTORY REMARK\nThe fragmentation relation D(z), where z = Phadron / Pquark, describes how quarks combine to form hadrons during processes like deep inelastic scattering. This relationship is crucial for understanding various experimentally observed phenomena. In this study, we will investigate the fragmentation processes of hadronic states that involve only one heavy quark, with a specific focus on charmed meson production in e+ e- annihilation events.",
        "ori-fast-z-score": -0.11867816581938533,
        "water-fast-z-score": 5.976143046671968,
        "rewrite-fast-z-score": 0.8551861104941365
    },
    {
        "original_text": "We present near-infrared photometry for brown dwarfs (BDs) with masses below 0.075 Msun, members of the young open cluster Lambda Orionis. We find that these BDs have redder J-K colors than field objects at similar spectral types. This is consistent with previous studies showing that low mass stars and BDs are more dusty than higher mass counterparts. The observed color excesses can be explained by accretion disks around the BDs. Using our data we estimate disk fractions between 20-50% among the lowest mass BDs in this sample. These results suggest that most BDs form via core accretion as do high-mass stars. However, it remains unclear whether or not all BDs accrete material to become fully fledged planets. In addition, we show that there may exist two populations of very-low mass BDs: one population which has been affected by accretion processes during its formation; another population whose properties resemble those of older field BDs.",
        "watermark_text": "We see near - infrared photometry for brown dwarfs ( BDs ) with masses below 0 . 075 Msun , part of the young open dwarf Lambda Orionis . We see that these BDs have redder J - K colors than field objects at comparable spectral classes .This is consistent with previous research indicating that minimum mass stars and BDs are more dusty than higher mass rivals . The observed brightness excesses can be described by accretion disks around the BDs .Using our information we estimate disk fractions between 20 - 50 % among the lowest mass BDs in this specimen . These data suggest that most BDs form via nucleus accretion as do large - density stars .However , it remains unsure whether or not all BDs accrete material to become completely fledged planets . In addition , we find that there may contain two communities of very - low mass BDs : one community which has been affected by accretion events during its formation ; another population whose characteristics resemble those of older field BDs .",
        "rewrite_text": "We present near-infrared photometry of brown dwarfs (BDs) with masses below 0.075 Msun, located within the young open cluster Lambda Orionis. Our observations reveal that these BDs exhibit redder J-K colors compared to field objects with similar spectral classifications. This finding aligns with prior studies suggesting that both minimum mass stars and BDs are dustier than their higher mass counterparts. The observed brightness excesses can be attributed to accretion disks surrounding the BDs. Based on our data, we estimate that the disk fraction among the lowest mass BDs in this sample ranges from 20% to 50%. This information implies that the majority of BDs likely form through nucleus accretion, similar to higher-density stars. However, it remains unclear whether all BDs accumulate material to develop fully into planets. Additionally, we propose the existence of two distinct communities of very low-mass BDs: one group that has undergone accretion events during formation, and another whose properties are more representative of older field BDs.",
        "ori-fast-z-score": -1.61245154965971,
        "water-fast-z-score": 5.169842621131974,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We report on the detection and analysis of an optical shock front in the supernova remnant (SNR) Tycho using data obtained with Subaru High Dispersion Spectrograph (HDS). The observed spectrum shows strong emission lines of hydrogen, helium, nitrogen, oxygen, sulfur, argon, calcium, magnesium, silicon, iron ions at wavelengths between 3200Å and 9400Å. We find that these line emissions are well reproduced by a model consisting of two components; one is a photoionized plasma component which emits forbidden lines such as  O III  λλ4959, 5007 and  S II λλ6716, 6731, while another is a collisionally ionized plasma component which produces prominent Balmer series lines including Hα. From this result we conclude that the detected shock front is dominated by collisional ionization rather than photo-ionization. \n \n Keywords: Supernova remnants",
        "watermark_text": "We report on the discovery and evaluation of an optical shock front in the supernova remnant ( SNR ) Tycho using data received with Subaru High Dispersion Spectrograph ( HDS ) . The observed spectrum displays large emitted lines of sulfur , helium , nitrogen , hydrogen , sulfur , argon , potassium , magnesium , silicon , iron ions at wavelengths between 3200Å and 9400Å .We see that these line emissions are better illustrated by a theory consisting of two parts ; one is a photoionized plasma component which emits forbidden lines such as O III λλ4959 , 5007 and S II λλ6716 , 6731 , while another is a collisionally ionized plasma component which generates distinctive Balmer series curves including Hα . From this consequence we suppose that the detected shock front is dominated by collisional ionization instead than photo - ionization .Keywords: Supernova remnants",
        "rewrite_text": "We present our findings on the discovery and analysis of an optical shock front in the supernova remnant (SNR) Tycho, utilizing data obtained from the Subaru High Dispersion Spectrograph (HDS). The spectrum we observed features prominent emission lines from elements such as sulfur, helium, nitrogen, hydrogen, argon, potassium, magnesium, silicon, and iron, with wavelengths ranging from 3200 Å to 9400 Å. Our analysis indicates that these line emissions can be more effectively explained by a dual-component theory. One component consists of a photoionized plasma that emits forbidden lines, such as [O III] λλ4959, 5007 and [S II] λλ6716, 6731, while the other is a collisionally ionized plasma that produces the characteristic Balmer series, including Hα. Based on these observations, we infer that the identified shock front is primarily influenced by collisional ionization rather than photoionization. Keywords: Supernova remnants.",
        "ori-fast-z-score": -1.8203641092364127,
        "water-fast-z-score": 3.780756226875626,
        "rewrite-fast-z-score": -0.2721655269759087
    },
    {
        "original_text": "The Sun is the nearest star to Earth, and its activity has been studied for thousands of years.  The Sun s magnetic field plays an important role in solar activity.   In this talk I will discuss how we can use observations made by spacecraft such as SOHO (Solar and Heliospheric Observatory) and SDO (Solar Dynamics Observatory), together with ground-based telescopes on the Canary Islands, to study the Sun s magnetic fields and their relationship to solar activity. This research helps us understand what happens when stars like our Sun die out - they become red giants that eject huge amounts of material into space which may eventually form new planets or even life elsewhere in the Universe. Keywords: Solar Activity, Red Giant Star, Magnetic Field, Space Weather. Title: Cool Stars in Hot Places. Abstract: The Sun is the nearest star  to Earth, and its  activity has been studied for thousand of years. The Sun s magnetic field play an important role in solar activities. In this talk I will discus how we can use observation made by spacecrafts such as SOHO(Solar and Heliospherical Observatory)  and SDO(Solar Dynamics Observatory),  together with ground based telescope on the Canary Island,  to study the Sun s magentic fields and their relation to solar activity. This reseach help us understant what happen when stars like our sun dies out- they becom red giant stars that ejecta huge amount of materiel into space whic may eventually form new planet or even life elsewere in universe.",
        "watermark_text": "The Sun is the nearest star to Earth , and its activity has been studied for thousands of years . The Sun s magnetic force plays an important role in solar activity .In this talk I will explore how we can using observations made by satellites such as SOHO ( Solar and Heliospheric Observatory ) and SDO ( Solar Dynamics Observatory ) , combined with ground - based telescopes on the Canary Islands , to study the Sun s magnetic waves and their connection to solar activity . This research allows us explain what comes when stars like our Sun die out - they become red giants that eject huge amounts of material into space which would eventually form new planets or even life abroad in the Universe .Keywords : Solar Activity , Red Giant Star , Magnetic Field , Space Weather . Title : Cool Stars in Hot Places .Abstract : The Sun is the nearest star to Earth , and its activity has been studied for thousand of years . The Sun s magnetic force serve an important role in solar movements .In this talk I will discus how we can using observation made by spacecrafts such as SOHO ( Solar and Heliospherical Observatory ) and SDO ( Solar Dynamics Observatory ) , combined with ground based observatory on the Canary Island , to study the Sun s magentic fields and their connection to solar activity . This reseach help us understant what happen when stars like our sun kills out - they becom red massive planets that ejecta huge amount of materiel into space whic may eventually form new planet or even life elsewere in universe .",
        "rewrite_text": "The Sun, our closest star, has been a subject of study for thousands of years. Its magnetic forces play a crucial role in solar phenomena. In this presentation, I will discuss how we can leverage data collected from spacecraft like the Solar and Heliospheric Observatory (SOHO) and the Solar Dynamics Observatory (SDO), along with observations from ground-based telescopes on the Canary Islands, to investigate the Sun’s magnetic waves and their relationship to solar activity. This research helps us understand what occurs when stars like the Sun reach the end of their life cycles; they transform into red giants that expel vast amounts of material into space, which may eventually lead to the formation of new planets and potentially even life elsewhere in the universe. \n\n**Keywords:** Solar Activity, Red Giant Star, Magnetic Field, Space Weather.  \n**Title:** Cool Stars in Hot Places.  \n**Abstract:** The Sun, our nearest star, has intrigued scientists for thousands of years. Its magnetic forces play a significant role in solar movements. In this talk, I will discuss how we can utilize observations made by spacecraft such as the Solar and Heliospheric Observatory (SOHO) and the Solar Dynamics Observatory (SDO), in conjunction with data from ground-based observatories in the Canary Islands, to study the Sun’s magnetic fields and their connection to solar activity. This research enhances our understanding of what happens when stars like our Sun reach the end of their life cycles; they evolve into massive red giants that eject substantial amounts of material into space, which may eventually contribute to the formation of new planets or even life elsewhere in the universe.",
        "ori-fast-z-score": 2.041241452319315,
        "water-fast-z-score": 6.736096792653741,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "We demonstrate that it is possible to generate all four Bell states in one single nonlinear crystal by using two pump beams with orthogonal polarizations and slightly different wavelengths, which are generated via second-harmonic generation (SHG) inside an optical parametric oscillator (OPO). The OPO consists of a periodically poled lithium niobate (PPLN) crystal as nonlinear medium and a concave mirror for cavity feedback. We show experimentally that this approach allows us to obtain high-visibility quantum interference between photons emitted at degenerate wavelength pairs across the entire PPLN acceptance bandwidth. This method can be used to simplify future experiments on continuous-variable entanglement distribution over large distances. \n \n Quantum information processing requires the ability to create and manipulate entangled states of light. In particular, the Bell state measurement plays a key role in many applications such as teleportation or quantum repeaters  1  . However, generating these highly nonclassical states is challenging because they require indistinguishable photon pairs  2  , which cannot be produced deterministically  3  .\nIn recent years, several approaches have been developed to overcome this problem  4  . One possibility is based on spontaneous parametric down-conversion (SPDC), where a pump beam creates correlated pairs of signal and idler photons  5  . By adjusting the relative phases of the pump fields  6  , it has become possible to produce any desired superposition of the four Bell states  7, 8  . Another option uses squeezed vacuum states  9  or displaced number states  10  instead of coherent laser pulses  11  . These methods allow for efficient generation of entangled states but usually suffer from low visibility due to imperfections  12  .",
        "watermark_text": "We suggest that it is easy to create all four Bell states in one single nonlinear crystal by using two pump beams with orthogonal polarizations and slightly different wavelengths , which are produced via second - harmonic production ( SHG ) inside an optical parametric oscillator ( OPO ) . The OPO consists of a periodically poled lithium niobate ( PPLN ) crystal as nonlinear material and a concave frame for cavity feedback .We suggest experimentally that this methodology allows us to obtain high - visibility particle interference between photons generated at degenerate intensity pairs across the entire PPLN acceptance bandwidth . This method can be used to simplify future research on continuous - variable entanglement flow over large distances .Quantum knowledge processing requires the ability to create and manipulate entangled states of light . In particular , the Bell state measurement plays a key importance in many applications such as teleportation or quantum repeaters 1 .However , constructing these strongly nonclassical states is problematic because they use indistinguishable photon pairs 2 , which cannot be made deterministically 3 . In recent years , various approaches have been formulated to overcome this situation 4 .One possibility is based on spontaneous parametric down - transfer ( SPDC ) , where a pump beam creates correlated pairs of signal and idler photons 5 . By adjusting the relative modes of the pump fields 6 , it has become able to produce any desired superposition of the four Bell states 7 , 8 .Another option uses squeezed vacuum states 9 or displaced number states 10 instead of coherent beam waves 11 . These methods provide for efficient production of entangled states but typically suffer from small brightness due to imperfections 12 .",
        "rewrite_text": "We propose that all four Bell states can be easily generated within a single nonlinear crystal by using two pump beams that have orthogonal polarizations and slightly varying wavelengths, produced through second-harmonic generation (SHG) in an optical parametric oscillator (OPO). The OPO features a periodically poled lithium niobate (PPLN) crystal as its nonlinear medium and a concave cavity frame for feedback. We experimentally demonstrate that this approach enables high-visibility particle interference among photons created at degenerate intensity pairs throughout the full acceptance bandwidth of the PPLN. This technique can facilitate future research on continuous-variable entanglement transmission over long distances. Quantum information processing necessitates the ability to generate and manipulate entangled light states. Notably, Bell state measurement is crucial for many applications, including teleportation and quantum repeaters. However, generating these strongly nonclassical states poses challenges because they rely on indistinguishable photon pairs, which cannot be produced deterministically. In recent years, several strategies have emerged to address this issue. One approach involves spontaneous parametric down-conversion (SPDC), where a pump beam produces correlated signal and idler photon pairs. By fine-tuning the relative modes of the pump fields, it is now possible to create any desired superposition of the four Bell states. Alternatively, squeezed vacuum states or displaced number states can be utilized instead of coherent beams. While these methods enable the efficient production of entangled states, they often struggle with low brightness due to various imperfections.",
        "ori-fast-z-score": 0.7302967433402214,
        "water-fast-z-score": 6.818181818181818,
        "rewrite-fast-z-score": -1.4855627054164149
    },
    {
        "original_text": "We present the first detection and characterization of polarized foreground emission at microwave frequencies using three years of data from WMAP. We find that this emission is dominated by synchrotron radiation, with an amplitude consistent with previous measurements in the literature. The polarization fraction for this component ranges between 0.5% to 2% across the sky. In addition we detect significant levels of polarized dust emission over much of the sky. This emission has a lower fractional polarization than previously reported but its total intensity is comparable or higher. Finally, we report on the detection of polarized thermal Sunyaev-Zeldovich effect associated with galaxy clusters. These results are important as they provide new information about Galactic foregrounds which will be used to extract cosmological signals such as primordial gravitational waves. \n \n Keywords: Cosmic microwave background anisotropies, Galaxy cluster, Synchrotron Radiation, Dust Emission, Thermal Sunyaev-Zeldovitch Effect",
        "watermark_text": "We present the first recognition and description of polarized foreground emission at microwave frequencies using three years of measurements from WMAP . We see that this emission is dominated by synchrotron emission , with an frequency consistent with previous detection in the literature .The polarization fraction for this component varies between 0 . 5 % to 2 % across the sky . In addition we find considerable rates of polarized dust pollution over much of the sky .This emission has a smaller fractional polarization than previously reported but its total activity is equal or greater . Finally , we publish on the observation of polarized thermal Sunyaev - Zeldovich effect related with star clusters .These data are important as they give novel knowledge about Galactic foregrounds which will be used to extract cosmological messages such as primordial gravitational waves . Keywords : Cosmic microwave background anisotropies , Galaxy cluster , Synchrotron Radiation , Dust Emission , Thermal Sunyaev - Zeldovitch Effect",
        "rewrite_text": "We introduce the initial recognition and characterization of polarized foreground emission at microwave frequencies, based on three years of data from WMAP. Our findings indicate that this emission is primarily driven by synchrotron radiation, with frequencies aligning with prior observations in the literature. The polarization fraction of this component ranges from 0.5% to 2% across various regions of the sky. Additionally, we observe notable levels of polarized dust contamination over a significant portion of the sky. Although this emission exhibits a lower fractional polarization than previously reported, its overall intensity is equal to or exceeds earlier findings. Lastly, we present our observations of the polarized thermal Sunyaev-Zeldovich effect associated with galaxy clusters. These data are crucial as they provide new insights into Galactic foregrounds, which will aid in extracting cosmological information, such as primordial gravitational waves. Keywords: Cosmic microwave background anisotropies, galaxy clusters, synchrotron radiation, dust emission, thermal Sunyaev-Zeldovich effect.",
        "ori-fast-z-score": -1.0327955589886444,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": -1.0681034923744679
    },
    {
        "original_text": "The thermal Casimir force is the quantum mechanical effect that arises when two objects are separated by vacuum, which causes them to attract each other due to zero-point fluctuations in their electromagnetic fields.  In this talk I will present some recent results for the thermal Casimir force between dielectrics as well as related problems such as the van der Waals interaction between polarizable atoms or molecules at finite temperature. The first part of my talk will be devoted to an overview of our work on the subject published recently in Physical Review Letters (PRL)  1  . This includes new exact expressions for the thermal Casimir energy density and pressure valid for arbitrary temperatures and dielectric functions. These formulas can also be used to calculate the leading order corrections to Lifshitz theory  2  , which has been widely applied to describe the Casimir force between real materials  3  .\nIn the second part of my talk I will discuss how these results have been extended to include retardation effects  4  . We find that retardation leads to additional contributions to both the energy density and pressure that depend strongly on the distance between the bodies. Finally, we will show how these results can be used to study the van der Waals interactions between polarizable atoms or molecules; i.e., systems where retardation plays no role but where the dispersion forces still give rise to non-trivial behavior  5  .  For example, we will demonstrate how one can use our formalism to obtain accurate predictions for the critical point of the liquid-vapor phase transition in water  6  .",
        "watermark_text": "The thermal Casimir force is the quantum mechanical effect that arises when two bodies are apart by vacuum , which makes them to attract each other owing to zero - point fluctuations in their electromagnetic fields . In this talk I will present some latest findings for the thermal Casimir force between dielectrics as well as related problems such as the van der Waals interaction between polarizable atoms or compounds at finite temperature .The first part of my talk will be devoted to an overview of our work on the subject released lately in Physical Review Letters ( PRL ) 1 . This contains new accurate expressions for the thermal Casimir energy density and tension applicable for arbitrary pressures and dielectric functions .These formulas can also be used to estimate the main order corrections to Lifshitz principle 2 , which has been widely applied to explain the Casimir force between real substances 3 . In the second part of my talk I will explore how these results have been extended to include retardation properties 4 .We see that retardation contributes to extra contributions to both the power concentration and tension that rely highly on the distance between the bodies . Finally , we will show how these results can be used to study the van der Waals relationships between polarizable atoms or compounds ; i . e . , systems where retardation plays no part but where the dispersion forces nevertheless give rise to non - simple properties 5 .For instance , we will prove how one can using our formalism to obtain precise predictions for the important moment of the liquid - fluid phase shift in water 6 .",
        "rewrite_text": "The thermal Casimir force is a quantum mechanical phenomenon that occurs when two bodies are separated by a vacuum, causing them to attract each other due to zero-point fluctuations in their electromagnetic fields. In this presentation, I will share some recent discoveries regarding the thermal Casimir force between dielectric materials, as well as related topics such as the van der Waals interactions among polarizable atoms or molecules at finite temperatures. The initial segment of my talk will provide an overview of our recent research published in Physical Review Letters (PRL), which introduces new accurate formulas for the thermal Casimir energy density and tension that are applicable under a variety of pressures and dielectric functions. These expressions also allow us to estimate the leading-order corrections to the Lifshitz principle, which has been extensively used to describe the Casimir force between real materials. In the latter part of my presentation, I will discuss how these findings have been adapted to account for retardation effects. Our analysis reveals that retardation introduces additional contributions to both power concentration and tension, which are significantly influenced by the distance between the bodies. Finally, we will illustrate how this framework can be employed to investigate the van der Waals interactions between polarizable atoms or compounds—systems where retardation may not be a factor, yet dispersion forces still lead to complex behaviors. For example, we will demonstrate how our formalism can yield precise predictions for the critical moment of the liquid-fluid phase transition in water.",
        "ori-fast-z-score": 0.4622501635210242,
        "water-fast-z-score": 7.799204203436179,
        "rewrite-fast-z-score": 0.3713906763541037
    },
    {
        "original_text": "The growth in stellar mass is driven by star formation, which occurs when dense gas collapses to form stars.  The rate at which this happens depends on how much gas there is available for collapse.   In turn, the amount of gas available for collapse depends on the balance between inflow (from larger scales) and outflow (from smaller scales).   This talk will discuss recent work that has been done using numerical simulations to understand these processes better. It will also present some new results on the role played by magnetic fields during star formation. Star formation takes place when dense gas clouds collapse under their own gravity into bound objects called protostars.  These protostars then grow through accretion until they reach the main sequence stage where nuclear fusion begins.  During this process, gravitational energy is released as radiation and kinetic energy, heating up surrounding material and driving powerful winds and jets.  As such, understanding what controls the rate at which stars are formed requires an understanding of how gas flows onto forming stars.  Gas can flow towards forming stars either directly or indirectly via filaments.  Directly flowing gas may be channeled along magnetic field lines while indirectly flowing gas may be transported along with turbulent motions.  Both types of flows have been observed around young stars but it remains unclear whether one type dominates over another.  Recent observations suggest that both direct and indirect flows play important roles depending on the environment.  For example, observations show that most low-mass stars accrete...",
        "watermark_text": "The growth in stellar mass is caused by star formation , which occurs when dense gas collapses to form stars . The rate at which this happens depends on how many gas there is provided for collapse .In turn , the quantity of gas provided for collapse depends on the balance between inflow ( from larger scales ) and outflow ( from lower scales ) . This discussion will explore latest work that has been performed using numerical simulations to comprehend these mechanisms better .It will also bring some fresh results on the part played by magnetic fields during star formation . Star formation occurs place when dense gas formations sink under their own gravitational into binding structures called protostars .These protostars then grow through accretion until they reach the main sequence phase where nuclear fusion occurs . During this process , gravity energy is released as radiation and kinetic power , heating up surrounding rock and generating powerful storms and jets .As such , studying what controls the pace at which stars are created requires an knowledge of how gas transfers onto forming stars . Gas can move towards forming stars either directly or indirectly via filaments .Directly flowing gas may be channeled along magnetic field lines while indirectly running gas may be transported along with turbulent movements . Both types of flows have been observed around old galaxies but it remains unsure whether one sort dominates over another .Recent measurements suggest that both direct and indirect flows act key roles depending on the surroundings . For instance , observations show that most lowest - weight objects accrete . . .",
        "rewrite_text": "The increase in stellar mass is driven by star formation, which occurs when dense gas collapses to create stars. The rate of this process is influenced by the amount of gas available for collapse, which is determined by the balance between inflow from larger scales and outflow from smaller scales. This discussion will delve into recent advancements made through numerical simulations to enhance our understanding of these mechanisms. Additionally, it will present new findings regarding the role of magnetic fields in star formation. Star formation takes place when dense gas clouds succumb to their own gravitational forces, forming binding structures known as protostars. These protostars continue to grow through accretion until they enter the main sequence phase, where nuclear fusion begins. During this phase, energy from gravity is released as radiation and kinetic energy, which heats the surrounding material and produces powerful storms and jets. Therefore, investigating what dictates the rate of star creation necessitates an understanding of how gas transfers to forming stars. Gas can flow toward developing stars in both direct and indirect manners, such as through filaments. Direct gas flow may be guided along magnetic field lines, while indirect flow occurs through turbulent movements. Although both types of flows have been observed around older galaxies, it remains unclear whether one type predominates. Recent findings indicate that both direct and indirect flows play significant roles, depending on their environment. For example, observations reveal that the least massive objects tend to undergo accretion.",
        "ori-fast-z-score": -0.7689218919450849,
        "water-fast-z-score": 8.172062695283987,
        "rewrite-fast-z-score": -0.9610744623271417
    },
    {
        "original_text": "The formation and characterization of bovine serum albumin (BSA) nanoparticles formed by electrostatic interaction between anionic graft copolymer poly(acrylic acid-co-methacrylamide) (PAM-g-PAA) and protein in aqueous solution are reported here. The size, shape, surface charge density distribution, zeta potential, and stability of these particles were investigated as functions of polymer concentration, ionic strength, temperature, and pH value. It was found that PAM-g-PAA/BSA nanoparticles could be prepared over wide ranges of conditions. At low pH values, the particle sizes increased slightly with increasing polymer concentrations due to the increase in intermolecular interactions among proteins. However, when the pH value reached 7.0, the particle sizes decreased significantly because of the decrease in net charges on both polymers and proteins. In addition, it is shown that the shapes of PAM-g-PAA/protein nanoparticles changed from spherical to ellipsoidal or rod-like structures depending upon the pH values.",
        "watermark_text": "The formation and description of bovine plasma albumin ( BSA ) nanoparticles formed by electrostatic interaction between anionic graft copolymer poly ( acrylic acid - co - methacrylamide ) ( PAM - g - PAA ) and protein in aqueous solution are published here . The size , shape , surface charge density distribution , zeta potential , and strength of these ions were researched as functions of polymer concentration , ionic intensity , temperature , and pH value .It was shown that PAM - h - PAA / BSA nanoparticles able be formed over broad ranges of conditions . At reduced pH levels , the particle sizes increased slightly with higher polymer levels related to the increase in intermolecular interactions among proteins .However , when the pH value reached 7 . 0 , the particle sizes reduced greatly because of the decrease in net charges on both polymers and proteins . In addition , it is demonstrated that the shapes of PAM - h - PAA / gene nanoparticles changed from spherical to ellipsoidal or rod - like structures depending upon the pH levels .",
        "rewrite_text": "This study presents the formation and characterization of bovine serum albumin (BSA) nanoparticles created through the electrostatic interaction between the anionic graft copolymer poly(acrylic acid-co-methacrylamide) (PAM-g-PAA) and protein in an aqueous environment. The research investigated various properties of these nanoparticles, including size, shape, surface charge density distribution, zeta potential, and ionic strength, as they relate to polymer concentration, ionic strength, temperature, and pH. The findings indicate that PAM-g-PAA/BSA nanoparticles can be successfully formed across a wide range of conditions. At lower pH levels, particle sizes exhibited a slight increase with higher polymer concentrations due to enhanced intermolecular interactions among proteins. Conversely, when the pH reached 7.0, particle sizes significantly decreased due to a reduction in net charges on both the polymers and proteins. Furthermore, it was observed that the shapes of PAM-g-PAA/gene nanoparticles transformed from spherical to ellipsoidal or rod-like structures in response to changes in pH levels.",
        "ori-fast-z-score": -1.0681034923744679,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": 0.3418817293789138
    },
    {
        "original_text": "We present an explicit, physically sound formulation for the dynamical Casimir effect (DCE) in terms of a time-dependent Schrödinger equation with a non-Hermitian effective potential that is derived directly from first principles and has no free parameters.  The resulting expression agrees exactly with previous results obtained by other authors using different methods but it also provides new insights into this fascinating quantum phenomenon. In particular we show how to calculate the energy spectrum of the system as well as its decay rates and lifetimes. We demonstrate our approach on two examples - one involving a single harmonic oscillator coupled to a thermal bath at finite temperature and another where the oscillators are replaced by fermions. Finally, we discuss possible extensions of these ideas beyond the standard model of particle physics. The dynamical Casimir effect (DCE), predicted more than twenty years ago  1-3 , refers to the generation of photons due to vacuum fluctuations when macroscopic objects move or change shape  4  . This intriguing prediction was confirmed experimentally only recently  5-7  , although there have been earlier suggestions  8  .\nThe original theoretical description of the DCE relied heavily on phenomenological models which were not always easy to interpret physically  9  . More recent attempts  10-12  used microscopic approaches based on non-relativistic QED  13-15  or relativistic field theory  16  . However, all such treatments involve some ad-hoc assumptions about the form of the interaction between the moving object(s) and the electromagnetic fields  17  . Here we propose a completely different method that avoids any such approximations and leads to a simple, transparent physical picture of the process. Our starting point is the exact Heisenberg-Langevin equations describing the dynamics of the electric field operatorsÊ(r, t). These can be written in the compact form:",
        "watermark_text": "We present an explicit , physically sound formulation for the dynamical Casimir effect ( DCE ) in terms of a time - dependent Schrödinger equation with a non - Hermitian effective potential that is developed directly from initial principles and has no free parameters . The resulting expression agrees exactly with previous findings obtained by other researchers using separate methods but it also provides new information into this fascinating quantum concept .In particular we prove how to estimate the power spectrum of the system as well as its degradation times and lifetimes . We test our approach on two examples - one utilizing a single harmonic oscillator coupled to a heat shower at finite temperature and another where the oscillators are replaced by fermions .Finally , we investigate possible extensions of these ideas beyond the standard theory of particle theory . The dynamical Casimir effect ( DCE ) , predicted more than twenty years previously 1 - 3 , relates to the generation of photons due to vacuum fluctuations when macroscopic objects moving or change form 4 .This unusual prediction was confirmed experimentally only recently 5 - 7 , although there have been earlier suggestions 8 . The original theoretical formulation of the DCE depended heavily on phenomenological models which were not always easier to predict physically 9 .More current approaches 10 - 12 used microscopic techniques based on non - relativistic QED 13 - 15 or relativistic field principle 16 . However , all such treatments include some ad - hoc assumptions about the form of the interaction between the moved object ( s ) and the electromagnetic forces 17 .Here we attempt a completely different method that avoids any such approximations and leads to a simple , straightforward mechanical picture of the process . Our starting point is the exact Heisenberg - Langevin coefficients governing the dynamics of the electric field [UNK] ( r , t ) .These can be written in the compact form :",
        "rewrite_text": "We offer a clear and physically robust formulation of the dynamical Casimir effect (DCE) by utilizing a time-dependent Schrödinger equation that incorporates a non-Hermitian effective potential, derived directly from fundamental principles and devoid of free parameters. Our formulation aligns perfectly with previous results achieved by researchers employing varied methodologies while also contributing novel insights into this captivating quantum phenomenon. Notably, we establish a method to estimate the system's power spectrum, as well as its degradation times and lifetimes. We validate our approach through two examples: one involving a single harmonic oscillator in contact with a heat bath at finite temperature, and another in which the oscillators are substituted with fermions. Furthermore, we explore potential extensions of our framework beyond the conventional particle theory. The dynamical Casimir effect, initially predicted over two decades ago, pertains to the generation of photons stemming from vacuum fluctuations as macroscopic objects move or alter shape. Although this intriguing prediction was only recently confirmed through experiments, it had been suggested earlier. The original theoretical treatment of the DCE relied heavily on phenomenological models that were not always straightforward to interpret. More recent methodologies have employed microscopic techniques grounded in non-relativistic quantum electrodynamics or relativistic field principles. However, these approaches often include arbitrary assumptions regarding the nature of the interaction between the moving objects and electromagnetic forces. Our approach deviates fundamentally from these methods by eliminating such assumptions and presenting a clear, intuitive mechanical representation of the process. We begin with the exact Heisenberg-Langevin coefficients that govern the dynamics of the electric field, which can be concisely represented as:",
        "ori-fast-z-score": 0.43033148291193524,
        "water-fast-z-score": 7.150554858373488,
        "rewrite-fast-z-score": 1.153563462240948
    },
    {
        "original_text": "We report on the temporal evolution of step fluctuations under electromigration conditions in Cu(111) and Ag(111). The experiments were performed by scanning tunneling microscopy (STM), which allows for direct observation of atomic-scale processes at surfaces. We find that, during current-induced mass transport along steps, the step edges fluctuate strongly with time. These fluctuations are characterized by an exponential growth followed by saturation after about 1 hour. In addition to this general behavior we observe different types of fluctuations depending on the applied voltage and temperature. For low voltages and temperatures below 300 K, the fluctuations show a sawtooth-like shape indicating periodic changes between two states. At higher voltages or temperatures above 400 K, the fluctuations become more irregular but still exhibit some periodicity. Finally, at high voltages and temperatures around 500 K, no regular pattern is observed anymore. Our results suggest that these fluctuations can be explained as a result of competition between diffusion and drift currents.",
        "watermark_text": "We report on the temporal evolution of step fluctuations under electromigration conditions in Cu ( 111 ) and Ag ( 111 ) . The studies were performed by scanning tunneling microscopy ( STM ) , which allows for detailed observation of atomic - scale processes at surfaces .We see that , during current - mediated mass transport along stairs , the step paths fluctuate strongly with time . These fluctuations are marked by an exponential growth followed by saturation after about 1 hour .In addition to this special phenomenon we study various types of fluctuations based on the introduced voltage and heat . For low voltages and conditions below 300 K , the fluctuations show a sawtooth - like shape suggesting periodic shifts between two states .At higher voltages or temperatures above 400 K , the fluctuations get more erratic but still exhibit some periodicity . Finally , at high voltages and temperatures around 500 K , no normal behavior is observed anymore .Our results propose that these fluctuations can be described as a outcome of competition between diffusion and drift currents .",
        "rewrite_text": "We present our findings on the time-dependent behavior of step fluctuations under electromigration conditions in Cu (111) and Ag (111). These investigations were conducted using scanning tunneling microscopy (STM), which provides a detailed view of atomic-scale processes occurring at surfaces. Our observations reveal that, during current-driven mass transport along the steps, the step paths exhibit significant temporal fluctuations. Notably, these fluctuations display an exponential increase followed by a saturation phase after approximately one hour. Additionally, we explore various types of fluctuations influenced by the applied voltage and temperature. At lower voltages and temperatures below 300 K, the fluctuations take on a sawtooth-like pattern, indicating periodic transitions between two states. In contrast, at elevated voltages or temperatures exceeding 400 K, the fluctuations become more irregular, though some periodic behavior persists. Ultimately, at high voltages and around 500 K, conventional behavior ceases to be observed. Our findings suggest that these fluctuations can be understood as a result of the interplay between diffusion and drift currents.",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 5.986302773458956,
        "rewrite-fast-z-score": -1.1785113019775793
    },
    {
        "original_text": "The problem of finding the energy levels of an electron bound to a nucleus is one of the most important problems in physics, which has been studied for many years. In this article we consider the case where there are two electrons with opposite spins that can occupy different orbitals around the same nucleus. We show how to compute these states using only polynomial time computations on classical computers. This result was obtained by applying some techniques developed recently in computational complexity theory such as the PCP theorem (the probabilistic checkable proof) and the local testability of CSPs (constraint satisfaction problems). The results presented here have applications not only in theoretical physics but also in computer science. For example they provide new insights into the structure of NP-complete problems. Quantum mechanical systems play an essential role in modern physics. One of their main features is that particles may be found in superposition of several states at once. A famous example is Schrödinger s cat experiment  1  . Another feature is entanglement  2  , i.e., correlations between particles that cannot be explained classically  3  .\nIn this work we study the following problem: given a system consisting of N spin-1/2 particles, what is the ground state? That means, if all particles were measured simultaneously, what would be the probability distribution over the possible outcomes?\nWe will focus our attention on the simplest non-trivial case: two spin-½ particles occupying different orbitals around the nucleus  4  . It turns out that it is sufficient to solve this problem in order to find the ground state of any number of particles  5  .",
        "watermark_text": "The question of finding the power concentrations of an electron bound to a nucleus is one of the most important problems in physics , which has been studied for numerous years . In this article we define the case where there are two electrons with opposite spins that can occupy separate orbitals around the same nucleus .We see how to compute these states using only polynomial period computations on classical computers . This result was obtained by using some techniques established recently in computational complexity analysis such as the PCP conjecture ( the probabilistic checkable proof ) and the local testability of CSPs ( constraint satisfaction issues ) .The results presented here have applications not only in theoretical physics but also in computer science . For instance they give novel knowledge into the formation of NP - full problems .Quantum mechanical models play an essential part in modern physics . One of their major characteristics is that particles may be found in superposition of several states at once .A popular example is Schrödinger s cat experiment 1 . Another phenomenon is entanglement 2 , i . e . , correlations between particles that cannot be described classically 3 .In this research we study the following puzzle : given a system consisting of N spin - 1 / 2 particles , what is the ground state ? That implies , if all particles were measured simultaneously , what would be the probability distribution over the possible events ?We will focus our focus on the trivial non - trivial case : two spin - ½ particles occupying various orbitals around the atom 4 . It turns out that it is enough to solve this situation in order to find the ground state of any number of particles 5 .",
        "rewrite_text": "The quest to determine the power distributions of electrons bound to a nucleus remains one of the most crucial challenges in physics, studied extensively over many years. In this article, we examine a scenario involving two electrons with opposite spins that can occupy distinct orbitals around the same nucleus. We demonstrate how to calculate these states using polynomial-time computations on classical computers. This achievement is based on recent advancements in computational complexity analysis, including techniques such as the PCP conjecture (probabilistic checkable proof) and the local testability of constraint satisfaction problems (CSPs). The findings presented in this work have implications not only for theoretical physics but also for computer science, providing new insights into the nature of NP-complete problems. Quantum mechanical models are fundamental to modern physics, characterized by the ability of particles to exist in superpositions of multiple states simultaneously. A well-known illustration of this is Schrödinger's cat experiment. Another key phenomenon is entanglement, which refers to correlations between particles that cannot be classically explained. In this study, we tackle the following question: for a system comprised of N spin-1/2 particles, what is the ground state? This inquiry leads us to consider the probability distribution of events if all particles were measured at once. Our focus will be on the seemingly trivial yet non-trivial case of two spin-1/2 particles in various orbitals around an atom. Remarkably, solving this case is sufficient to determine the ground state for any number of particles.",
        "ori-fast-z-score": -0.7242859683401482,
        "water-fast-z-score": 5.7272727272727275,
        "rewrite-fast-z-score": -0.3651483716701107
    },
    {
        "original_text": "We report on the detection and analysis of radio emission associated with an impulsive solar flare that occurred in active region NOAA 10486 (SOL2010-07-20T17:48) at 17:48 UT on July 20, 2010 using the Nançay Decameter Array (NDA). The event was accompanied by a fast halo coronal mass ejection (CME), which reached Earth at 18:20 UT on July 21. We find that the radio source is located near the center of the CME front as seen in white light images taken by STEREO-Ahead/EUVI 195 Å . The radio flux density shows rapid evolution during the first hour after the onset of the flare, followed by gradual decay over several hours. The radio spectrum has a power-law shape between 1 MHz to 5 GHz. The spectral index decreases rapidly below 100 MHz but remains nearly constant above this frequency.",
        "watermark_text": "We report on the detection and assessment of radio emission associated with an impulsive solar flare that occurred in active region NOAA 10486 ( SOL2010 - 07 - 20T17 : 48 ) at 17 : 48 UT on July 20 , 2010 using the Nançay Decameter Array ( NDA ) . The event was accompanied by a rapid halo coronal mass ejection ( CME ) , which reached Earth at 18 : 20 UT on July 21 .We see that the radio source is situated near the center of the CME front as shown in white light pictures taken by STEREO - Ahead / EUVI 195 Å . The radio flux concentration displays rapid change during the first hour after the outbreak of the flare , followed by rapid fading over numerous weeks .The radio signal has a power - law shape between 1 MHz to 5 GHz . The spectral index drops rapidly below 100 MHz but maintains fairly constant above this signal .",
        "rewrite_text": "We present our findings on the detection and analysis of radio emissions linked to an impulsive solar flare that occurred in active region NOAA 10486 (SOL2010 - 07 - 20T17:48) at 17:48 UT on July 20, 2010, utilizing the Nançay Decameter Array (NDA). This event was accompanied by a fast halo coronal mass ejection (CME), which reached Earth at 18:20 UT on July 21. Our observations indicate that the radio source is located near the center of the CME front, as illustrated by white light images captured by STEREO-Ahead/EUVI at 195 Å. The radio flux shows rapid variations during the first hour following the flare's onset, followed by a steady decline over several weeks. The radio signal exhibits a power-law distribution across frequencies from 1 MHz to 5 GHz, with a spectral index that decreases sharply below 100 MHz, while remaining relatively stable at higher frequencies.",
        "ori-fast-z-score": -1.1920791213585393,
        "water-fast-z-score": 3.841143835488627,
        "rewrite-fast-z-score": 0.508000508000762
    },
    {
        "original_text": "We study statistics of conductance oscillations in open quantum dots with electron-phonon interaction and dephasing time saturation at high temperatures. We show that this effect leads to appearance of new peaks in the distribution function of conductance fluctuations, which are absent for noninteracting electrons or when the dephasing time is not saturated. The positions of these peaks depend on temperature and dot size. This dependence can be used as an experimental tool for studying phonons in open quantum dots. \n \n Introduction \n \n In recent years there has been growing interest in transport through mesoscopic systems such as semiconductor nanowires  1  , carbon nanotubes  2  , graphene  3  . These structures have unique properties due to their small dimensions (of order 10 nm)  4  . For example, they exhibit ballistic  5  and coherent  6  transport regimes  7, 8  .\n \nIn particular, it was shown experimentally  9  that the amplitude of conductance fluctuations in open quantum dots depends strongly on temperature T and dot size L. It decreases rapidly with increasing T and decreasing L  10  . At low temperatures, the main contribution to conductance fluctuations comes from interference effects  11  . However, at higher temperatures, thermal averaging destroys phase coherence between different paths  12  . As a result, the amplitude of conductance fluctuation decreases exponentially with temperature  13  :",
        "watermark_text": "We research data of conductance oscillations in open quantum dots with electron - phonon interaction and dephasing time saturation at high temperatures . We see that this effect results to appearance of new peaks in the distribution function of conductance fluctuations , which are missing for noninteracting particles or when the dephasing time is not saturated .The places of these peaks vary on temperature and dot size . This dependence can be used as an research technique for studying phonons in open quantum dots .Introduction In recent years there has been growing interest in transport through mesoscopic systems such as semiconductor nanowires 1 , carbon nanotubes 2 , graphene 3 . These structures have special characteristics owing to their tiny dimensions ( of order 10 nm ) 4 .For instance , they demonstrate ballistic 5 and coherent 6 transport regimes 7 , 8 . In particular , it was shown experimentally 9 that the frequency of conductance fluctuations in open quantum dots relies highly on pressure T and dot width L . It drops rapidly with rising T and decreasing L 10 .At small temperatures , the main contribution to conductance fluctuations depends from interference effects 11 . However , at higher temperatures , thermal averaging destroys phase coherence between various paths 12 .As a result , the frequency of conductance fluctuation decreases exponentially with temperature 13 :",
        "rewrite_text": "We investigate the conductance oscillations in open quantum dots, focusing on the effects of electron-phonon interactions and the saturation of dephasing time at high temperatures. Our findings indicate that these interactions lead to the emergence of new peaks in the distribution function of conductance fluctuations—features that are absent in the case of noninteracting particles or when dephasing time is not saturated. The positions of these peaks are dependent on temperature and dot size, offering a potential technique for probing phonon behavior in open quantum dots.\n\n**Introduction**  \nIn recent years, there has been an increased interest in transport phenomena within mesoscopic systems, including semiconductor nanowires, carbon nanotubes, and graphene. These materials exhibit unique properties due to their small dimensions (on the order of 10 nm). For example, they display ballistic and coherent transport regimes. Experimental evidence has demonstrated that the frequency of conductance fluctuations in open quantum dots is significantly affected by temperature and dot width; it declines rapidly as temperature increases and dot width decreases. At lower temperatures, interference effects primarily contribute to conductance fluctuations. However, as the temperature rises, thermal averaging disrupts phase coherence among various pathways, causing the frequency of conductance fluctuations to decrease exponentially with temperature.",
        "ori-fast-z-score": -0.9434563530497265,
        "water-fast-z-score": 3.8786538958710977,
        "rewrite-fast-z-score": -0.10153461651336192
    },
    {
        "original_text": "We present measurements of galaxy clustering on scales between 1 and 100 h-1 Mpc, using data from the Cosmic Evolution Survey (COSMOS). We use photometric redshifts to select samples with different luminosities at z = 0.5-0.8. The correlation function is measured for each sample by counting pairs as a function of separation angle. We find that the amplitude of the correlation function increases strongly with luminosity over this range. This trend can be explained if we assume that more luminous galaxies are biased towards higher density regions than less luminous ones. In addition, we measure the evolution of the bias factor with redshift. Our results show that the bias factor evolves rapidly out to z ~1 but then flattens off beyond this point. These findings suggest that the growth rate of dark matter haloes decreases significantly after z ~1. Finally, we compare our results with those obtained from numerical simulations.",
        "watermark_text": "We create measurements of galaxy clustering on scales between 1 and 100 h - 1 Mpc , using data from the Cosmic Evolution Survey ( COSMOS ) . We use photometric redshifts to select samples with various luminosities at z = 0 . 5 - 0 . 8 .The interaction function is measured for each specimen by counting pairs as a function of separation angle . We see that the frequency of the correlation function increases strongly with luminosity over this spectrum .This trend can be described if we suppose that more luminous clusters are biased towards higher velocity centers than less luminous ones . In addition , we measure the evolution of the bias coefficient with redshift .Our results show that the bias coefficient evolves fast out to z ~ 1 but then flattens off beyond this point . These conclusions show that the development frequency of dark matter haloes drops considerably after z ~ 1 .Finally , we compare our findings with those achieved from numerical simulations .",
        "rewrite_text": "We measure galaxy clustering over scales ranging from 1 to 100 h - 1 Mpc, utilizing data from the Cosmic Evolution Survey (COSMOS). By employing photometric redshifts, we select samples with varying luminosities at redshifts of z = 0.5 - 0.8. The interaction function is calculated for each sample by counting pairs based on their separation angle. Our analysis reveals a significant increase in the correlation function's frequency with luminosity across this range. This pattern suggests that more luminous clusters are biased towards higher-velocity centers compared to their less luminous counterparts. Additionally, we assess how the bias coefficient evolves with redshift. Our findings indicate that the bias coefficient changes rapidly up to z ~ 1, after which it plateau. These results imply a notable decline in the formation rate of dark matter haloes beyond z ~ 1. Lastly, we juxtapose our observations with results obtained from numerical simulations.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": -0.254000254000381
    },
    {
        "original_text": "We present an analysis of the time variation in the supernova neutrino signal observed by KamLAND, based on the results obtained with the latest version (v5) of the numerical simulation code for core-collapse supernovae developed at Garching.  We find that the simulated time variations are consistent with those observed by KamLAND within statistical errors when we take into account the uncertainties associated with the nuclear reaction rates used to calculate the energy generation rate inside the supernova envelope as well as the uncertainty in the initial conditions assumed for the simulations. The agreement between theory and experiment is improved if we assume that the central density of the progenitor star was higher than previously thought. This result suggests that future observations of gravitational waves emitted during the collapse phase may be able to provide information about the structure of the progenitor stars prior to their explosion. In addition, our study shows that the effect of convection plays only a minor role in determining the temporal behavior of the neutrino fluxes detected by KamLAND.",
        "watermark_text": "We present an assessment of the period change in the supernova neutrino wave observed by KamLAND , relying on the results derived with the latest version ( v5 ) of the numerical model code for core - collapse supernovae published at Garching . We see that the simulated time variations are compatible with those observed by KamLAND within statistical errors when we took into consideration the uncertainties involved with the atomic reaction rates taken to estimate the power generation rate inside the supernova envelope as also as the instability in the early conditions assumed for the simulations .The agreement between theoretical and observation is enhanced if we assume that the main concentration of the progenitor star was greater than previously thought . This result suggests that future discoveries of gravitational waves emitted during the failure phase may be possible to provide details about the composition of the progenitor stars prior to their explosion .In addition , our research shows that the impact of convection plays only a minor importance in shaping the temporal activity of the neutrino fluxes observed by KamLAND .",
        "rewrite_text": "We provide an evaluation of the changes in the supernova neutrino wave detected by KamLAND, utilizing data obtained from the latest version (v5) of the numerical model code for core-collapse supernovae developed in Garching. Our analysis indicates that the simulated time variations align with those recorded by KamLAND within the bounds of statistical error, especially when we account for uncertainties associated with atomic reaction rates used to estimate the power generation in the supernova envelope, as well as the variability in the initial conditions set in the simulations. The concordance between theoretical predictions and observed data improves if we assume that the initial mass concentration of the progenitor star was higher than previously estimated. This finding implies that future detections of gravitational waves emitted during core collapse could provide insights into the composition of progenitor stars before their explosions. Additionally, our study indicates that convection has a minimal effect on the temporal behavior of the neutrino fluxes observed by KamLAND.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 7.288480720939871,
        "rewrite-fast-z-score": -0.23249527748763857
    },
    {
        "original_text": "In this work, we introduce the higher order Schwarzian derivative (HOSD) to study chaotic behavior in dynamical systems. The HOSD is defined as the second-order differential operator with respect to time variable t acting on the first-order derivatives of the state variables x(t). We show that the HOSD can be used to construct new invariant sufficient conditions of chaos by using its properties such as non-negativity and monotonicity under some suitable assumptions. In addition, it also provides an alternative way to investigate the existence of periodic orbits in nonlinear autonomous systems. Finally, numerical examples are given to illustrate our results. Keywords: Dynamical systems; Chaos; Periodic orbit; Nonlinearity; Higher order Schwarzian derivative. 1 Introduction Let us consider the following nonautonomous ordinary differential equations (ODEs)\nx = f(t; x; u), where f: R × Rn × Rm → Rn, (1.1) which describes many physical phenomena arising in engineering fields  1  . Here, t ∈  0, T  denotes time; x ∈ Rn represents the state vector; and u ∈ Rm stands for control input or parameter vector. It should be noted that the function f may depend explicitly on both time t and control parameters u. For example, if one considers the motion of a particle moving along a straight line at constant speed v, then the position of the particle at any instant of time t is described by the equation x = vt + x0, where x0 is the initial position of the particle  2  .\nThe main goal of this article is to present a novel approach based on the higher order Schwarzian",
        "watermark_text": "In this research , we introduce the higher order Schwarzian derivative ( HOSD ) to study chaotic dynamics in dynamical systems . The HOSD is characterized as the second - order differential function with regard to time variable t acting on the first - order derivatives of the state components x ( t ) .We see that the HOSD can be used to build new invariant sufficient conditions of chaos by using its properties such as non - negativity and monotonicity under some suitable assumptions . In addition , it also provides an additional means to probe the existence of periodic orbits in nonlinear autonomous regions .Finally , numerical examples are given to illustrate our findings . Keywords : Dynamical systems ; Chaos ; Periodic orbit ; Nonlinearity ; Higher degree Schwarzian function .1 Introduction Let us consider the following nonautonomous ordinary differential equations ( ODEs ) x = f ( t ; x ; u ) , where f : R × Rn × Rm → Rn , ( 1 . 1 ) which explains many mechanical phenomena arising in engineering fields 1 . Here , t ∈ 0 , T denotes speed ; x ∈ Rn represents the state vector ; and u ∈ Rm stood for control input or parameter vector .It should be mentioned that the function f may depend explicitly on both time t and control values u . For instance , if one looks the movement of a particle moving along a straight line at constant speed v , then the placement of the particle at any instant of time t is characterized by the equation x = vt + x0 , where x0 is the first orientation of the particle 2 .The main goal of this page is to provide a novel method based on the higher order Schwarzian",
        "rewrite_text": "In this study, we present the higher-order Schwarzian derivative (HOSD) as a tool for investigating chaotic dynamics within dynamical systems. The HOSD is defined as a second-order differential function with respect to the time variable \\( t \\), applied to the first-order derivatives of the state components \\( x(t) \\). We demonstrate that the HOSD can be utilized to establish new invariant sufficient conditions for chaos, leveraging its properties such as non-negativity and monotonicity under certain appropriate assumptions. Additionally, it serves as an alternative approach for examining the existence of periodic orbits in nonlinear autonomous regions. Finally, we provide numerical examples to illustrate our findings. \n\n**Keywords**: Dynamical systems; Chaos; Periodic orbits; Nonlinearity; Higher-order Schwarzian function.\n\n**1 Introduction** \nWe consider the following nonautonomous ordinary differential equations (ODEs): \n\\[ x = f(t; x; u), \\]\nwhere \\( f: \\mathbb{R} \\times \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^n \\). This framework describes various mechanical phenomena encountered in engineering fields. Here, \\( t \\in [0, T] \\) denotes time; \\( x \\in \\mathbb{R}^n \\) signifies the state vector; and \\( u \\in \\mathbb{R}^m \\) represents the control input or parameter vector. It is important to note that the function \\( f \\) may explicitly depend on both the time \\( t \\) and the control values \\( u \\). For example, consider a particle moving along a straight line at a constant speed \\( v \\); the position of the particle at any moment \\( t \\) is given by the equation \\( x = vt + x_0 \\), where \\( x_0 \\) denotes the initial position of the particle. The primary objective of this section is to introduce a novel methodology based on the higher-order Schwarzian derivative.",
        "ori-fast-z-score": 0.08873565094161139,
        "water-fast-z-score": 5.833630944789017,
        "rewrite-fast-z-score": 0.6163156344279367
    },
    {
        "original_text": "We study the effect of confinement on the structure and dynamics of a simple model system, namely an ensemble of N identical particles interacting via repulsive pair potentials confined in a volume V by two parallel impenetrable walls at distance L apart.  We use Monte Carlo simulations to calculate the density profiles for different values of the wall separation L and particle number N . The results show that the density profile is not affected significantly when increasing the wall separation beyond a certain value which depends on both the temperature T and the particle number N .  In addition we find that the self-diffusion coefficient D decreases with decreasing wall separation but increases again if one further reduces the wall separation below some critical value depending on the temperature T .\nThe observed behavior can be explained within the framework of mode-coupling theory (MCT) using a generalized version of MCT developed recently by us  Physica A, vol. 315, no. 1, pp. 39-48, (2003), Physica A, vol. 320, no. 3, pp. 633-646, (2004) .",
        "watermark_text": "We research the impact of confinement on the composition and dynamics of a simple model network , namely an ensemble of N identical particles interacting via repulsive pair potentials restricted in a volume V by two connected impenetrable walls at distance L apart . We use Monte Carlo simulations to estimate the density characteristics for different values of the wall separation L and electron number N .The results show that the density profile is not affected greatly when increasing the wall separation beyond a certain value which depends on both the temperature T and the particle number N . In addition we find that the self - diffusion coefficient D reduces with decreasing wall separation but grows again if one further decreases the wall separation below some significant value depending on the temperature T .The observed behavior can be described within the framework of mode - correlation theory ( MCT ) using a generalized version of MCT created recently by us Physica A , vol . 315 , no .1 , pp . 39 - 48 , ( 2003 ) , Physica A , vol .320 , no . 3 , pp .633-646, (2004) .",
        "rewrite_text": "We investigate how confinement affects the composition and dynamics of a simple model network, specifically an ensemble of N identical particles that interact through repulsive pair potentials within a volume V confined by two adjacent impenetrable walls separated by a distance L. Utilizing Monte Carlo simulations, we assess the density characteristics for various wall separations L and particle numbers N. Our findings indicate that the density profile remains relatively stable when the wall separation exceeds a certain threshold, which is influenced by both the temperature T and the particle number N. Furthermore, we observe that the self-diffusion coefficient D decreases as the wall separation reduces, but exhibits an increase if the wall separation is decreased further below a critical value determined by the temperature T. This behavior can be explained within the framework of mode-coupling theory (MCT), using a generalized form of MCT that we recently developed, as detailed in Physica A, vol. 315, no. 1, pp. 39-48 (2003) and Physica A, vol. 320, no. 3, pp. 633-646 (2004).",
        "ori-fast-z-score": 0.45291081365783825,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": 2.6210932585716726
    },
    {
        "original_text": "We present the discovery, photometric and spectroscopic observations of SN 2005hj (PTF10ygu), an apparently normal-bright Type Ia supernova discovered by PTF in late March 2005 at z = 0.084. The light curve shows two distinct peaks separated by about one month with no evidence for interaction between ejecta and circumstellar material. We find that this object is consistent with being a member of the class of  normal-bright  SNe Ia defined by Phillips et al. (1999) but has a higher peak luminosity than most members of this class. Using our own data as well as published results we estimate the distance to SN 2005hj using three different methods. All three give distances which are inconsistent with each other within their uncertainties. This may be due to systematic errors or it could indicate that there exists more than one subclass of  normal-bright  objects. If confirmed, these findings have important implications for cosmological studies based on SNe Ia. \n \n Keywords: Supernovae",
        "watermark_text": "We present the discovery , photometric and spectroscopic observations of SN 2005hj ( PTF10ygu ) , an apparently normal - bright Type Ia supernova discovered by PTF in late March 2005 at z = 0 . 084 . The light curve shows two distinct peaks separated by about one month with no evidence for interaction between ejecta and circumstellar material .We see that this body is compatible with being a member of the class of normal - faint SNe Ia defined by Phillips et al . ( 1999 ) but has a higher peak luminosity than most members of this class .Using our own observations as well as published results we estimate the distance to SN 2005hj using three different methods . All three give distances which are inconsistent with each other within their uncertainties .This might be due to systematic errors or it could indicate that there exists more than one subclass of normal - faint objects . If confirmed , these results have important implications for cosmological researchers using on SNe Ia .Keywords: Supernovae",
        "rewrite_text": "We report the discovery and observations—both photometric and spectroscopic—of SN 2005hj (PTF10ygu), a seemingly typical bright Type Ia supernova identified by the Palomar Transient Factory (PTF) in late March 2005 at a redshift of z = 0.084. The light curve reveals two distinct peaks spaced approximately one month apart, with no signs of interaction between the ejecta and surrounding circumstellar material. Our analysis suggests that this supernova aligns with the category of normal-faint SNe Ia as characterized by Phillips et al. (1999); however, it exhibits a higher peak luminosity than most others in this category. Utilizing our observational data alongside previously published findings, we estimate the distance to SN 2005hj using three different methodologies. Unfortunately, the resulting distances are inconsistent with one another within their respective uncertainties. This discrepancy may stem from systematic errors or could suggest the presence of multiple subclasses of normal-faint objects. If validated, these findings could have significant implications for cosmologists studying Type Ia supernovae. Keywords: Supernovae.",
        "ori-fast-z-score": 1.2909944487358056,
        "water-fast-z-score": 3.9691432779197755,
        "rewrite-fast-z-score": 0.12216944435630522
    },
    {
        "original_text": "We present multimodal nested sampling (MNS), a novel algorithm that is able to efficiently explore the posterior distribution in high-dimensional parameter spaces, such as those encountered when fitting complex models to observational data sets. MNS combines ideas from simulated annealing with importance sampling techniques to find the global maximum likelihood solution within a given tolerance level. We demonstrate how this method can be used on real-world problems by applying it to two different astrophysics applications: modelling the observed fluxes of gamma-ray bursts using a time-dependent model; and determining the parameters of a binary black hole merger event detected by gravitational waves. In both cases we show that our new approach outperforms existing Markov chain Monte Carlo algorithms. The code implementing these examples will be made publicly available at https://github.com/mns-method/mns-method/tree/master/examples. Multimodal nested sampling (M NS) is a novel algorithm that is capable of exploring the posterior distribution in high dimensional parameter spaces, such as are found when fitting complex models to large observational datasets. It combines ideas from simulated annealling with importance sampling techniques to locate the global maximum likelihood solution to any problem within some specified tolerance. This talk describes the basic principles behind M NS and demonstrates its application to two astrophysics problems: modelling the observed light curves of gamma ray bursts; and determining the physical properties of a binary black hole system inferred from gravitational wave observations.",
        "watermark_text": "We create multimodal nested sampling ( MNS ) , a new algorithm that is able to easily examine the posterior distribution in high - dimensional parameter sets , such as those experienced when fitting large models to observational data sets . MNS mixes ideas from simulated annealing with importance sampling methods to find the global maximum likelihood solution within a given tolerance level .We suggest how this algorithm can be used on real - global issues by using it to two different astrophysics applications : modelling the emitted fluxes of gamma - ray clusters using a time - dependent model ; and determining the variables of a binary white hole merger event detected by gravitational waves . In both cases we prove that our new approach outperforms previous Markov chain Monte Carlo algorithms .The software implementing these examples will be made fully available at https : / / github . com / mns - method / mns - method / tree / master / examples . Multimodal nested filtering ( M NS ) is a novel algorithm that is capable of analyzing the posterior distribution in high dimensional parameter sets , such as are found when fitting large models to large observational datasets .It combines insights from simulated annealling with importance sampling methods to locate the global maximum likelihood solution to any question within some specified tolerance . This discussion describes the fundamental principles behind M NS and demonstrates its use to two astrophysics issues : modelling the emitted light curves of gamma radiation bursts ; and determining the physical properties of a binary white hole system inferred from gravitational wave surveys .",
        "rewrite_text": "We present multimodal nested sampling (MNS), an innovative algorithm designed to effectively analyze the posterior distribution in high-dimensional parameter spaces, particularly relevant when fitting complex models to extensive observational datasets. MNS integrates concepts from simulated annealing and importance sampling techniques to identify the global maximum likelihood solution within a defined tolerance. We illustrate the applicability of this algorithm to pressing global challenges by demonstrating its use in two astrophysical contexts: modeling the emitted fluxes of gamma-ray bursts with a time-dependent approach, and extracting the variables surrounding a binary white hole merger event detected through gravitational waves. In both scenarios, we demonstrate that our method surpasses traditional Markov chain Monte Carlo algorithms. The software implementing these applications will be openly accessible at https://github.com/mns-method/mns-method/tree/master/examples. Multimodal nested filtering (MNS) is a cutting-edge algorithm capable of exploring the posterior distribution in high-dimensional parameter sets, which is common when dealing with large observational datasets. It fuses principles from simulated annealing with importance sampling methods to pinpoint the global maximum likelihood solution for a given query, adhering to specific tolerance levels. This article outlines the core concepts of MNS and showcases its effectiveness in two astrophysical challenges: the modeling of light curves from gamma-ray bursts and the assessment of physical characteristics of a binary white hole system as suggested by gravitational wave observations.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 8.348430768491543,
        "rewrite-fast-z-score": 2.3162640965743444
    },
    {
        "original_text": "We report on the detection of an extremely rare supersoft X-ray outburst event in the transient source 1E0102. 2-7219 (=GX 354-0) with the Suzaku satellite, which was triggered by a large increase in its hard Xray flux. The duration and peak luminosity are estimated to be about 100 s and 2×1036 erg/s at 6 kpc distance, respectively. This is one of only two such events ever detected for this object. We discuss possible origins of these events based on their observed properties. \n \n Keywords: Supernova remnant, Soft gamma-ray repeater, Transient source, Supersoft X-ray emission, Hard X-ray bursts \n \n \n \n 1 Introduction \n \n In recent years, several new classes of transients have been discovered through systematic searches using satellites like RXTE/ASM or Swift/BAT. These include soft-gamma repeaters (SGRs; e.g., Hurley et al. 2005), anomalous X-ray pulsars (AXPs; e.g., Kaspi & Beloborodov 2017) , and magnetar candidates (e.g., Rea et al. 2012) . Among them, SGRs show repeated short-duration bursts of high-energy radiation ranging from radio waves to gammarays. AXPs are characterized by persistent X-ray emissions that often exhibit periodic pulsations. Magnetar candidates also show similar characteristics as those of AXPs but lack clear evidence of periodicity. All three types of sources occasionally emit giant flares accompanied by energetic particle acceleration phenomena (e.g., Palmer 2014; Kashiyama et al. 2013 ). On the other hand, some of these objects sometimes undergo very faint outbursts lasting for hours to days. For example, SGR 0526-66 showed a series of such outbursts between 1979 and 1989 (Mazets et al. 1981; Cline et al. 1982; Kulkarni et al. 1993; Kouveliotou et al. 1998 ) while SGR 1900+14 exhibited another series of fainter ones between 1997 and 2001 . Such",
        "watermark_text": "We report on the observation of an incredibly rare supersoft X - ray outburst incident in the transient stream 1E0102 . 2 - 7219 ( = GX 354 - 0 ) with the Suzaku spacecraft , which was sparked by a large rise in its hard Xray flux .The period and peak luminosity are estimated to be about 100 s and 2×1036 erg / s at 6 kpc distance , respectively . This is one of only two such events ever observed for this object .We discuss possible origins of these events according on their observed properties . Keywords : Supernova remnant , Soft gamma - ray repeater , Transient precursor , Supersoft X - ray radiation , Hard X - ray flare 1 Introduction In recent years , various additional types of transients have been detected through widespread searches using satellites like RXTE / ASM or Swift / BAT .These include soft - gamma repeaters ( SGRs ; e . g . , Hurley et al . 2005 ) , anomalous X - ray pulsars ( AXPs ; e . g . , Kaspi & Beloborodov 2017 ) , and magnetar candidates ( e . g . , Rea et al .2012 ) . Among them , SGRs exhibit frequent short - duration bursts of high - energy rays ranging from radio beams to gammarays .AXPs are marked by persistent X - ray emissions that frequently exhibit periodic pulsations . Magnetar candidates often show identical traits as those of AXPs but lack firm indication of periodicity .All three categories of sources occasionally emit giant flares driven by energetic particle acceleration phenomena ( e . g . , Palmer 2014 ; Kashiyama et al . 2013 ) .On the other hand , some of these objects sometimes undergo very faint outbursts lasting for days to days . For instance , SGR 0526 - 66 demonstrated a sequence of such outbursts between 1979 and 1989 ( Mazets et al .1981 ; Cline et al . 1982 ; Kulkarni et al .1993 ; Kouveliotou et al . 1998 ) while SGR 1900 + 14 exhibited another series of fainter ones between 1997 and 2001 .Such",
        "rewrite_text": "We present our findings on an exceptionally rare supersoft X-ray outburst observed in the transient source 1E0102.2-7219 (also known as GX 354-0) using the Suzaku spacecraft. This outburst was triggered by a significant increase in its hard X-ray flux. The estimated period of the event is approximately 100 seconds, with a peak luminosity of 2×10^36 erg/s at a distance of 6 kpc. Notably, this marks one of only two such occurrences ever recorded for this particular object. We explore potential explanations for these events based on their observed characteristics. \n\n**Keywords:** Supernova remnant, Soft gamma-ray repeater, Transient precursor, Supersoft X-ray radiation, Hard X-ray flare \n\n**1 Introduction**  \nIn recent years, a diverse range of transient astronomical events has been identified through extensive surveys conducted with satellites such as RXTE/ASM and Swift/BAT. These events include soft-gamma repeaters (SGRs; e.g., Hurley et al. 2005), anomalous X-ray pulsars (AXPs; e.g., Kaspi & Beloborodov 2017), and candidates for magnetars (e.g., Rea et al. 2012). SGRs are characterized by frequent, short-duration bursts of high-energy radiation encompassing a spectrum from radio waves to gamma rays. AXPs are distinguished by persistent X-ray emissions that often display periodic pulsations. Magnetar candidates exhibit behavior similar to AXPs, but lack definitive evidence of periodicity. All three categories of sources are known to occasionally produce powerful flares caused by energetic particle acceleration processes (e.g., Palmer 2014; Kashiyama et al. 2013). Additionally, some of these sources undergo faint outbursts lasting several days. For example, SGR 0526-66 exhibited a series of such outbursts between 1979 and 1989 (Mazets et al. 1981; Cline et al. 1982; Kulkarni et al. 1993; Kouveliotou et al. 1998), while SGR 1900+14 demonstrated another series of weaker outbursts from 1997 to 2001.",
        "ori-fast-z-score": -1.1141720290623112,
        "water-fast-z-score": 5.501778368617852,
        "rewrite-fast-z-score": -0.3563483225498992
    },
    {
        "original_text": "We present new constraints on warm dark matter (WDM) models by combining the results of two recent surveys for gravitationally lensed quasars, SDSS and CFHTLS Wide. We find that the observed number density of lenses is consistent with predictions based on cold dark matter simulations but inconsistent at more than 3 sigma confidence level if we assume a standard thermal relic WDM model with mass mX = 1 keV. This result suggests either that the current WDM scenario needs to be modified or that there are other systematic effects which have not been taken into account in our analysis. The full text can be found at: http://arxiv.org/abs/astro-ph/0604070v1.pdf . \nThe existence of dark matter has now been established beyond reasonable doubt through its gravitational influence on visible matter. However, despite decades of research, little else about this mysterious substance is known. In particular, it remains unclear whether dark matter consists of one particle species only - as assumed in most theoretical studies -or whether it comprises several different particles. One possibility is that dark matter consists of weakly interacting massive particles (WIMPs), such as neutralinos predicted within supersymmetric extensions of the Standard Model  1  .\nIn order to test these scenarios observationally, astronomers look for signatures of dark matter in astrophysical objects like galaxies  2  , clusters  3  and quasars  4  . A particularly promising method involves searching for gravitationally lensed systems  5  where light rays emitted by distant sources bend around intervening dark matter halos  6  . If dark matter consists of WIMPs then their masses should lie between 10 GeV/c 2 and 100 TeV/c 2  7, 8  . For example, the recently discovered galaxy cluster Abell 2218  9  may contain a halo made up entirely of WIMPs  10  .",
        "watermark_text": "We introduce novel constraints on warm dark matter ( WDM ) estimates by combining the conclusion of two latest surveys for gravitationally lensed quasars , SDSS and CFHTLS Wide . We see that the seen number density of lenses is compatible with predictions based on cold bright matter simulations but inconsistent at more than 3 sigma confidence rate if we expect a traditional thermal relic WDM theory with volume mX = 1 keV .This result suggests either that the present WDM situation needs to be altered or that there are other systematic effects which have not been took into consideration in our analysis . The full text can be found at : www : / / arxiv . org / abs / astro - ph / 0604070v1 . pdf .The nature of dark matter has now been known beyond reasonable question through its gravitational impact on visible matter . However , despite decades of research , nothing much about this secret quantity is known .In particular , it remains unsure whether dark matter contains of one particle species only - as implied in most theoretical researchers - or whether it contains multiple distinct objects . One possibility is that dark matter contains of weakly interacting massive electrons ( WIMPs ) , such as neutralinos expected within supersymmetric extensions of the Standard Model 1 .In order to test these scenarios observationally , astronomers look for signatures of dark matter in astrophysical objects like stars 2 , galaxies 3 and quasars 4 . A particularly useful technique means searching for gravitationally lensed systems 5 where light rays generated by distant sources bend around intervening black material halos 6 .If heavy material contains of WIMPs then their masses should lie between 10 GeV / c 2 and 100 TeV / c 2 7 , 8 . For instance , the recently discovered star cluster Abell 2218 9 would contain a halo made up completely of WIMPs 10 .",
        "rewrite_text": "We present new constraints on warm dark matter (WDM) estimates by synthesizing results from two recent surveys of gravitationally lensed quasars: SDSS and CFHTLS Wide. Our findings indicate that the observed number density of lenses aligns with predictions from cold dark matter simulations, but exhibits more than a 3-sigma discrepancy when considering traditional thermal relic WDM theory with a particle mass (mX) of 1 keV. This outcome implies that the current understanding of WDM may require revision, or that additional systematic effects have not been accounted for in our analysis. For further details, the full text is available at: www.arxiv.org/abs/astro-ph/0604070v1. While dark matter's existence is well-established due to its gravitational effects on visible matter, much remains unknown about this elusive component. It is still uncertain if dark matter consists of a single particle species, as most theoretical models suggest, or if it comprises multiple distinct entities. One potential candidate for dark matter is weakly interacting massive particles (WIMPs), such as neutralinos predicted in supersymmetric extensions of the Standard Model. To investigate these possibilities, astronomers search for signatures of dark matter in various astrophysical objects, including stars, galaxies, and quasars. A particularly effective method is to examine gravitationally lensed systems, where light from distant sources is bent by intervening dark matter halos. If dark matter comprises WIMPs, their masses are expected to range between 10 GeV/c² and 100 TeV/c². For example, the recently identified star cluster Abell 2218 is thought to be surrounded by a halo composed entirely of WIMPs.",
        "ori-fast-z-score": 0.08804509063256238,
        "water-fast-z-score": 9.068644335153925,
        "rewrite-fast-z-score": -0.09090909090909091
    },
    {
        "original_text": "The zeta-function is the main object in number theory and algebraic geometry.  In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non-trivial topology.   The basic idea is that one considers the Feynman path integral over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm.    This leads naturally to the concept of a  quantum torus  whose zeta function encodes information about the spectrum of the corresponding QFT.   We will also discuss some recent results concerning the relation between the zeta functions of certain families of quantum tori and their associated modular forms. Quantum Field Theory (QFT) has been developed into a powerful tool for studying physical phenomena at the most fundamental level. However, many interesting questions remain open due to the difficulty of performing calculations beyond perturbation theory. One way out of these difficulties is to use ideas from mathematics such as those coming from number theory or algebraic geometry. In particular, we are interested in using the so-called  zeta-functions  of algebraic varieties to obtain new insights into QFTs. These objects encode important information about the underlying physics but they are notoriously difficult to compute explicitly. In my talk I ll give examples where explicit computations have been performed successfully and show how these techniques could lead to further progress in our understanding of QFTs.",
        "watermark_text": "The zeta - function is the main object in number theory and algebraic topology . In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non - trivial geometry .The basic idea is that one considers the Feynman line expansion over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm . This leads naturally to the idea of a quantum torus whose zeta function encodes data about the spectrum of the associated QFT .We will also discuss some latest findings concerning the relation between the zeta functions of certain classes of quantum tori and their associated modular forms . Quantum Field Theory ( QFT ) has been built into a powerful tool for studying physical phenomena at the most important level .However , many interesting problems continue open thanks to the difficulty of performing calculations beyond perturbation theory . One path out of these problems is to use insights from mathematics such as those coming from number theory or algebraic theory .In particular , we are concerned in use the so - called zeta - functions of algebraic fields to obtain new understanding into QFTs . These items encode crucial data about the fundamental theories but they are notoriously difficult to compute accurately .In my talk I ll offer examples where explicit computations have been performed effectively and suggest how these procedures could lead to further progress in our understanding of QFTs .",
        "rewrite_text": "The zeta function serves as a central focus in both number theory and algebraic topology. In this presentation, I will demonstrate how it can be leveraged to investigate quantum field theories (QFTs) on curved spacetimes that exhibit non-trivial geometries. The fundamental concept involves treating the Feynman line expansion across all fields in spacetime as an infinite-dimensional functional integration, which can be regularized by substituting the field space with a finite-dimensional vector space defined by an appropriate norm. This approach naturally leads to the notion of a quantum torus, whose zeta function contains information about the spectrum of the corresponding QFT. We will also explore recent discoveries regarding the connection between the zeta functions of specific classes of quantum tori and their related modular forms. Quantum Field Theory has evolved into a robust framework for analyzing physical phenomena at a fundamental level. Nevertheless, many intriguing challenges remain, largely due to the complexities involved in calculations beyond perturbation theory. One pathway to address these challenges is by incorporating mathematical insights, particularly from number theory and algebraic studies. Our emphasis will be on utilizing the zeta functions associated with algebraic fields to gain new insights into QFTs. These functions encapsulate vital information regarding fundamental theories, but obtaining accurate computations can be notoriously challenging. In my talk, I will present examples where explicit computations have been successfully carried out, and I will propose ways in which these methods could contribute to further advancements in our understanding of quantum field theories.",
        "ori-fast-z-score": 0.5570860145311556,
        "water-fast-z-score": 6.009252125773315,
        "rewrite-fast-z-score": 0.09090909090909091
    },
    {
        "original_text": "We study the stability properties of planar streamers in air, which are ionized regions that propagate into un-ionized gas under the influence of an applied electric field. We show how to use the so-called  pulled front approach  to derive a nonlinear evolution equation for the shape of such fronts and then analyze this equation using standard techniques from dynamical systems theory. In particular we find that there is a critical value of the applied voltage beyond which the planar solution becomes unstable with respect to small perturbations. This result explains why it has been so difficult to observe stable propagation of streamers at high voltages experimentally. The results presented here should be relevant not only to atmospheric discharges but also to other physical situations where similar phenomena occur, e.g., combustion flames or chemical waves on catalytic surfaces. Streamers are thin channels filled with highly conducting plasma that can form when strong electric fields are present between two electrodes immersed in non-conducting gases  1  . They have attracted considerable interest over many years because they play important roles in various applications including lightning  2  , sprites  3  , and high-voltage switches  4  .\nIn recent years much progress has been made towards understanding their formation mechanisms  5, 6, 7, 8  as well as their dynamics  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96,",
        "watermark_text": "We explore the stability properties of planar streamers in atmosphere , which are ionized areas that propagate into un - ionized gas under the effects of an applied electric field . We see how to use the so - called tugged forward approach to derive a nonlinear development formula for the shape of such fronts and then investigate this equation using conventional methods from dynamical systems models .In particular we find that there is a critical factor of the introduced voltage beyond which the planar solution gets unstable with regard to small perturbations . This result explains why it has been so difficult to observe consistent diffusion of streamers at high voltages experimentally .The results presented here should be applicable not only to ambient discharges but also to other physical conditions where similar phenomena arise , e . g . , combustion burning or molecular currents on catalytic surfaces . Streamers are thin tubes filled with highly conducting plasma that can form when strong magnetic waves are present between two electrodes immersed in non - conducting gases 1 .They have garnered considerable interest over numerous years because they serve major roles in different applications notably lightning 2 , sprites 3 , and large - frequency switches 4 . In recent years much work has been given towards studying their formed mechanisms 5 , 6 , 7 , 8 as well as their mechanics 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 ,",
        "rewrite_text": "We investigate the stability characteristics of planar streamers in the atmosphere, which are ionized regions that advance into non-ionized gas due to the influence of an applied electric field. Utilizing the tugged forward technique, we derive a nonlinear development formula for the shape of these fronts and analyze the equation through established methods in dynamical systems. Notably, we discover a critical threshold of the applied voltage beyond which the planar solution becomes unstable in response to slight disturbances. This finding elucidates the challenges faced in consistently observing streamer diffusion at high voltages in experimental settings. The insights gained from this study can be applied not only to ambient discharges but also to other scenarios involving similar effects, such as combustion and molecular currents on catalytic surfaces. Streamers are thin columns filled with highly conductive plasma that can emerge when strong magnetic waves are present between two electrodes immersed in non-conductive gases. They have attracted significant interest over the years due to their important functions in various applications, notably lightning, sprites, and high-frequency switches. Recent research has focused extensively on understanding their formation mechanisms and underlying mechanics.",
        "ori-fast-z-score": -1.5689290811054724,
        "water-fast-z-score": 7.256297000112809,
        "rewrite-fast-z-score": 2.592379236826063
    },
    {
        "original_text": "We present an optical source catalog for the north ecliptic pole region (NEPR) based on data obtained with the Palomar Observatory Sky Survey (POSS-II). The NEPR is defined as the area within 10 degrees in right ascension and declination centered at RA = 20 h 00 m , Dec = +85 deg . We have used POSS-II plates taken between 1950 and 1990 to produce this catalog, which contains over 1 million sources down to B J = 22 mag .\nThe photometric calibration was performed using Landolt standard stars observed during the same nights that the sky survey plates were exposed. Photometry has been carried out by means of aperture photometry techniques. Magnitudes are given in the Johnson system. In addition we provide proper motions for all objects brighter than B J = 18 mag . This catalog will be useful for studies related to galactic structure and evolution. \n \n Keywords: Palomar Observatory Sky Survey",
        "watermark_text": "We create an optical source catalog for the north ecliptic pole region ( NEPR ) based on evidence derived with the Palomar Observatory Sky Survey ( POSS - II ) . The NEPR is characterized as the area within 10 degrees in right ascension and declination centered at RA = 20 h 00 m , Dec = + 85 deg .We have utilized POSS - II sheets taken between 1950 and 1990 to produce this database , which contains over 1 million sources down to B J = 22 mag . The photometric calibration was done utilizing Landolt standard stars observed during the same hours that the sky survey plates were uncovered .Photometry has been carried out by means of aperture photometry method . Magnitudes are given in the Johnson system .In addition we provide proper motions for all bodies brighter than B J = 18 mag . This collection will be valuable for research associated to galactic composition and evolution .Keywords: Palomar Observatory Sky Survey",
        "rewrite_text": "We have compiled an optical source catalog for the North Ecliptic Pole Region (NEPR) using data from the Palomar Observatory Sky Survey (POSS-II). The NEPR is defined as the area within 10 degrees of right ascension and declination, centered at RA = 20h 00m, Dec = +85°. Our database is based on POSS-II plates taken between 1950 and 1990 and includes over 1 million sources with magnitudes down to B_J = 22 mag. Photometric calibration was achieved using Landolt standard stars observed during the same periods that the sky survey plates were exposed. We performed photometry using the aperture photometry method, and the magnitudes are reported in the Johnson system. Additionally, we provide proper motions for all sources brighter than B_J = 18 mag. This collection will serve as a valuable resource for research related to galactic composition and evolution. Keywords: Palomar Observatory Sky Survey.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.714951667914447,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We report on new spectroscopic observations of the eclipsing binary system HAT-TR-205-1013, which was discovered by the Hungarian Automated Telescope (HAT) survey for transiting exoplanets. The light curve shows two partial eclipses with an orbital period of 1.8 days. We find that this is most likely caused by reflection effects rather than occultation events due to the presence of a third body. Using our radial velocity measurements we determine the mass function f(m) = 0.0027 ± 0.0007M⊙, where m is the mass of the unseen companion star. Assuming circular orbits, we derive the minimum masses of both components as M1 sin i = 0.84 ± 0.04M⊙ and M2 sin i = 0.16 ± 0.02M⊙. From these values we estimate the radius ratio between the primary and secondary component as q = 0.53 ± 0.06.",
        "watermark_text": "We report on new spectroscopic observations of the eclipsing binary system HAT - TR - 205 - 1013 , which was discovered by the Hungarian Automated Telescope ( HAT ) study for transiting exoplanets . The light curve shows two partial eclipses with an orbital period of 1 . 8 days .We see that this is most likely due by reflection influences rather than occultation processes due to the presence of a third body . Using our radial speed measurements we determine the mass function f ( m ) = 0 . 0027 ± 0 . [UNK] , where m is the mass of the unseen companion star .Assuming circular orbits , we derive the minimum masses of both components as M1 sin i = 0 . 84 ± 0 . [UNK] and M2 sin i = 0 . 16 ± 0 . [UNK] . From these values we estimate the radius ratio between the primary and secondary component as q = 0 . 53 ± 0 . 06 .",
        "rewrite_text": "We present new spectroscopic observations of the eclipsing binary system HAT-TR-205-1013, identified by the Hungarian Automated Telescope (HAT) survey for transiting exoplanets. The light curve reveals two partial eclipses with an orbital period of 1.8 days. Our findings suggest that these eclipses are likely influenced by reflection effects rather than by occultation processes caused by a third body. Through our radial velocity measurements, we calculate the mass function f(m) = 0.0027 ± 0.[UNK], where m denotes the mass of the unseen companion star. Assuming circular orbits, we determine the minimum masses for both components as M1 sin i = 0.84 ± 0.[UNK] and M2 sin i = 0.16 ± 0.[UNK]. From these calculations, we estimate the radius ratio of the primary to the secondary component as q = 0.53 ± 0.06.",
        "ori-fast-z-score": 0.6401843996644799,
        "water-fast-z-score": 2.1766269588592317,
        "rewrite-fast-z-score": 0.13018891098082389
    },
    {
        "original_text": "We present the first direct determination of the stellar radius in an interacting binary system, using interferometric observations obtained with the VLTI and AMBER instrument. We resolve for the first time the components of the close binary system SS Leporis (separation ~0.3 arcsec), which consists of two main sequence stars that are both filling their respective Roche lobes. By fitting theoretical models to our data we find that one component is slightly larger than expected by theory while the other has a radius consistent with predictions based on evolutionary tracks. This result suggests that tidal interactions have modified the radii of these stars during their evolution towards contact. Our results also show that the orbital inclination angle i = 60 ± 5 degrees, as determined previously through radial velocity measurements, agrees well with our new estimate derived directly from the observed separation between the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius",
        "watermark_text": "We present the first complete measurement of the stellar radius in an interacting binary system , using interferometric observations derived with the VLTI and AMBER method . We resolve for the first time the parts of the distant binary system SS Leporis ( separation ~ 0 . 3 arcsec ) , which consists of two principal sequence stars that are both filling their separate Roche lobes .By fitting theoretical estimates to our information we find that one part is slightly larger than expected by hypothesis while the other has a diameter compatible with predictions based on evolutionary tracks . This result suggests that tidal interactions have modified the radii of these stars during their development towards contact .Our results also demonstrate that the orbital inclination distance i = 60 ± 5 degrees , as determined earlier through radial speed measurements , agrees well with our new estimate calculated directly from the observed separation between the two stars . Keywords : Interferometry ; Binary Stars ; Stellar Radius",
        "rewrite_text": "We present the first comprehensive measurement of the stellar radius in an interacting binary system, utilizing interferometric observations obtained through the VLTI and AMBER method. For the first time, we resolve components of the distant binary system SS Leporis, which has a separation of approximately 0.3 arcseconds. This system comprises two main sequence stars, each filling its respective Roche lobe. By fitting theoretical models to our observational data, we discover that one star is slightly larger than predicted, while the other’s diameter aligns well with expectations based on evolutionary models. This finding indicates that tidal interactions have altered the radii of these stars as they evolved towards contact. Furthermore, our results confirm that the previously determined orbital inclination of i = 60 ± 5 degrees, derived from radial velocity measurements, is consistent with our new estimate obtained directly from the observed separation of the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We report on the observation of ground state magneto optical resonance (GMOR) in cesium vapor confined to a sub-micron thickness layer inside a glass micro-cell. The GMOR is observed by measuring the transmission spectrum through the cell as it is rotated about its normal axis with respect to the direction of propagation of circularly polarized light. We show that this effect can be explained using simple classical electrodynamics and we present experimental results which demonstrate the dependence of the GMOR signal strength on various parameters such as the intensity, frequency detuning and polarization angle of the incident laser beam. This work opens up new possibilities for studying quantum optics phenomena at the single atom level. \n \n In recent years there has been considerable interest in developing techniques for trapping atoms or molecules within microscopic volumes  1  . Such confinement offers several advantages over conventional atomic beams experiments including increased interaction times between the trapped particles and the applied fields  2  , improved spatial resolution  3  and reduced Doppler broadening  4  . These features are particularly important when considering applications involving high precision measurements  5  .\nIn addition to these practical benefits, confining neutral matter to small dimensions also provides opportunities for exploring fundamental physics  6  . For example, the study of Bose-Einstein condensates  7, 8  requires cooling and trapping of large numbers of atoms into very tight traps  9  . Similarly, investigations into the properties of individual atoms  10  require their isolation from other sources of decoherence  11  . Finally, studies of macroscopic quantum effects  12  may benefit from the ability to control the number of particles involved  13  . \n \n Here we describe our efforts towards achieving controlled confinement of neutral matter to extremely small dimensions. Specifically, we have developed a technique for producing a thin film of cesium gas inside a glass micro-cell  14  . By exploiting the strong magnetic dipole moment associated with the cesium ground state  15  , we observe a novel form of magneto-optical resonance  16  known as ground state magneto-optical resonance  17  . Our observations suggest that this phenomenon could provide a useful tool for investigating quantum optics processes occurring at the single atom level  18  .",
        "watermark_text": "We report on the observation of ground state magneto optical resonance ( GMOR ) in cesium vapor confined to a sub - micron thickness sheet inside a glass micro - cell . The GMOR is observed by monitoring the propagation spectrum through the cell as it is rotated about its regular axis with regard to the direction of propagation of circularly polarized light .We see that this effect can be described using simple classical electrodynamics and we present experimental results which demonstrate the dependence of the GMOR wave strength on various variables such as the frequency , frequency detuning and polarization angle of the incident beam beam . This research raises up new possibilities for studying quantum optics dynamics at the single atom level .In recent years there has been substantial interest in improving procedures for trapping atoms or compounds within microscopic volumes 1 . Such confinement gives numerous benefits over traditional molecular beams studies namely increased interaction times between the captured particles and the applied fields 2 , enhanced angular resolution 3 and reduced Doppler broadening 4 .These features are particularly important when assessing uses concerning high precision observations 5 . In addition to these useful benefits , confining neutral matter to small dimensions additionally offers options for studying basic physics 6 .For instance , the investigations of Bose - Einstein condensates 7 , 8 requires freezing and trapping of large numbers of atoms into very strict trapping 9 . Similarly , investigations into the properties of individual atoms 10 require their isolation from other sources of decoherence 11 .Finally , investigations of macroscopic quantum effects 12 may benefit from the ability to affect the quantity of particles concerned 13 . Here we explain our initiatives towards attain controlled confinement of neutral matter to incredibly small sizes .Specifically , we have developed a technique for producing a thin film of cesium gas inside a glass micro - cell 14 . By exploiting the strong magnetic dipole point involved with the cesium ground state 15 , we study a new form of magneto - optical resonance 16 known as ground state magneto - optical resonance 17 .Our observations suggest that this phenomenon might give a helpful resource for investigating quantum optics processes observed at the single atom level 18 .",
        "rewrite_text": "We present our findings on the observation of ground state magneto-optical resonance (GMOR) in cesium vapor constrained within a sub-micron thick layer inside a glass micro-cell. The GMOR is detected by examining the transmission spectrum through the cell as it rotates around its axis relative to the path of circularly polarized light. Our results indicate that this phenomenon can be understood through classical electrodynamics. We also provide experimental data demonstrating how the strength of the GMOR waves varies with different factors, including frequency, frequency detuning, and the polarization angle of the incident beam. This research opens up new avenues for exploring quantum optical dynamics at the single-atom level. Recently, there has been significant interest in enhancing methods for trapping atoms or molecules in microscopic volumes. Such confinement offers numerous advantages over conventional molecular beam studies, including longer interaction times between trapped particles and applied fields, improved angular resolution, and minimized Doppler broadening. These characteristics are crucial for applications requiring high precision observations. Additionally, confining neutral matter to small dimensions provides opportunities for investigating fundamental physics. For example, studying Bose-Einstein condensates necessitates the trapping of large numbers of atoms in tight confinement. Likewise, research into the properties of individual atoms demands their isolation from sources of decoherence. Moreover, explorations of macroscopic quantum effects may benefit from controlling the number of involved particles. In this work, we detail our efforts to achieve controlled confinement of neutral matter at extremely small scales. We specifically developed a technique for producing a thin film of cesium gas within a glass micro-cell. By leveraging the strong magnetic dipole interactions associated with the cesium ground state, we investigate a novel type of magneto-optical resonance known as ground state magneto-optical resonance. Our observations suggest that this phenomenon could serve as a valuable tool for probing quantum optics processes at the single-atom scale.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.460169506877884,
        "rewrite-fast-z-score": 1.005970202294378
    },
    {
        "original_text": "The low-energy parameters of neutron-proton scattering are determined by using modern experimental data obtained in partial-wave analyses (PWAs). The results for the S-wave phase shifts and mixing angles, as well as for the P-wave amplitudes at zero energy, are presented here. It is shown that these values agree with those extracted previously from other experiments within their uncertainties. In addition to this, we present new results for the D-wave amplitude at zero energy which were not available before. \nI. INTRODUCTIO N\nNeutron-proton elastic scattering has been studied extensively over many decades  1  . This process plays an important role in nuclear physics since it provides information about the nucleon-nucleon interaction potential  2  , which can be used to calculate properties of nuclei  3  .\nIn recent years there have been significant advances in our understanding of the structure of the nucleon-nucleus system  4  . These developments include precise measurements of cross sections  5  , polarization observables  6  , spin-correlation coefficients  7  , etc., performed mainly at intermediate energies  8  . However, despite all efforts made so far, some questions remain open  9  . For example, one still needs more accurate determinations of the lowenergy parameters of the nucleonnucleon interaction  10  .",
        "watermark_text": "The lowest - energy parameters of neutron - proton diffusion are decided by using contemporary experimental evidence derived in partial - wave assessments ( PWAs ) . The results for the S - wave phase change and mixing angles , as well as for the P - wave amplitudes at zero energy , are presented here .It is demonstrated that these parameters agree with those retrieved previously from other experiments within their uncertainties . In addition to this , we present new data for the D - wave amplitude at zero energy which were not accessible before .I . INTRODUCTIO N Neutron - proton elastic scattering has been studied extensively over numerous years 1 . This process holds an important role in nuclear science since it gives information about the nucleon - nucleon collision potential 2 , which can be used to estimate features of nuclei 3 .In recent years there have been significant advances in our knowing of the structure of the nucleon - nucleus system 4 . These advances require precise observations of cross sections 5 , polarization observables 6 , spin - correlation coefficients 7 , etc . , conducted predominantly at intermediate energies 8 .However , despite all efforts made so far , some questions remain open 9 . For instance , one currently needs more accurate determinations of the lowenergy variables of the nucleonnucleon interaction 10 .",
        "rewrite_text": "The low-energy parameters governing neutron-proton diffusion are established using current experimental data obtained from partial-wave analyses (PWAs). Here, we present findings on the S-wave phase shift and mixing angles, along with the P-wave amplitudes at zero energy. Our results show that these parameters are consistent with those previously obtained from other experiments, within their respective uncertainties. Additionally, we introduce new data on the D-wave amplitude at zero energy, which were previously unavailable. \n\nI. INTRODUCTION  \nNeutron-proton elastic scattering has been the subject of extensive investigation for many years. This process plays a crucial role in nuclear science, as it provides insights into the nucleon-nucleon collision potential, which can be utilized to infer various characteristics of nuclei. Recently, there have been significant advancements in our understanding of the nucleon-nucleus system. These developments necessitate accurate measurements of cross sections, polarization observables, spin-correlation coefficients, and more, primarily conducted at intermediate energies. Nevertheless, despite the progress made, several questions remain unresolved. For example, there is a pressing need for more precise determinations of the low-energy variables associated with the nucleon-nucleon interaction.",
        "ori-fast-z-score": -0.21081851067789195,
        "water-fast-z-score": 6.81385143869247,
        "rewrite-fast-z-score": 1.762817881041723
    },
    {
        "original_text": "The WiFeS instrument is an integral field spectrograph for the Australian National University 2.3m telescope at Siding Spring Observatory, Australia.  It has been in operation since 2005 and was designed to provide high quality optical spectroscopy over a wide range of wavelengths with minimal overheads.   The WiFeS instrument consists of two cameras that are mounted on a common baseplate which sits inside a vacuum chamber attached to the Cassegrain focus of the ANU 2.3m telescope.    Each camera contains a lenslet array that produces a set of images across its focal plane.  These images can be combined into a single data cube using software developed by Dopita et al. (2007).    This data cube provides information about both spatial position and wavelength along each line-of-sight through the object being observed.  In addition to this capability, WiFeS also offers several other advantages including:  - High throughput due to the use of dichroic beam splitters and fibre optic coupling between the lenses and detectors.  - Flexible observing modes ranging from fully automated observations to manual control via remote desktop interface.  - Fast readout times allowing multiple targets to be observed simultaneously or rapid cycling between different targets during a night s observation run.  - Low maintenance requirements as it does not require any moving parts within the vacuum enclosure.  - Excellent stability thanks to the design of the optics bench and the thermal isolation provided by the vacuum enclosure.  - Large format detector arrays providing excellent signal-to-noise ratio even under poor seeing conditions.  - Robust mechanical design ensuring long-term reliability...",
        "watermark_text": "The WiFeS instrument is an integral field spectrograph for the Australian National University 2 . 3m telescope at Siding Spring Observatory , Australia . It has been in operation since 2005 and was built to provide high quality optical spectroscopy over a broad variety of wavelengths with minimal overheads .The WiFeS instrument consists of two cameras that are installed on a common baseplate which sits inside a vacuum chamber connected to the Cassegrain scope of the ANU 2 . 3m telescope . Each lens includes a lenslet antenna that generates a pair of pictures across its focal surface .These photographs can be merged into a single data cube using software created by Dopita et al . ( 2007 ) .This data cube provides information about both visual orientation and wavelength along each line - of - view through the element being detected . In addition to this capability , WiFeS also provides various other advantages namely : - High throughput owing to the using of dichroic laser splitters and fibre optic coupling between the lenses and detectors .- Flexible monitoring mechanisms ranging from fully autonomous observations to manual control via remote desktop interface . - Fast readout hours permitting multiple targets to be spotted concurrently or rapid cycling between various destinations during a night s observation run .- Low maintenance requirements as it does not require any shifting components within the vacuum enclosure . - Good integrity thanks to the design of the optics bar and the thermal isolation provided by the vacuum enclosure .- Large format detector arrays offering good signal - to - noise ratio especially under bad seeing conditions . - Robust mechanical architecture ensuring long - term reliability . . .",
        "rewrite_text": "The WiFeS instrument is an integral field spectrograph designed for the Australian National University’s 2.3m telescope located at Siding Spring Observatory in Australia. Since its inception in 2005, it has delivered high-quality optical spectroscopy across a wide range of wavelengths with minimal overhead. The instrument features two cameras mounted on a shared baseplate within a vacuum chamber that is connected to the Cassegrain focus of the ANU 2.3m telescope. Each camera is equipped with a lenslet array that produces two images across the focal plane, which can then be combined into a single data cube using software developed by Dopita et al. (2007). This data cube provides detailed information regarding both spatial orientation and wavelength along each line of sight through the observed object. In addition to this primary function, WiFeS offers several notable advantages, including: high throughput achieved through the use of dichroic laser splitters and fiber optic coupling between lenses and detectors; flexible operational modes ranging from fully autonomous observations to manual control via a remote desktop interface; rapid readout times that allow for simultaneous observation of multiple targets or quick cycling between locations during a night’s observation; low maintenance needs due to the absence of moving components within the vacuum enclosure; strong structural integrity owing to the design of the optics bar and the thermal isolation of the vacuum chamber; and large-format detector arrays that provide an excellent signal-to-noise ratio even under poor seeing conditions, complemented by a robust mechanical design for long-term reliability.",
        "ori-fast-z-score": -0.6211495565912797,
        "water-fast-z-score": 7.0101164243872995,
        "rewrite-fast-z-score": 0.26211121699831136
    },
    {
        "original_text": "We present an overview of the state-of-the-art methods for spectral analysis on spherical data, with emphasis on their applications to problems arising in geophysical sciences (e.g., global seismological tomography) and astrophysics (e.g., cosmic microwave background). We also discuss some recent advances in this area that have been made by our group at Columbia University. The main focus is on the development of new algorithms for computing accurate estimates of the power spectrum of signals defined over the surface of the unit sphere using only partial information about these signals. In particular, we consider two classes of methods: those based on the use of spherical harmonic expansions and those based on wavelet transforms. Finally, we briefly describe several open research questions related to the topic discussed here. Spherical data arise naturally in many areas of science including astronomy, meteorology, oceanography, geodesy, and medicine. For example, astronomers routinely collect large amounts of data describing the positions of celestial objects such as stars or galaxies; similarly, weather forecasters gather measurements of atmospheric pressure, temperature, humidity, wind speed, etc., at various locations around the globe. These types of data are often represented mathematically as functions defined over the surface of a sphere.",
        "watermark_text": "We bring an overview of the state - of - the - art methods for spectral modeling on spherical measurements , with emphasis on their applications to problems arose in geophysical sciences ( e . g . , global seismological tomography ) and astrophysics ( e . g . , planetary microwave background ) . We also discuss some latest advances in this area that have been made by our team at Columbia University .The main interest is on the development of new method for calculation precise estimates of the power spectrum of waves defined over the surface of the unit sphere utilizing only partial knowledge about these signals . In particular , we investigate two groups of methods : those relying on the using of spherical harmonic expansions and those based on wavelet transforms .Finally , we briefly illustrate numerous open scientific questions related to the subject mentioned here . Spherical data arise naturally in multiple fields of science including science , meteorology , oceanography , geodesy , and medicine .For instance , astronomers typically gather huge amounts of statistics explaining the places of astronomical bodies such as planets or galaxies ; likewise , weather forecasters gather measurements of atmospheric pressure , temperature , moisture , breeze pressure , etc . , at numerous locations around the globe . These sorts of statistics are often depicted mathematically as functions defined over the surface of a sphere .",
        "rewrite_text": "We provide an overview of cutting-edge techniques for spectral modeling of spherical measurements, focusing on their applications in geophysical sciences, such as global seismological tomography, and astrophysics, like planetary microwave background studies. In addition, we highlight recent advancements made by our team at Columbia University. Our primary aim is to develop new methods for accurately estimating the power spectrum of waves defined on the surface of the unit sphere using only partial information about these signals. We explore two main categories of methods: those that utilize spherical harmonic expansions and those that employ wavelet transforms. Lastly, we briefly address several open scientific questions related to this topic. Spherical data naturally occur in various scientific fields, including meteorology, oceanography, geodesy, and medicine. For example, astronomers collect vast amounts of data on the locations of celestial bodies such as planets and galaxies, while meteorologists gather measurements of atmospheric pressure, temperature, humidity, and wind at numerous sites around the world. These types of data are often mathematically represented as functions defined over the surface of a sphere.",
        "ori-fast-z-score": -2.011435198964418,
        "water-fast-z-score": 6.674238124719146,
        "rewrite-fast-z-score": 1.4596008983995234
    },
    {
        "original_text": "The search is performed in the context of the Minimal Supersymmetric Standard Model (MSSM) using data collected by the Compact Muon Solenoid experiment at sqrt(s) = 7 TeV, corresponding to an integrated luminosity of 5 fb-1 . The results are interpreted as limits on the production cross section times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance. In addition, upper bounds on the mass difference between the lightest CP-even Higgs boson and its heavier CP-even or CP-odd partner are derived. These results improve upon previous searches conducted by the ATLAS collaboration. \n \n A summary of this work has been presented at: \n \n \n \n \n \n This document contains additional information that may be useful to readers interested in reproducing our analysis or applying it to other datasets. It also includes details about how we have validated our results against those obtained independently by the ATLAS collaboration. \n \nIntroduction\n\nThe discovery of a new particle consistent with the Standard Model (SM) Higgs boson  1–3  has opened up a new era in particle physics. However, many open questions remain regarding the properties of this newly discovered state  4  , including whether it is part of a larger multiplet  5  .\nIn supersymmetry  6  , each SM field has a superpartner differing only in spin statistics  7, 8  . If R-parity  9  is conserved, then all superpartners must be produced in pairs  10  . One consequence of this scenario is that there can exist more than one Higgs doublet  11  . In particular, if the lighter scalar Higgs boson observed at the LHC  12–18  corresponds to the lightest CP-eigenstate h0 of such a model  19, 20  , then the next-to-lightest CP-eigenstates H0 and A0 could both couple strongly to fermions  21  . Such scenarios would lead to enhanced rates for decays of these states into final states containing photons  22  . \n \n In order to explore possible deviations from the SM predictions  23  , precise measurements of the masses and couplings of the Higgs bosons predicted by",
        "watermark_text": "The scan is conducted in the context of the Minimal Supersymmetric Standard Model ( MSSM ) using data taken by the Compact Muon Solenoid experiment at sqrt ( s ) = 7 TeV , corresponding to an integrated luminosity of 5 fb - 1 . The results are seen as limits on the production cross section times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance .In addition , upper limits on the mass ratio between the lightest CP - even Higgs boson and its lighter CP - even or CP - even partner are derived . These data improve upon recent searches undertaken by the ATLAS collaboration .A description of this research has been presented at : This text includes added details that might be valuable to readers interested in reproducing our analysis or applying it to other datasets . It additionally contains details about how we have validated our findings against those acquired independently by the ATLAS collaboration .Introduction The discovery of a new particle compatible with the Standard Model ( SM ) Higgs boson 1 – 3 has opened up a new period in particle science . However , many open questions remain regarding the properties of this freshly found state 4 , particularly whether it is part of a greater multiplet 5 .In supersymmetry 6 , each SM field has a superpartner varying only in spin statistics 7 , 8 . If R - parity 9 is conserved , then all superpartners must be made in pairs 10 .One result of this situation is that there can occur more than one Higgs doublet 11 . In particular , if the heavier scalar Higgs boson seen at the LHC 12 – 18 corresponds to the lightest CP - eigenstate h0 of such a theory 19 , 20 , then the second - to - lightest CP - eigenstates H0 and A0 could both couple strongly to fermions 21 .Such scenarios would result to accelerated rates for decays of these states into last states carrying photons 22 . In order to examine possible deviations from the SM predictions 23 , detailed observations of the masses and couplings of the Higgs bosons predicted by",
        "rewrite_text": "The scan is performed within the framework of the Minimal Supersymmetric Standard Model (MSSM), utilizing data collected by the Compact Muon Solenoid experiment at a center-of-mass energy of \\(\\sqrt{s} = 7 \\, \\text{TeV}\\), corresponding to an integrated luminosity of \\(5 \\, \\text{fb}^{-1}\\). The findings are presented as upper limits on the production cross-section multiplied by the branching fraction into two photons for neutral Higgs bosons decaying within the detector's acceptance. Additionally, upper limits on the mass ratio between the lightest CP-even Higgs boson and its lighter CP-even or CP-odd partner have been established. This dataset advances previous searches conducted by the ATLAS collaboration. A comprehensive overview of this research is available, providing essential details that may benefit readers interested in replicating our analysis or applying it to other datasets. It also includes information on how we have cross-validated our results with independent findings from the ATLAS collaboration. \n\nThe discovery of a new particle consistent with the Standard Model (SM) Higgs boson has ushered in a new era in particle physics. Nevertheless, numerous unresolved questions persist about the characteristics of this newly identified state, particularly its potential inclusion in a larger multiplet. In the context of supersymmetry, each SM field has a superpartner that differs only in spin statistics. If R-parity is conserved, superpartners must exist in pairs. This leads to the possibility of multiple Higgs doublets. Specifically, if the heavier scalar Higgs boson observed at the LHC corresponds to the lightest CP-eigenstate \\(h^0\\) in such a theoretical framework, the next-to-lightest CP-eigenstates \\(H^0\\) and \\(A^0\\) could have significant couplings to fermions. Such scenarios could result in increased decay rates of these states into final states that include photons. To explore potential deviations from the predictions of the SM, it is crucial to conduct detailed studies of the masses and couplings of the Higgs bosons anticipated by these models.",
        "ori-fast-z-score": -2.658425641381813,
        "water-fast-z-score": 6.141879930089016,
        "rewrite-fast-z-score": -2.172857905020445
    },
    {
        "original_text": "Graphene nanoribbons (GNRs) are promising candidates for future nanoelectronic devices due to their unique electronic properties and high carrier mobility at room temperature. However, the scaling behaviors of GNRFETs have not been fully understood yet because of the difficulty in simulating realistic device structures with atomistic details using conventional approaches such as density functional theory or tight-binding method. In this work, we perform large-scale quantum transport simulations on GNR-FETs by solving the nonequilibrium Green s function equations within an effective mass approximation. We find that the subthreshold swing decreases rapidly when the channel length is reduced below 10 nm while it increases slowly beyond 20 nm. The ON/OFF current ratio also shows similar trends but its value becomes saturated around 100 nm. These results suggest that the optimal channel length should be between 10-20 nm depending on the required performance parameters. Our findings can provide useful guidelines for designing practical graphene-based transistors.",
        "watermark_text": "Graphene nanoribbons ( GNRs ) are promising candidates for future nanoelectronic devices owing to their distinct mechanical behavior and large carrier movement at room temperature . However , the scaling interactions of GNRFETs have not been totally understood yet because of the difficulty in simulating realistic device designs with atomistic features using conventional approaches such as density functional theory or tight - binding method .In this research , we perform large - scale quantum carrier simulations on GNR - FETs by solving the nonequilibrium Green s function equations within an effective mass approximation . We see that the subthreshold swing decreases quickly when the channel length is decreased below 10 nm while it rises steadily beyond 20 nm .The ON / OFF present ratio therefore displays similar trends but its value gets saturated around 100 nm . These data suggest that the ideal network duration should be between 10 - 20 mm depending on the necessary performance parameters .Our findings can provide useful guidelines for constructing practical graphene - based transistors .",
        "rewrite_text": "Graphene nanoribbons (GNRs) are promising candidates for next-generation nanoelectronic devices due to their unique mechanical properties and high carrier mobility at room temperature. However, the scaling behaviors of GNR field-effect transistors (GNRFETs) are not fully understood, largely due to the challenges of realistically simulating their atomistic features with traditional methods like density functional theory or the tight-binding approach. In this study, we conduct large-scale quantum carrier simulations of GNRFETs by solving the nonequilibrium Green's function equations under the effective mass approximation. Our results indicate that the subthreshold swing decreases rapidly when the channel length falls below 10 nm, while it gradually increases beyond 20 nm. Consequently, the ON/OFF current ratio exhibits similar trends, leveling off at around 100 nm. These findings suggest that the optimal channel length for GNRFETs should fall within the range of 10 to 20 nm, depending on the desired performance characteristics. Our research offers valuable insights for the design of practical graphene-based transistors.",
        "ori-fast-z-score": 0.8834522085987723,
        "water-fast-z-score": 6.184165460191406,
        "rewrite-fast-z-score": 0.3333333333333333
    },
    {
        "original_text": "The future neutrino factory experiments will be able to search for new physics beyond the Standard Model (SM) with unprecedented precision, and are expected to provide important information on the origin of matter-antimatter asymmetry as well as dark matter candidates.  In this talk I will present an overview of our recent studies on how to probe various types of new physics using these facilities. The results presented here were obtained by combining the analyses performed at the T2K experiment and its off-axis near detector ND280. These include searches for sterile neutrinos, lepton flavor violating processes such as neutrinoless double beta decay, CP violation effects in leptonic sector, and exotic Higgs bosons that can couple to both quarks and leptons. We also discuss possible improvements in sensitivity which may be achieved if we combine the data taken at T2K and NOvA experiments. Finally, prospects for probing new physics at future accelerator-based neutrino factories are discussed.",
        "watermark_text": "The future neutrino plant experiments will be possible to search for fresh physics beyond the Standard Model ( SM ) with incredible clarity , and are expected to provide important information on the origin of matter - antimatter asymmetry as well as dark matter candidates . In this talk I will present an overview of our latest studies on how to probe various types of new science using these facilities .The results presented here were obtained by combining the calculations performed at the T2K experiment and its off - axis near sensor ND280 . These include searches for sterile neutrinos , lepton flavor violating reactions such as neutrinoless single gamma decay , CP violation processes in leptonic sector , and rare Higgs bosons that can couple to both quarks and leptons .We also discuss possible changes in sensitivity which would be obtained if we merge the information taken at T2K and NOvA studies . Finally , prospects for probing novel physics at possible accelerator - based neutrino farms are discussed .",
        "rewrite_text": "Future neutrino experiments promise to explore new physics beyond the Standard Model with remarkable precision, providing critical insights into the origins of matter-antimatter asymmetry and potential dark matter candidates. In this presentation, I will outline our recent research on how these facilities can investigate various aspects of new science. The findings I will share are the result of integrating calculations from the T2K experiment and its off-axis near detector, ND280. Our work includes searches for sterile neutrinos, lepton flavor-violating processes like neutrinoless single gamma decay, CP violation in the lepton sector, and rare Higgs bosons that may couple to both quarks and leptons. Additionally, we will explore potential enhancements in sensitivity that could arise from combining data from T2K and NOvA studies. Lastly, I will discuss the future possibilities for uncovering novel physics at prospective accelerator-based neutrino facilities.",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 0.75
    },
    {
        "original_text": "The statistical mechanics of time-independent non-dissipative nonequilibrium states is studied in the framework of generalized entropy production and fluctuation theorem. The main results are formulated for systems with discrete energy levels, but they can be extended to continuous spectrum as well. It is shown that the probability distribution function (PDF) of the total entropy production has an exponential tail at large values of entropy production. This result agrees with recent experimental data on PDFs of heat fluxes between two reservoirs connected by a single molecule junction. In addition it is demonstrated how this approach allows one to calculate the average value of the total entropy production and its fluctuations. Finally we discuss possible applications of these results to transport phenomena in molecular junctions. We show that our theory predicts correctly the dependence of the current-voltage characteristics on temperature difference between the leads. Statistical mechanics of time independent non-dissipative non-equilibrium states is studied within the framework of generalized entropy-production and fluctuation theorem. Main results are formulated for systems having discrete energy-levels, however extension to continuum spectra is also discussed. It is shown that probability-distribution-function (PDF) of total entropy-production has exponential tails at large values of entropy-production. This result agrees with recently obtained experimental data on PDF s of heat-fluxes between two reservoirs coupled via a single-molecule-junction. In addition it is shown how this approach allows us to calculate the average-value of total entropy-production and it s fluctuations. Finally we discuss possibilities of applying these results to transport phenomena occurring in molecular junctions.",
        "watermark_text": "The statistical mechanics of time - independent non - dissipative nonequilibrium states is studied in the framework of generalized entropy production and fluctuation theorem . The main results are formulated for models with discrete energy levels , but they can be applied to continuous spectrum as also .It is demonstrated that the probability distribution relation ( PDF ) of the total entropy production has an exponential tail at large values of entropy production . This result agrees with recent experimental evidence on PDFs of thermal fluxes between two basins connected by a single molecule interchange .In addition it is demonstrated how this methodology allows one to estimate the average value of the total entropy production and its fluctuations . Finally we explain possible use of these results to transport phenomena in molecular junctions .We see that our theory predicts correctly the relationship of the current - electrical properties on temperature difference between the leads . Statistical mechanics of time independent non - dissipative non - equilibrium states is studied within the framework of generalized entropy - production and fluctuation principle .Main findings are formulated for models having discrete energy - rates , however application to continuum spectra is also discussed . It is seen that probability - distribution - function ( PDF ) of gross entropy - production has exponential tails at large values of entropy - production .This result agrees with recently obtained experimental evidence on PDF s of temperature - fluxes between two basins coupled via a single - atom - junction . In addition it is demonstrated how this methodology allows us to estimate the average - value of gross entropy - production and it s fluctuations .Finally we explain options of using these results to transport phenomena occurring in molecular junctions .",
        "rewrite_text": "The statistical mechanics of time-independent, non-dissipative nonequilibrium states is examined through the lens of generalized entropy production and the fluctuation theorem. While the primary findings are articulated for models featuring discrete energy levels, the results can also be extended to continuous spectra. It is shown that the probability distribution function (PDF) of total entropy production exhibits an exponential tail for large values of entropy production. This observation is consistent with recent experimental data on the PDFs of thermal fluxes between two reservoirs connected by a single molecular interchange. Furthermore, the methodology presented allows for the estimation of the average total entropy production and its fluctuations. Finally, we discuss how these findings can be applied to transport phenomena in molecular junctions, revealing that our theory accurately predicts the relationship between current and electrical properties with respect to the temperature difference across the junctions.",
        "ori-fast-z-score": 1.8347785189881796,
        "water-fast-z-score": 8.30018853827986,
        "rewrite-fast-z-score": 1.212678125181665
    },
    {
        "original_text": "Spin echoes are observed when the spin system is subjected to two successive radio-frequency (RF) pulses separated by an interval, known as the pulse separation time Tsep. The first RF pulse creates a macroscopic magnetization vector M0 that precesses around the external magnetic field Bext at Larmor frequency fL = γBext where γ is gyromagnetic ratio for nuclear spins. After the second RF pulse with flip angle θ2 and phase shift φ2 relative to the first one, the transverse component of the magnetization vector M2(t) decays exponentially due to dephasing caused by local magnetic fields created by neighboring nuclei. In this work we show how strong π-pulses can be used to generate spin echoes even if there is no net initial magnetization present before applying these pulses. We demonstrate theoretically and experimentally that such spin echoes originate intrinsically from the dipolar interactions between nuclear spins.",
        "watermark_text": "Spin signals are observed when the spin network is subjected to two subsequent radio - frequency ( RF ) bursts separated by an interval , known as the pulse splitting rate Tsep . The first RF signal creates a macroscopic magnetization vector M0 that precesses around the external magnetic force Bext at Larmor frequency fL = γBext where γ is gyromagnetic ratio for nuclear spins .After the second RF signal with flip angle θ2 and phase shift φ2 relative to the first one , the transverse component of the magnetization vector M2 ( t ) decays exponentially due to dephasing generated by local magnetic waves created by adjacent nuclei . In this research we show how strong π - pulses can be used to create spin signals even if there is no net initial magnetization present before applying these pulses .We suggest theoretically and experimentally that such spin signals originate intrinsically from the dipolar relationships between nuclear spins .",
        "rewrite_text": "Spin signals are generated when a spin network is exposed to two consecutive radio-frequency (RF) bursts, with a time interval known as the pulse splitting rate Tsep between them. The initial RF burst induces a macroscopic magnetization vector M0 that precesses around the external magnetic field Bext at the Larmor frequency fL = γBext, where γ represents the gyromagnetic ratio for nuclear spins. Following the second RF pulse, which features a flip angle θ2 and a phase shift φ2 relative to the first, the transverse component of the magnetization vector M2(t) experiences exponential decay due to dephasing caused by local magnetic fluctuations from neighboring nuclei. In this study, we present evidence that robust π-pulses can generate spin signals even in the absence of any net initial magnetization prior to their application. Our theoretical and experimental findings suggest that these spin signals stem intrinsically from the dipolar interactions among nuclear spins.",
        "ori-fast-z-score": -1.8325416653445783,
        "water-fast-z-score": 3.0542361089076304,
        "rewrite-fast-z-score": 0.11867816581938533
    },
    {
        "original_text": "We present the results of our numerical simulations of magnetorotational collapse (MRC) in primordial stars with initial masses between 100 and 1000 M⊙, which are formed at redshifts z = 20 − 30. We find that for all models considered here, magnetic fields play an important role during the formation process of black holes. The final mass of the central object is determined by the strength of the magnetic field. For weak magnetic fields (B < 10^10 G), we obtain stellar-mass black holes; while for stronger fields (B > 10^{10}G), supermassive black holes form. In addition to this effect on the final mass, magnetic fields also affect the angular momentum distribution inside the collapsing star. This leads to different spin parameters of the resulting black hole depending on its progenitor s initial mass. \n \n Keywords: Black Hole, Primordial Star Formation, Magnetohydrodynamics",
        "watermark_text": "We present the conclusion of our numerical simulations of magnetorotational formation ( MRC ) in primordial stars with initial masses between 100 and 1000 [UNK] , which are created at redshifts z = 20 − 30 . We see that for all models discussed here , magnetic fields play an important role during the formation period of red holes .The final mass of the main object is chosen by the strength of the magnetic force . For weak magnetic fields ( B < 10 ^ 10 G ) , we obtain stellar - weight blue holes ; while for heavier fields ( B > 10 ^ { 10 } G ) , supermassive black holes create .In addition to this effect on the finished mass , magnetic waves additionally affect the angular velocity distribution inside the falling star . This leads to different spinning characteristics of the resulting black hole depending on its progenitor s initial mass .Keywords: Black Hole, Primordial Star Formation, Magnetohydrodynamics",
        "rewrite_text": "We present the findings from our numerical simulations of magnetorotational collapse (MRC) in primordial stars with initial masses ranging from 100 to 1000 solar masses, formed at redshifts of z = 20–30. Our results indicate that magnetic fields significantly influence the formation phase of red holes across all models examined. The ultimate mass of the primary object is determined by the strength of the magnetic forces. In scenarios with weak magnetic fields (B < 10^10 G), we observe the formation of stellar-mass blue holes, whereas stronger fields (B > 10^10 G) lead to the creation of supermassive black holes. Furthermore, magnetic waves also impact the distribution of angular velocity within the collapsing star, resulting in varying spin characteristics of the resulting black hole that correspond to the initial mass of its progenitor.  \n**Keywords:** Black Hole, Primordial Star Formation, Magnetohydrodynamics",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": -0.24253562503633297
    },
    {
        "original_text": "We study the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial-vector or tensor meson in the framework of QCD factorization with generalized form factors at large recoil.  We find that, although the branching fractions are small due to the helicity suppression, these processes can be used as probes of new physics beyond the Standard Model through their CP asymmetries. \nPACS numbers: 11.15.Tk, 12.38.Qk, 13 .25.Hw \nI. INTRODUCTORY REMAR K\nIn this work we will consider the following two types of charmless hadronic:  B → S V (S = P , A 0 ;V = T 1 )andB → SV(S=P;V=A1). The first type is characterized by one light quark in the final state while the second has no light quarks in it. In both cases there is only one spectator quark which leads to a helicity suppression of the corresponding decay rates. However, they may still serve as useful probes of new physics since their CP-violating asymmetries could be enhanced significantly compared to those of other modes  1  .\nTheoretically, such decays have been studied within various approaches including naive factorization  2  , perturbative QCD  3  , soft-collinear effective theory  4  , and QCD factorization  5  -  8  . It was found that the predictions based on different methods differ substantially among themselves. For example, using naive factorization, Ref.  2  predicted Br(B − →K * 0 π − )/Br(B − →Kπ)=0.27 ±0.04, whereas Refs.  6, 7  obtained values around 0.1−0.2. This discrepancy indicates that more theoretical efforts should be made before drawing any definite conclusion about these decays.",
        "watermark_text": "We research the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial - vector or vector meson in the framework of QCD factorization with generalized form factors at large recoil . We see that , although the branching fractions are small owing to the helicity suppression , these mechanisms can be used as probes of new dynamics beyond the Standard Model through their CP asymmetries .PACS numbers : 11 . 15 . Tk , 12 . 38 . Qk , 13 . 25 . Hw I . INTRODUCTORY REMAR K In this study we will explore the following two forms of charmless hadronic : B → S V ( S = P , A 0 ; V = T 1 ) andB → SV ( S = P ; V = A1 ) .The first sort is characterized by one light quark in the last position while the second has no light quarks in it . In both cases there is only one spectator quark which results to a helicity suppression of the associated decay rates .However , they may still provide as helpful probes of new theory since their CP - breaking asymmetries may be enhanced considerably compared to those of other modes 1 . Theoretically , such decays have been studied within various approaches including naive factorization 2 , perturbative QCD 3 , soft - collinear effective theory 4 , and QCD factorization 5 - 8 .It was shown that the estimates based on various methods varies dramatically among themselves . For instance , using naive factorization , Ref .2 observed Br ( B − →K * 0 π − ) / Br ( B − →Kπ ) = 0 . 27 ±0 . 04 , whereas Refs . 6 , 7 obtained values around 0 . 1−0 . 2 .This discrepancy implies that more theoretical efforts should be made before drew any explicit conclusion about these decays .",
        "rewrite_text": "In this study, we investigate the decay amplitudes for charmless hadronic B decays into a scalar meson paired with either an axial-vector or vector meson, applying the principles of QCD factorization along with generalized form factors at large recoil. While the branching fractions are low due to helicity suppression, these decay modes may serve as valuable indicators of new physics beyond the Standard Model through their CP asymmetries. Our analysis focuses on two specific types of charmless hadronic decays: B → SV (with S = P, A0 and V = T1) and B → SV (with S = P and V = A1). The first category is defined by the presence of one light quark in the final state, while the second does not feature any light quarks. In both scenarios, only a single spectator quark is involved, leading to helicity suppression of the decay rates. Despite this, the CP-violating asymmetries in these decays may be significantly larger than those observed in other decay modes, offering potential insights into new theoretical frameworks. Existing theoretical studies have approached these decays through a variety of methodologies, including naive factorization, perturbative QCD, soft-collinear effective theory, and QCD factorization. Notably, estimates yielded by different techniques can vary widely. For example, naive factorization reported a branching ratio of Br(B− → K*0 π−) / Br(B− → Kπ) = 0.27 ± 0.04, while other studies provided values ranging from 0.1 to 0.2. This inconsistency highlights the necessity for further theoretical investigations before reaching definitive conclusions regarding these decay processes.",
        "ori-fast-z-score": 0.3375263702778072,
        "water-fast-z-score": 6.484597134749389,
        "rewrite-fast-z-score": 2.0851441405707476
    },
    {
        "original_text": "We study the phenomenology of string compactifications with large extra dimensions, focusing on supersymmetric particles in the mass range accessible to current experiments at the Large Hadron Collider (LHC). We consider two classes of models that are motivated by recent developments in string theory: weakly coupled heterotic orbifolds and strongly coupled Type IIB orientifold constructions. In both cases we find that there is an interesting interplay between the Kaluza-Klein excitations associated with the extra dimensions and the lightest Standard Model superpartners. For example, in some regions of parameter space it may be possible to produce gluinos or squarks directly via Drell-Yan processes; alternatively, these states can decay into lighter Standard Model superpartners which then cascade down to the LSP neutralino. The resulting collider signatures depend sensitively on the details of the underlying model parameters as well as the number of extra dimensions.",
        "watermark_text": "We research the phenomenology of string compactifications with large extra dimensions , concentrating on supersymmetric particles in the mass range available to recent experiments at the Large Hadron Collider ( LHC ) . We consider two groups of models that are motivated by recent developments in string theory : weakly connected heterotic orbifolds and strongly coupled Type IIB orientifold constructions .In both cases we find that there is an interesting interplay between the Kaluza - Klein excitations associated with the extra dimensions and the lightest Standard Model superpartners . For instance , in some regions of parameter room it could be possible to produce gluinos or squarks directly via Drell - Yan processes ; alternatively , these states can evolve into heavier Standard Model superpartners which then cascade down to the LSP neutralino .The produced collider signatures vary sensitively on the details of the underlying model variables as also as the number of added dimensions .",
        "rewrite_text": "We investigate the phenomenology of string compactifications featuring large extra dimensions, focusing on supersymmetric particles within the mass range accessible to recent experiments at the Large Hadron Collider (LHC). Our study examines two categories of models inspired by recent advancements in string theory: weakly coupled heterotic orbifolds and strongly coupled Type IIB orientifold constructions. In both scenarios, we observe an intriguing interaction between the Kaluza-Klein excitations linked to the extra dimensions and the lightest superpartners of the Standard Model. For example, in certain regions of the parameter space, it may be feasible to directly produce gluinos or squarks through Drell-Yan processes. Alternatively, these states could decay into heavier Standard Model superpartners, which then cascade down to the lightest stable particle, the neutralino. The collider signatures produced are highly sensitive to the specific parameters of the underlying model, as well as the number of extra dimensions involved.",
        "ori-fast-z-score": 0.9271726499455306,
        "water-fast-z-score": 5.165676192553671,
        "rewrite-fast-z-score": -0.1259881576697424
    },
    {
        "original_text": "We present an algorithm for consistent hypothesis testing in which we consider all possible hypotheses that are compatible with some given set of observations, and select those that maximize their posterior probability according to Bayes  theorem.  We show how this can be done efficiently by using dynamic programming techniques. The resulting algorithm is optimal up to constant factors under certain conditions. Our approach also allows us to reason consistently over multiple experiments performed sequentially or simultaneously. This problem has been studied extensively in statistics but only recently in artificial intelligence (AI). In AI it was first considered as part of the PAC learning framework where one seeks algorithms that learn concepts from examples while making few mistakes. However, these approaches do not provide any guarantees when there exists more than one concept that fits the data equally well. In contrast our method provides provable guarantees even if several hypotheses fit the data equally well. Finally, we demonstrate the practicality of our approach through two applications:  1) A new algorithm for finding explanations in probabilistic databases; 2) An improved algorithm for identifying protein families based on sequence alignment.",
        "watermark_text": "We present an algorithm for consistent hypothesis testing in which we investigate all possible hypotheses that are compatible with some particular set of measurements , and select those that maximize their posterior likelihood according to Bayes principle . We see how this can be performed efficiently by using dynamic programming tools .The resulting algorithm is efficient up to constant factors under certain conditions . Our solution therefore allows us to reason consistently over multiple studies performed sequentially or separately .This problem has been studied frequently in statistics but only lately in artificial intelligence ( AI ) . In AI it was first considered as part of the PAC learning framework where one seeks methods that learn concepts from instances while making few errors .However , these perspectives do not offer any promises when there exists more than one concept that fits the information equally perfectly . In comparison our technique provides provable assurance even if several hypotheses fitted the information equally perfectly .Finally , we prove the practicality of our approach through two applications : 1 ) A modern algorithm for finding explanations in probabilistic libraries ; 2 ) An upgraded method for finding protein families using on sequence alignment .",
        "rewrite_text": "We propose an algorithm for consistent hypothesis testing that explores all potential hypotheses consistent with a specific set of measurements and selects those that maximize their posterior likelihood based on Bayes' principle. We demonstrate that this process can be executed efficiently through dynamic programming techniques. The resultant algorithm is efficient up to constant factors under certain conditions, enabling consistent reasoning across multiple studies that may be conducted either sequentially or independently. While this problem has been widely examined in statistics, it has only recently gained attention in artificial intelligence (AI). In AI, it was initially addressed within the PAC learning framework, which seeks to develop methods for learning concepts from instances with minimal errors. However, this approach does not guarantee effective outcomes when multiple concepts can equally explain the data. In contrast, our technique offers provable guarantees even when several hypotheses adequately fit the information. Finally, we demonstrate the practicality of our method through two applications: 1) a modern algorithm for generating explanations in probabilistic libraries, and 2) an enhanced approach for identifying protein families using sequence alignment.",
        "ori-fast-z-score": -2.3849888978799783,
        "water-fast-z-score": 5.848076606885379,
        "rewrite-fast-z-score": -0.10369516947304253
    },
    {
        "original_text": "We present the catalog of host galaxies used by the Laser Interferometer Gravitational-Wave Observatory (LIGO) to search for gravitational waves associated with compact binary coalescences, including black hole-neutron star and neutron star-black hole binaries.  The catalog contains information on all known host galaxies within the sensitivity volume of the first-generation Advanced LIGO detectors during their initial science runs between September 2005 and January 2010. We provide positions, redshifts, distances, and references for each galaxy. This is an update to our previous publication  1  . \nThe data are available at http://arxiv.org/abs/1306.5481. They can also be downloaded as a text file or via the Zenodo repository  2  .\nAuthor Information\n\nCorresponding Author:\nE-mail: jennifer.marrone@ligo.gov; Jennifer Marrone, National Science Foundation",
        "watermark_text": "We present the catalog of host galaxies utilized by the Laser Interferometer Gravitational - Wave Observatory ( LIGO ) to search for gravitational waves related with compact binary coalescences , notably black hole - neutron star and neutron star - black hole binaries . The collection contains information on all known host galaxies within the sensitivity volume of the first - generation Advanced LIGO detectors during their early scientific ran between September 2005 and January 2010 .We derive positions , redshifts , distances , and references for each galaxy . This is an update to our previous publish 1 .The data are available at http : / / arxiv . org / abs / 1306 . 5481 . They can also be downloaded as a text file or via the Zenodo repository 2 .Author Information Corresponding Author : E - mail : jennifer . marrone @ ligo . gov ; Jennifer Marrone , National Science Foundation",
        "rewrite_text": "We introduce a catalog of host galaxies used by the Laser Interferometer Gravitational-Wave Observatory (LIGO) to investigate gravitational waves associated with compact binary coalescences, specifically black hole-neutron star and neutron star-black hole binaries. This compilation encompasses all known host galaxies situated within the sensitivity volume of the first-generation Advanced LIGO detectors during their initial scientific run from September 2005 to January 2010. For each galaxy, we provide data on positions, redshifts, distances, and pertinent references. This serves as an update to our prior publication. The data is accessible at http://arxiv.org/abs/1306.5481 and can also be downloaded as a text file or through the Zenodo repository. For further inquiries, please contact the corresponding author, Jennifer Marrone, by email at jennifer.marrone@ligo.gov, National Science Foundation.",
        "ori-fast-z-score": 0.42008402520840293,
        "water-fast-z-score": 2.3804761428476167,
        "rewrite-fast-z-score": -0.2672612419124244
    },
    {
        "original_text": "We study the quantum noise properties of gravitational wave detectors with signal recycling mirror (SRM). We show that, for an interferometer with SRM, there is no fundamental limit on its sensitivity at high frequencies due to vacuum fluctuations. The shot-noise limited sensitivity can be improved by increasing the circulating power inside the cavity formed between the two arms of the detector. This improvement comes at the expense of increased thermal noise associated with the mirrors forming this cavity. In addition we find that the effect of radiation pressure noise depends strongly on whether or not the input test mass is suspended. Finally, we discuss how these results may be used to improve existing designs of advanced LIGO-like detectors. PACS numbers: 04.80.Nn, 95.35.+d, 98.80.Cq Quantum noise limits the performance of current generation ground-based gravitationalwave detectors such as Advanced LIGO  1  . These detectors are based on Michelson-interferometric configurations which use Fabry-Perot cavities to enhance their sensitivity  2  . However, it has been shown recently  3  , using semi-classical analysis, that the shot-noise limited sensitivity of these detectors cannot be further improved beyond certain frequency range without introducing additional technical noise sources into the system. It was also suggested  4  that one way to overcome this problem could be to introduce another mirror called  signal recycling  mirror (SRM) into the optical path of the interferometer. Using this approach, it should be possible to increase the circulating power within the interferometer while keeping the same level of shot-noise limited sensitivity achieved before adding the SRM.",
        "watermark_text": "We research the quantum noise characteristics of gravitational wave detectors with signal recycling window ( SRM ) . We see that , for an interferometer with SRM , there is no profound limit on its sensitivity at high frequencies owing to vacuum fluctuations .The shot - noise limited quality can be improved by increasing the circulating force inside the cavity formed between the two arms of the sensor . This improvement comes at the cost of enhanced thermal noise identified with the mirrors forming this cavity .In addition we find that the impact of radiation volume noise depends strongly on whether or not the input test mass is suspended . Finally , we explain how these results may be used to upgrade existing models of advanced LIGO - like detectors .PACS codes : 04 . 80 . Nn , 95 . 35 . + d , 98 . 80 . Cq Quantum noise limits the performance of recent generation ground - based gravitationalwave detectors such as Advanced LIGO 1 . These detectors are based on Michelson - interferometric designs which use Fabry - Perot cavities to expand their sensitivity 2 .However , it has been shown recently 3 , using semi - classical study , that the shot - noise limited accuracy of these detectors cannot be further increased beyond particular wavelength range without introducing additional technical sound sources into the device . It was also suggested 4 that one means to overcome this situation could be to introduce another glass called wave filtering mirror ( SRM ) into the optical track of the interferometer .Using this methodology , it should be possible to expand the circulating capacity within the interferometer while maintaining the same level of shooting - noise limited accuracy achieved before add the SRM .",
        "rewrite_text": "We investigate the quantum noise properties of gravitational wave detectors utilizing a signal recycling mirror (SRM). Our findings indicate that for an interferometer equipped with an SRM, there are no significant limitations on its sensitivity at high frequencies due to vacuum fluctuations. The quality limited by shot noise can be enhanced by increasing the circulating force within the cavity formed by the two arms of the detector. However, this enhancement leads to an increase in thermal noise associated with the mirrors in the cavity. Additionally, we observe that the influence of radiation volume noise is significantly affected by whether the input test mass is suspended. Lastly, we discuss how these insights can be applied to refine existing models of advanced detectors similar to LIGO. PACS codes: 04.80.Nn, 95.35+d, 98.80.Cq Quantum noise constrains the performance of state-of-the-art ground-based gravitational wave detectors like Advanced LIGO. These detectors employ Michelson-interferometric configurations with Fabry-Pérot cavities to enhance their sensitivity. Recent semi-classical studies have demonstrated that the shot-noise limited precision of these detectors cannot be improved beyond a certain wavelength range without introducing additional technical noise sources. Furthermore, it has been proposed that adding a new optical element, known as a wave-filtering mirror (SRM), to the interferometer's optical path could help mitigate this issue. This strategy may allow for an increase in the circulating capacity of the interferometer while achieving the same level of shot-noise limited accuracy as before the introduction of the SRM.",
        "ori-fast-z-score": 0.45454545454545453,
        "water-fast-z-score": 7.967145651741631,
        "rewrite-fast-z-score": 1.5716505559714824
    },
    {
        "original_text": "We study non-commutative Donaldson–Thomas invariants for Calabi-Yau threefolds in terms of their crepant resolutions, which are called conifolds. We show that these invariants can be computed by counting holomorphic curves on the resolved spaces with suitable boundary conditions. As an application we compute the Donaldson–Thomas invariant of the quintic threefold using its crepant resolution as well as the mirror quintic. The results agree with those obtained previously via other methods. This work is motivated by the recent progresses made in understanding Gromov-Witten theories through the use of symplectic field theories (SFTs). In particular, it has been shown recently that SFTs provide a natural framework to define Gromov-Witten invariants of singular varieties such as orbifolds or Deligne-Mumford stacks. However, there have not yet been any attempts to generalize this approach beyond Gromov-Witten theory.",
        "watermark_text": "We research non - commutative Donaldson – Thomas invariants for Calabi - Yau threefolds in terms of their crepant resolutions , which are called conifolds . We see that these invariants can be computed by counting holomorphic curves on the resolved spaces with suitable boundary constraints .As an implementation we compute the Donaldson – Thomas invariant of the quintic threefold using its crepant resolution as well as the mirror quintic . The results agree with those achieved prior via other methods .This research is prompted by the recent progresses done in understanding Gromov - Witten theories through the using of symplectic field theories ( SFTs ) . In particular , it has been shown lately that SFTs allow a natural framework to define Gromov - Witten invariants of singular varieties such as orbifolds or Deligne - Mumford stacks .However , there have not already been any efforts to generalize this methodology beyond Gromov - Witten theory .",
        "rewrite_text": "We investigate non-commutative Donaldson-Thomas invariants for Calabi-Yau threefolds in relation to their crepant resolutions, referred to as conifolds. Our findings indicate that these invariants can be calculated by counting holomorphic curves in the resolved spaces with appropriate boundary conditions. As a practical application, we compute the Donaldson-Thomas invariant of the quintic threefold using its crepant resolution and the mirror quintic, with results aligning with those obtained through previous methods. This research is motivated by recent advances in understanding Gromov-Witten theories, particularly the use of symplectic field theories (SFTs) to define Gromov-Witten invariants for singular varieties, including orbifolds and Deligne-Mumford stacks. However, there has yet to be any attempt to extend this methodology beyond Gromov-Witten theory.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.041451884327381,
        "rewrite-fast-z-score": -0.7453559924999299
    },
    {
        "original_text": "We present new observations and analysis of the neutral hydrogen (HI) disks surrounding isolated galaxies, using data obtained with the Very Large Array (VLA). We have observed 12 nearby galaxies at 21 cm wavelength to determine their total HI mass and distribution within the optical disk. The sample includes both late-type spirals and dwarf irregulars. Our results show that all but one galaxy has an extended HI halo beyond its optical radius; this is true even for the most gas-rich systems such as NGC 4254 and NGC 5253. In addition, we find evidence for significant amounts of atomic gas outside our detection limits which may be associated with tidal features or other interactions between these galaxies and neighboring companions. These findings are consistent with previous studies showing that many isolated spiral galaxies contain large quantities of cold interstellar medium distributed over several kiloparsecs. \n \n Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "watermark_text": "We report new studies and investigation of the neutral hydrogen ( HI ) disks comprising isolated stars , using data acquired with the Very Large Array ( VLA ) . We have noted 12 nearby galaxies at 21 cm wavelength to obtain their total HI mass and distribution within the optical disk .The sample comprises both late - class spirals and dwarf irregulars . Our results show that all but one star has an extended HI halo beyond its optical diameter ; this is true even for the most gas - rich complexes such as NGC 4254 and NGC 5253 .In addition , we find proof for significant amounts of atomic liquid outside our detection limits which may be correlated with tidal features or other relationships between these objects and surrounding companions . These conclusions are compatible with previous research indicating that several isolated spiral clusters hold significant amounts of cold interstellar medium spread over several kiloparsecs .Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "rewrite_text": "We present new findings from our studies of neutral hydrogen (HI) disks surrounding isolated stars, based on data collected with the Very Large Array (VLA). We examined 12 nearby galaxies at a wavelength of 21 cm to determine their total HI mass and distribution within the optical disk. The sample includes both late-type spiral galaxies and dwarf irregular galaxies. Our findings indicate that nearly all stars, except one, possess an extended HI halo that exceeds their optical diameter; this holds true even for gas-rich regions like NGC 4254 and NGC 5253. Furthermore, we have identified substantial amounts of atomic hydrogen outside our detection limits, which may be linked to tidal features or other connections between these galaxies and their nearby companions. These results align with earlier studies suggesting that several isolated spiral clusters contain considerable quantities of cold interstellar medium distributed over several kiloparsecs.  \nKeywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 5.969098507002659,
        "rewrite-fast-z-score": -0.9058216273156765
    },
    {
        "original_text": "The lattice Boltzmann method (LBM) is an alternative approach for solving the Navier-Stokes equations in fluid dynamics, which has been widely used due to its advantages over traditional numerical methods such as finite difference and finite element approaches.  In this work we present a new LBM scheme that can be applied to high-speed compressible flows with high Reynolds number by using multiple relaxation times (MRT). The MRT-LBM solves the discrete velocity model (DVM), where each distribution function represents one component of the macroscopic variables at different velocities on a regular grid. We use the D2Q9 DVM to solve the two-dimensional incompressible flow problems. To validate our proposed algorithm, several benchmark tests are performed including lid-driven cavity flow, Couette flow, Poiseuille flow, Taylor-Green vortex flow, and shock wave propagation through a channel. Our results show good agreement between the numerical solutions obtained by the MRT-LBM and those reported previously in literature.",
        "watermark_text": "The lattice Boltzmann technique ( LBM ) is an additional method for solving the Navier - Stokes equations in flow dynamics , which has been widely using due to its benefits over traditional numerical methods such as finite difference and finite element approaches . In this study we present a new LBM technique that can be applied to large - speed compressible flows with high Reynolds number by using multiple relaxation times ( MRT ) .The MRT - LBM solves the discrete momentum system ( DVM ) , where each distribution function reflects one element of the macroscopic parameters at different velocities on a regular grid . We use the D2Q9 DVM to solve the two - dimensional incompressible flow difficulties .To validate our proposed algorithm , various benchmark tests are performed including lid - driven cavity flow , Couette fluid , Poiseuille flow , Taylor - Green vortex flow , and blast wave propagation through a channel . Our results show good agreement between the numerical answers obtained by the MRT - LBM and those published previously in literature .",
        "rewrite_text": "The lattice Boltzmann method (LBM) is an alternative approach for solving the Navier-Stokes equations in fluid dynamics, gaining popularity due to its advantages over conventional numerical techniques like finite difference and finite element methods. In this research, we introduce a novel LBM technique suitable for high-speed compressible flows with elevated Reynolds numbers, implemented through multiple relaxation times (MRT). The MRT-LBM addresses the discrete velocity model (DVM), where each distribution function corresponds to a specific macroscopic parameter at various velocities on a structured grid. We utilize the D2Q9 DVM to tackle two-dimensional incompressible flow challenges. To corroborate our proposed algorithm, we conduct several benchmark tests, including lid-driven cavity flow, Couette flow, Poiseuille flow, Taylor-Green vortex flow, and blast wave propagation in a channel. Our results demonstrate a strong correlation between the numerical outcomes obtained from the MRT-LBM and previously published data in the literature.",
        "ori-fast-z-score": 1.462614271203831,
        "water-fast-z-score": 5.737948294722722,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new observations of the distant galaxy cluster RX J1117.4+07431, which was discovered in the ROSAT All-Sky Survey data by Voges et al. (1999) . The cluster is located at redshift z = 0.485 ± 0.001 with an estimated mass M500 = 1.7 × 1013 h-1M⊙ within r500 = 2.1h-1Mpc . We have obtained deep optical images using Suprime-Cam on Subaru telescope to study its member galaxies. In addition we observed this cluster with Chandra ACIS-I for about 50 ks. Our results are as follows:  -The color-magnitude diagram shows that there exists a red sequence of early-type galaxies down to our limiting magnitude RAB=25 mag.  -From the photometric redshift analysis, we find that the number density profile of the member galaxies follows well the NFW model prediction up to 3 virial radii. -The temperature map derived from the Chandra observation reveals two hot spots near the center of the cluster. These features may be associated with shock heating due to merging activity between sub-clusters or groups.",
        "watermark_text": "We present new observations of the distant galaxy cluster RX J1117 . 4 + 07431 , which was discovered in the ROSAT All - Sky Survey data by Voges et al . ( 1999 ) .The cluster is situated at redshift z = 0 . 485 ± 0 . 001 with an estimated mass M500 = 1 . 7 × 1013 h - [UNK] within r500 = 2 . 1h - 1Mpc . We have achieved deep optical images using Suprime - Cam on Subaru observatory to study its member galaxies .In addition we studied this cluster with Chandra ACIS - I for about 50 ks . Our results are as follows : - The color - magnitude diagram indicates that there exists a red series of early - class stars down to our limiting magnitude RAB = 25 mag .- From the photometric redshift studies , we find that the number density profile of the member galaxies resembles better the NFW model prediction up to 3 virial radii . - The temperature diagram derived from the Chandra observation discovers two hot areas near the center of the cluster .These features could be correlated with shock heating due to merging behavior between sub - communities or bands .",
        "rewrite_text": "We present new observations of the distant galaxy cluster RX J1117.4+07431, first identified by Voges et al. (1999) in the ROSAT All-Sky Survey data. This cluster is located at a redshift of z = 0.485 ± 0.001, with an estimated mass of M500 = 1.7 × 10^13 h^-1 within a radius of r500 = 2.1 h^-1 Mpc. We obtained deep optical images using Suprime-Cam at the Subaru Observatory to analyze its member galaxies. Additionally, we studied the cluster with Chandra ACIS-I for approximately 50 ks. Our findings are as follows: the color-magnitude diagram reveals a distinct red sequence of early-type galaxies down to our limiting magnitude of RAB = 25 mag. Photometric redshift analysis indicates that the number density profile of the member galaxies aligns more closely with the NFW model prediction up to three virial radii. Furthermore, the temperature map derived from the Chandra observations identifies two hot regions near the cluster's center, which may be linked to shock heating resulting from merging activity between substructures or groups.",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 3.9524197172898554,
        "rewrite-fast-z-score": 0.23570226039551587
    },
    {
        "original_text": "We present new spectroscopic data for the galaxy cluster Abell115 (z=0.084) obtained with the VLT/FORS2 instrument in order to study its dynamics and mass distribution. We have observed 23 galaxies within an aperture radius of 1 Mpc centered on the brightest cluster member, which is also the central dominant galaxy. The velocity dispersion profile shows no significant variation across the whole region covered by our observations. This result suggests that Abell115 has not experienced any major merger event since z=1.5-2.0. Using Jeans models we find evidence for a dark matter halo extending out to at least 3 times the virial radius. From this analysis we derive a total mass of 2.1(+0.7-0.6)x10^14M_sun inside a sphere of radius R200=850kpc. Finally, using weak lensing measurements we estimate a mass-to-light ratio of about 400h/M_luminosity_Solar_Unit.",
        "watermark_text": "We present new spectroscopic data for the galaxy cluster Abell115 ( z = 0 . 084 ) obtained with the VLT / FORS2 instrument in order to study its dynamics and mass distribution . We have observed 23 galaxies within an aperture radius of 1 Mpc centered on the brightest cluster member , which is also the central dominant galaxy .The velocity dispersion model shows no considerable variation across the whole area covered by our observations . This result suggests that Abell115 has not experienced any big merger event since z = 1 . 5 - 2 . 0 .Using Jeans techniques we find proof for a black material halo reaching out to at least 3 times the virial diameter . From this analysis we derive a total mass of 2 . 1 ( + 0 . 7 - 0 . 6 ) x10 ^ 14M _ sun inside a sphere of radius R200 = 850kpc .Finally , using weak lensing observations we estimate a mass - to - light percentage of about 400h / M _ luminosity _ Solar _ Unit .",
        "rewrite_text": "We present new spectroscopic data for the galaxy cluster Abell 115 (z = 0.084), obtained with the VLT/FORS2 instrument, to investigate its dynamics and mass distribution. Our study includes observations of 23 galaxies within a 1 Mpc radius aperture centered on the brightest cluster member, which also serves as the central dominant galaxy. The velocity dispersion model indicates no significant variation across the observed area, suggesting that Abell 115 has not undergone any major merger events since z = 1.5 - 2.0. Utilizing Jeans techniques, we find evidence for a dark matter halo extending to at least three times the virial diameter. From this analysis, we calculate a total mass of 2.1 (+0.7 -0.6) × 10^14 M_sun within a sphere of radius R200 = 850 kpc. Finally, based on weak lensing observations, we estimate a mass-to-light ratio of approximately 400 h / M_luminosity_Solar_Unit.",
        "ori-fast-z-score": 0.762000762001143,
        "water-fast-z-score": 4.0,
        "rewrite-fast-z-score": 0.12403473458920847
    },
    {
        "original_text": "In this thesis, we study power control problems in cellular mobile radio systems with code division multiple access (CDMA). We consider the uplink scenario where each user transmits to its base station using an orthogonal spreading sequence and all users share the same frequency band. The objective is to minimize the total transmit power subject to individual quality-of-service constraints at each user s receiver. In order to obtain tractable results, we make use of tools from stochastic geometry which allow us to model the locations of both mobiles and interferers as point processes. Our main contributions are summarized below.  First, we derive closed-form expressions for the outage probability when the number of active users grows without bound. These expressions can be used to determine how many users can simultaneously communicate reliably over the network. Second, we propose distributed power control schemes that achieve these limits asymptotically under certain conditions. Third, we develop centralized power control algorithms that guarantee performance close to optimality even if only partial information about the channel gains or interference levels is available. Finally, we present simulation results illustrating our theoretical findings.",
        "watermark_text": "In this dissertation , we study control power problems in cell mobile radio circuits with code division multiple entry ( CDMA ) . We consider the uplink situation where each consumer transmits to its base station use an orthogonal spreading pattern and all users share the same frequency band .The goal is to minimize the total broadcast capacity related to individual quality - of - service restrictions at each consumer s receiver . In order to obtain tractable results , we make using of tools from stochastic geometry which allow us to model the places of both mobiles and interferers as point processes .Our main contributions are presented below . First , we derive closed - form expressions for the outage likelihood when the number of active usage rises without bound .These expressions can be used to predict how many users can independently connect reliably over the network . Second , we develop dispersed power control schemes that attain these limits asymptotically under certain conditions .Third , we develop concentrated control power methods that guarantee efficiency high to optimality even if only partial knowledge about the channel gains or interference concentrations is accessible . Finally , we present modeling results illustrating our theoretical results .",
        "rewrite_text": "In this dissertation, we investigate control power issues in mobile radio circuits utilizing code division multiple access (CDMA). Our focus is on the uplink scenario, where each user sends data to their base station using an orthogonal spreading pattern while sharing the same frequency band. The objective is to minimize the total broadcast capacity while adhering to individual quality-of-service requirements for each user's receiver. To achieve tractable results, we employ tools from stochastic geometry, modeling the locations of both mobile users and interferers as point processes. Our key contributions are outlined as follows: first, we derive closed-form expressions for the outage probability as the number of active users approaches infinity. These expressions can help predict the maximum number of users that can connect reliably to the network. Second, we propose distributed power control strategies that asymptotically reach these limits under specific conditions. Third, we introduce centralized power control techniques that ensure high efficiency and near-optimality, even with only partial knowledge of channel gains or interference levels. Lastly, we provide modeling results that demonstrate our theoretical findings.",
        "ori-fast-z-score": 0.19802950859533489,
        "water-fast-z-score": 7.4,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study the statistical properties of simulated dark matter halos in cosmological N-body simulations, focusing on their shapes and orientations with respect to each other. We find that these quantities are strongly correlated for pairs of halos separated by less than one virial radius (the region within which the density is roughly constant). This correlation persists even when we consider only those pairs whose mutual separation lies along the line-of-sight between them. The correlations can be understood as arising due to tidal forces exerted by neighboring halos. In particular, we show that the distribution of halo shapes depends sensitively upon whether or not they lie close to an axis of symmetry of the local gravitational potential field. Finally, we compare our results against observations of galaxy clusters obtained using weak lensing techniques. Our analysis suggests that the observed cluster morphologies may provide useful constraints on the nature of primordial fluctuations responsible for structure formation in the universe.",
        "watermark_text": "We research the statistical characteristics of virtual dark matter halos in cosmological N - bodies simulations , concentrating on their shapes and orientations with regard to each other . We see that these quantities are strongly correlated for pairs of halos separated by less than one virial diameter ( the region within which the density is approximately zero ) .This coupling persists even when we treat only those couples whose mutual separation lies along the line - of - view between them . The correlations can be understood as occurring due to wave forces exerted by adjacent halos .In particular , we prove that the distribution of halo patterns depends sensitively upon whether or not they lay close to an axis of symmetry of the local gravity potential field . Finally , we compare our findings against measurements of galaxy galaxies acquired using weak lensing methods .Our study implies that the known cluster morphologies may provide useful limitations on the nature of primordial fluctuations involved for structure development in the universe .",
        "rewrite_text": "We investigate the statistical properties of virtual dark matter halos in cosmological N-body simulations, focusing on their shapes and orientations relative to one another. Our findings reveal a strong correlation between these characteristics for halo pairs that are less than one virial diameter apart (the region where density is nearly zero). This correlation remains significant even when examining only those pairs whose separation aligns with the line of sight between them. We propose that these correlations arise from the wave forces exerted by neighboring halos. Notably, we demonstrate that the distribution of halo arrangements is highly sensitive to their proximity to an axis of symmetry within the local gravitational potential field. Lastly, we compare our results with galaxy measurements obtained through weak lensing techniques. Our study suggests that the known morphologies of galaxy clusters may provide valuable insights into the nature of primordial fluctuations that contributed to structure formation in the universe.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 6.671345390179443,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We investigate whether we can detect anisotropy in quasar H II regions during reionization through their small-scale redshifted 21 cm power spectrum (21-cm PS). In our model, quasars are assumed to be located at peaks of dark matter density fluctuations and ionize surrounding gas with an anisotropic Strömgren sphere whose shape is determined by the local tidal field. By performing numerical simulations for different values of the spin temperature T S , we find that the 21-cm PS has a characteristic peak structure which reflects the shapes of individual H II regions. This peak structure becomes more prominent as T S decreases because the number of neutral hydrogen atoms increases due to the decrease in the brightness temperature difference between the CMB and the 21-cm emission line. Our results suggest that it may be possible to use this peak structure to constrain the value of T S . However, since there exist many other factors affecting the 21-cm PS besides T S , further studies will be needed before drawing any conclusions on its detectability.",
        "watermark_text": "We explore whether we can identify anisotropy in quasar H II regions during reionization through their tiny - scale redshifted 21 cm power spectrum ( 21 - cm PS ) . In our model , quasars are expected to be found at peaks of dark matter density fluctuations and ionize neighboring plasma with an anisotropic Strömgren ball whose shape is chosen by the local tidal field .By conducting numerical simulations for different values of the spin temperature T S , we find that the 21 - cm PS has a typical peak structure which reflects the shapes of different H II regions . This peak structure becomes more prominent as T S drops because the proportion of neutral hydrogen atoms increases owing to the decrease in the brightness temperature difference between the CMB and the 21 - cm absorption line .Our results propose that it could be possible to use this peak structure to constrain the value of T S . However , since there remain many other influences involving the 21 - cm PS besides T S , further studies will be needed before drew any findings on its detectability .",
        "rewrite_text": "We investigate the potential for detecting anisotropy in quasar H II regions during the reionization epoch by analyzing their small-scale redshifted 21 cm power spectrum (21-cm PS). Our model posits that quasars are situated at the peaks of dark matter density fluctuations and ionize the surrounding plasma, forming an anisotropic Strömgren sphere shaped by the local tidal field. Through numerical simulations with varying spin temperatures (T_S), we observe a characteristic peak structure in the 21-cm PS that reflects the geometries of various H II regions. This peak structure becomes more pronounced as T_S decreases, due to an increase in neutral hydrogen atoms driven by the shrinking brightness temperature difference between the CMB and the 21-cm absorption line. Our findings suggest that this peak structure could potentially be used to estimate the value of T_S. However, given that factors beyond T_S also affect the 21-cm PS, additional research is necessary before drawing definitive conclusions about its detectability.",
        "ori-fast-z-score": 1.7320508075688772,
        "water-fast-z-score": 6.350852961085883,
        "rewrite-fast-z-score": 3.0224386073393013
    },
    {
        "original_text": "We study the dynamics of two interacting bosonic species confined to an optical lattice, with one species being initially prepared as a coherent state at each site while the other is initially prepared as a thermal cloud. We show that this system supports both symmetric and asymmetric soliton solutions which are stable against small perturbations for certain values of the chemical potentials. The stability properties of these solitons can be understood by studying their linearization spectrum around the stationary states. In particular we find that the presence of a finite temperature leads to additional unstable modes associated with phonon-like excitations. Finally, we demonstrate how our results may be used to describe experiments on spinor condensates loaded into optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases consisting of several different atomic species  1  . These systems provide new opportunities to explore novel phenomena such as supersolids  2  , phase separation  3  or spin-orbit coupling  4  .\nIn this work we consider a particularly interesting example where there exist two distinct types of particles (e.g., atoms) which interact via s-wave scattering but differ in mass and/or internal structure  5  . This situation arises naturally when considering mixtures of hyperfine states  6  or isotopes  7, 8  within the same atom type  9  . For instance, recent experiments involving 87 Rb and 41 K  10  have demonstrated the formation of a mixture of two different hyperfine states after evaporative cooling  11  . Another possibility would involve using 40 K and 6 Li  12  . Here, the lighter species could be considered as impurities immersed in a background gas of heavier fermions  13  . Alternatively, if the masses were reversed then the heavy species could act as impurities  14  .",
        "watermark_text": "We research the dynamics of two interacting bosonic species confined to an optical lattice , with one species being initially made as a coherent state at each site while the other is initially prepared as a heat bubble . We see that this scheme holds both symmetric and asymmetric soliton solutions which are stable against small perturbations for particular values of the chemical potentials .The stability properties of these solitons can be understood by examining their linearization spectrum around the stationary states . In particular we find that the presence of a finite temperature leads to extra weak modes associated with phonon - like excitations .Finally , we prove how our findings may be used to explain studies on spinor condensates stacked into optical lattices . Introduction : - Recent research developments have enabled it able to create quantum degenerate gases composed of several different atomic species 1 .These systems present new opportunities to examine novel processes such as supersolids 2 , phase splitting 3 or spin - orbit resonance 4 . In this research we imagine a particularly exciting example where there exist two different kinds of atoms ( e . g . , atoms ) which interact via s - wave scattering but differ in mass and / or internal structure 5 .This condition occurs naturally when examining mixtures of hyperfine states 6 or isotopes 7 , 8 within the same atom type 9 . For instance , recent experiments concerning 87 Rb and 41 K 10 have demonstrated the formation of a mixture of two different hyperfine states after evaporative cooling 11 .Another possibility would include utilizing 40 K and 6 Li 12 . Here , the softer species may be regarded as impurities immersed in a background gas of heavier fermions 13 .Alternatively , if the masses were reversed then the heavy species may function as impurities 14 .",
        "rewrite_text": "We investigate the interactions between two bosonic species confined within an optical lattice, where one species is initially configured as a coherent state at each lattice site, while the other is prepared as a thermal excitation. Our findings reveal that this setup permits both symmetric and asymmetric soliton solutions that remain stable against minor disturbances for specific chemical potential values. The stability of these solitons can be analyzed through their linearization spectrum around the stationary states. Notably, we discover that the introduction of a finite temperature leads to additional weak modes linked to phonon-like excitations. Ultimately, we demonstrate how these results can be applied to enhance our understanding of spinor condensates organized within optical lattices. \n\nIntroduction: Recent advances in the field have made it possible to create quantum degenerate gases composed of multiple atomic species. These systems offer unique opportunities to explore novel phenomena such as supersolids, phase separation, and spin-orbit resonance. In this study, we consider an intriguing scenario in which two distinct types of atoms interact through s-wave scattering while differing in mass or internal structure. This situation often arises in the context of mixtures of hyperfine states or isotopes within the same atomic species. For example, recent experiments with 87Rb and 41K have shown the formation of a mixture of two hyperfine states following evaporative cooling. Another potential combination could involve 40K and 6Li, where the lighter species can be viewed as impurities within a denser background of heavier fermions. Conversely, if the mass roles were reversed, the heavier species could serve as the impurities.",
        "ori-fast-z-score": -0.3481553119113957,
        "water-fast-z-score": 6.5033247714309,
        "rewrite-fast-z-score": 1.1917080461366747
    },
    {
        "original_text": "We consider an additive white Gaussian noise (AWGN) channel with multiple paths, each having different delay and gain coefficients. We assume that these channels are sparse; i.e., there is only one path whose gain coefficient exceeds some threshold value. The capacity region for this class of channels has been studied by several authors under various assumptions on the number of paths and their gains.  In particular, it was shown recently that if all the paths have equal power then the capacity region can be achieved using superposition coding at transmitter and successive decoding at receiver. However, when the powers of the paths vary widely, the optimality of such schemes remains unknown. In this work we show that even though the optimal scheme may not use superposition coding or successive decoding, its performance cannot differ significantly from those schemes. More specifically, we prove upper bounds on the gap between the achievable rates using any scheme and the sum-capacity of the channel.",
        "watermark_text": "We consider an additive white Gaussian sound ( AWGN ) channel with many lanes , each having specific delay and gain values . We assume that these networks are sparse ; i . e . , there is only one path whose gain coefficient exceeds some threshold value .The capacity area for this class of channels has been studied by various scientists under various observations on the number of paths and their benefits . In particular , it was shown lately that if all the paths have equal energy then the ability area can be obtained using superposition coding at transmitter and subsequent decoding at receiver .However , when the powers of the paths differ widely , the optimality of such schemes remains undisclosed . In this research we prove that even though the ideal method may not use superposition coding or successive decoding , its effectiveness cannot differ significantly from those schemes .More specifically , we prove upper limits on the gap between the achievable rates using any scheme and the sum - capacity of the channel .",
        "rewrite_text": "We examine an additive white Gaussian noise (AWGN) channel with multiple paths, each characterized by distinct delay and gain values. We operate under the assumption that these networks are sparse, meaning there is only one path whose gain coefficient surpasses a certain threshold. The capacity region for this type of channel has been analyzed by various researchers with different focus on the number of paths and their advantages. Recently, it has been established that if all paths possess equal energy, the capacity region can be achieved through superposition coding at the transmitter, followed by decoding at the receiver. However, the optimality of such approaches remains unclear when the power levels of the paths vary significantly. In this study, we demonstrate that, although the ideal approach may not involve superposition coding or successive decoding, its performance will not diverge substantially from those methods. More specifically, we provide upper bounds on the difference between the achievable rates of any scheme and the sum-capacity of the channel.",
        "ori-fast-z-score": 0.22645540682891913,
        "water-fast-z-score": 5.737948294722722,
        "rewrite-fast-z-score": -0.11547005383792514
    },
    {
        "original_text": "We study one-way permutation families with respect to their ability to resist certain attacks by adversaries that are computationally restricted in the sense that they can only perform limited number of queries to the permutation family. We show how this notion relates to computational complexity theory and present several new results on the topic. In particular we prove that any one-way permutation family is resistant against an adaptive chosen-ciphertext attack if it resists a non-adaptive one. This result implies that for every one-way permutation there exists another one-way permutation which is secure under both types of attacks simultaneously. Finally, we introduce a novel concept called distortion measure between two one-way permutation families and use it to derive lower bounds on the security parameters required to achieve resistance against different classes of attacks. The research leading to these results has been funded by the European Research Council under the Seventh Framework Programme (FP7/2007-2013) / ERC Grant Agreement n o 339032",
        "watermark_text": "We research one - way permutation families with regard to their ability to resist various attacks by adversaries that are computationally confined in the sense that they can only conduct small number of queries to the permutation family . We see how this phenomenon relates to computational complexity analysis and publish numerous current conclusions on the subject .In particular we prove that any one - way permutation family is resistant against an adaptive chosen - ciphertext attack if it resists a non - adaptive one . This result requires that for every one - way permutation there exists another one - way permutation which is stable under both types of assaults simultaneously .Finally , we approach a new notion called distortion function between two one - way permutation families and use it to derive smaller bounds on the safety parameters essential to achieve resistance against diverse classes of assaults . The studies leading to these results has been sponsored by the European Research Council under the Seventh Framework Programme ( FP7 / 2007 - 2013 ) / ERC Grant Agreement n o 339032",
        "rewrite_text": "We investigate one-way permutation families in terms of their resistance to various attacks conducted by limited adversaries who can execute only a small number of queries to the permutation family. Our exploration highlights the relationship between this phenomenon and computational complexity analysis, and we present several current findings on the topic. Notably, we demonstrate that any one-way permutation family is resilient to adaptive chosen-ciphertext attacks if it can withstand non-adaptive ones. This conclusion necessitates that for every one-way permutation, there exists another one that is stable against both types of attacks simultaneously. Furthermore, we introduce a novel concept known as the distortion function between two one-way permutation families and utilize it to derive tighter bounds on the security parameters necessary for resisting various classes of attacks. This research has been supported by the European Research Council through the Seventh Framework Programme (FP7/2007-2013) under ERC Grant Agreement No. 339032.",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 0.629940788348712
    },
    {
        "original_text": "We present the results on galaxy population in the most massive supercluster, SCl 126 (Abell 1689), based on spectroscopic data obtained with VLT/VIMOS and Keck/DEIMOS telescopes. We find that galaxies are distributed along filaments which connect clusters at different redshifts. The fraction of blue galaxies increases towards lower redshift, while the fraction of early-type galaxies decreases. This trend is more pronounced for bright galaxies than faint ones. In addition to this general picture we also detect some interesting features such as an excess of late-type galaxies around Abell 1689A cluster or a lack of bright galaxies between Abell 1689B and C clusters. These findings suggest that there may be significant differences among galaxy properties within individual clusters depending on their location relative to other structures. Our analysis shows that the observed trends can not be explained by simple passive evolution of stellar populations but require additional mechanisms like mergers and/or interactions.",
        "watermark_text": "We present the results on galaxy population in the most large supercluster , SCl 126 ( Abell 1689 ) , using on spectroscopic data acquired with VLT / VIMOS and Keck / DEIMOS telescopes . We see that galaxies are distributed along filaments which link clusters at different redshifts .The percentage of blue stars increases towards lower redshift , while the fraction of early - class objects decreases . This trend is more pronounced for strong galaxies than dim ones .In addition to this general picture we also observe some interesting features such as an accumulation of late - class objects around Abell 1689A cluster or a lack of bright clusters between Abell 1689B and C clusters . These conclusions show that there may be considerable variations among galaxy structures within individual clusters depending on their placement relative to other structures .Our study shows that the known trends can not be described by simple passive evolution of stars populations but need extra causes like mergers and / or relationships .",
        "rewrite_text": "We present our findings on the galaxy population within the largest supercluster, SCl 126 (Abell 1689), utilizing spectroscopic data obtained from the VLT/VIMOS and Keck/DEIMOS telescopes. Our analysis reveals that galaxies are organized along filaments that connect clusters at various redshifts. We observe an increase in the percentage of blue star galaxies at lower redshifts, accompanied by a decline in the fraction of early-type objects. This pattern is more pronounced in brighter galaxies than in fainter ones. In addition to this overarching trend, we identify notable features such as a concentration of late-type galaxies around the Abell 1689A cluster and a scarcity of bright clusters between the Abell 1689B and C clusters. These results suggest significant variations in galaxy structures within individual clusters, influenced by their spatial relationships with other structures. Our study indicates that the established trends cannot be fully explained by simple passive evolution of stellar populations, but rather require additional factors such as mergers and interactions.",
        "ori-fast-z-score": -2.324952774876386,
        "water-fast-z-score": 5.347391382215687,
        "rewrite-fast-z-score": -0.808290376865476
    },
    {
        "original_text": "We present the discovery and characterization of two  hot Jupiter  planets orbiting stars that are members of wide binaries, HD 196885AB (a = 1.8 AU) and HD 208598AB (a = 3.6 AU). The planet around HD 196885A is an inflated gas giant with M sin i = 0.88 MJup and P = 4.3 days; it orbits its primary at a distance of only 0.04 AU. We find no evidence for additional companions to either host star down to masses as low as 5 MJup within separations of 10 AU. Both systems have orbital eccentricities consistent with zero. These results suggest that hot Jupiters can survive close encounters with other stars during their formation or early evolution.  - Introduction \n \n Hot Jupiters are massive gaseous planets on short-period orbits about solar-type stars. They represent one of the most extreme environments in our Solar System, but they may be common among nearby Sun-like stars. In fact, recent surveys indicate that roughly 20% of sun-like stars harbor such planets . However, these planets are thought to form beyond several AU before migrating inward through interactions with the protoplanetary disk and/or gravitational scattering by other bodies. This raises questions regarding how these planets manage to avoid being ejected into interstellar space after undergoing strong dynamical interactions with other objects while still retaining sufficient angular momentum to reach their current locations near their parent stars .\n\nIn this Letter we report the detection of two new  hot Jupiter  planets using high-precision radial velocity measurements obtained over more than eight years with the High Accuracy Radial Velocity Planet Searcher instrument (HARPS), which is installed on the European Southern Observatory s 3.6-m telescope located at La Silla Observatory in Chile. One of these planets has an extremely small semi-major axis of just 0.04 AU, making it one of the closest known exoplanets to its parent star.",
        "watermark_text": "We present the discovery and description of two hot Jupiter planets orbiting planets that are part of wide binaries , HD 196885AB ( a = 1 . 8 AU ) and HD 208598AB ( a = 3 . 6 AU ) . The planet around HD 196885A is an inflated gas giant with M sin i = 0 . 88 MJup and P = 4 . 3 days ; it orbits its primary at a distance of only 0 . 04 AU .We see no evidence for additional companions to either host star down to masses as low as 5 MJup within separations of 10 AU . Both components have orbital eccentricities consistent with zero .These data suggest that hot Jupiters can endure close contacts with other stars during their development or early evolved . - Introduction Hot Jupiters are enormous gaseous planets on short - duration orbits about solar - class stars .They represent one of the most intense environments in our Solar System , but they may be prevalent among neighboring Sun - like stars . In reality , recent studies confirm that approximately 20 % of sun - like stars harbor such planets .However , these planets are said to form beyond many AU before migrating inward through interactions with the protoplanetary disk and / or gravitational absorption by other bodies . This opens questions regarding how these planets cope to resist being ejected into interstellar space after undergoing stable dynamical interactions with other objects while nevertheless possessing adequate angular velocity to reach their current places near their sister planets .In this Letter we publish the observation of two new warm Jupiter planets using high - precision radial speed measurements obtained over more than eight years with the High Accuracy Radial Velocity Planet Searcher instrument ( HARPS ) , which is installed on the European Southern Observatory s 3 . 6 - m observatory situated at La Silla Observatory in Chile . One of these planets has an incredibly small semi - major axis of just 0 . 04 AU , making it one of the nearest known exoplanets to its parent star .",
        "rewrite_text": "We report the discovery and characterization of two hot Jupiter planets orbiting wide binary systems, HD 196885AB (with a semi-major axis of 1.8 AU) and HD 208598AB (with a semi-major axis of 3.6 AU). The planet orbiting HD 196885A is an exceptionally inflated gas giant, with a mass of M sin i = 0.88 MJup and an orbital period of 4.3 days; it orbits its host star at a remarkably close distance of just 0.04 AU. We find no evidence of additional companions around either host star, down to masses as low as 5 MJup within a 10 AU separation. Both stars exhibit orbital eccentricities consistent with being close to zero. These findings imply that hot Jupiters may survive close encounters with other stars during their formation or early evolutionary stages.\n\n**Introduction**: Hot Jupiters are large gaseous planets with short orbital periods around sun-like stars. They represent some of the most extreme environments in our Solar System and are thought to be common among nearby solar-type stars. Recent studies indicate that approximately 20% of sun-like stars have such planets. However, these planets are believed to form at distances far beyond the inner solar system before migrating inward through interactions with the protoplanetary disk or via gravitational interactions with other bodies. This raises questions about their ability to avoid ejection into interstellar space after dynamic encounters while maintaining sufficient angular momentum to settle into their current orbits near their host stars. In this Letter, we provide observations of two new warm Jupiter planets, utilizing high-precision radial velocity measurements collected over more than eight years with the High Accuracy Radial Velocity Planet Searcher (HARPS), located at the 3.6-meter telescope at the La Silla Observatory in Chile. Notably, one of these planets has an extremely close semi-major axis of just 0.04 AU, making it one of the closest known exoplanets to its host star.",
        "ori-fast-z-score": -1.7960530202677492,
        "water-fast-z-score": 6.286185570937122,
        "rewrite-fast-z-score": -0.7689218919450849
    },
    {
        "original_text": "We study the evolution of magnetized, rotating flows in the presence of strong magnetic fields and rotation using 3D numerical simulations with ideal MHD equations. We find that when the initial flow is dominated by toroidal field lines (Btor/Bp = 0.5), it becomes unstable to non-axisymmetric perturbations at t ~ 1.2P0/c where P0 is the initial pressure scale height. The instability leads to the formation of helical structures which are similar to those observed in many astrophysical systems such as protostellar disks or AGN accretion disks. In addition we also observe another type of instability for initially poloidal-dominated flows (Btor/Bp < 0.1) which develops into an axisymmetric spiral structure. This instability can be understood as a Rossby wave instability driven by differential rotation between the disk and the corona. Finally, we show that these two types of instabilities lead to different observational signatures.",
        "watermark_text": "We research the evolution of magnetized , moving flows in the presence of large magnetic fields and rotation utilizing 3D numerical simulations with ideal MHD equations . We see that when the first flow is dominated by toroidal field lines ( Btor / Bp = 0 . 5 ) , it becomes unstable to non - axisymmetric perturbations at t ~ 1 . 2P0 / c where P0 is the initial pressure scale length .The instability leads to the formation of helical structures which are comparable to those observed in many astrophysical systems such as protostellar disks or AGN accretion disks . In addition we also observe another type of instability for initially poloidal - dominated streams ( Btor / Bp < 0 . 1 ) which becomes into an axisymmetric spiral shape .This instability can be understood as a Rossby wave disturbance driven by differential rotation between the disk and the corona . Finally , we find that these two forms of instabilities lead to different observational signatures .",
        "rewrite_text": "Our research focuses on the evolution of magnetized, moving flows under the influence of strong magnetic fields and rotation, using 3D numerical simulations based on ideal magnetohydrodynamics (MHD) equations. We observe that when the initial flow is primarily governed by toroidal magnetic field lines (with Btor / Bp = 0.5), it becomes susceptible to non-axisymmetric perturbations at approximately t ~ 1.2P0 / c, where P0 represents the initial pressure scale length. This instability results in the development of helical structures, similar to those found in various astrophysical contexts, including protostellar disks and active galactic nucleus (AGN) accretion disks. Additionally, we detect a different type of instability in streams predominantly characterized by poloidal fields (where Btor / Bp < 0.1), which evolves into an axisymmetric spiral configuration. This phenomenon can be interpreted as a Rossby wave disturbance induced by the differential rotation between the disk and the corona. Ultimately, we conclude that these two instability types produce distinct observational signatures.",
        "ori-fast-z-score": 0.508000508000762,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 0.9428090415820635
    },
    {
        "original_text": "The article is devoted to the problem of possible existence of dark matter particles in our Galaxy, which are not detected by other methods than their gravitational effects on visible objects (stars). The author considers the possibility that these hypothetical particles can be described as celestial mechanics daemons with certain properties. In particular, it is shown how such daemons could explain some features observed recently for the DAMA experiment at Gran Sasso National Laboratory. It should be noted that this explanation does not contradict any known experimental data. However, there are also serious difficulties associated with the proposed model. These problems will require further study. This work was supported by Russian Science Foundation grant No 14-50-00040. URL: http://arxiv.org/abs/1409.5189 . \nI. INTRODUCTORY REMARK .\nDark Matter (DM) is one of the most important mysteries of modern physics  1  -  4  . Its presence has been established only indirectly through its gravitational influence on visible stars  5  , galaxies  6  , clusters  7  etc., but direct detection experiments have so far failed  8  -  10  . There exist many theoretical models describing DM  11  -  13  ; however, none of them has yet been confirmed experimentally  14  . One of the possibilities is that DM consists of new elementary particles  15  -  17  . If they interact weakly or electromagnetically with ordinary matter then they would escape detection even if they were produced in large quantities  18  . On the other hand, if they interact strongly enough with normal matter, then they may be detectable directly  19  -  21  . A number of experiments searching for DM particles have been carried out  22  -  26  . Recently, the results obtained by the DAMA collaboration  27  attracted considerable attention  28  -  30  . According to these results, the annual modulation effect  31  -  33  caused by the motion of Earth around Sun  34  -  36  leads to an increase in the rate of nuclear recoils registered by detectors during June-October period  37  compared to December-February period. Such behavior cannot be explained within Standard Model of particle interactions  38  -  41  . Several authors suggested different explanations based on",
        "watermark_text": "The essay is devoted to the question of possible existence of dark matter molecules in our Galaxy , which are not observed by other methods than their gravitational impacts on visible objects ( stars ) . The author considers the prospect that these hypothetical particles can be described as celestial mechanics daemons with certain characteristics .In particular , it is demonstrated how such daemons might explain some features detected lately for the DAMA experiment at Gran Sasso National Laboratory . It should be mentioned that this explanation does not contradict any established experimental evidence .However , there are also serious difficulties linked with the suggested model . These difficulties will demand further study .This project was supported by Russian Science Foundation program No 14 - 50 - 00040 . URL : www : / / arxiv . org / abs / 1409 . 5189 .I.INTRODUCTORY REMARK .Dark Matter ( DM ) is one of the most important wonders of modern physics 1 - 4 . Its presence has been known only indirectly through its gravitational impact on visible stars 5 , galaxies 6 , galaxies 7 etc . , but direct detection experiments have so far unsuccessful 8 - 10 .There exist many theoretical theories describing DM 11 - 13 ; however , none of them has already been confirmed experimentally 14 . One of the possibilities is that DM consists of new primary nucleus 15 - 17 .If they interact weakly or electromagnetically with everyday matter then they may survive observation even if they were produced in large quantities 18 . On the other hand , if they interact heavily enough with normal matter , then they may be detectable directly 19 - 21 .A variety of studies looking for DM nuclei have been carried out 22 - 26 . Recently , the results derived by the DAMA collaboration 27 drew substantial scrutiny 28 - 30 .According to these results , the annual modulation effect 31 - 33 induced by the movement of Earth around Sun 34 - 36 results to an increase in the frequency of nuclear recoils registered by detectors during June - October year 37 contrast to December - February time . Such action cannot be described within Standard Model of particle particles 38 - 41 .Several articles suggested different explanations based on",
        "rewrite_text": "This essay explores the potential existence of dark matter molecules within our Galaxy, which remain undetected by methods other than their gravitational influence on visible objects such as stars. The author speculates that these hypothetical particles could be represented as celestial mechanics entities with particular attributes. Specifically, it illustrates how these entities might account for recent findings from the DAMA experiment at the Gran Sasso National Laboratory. Notably, this explanation aligns with established experimental evidence. However, significant challenges associated with the proposed model also exist, necessitating further investigation. This project received support from the Russian Science Foundation under program No. 14-50-00040. For further details, visit: www.arxiv.org/abs/1409.5189.\n\n**I. INTRODUCTORY REMARKS.** Dark Matter (DM) represents one of the most intriguing mysteries in modern physics. Its existence is known only indirectly through gravitational effects on visible stars, galaxies, and other cosmic structures; direct detection efforts have yet to produce results. Numerous theoretical frameworks have been put forth to explain DM, but none have been experimentally validated to date. One possibility suggests that DM could consist of new, primary nuclei. If these particles interact weakly or electromagnetically with ordinary matter, they might evade observation despite potentially being produced in significant quantities. Conversely, if they interact strongly with normal matter, they may be directly detectable. Various studies have sought to identify DM nuclei, and recently, the findings from the DAMA collaboration have garnered considerable attention. According to their results, an annual modulation effect caused by Earth's orbit around the Sun leads to an increased frequency of nuclear recoils detected between June and October, in contrast to the December to February period. This phenomenon cannot be adequately explained within the framework of the Standard Model of particle physics. Several papers have proposed alternative explanations based on this observation.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.780541105074454,
        "rewrite-fast-z-score": 1.2722833945199565
    },
    {
        "original_text": "The following is an excerpt from the author s thesis, which can be downloaded at http://arxiv.org/abs/1306.5189. \n \n The formula evaluation problem for propositional logic formulas in conjunctive normal form (CNF) has been studied extensively and many efficient algorithms have been developed. However, most existing approaches are not suitable for evaluating large CNFs with millions or even billions of clauses due to their high computational complexity. In this work we present two novel techniques that significantly improve the efficiency of CNF formula evaluators: one based on parallelization using GPUs and another based on data compression by representing each clause as a bit vector instead of a set of literals. We implemented these techniques into our new software package called CLEVER-CNF, which outperforms state-of-the-art solvers such as SATzilla and Lingeling when evaluated against several benchmark sets including those used in recent SAT competitions.",
        "watermark_text": "The following is an excerpt from the writer s thesis , which can be downloaded at http : / / arxiv . org / abs / 1306 . 5189 . The formula evaluation problem for propositional logic formulas in conjunctive normal form ( CNF ) has been studied thoroughly and many efficient methods have been built .However , most existing techniques are not suitable for evaluating large CNFs with thousands or maybe billions of clauses due to their high computational complexity . In this research we present two novel techniques that significantly improve the performance of CNF formula evaluators : one based on parallelization use GPUs and another based on data encoding by representing each sentence as a bit vector rather of a group of literals .We introduced these techniques into our new software suite named CLEVER - CNF , which outperforms state - of - the - art solvers such as SATzilla and Lingeling when evaluated against several benchmark sets including those utilized in recent SAT tests .",
        "rewrite_text": "The following excerpt is taken from the author's thesis, available for download at http://arxiv.org/abs/1306.5189. The evaluation of propositional logic formulas in conjunctive normal form (CNF) has been extensively researched, leading to the development of various efficient methods. However, many existing approaches struggle with large CNFs that contain thousands or even billions of clauses due to their significant computational demands. In this study, we introduce two innovative techniques that greatly enhance the performance of CNF formula evaluators: one utilizes GPU-based parallel processing, while the other employs data encoding by representing each clause as a bit vector instead of a collection of literals. These techniques have been incorporated into our new software suite, CLEVER-CNF, which outperforms leading solvers like SATzilla and Lingeling when tested against several benchmark sets, including those used in recent SAT competitions.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "The composition of cosmic rays is studied by measuring their energy spectrum and mass distribution at Earth. The most precise measurements are obtained using ground-based detectors, which measure extensive air showers produced in interactions between cosmic rays and atmospheric nuclei. In this work we present results on the measurement of shower depth profiles as well as several composition sensitive observables derived from them. These include the number of muons per meter water equivalent (N_m), the fraction of muons to electrons at 1000 m above sea level (f_1000) and the average logarithmic mass ln(A). We compare these results for different zenith angles and energies. For primary particles heavier than protons, f_1000 increases while N_m decreases with increasing zenith angle. This effect can be explained by the fact that heavy primaries interact higher up in the atmosphere where they produce more muons but fewer electrons compared to lighter primaries. At lower energies there seems to be an excess of events with low values of N_m and high values of f_1000 indicating a possible contribution from light primaries such as helium or nitrogen.",
        "watermark_text": "The composition of cosmic rays is studied by monitoring their power spectrum and mass distribution at Earth . The most accurate measurements are achieved using ground - based detectors , which measure significant air showers generated in interactions between cosmic rays and atmospheric atoms .In this research we present results on the determination of shower depth profiles as also as several composition sensitive observables generated from them . These include the quantity of muons per meter water equivalent ( N _ m ) , the fraction of muons to ions at 1000 m above water level ( f _ 1000 ) and the average logarithmic mass ln ( A ) .We contrast these results for different zenith angles and energies . For principal particles heavier than protons , f _ 1000 increases while N _ m falls with rising zenith angle .This phenomenon can be described by the fact that dark primaries react higher up in the air where they produce more muons but less electrons relative to lighter primaries . At lower energies there seems to be an accumulation of events with lowest values of N _ m and low values of f _ 1000 suggesting a possible input from light primaries such as helium or nitrogen .",
        "rewrite_text": "The composition of cosmic rays is analyzed by observing their power spectrum and mass distribution at Earth. The most precise measurements are obtained through ground-based detectors, which capture considerable air showers resulting from interactions between cosmic rays and atmospheric atoms. In this study, we present findings on the assessment of shower depth profiles, along with various composition-sensitive observables derived from them. These observables include the number of muons per meter water equivalent (N_m), the ratio of muons to ions at 1000 meters above sea level (f_1000), and the average logarithmic mass ln(A). We compare these results across different zenith angles and energy levels. For primary particles heavier than protons, f_1000 tends to increase while N_m decreases as the zenith angle rises. This behavior can be explained by the fact that heavier primaries interact higher in the atmosphere, producing more muons but fewer electrons compared to lighter primaries. At lower energies, there appears to be a clustering of events characterized by low N_m values and low f_1000 values, suggesting the potential influence of lighter primaries such as helium or nitrogen.",
        "ori-fast-z-score": -1.2792042981336627,
        "water-fast-z-score": 6.182820774312702,
        "rewrite-fast-z-score": 0.31799936400190804
    },
    {
        "original_text": "The spin transistor is an important device for future quantum information processing and communication technologies, but its realization in practice has been challenging due to the lack of suitable materials with large spin-orbit coupling (SOC). Here we propose that graphene can be used as such material by exploiting its unique electronic structure. We show how this leads to a novel type of spin transistor which operates at room temperature without external magnetic fields or applied gate voltages. The proposed device consists of two ferromagnetic contacts connected via a single layer of graphene. By applying a voltage between these contacts one can control the SOC strength in the graphene channel leading to a change in the transmission probability through it. This results in a switching behavior similar to conventional transistors. In addition, our analysis shows that the proposed device exhibits high on/off ratios even when operating under realistic conditions. Finally, we discuss possible experimental realizations of the proposed device. Graphene is a promising candidate for applications in spintronics because of its unique electronic properties  1  . It offers the possibility to realize devices based on pure spin currents  2  , which are not limited by Joule heating effects  3  .\nIn particular, the spin Hall effect  4  allows for efficient generation  5  and detection  6  of spin currents using only electric fields  7, 8  . However, despite many theoretical proposals  9  , there have so far been very few successful attempts to experimentally demonstrate spintronic devices based on graphene  10  . One reason might be the difficulty to find appropriate materials with sufficiently strong spin-orbit interaction  11  . Another problem is related to the fact that most experiments were performed at low temperatures  12  where thermal fluctuations limit the performance of spintronic devices  13  .",
        "watermark_text": "The spin transistor is an important technology for future quantum information processing and communication technologies , but its acceptance in practice has been challenging due to the lack of appropriate substances with large spin - orbit coupling ( SOC ) . Here we propose that graphene can be used as such material by exploiting its unique electronic content .We see how this results to a new kind of spin transistor which operates at room temperature without external magnetic fields or applied gate voltages . The proposed system consists of two ferromagnetic contacts connected via a single thickness of graphene .By applying a voltage between these contacts one can influence the SOC intensity in the graphene channel resulting to a change in the propagation probability through it . This results in a switching environment similar to conventional transistors .In addition , our analysis shows that the suggested system displays high on / off ratios even when operating under realistic conditions . Finally , we study possible experimental realizations of the suggested system .Graphene is a potential candidate for applications in spintronics because of its unique electronic properties 1 . It provides the prospect to realize devices using on true spin currents 2 , which are not limited by Joule cooling effects 3 .In particular , the spin Hall phenomenon 4 enables for efficient production 5 and confirmation 6 of spin currents using only electric forces 7 , 8 . However , despite many theoretical proposals 9 , there have so far been very few successful proposals to experimentally prove spintronic systems based on graphene 10 .One reason could be the difficulty to find adequate devices with adequate strong spinning - orbit interaction 11 . Another difficulty is related to the fact that most studies were performed at low temperatures 12 where thermal fluctuations limit the performance of spintronic systems 13 .",
        "rewrite_text": "The spin transistor represents a crucial technology for the advancement of quantum information processing and communication systems. However, its practical implementation has faced hurdles due to the scarcity of suitable materials with significant spin-orbit coupling (SOC). In this context, we suggest that graphene may serve as an ideal candidate by leveraging its distinctive electronic properties. This leads to the development of a novel type of spin transistor that operates at room temperature without the need for external magnetic fields or applied gate voltages. The proposed structure features two ferromagnetic contacts linked by a single layer of graphene. By applying a voltage across these contacts, one can manipulate the SOC strength in the graphene channel, thereby altering the probability of spin propagation through it. This creates a switching mechanism akin to that of traditional transistors. Our findings also indicate that the proposed system demonstrates high on/off ratios, even under realistic operating conditions. Furthermore, we explore potential experimental realizations of this system. Graphene stands out as a promising material for spintronics due to its exceptional electronic characteristics. It offers the potential to implement devices based on authentic spin currents, which are not subjected to Joule heating effects. Notably, the spin Hall effect facilitates the efficient generation and detection of spin currents using solely electric fields. Despite numerous theoretical proposals, there have been limited successful experimental demonstrations of spintronic systems utilizing graphene. This may be attributed to challenges in finding devices with sufficiently strong spin-orbit interactions, as well as the tendency of most studies to be conducted at low temperatures, where thermal fluctuations can hinder the performance of spintronic devices.",
        "ori-fast-z-score": 1.4316582658130823,
        "water-fast-z-score": 8.397070403831712,
        "rewrite-fast-z-score": -0.2526455763199557
    },
    {
        "original_text": "The Relativistic Heavy Ion Collider (RHIC) is an accelerator complex located in Brookhaven National Laboratory, New York. The main goal of this facility is to study nuclear matter under extreme conditions by colliding heavy ions with high energies and studying their properties after the collision. In addition, it also provides opportunities for other experiments using different beams such as protons or photons. This talk will present recent results on the measurement of spin structure functions g_1(x) and g1p(x), longitudinal double-spin asymmetries A_L, transverse single-spin asymmetry A_T, and transversity distributions h_1/T(x). These measurements are performed by the Solenoidal Tracker At RHIC experiment (STAR) which uses two large Time Projection Chambers (TPCs) filled with a gas mixture consisting of 90% helium-4 and 10% isobutane-1.",
        "watermark_text": "The Relativistic Heavy Ion Collider ( RHIC ) is an accelerator complex located in Brookhaven National Laboratory , New York . The main goal of this lab is to study nuclear material under extreme circumstances by colliding heavy ions with high energies and studying their properties after the interaction .In addition , it also provides opportunities for other experiments using separate beams such as protons or photons . This discussion will present recent results on the determination of spin structure parameters h _ 1 ( x ) and g1p ( x ) , longitudinal single - spinning asymmetries A _ L , transverse single - spin asymmetry A _ T , and transversity distributions x _ 1 / T ( x ) .These measurements are performed by the Solenoidal Tracker At RHIC observation ( STAR ) which uses two huge Time Projection Chambers ( TPCs ) filled with a gas mixture consisting of 90 % helium - 4 and 10 % isobutane - 1 .",
        "rewrite_text": "The Relativistic Heavy Ion Collider (RHIC) is an accelerator complex situated at the Brookhaven National Laboratory in New York. Its primary objective is to investigate nuclear matter under extreme conditions by colliding high-energy heavy ions and analyzing the resulting interactions. Additionally, the facility supports a range of other experiments utilizing different beams, such as protons or photons. This discussion will focus on recent findings regarding the spin structure parameters \\(h_1(x)\\) and \\(g_{1p}(x)\\), as well as longitudinal single-spin asymmetries \\(A_L\\), transverse single-spin asymmetries \\(A_T\\), and transversity distributions \\(x_{1/T}(x)\\). These measurements are conducted by the Solenoidal Tracker at RHIC (STAR), which employs two large Time Projection Chambers (TPCs) filled with a gas mixture of 90% helium-4 and 10% isobutane-1.",
        "ori-fast-z-score": 1.386750490563073,
        "water-fast-z-score": 4.9923017660270625,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new results on thermal inertia measurements of NEAs based on infrared observations with Spitzer Space Telescope (SST). We use these data to derive an improved estimate of the mean value of the surface thermal inertia, I = 100 ± 50 J m-2 s-1/2 K-1, which is in good agreement with previous estimates obtained by other authors using different methods. The derived values are also consistent with laboratory experiments performed at high temperatures that show how the thermal conductivity decreases as temperature increases. Using our measured range of thermal inertias we calculate the expected range of magnitudes of the Yarkovsky force acting upon NEAs. Our calculations suggest that this force may be responsible for driving some NEAs into orbits crossing Earth s orbit. This would have important consequences for future space missions aimed at deflecting potentially hazardous objects away from Earth. Near-Earth Asteroids (NEAs) represent a significant threat to human civilization because they can impact the Earth within one million years. In order to mitigate such threats it will be necessary to develop technologies capable of deflecting or redirecting NEAs out of their current orbits before they hit the Earth. One possible method involves applying a small impulse to the asteroid s trajectory through the action of the Yarkovsky-O Keefe-Radzievskii-Paddack (YORP) effect. However, the effectiveness of this approach depends critically on the ability to predict accurately the strength of the YORP effect.",
        "watermark_text": "We report new data on cooling inertia studies of NEAs based on infrared observations with Spitzer Space Telescope ( SST ) . We use these information to derive an better estimate of the mean value of the surface heat inertia , I = 100 ± 50 J m - 2 s - 1 / 2 K - 1 , which is in good agreement with previous calculated obtained by other researchers using separate methods .The derived values are also consistent with lab experiments conducted at high temperatures that demonstrate how the thermal conductivity decreases as temperature increases . Using our measured range of thermal inertias we determine the expected range of magnitudes of the Yarkovsky force acting upon NEAs .Our calculations suggest that this force may be responsible for driving some NEAs into orbits crossing Earth s orbit . This might have important implications for future space missions targeted at deflecting possibly hazardous objects away from Earth .Near - Earth Asteroids ( NEAs ) constitute a major danger to human civilization because they can affect the Earth within one million years . In order to mitigate such threats it will be required to develop technologies capable of deflecting or redirecting NEAs out of their current orbits before they struck the Earth .One possible method means using a small impulse to the asteroid s path through the operation of the Yarkovsky - O Keefe - Radzievskii - Paddack ( YORP ) effect . However , the performance of this methodology varies critically on the ability to predict correctly the strength of the YORP effect .",
        "rewrite_text": "We present new findings from cooling inertia studies of near-Earth asteroids (NEAs) based on infrared observations from the Spitzer Space Telescope (SST). These findings allow us to provide an improved estimate of the average surface heat inertia, I = 100 ± 50 J m^-2 s^-1/2 K^-1, which aligns well with previous calculations made by other researchers using different methodologies. Our derived values corroborate laboratory experiments at elevated temperatures, which show a decrease in thermal conductivity as temperatures rise. By applying our measured range of thermal inertias, we estimate the magnitude of the Yarkovsky force acting on NEAs. Our analysis indicates that this force could contribute to some NEAs entering orbits that cross the Earth's path, which has significant implications for future space missions aimed at redirecting potentially hazardous objects away from our planet. NEAs pose a substantial threat to humanity, as they can impact the Earth within a million years. To mitigate such dangers, technologies must be developed to deflect or redirect NEAs from their current trajectories before any collision with Earth occurs. One potential strategy involves delivering a small impulse to an asteroid's trajectory through the Yarkovsky-O'Keefe-Radzievskii-Paddack (YORP) effect. However, the effectiveness of this approach hinges critically on accurately predicting the strength of the YORP effect.",
        "ori-fast-z-score": 1.3471506281091268,
        "water-fast-z-score": 7.319250547113999,
        "rewrite-fast-z-score": 0.5241424183609592
    },
    {
        "original_text": "We present the results obtained with a new approach to nuclear fusion, which combines the advantages of both microscopic and macroscopic models. The method is based on an extension of the statistical Hauser-Feshbach theory that includes the effects of nuclear structure in the form of single-particle level densities and transmission coefficients calculated within the framework of the Hartree-Fock-Bogoliubov (HFB) approximation. We apply this formalism to calculate cross sections for neutron-induced reactions on nuclei near the doubly-magic 132Sn nucleus. In particular we study the influence of pairing correlations on the fusion process by comparing our results with those obtained using the standard HFB+BCS approach. Our calculations show that the inclusion of pairing correlations leads to significant changes in the predicted fusion probabilities as well as in the corresponding astrophysical S-factors. These differences are particularly pronounced when considering heavy-ion collisions at low incident energies below the Coulomb barrier. \n \n Keywords: Nuclear fusion, Statistical model",
        "watermark_text": "We present the results derived with a new approach to nuclear fusion , which mixes the advantages of both microscopic and macroscopic models . The method is based on an extension of the empirical Hauser - Feshbach theory that contains the effects of nuclear formation in the form of double - nucleus level densities and transmission coefficients calculated within the framework of the Hartree - Fock - Bogoliubov ( HFB ) algorithm .We use this formalism to estimate cross sections for neutron - caused reactions on clusters near the doubly - magic 132Sn element . In particular we study the impact of coupling correlations on the fusion reaction by comparing our findings with those achieved using the standard HFB + BCS approach .Our calculations show that the introduction of pairing correlations brings to significant variations in the expected fusion probabilities as well as in the associated astrophysical S - variables . These changes are particularly pronounced when using heavy - ion collisions at low collision energies below the Coulomb barrier .Keywords: Nuclear fusion, Statistical model",
        "rewrite_text": "We present our findings based on a novel approach to nuclear fusion that combines the strengths of both microscopic and macroscopic models. This method extends the empirical Hauser-Feshbach theory by incorporating the effects of nuclear formation through double-nucleus level densities and transmission coefficients determined using the Hartree-Fock-Bogoliubov (HFB) algorithm. We apply this framework to estimate cross sections for neutron-induced reactions on clusters near the doubly magic isotope 132Sn. Specifically, we investigate how coupling correlations affect fusion reactions by comparing our results with those obtained from the traditional HFB + BCS method. Our calculations reveal that including pairing correlations leads to significant changes in predicted fusion probabilities and associated astrophysical S factors. These effects are especially notable in heavy-ion collisions at low energies, below the Coulomb barrier. \nKeywords: Nuclear fusion, Statistical model.",
        "ori-fast-z-score": -0.819288030372914,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 1.9402850002906638
    },
    {
        "original_text": "We report on the first simultaneous observations with XMM-Newton and Spitzer Space Telescope (SST) during an active state of the Galactic black hole candidate GRS 1915+105, which is known to show strong variability at all wavelengths. We find that there are two parallel tracks in the evolution of the source s luminosity between the near-infrared (NIR), optical/UV, soft/hard X-ray bands as well as radio wavebands. The NIR-optical track follows closely the hard X-ray light curve but lags behind by about 20 days; while the UV-X-ray track shows no lag compared to the hard X-ray light curve. This suggests that different physical processes may be responsible for these two tracks. In addition, we also found evidence for a hysteresis effect in both tracks. These results provide new insights into the physics of accretion flows onto supermassive black holes.",
        "watermark_text": "We report on the first simultaneous measurements with XMM - Newton and Spitzer Space Telescope ( SST ) during an active state of the Galactic brown hole member GRS 1915 + 105 , which is known to indicate strong variability at all wavelengths . We see that there are two simultaneous tracks in the evolution of the target s luminosity between the near - infrared ( NIR ) , optical / UV , soft / soft X - ray regions as well as radio wavebands .The NIR - optical track follows carefully the hard X - ray light line but lags behind by about 20 days ; while the UV - X - ray band sees no lag compared to the hard X - ray light line . This implies that different physical processes possibly be responsible for these two lines .In addition , we also discovered evidence for a hysteresis effect in both tracks . These data provide fresh insights into the physics of accretion flows onto supermassive black holes .",
        "rewrite_text": "We present the first simultaneous observations using XMM-Newton and the Spitzer Space Telescope (SST) during an active phase of the Galactic black hole GRS 1915+105, which is known for its significant variability across all wavelengths. Our findings reveal two distinct tracks in the target's luminosity evolution across the near-infrared (NIR), optical/UV, soft X-ray, and radio wavebands. The NIR-optical track closely follows the hard X-ray light curve, with a delay of approximately 20 days, whereas the UV-X-ray band shows no lag relative to the hard X-ray light curve. This suggests that different physical processes may be at play for each of these tracks. Additionally, we observed evidence of a hysteresis effect in both tracks. These results offer new perspectives on the dynamics of accretion flows onto supermassive black holes.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.75,
        "rewrite-fast-z-score": 1.524001524002286
    },
    {
        "original_text": "The Variable Star One-Shot project is an open-source software package for the analysis of astronomical data.  It was developed by members of the Harvard-Smithsonian Center for Astrophysics (CfA) in collaboration with researchers at other institutions around the world.   The goal of this project is to provide a single tool that can be used to analyze all types of astronomical data sets, including photometric time series, spectroscopic observations, images, etc., using state-of-the-art techniques such as image subtraction, cross correlation, period finding algorithms, spectral line fitting, etc.    This software has been released under the GNU General Public License v3.0 and is available on GitHub at: https://github.com/VariableStar/one-shot-astro .\nOne Shot Astro includes several tools which are useful for analyzing large amounts of astronomical data quickly and efficiently.   These include one-shot-datacleaner , which performs automated quality control checks on raw data files;   one-shot-mosaic , which creates mosaicked images from multiple dithered exposures;  one-shot-astrometry , which determines astrometric solutions for individual frames or entire mosaic images;    one-shot-photometry , which measures fluxes and/or magnitudes for stars across a field-of-view;    one-shot-pipeline , which automates the process of running these various tasks together into a pipeline;    one-shot-wikimapia , which allows users to create custom sky maps based on their own catalogs of objects.     In addition to these core packages, there are also many additional modules available through the one-shot-astro repository which allow users to perform more specialized analyses, such as:    one-shot-catalog , which provides access to a variety of different astrophysical databases via SQL queries;    one-shot-fastphot , which uses machine learning methods to measure stellar",
        "watermark_text": "The Variable Star One - Shot initiative is an free - source software tool for the analysis of astronomical data . It was developed by participants of the Harvard - Smithsonian Center for Astrophysics ( CfA ) in partnership with researchers at other institutions around the world .The goal of this project is to provide a single method that can be used to analyze all types of astronomical data sets , including photometric period series , spectroscopic observations , photographs , etc . , using state - of - the - art methods such as image subtraction , inter correlation , period finding methods , spectral line fitting , etc . This program has been released under the GNU General Public License v3 . 0 and is accessible on GitHub at : https : / / github . com / VariableStar / one - shot - astro .One Shot Astro includes several methods which are helpful for studying huge amounts of astronomical data easily and smoothly . These include one - shot - datacleaner , which performs automated quality control checks on raw data data ; one - shot - mosaic , which forms mosaicked images from multiple dithered exposures ; one - shot - astrometry , which determines astrometric solutions for individual frames or entire mosaic images ; one - shot - photometry , which estimates fluxes and / or magnitudes for planets across a field - of - view ; one - shot - pipeline , which automates the process of running these numerous tasks combined into a pipeline ; one - shot - wikimapia , which allows users to create custom skies mapping based on their own catalogs of items .In addition to these core programs , there are also many extra modules available through the one - shot - astro repository which allow users to conduct more sophisticated analyses , such as : one - shot - collection , which offers connections to a variety of different astrophysical databases via SQL queries ; one - shot - fastphot , which uses computer learning techniques to measure stellar",
        "rewrite_text": "The Variable Star One-Shot initiative is an open-source software tool designed for the analysis of astronomical data. Developed by participants at the Harvard-Smithsonian Center for Astrophysics (CfA) in collaboration with researchers from various institutions worldwide, this project aims to provide a unified method for analyzing all types of astronomical datasets. These include photometric period series, spectroscopic observations, photographs, and more, utilizing advanced techniques like image subtraction, cross-correlation, period-finding algorithms, and spectral line fitting. Released under the GNU General Public License v3.0, the software is available on GitHub at https://github.com/VariableStar/one-shot-astro. \n\nOne-Shot Astro features several tools that simplify the analysis of large volumes of astronomical data. Key components include One-Shot DataCleaner for automated quality control of raw data, One-Shot Mosaic for creating mosaicked images from multiple exposures, One-Shot Astrometry for determining astrometric solutions for individual frames or entire mosaics, One-Shot Photometry for estimating fluxes and magnitudes of celestial objects, and One-Shot Pipeline, which automates the execution of these tasks in sequence. Additionally, One-Shot Wikimapia enables users to create custom sky maps based on their own item catalogs. \n\nBeyond these essential tools, the One-Shot Astro repository also offers numerous supplementary modules for more advanced analyses, such as One-Shot Collection, which facilitates connections to various astrophysical databases using SQL queries, and One-Shot FastPhot, which employs machine learning techniques to measure stellar brightness.",
        "ori-fast-z-score": 1.2094157958139042,
        "water-fast-z-score": 8.465910570697329,
        "rewrite-fast-z-score": 0.658504607868518
    },
    {
        "original_text": "We present an algorithm for obtaining the spacetime metric from observational data, such as those obtained by the Planck satellite and other experiments. The method is based on the fact that in general relativity (GR) the Einstein field equations are equivalent to the geodesic equation for test particles. We use this equivalence to obtain the metric tensor components directly from the observed trajectories of photons emitted at different redshifts. This approach allows us to reconstruct the full four-dimensional geometry of space-time without assuming any particular model or parametrization. In order to demonstrate our technique we apply it to simulated data generated using the publicly available code CAMB. Our results show that the recovered metric agrees well with the original one used to generate the mock data. Finally, we discuss possible applications of our method to real astrophysical datasets. Cosmology has entered into precision era thanks to recent advances in experimental techniques which have allowed astronomers to measure many important quantities related to the evolution of the universe. Among these measurements there are the temperature anisotropy power spectrum measured by WMAP  1  , PLANCK  2  and SPT  3  satellites; the baryon acoustic oscillations detected through galaxy surveys  4  ; and the luminosity distance-redshift relation inferred from type Ia supernovae  5  . These new data provide unprecedented opportunities to study fundamental physics beyond the Standard Model  6  .\nIn addition to providing accurate measurements of various physical parameters describing the state of the universe today, modern cosmological experiments also allow us to probe its large-scale structure over time  7, 8  . For example, the measurement of the cosmic microwave background radiation provides information about the early stages of the universe s history when the energy density was dominated by dark matter and radiation  9  . On the other hand, the detection of distant galaxies gives access to the late stage of the universe s expansion when dark energy starts dominating  10  .",
        "watermark_text": "We present an algorithm for acquiring the spacetime metric from observational data , such as those achieved by the Planck satellite and other experiments . The method is based on the fact that in general relativity ( GR ) the Einstein field equations are comparable to the geodesic equation for test particles .We use this equivalence to obtain the metric tensor parts directly from the seen trajectories of photons generated at different redshifts . This method enables us to reconstruct the full four - dimensional topology of space - time without assuming any specific theory or parametrization .In order to test our technique we apply it to simulated evidence generated using the publicly accessible code CAMB . Our results show that the recovered metric fits well with the original one used to create the mock data .Finally , we explain possible use of our technique to real astrophysical datasets . Cosmology has entered into precision era thanks to recent developments in experimental methods which have enabled astronomers to measure various crucial variables connected to the evolution of the universe .Among these measurements there are the temperature anisotropy energy spectrum measured by WMAP 1 , PLANCK 2 and SPT 3 spacecraft ; the baryon acoustic oscillations detected through galaxy surveys 4 ; and the luminosity distance - redshift function inferred from type Ia supernovae 5 . These new data provide great opportunities to study theoretical physics beyond the Standard Model 6 .In addition to offering accurate measurements of several physical factors describing the state of the universe today , modern cosmological experiments also enable us to probe its large - scale nature over time 7 , 8 . For instance , the observation of the cosmic microwave background radiation presents knowledge about the early stages of the universe s history when the power concentration was dominated by black material and radiation 9 .On the other hand , the observation of distant galaxies provides access to the late stage of the universe s advance when dark energy starts dominating 10 .",
        "rewrite_text": "We introduce an algorithm designed to derive the spacetime metric from observational data, such as that obtained from the Planck satellite and other related experiments. This approach leverages the relationship between the Einstein field equations in general relativity (GR) and the geodesic equation for test particles. By using this connection, we can extract the components of the metric tensor directly from the observed trajectories of photons emitted at various redshifts. This technique allows us to reconstruct the complete four-dimensional topology of spacetime without relying on any specific theoretical framework or parameterization. To validate our method, we apply it to simulated data generated with the publicly accessible CAMB code, and our findings indicate that the recovered metric aligns well with the original metric used to create the simulated observations. We also discuss the potential applications of our technique to real astrophysical data sets. The field of cosmology has entered a precision era, thanks to advancements in experimental techniques that allow astronomers to measure key variables related to the universe's evolution. Notable measurements include the temperature anisotropy energy spectrum obtained from the WMAP, PLANCK, and SPT missions; baryon acoustic oscillations detected through galaxy surveys; and the luminosity distance-redshift relation derived from Type Ia supernova observations. These new data sets present significant opportunities to explore theoretical physics beyond the Standard Model. Furthermore, modern cosmological experiments not only provide precise measurements of several physical parameters that describe the current state of the universe, but they also enable us to investigate its large-scale evolution over time. For example, observations of the cosmic microwave background radiation enhance our understanding of the universe's early history when energy density was predominantly influenced by matter and radiation. Conversely, observations of distant galaxies grant insight into the universe’s later stages, characterized by the influence of dark energy.",
        "ori-fast-z-score": 0.9669875568304563,
        "water-fast-z-score": 8.273159087695738,
        "rewrite-fast-z-score": 2.3445110420178854
    },
    {
        "original_text": "We present the results on the distribution and luminosity function (LF) of active galactic nuclei (AGNs) in clusters of galaxies based on Chandra observations of 16 galaxy clusters at redshifts 0.1 < z < 1.0. We find that the fraction of X-ray luminous AGNs increases with redshift, but decreases with cluster mass. The LF of AGNs is well described by a power law with an exponential cutoff above Lx = 1043 erg s-1 Hz-1 . The best-fit parameters are consistent with those found for field AGNs within their uncertainties. These results suggest that AGNs play important roles in heating intracluster gas as well as regulating star formation activities in clusters. This work was supported by NASA grant NNG06GH50G to J.E.K., and NSF grants AST-0707765 and AST-0708164 to A.M..",
        "watermark_text": "We present the conclusion on the distribution and luminosity function ( LF ) of active galactic nuclei ( AGNs ) in clusters of stars based on Chandra measurements of 16 galaxy galaxies at redshifts 0 . 1 < z < 1 . 0 . We see that the fraction of X - ray luminous AGNs increases with redshift , but decreases with cluster mass .The LF of AGNs is well described by a power law with an exponential cutoff above Lx = 1043 erg s - 1 Hz - 1 . The best - fitting characteristics are compatible with those observed for field AGNs within their uncertainties .These data suggest that AGNs serve active roles in heating intracluster gas as well as affecting star formation interactions in clusters . This research was supported by NASA award NNG06GH50G to J . E . K . , and NSF grants AST - 0707765 and AST - 0708164 to A . M . .",
        "rewrite_text": "We conclude our analysis of the distribution and luminosity function (LF) of active galactic nuclei (AGNs) in star clusters, utilizing Chandra measurements from 16 galaxies at redshifts between 0.1 and 1.0. Our findings indicate that the proportion of X-ray luminous AGNs rises with increasing redshift but declines with greater cluster mass. The LF of AGNs can be effectively characterized by a power law featuring an exponential cutoff above Lx = 10^43 erg s^-1 Hz^-1. The optimal fitting parameters align well with those recorded for field AGNs, within their respective uncertainties. This research indicates that AGNs play a significant role in heating intracluster gas and influencing star formation processes within clusters. The study was funded by NASA grant NNG06GH50G awarded to J. E. K., alongside NSF grants AST-0707765 and AST-0708164 awarded to A. M.",
        "ori-fast-z-score": -0.4375949744936837,
        "water-fast-z-score": 5.10527470242631,
        "rewrite-fast-z-score": 0.282842712474619
    },
    {
        "original_text": "In this work, we propose an efficient direct-voting scheme to provide data fusion assurance (DFA) in wireless sensor networks (WSNs). The proposed DFA scheme is based on the concept that each node can directly vote its local decision with other nodes  decisions and then obtain final global decision by majority voting rule. In order to reduce communication overheads caused by direct voting process, our scheme adopts two novel techniques: 1) only one-hop neighbors are allowed to cast their votes at any time instant; 2) all nodes use different time slots to transmit their votes simultaneously. We also present theoretical analysis to show how much energy consumption will be saved compared with existing schemes. Finally, extensive simulation results demonstrate that our scheme outperforms state-of-the-art approaches in terms of both accuracy and power efficiency. Keywords: Data fusion assurance, Wireless sensor network, Power-efficient direct-voting scheme. This research was supported by National Natural Science Foundation of China under Grant No. 61371040",
        "watermark_text": "In this research , we develop an efficient direct - voted scheme to provide resource fusion assurance ( DFA ) in wireless sensor networks ( WSNs ) . The proposed DFA system is based on the idea that each node can personally voting its local decision with other nodes decisions and then obtain final global ruling by majority voting control .In order to reduce communication overheads caused by direct voting process , our scheme adopts two novel techniques : 1 ) only one - hop neighbors are allowed to cast their votes at any time instant ; 2 ) all nodes use different time slots to transmit their votes concurrently . We additionally offer experimental analysis to see how many electricity usage will be saved compared with existing plans .Finally , extensive modeling results show that our scheme outperforms state - of - the - art methods in terms of both precision and power performance . Keywords : Data fusion assurance , Wireless sensor network , Power - efficient direct - voting system .This research was supported by National Natural Science Foundation of China under Grant No.61371040",
        "rewrite_text": "In this study, we introduce an effective direct-voting scheme designed to enhance resource fusion assurance (DFA) in wireless sensor networks (WSNs). Our proposed DFA system operates on the principle that each node can independently cast its local decision in alignment with the decisions of neighboring nodes, ultimately arriving at a global consensus through majority voting. To mitigate the communication overhead associated with the direct voting process, our approach incorporates two innovative techniques: 1) only one-hop neighbors are permitted to vote at any given time; and 2) all nodes utilize different time slots for concurrent vote transmission. We also perform experimental analyses to evaluate the energy savings achieved compared to existing methodologies. Finally, extensive modeling results demonstrate that our scheme surpasses current state-of-the-art methods in terms of both accuracy and energy efficiency. \n\nKeywords: Data fusion assurance, Wireless sensor networks, Power-efficient direct-voting system. This research was supported by the National Natural Science Foundation of China under Grant No. 61371040.",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 5.7655666019705505,
        "rewrite-fast-z-score": 1.6865480854231356
    },
    {
        "original_text": "The missing satellites problem (MSP) is one of the most important problems in space science and technology, with applications ranging from satellite navigation to space debris removal.  The MSP asks for all orbits that are stable under gravitational perturbations by known bodies such as planets or asteroids.   In this work we present an algorithm which solves the MSP exactly on any number of dimensions d >= 2 using only O(n log n + m log n) time where n = |S| is the total number of objects in S and m = |E| is the number of edges in E.   Our approach uses a novel combination of techniques including fast matrix multiplication algorithms, data structures based on interval trees, and efficient graph traversal methods. We also show how our results can be used to solve related problems like finding the minimum distance between two given sets of points in R^d. Finally, we demonstrate the practicality of our method through experiments performed on real-world datasets.",
        "watermark_text": "The missing satellites question ( MSP ) is one of the most important problems in space physics and technology , with applications diverse from satellite communication to space wreckage extraction . The MSP asks for all orbits that are stable under gravity perturbations by known objects such as planets or asteroids .In this project we present an algorithm which solves the MSP exactly on any number of dimensions d > = 2 using only O ( n log n + m log n ) time where n = | S | is the total number of items in S and m = | E | is the number of vertices in E . Our solution uses a innovative combination of techniques including rapid matrix multiplication methods , data models using on interval trees , and elegant graph traversal methods . We additionally prove how our findings can be used to solve related problems like finding the minimum height between two given sets of points in R ^ d .Finally , we prove the practicality of our technique through experiments conducted on real - time datasets .",
        "rewrite_text": "The question of missing satellites (MSP) represents a significant challenge in the fields of space physics and technology, with implications ranging from satellite communication to the retrieval of space debris. The MSP seeks to identify all orbits that remain stable under the gravitational influences of known celestial bodies, such as planets and asteroids. In this project, we introduce an algorithm that provides an exact solution to the MSP across any number of dimensions \\( d \\geq 2 \\), requiring only \\( O(n \\log n + m \\log n) \\) time complexity, where \\( n = |S| \\) denotes the total number of items in \\( S \\), and \\( m = |E| \\) signifies the number of vertices in \\( E \\). Our approach employs a novel combination of techniques, including rapid matrix multiplication, data models based on interval trees, and sophisticated graph traversal methods. Furthermore, we demonstrate how our results can be applied to tackle related issues, such as determining the minimum height between two specified sets of points in \\( R^d \\). Finally, we validate the effectiveness of our method through experiments on real-time datasets.",
        "ori-fast-z-score": -1.709408646894569,
        "water-fast-z-score": 4.529108136578382,
        "rewrite-fast-z-score": 1.5275252316519468
    },
    {
        "original_text": "We report on the polarization dependent photoluminescence (PL) and Raman scattering spectra of CdSe/CdS core/shell quantum dots in solution at room temperature. The PL spectrum shows that the emission is polarized along the direction perpendicular to the excitation light, which can be explained by the selection rules for dipole transitions between electronic states with different angular momenta. In addition we observe an anisotropic broadening of the Stokes linewidths as well as a splitting into two components when exciting circularly polarized light. These effects are attributed to the presence of exciton fine structure due to spin-orbit coupling. We also find evidence for a strong electron-phonon interaction leading to phonon sidebands in both the Stokes and anti-Stokes parts of the Raman spectrum. Finally, we show how these results can be used to determine the orientation of individual QDs embedded in a polymer matrix. Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy.",
        "watermark_text": "We report on the polarization dependent photoluminescence ( PL ) and Raman absorption spectra of CdSe / CdS core / shell quantum dots in solution at room temperature . The PL spectrum reveals that the emission is polarized along the direction perpendicular to the excitation light , which can be described by the selection rules for dipole changes between electronic states with various angular momenta .In addition we witness an anisotropic broadening of the Stokes linewidths as well as a dividing into two parts when exciting circularly polarized light . These effects are traced to the presence of exciton fine structure owing to spin - orbit bonding .We additionally find proof for a powerful atom - phonon interaction leading to phonon sidebands in both the Stokes and anti - Stokes parts of the Raman spectrum . Finally , we find how these results can be used to predict the orientation of different QDs integrated in a polymer matrix .Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy .",
        "rewrite_text": "We present our findings on the polarization-dependent photoluminescence (PL) and Raman absorption spectra of CdSe/CdS core/shell quantum dots in solution at room temperature. The PL spectrum indicates that the emission is polarized in a direction perpendicular to the excitation light, which aligns with the selection rules governing dipole transitions between electronic states with varying angular momenta. Furthermore, we observe an anisotropic broadening of the Stokes linewidths, which also separates into two distinct components when excited by circularly polarized light. These phenomena are attributed to the presence of exciton fine structure resulting from spin-orbit coupling. Additionally, we provide evidence for a significant atom-phonon interaction that generates phonon sidebands in both the Stokes and anti-Stokes regions of the Raman spectrum. Ultimately, we demonstrate how these findings can be utilized to infer the orientation of different quantum dots when integrated into a polymer matrix. Polarized luminescence measurements were conducted on single quantum dot emitters using confocal microscopy.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 5.5,
        "rewrite-fast-z-score": 1.099524999206747
    },
    {
        "original_text": "We report on deep near-infrared observations with VLT/VISIR, which reveal that the X-ray source NGC 300 X-1 is associated with an infrared point-like object (X-ray counterpart) at RA = 03h45m55s.6 DEC = -27d19 59.9  (J2000), located in the central part of the galaxy s spiral arm. The observed fluxes are compatible with those expected for a WR star with T eff ~ 50 kK and log g ~ 5.5-6.0. We also find evidence for extended emission around this source, possibly due to dust heated by the stellar wind or radiation field. Using our VISIR data we estimate the total number of WR stars in NGC 300 as NWR = 1.1 x 10^7 ± 0.3 x 10^7 per M_sun. This value agrees well with previous estimates based on HST/ACS imaging and spectroscopy. \n \n \n \n Keywords: Wolf-Rayet stars",
        "watermark_text": "We report on far near - infrared observations with VLT / VISIR , which show that the X - ray source NGC 300 X - 1 is associated with an infrared point - like body ( X - ray counterpart ) at RA = 03h45m55s . 6 DEC = - 27d19 59 . 9 ( J2000 ) , located in the central half of the universe s spiral arm . The observed fluxes are compatible with those expected for a WR galaxy with T eff ~ 50 kK and log k ~ 5 . 5 - 6 . 0 .We additionally find proof for extended emission around this source , possibly due to dust warmed by the planetary wind or radiation field . Using our VISIR information we estimate the total number of WR galaxies in NGC 300 as NWR = 1 . 1 x 10 ^ 7 ± 0 . 3 x 10 ^ 7 per M _ sun .This value agrees well with previous accounts based on HST / ACS observation and spectroscopy . Keywords : Wolf - Rayet stars",
        "rewrite_text": "We present far-infrared and near-infrared observations conducted with VLT/VISIR, which reveal that the X-ray source NGC 300 X-1 is associated with an infrared point-like object (the X-ray counterpart) located at RA = 03h45m55.6s and DEC = -27d19m59.9s (J2000), situated in the central region of the spiral arm of the galaxy. The observed fluxes align with expectations for a Wolf-Rayet (WR) galaxy characterized by an effective temperature of approximately 50 kK and a logarithmic value of k between 5.5 and 6.0. Furthermore, we find evidence of extended emission surrounding this source, which may be the result of dust being heated by the planetary wind or the radiation field. Utilizing our VISIR data, we estimate the total number of WR galaxies in NGC 300 to be NWR = 1.1 x 10^7 ± 0.3 x 10^7 per solar mass. This estimate is consistent with previous findings derived from HST/ACS observations and spectroscopy. Keywords: Wolf-Rayet stars.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 4.180039147272101,
        "rewrite-fast-z-score": 0.1259881576697424
    },
    {
        "original_text": "We present new models for non-abelian string networks, which are based on the idea that the network is composed by many entangled strings with different velocities and orientations. We show how to construct such velocity-dependent models in terms of Feynman diagrams. In particular we consider two types of diagrams: (i) ladder-like diagrams corresponding to the exchange of gluons between pairs of strings, (ii) cross-ladder like-diagrams describing interactions among three or more strings. The latter type of diagrams can be interpreted as representing junctions where several strings meet at one point. We discuss some properties of these models and compare them with previous results obtained within the framework of Abelian-Higgs model. Finally, we study numerically the evolution of an initial configuration consisting of a single straight string into a complicated tangle of interacting strings using Monte Carlo simulations. This work was supported by the DFG under contract SFB-TR9  Gravitational Physics ",
        "watermark_text": "We introduce novel theories for non - abelian string systems , which are based on the idea that the network is composed by many entangled strings with various velocities and orientations . We see how to build such velocity - dependent models in terms of Feynman diagrams .In particular we define two forms of diagrams : ( i ) ladder - like diagrams corresponding to the transfer of gluons between pairs of sequences , ( ii ) cross - ladder like - diagrams describing relationships among three or more strings . The last sort of diagrams can be interpreted as representing junctions where many strings join at one point .We discuss some properties of these models and link them with previous findings obtained within the framework of Abelian - Higgs system . Finally , we study numerically the evolution of an initial structure consisting of a single straight string into a complicated tangle of interacting strings using Monte Carlo simulations .This project was supported by the DFG under contract SFB - TR9 Gravitational Physics",
        "rewrite_text": "We present innovative theories for non-abelian string systems, grounded in the concept that the network comprises multiple entangled strings with diverse velocities and orientations. We outline how to construct these velocity-dependent models using Feynman diagrams. Specifically, we define two types of diagrams: (i) ladder-like diagrams that represent the transfer of gluons between pairs of sequences, and (ii) cross-ladder-like diagrams that illustrate interactions among three or more strings. The latter type can be interpreted as junctions where several strings converge at a single point. We explore some characteristics of these models and relate them to previous results found within the Abelian-Higgs framework. Finally, we perform numerical studies using Monte Carlo simulations to observe the evolution of an initial configuration of a single straight string into a complex tangle of interacting strings. This research was supported by the DFG under contract SFB-TR9 Gravitational Physics.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 5.41602560309064,
        "rewrite-fast-z-score": 0.8682431421244593
    },
    {
        "original_text": "We present the results of an investigation into the properties and evolution of transients that arise when initial conditions are generated using Lagrangian perturbation theory (LPT) for cosmological N-body simulations. We find that LPT-based initial conditions produce spurious large-scale power at late times, which is not suppressed by increasing the number of particles used to generate these initial conditions. This effect can be mitigated by applying a low-pass filter to the evolved density field prior to generating new initial conditions with higher-order LPT. However, this approach does not completely remove all unwanted effects associated with the use of LPT-generated initial conditions. In addition, we show how the choice of time step used to evolve the initial conditions affects their accuracy. Finally, we demonstrate that it is possible to construct accurate initial conditions for large-volume cosmological simulations without having to resort to expensive high-resolution hydrodynamic simulations. The generation of realistic initial conditions for cosmological N-body",
        "watermark_text": "We present the conclusion of an research into the properties and evolution of transients that arise when initial conditions are produced utilizing Lagrangian perturbation theory ( LPT ) for cosmological N - bodies simulations . We see that LPT - based initial conditions generate spurious large - scale power at late times , which is not suppressed by expanding the quantity of atoms used to create these initial conditions .This phenomenon can be mitigated by using a reduced - pass filter to the evolved density field prior to generating new initial conditions with higher - order LPT . However , this methodology does not totally reduce all unwanted phenomena associated with the using of LPT - produced early conditions .In addition , we show how the selection of time phase employed to evolve the first parameters impacts their accuracy . Finally , we prove that it is easy to build correct preliminary conditions for large - volume cosmological simulations without having to resort to expensive high - resolution hydrodynamic simulations .The generation of realistic initial conditions for cosmological N - body",
        "rewrite_text": "We present the findings of our research on the properties and evolution of transients generated by initial conditions created using Lagrangian perturbation theory (LPT) for cosmological N-body simulations. Our study indicates that LPT-based initial conditions lead to artificial large-scale power at later times, a problem that persists regardless of increasing the number of particles used to establish these conditions. To address this issue, we recommend applying a reduced-pass filter to the evolved density field before creating new initial conditions with higher-order LPT. However, it's important to note that this approach does not completely eliminate all undesired effects associated with LPT-generated early conditions. Furthermore, we demonstrate that the choice of time phase used to evolve the initial parameters has a significant impact on their accuracy. Finally, we show that it is feasible to create accurate initial conditions for large-volume cosmological simulations without the need for costly high-resolution hydrodynamic simulations.",
        "ori-fast-z-score": -0.2182178902359924,
        "water-fast-z-score": 6.037034299432969,
        "rewrite-fast-z-score": -0.5488212999484517
    },
    {
        "original_text": "The work function is one of the most important parameters in semiconductor devices, and its accurate measurement is essential for device design and performance prediction.  In this article we review some recent progresses on the determination of the work functions of various materials by using ultraviolet photoelectron spectroscopy (UPS) with synchrotron radiation as well as other techniques such as scanning tunneling microscopy/spectroscopy (STM/STS), inverse photoemission spectroscopy (IPES), and Kelvin probe force microscopy (KPFM). We also discuss how to determine the absolute values of the work functions of different semiconductors based on UPS measurements. Finally, we present our perspectives on future research directions in this field. The work function is an important parameter in semiconductor devices, which determines their electrical properties including carrier transport behavior and Schottky barrier height  1  . Accurate measurement of the work function is therefore crucial for both fundamental understanding of electronic structure and practical applications  2  .\nIn this article, we will first briefly introduce several experimental methods used to measure the work function of various materials. Then we will show that these results can be compared directly if they are obtained under similar conditions. Afterwards, we will demonstrate how to determine the absolute value of the work function of different semiconductors through ultraviolet photoelectron spectroscopy (UPES) experiments. Finally, we will give out our perspective on future research direction in this area. \nExperimental Methods\n\nUltraviolet Photoelectron Spectroscopy (UPS)\nUltraviolet photoelectron spectroscopy has been widely applied to study the surface electronic structures of many kinds of materials  3  , especially those with low electron binding energies  4  . It measures the kinetic energy distribution of electrons emitted from a sample when it is illuminated by monochromatic light at a specific photon energy hν  5  . By measuring the kinetic energy Ekin of photoelectrons emitted from the Fermi level EF into vacuum  6  , the work function Φ can then be determined according to the following equation: \nwhere e is the elementary charge and m* is the effective mass of the photoelectrons  7, 8  . For example, Figure 1 shows",
        "watermark_text": "The job function is one of the most important characteristics in semiconductor devices , and its precise measurement is crucial for product design and performance measurement . In this article we review some latest progresses on the determination of the work functions of different materials by using ultraviolet photoelectron spectroscopy ( UPS ) with synchrotron rays as well as other techniques such as scanning tunneling microscopy / spectroscopy ( STM / STS ) , inverse photoemission spectroscopy ( IPES ) , and Kelvin probe force microscopy ( KPFM ) .We also discuss how to predict the absolute values of the work functions of different semiconductors based on UPS studies . Finally , we present our perspectives on future research paths in this area .The job function is an important element in semiconductor devices , which determines their electrical properties including carrier carrier behavior and Schottky barrier elevation 1 . Accurate measurement of the work integral is consequently essential for both basic knowing of electronic stability and useful use 2 .In this article , we will first briefly provide several experimental methods used to measure the labor function of different materials . Then we will show that these results can be compared directly if they are derived under corresponding conditions .Afterwards , we will prove how to estimate the absolute value of the work function of different semiconductors through ultraviolet photoelectron spectroscopy ( UPES ) experiments . Finally , we will giving out our viewpoint on future research direction in this area .Experimental Methods Ultraviolet Photoelectron Spectroscopy ( UPS ) Ultraviolet photoelectron spectroscopy has been widely applied to study the surface electronic properties of several kinds of substances 3 , particularly those with lowest ion binding temperatures 4 . It studies the kinetic power distribution of atoms produced from a sample when it is lit by monochromatic light at a certain photon energy hν 5 .By measuring the kinetic power Ekin of photoelectrons induced from the Fermi level EF into vacuum 6 , the work function Φ can then be determined according to the following equation : where e is the elementary charge and m * is the effective mass of the photoelectrons 7 , 8 . For instance , Figure 1 shows",
        "rewrite_text": "The work function is a critical property of semiconductor devices, playing a vital role in their design and performance evaluation. In this article, we explore recent advancements in determining the work functions of various materials using techniques such as ultraviolet photoelectron spectroscopy (UPS) with synchrotron radiation, as well as other methods like scanning tunneling microscopy/spectroscopy (STM/STS), inverse photoemission spectroscopy (IPES), and Kelvin probe force microscopy (KPFM). We will also discuss how to predict the absolute values of work functions for different semiconductors based on UPS data. Lastly, we will provide our insights on future research avenues in this field.\n\nThe work function significantly influences the electrical properties of semiconductor devices, affecting carrier dynamics and Schottky barrier heights. Therefore, precise measurement of the work function is essential for a fundamental understanding of electronic stability and practical applications. This article will begin by summarizing various experimental methods utilized to measure the work function of different materials. We will demonstrate that results obtained under comparable conditions can be directly compared. Subsequently, we will illustrate how to estimate the absolute work function values of various semiconductors through UPS experiments. Finally, we will share our perspectives on potential future research directions within this domain.\n\n### Experimental Methods\n\n**Ultraviolet Photoelectron Spectroscopy (UPS)**  \nUltraviolet photoelectron spectroscopy has been extensively used to investigate the surface electronic properties of various materials, particularly those with low ion binding energies. This technique analyzes the kinetic energy distribution of electrons ejected from a sample when illuminated by monochromatic light at a specific photon energy (hν). By measuring the kinetic energy (Ekin) of photoelectrons emitted from the Fermi level (EF) into vacuum, the work function (Φ) can be determined using the relationship where e represents the elementary charge and m* is the effective mass of the photoelectrons. For example, Figure 1 illustrates this process.",
        "ori-fast-z-score": -1.8717134551736667,
        "water-fast-z-score": 7.568232666571783,
        "rewrite-fast-z-score": -0.9135002783911397
    },
    {
        "original_text": "We study the (0, 2) gauged linear sigma model in four dimensions with gauge group SU(N). We consider two different supermanifolds as target spaces for this model: one is the supersphere and another is the supercoset space $SL(3,R)/SO(3)$. The latter has been studied by Witten  1  . In both cases we find that there are no tachyons or ghosts at tree level. However, when we calculate the beta functions to one-loop order, we find that they do not vanish even if we take into account all possible counterterms allowed by symmetries. This indicates that these models suffer from perturbative non-renormalizability. On the other hand, it turns out that the theories have exact global symmetry which can be used to construct an infinite number of conserved currents. These currents lead us to propose new Ward identities which relate correlation functions involving operators of different spin.",
        "watermark_text": "We explore the ( 0 , 2 ) gauged linear sigma system in four dimensions with gauge group SU ( N ) . We consider two different supermanifolds as target spaces for this model : one is the supersphere and another is the supercoset space $ SL ( 3 , R ) / SO ( 3 ) $ .The last has been studied by Witten 1 . In both cases we find that there are no tachyons or ghosts at tree level .However , when we estimate the beta functions to one - loop order , we find that they do not vanish even if we took into consideration all possible counterterms allowed by symmetries . This implies that these models suffer from perturbative non - renormalizability .On the other hand , it turns out that the theories have precise global symmetry which can be used to build an endless number of conserved flows . These currents help us to propose new Ward identities which compare coupling systems concerning functions of different spin .",
        "rewrite_text": "We investigate the (0, 2) gauged linear sigma model in four dimensions with the gauge group SU(N). For this model, we examine two distinct supermanifolds as target spaces: one being the supersphere, and the other the supercoset space \\( SL(3, R) / SO(3) \\), which has previously been explored by Witten. In both scenarios, we observe that there are no tachyons or ghosts at tree level. However, upon calculating the beta functions to one-loop order, we find that they do not vanish, even after accounting for all possible counterterms permitted by the symmetries. This indicates that these models exhibit perturbative non-renormalizability. Additionally, we discover that the theories possess a well-defined global symmetry, which allows for the construction of an infinite number of conserved current flows. These currents enable us to propose new Ward identities that relate coupling constants across various spin functions.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 3.9391929857916765,
        "rewrite-fast-z-score": -0.2626128657194451
    },
    {
        "original_text": "We present an experimental investigation into complementarity and quantum nonlocality using a photonic implementation of the Einstein-Bohr photon box thought experiment. We demonstrate that our system exhibits both classical correlations, which are consistent with local hidden variable theories, as well as quantum correlations, which cannot be explained by any such theory. Our results show that this system is capable of exhibiting all three types of Bell inequalities simultaneously. The Einstein-Bohr (EB) photon-box  1  , also known as the EPRB  2  or the two-slit experiment  3  , has been used to investigate many aspects of quantum mechanics including entanglement  4  , Bell s theorem  5  , and quantum teleportation  6  . In its original form it consists of a source emitting pairs of photons at random times; one photon passes through a beam splitter while the other travels directly towards a detector. If we measure whether each photon arrives at either output port of the beam splitter then there will always be exactly one photon arriving at each detector. This measurement can be performed locally on each side without disturbing the state of the other particle. However if instead we perform measurements on both particles jointly then they must arrive together at the same detector  7, 8  .\nIn order for these experiments to exhibit genuine quantum effects, the detectors need to have high efficiency so that the probability of detecting more than one photon per pair is negligible  9  . Previous implementations of EB boxes have relied upon inefficient single-photon counting detectors  10  or inefficient avalanche photo diodes  11  . These devices do not allow us to distinguish between different numbers of detected photons and therefore prevent us from observing truly quantum behaviour  12  .",
        "watermark_text": "We present an experimental inquiry into complementarity and quantum nonlocality utilizing a photonic implementation of the Einstein - Bohr photon box thought experiment . We suggest that our system displays both classical correlations , which are compatible with local hidden variable theories , as well as particle correlations , which cannot be described by any such theory .Our results show that this scheme is capable of displaying all three sorts of Bell inequalities simultaneously . The Einstein - Bohr ( EB ) photon - box 1 , sometimes called as the EPRB 2 or the two - slit experiment 3 , has been used to examine multiple matters of quantum mechanics including entanglement 4 , Bell s theorem 5 , and quantum teleportation 6 .In its initial structure it consists of a source emitting sets of photons at random times ; one photon passes through a beam splitter while the other travels immediately towards a detector . If we measure whether each photon arrives at either output port of the beam splitter then there will always be exactly one photon entering at each sensor .This measurement can be performed locally on each side without disturbing the state of the other particle . However if instead we perform observations on both particles jointly then they must appear together at the same detector 7 , 8 .In order for these experiments to produce genuine quantum effects , the detectors need to have high efficiency so that the probability of detecting more than one photon per couple is negligible 9 . Earlier implementations of EB Box have relied upon inefficient single - photon counting detectors 10 or inefficient avalanche photographic diodes 11 .These machines do not enable us to distinguish between multiple numbers of identified photons and therefore prevent us from observing truly quantum behaviour 12 .",
        "rewrite_text": "We present an experimental investigation into complementarity and quantum nonlocality using a photonic version of the Einstein-Bohr photon box thought experiment. We propose that our system exhibits both classical correlations, which align with local hidden variable theories, and particle correlations that defy such explanations. Our findings indicate that this setup can demonstrate all three types of Bell inequalities at once. The Einstein-Bohr (EB) photon box, also referred to as the EPRB or two-slit experiment, has been employed to explore various aspects of quantum mechanics, including entanglement, Bell's theorem, and quantum teleportation. In its original configuration, the system consists of a source emitting photon pairs at random intervals; one photon passes through a beam splitter while the other heads directly to a detector. When we measure whether each photon reaches one of the beam splitter's output ports, exactly one photon will enter each sensor. This measurement can be conducted locally on each side without disturbing the state of the other photon. Conversely, if we perform joint measurements on both particles, they must appear together at the same detector. To achieve genuine quantum effects in these experiments, the detectors must be highly efficient, minimizing the likelihood of detecting more than one photon from each pair. Previous implementations of the EB box have utilized inefficient single-photon counting detectors or low-efficiency avalanche photodiodes that fail to distinguish among multiple detected photons, thereby inhibiting the observation of truly quantum behavior.",
        "ori-fast-z-score": 0.0949157995752499,
        "water-fast-z-score": 5.651175082804793,
        "rewrite-fast-z-score": 1.6
    },
    {
        "original_text": "We present an exact solution for the nonequilibrium dynamics of a two-level system coupled to fermionic reservoirs in terms of a Coulomb gas model defined on the Keldysch contour. The partition function is expressed as a functional integral over complex fields, which are related to the density matrix and current operators by means of a Hubbard-Stratonovich transformation. We show that this formulation allows us to obtain explicit expressions for the time evolution of observables such as the occupation probabilities and currents through the junction between the leads and the central region. In particular we find that the relaxation towards equilibrium occurs via damped oscillations whose frequency depends on the bias voltage across the junction. This behavior can be understood within a simple rate equation approach. Finally, we discuss how our results may be generalized to more complicated systems with multiple levels or spin degrees of freedom. \nI. INTRODUCTORY REMARK\nThe study of transport properties of mesoscopic devices has attracted considerable attention during recent years due to their potential applications in quantum information processing  1  . A particularly interesting class of problems concerns the description of charge transfer processes taking place at low temperatures when the electronic states involved in the process are localized near Fermi surfaces  2  .\nIn order to describe these phenomena one usually considers models where electrons tunnel coherently between different regions (leads) connected by some scattering center  3  , e.g., a single level  4  or multi-level  5  impurity. These models have been studied extensively using various techniques ranging from perturbation theory  6  to numerical methods  7, 8  . However, it turns out that many important features cannot be captured by perturbative approaches  9  while standard numerical schemes suffer from severe limitations  10  . For example, they do not allow to treat large systems and/or strong interactions  11  . Therefore, new theoretical tools are needed to understand the physics behind these phenomena  12  .\nRecently, there has been growing interest in developing analytical solutions for non-equilibrium transport problems based on mapping them onto effective statistical mechanics models  13  . One of the most successful examples of this kind is provided by the so-called Caldeira-Leggett model  14  describing the interaction of",
        "watermark_text": "We present an precise solving for the nonequilibrium dynamics of a two - level scheme coupled to fermionic reservoirs in terms of a Coulomb gas model characterized on the Keldysch contour . The partition function is expressed as a functional integral over complex fields , which are related to the density function and current operators by means of a Hubbard - Stratonovich decomposition .We see that this interpretation permits us to obtain precise expressions for the period evolution of observables such as the occupation probabilities and currents through the junction between the leads and the main region . In particular we find that the relaxation towards equilibrium results via damped oscillations whose frequency depends on the bias frequency across the junction .This phenomenon can be understood within a simple rate equation methodology . Finally , we explain how our findings may be generalized to more complicated machines with many levels or spin degrees of freedom .I . INTRODUCTORY REMARK The investigation of transport properties of mesoscopic devices has garnered considerable scrutiny during recent seasons due to their potential applications in quantum information processing 1 .A notably important group of difficulty concerns the description of charge transfer mechanisms taking place at low temperatures when the electronic states participating in the process are localized near Fermi surfaces 2 . In order to explain these phenomena one usually uses models where electrons tunnel coherently between various regions ( leads ) connected by some scattering center 3 , e . g . , a single level 4 or multi - level 5 impurity .These systems have been studied frequently using numerous tools including from perturbation theory 6 to numerical theories 7 , 8 . However , it turns out that several important features cannot be captured by perturbative approaches 9 while standard numerical theories suffer from severe constraints 10 .For instance , they do not enable to treat large systems and / or strong interactions 11 . Therefore , new theoretical tools are needed to comprehend the physics behind these phenomena 12 .Recently , there has been growing interest in building mathematical solutions for non - equilibrium transport issues based on mapping them onto effective statistical mechanics models 13 . One of the most popular instance of this kind is provided by the so - called Caldeira - Leggett model 14 modeling the interaction of",
        "rewrite_text": "We present a precise solution for the nonequilibrium dynamics of a two-level system coupled to fermionic reservoirs, utilizing a Coulomb gas model defined along the Keldysh contour. The partition function is formulated as a functional integral over complex fields, which relate to the density and current operators through a Hubbard-Stratonovich transformation. This framework enables us to derive accurate expressions for the time evolution of observables, such as occupation probabilities and currents at the junction connecting the leads to the central region. Notably, we observe that the relaxation towards equilibrium occurs through damped oscillations, where the oscillation frequency is influenced by the bias frequency across the junction. This behavior can be analyzed using a straightforward rate equation approach. Additionally, we discuss the potential to extend our findings to more complex systems incorporating multiple levels or spin degrees of freedom.\n\nI. INTRODUCTORY REMARK \n\nThe exploration of transport properties in mesoscopic devices has attracted significant attention in recent years due to their promising applications in quantum information processing. A particularly challenging aspect involves describing charge transfer mechanisms at low temperatures, where the electronic states involved are localized near the Fermi surface. To address these phenomena, researchers typically employ models in which electrons tunnel coherently between different regions (leads) linked by scattering centers, such as single or multi-level impurities. These systems have been extensively studied using various methods, from perturbation theory to numerical simulations. However, important features of these systems often elude perturbative methods, while conventional numerical approaches face limitations that prevent them from handling large systems or strong interactions. Consequently, there is a clear need for new theoretical tools to better understand the underlying physics. Recently, there has been an increasing interest in constructing mathematical solutions for nonequilibrium transport issues by mapping them onto effective statistical mechanics models. A notable example of this approach is the Caldeira-Leggett model, which describes the interaction of...",
        "ori-fast-z-score": 1.4045204148136883,
        "water-fast-z-score": 8.648888870168502,
        "rewrite-fast-z-score": 1.005970202294378
    },
    {
        "original_text": "We study the effect of an external magnetic field on the modified Coulomb potential for two particles with opposite charges and masses, which are confined to move along one dimension. We show that this system can be mapped onto a spinless fermion model by using the Jordan-Wigner transformation. The ground state energy is calculated exactly within the framework of Bethe ansatz method. It turns out that there exists a critical value of the magnetic field strength beyond which the ground state becomes degenerate. This result agrees well with previous numerical calculations based on exact diagonalization technique. \n \n In addition we calculate the density-density correlation function as well as the momentum distribution function numerically. These results agree very well with those obtained analytically through the use of Bethe ansatz equations. Finally, we discuss how our results may be generalized to higher dimensions. Introduction:-In recent years considerable attention has been paid to the problem of strongly correlated electrons in low dimensional systems such as quantum wires or carbon nanotubes  1-3 . One of the most interesting phenomena observed experimentally in these systems is the fractional quantized Hall effect (FQHE)  4  . In particular it was shown that when the number of electrons N is odd ,the lowest Landau level(LLL) will contain only one electron per flux quanta  5  .The FQHEs have attracted much interest because they provide us with a unique opportunity to investigate many-body effects in condensed matter physics  6  .\nRecently, several authors  7-10  studied the properties of the modified coulomb interaction between two oppositely charged particles moving in a uniform magnetic field B perpendicularly to their plane of motion. They found that the ground-state energy depends crucially upon whether the total angular momentum J = L + S is zero or not where L is orbital angular momentum and S is spin angular momentum. For example if J=0 then the ground state energy is given by E0=−e2/lB+O(1/N),where lB=eB/mc is the magnetic length  11  .On the other hand if J=1/2 then the ground state energy takes the form E0",
        "watermark_text": "We explore the impact of an external magnetic force on the modified Coulomb potential for two particles with opposite charges and masses , which are confined to move along one dimension . We see that this scheme can be mapped onto a spinless fermion theory by using the Jordan - Wigner transformation .The ground state energy is calculated exactly within the framework of Bethe ansatz technique . It happens out that there exists a critical quantity of the magnetic force power beyond which the ground state remains degenerate .This result agrees well with previous quantitative calculations based on exact diagonalization technique . In addition we estimate the density - density correlation function as well as the velocity distribution relation numerically .These conclusions follow very best with those achieved analytically through the using of Bethe ansatz equations . Finally , we talk how our findings may be generalized to higher dimensions .Introduction : - In recent years considerable focus has been paid to the question of highly correlated electrons in low dimensional complexes such as quantum wires or carbon nanotubes 1 - 3 . One of the most exciting phenomena observed experimentally in these systems is the fractional quantized Hall influence ( FQHE ) 4 .In particular it was shown that when the number of atoms N is odd , the lowest Landau scale ( LLL ) will hold only one particle per flux quanta 5 . The FQHEs have garnered great concern because they give us with a unique opportunity to examine multiple - bodies phenomena in condensed matter theory 6 . Recently , various scientists 7 - 10 studied the properties of the modified coulomb interaction between two oppositely charged particles moving in a uniform magnetic force B perpendicularly to their direction of movement .They found that the ground - state energy relies crucially upon whether the total angular velocity J = L + S is zero or not where L is orbital angular velocity and S is spin angular velocity . For instance if J = 0 then the ground state energy is given by E0 = −e2 / lB + O ( 1 / N ) , where lB = eB / mc is the magnetic speed 11 . On the other hand if J = 1 / 2 then the ground state energy takes the form E0",
        "rewrite_text": "We investigate the effects of an external magnetic field on the modified Coulomb potential for two particles with opposite charges and masses, constrained to move in one dimension. This scenario can be transformed into a spinless fermion theory using the Jordan-Wigner transformation. We compute the ground state energy precisely by employing the Bethe ansatz technique. Our analysis reveals a critical value of the magnetic field strength beyond which the ground state maintains a degenerate configuration. This finding aligns closely with previous quantitative assessments conducted using exact diagonalization methods. Additionally, we numerically estimate the density-density correlation function and the velocity distribution relation, which correspond well with the analytical results obtained through the Bethe ansatz equations. Finally, we discuss the potential generalization of our results to higher dimensions.\n\n**Introduction:** In recent years, significant attention has been devoted to the study of highly correlated electrons in low-dimensional structures, such as quantum wires and carbon nanotubes. Among the most intriguing phenomena observed in these systems is the fractional quantum Hall effect (FQHE). Notably, it has been demonstrated that when the number of atoms \\( N \\) is odd, the lowest Landau level (LLL) accommodates only one particle per flux quantum. The FQHE is of particular interest as it provides a unique opportunity to explore many-body phenomena within condensed matter theory. Recently, several researchers have examined the modified Coulomb interaction between two oppositely charged particles moving under a uniform magnetic field \\( B \\) orthogonal to their motion. Their studies indicate that the ground state energy is heavily influenced by whether the total angular momentum \\( J = L + S \\) is zero or not, where \\( L \\) is the orbital angular momentum and \\( S \\) is the spin angular momentum. For example, if \\( J = 0 \\), the ground state energy is expressed as \\( E_0 = -e^2 / l_B + O(1/N) \\), where \\( l_B = eB/mc \\) represents the magnetic length. Conversely, if \\( J = 1/2 \\), the ground state energy is given in a different form.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.058229640253803,
        "rewrite-fast-z-score": -0.24413653763134782
    },
    {
        "original_text": "We study the evolutionarily stable strategy (ESS) and its stability properties for a class of ultimatum games with two-sided incomplete information, where one player is informed about his opponent s type while the other has no such information. We show that there exists a unique ESS which coincides with the Nash equilibrium if the set of types is finite or compactly supported on  0, 1  . If this set contains unbounded elements then we prove that the ESS may be unstable under small perturbations of the payoff functions. In particular, it can be shown that any ESS must satisfy certain conditions related to the distribution function of the types. Finally, we present some numerical examples illustrating our results. The evolutionary game theory studies how strategies evolve over time when players interact repeatedly within large populations. It provides a natural framework for analyzing strategic interactions between self-interested agents who are unable to commit themselves ex-ante but have the opportunity to learn by observing past play. A typical example of such situation arises in bargaining problems where each agent makes offers sequentially without knowing what proposals will be made by their opponents. This problem was first studied by Guth et al. (1982) , who introduced the so-called ultimatum game as a model of bargaining between two selfish individuals. In this game, Player 1 proposes a division of a fixed amount of money M into shares x and y = M −x offered to himself and Player 2 respectively; Player 2 either accepts or rejects the offer. If he accepts, both players receive their respective shares according to the proposal; otherwise they get nothing. Since the seminal work of Guth et al., many authors have investigated various aspects of the ultimatum game including existence and multiplicity of equilibria, efficiency loss due to lack of commitment power etc. (see e.g. Binmore & Shaked, 1993; Ochs & Roth, 1989) . However, all these works assume complete information among the players.",
        "watermark_text": "We research the evolutionarily stable strategy ( ESS ) and its stability properties for a class of ultimatum games with two - sided unfinished data , where one player is informed about his opponent s type while the other has no such information . We see that there exists a unique ESS which coincides with the Nash equilibrium if the set of types is finite or compactly backed on 0 , 1 .If this set contains unbounded members then we prove that the ESS might be unstable under small perturbations of the payoff distributions . In particular , it can be shown that any ESS must satisfy certain conditions related to the distribution function of the types .Finally , we present some numerical examples illustrating our findings . The evolutionary game theory explores how strategies evolve over time when teams engage consistently within large populations .It provides a natural framework for evaluating strategic interactions between self - interested agents who are unable to commit themselves ex - ante but have the option to teach by observing past games . A typical example of such situation occurs in negotiating conflicts where each player makes options sequentially without knowing what bids will be made by their opponents .This problem was first examined by Guth et al . ( 1982 ) , who developed the so - called ultimatum game as a theory of negotiating between two selfish individuals .In this player , Player 1 suggests a division of a specified quantity of money M into shares x and y = M −x offered to himself and Player 2 respectively ; Player 2 either accepts or accepts the offer . If he agrees , both participants receive their respective stocks according to the proposal ; therefore they get nothing .Since the seminal research of Guth et al . , various scientists have analyzed various components of the ultimatum game including existence and multiplicity of equilibria , efficiency losing resulting to lack of commitment power etc . ( saw e . g .Binmore & Shaked , 1993 ; Ochs & Roth , 1989 ) . However , all these works assume complete data among the players .",
        "rewrite_text": "We investigate the evolutionarily stable strategy (ESS) and its stability characteristics in a specific class of ultimatum games featuring two-sided incomplete information. In this context, one player is aware of their opponent's type while the other lacks this knowledge. Our findings indicate that a unique ESS exists, coinciding with the Nash equilibrium when the set of types is either finite or compactly supported on the interval [0, 1]. However, if the set includes unbounded elements, we demonstrate that the ESS may become unstable in response to minor changes in the payoff distributions. Specifically, we show that any ESS must adhere to certain conditions pertaining to the distribution function of the types. Additionally, we present numerical examples that illustrate our results. Evolutionary game theory investigates how strategies develop over time as groups interact repeatedly within large populations. This framework is ideal for assessing strategic interactions among self-interested agents who cannot make binding commitments beforehand but can learn from observing past games. A common scenario of this nature arises in negotiation conflicts, where players make sequential choices without knowledge of their opponents' bids. This issue was initially explored by Guth et al. (1982), who introduced the ultimatum game as a theory of negotiation between two self-serving individuals. In this setup, Player 1 proposes a division of a specified amount of money, M, into shares x and y = M - x, allocated to themselves and Player 2, respectively. Player 2 then decides to either accept or reject the offer. If they accept, both players receive their proposed shares; if not, they both receive nothing. Since Guth et al.'s pivotal study, numerous researchers have examined various aspects of the ultimatum game, such as the existence and multiplicity of equilibria, the inefficiencies arising from a lack of commitment power, and more (see, e.g., Binmore & Shaked, 1993; Ochs & Roth, 1989). Nonetheless, all these studies assume that players operate with complete information.",
        "ori-fast-z-score": 1.5778641172210595,
        "water-fast-z-score": 8.166666666666666,
        "rewrite-fast-z-score": -0.24743582965269675
    },
    {
        "original_text": "Entropies are measures of uncertainty, and entropy power inequalities (EPIs) quantify the trade-off between information transmission rates in different communication channels.  In this work we present new proofs for EPIs based on information theory concepts such as mutual information and channel capacity.   We also show that these results can be used to prove Shannon s source coding theorem by applying them to an appropriate binary memoryless symmetric channel model. Finally, we discuss how our approach could potentially lead to improved bounds on the minimum distance of linear block codes over finite fields. Entropies are measures of uncertainty; entropy power inequalities (EPIS) quantify the tradeoff between information transmission rates in various communication channels. In this work we present novel proofs for EPIs using information-theory concepts like mutual information and channel capacity. We also demonstrate that these results may be utilized to verify Shannon s source coding theorem via their application to a suitable binary memoryless-symmetric channel model. Finally,we discuss how our technique might possibly yield better bounds on the minimal distance of linear block codes across finite fields.",
        "watermark_text": "Entropies are measures of uncertainty , and entropy energy inequalities ( EPIs ) quantify the trade - off between information transmission rates in different communication channels . In this research we present new proofs for EPIs based on communication theory ideas such as mutual information and channel capacity .We additionally prove that these results can be used to prove Shannon s source coding theorem by application them to an appropriate binary memoryless symmetric channel structure . Finally , we talk how our approach could potentially lead to greater bounds on the minimum length of linear block sequences over discrete fields .Entropies are measures of uncertainty ; entropy energy inequalities ( EPIS ) quantify the tradeoff between information transmission rates in different communication streams . In this research we present new proofs for EPIs using info - theory ideas like mutual information and channel capacity .We additionally prove that these results may be employed to confirm Shannon s source coding theorem via their application to a suitable binary memoryless - symmetric channel structure . Finally , we explain how our technique may possibly deliver improved bounds on the reduced distance of linear block sequences across bounded fields .",
        "rewrite_text": "Entropies serve as indicators of uncertainty, while entropy energy inequalities (EPIs) articulate the trade-off between information transmission rates across various communication channels. In this study, we provide new proofs for EPIs grounded in concepts from communication theory, including mutual information and channel capacity. We also demonstrate that these findings can be utilized to validate Shannon's source coding theorem by applying them to an appropriate binary memoryless symmetric channel model. Lastly, we discuss how our methodology could potentially yield tighter bounds on the minimum length of linear block sequences over discrete fields.",
        "ori-fast-z-score": 0.8,
        "water-fast-z-score": 7.0710678118654755,
        "rewrite-fast-z-score": 0.565685424949238
    },
    {
        "original_text": "We present a real-time renormalization-group approach for calculating the current through an interacting quantum dot coupled to leads at finite bias voltage, temperature, and gate voltage. We show that this method allows us to calculate the current as well as the time-dependent density matrix with high accuracy even when the system is far away from equilibrium. The results are compared to those obtained by numerically solving the Kadanoff-Baym equations within the Keldysh formalism. In particular we find excellent agreement between both methods if one chooses the cutoff scale appropriately. This shows that our method can be used to study strongly correlated systems out of equilibrium without any restriction on the strength of interactions or the coupling to external reservoirs. \nI. INTRODUCTIO N\nThe transport properties of nanoscale devices such as single-molecule transistors  1  , carbon nanotubes  2  , semiconductor nanowires  3  , and quantum dots  4  have attracted considerable interest over recent years due to their potential applications in future electronic circuits  5  . However, it has been shown recently  6  that these devices often operate far away from thermal equilibrium which makes theoretical predictions based on standard approaches like the LandauerBüttiker formula  7, 8  questionable  9  .\nIn order to describe non-equilibrium phenomena correctly, various extensions of the conventional scattering theory  10  were developed  11  -  16  . These theories usually rely on the assumption that the relaxation times associated with different degrees of freedom (e.g., charge carriers) are much longer than typical time-scales characterizing the dynamics of the device  17  . As a consequence they cannot account for situations where strong correlations lead to fast equilibration processes  18  . Moreover, most of them do not allow to treat non-Markovian effects arising e.g.",
        "watermark_text": "We present a real - time renormalization - group method for calculating the charge through an interacting quantum dot connected to leads at finite bias voltage , temperature , and gate current . We see that this method enables us to estimate the current as well as the time - dependent density matrix with high sensitivity especially when the system is far back from equilibrium .The results are compared to those achieved by numerically solving the Kadanoff - Baym equations within the Keldysh formalism . In particular we find excellent agreement between both approaches if one chooses the cutoff scale appropriately .This shows that our technique can be used to study highly correlated systems out of equilibrium without any restriction on the strength of coupling or the interaction to external reservoirs . I . INTRODUCTIO N The transport properties of nanoscale devices such as single - molecule transistors 1 , silicon nanotubes 2 , semiconductor nanowires 3 , and quantum dots 4 have garnered considerable interest over recent years owing to their potential applications in future electronic circuits 5 .However , it has been shown recently 6 that these systems often act close away from temperature equilibrium which makes theoretical estimates based on normal approaches like the LandauerBüttiker equation 7 , 8 questionable 9 . In order to explain non - equilibrium phenomena correctly , various extensions of the usual absorption theory 10 were developed 11 - 16 .These studies typically rely on the assumption that the relaxation minutes associated with various degrees of freedom ( e . g . , charge carriers ) are greatly lengthy than usual time - scales characterizing the dynamics of the device 17 . As a consequence they cannot account for situations where weak correlations lead to rapid equilibration processes 18 .Moreover , most of them do not enable to treat non - Markovian influences arising e . g .",
        "rewrite_text": "We introduce a real-time renormalization-group method for calculating the charge in an interacting quantum dot connected to leads at finite bias voltage, temperature, and gate current. This method allows us to estimate both the current and the time-dependent density matrix with remarkable sensitivity, particularly when the system is significantly out of equilibrium. We compare our results with those obtained by numerically solving the Kadanoff-Baym equations within the Keldysh formalism. Notably, we find that both approaches yield excellent agreement when the cutoff scale is chosen appropriately. This demonstrates that our technique is effective for studying highly correlated out-of-equilibrium systems without limitations on the coupling strength or interactions with external reservoirs.\n\nI. INTRODUCTION  \nThe transport properties of nanoscale devices, including single-molecule transistors, silicon nanotubes, semiconductor nanowires, and quantum dots, have attracted considerable interest in recent years due to their potential applications in future electronic circuits. However, recent findings indicate that these systems often operate away from thermal equilibrium, casting doubt on theoretical estimates derived from traditional methods such as the Landauer-Büttiker equation. To accurately describe non-equilibrium phenomena, various extensions of standard absorption theory have been developed. These studies typically assume that the relaxation times associated with different degrees of freedom (e.g., charge carriers) are much longer than the typical timescales governing the device dynamics. Consequently, they fail to address situations where weak correlations lead to rapid equilibration processes. Furthermore, most of these approaches do not account for non-Markovian effects arising, for example, from interactions with external reservoirs.",
        "ori-fast-z-score": -1.986254132645683,
        "water-fast-z-score": 5.361109642475096,
        "rewrite-fast-z-score": 0.16666666666666666
    },
    {
        "original_text": "We present results on testing different outer boundary conditions in numerical relativity, using two black hole spacetimes as testbeds.  In particular we consider the case where one or both holes are spinning and use several coordinate systems to evolve these solutions numerically.   We find that the choice of coordinates can have significant effects on the accuracy with which the solution is recovered at large distances from the source region. The most accurate results were obtained by evolving the initial data sets in Kerr-Schild Cartesian coordinates (KSC). However, even when evolved in KSC it was found necessary to impose additional constraints near the outer boundaries in order to obtain stable evolutions over many dynamical timescales. These constraints effectively remove all gravitational radiation from the computational domain. Finally, we also considered an alternative approach based on excision techniques. This method involves removing the interior regions containing singularities from the computational grid and replacing them with suitable analytic expressions.",
        "watermark_text": "We report findings on proving different exterior boundary parameters in mathematical relativity , using two black hole spacetimes as testbeds . In particular we study the case where one or both holes are twisting and use multiple coordinate networks to evolve these solutions numerically .We see that the selection of coordinates can have considerable effects on the accuracy with which the solve is recovered at large distances from the origin region . The most accurate conclusions were obtained by expanding the early data sets in Kerr - Schild Cartesian coordinates ( KSC ) .However , even when evolved in KSC it was found necessary to apply additional constraints near the exterior walls in order to obtain stable evolutions over numerous dynamical timescales . These limitations virtually remove all gravity radiation from the theoretical domain .Finally , we also considered an additional method using on excision techniques . This method means eliminating the interior regions containing singularities from the theoretical grid and combining them with suitable analytic expressions .",
        "rewrite_text": "We present our findings on the impact of various exterior boundary parameters in mathematical relativity, utilizing two black hole spacetimes as experimental frameworks. Our focus is particularly on scenarios involving one or both twisting black holes, and we employ multiple coordinate systems to numerically evolve these solutions. Our analysis indicates that the choice of coordinate system can significantly affect the precision with which the solution is recovered at large distances from the origin. The most reliable results were achieved by expanding the early data sets using Kerr-Schild Cartesian coordinates (KSC). Nevertheless, even when evolved in KSC, it was necessary to implement additional constraints near the exterior boundaries to ensure stable evolution over extended dynamical timescales. These constraints effectively eliminate all gravitational radiation from the theoretical framework. Lastly, we explored an alternative approach involving excision techniques, which entails removing the interior regions containing singularities from the computational grid and supplementing them with appropriate analytical expressions.",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 7.509343773089564,
        "rewrite-fast-z-score": 1.5652475842498528
    },
    {
        "original_text": "We present new exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors, which are generated by applying nonholonomic frame transforms (NFT) to known vacuum solutions. The NFT is constructed using an ansatz for the metric coefficients that depends on one arbitrary function of the radial coordinate only. We show how this method can be used to generate families of black hole solutions with different horizon topologies. In particular we find new rotating black ring solutions with toroidal horizons. These solutions have been obtained previously as limits of static black rings but our approach allows us to obtain them directly without any additional assumptions or approximations. Finally, we discuss some open problems related to these results. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq \nI. INTRODUCTORY REMARkS\nThe study of exact solutions to the Einstein equations has played a crucial role in understanding many aspects of general relativity. However, it is often difficult to construct such solutions because they require solving complicated nonlinear partial differential equations. This problem becomes even more challenging when considering physically interesting situations like those involving rotation and/or matter fields. Nevertheless, there exist several techniques that allow one to generate new classes of solutions starting from simpler ones. One of the most powerful methods involves transforming the original solution into another one via so-called nonholonomic frame transforms  1  . Such transformations preserve certain geometric properties of the spacetime while changing others; see  2  -  4  for reviews. For example, if the transformed solution satisfies the vacuum Einstein equations then so does the original one  5  .\nIn this work we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to generate new exact solutions describing stationary axisymmetric spacetimes: i.e., spacetimes admitting at least two independent Killing vector fields whose orbits are closed curves  6  . Stationary axisymmetric spacetimes play an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars, planets, and black holes  7, 8 ",
        "watermark_text": "We introduce novel exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors , which are produced by using nonholonomic frame transforms ( NFT ) to known vacuum solutions . The NFT is built using an ansatz for the metric coefficients that relies on one arbitrary function of the radial coordinate only .We see how this algorithm can be used to create families of grey hole problems with various horizon topologies . In particular we find unique spinning black ring solutions with toroidal horizons .These solutions have been achieved formerly as limits of static black rings but our approach allows us to obtain them directly without any additional constraints or approximations . Finally , we explain some open problems related to these results .PACS codes : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq I . INTRODUCTORY REMARkS The investigation of precise solutions to the Einstein equations has held a crucial role in understanding several parts of general relativity .However , it is often challenging to build such problems because they demand solving complicated nonlinear partial differential equations . This problem remains especially more challenging when exploring physically exciting situations like those concerning rotation and / or matter forces .Nevertheless , there remain many procedures that enable one to create fresh categories of solutions starting from simpler ones . One of the most popular methods means mapping the previous solve into another one via so - called nonholonomic frame transforms 1 .Such transformations maintain certain geometric properties of the spacetime while altering others ; look 2 - 4 for reviews . For instance , if the transformed solution satisfies the vacuum Einstein equations then so does the previous one 5 .In this research we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to create novel exact solutions governing stationary axisymmetric spacetimes : i . e . , spacetimes admitting at least two independent Killing matrix fields whose orbits are closed curves 6 . Stationary axisymmetric spacetimes serve an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars , planets , and dark holes 7 , 8",
        "rewrite_text": "We present new exact solutions to the Einstein field equations for stationary axisymmetric spacetimes characterized by two commuting Killing vectors. These solutions are derived using nonholonomic frame transforms (NFT) applied to known vacuum solutions. The NFT is formulated through an ansatz for the metric coefficients that hinges on a single arbitrary function of the radial coordinate. We demonstrate how this method can generate families of grey hole scenarios with diverse horizon topologies. Notably, we discover unique spinning black ring solutions featuring toroidal horizons. While these solutions have previously been obtained as limits of static black rings, our approach enables us to derive them directly without imposing additional constraints or approximations. Lastly, we discuss several open problems related to our findings.\n\n**PACS codes:** 04.20.-q, 11.10.-z, 98.80.Cq \n\n**I. INTRODUCTORY REMARKS** \n\nThe quest for precise solutions to the Einstein equations plays a vital role in enhancing our understanding of various aspects of general relativity. However, constructing such solutions is often a complex task due to the necessity of solving intricate nonlinear partial differential equations. This challenge is even more pronounced when investigating scenarios involving rotation and/or matter forces. Nevertheless, numerous methods exist to generate new classes of solutions starting from simpler ones. One of the most widely used approaches involves mapping existing solutions to new ones through what are known as nonholonomic frame transforms. These transformations preserve certain geometric properties of spacetime while modifying others; see references for reviews. For instance, if the transformed solution satisfies the vacuum Einstein equations, then the original solution does as well. In this work, we employ nonholonomic frame transforms on known vacuum solutions of the Einstein equations to produce innovative exact solutions for stationary axisymmetric spacetimes, which contain at least two independent Killing vector fields with closed orbits. Such spacetimes are crucial in astrophysics as they model the exterior gravitational fields of rotating objects such as stars, planets, and black holes.",
        "ori-fast-z-score": -1.3805369799252667,
        "water-fast-z-score": 7.777427086962838,
        "rewrite-fast-z-score": 1.372487132993442
    },
    {
        "original_text": "The concept of dynamical 3-space is introduced in this review article as an alternative to the standard space-time picture of relativistic physics. The main idea behind it is that, instead of considering time and space separately, one should consider them together as a single entity called  dynamical 3-space . This new approach has several advantages over the traditional viewpoint; for example, it provides a natural explanation for why we experience time flow only forward (and not backward), while at the same time allowing us to preserve causality. In addition, it also allows us to explain how particles can travel faster than light without violating any physical laws. Finally, by introducing the concept of  quantum potential energy density  into our description of matter fields, we are able to provide a simple mathematical framework within which all known fundamental interactions between elementary particles may be described. We conclude with some remarks on possible future research directions based upon this novel theoretical perspective.",
        "watermark_text": "The concept of dynamical 3 - space is proposed in this review article as an alternative to the standard space - time view of relativistic physics . The main idea behind it is that , rather of considering time and space simultaneously , one should consider them combined as a common organization called dynamical 3 - space .This new approach has numerous benefits over the usual interpretation ; for example , it gives a natural explanation for why we experience time flow only ahead ( and not backward ) , while at the same time allowing us to restore causality . In addition , it also enables us to explain how atoms can travel quicker than light without violating any physical rules .Finally , by bringing the notion of quantum potential energy density into our description of matter fields , we are able to provide a simple mathematical framework within which all known fundamental interactions between elementary particles may be described . We continue with some remarks on potential future research paths based upon this new theoretical perspective .",
        "rewrite_text": "This review article presents the concept of dynamical 3-space as an alternative to the conventional space-time framework of relativistic physics. The core idea is to merge time and space into a unified construct known as dynamical 3-space, rather than treating them as separate entities. This innovative approach offers several advantages over traditional interpretations; for instance, it naturally explains why we perceive the flow of time as forward rather than backward, while also reestablishing causality. Additionally, it provides a means to account for how atoms can move faster than light without breaching any physical laws. By incorporating the notion of quantum potential energy density into the description of matter fields, we create a straightforward mathematical framework that encompasses all known fundamental interactions between elementary particles. We conclude with insights on potential avenues for future research stemming from this fresh theoretical perspective.",
        "ori-fast-z-score": 1.3093073414159544,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results of laboratory measurements on composite interstellar grains, which are composed of amorphous silicate and carbonaceous materials with various compositions. The samples were irradiated by energetic protons in order to simulate cosmic ray bombardment under conditions similar to those found in dense clouds where dust is formed. We have measured infrared (IR) emission spectra before and after proton irradiation at energies ranging from 1 MeV/nucleon up to 100 MeV/nucleon for different sample temperatures between 10 K and 300 K. In addition we performed IR transmission spectroscopy experiments using synchrotron radiation as well as electron energy loss spectroscopy (EELS). Our experimental data show that the composition of the grain material has an important influence on its response towards proton irradiation. For example, the intensity ratio of the 3.4 micron feature over the 11 micron feature increases significantly when the amount of aromatic hydrocarbons relative to silicates decreases.",
        "watermark_text": "We present the results of research studies on composite interstellar fragments , which are composed of amorphous silicate and carbonaceous materials with various compositions . The samples were irradiated by energetic protons in order to simulate cosmic ray bombardment under environments similar to those observed in dense clouds where dust is formed .We have recorded laser ( IR ) emission spectra before and after proton irradiation at energies ranging from 1 MeV / nucleon up to 100 MeV / nucleon for different specimen temperatures between 10 K and 300 K . In addition we performed IR transmission spectroscopy observations use synchrotron rays as well as electron energy loss spectroscopy ( EELS ) . Our research data reveal that the composition of the grain matter has an important affect on its reactions towards proton irradiation .For instance , the intensity ratio of the 3 . 4 micron feature over the 11 micron feature grows significantly when the proportion of aromatic hydrocarbons compared to silicates drops .",
        "rewrite_text": "We present findings from research studies on composite interstellar fragments, which consist of amorphous silicate and carbonaceous materials with varying compositions. To mimic cosmic ray bombardment as experienced in dense dust-forming clouds, the samples were irradiated with energetic protons. We recorded laser (IR) emission spectra before and after proton irradiation at energies ranging from 1 MeV/nucleon to 100 MeV/nucleon, while varying specimen temperatures from 10 K to 300 K. Additionally, we conducted IR transmission spectroscopy using synchrotron radiation and performed electron energy loss spectroscopy (EELS). Our data indicate that the composition of the grain material significantly influences its reaction to proton irradiation. Notably, the intensity ratio of the 3.4-micron feature to the 11-micron feature increases substantially when the proportion of aromatic hydrocarbons relative to silicates decreases.",
        "ori-fast-z-score": -1.8073922282301278,
        "water-fast-z-score": 4.131182235954578,
        "rewrite-fast-z-score": 0.5443310539518174
    },
    {
        "original_text": "We present the first fully self-consistent, atomistic quantum transport calculations for ballistic graphene nanoribbons (GNRs) with realistic band structure and electrostatic potential profiles using nonequilibrium Green s function formalism in combination with density functional theory (DFT). We show that the GNRs  electronic properties are strongly dependent on their widths as well as edge structures. The calculated current-voltage characteristics reveal several interesting features such as negative differential resistance at low bias voltages due to resonant tunneling through localized states near the Fermi level. In addition, we find that the presence of hydrogen passivation layers can significantly enhance the device performance by suppressing the backscattering effect caused by defects or impurities along the edges. \n \n Keywords: Ballistic transport, Graphene nanoribbon, Nonequilibrium Green s functions, Density functional theory, Quantum transport calculation. 1 Introduction \n \n Graphene is an emerging material which has attracted considerable attention recently because it exhibits unique physical properties  1  . It consists of carbon atoms arranged into a honeycomb lattice where each carbon atom forms covalent bonds with three neighboring carbons  2  . Due to its two-dimensional nature, graphene shows high carrier mobility  3  , thermal conductivity  4  , mechanical strength  5  , optical transparency  6  , and flexibility  7  . These remarkable properties make graphene promising candidates for future nanoelectronic devices  8  .\n \nGraphene nanoribbons (G-NR), i.e., strips of graphene with finite width  9  , have been proposed as building blocks for various applications including transistors  10  , interconnects  11  , photodetectors  12  , solar cells  13  , sensors  14  , etc.. Compared to conventional silicon-based electronics  15  , GNRs offer many advantages  16  : they exhibit higher electron mobilities  17  ; they allow better control over the charge carriers  18  ; they provide more design freedom  19  ; and they enable new functionality  20  . However, there still exist some challenges associated with practical realization of GNRs  21  . For example, the fabrication process requires precise control of ribbon width  22  and edge roughness  23  . Moreover, the electrical properties of GNRs depend sensitively",
        "watermark_text": "We present the first fully self - stable , atomistic quantum travel calculations for ballistic graphene nanoribbons ( GNRs ) with realistic band structure and electrostatic potential configurations using nonequilibrium Green s function formalism in combination with density functional theory ( DFT ) . We suggest that the GNRs magnetic properties are strongly dependent on their widths as well as edge properties .The measured power - voltage parameters reveal numerous interesting features such as negative integral resistance at low bias voltages due to resonant tunneling through confined states near the Fermi level . In addition , we find that the presence of hydrogen passivation layers can significantly boost the device performance by suppressing the backscattering effect caused by defects or impurities along the edges .Keywords : Ballistic diffusion , Graphene nanoribbon , Nonequilibrium Green s functions , Density functional theory , Quantum transport calculation . 1 Introduction Graphene is an developing material which has garnered considerable scrutiny lately because it displays unusual physical properties 1 .It consists of carbon atoms arranged into a honeycomb structure where each carbon molecule forms covalent bonds with three adjacent carbons 2 . Due to its two - dimensional existence , graphene shows high carrier density 3 , thermal conductivity 4 , thermal strength 5 , optical transparency 6 , and flexibility 7 .These exceptional properties make graphene promising candidates for future nanoelectronic technologies 8 . Graphene nanoribbons ( G - NR ) , i . e . , sheets of graphene with finite width 9 , have been proposed as building blocks for various uses including transistors 10 , interconnects 11 , photodetectors 12 , sun cells 13 , cameras 14 , etc . .Compared to conventional silicon - based equipment 15 , GNRs offer several advantages 16 : they represent greater electron mobilities 17 ; they allow easier control over the charge carriers 18 ; they give more configuration autonomy 19 ; and they enable new capabilities 20 . However , there still emerge some challenges associated with practical development of GNRs 21 .For instance , the fabrication process requires careful management of ribbon diameter 22 and edge roughness 23 . Moreover , the electrical properties of GNRs depend sensitively",
        "rewrite_text": "We present the first fully self-stable, atomistic quantum travel calculations for ballistic graphene nanoribbons (GNRs) that incorporate realistic band structures and electrostatic potential configurations, using a combination of nonequilibrium Green's function formalism and density functional theory (DFT). Our findings indicate that the magnetic properties of GNRs are significantly influenced by both their widths and edge characteristics. The power-voltage measurements reveal several intriguing features, including negative integral resistance at low bias voltages, which arises from resonant tunneling through confined states near the Fermi level. Furthermore, we discover that hydrogen passivation layers can greatly enhance device performance by mitigating the backscattering effects induced by defects or impurities along the edges.\n\nKeywords: Ballistic diffusion, Graphene nanoribbon, Nonequilibrium Green's functions, Density functional theory, Quantum transport calculation. \n\n1. Introduction  \nGraphene is a promising material that has attracted significant attention due to its remarkable physical properties. It is composed of carbon atoms arranged in a honeycomb lattice, where each carbon atom forms covalent bonds with three neighboring carbons. Its two-dimensional structure enables graphene to exhibit high carrier density, thermal conductivity, thermal strength, optical transparency, and flexibility. These exceptional characteristics position graphene as a potential candidate for next-generation nanoelectronic technologies. Graphene nanoribbons (GNRs), which are strips of graphene with finite widths, have been suggested as crucial components for a variety of applications, including transistors, interconnects, photodetectors, solar cells, and cameras. Compared to traditional silicon-based devices, GNRs offer several advantages: enhanced electron mobilities, easier control over charge carriers, greater configurational flexibility, and new functionalities. However, there are still challenges in the practical development of GNRs, such as the need for precise control over ribbon diameter and edge roughness. Additionally, the electrical properties of GNRs are highly sensitive to these factors.",
        "ori-fast-z-score": -0.32025630761017426,
        "water-fast-z-score": 8.327056459580765,
        "rewrite-fast-z-score": 2.1517753103661565
    },
    {
        "original_text": "We study the evolution of age structure and life history traits by using an age-structured population model with stochastic demographic events, where individuals can reproduce at any age. We show that selection against demographic stochasticity leads to increased reproductive effort early in life (i.e., earlier reproduction) and decreased mortality rates later in life. This is because higher reproductive efforts increase offspring numbers but also lead to more deaths due to competition for resources among siblings. In contrast, lower mortality rates decrease sibling competition and thus reduce the number of deaths caused by this source of demographic stochasticity. Our results are robust across different levels of environmental stochasticity and initial conditions. The evolutionary consequences of demographic stochasticity have been studied extensively in recent years  1–3  . However, most studies on this topic focus on populations without age structure or assume that all individuals reproduce at the same age  4–6  , which may not be realistic  7–9  .\nHere we use an age-structured model with stochastic demographic events  10, 11  to investigate how selection acts on life-history traits when there is variation in individual ages at first reproduction  12  . Specifically, we consider a scenario where individuals can reproduce at various ages and compete for limited resources within their family groups  13  . We find that selection against demographic stochasticities increases reproductive effort early in life and decreases mortality rates late in life. These findings are consistent with previous theoretical work showing that selection favors reduced variance in offspring number  14–18  .",
        "watermark_text": "We research the evolution of age structure and life history characteristics by using an age - organized community model with stochastic demographic conditions , where adults can mature at any age . We suggest that selection against demographic stochasticity causes to greater sexual attempt early in life ( i . e . , earlier reproduction ) and diminished mortality rates later in life .This is because higher reproductive attempts increase offspring numbers but also lead to more killed due to competition for resources among brothers . In comparison , lower deaths levels decrease brother conflict and therefore decrease the quantity of fatalities caused by this source of demographic stochasticity .Our results are robust across different levels of environmental stochasticity and original conditions . The evolutionary impacts of demographic stochasticity have been studied significantly in recent periods 1 – 3 .However , most studies on this theme focus on groups without age structure or assume that all individuals mature at the same age 4 – 6 , which may not be realistic 7 – 9 . Here we using an age - organized model with stochastic demographic patterns 10 , 11 to examine how selection acts on life - history characteristics when there is variation in individual ages at first reproduction 12 .Specifically , we investigate a situation where adults can mature at several ages and contest for limited supplies within their family groups 13 . We see that selection against demographic stochasticities raises reproductive effort early in life and decreases mortality rates soon in life .These conclusions are compatible with previous conceptual research indicating that selection favors reduced variance in offspring year 14 – 18 .",
        "rewrite_text": "We investigate the evolution of age structure and life history traits using an age-organized community model under stochastic demographic conditions, where adults can reach maturity at various ages. Our findings suggest that selection against demographic stochasticity leads to increased reproductive efforts early in life, resulting in earlier reproduction and reduced mortality rates later on. Higher reproductive efforts can raise the number of offspring, but they may also intensify competition among siblings for resources, potentially increasing mortality rates. Conversely, lower death rates can mitigate sibling competition, thereby decreasing fatalities arising from demographic stochasticity. Our results remain consistent across various levels of environmental stochasticity and initial conditions. While the evolutionary effects of demographic stochasticity have garnered significant attention in recent research, most studies have primarily focused on populations without age structure or have assumed uniform maturation ages for individuals, which may not accurately reflect reality. In this study, we employ an age-organized model with stochastic demographic patterns to explore how selection influences life-history traits in a context where age at first reproduction varies among individuals. Specifically, we analyze scenarios in which adults mature at different ages and compete for limited resources within their familial groups. Our results indicate that selection against demographic stochasticity promotes earlier reproductive efforts while lowering mortality rates early in life. These conclusions align with previous conceptual research suggesting that selection favors reduced variance in offspring production.",
        "ori-fast-z-score": -1.2456821978060995,
        "water-fast-z-score": 8.055411545812778,
        "rewrite-fast-z-score": 0.5813183589761798
    },
    {
        "original_text": "We study evolutionary dynamics in an ensemble of genotypes, each with its own fitness value and connected to other genotypes by mutations. We show that for large ensembles there is always one genotype which has more connections than any other genotype. This genotype dominates all others at equilibrium. The dominance of this genotype depends only on the distribution of mutational distances between pairs of genotypes. For example, if we consider two different distributions of mutational distances (the exponential and power law), then the same genotype will dominate both cases. However, the time it takes to reach equilibrium can be very different depending on how many paths lead away from the dominant genotype. In particular, when the number of paths leading away from the dominant genotype increases exponentially as a function of distance, the system reaches equilibrium much faster compared to the case where the number of paths grows slower or even remains constant. \n \n We also find that the average population size of the dominant genotype decreases slowly over time until reaching equilibrium.",
        "watermark_text": "We see adaptive dynamics in an ensemble of genotypes , each with its own health quality and related to other genotypes by genes . We see that for large ensembles there is usually one genotype which has more networks than any other genotype .This genotype dominates all others at equilibrium . The dominance of this genotype determines only on the spread of mutational lengths between pairs of genotypes .For instance , if we treat two different distributions of mutational distances ( the exponential and power law ) , then the same genotype will dominate both cases . However , the period it takes to reach equilibrium can be very different depending on how many routes go away from the dominant genotype .In particular , when the quantity of paths leading away from the dominant genotype rises exponentially as a function of distance , the system reaches equilibrium somewhat quickly contrast to the case where the quantity of paths grows faster or even keeps constant . We additionally find that the average population size of the dominant genotype decreases slowly over time until achieving equilibrium .",
        "rewrite_text": "Adaptive dynamics can be observed within a group of genotypes, each characterized by its distinct health quality and interconnected through genetic relationships. In large groups, there often emerges a single genotype that possesses more connections than any other, thereby dominating the equilibrium state. The extent of this dominance is influenced solely by the variability of mutational lengths between genotype pairs. For example, whether we analyze two different mutational distance distributions, such as exponential and power law, the same genotype tends to assert dominance in both scenarios. However, the time required to reach equilibrium can vary significantly based on the number of pathways diverging from the dominant genotype. Notably, when the number of pathways increasing with distance from the dominant genotype grows exponentially, the system stabilizes relatively quickly. In contrast, if the number of pathways grows more rapidly or remains constant, equilibrium takes longer to achieve. Additionally, we observe that the average population size of the dominant genotype gradually decreases over time until it reaches equilibrium.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 6.812733176437583,
        "rewrite-fast-z-score": -1.811643254631353
    },
    {
        "original_text": "We present an open-source code, called XSCORT (X-ray Spectral Code for Reprocessed Torus), which computes the reprocessing of radiation by optically-thick matter in the form of a torus and its associated wind. The code is written as a set of IDL routines that can be easily modified to study different geometries or physical conditions. We describe how we implemented our model using Monte Carlo techniques and discuss some tests performed with simulated data. Finally, we show examples of applications of this new tool to two well-studied objects, NGC 4151 and Mrk 509. \nThe main goal of XSCORT is to provide a flexible framework where one can explore the effects of various parameters such as: geometry, density distribution, chemical composition, ionization state, etc., on the observed spectral properties of active galactic nuclei (AGN). This will allow us to better understand their nature and evolution. \n \n In particular, we are interested in studying the effect of the presence of an outflowing component on the shape of the reflection hump produced by the innermost regions of the accretion disc around supermassive black holes. These winds may play an important role in shaping the broad-band continuum emission of these sources through absorption and/or scattering processes. They also affect the amount of material available to produce the reflected emission at larger distances from the central source.",
        "watermark_text": "We produce an open - source code , known XSCORT ( X - ray Spectral Code for Reprocessed Torus ) , which computes the reprocessing of radiation by optically - heavy material in the form of a torus and its attendant wind . The language is read as a group of IDL routines that can be easily modified to study various geometries or material conditions .We discuss how we implemented our model utilizing Monte Carlo methods and explain some experiments conducted with simulated evidence . Finally , we give instance of applications of this new method to two better - examined objects , NGC 4151 and Mrk 509 .The main goal of XSCORT is to provide a broad platform where one can examine the effects of several variables such as : topography , density flow , chemical composition , ionization state , etc . , on the known spectral properties of active galactic nuclei ( AGN ) . This will provide us to easier understand their nature and evolution .In particular , we are concerned in examining the impact of the presence of an outflowing component on the form of the reflection hump produced by the innermost sectors of the accretion disc around supermassive black holes . These winds may play an important role in shaping the broad - band continuum emission of these sources through absorption and / or scattering mechanisms .They also affect the quantity of material provided to produce the reflected emission at larger distances from the main source .",
        "rewrite_text": "We have developed an open-source code called XSCORT (X-ray Spectral Code for Reprocessed Torus) that calculates the radiation reprocessing by optically dense material configured in a toroidal shape, along with its associated wind. This code consists of a set of IDL routines, which are easily customizable for analyzing different geometries and material conditions. In this discussion, we outline how our model is implemented using Monte Carlo methods and share results from experiments conducted with simulated data. We also demonstrate the application of this new approach to two well-studied objects, NGC 4151 and Mrk 509. XSCORT's primary objective is to offer a comprehensive platform for exploring the impact of various parameters—such as topography, density flow, chemical composition, and ionization state—on the spectral characteristics of active galactic nuclei (AGN). This will enhance our understanding of their nature and evolution. Specifically, we focus on investigating how an outflowing component influences the reflection hump produced by the innermost regions of the accretion disk surrounding supermassive black holes. These outflows are likely significant in shaping the broad-band continuum emission of these sources through absorption and scattering processes. Additionally, they can influence the amount of material responsible for generating reflected emissions at greater distances from the primary source.",
        "ori-fast-z-score": -1.7650452162436565,
        "water-fast-z-score": 6.601706163700764,
        "rewrite-fast-z-score": 0.8081220356417685
    },
    {
        "original_text": "We report the observation of electron-hole puddles in graphene using scanning single-electron transistors (SETs). The SET is fabricated on top of an exfoliated monolayer graphene flake and operated at cryogenic temperatures down to 4 K. We observe that the conductance through the SET depends strongly on its position with respect to the underlying graphene sheet, which we attribute to local variations in charge carrier density induced by charged impurities trapped between the substrate and the graphene layer. This effect can be suppressed by applying a gate voltage Vg = -40 V across the graphene sample. Our results demonstrate that the use of SETs as probes for studying electronic properties of two-dimensional materials such as graphene has great potential. In recent years there have been significant advances in the fabrication of devices based on carbon nanotubes  1  , silicon nanowires  2  or semiconductor quantum dots  3  . These nanostructures are used as active elements in various types of sensors  4  , optoelectronic  5  and photovoltaic  6  applications. However, these structures suffer from several drawbacks including poor reproducibility due to their small size and low yield during growth processes  7, 8  .\nIn contrast, graphene  9  offers many advantages over other two dimensional materials  10  : it is mechanically flexible  11  , chemically stable  12  , biocompatible  13  and electrically conductive  14  . Moreover, it can be produced in large quantities via chemical vapor deposition  15  or mechanical exfoliation  16  techniques  17  . Recently, graphene-based field-effect transistors  18  were demonstrated  19, 20  opening up new avenues towards high-performance electronics  21  . Despite all these attractive features, however, one major challenge remains in achieving high-quality electrical contacts to graphene  22  .",
        "watermark_text": "We report the observation of electron - hole puddles in graphene using scanning single - ion transistors ( SETs ) . The SET is manufactured on top of an exfoliated monolayer graphene flake and operated at cryogenic temperatures down to 4 K . We establish that the conductance through the SET depends strongly on its position with regard to the underlying graphene cover , which we attribute to local differences in charge carrier density resulting by charged impurities stored between the substrate and the graphene layer .This phenomenon can be suppressed by using a gate pressure Vg = - 40 V across the graphene sample . Our results show that the using of SETs as probes for studying electronic properties of two - dimensional surfaces such as graphene has tremendous promise .In recent years there have been significant advances in the fabrication of applications based on carbon nanotubes 1 , silicon nanowires 2 or semiconductor quantum dots 3 . These nanostructures are applied as active elements in different kinds of sensors 4 , optoelectronic 5 and photovoltaic 6 applications .However , these structures face from several drawbacks including low reproducibility due to their tiny size and low yield during growth processes 7 , 8 . In comparison , graphene 9 offers several advantages over other two dimensional ceramics 10 : it is mechanically flexible 11 , chemically neutral 12 , biocompatible 13 and electrically conductive 14 .Moreover , it can be made in large quantities via chemical vapor precipitation 15 or mechanical exfoliation 16 techniques 17 . Recently , graphene - based field - effect transistors 18 were shown 19 , 20 closing up new avenues towards high - performance computers 21 .Despite all these interesting features , however , one major challenge lies in obtaining high - grade electrical contacts to graphene 22 .",
        "rewrite_text": "We present the observation of electron-hole puddles in graphene utilizing scanning single-ion transistors (SETs). The SET is fabricated atop an exfoliated monolayer graphene flake and operates at cryogenic temperatures down to 4 K. We find that the conductance through the SET is highly dependent on its position relative to the underlying graphene layer, which we attribute to localized variations in charge carrier density caused by charged impurities located between the substrate and the graphene. This effect can be mitigated by applying a gate voltage Vg = -40 V across the graphene sample. Our findings indicate that SETs hold significant potential as probes for investigating the electronic properties of two-dimensional surfaces like graphene. Recent advancements in the fabrication of carbon nanotube-based applications, silicon nanowires, and semiconductor quantum dots have paved the way for their use as active elements in various sensors, optoelectronic devices, and photovoltaic applications. However, these nanostructures often suffer from challenges such as low reproducibility due to their small size and low yield during growth processes. In contrast, graphene offers several advantages over other two-dimensional materials: it is mechanically flexible, chemically stable, biocompatible, and highly conductive. Additionally, it can be produced in large quantities through chemical vapor deposition or mechanical exfoliation. Recently, graphene-based field-effect transistors have opened new possibilities for high-performance computing. Nevertheless, a significant obstacle remains in achieving high-quality electrical contacts to graphene.",
        "ori-fast-z-score": -1.034792955221957,
        "water-fast-z-score": 5.9941491941228415,
        "rewrite-fast-z-score": -0.19611613513818404
    },
    {
        "original_text": "We study the scattering amplitudes for gluons and gravitons at strong coupling using classical string solutions in Anti-de Sitter space (AdS). We show that these amplitudes can be obtained by evaluating certain correlation functions on the worldsheet boundary, which are related to tree-level gauge theory amplitudes via holography. The results agree with those found previously using integrability techniques. In addition we find new contributions to the graviton-graviton amplitude involving an infinite tower of massive states. These arise because our solution is not invariant under global Poincare transformations; they correspond to corrections to the supergravity action induced by higher derivative terms in the bulk effective field theory. \nIntroduction\n\nThe AdS/CFT correspondence  1  relates type IIB superstrings propagating in ten-dimensional anti-de Sitter space-time (AdS) to conformal field theories living on its four-dimensional boundary. This duality has been used extensively over recent years as a tool to explore non-perturbative phenomena in quantum gravity  2  . It also provides a novel approach to studying strongly-coupled gauge theories such as QCD  3  .\nIn this talk we will consider the simplest example of the AdS/CFT correspondence -the maximally supersymmetric Yang-Mills (N=4 SYM) theory  4  , whose dual description involves type IIA strings moving in AdS 5 × S 5  5  . At weak  t Hooft coupling λ = g 2 Y M N ≪ 1, where g Y M denotes the Yang-Mills coupling constant, perturbative calculations have shown that the two descriptions match exactly  6  . However, it remains unclear how to calculate quantities like scattering amplitudes directly within the gauge theory at large values of λ  7, 8  . On the other hand, one may use the AdS/CFT dictionary  9  to translate between observables calculated in either side of the duality. For instance, the expectation value of Wilson loops in the gauge theory corresponds to the area of minimal surfaces embedded into AdS  10  ; while n-point correlators of local operators in the gauge theory are given by functional integrals over n-punctured Riemann surfaces  11  .",
        "watermark_text": "We research the scattering amplitudes for gluons and gravitons at strong coupling using traditional string solutions in Anti - de Sitter space ( AdS ) . We see that these amplitudes can be obtained by evaluating several coupling functions on the worldsheet border , which are related to tree - level gauge theory amplitudes via holography .The results agree with those shown previously used integrability methods . In addition we find new contributions to the graviton - graviton amplitude involving an endless tower of large states .These occur because our solution is not invariant under universal Poincare shifts ; they relate to corrections to the supergravity act caused by higher derivative conditions in the bulk effective field theory . Introduction The AdS / CFT relationship 1 relates class IIB superstrings propagating in ten - dimensional anti - de Sitter space - time ( AdS ) to conformal field schemes residing on its four - dimensional boundary .This duality has been used heavily over recent years as a platform to examine anti - perturbative processes in particle gravity 2 . It additionally offers a new approach to investigating strongly - coupled gauge fields such as QCD 3 .In this talk we will take the simplest example of the AdS / CFT relationship - the maximally supersymmetric Yang - Mills ( N = 4 SYM ) theory 4 , whose dual description requires type IIA strings moving in AdS 5 × S 5 5 . At weak t Hooft coupling λ = g 2 Y M N [UNK] 1 , where h Y M denotes the Yang - Mills coupling constant , perturbative calculations have shown that the two explanations match exactly 6 .However , it remains unclear how to estimate quantities like absorption amplitudes directly within the gauge theory at large values of λ 7 , 8 . On the other hand , one may use the AdS / CFT dictionary 9 to translate between observables calculated in either face of the duality .For instance , the expectation value of Wilson loops in the gauge theory corresponds to the area of minimal surfaces embedded into AdS 10 ; while n - point correlators of local operators in the gauge theory are given by functional integrals over n - punctured Riemann spheres 11 .",
        "rewrite_text": "We investigate the scattering amplitudes of gluons and gravitons under strong coupling conditions, utilizing traditional string solutions within Anti-de Sitter space (AdS). Our analysis reveals that these amplitudes can be derived by evaluating various coupling functions at the boundary of the worldsheet, which are connected to tree-level gauge theory amplitudes through holography. The findings are consistent with previous results obtained through integrability methods. Furthermore, we uncover new contributions to the graviton-graviton amplitude that involve an infinite series of large states. These contributions arise due to our solution's lack of invariance under universal Poincaré shifts and correspond to modifications of the supergravity action induced by higher derivative terms in the bulk effective field theory.\n\n**Introduction**: The AdS/CFT correspondence relates type IIB superstrings propagating in ten-dimensional Anti-de Sitter space-time (AdS) to conformal field theories defined on its four-dimensional boundary. This duality has been extensively utilized in recent years to explore non-perturbative phenomena in gravitational contexts and to investigate strongly-coupled gauge theories like QCD. In this discussion, we consider the simplest case of the AdS/CFT correspondence, which is the maximally supersymmetric Yang-Mills (N=4 SYM) theory. Its dual description involves type IIA strings moving in AdS5 × S5. At weak 't Hooft coupling (λ = g²YM N << 1), where gYM is the Yang-Mills coupling constant, perturbative calculations demonstrate perfect agreement between both descriptions. However, it remains challenging to compute quantities such as absorption amplitudes directly within the gauge theory at large coupling values. Conversely, the AdS/CFT dictionary allows for the translation of observables calculated in either side of the duality. For example, the expectation value of Wilson loops in the gauge theory corresponds to the area of minimal surfaces embedded in AdS, while n-point correlation functions of local operators in the gauge theory are represented by functional integrals over n-punctured Riemann surfaces.",
        "ori-fast-z-score": -1.643989873053573,
        "water-fast-z-score": 5.488853867477571,
        "rewrite-fast-z-score": 0.2491364395612199
    },
    {
        "original_text": "The increasing amount and complexity of oceanographic data requires new approaches to the analysis, visualization and interpretation of these datasets. In this work we present an interactive visual analytics system that allows users to explore large volumes of 3D oceanographic data in order to detect patterns and anomalies. The main goal is to provide scientists with powerful tools to analyze their data without having to be experts on computer graphics or visualization techniques. We use state-of-the-art volume rendering algorithms combined with advanced interaction techniques such as brushing, linking and querying. Our approach has been tested by using real-world oceanographic data sets collected during several research cruises around Europe. This article presents our results and discusses future directions. Oceanographers are increasingly collecting massive amounts of data about the oceans  physical properties (e.g., temperature, salinity) and biological processes (e.g., plankton blooms). These data can be used to study phenomena like global warming, pollution spreading, marine life migration etc.. However, analyzing and interpreting these huge volumes of data remains challenging due to its high dimensionality and complex relationships between variables. Interactive visual analytics systems have shown great potential in helping scientists discover hidden patterns and anomalies within their data  1  . They allow researchers to interactively explore their data through different views while simultaneously performing queries and applying filters  2  .\nIn this work we present Diagnostic Tools for Unstructured Data (DUT), a novel visual analytics tool designed specifically for exploring large volumes of 3D oceanic data. DUT provides scientists with powerful tools to perform exploratory analyses of their data without requiring them to be experts on computer science or visualization techniques  3  , thus allowing them to focus more on the actual content of their data rather than how it should be presented  4  .",
        "watermark_text": "The increasing quantity and complexity of oceanographic data requires innovative approaches to the analysis , visualization and understanding of these datasets . In this project we present an interactive visual analytics system that enables users to examine large quantities of 3D oceanographic data in order to identify trends and anomalies .The main goal is to provide researchers with powerful tools to analyze their information without having to be specialists on digital graphics or visualization techniques . We use state - of - the - art book graphics methods coupled with sophisticated interaction tools such as brushing , linking and querying .Our concept has been tested by using real - time oceanographic observations sets generated during many research cruises around Europe . This page presents our findings and explains future directions .Oceanographers are increasingly collecting huge amounts of statistics about the oceans physical properties ( e . g . , temperature , salinity ) and biological phenomena ( e . g . , plankton blooms ) . These data can be used to study phenomena like global cooling , contamination spreading , sea life growth etc . .However , reviewing and interpreting these enormous volumes of evidence remains challenging due to its high dimensionality and difficult connections between parameters . Interactive graphical analytics systems have shown great potential in assisting scientists find hidden patterns and anomalies within their information 1 .They allow scientists to interactively examine their information through different views while simultaneously performing queries and using filters 2 . In this project we present Diagnostic Tools for Unstructured Data ( DUT ) , a new graphical analytics method designed specifically for studying huge volumes of 3D oceanic data .DUT offers scientists with powerful tools to conduct exploratory analyses of their information without mandate them to be specialists on computer science or visualization techniques 3 , thus allowing them to focus more on the actual text of their information rather than how it should be displayed 4 .",
        "rewrite_text": "The growing volume and complexity of oceanographic data necessitate innovative strategies for analysis, visualization, and comprehension. This project introduces an interactive visual analytics system that allows users to explore vast amounts of 3D oceanographic data to uncover trends and anomalies. Our primary aim is to equip researchers with effective tools for data analysis without requiring them to be experts in digital graphics or visualization methods. We integrate advanced graphical techniques with sophisticated interaction features such as brushing, linking, and querying. Our approach has been validated using real-time oceanographic datasets collected from various research cruises throughout Europe. This article outlines our findings and discusses future directions. Oceanographers are increasingly gathering extensive statistics on oceanic physical attributes (such as temperature and salinity) and biological events (like plankton blooms). These datasets are valuable for investigating critical phenomena, including global cooling, contamination spread, and marine life growth. However, the high dimensionality and intricate relationships among parameters present significant challenges in reviewing and interpreting these vast amounts of data. Interactive graphical analytics systems have demonstrated considerable potential in helping scientists uncover hidden patterns and anomalies in their datasets. They enable scientists to interactively assess their information through various views while performing queries and applying filters simultaneously. In this project, we present Diagnostic Tools for Unstructured Data (DUT), an innovative graphical analytics method tailored for analyzing large volumes of 3D oceanic data. DUT provides scientists with robust tools for exploratory analysis without requiring them to be experts in computer science or visualization techniques, allowing them to concentrate on the actual content of their data rather than its presentation.",
        "ori-fast-z-score": -0.3223291856101521,
        "water-fast-z-score": 9.895506688976516,
        "rewrite-fast-z-score": -0.5222329678670935
    },
    {
        "original_text": "We present new observations in the mid-infrared (MIR) and near-infrared (NIR), as well as archival data, to study the physical conditions of the gas surrounding the young stellar object Re 4 IRS located at the center of the reflection nebula NGC 1333 IRAS 2. We find that the MIR spectrum is dominated by strong ro-vibrational lines of H$_2$O and CO, while the NIR spectrum shows prominent Brackett-γ line emission with an equivalent width of ~1 Å. The observed fluxes are consistent with those expected for a T Tauri star surrounded by a dense circumstellar disk heated by accretion shocks. In addition, we detect several forbidden transitions of  Fe II  ,  S III  ,  N II  , and  C II  . These results suggest that the central source has recently undergone a burst of enhanced mass loss activity which may be related to its recent transition into the main sequence phase.",
        "watermark_text": "We use new images in the mid - infrared ( MIR ) and near - infrared ( NIR ) , as well as archival data , to study the physical conditions of the gas surrounding the young stellar object Re 4 IRS located at the center of the reflection nebula NGC 1333 IRAS 2 . We see that the MIR spectrum is dominated by weak ro - vibrational lines of H $ _ 2 $ O and CO , while the NIR spectrum displays rare Brackett - γ line emission with an comparable width of ~ 1 Å .The observed fluxes are compatible with those expected for a T Tauri star supported by a dense circumstellar disk heated by accretion shocks . In addition , we find several forbidden transitions of Fe II , S III , N II , and C II .These data suggest that the main source has recently undergone a burst of enhanced mass loss activity which may be connected to its recent shift into the main series process .",
        "rewrite_text": "We analyze new mid-infrared (MIR) and near-infrared (NIR) images, along with archival data, to investigate the physical conditions of the gas surrounding the young stellar object Re 4 IRS, situated at the heart of the reflection nebula NGC 1333 IRAS 2. Our findings reveal that the MIR spectrum is primarily characterized by weak ro-vibrational lines of H₂O and CO, while the NIR spectrum features a notable Brackett-γ line emission with a width of approximately 1 Å. The observed fluxes are consistent with those expected from a T Tauri star, which is supported by a dense circumstellar disk that is heated by accretion shocks. Additionally, we identify several forbidden transitions of Fe II, S III, N II, and C II. This data indicates that the primary source has recently experienced a surge in mass loss activity, potentially linked to its recent transition into the main sequence phase.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 1.889822365046136
    },
    {
        "original_text": "We present an efficient numerical scheme to detect the existence of periodic orbits in chaotically behaving dynamical systems, such as chaotic maps or chaotic flows. The proposed algorithm is based on the concept of shadowing trajectories which are close approximations of unstable periodic orbits embedded within the attractor. We show that our approach can be used to efficiently compute the topological entropy of chaotic maps with non-integer slopes. Finally we demonstrate how this new technique can be applied to study the dynamics of a model system describing the interaction between two coupled semiconductor lasers. Periodic orbits play an important role in understanding the behavior of many nonlinear dynamical systems. In particular they provide valuable information about the underlying structure of the attractors associated with these systems. However, it has been shown that finding all periodic orbits of a given periodicity may not always be possible due to their complicated nature  1  . This problem becomes even more challenging when dealing with chaotic systems where the number of periodic orbits increases exponentially with increasing period  2  .\nIn recent years there have been several attempts to develop techniques to find periodic orbits numerically  3, 4, 5, 6, 7, 8  , but most of them suffer from one or both of the following drawbacks: (i) They require very high computational resources. (ii) They do not guarantee convergence towards the desired orbit. Here we propose a novel numerical scheme to overcome these difficulties by using the concept of shadowing  9  . Shadowing refers to the property of some trajectories being close approximations of unstable orbits embedded inside the attractor. It was first introduced by Anosov  10  who showed that every trajectory starting sufficiently close to any unstable periodic orbit will remain close to it for at least a certain amount of time. Since then various authors  11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44",
        "watermark_text": "We present an efficient numerical system to identify the existence of periodic orbits in chaotically behaving dynamical systems , such as chaotic maps or turbulent flows . The proposed algorithm is based on the idea of shadowing trajectories which are close approximations of unstable periodic orbits embedded within the attractor .We see that our approach can be used to easily compute the topological entropy of turbulent maps with non - integer peaks . Finally we prove how this new technique can be applied to study the dynamics of a prototype system describing the interaction between two coupled semiconductor lasers .Periodic orbits take an important role in understanding the dynamics of several nonlinear dynamical systems . In particular they give valuable info about the fundamental structure of the attractors associated with these systems .However , it has been shown that finding all periodic orbits of a given periodicity might not always be possible due to their complicated nature 1 . This problem arises even more challenging when dealing with turbulent systems where the number of periodic orbits changes exponentially with expanding period 2 .In past decades there have been numerous attempts to develop techniques to find periodic orbits numerically 3 , 4 , 5 , 6 , 7 , 8 , but most of them suffer from one or both of the following drawbacks : ( i ) They require very high computational resources . ( ii ) They do not guarantee convergence towards the desired orbit .Here we develop a new numerical system to overcome these problems by using the idea of shadowing 9 . Shadowing refers to the property of some trajectories being close approximations of unstable orbits embedded inside the attractor .It was first developed by Anosov 10 who demonstrated that every orbital beginning sufficiently next to any weak periodic orbit will remain close to it for at least a certain quantity of time . Since then various authors 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44",
        "rewrite_text": "We introduce an effective numerical method for detecting periodic orbits in chaotic dynamical systems, such as chaotic maps and turbulent flows. Our proposed algorithm leverages the concept of shadowing trajectories, which serve as close approximations to unstable periodic orbits situated within the attractor. Our findings indicate that this method can be easily utilized to calculate the topological entropy of turbulent maps that exhibit non-integer peaks. Additionally, we demonstrate how this novel technique can be applied to analyze the dynamics of a prototype system representing the interaction between two coupled semiconductor lasers. Periodic orbits play a crucial role in understanding the dynamics of various nonlinear dynamical systems, providing insightful information about the fundamental structure of the associated attractors. However, it has been acknowledged that detecting all periodic orbits of a specific periodicity can be challenging due to their intricate nature. This difficulty intensifies in turbulent systems, where the quantity of periodic orbits can increase exponentially with the period. Over the past few decades, several attempts have been made to devise numerical techniques for locating periodic orbits; however, most approaches face one or both of the following limitations: they either demand exceptionally high computational resources or do not guarantee convergence to the targeted orbit. In response, we develop a new numerical framework that addresses these issues through the concept of shadowing. Shadowing, originating from Anosov's work, refers to the phenomenon where certain trajectories closely approximate unstable orbits embedded within the attractor, ensuring that any trajectory starting sufficiently near a weak periodic orbit will remain close to it for a considerable duration. Since this foundational concept was introduced, numerous authors have contributed to its development and application.",
        "ori-fast-z-score": -0.43033148291193524,
        "water-fast-z-score": 6.688444820557844,
        "rewrite-fast-z-score": 0.5183210553488161
    },
    {
        "original_text": "We study the propagation of traveling waves (TWs) in excitable media with spatially distributed parameters, which are subject to both external forcing and internal fluctuations. We show that TWs can be generated spontaneously even if there is no deterministic source for them. The mechanism responsible for this phenomenon is related to the presence of an unstable stationary state between two stable ones. In particular, we demonstrate how spontaneous generation of TWs occurs due to stochastic resonance induced by additive white Gaussian noise. Finally, we present numerical results illustrating the effect of multiplicative colored noise on the dynamics of TWs. Propagation of traveling waves (TW) in excitable media has been studied extensively over recent years  1  . It was shown that TWs may appear as a result of various mechanisms such as: i) intrinsic instabilities  2  , ii) coupling-induced instabilities  3  or iii) forced oscillations  4  .\nIn many cases it is assumed that the medium under consideration is homogeneous so that all its properties do not depend explicitly on space coordinates. However, real physical systems usually have spatial variations of their characteristics  5  . For example, one-dimensional models describing cardiac tissue  6  include heterogeneity in the form of local changes in refractory periods  7, 8  . Another important factor influencing wave propagation is noise  9  . Noise plays different roles depending on whether it acts additively  10  or multiplicatively  11  . Moreover, noise may also affect the shape of the propagating front  12  .",
        "watermark_text": "We test the propagation of traveling signals ( TWs ) in excitable media with spatially scattered characteristics , which are subject to both external forcing and internal fluctuations . We see that TWs can be emitted spontaneously even if there is no deterministic source for them .The pathway responsible for this phenomenon is related to the presence of an weak stationary state between two stable ones . In particular , we prove how premature formation of TWs occurs due to stochastic resonance caused by additive white Gaussian interference .Finally , we present numerical findings illustrating the impact of multiplicative colored interference on the dynamics of TWs . Propagation of traveling signals ( TW ) in excitable media has been studied thoroughly over recent years 1 .It was shown that TWs might appear as a outcome of several mechanisms such as : i ) inherent instabilities 2 , ii ) coupling - caused instabilities 3 or iii ) forced oscillations 4 . In many situations it is expected that the medium under consideration is homogeneous so that all its properties do not depend explicitly on space coordinates .However , real biological models usually have spatial variations of their characteristics 5 . For instance , one - dimensional models explaining cardiac tissue 6 include heterogeneity in the form of local changes in refractory intervals 7 , 8 .Another important process influencing wave propagation is sound 9 . Noise takes varied roles depending on whether it functions additively 10 or multiplicatively 11 .Moreover , noise might additionally affect the form of the propagating front 12 .",
        "rewrite_text": "We investigate the propagation of traveling waves (TWs) in excitable media characterized by spatial heterogeneity, influenced by both external forces and internal fluctuations. Our findings indicate that TWs can emerge spontaneously, even in the absence of a deterministic source. This phenomenon is linked to the existence of a weak stationary state situated between two stable states. Specifically, we demonstrate that the premature generation of TWs is facilitated by stochastic resonance due to additive white Gaussian noise. Additionally, we present numerical results that highlight the effects of multiplicative colored noise on the dynamics of TWs. The propagation of traveling waves in excitable media has been extensively researched in recent years. Previous studies have shown that TWs can arise from several mechanisms, including i) inherent instabilities, ii) instabilities due to coupling, and iii) forced oscillations. Typically, it is assumed that the medium is homogeneous, meaning its properties do not vary with spatial coordinates. However, real biological models often exhibit spatial variations. For example, one-dimensional models of cardiac tissue incorporate heterogeneity through localized alterations in refractory periods. Another significant factor affecting wave propagation is noise, which can play different roles depending on whether it acts additively or multiplicatively. Furthermore, noise can also influence the shape of the propagating front.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.023912859079006,
        "rewrite-fast-z-score": 3.5645311547160277
    },
    {
        "original_text": "In this article, we present an approach for studying special Finsler manifolds by using global methods and techniques in Riemannian geometry. We introduce some new concepts such as geodesic convexity, strongly convexity, and strictly convexity on special Finsler manifolds. Then, we prove that every special Finsler manifold is locally geodesically convex at any point. Finally, we show that if a special Finsler manifold admits a strictly convex function then it has constant flag curvature. Keywords: Geodesic convexity, Finsler metric, Flag curvature. Mathematics Subject Classification (2010): 53C20, 53C25, 53A10. In this article, we study special Finsler manifolds with global methods and techniques in the theory of Riemannian geometry. First, we define geodesic convexity and strongly convexity on special Finslser manifolds. Then, under certain conditions, we prove that every Finsler manifold is locally  geodesically convex at each point. Next, we give necessary and sufficient conditions for a special Finsler manifold to have constant flag curvature.",
        "watermark_text": "In this article , we present an perspective for studying special Finsler manifolds by using global tools and techniques in Riemannian topology . We introduce some additional concepts such as geodesic convexity , strongly convexity , and strictly convexity on particular Finsler manifolds .Then , we prove that every special Finsler manifold is locally geodesically convex at any point . Finally , we prove that if a unique Finsler manifold admits a strictly convex map then it has constant flag curvature .Keywords : Geodesic convexity , Finsler tensor , Flag curvature . Mathematics Subject Classification ( 2010 ) : 53C20 , 53C25 , 53A10 .In this article , we study unique Finsler manifolds with global approaches and techniques in the theory of Riemannian topology . First , we define geodesic convexity and strongly convexity on particular Finslser manifolds .Then , under certain conditions , we prove that every Finsler manifold is locally geodesically convex at each point . Next , we give necessary and sufficient requirements for a unique Finsler manifold to have constant flag curvature .",
        "rewrite_text": "In this article, we explore a framework for analyzing special Finsler manifolds using global strategies and methodologies from Riemannian topology. We introduce several key concepts, including geodesic convexity, strong convexity, and strict convexity, specific to certain Finsler manifolds. We demonstrate that every special Finsler manifold exhibits local geodesic convexity at any point. Furthermore, we establish that if a singular Finsler manifold possesses a strictly convex map, then it must have constant flag curvature. Keywords include: Geodesic convexity, Finsler tensor, Flag curvature. The Mathematics Subject Classification (2010) is 53C20, 53C25, 53A10. In this study, we investigate unique Finsler manifolds through global approaches within Riemannian topology. Initially, we define geodesic convexity and strong convexity for specific Finsler manifolds. Subsequently, we show that, under certain conditions, every Finsler manifold is locally geodesically convex at each point. Finally, we provide the necessary and sufficient conditions for a unique Finsler manifold to possess constant flag curvature.",
        "ori-fast-z-score": 0.8962581595302719,
        "water-fast-z-score": 4.318004318006477,
        "rewrite-fast-z-score": 2.25
    },
    {
        "original_text": "We show that any physical realization of the quantum NOT gate must be accompanied by an energy cost, which is bounded below by a universal constant times the number of qubits in the system. This result follows directly from our proof of the existence of a lower bound on the ground-state energy density of certain spin systems with competing interactions and open boundary conditions. Our results are relevant to recent efforts aimed at realizing large-scale quantum computers using solid state devices such as semiconductor quantum dots or trapped ions. We also discuss possible extensions of this work to other types of quantum gates. The ability to perform arbitrary unitary transformations on a set of n qubits would constitute a quantum computer capable of solving problems exponentially faster than classical computers  1  . However, it has been shown  2  that no quantum algorithm can solve all computational problems more efficiently than its best known classical counterpart unless the polynomial hierarchy collapses. Thus, practical quantum computing requires efficient methods for implementing only those algorithms whose solutions cannot be found classically  3  .\nIn order to implement these algorithms, one needs to be able to perform basic operations such as single-qubit rotations  4  , two-qubit entangling gates  5  , and measurements  6  . In particular, the so-called CNOT (controlled-NOT) gate plays a central role  7, 8  since it allows one to construct many important quantum circuits  9  . Unfortunately, there exists no known method for constructing a general CNOT gate  10  ; however, several proposals have recently emerged  11  -  16  . These schemes typically involve coupling the spins of individual atoms via magnetic fields  17  and/or optical cavities  18  . While some experimental progress towards building small-scale quantum computers has already been made  19, 20  , scaling up these technologies remains extremely challenging  21  .",
        "watermark_text": "We see that any physical formulation of the quantum NOT gate must be accompanied by an energy cost , which is bounded below by a universal constant times the number of qubits in the system . This result follows directly from our proof of the existence of a lower bound on the ground - state energy density of certain spin systems with competing interactions and open boundary constraints .Our results are applicable to recent efforts aimed at developing huge - scale quantum computers utilizing steady state systems such as semiconductor quantum dots or trapped ions . We especially consider possible extend of this research to other types of quantum gates .The able to conduct arbitrary unitary transformations on a group of n qubits would create a quantum computer capable of solution problems exponentially better than classical processors 1 . However , it has been shown 2 that no quantum algorithm can answer all computational problems more efficiently than its best known classical counterpart unless the linear hierarchy collapses .Thus , practical quantum modeling needs efficient methods for applying only those algorithms whose solutions cannot be found classically 3 . In order to execute these algorithms , one needs to be able to conduct basic operations such as single - qubit rotations 4 , two - qubit entangling gates 5 , and measurements 6 .In particular , the so - called CNOT ( restricted - NOT ) loop plays a central role 7 , 8 since it allows one to build much crucial quantum networks 9 . Unfortunately , there exists no available method for constructing a general CNOT gate 10 ; however , various options have recently surfaced 11 - 16 .These methods typically involve bonding the spins of individual atoms via magnetic waves 17 and / or optical cavities 18 . While some experimental development towards creating low - scale quantum computers has already been achieved 19 , 20 , scaling up these concepts remains incredibly problematic 21 .",
        "rewrite_text": "It is evident that any physical implementation of the quantum NOT gate incurs an energy cost, which has a lower bound determined by a universal constant multiplied by the number of qubits in the system. This conclusion stems from our proof establishing a lower limit on the ground-state energy density of specific spin systems that feature competing interactions and open boundary conditions. Our findings are relevant to current initiatives focused on building large-scale quantum computers that leverage steady-state systems like semiconductor quantum dots or trapped ions. We particularly explore the potential to extend this research to other forms of quantum gates. Achieving the capability to perform arbitrary unitary transformations on a set of n qubits would enable a quantum computer to solve problems exponentially faster than classical computers. However, it has been demonstrated that no quantum algorithm can outperform its most efficient classical counterpart for all computational tasks unless the linear hierarchy collapses. Therefore, effective quantum modeling necessitates reliable methods to apply only those algorithms whose solutions are not feasible with classical approaches. To implement these algorithms, fundamental operations such as single-qubit rotations, two-qubit entangling gates, and measurements must be conducted. The CNOT (controlled-NOT) gate is particularly significant as it facilitates the construction of essential quantum networks. Unfortunately, a general method for constructing a CNOT gate is not currently available; however, several recent approaches have emerged. These strategies generally involve coupling the spins of individual atoms using magnetic waves and/or optical cavities. While some progress has been made toward developing small-scale quantum computers, scaling up these concepts presents considerable challenges.",
        "ori-fast-z-score": -0.39405520311955033,
        "water-fast-z-score": 6.442528450810767,
        "rewrite-fast-z-score": -1.4795908857482156
    },
    {
        "original_text": "We report on the observation of electron doping in cuprate superconductors by interfacing them to manganite insulators through epitaxial growth and chemical bonding at interfaces.  The interface between La0.7Sr0.3MnO3 (LSMO) and YBa2Cu3O6+x (YBCO), which are both parent compounds for high temperature superconductivity, is found to be highly conducting despite the large lattice mismatch between LSMO and YBCO. This suggests that charge transfer across the interface occurs due to strong electronic hybridization rather than strain relaxation alone. We also find that the hole concentration in the YBCO layer can be controlled by varying the thickness of the LSMO layer grown on top of it. These results demonstrate an alternative approach towards engineering the carrier density in cuprate superconductors using oxide heterostructures. High-temperature superconductivity has been observed only in materials containing copper-oxygen planes known as CuO2 layers  1  . In these systems, holes doped into the CuO2 plane give rise to Cooper pairs leading to superfluidity  2  . However, the maximum critical temperature Tc = 92 K achieved so far in this class of materials is still well below the theoretical limit predicted by Bardeen-Cooper-Schrieffer theory  3  , raising questions about how to further enhance Tc  4  .\nIn recent years there have been significant efforts made to explore new routes toward enhancing Tc beyond its current record value  5  . One promising route involves introducing electrons into the CuO2 plane  6  . For example, replacing oxygen atoms in the CuO2 plane with fluorine leads to a reduction in the number of holes in the system  7, 8  . Alternatively, one may introduce electrons directly into the CuO2 plane by growing thin films of transition metal oxides such as SrTiO3  9  or LaAlO3  10  onto the surface of cuprate superconductors. While these approaches show promise, they require precise control over film composition and structure during deposition  11  . An alternative strategy would involve controlling the carrier density in cuprates without changing their crystal structures  12  .",
        "watermark_text": "We report on the observation of electron doping in cuprate superconductors by interfacing them to manganite insulators through epitaxial growth and chemical bonding at connections . The interface between La0 . 7Sr0 . 3MnO3 ( LSMO ) and YBa2Cu3O6 + x ( YBCO ) , which are both parent molecules for high heat superconductivity , is found to be highly conducting despite the huge lattice mismatch between LSMO and YBCO .This implies that charge transfer across the interface comes driven to strong electronic hybridization instead than strain relaxation alone . We additionally find that the gap concentration in the YBCO layer can be determined by varying the height of the LSMO layer grown on top of it .These data demonstrate an additional method towards engineering the carrier density in cuprate superconductors using oxide heterostructures . High - temperature superconductivity has been observed only in structures carrying copper - oxygen planes known as CuO2 layers 1 .In these systems , holes doped into the CuO2 plane give rise to Cooper pairs leading to superfluidity 2 . However , the maximum essential temperature Tc = 92 K attained so far in this class of substances is already much below the theoretical maximum expected by Bardeen - Cooper - Schrieffer model 3 , placing questions about how to further enhance Tc 4 .In recent seasons there have been significant efforts made to pursue new routes toward promoting Tc beyond its current record value 5 . One promising route includes introducing electrons into the CuO2 plane 6 .For instance , replacing oxygen atoms in the CuO2 plane with fluorine leads to a reduction in the number of holes in the system 7 , 8 . Alternatively , one may introduce electrons directly into the CuO2 plane by expanding narrow bands of transition iron oxides such as SrTiO3 9 or LaAlO3 10 onto the surface of cuprate superconductors .While these models show success , they demand exact power over movie structure and shape during deposition 11 . An alternative approach would include governing the carrier density in cuprates without altering their crystal structures 12 .",
        "rewrite_text": "We present our findings on electron doping in cuprate superconductors achieved by interfacing them with manganite insulators through epitaxial growth and chemical bonding at their junctions. The interface formed between La0.7Sr0.3MnO3 (LSMO) and YBa2Cu3O6+x (YBCO), both of which serve as fundamental constituents for high-temperature superconductivity, demonstrates remarkable conductivity despite the significant lattice mismatch between LSMO and YBCO. This observation suggests that the charge transfer at the interface is primarily driven by strong electronic hybridization rather than merely by strain relaxation. Additionally, we discover that the gap concentration within the YBCO layer can be controlled by adjusting the thickness of the LSMO layer deposited on top of it. Our results indicate a novel approach for engineering carrier density in cuprate superconductors through oxide heterostructures. High-temperature superconductivity has only been observed in materials containing copper-oxygen planes, specifically CuO2 layers. In these systems, holes introduced into the CuO2 plane facilitate Cooper pair formation, which results in superfluidity. However, the highest critical temperature (Tc) of 92 K achieved in this class of materials remains significantly below the theoretical limit predicted by the Bardeen-Cooper-Schrieffer model, raising questions about how to further enhance Tc. Recent efforts have focused on exploring new strategies to increase Tc beyond its current maximum. One promising approach involves the introduction of electrons into the CuO2 plane. For example, substituting oxygen with fluorine in the CuO2 lattice reduces the number of holes in the system. Alternatively, electrons can be directly introduced into the CuO2 plane by applying narrow bands of transition metal oxides such as SrTiO3 or LaAlO3 onto the surface of cuprate superconductors. While these methods have shown promise, they require precise control over the growth parameters and film morphology during deposition. A more efficient strategy would be to manipulate carrier density in cuprates without altering their crystal structures.",
        "ori-fast-z-score": 0.33567254331867563,
        "water-fast-z-score": 7.7754191435023525,
        "rewrite-fast-z-score": 1.1188618555710317
    },
    {
        "original_text": "We study number density correlation functions (NDCFs) in classical one-component plasma at high temperatures and find that they are not well defined due to infrared divergences, which is related with the fact that NDCF can be expressed as an integral over the whole space. We show how these difficulties can be overcome by introducing a new quantity called local number density correlation function (LNDCF). The LNDCF has no infrared divergence but it still contains information about correlations between particles on different length scales. In particular we calculate LNDCF for two limiting cases - when all particles have equal velocities or when their distribution is Maxwellian. Finally we discuss possible applications of our results. PACS numbers: 52.27.Lw, 52.35.Jm, 52.38.Bx \nI. INTRODUCTORY REMARK\nIn this work we consider classical one component plasma consisting of charged particles interacting via screened Coulomb potential  1  . This system is described by the following Hamiltonian:",
        "watermark_text": "We explore number density correlation functions ( NDCFs ) in classical one - component plasma at high altitudes and find that they are not well characterised due to infrared divergences , which is related with the fact that NDCF can be described as an integral over the whole space . We see how these problems can be overcome by using a new quantity called regional number density correlation function ( LNDCF ) .The LNDCF has no infrared divergence but it still contains information about correlations between particles on various length scales . In particular we estimate LNDCF for two limiting cases - when all particles have equal velocities or when their distribution is Maxwellian .Finally we explain possible applications of our findings . PACS codes : 52 . 27 . Lw , 52 . 35 . Jm , 52 . 38 . Bx I .INTRODUCTORY REMARK In this research we define classical one element plasma consisting of charged particles interacting via screened Coulomb potential 1 . This system is characterized by the following Hamiltonian :",
        "rewrite_text": "We investigate number density correlation functions (NDCFs) in classical one-component plasma at high altitudes and discover that they are poorly defined due to infrared divergences. This issue arises because the NDCF can be expressed as an integral over the entire space. We demonstrate that these challenges can be addressed by introducing a new metric called the regional number density correlation function (LNDCF). The LNDCF eliminates infrared divergences while still retaining information about particle correlations across various length scales. Specifically, we calculate the LNDCF for two extreme scenarios: when all particles have identical velocities and when their velocity distribution follows a Maxwellian distribution. Finally, we outline potential applications of our results. PACS codes: 52.27.Lw, 52.35.Jm, 52.38.Bx. \n\nI. INTRODUCTORY REMARK \nIn this study, we define a classical one-component plasma made up of charged particles interacting through a screened Coulomb potential. The system is characterized by the following Hamiltonian:",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We present new observational constraints on the cosmic ray (CR) energy density and its evolution with redshift, based on gamma-ray observations by Fermi/LAT in the range 0 < z < 1.5. We find that CRs contribute at most 10% to the total pressure budget of the universe at redshifts below 2. This upper limit is consistent with theoretical expectations for the contribution of CRs accelerated by supernovae. The results are also compatible with previous measurements using radio data. These limits can be used as priors when modeling the effects of CRs on cosmological observables such as galaxy clustering or weak lensing. Cosmic rays (CRs), charged particles which fill space uniformly over large volumes, have been observed throughout our Galaxy and beyond. They play an important role in many astrophysical phenomena including galactic winds, star formation, and possibly even the acceleration of ultra-high-energy cosmic rays  1  . However, their origin remains unknown  2  .\nIn this work we use gamma-ray observations made by the Large Area Telescope (LAT) aboard the Fermi satellite  3  , to place tight constraints on the amount of CRs contributing to the overall pressure budget of the Universe  4  . In particular, we consider two different models for the CR distribution function f(p,z). First, we assume that it follows a power law spectrum dN/dE ~ E^{-alpha} between energies Emin = 10 GeV and Emax = 100 TeV; secondly, we adopt a broken power-law model where the spectral index changes from alpha1 = -2.2 to alpha2 = -3 above some break energy Eb = 50 GeV. For both cases, we fix the normalization factor A by requiring that the integral of f(p,z) over all momenta equals unity. \nThe resulting CR distributions are shown in Figure 1 . \nTo calculate the effect of these CR populations on the expansion history of the universe, we solve numerically the coupled system of equations describing the time-evolution of the background...",
        "watermark_text": "We introduce novel observational restrictions on the cosmic ray ( CR ) energy density and its progression with redshift , using on gamma - ray observations by Fermi / LAT in the range 0 < z < 1 . 5 . We see that CRs contribute at most 10 % to the total pressure budget of the universe at redshifts below 2 .This upper maximum is compatible with theoretical expectations for the impact of CRs accelerated by supernovae . The results are also consistent with previous measurements involving radio data .These restrictions can be used as priors when modeling the effects of CRs on cosmological observables such as galaxy clustering or strong lensing . Cosmic rays ( CRs ) , charged particles which fill space uniformly over large quantities , have been observed throughout our Galaxy and beyond .They play an important role in different astrophysical processes including galactic winds , sun formation , and maybe even the acceleration of ultra - large - energy cosmic rays 1 . However , their source remains unidentified 2 .In this research we utilize gamma - ray observations made by the Large Area Telescope ( LAT ) aboard the Fermi satellite 3 , to place secure constraints on the proportion of CRs causing to the overall pressure budget of the Universe 4 . In particular , we consider two different models for the CR distribution relation h ( p , z ) .First , we suppose that it takes a power law spectrum dN / dE ~ E ^ { - alpha } between frequencies Emin = 10 GeV and Emax = 100 TeV ; secondly , we adopt a broken power - law description where the spectral index shifts from alpha1 = - 2 . 2 to alpha2 = - 3 above some broken power Eb = 50 GeV . For both cases , we solve the normalization factor A by requiring that the integral of f ( p , z ) over all momenta equals unity .The resulting CR variables are shown in Figure 1 . To estimate the impact of these CR populations on the expansion history of the universe , we solve numerically the coupled system of equations explaining the period - progression of the background . . .",
        "rewrite_text": "We present new observational constraints on the energy density of cosmic rays (CRs) and their evolution with redshift, utilizing gamma-ray data from the Fermi/LAT for the range 0 < z < 1.5. Our analysis indicates that CRs contribute at most 10% to the total pressure budget of the universe at redshifts below 2. This upper limit aligns with theoretical predictions regarding the influence of CRs accelerated by supernovae and is also consistent with prior measurements derived from radio data. These constraints can serve as priors when exploring the effects of CRs on cosmological phenomena such as galaxy clustering and strong lensing. Cosmic rays are charged particles that fill space uniformly across vast distances, having been observed throughout our Galaxy and beyond. They play a crucial role in various astrophysical processes, including galactic winds, star formation, and potentially the acceleration of ultra-high-energy cosmic rays. However, the origin of these particles remains elusive. In this study, we leverage gamma-ray observations from the Large Area Telescope (LAT) aboard the Fermi satellite to establish robust constraints on the fraction of CRs contributing to the overall pressure budget of the universe. We consider two distinct models for the CR distribution, represented by h(p, z). First, we assume a power-law spectrum, dN/dE ~ E^{-α}, valid between Emin = 10 GeV and Emax = 100 TeV. Secondly, we adopt a broken power-law model where the spectral index transitions from α1 = -2.2 to α2 = -3 beyond a break energy Eb = 50 GeV. In both scenarios, we determine the normalization factor A by ensuring that the integral of f(p, z) over all momenta equals one. The resulting CR parameters are illustrated in Figure 1. To assess the influence of these CR populations on the universe's expansion history, we numerically solve the coupled equations governing the evolution of the cosmic background.",
        "ori-fast-z-score": 1.0215078369104984,
        "water-fast-z-score": 8.342314001435737,
        "rewrite-fast-z-score": 3.2327050096458403
    },
    {
        "original_text": "We present new near-infrared (NIR) observations and modeling results for the young stellar object, Barnard 68 (B68). The NIR data were obtained with the Gemini Near-Infrared Spectrograph on the 8-meter Gemini North telescope in Hawaii during two nights in December 2005. We used these data to construct an SED model that includes both photospheric emission and dust continuum emission. Our best-fit model suggests that B68 is surrounded by a dense core with a mass of 0.1 M_solar_(M_solar = 1.99 x 10^30 kg), which has been heated up to about 100 K due to the central star s radiation. This temperature corresponds to a luminosity of 3 L_sun_(L_sun = 3.84 x 10^26 W). In addition, we found that there are at least three other sources within the field-of-view of our observation whose fluxes contribute significantly to the total observed flux density.",
        "watermark_text": "We report new near - infrared ( NIR ) observations and modeling results for the small stellar object , Barnard 68 ( B68 ) . The NIR data were obtained with the Gemini Near - Infrared Spectrograph on the 8 - meter Gemini North telescope in Hawaii during two evenings in December 2005 .We utilized these information to build an SED simulation that contains both photospheric emission and dust continuum emission . Our best - fitting model suggests that B68 is scattered by a dense core with a mass of 0 . 1 M _ solar _ ( M _ solar = 1 . 99 x 10 ^ 30 kg ) , which has been heated up to about 100 K due to the main star s radiation .This temperature corresponds to a luminosity of 3 L _ sun _ ( L _ sun = 3 . 84 x 10 ^ 26 W ) . In addition , we concluded that there are at least three other sources within the field - of - view of our observation whose fluxes contribute greatly to the total observed flux concentration .",
        "rewrite_text": "We present new near-infrared (NIR) observations and modeling results for the small stellar object Barnard 68 (B68). These NIR data were collected using the Gemini Near-Infrared Spectrograph on the 8-meter Gemini North telescope in Hawaii over two nights in December 2005. We used this data to create a spectral energy distribution (SED) simulation that incorporates both photospheric and dust continuum emissions. Our best-fitting model indicates that B68 is surrounded by a dense core with a mass of 0.1 solar masses (M_solar = 1.99 x 10^30 kg), which has been heated to approximately 100 K by the radiation from the central star. This temperature corresponds to a luminosity of 3 solar luminosities (L_sun = 3.84 x 10^26 W). Additionally, we found that there are at least three other sources within the field of view of our observations that significantly contribute to the overall flux concentration we detected.",
        "ori-fast-z-score": 1.0504514628777804,
        "water-fast-z-score": 4.464418717230567,
        "rewrite-fast-z-score": 2.6887744785908154
    },
    {
        "original_text": "The Double Chooz experiment is designed to measure the mixing angle θ13 by searching for the appearance of electron neutrinos in a muon neutrino beam produced at the CERN SPS accelerator complex and directed towards France. The near detector (ND) measures the flux, energy spectrum and composition of this beam with high precision. In addition it provides an accurate measurement of the backgrounds expected in the far detector (FD). This document describes how we exploit these measurements to improve our knowledge on the systematic uncertainties affecting the FD analysis. \nIntroduction\n\nDouble Chooz  1  aims at measuring the third mixing angle θ 13 . It uses a reactor-based neutrino source located at about 1 km distance from its near detector ND280  2  , which consists of several sub-detectors surrounding the target volume where neutrinos are created. The main goal of the experiment is to search for the appearance of electron-neutrinos in a muon-neutrino beam produced at CERN s Super Proton Synchrotron (SPS), as illustrated in Figure 1 .\nIn order to achieve the required statistical accuracy within reasonable running time, the experiment will run in two phases. Phase I started in 2011 and ran until 2014; during that phase only one out of four possible detectors was operational. Phase II has just begun and runs until 2019 or 2020 when all detectors should be fully operational. During both phases data taking takes place simultaneously with the far detector (FD) situated 12 m underground at a distance of 1 km from the ND280 target  3  . \nNeutrino Flux Prediction\nThe prediction of the neutrino flux Φ(Eν ) reaching the ND280 detector depends on many parameters such as: the number N p of protons hitting the production target per second, their kinetic energy T p , the fraction f π 0 of neutral pions decaying into photons, the pion momentum distribution dN/dpπ etc.. These quantities can be measured directly using dedicated calibration experiments  4  . For example, the proton current Ip = Np /T p is determined by counting the number of protons hitting the target over a given period of time",
        "watermark_text": "The Double Chooz project is designed to measure the mix circle θ13 by searching for the appearance of electron neutrinos in a muon neutrino laser produced at the CERN SPS accelerator complex and directed towards France . The near detector ( ND ) measures the flux , energy spectrum and formation of this beam with high precision .In addition it gives an accurate measurement of the backgrounds predicted in the far detector ( FD ) . This report explains how we utilize these measurements to improve our information on the systematic uncertainties affecting the FD analysis .Introduction Double Chooz 1 aims at calculating the third mixing angle θ 13 . It utilizes a reactor - based neutrino source located at about 1 mi distance from its near sensor ND280 2 , which consists of several sub - detectors surrounding the target volume where neutrinos are created .The main goal of the program is to search for the appearance of electron - neutrinos in a muon - neutrino laser produced at CERN s Super Proton Synchrotron ( SPS ) , as shown in Figure 1 . In order to achieve the necessary mathematical accuracy within reasonable running time , the program will go in two phases .Phase I begun in 2011 and ran until 2014 ; during that phase only one out of four possible detectors was operational . Phase II has just started and ran until 2019 or 2020 when all detectors should be fully deployed .During both phases information taking takes place concurrently with the far detector ( FD ) situated 12 m underground at a distance of 1 km from the ND280 target 3 . Neutrino Flux Prediction The calculation of the neutrino flux Φ ( Eν ) reaching the ND280 detector depends on numerous variables such as : the number N p of protons striking the production target per second , their kinetic power T p , the fraction f π 0 of neutral pions decaying into photons , the pion momentum function dN / dpπ etc . .These quantities can be determined directly using dedicated calibration experiments 4 . For instance , the proton current Ip = Np / T p is calculated by counting the proportion of protons striking the target over a given time of time",
        "rewrite_text": "The Double Chooz project aims to measure the mixing angle θ13 by investigating the appearance of electron neutrinos within a beam of muon neutrinos generated by a laser at the CERN SPS accelerator complex, which is directed towards France. The near detector (ND) accurately measures the flux, energy spectrum, and formation of this neutrino beam, while also providing precise evaluations of the background predictions for the far detector (FD). This report details how we leverage these measurements to enhance our understanding of the systematic uncertainties impacting the FD analysis. \n\nThe first phase of Double Chooz focuses on determining the third mixing angle θ13 by utilizing a reactor-based neutrino source located approximately 1 mile away from the near detector, ND280, which is equipped with several sub-detectors surrounding the target volume where neutrinos are emitted. The primary objective of the project is to detect electron neutrinos appearing from the muon neutrino laser produced at CERN's Super Proton Synchrotron (SPS), as illustrated in Figure 1. To achieve the required level of mathematical accuracy within a reasonable timeframe, the project is divided into two phases. Phase I commenced in 2011 and concluded in 2014, during which only one of the four planned detectors was operational. Phase II has recently begun and will continue until 2019 or 2020, at which point all detectors are expected to be fully operational.\n\nThroughout both phases, data collection occurs simultaneously with the far detector (FD), which is located 12 meters underground, 1 kilometer from the ND280 target. The prediction of neutrino flux (Φ(Eν)) reaching the ND280 detector relies on multiple variables, including the number of protons (Np) striking the production target per second, their kinetic energy (Tp), the fraction of neutral pions (fπ0) decaying into photons, and the pion momentum distribution (dN/dpπ), among others. These factors can be directly measured through specialized calibration experiments. For example, the proton current (Ip = Np / Tp) is determined by counting the number of protons impacting the target over a specified duration of time.",
        "ori-fast-z-score": -0.08362420100070908,
        "water-fast-z-score": 6.713450866373513,
        "rewrite-fast-z-score": 0.7689218919450849
    },
    {
        "original_text": "We present new results on the evolution of galaxy clustering in the range 0 < z < 5, based on an analysis of data obtained with the VIMOS spectrograph at the Very Large Telescope (VLT). The sample consists of about 2000 galaxies selected by their Lyman-alpha emission line fluxes and covers a wide range of redshifts between 2<z<5. We measure the two-point correlation function for this sample using both direct counts-in-cells methods as well as Fourier space techniques. Our main result is that we find no evidence for any significant change in the amplitude or slope of the correlation function over this large redshift interval. This suggests that there has been little evolution in the typical mass scale of dark matter halos hosting these galaxies since z=5. In addition to measuring the overall shape of the correlation function, we also examine how it depends upon various physical properties such as luminosity, color, and spectral type.",
        "watermark_text": "We current new data on the evolution of galaxy clustering in the range 0 < z < 5 , using on an assessment of evidence derived with the VIMOS spectrograph at the Very Large Telescope ( VLT ) . The sample consists of about 2000 galaxies determined by their Lyman - alpha emission line fluxes and covers a broad variety of redshifts between 2 < z < 5 .We estimate the two - point coupling function for this specimen using both direct counts - in - cells methods as well as Fourier space methods . Our main consequence is that we find no evidence for any large change in the frequency or slope of the correlation function over this big redshift interval .This implies that there has been nothing evolution in the typical mass scale of dark matter halos hosting these objects since z = 5 . In addition to assessing the overall shape of the interaction function , we also investigate how it depends upon several physical properties such as luminosity , color , and spectral type .",
        "rewrite_text": "We present new data on the evolution of galaxy clustering for redshifts ranging from 0 to 5, based on evidence gathered using the VIMOS spectrograph at the Very Large Telescope (VLT). The sample comprises approximately 2,000 galaxies identified by their Lyman-alpha emission line fluxes and spans a wide range of redshifts between 2 and 5. We estimate the two-point correlation function for this sample using both direct counts-in-cells methods and Fourier space techniques. Our primary finding is that we observe no significant changes in the frequency or slope of the correlation function across this extensive redshift range. This suggests that there has been no evolution in the typical mass scale of the dark matter halos hosting these galaxies since z = 5. Additionally, we analyze how the shape of the correlation function varies with several physical properties such as luminosity, color, and spectral type.",
        "ori-fast-z-score": -1.5428161556520092,
        "water-fast-z-score": 4.628448466956028,
        "rewrite-fast-z-score": -1.2309149097933272
    },
    {
        "original_text": "We study the cosmological evolution of modified-gravity theories with an action that contains higher-order curvature terms (f(R)) by using a combination of analytical techniques and numerical simulations. We show how to obtain exact solutions for the background expansion history of these models at early times when the universe is dominated by radiation or matter; we also derive approximate analytic expressions valid on all scales during the late-time accelerated phase driven by dark energy. In particular, we find that there are two classes of viable f(R) models which can reproduce the observed cosmic acceleration without introducing any new degrees of freedom beyond those present in general relativity. The first class includes models where the effective gravitational constant decreases as time goes on; this leads to a phantom-like behavior characterized by w < −1. The second class consists of models where the effective gravitational coupling increases with time; here one finds quintessence-like behaviors with w > −1. Finally, we discuss some observational tests that could be used to distinguish between different types of f(R) models.",
        "watermark_text": "We research the cosmological evolution of modified - gravity theories with an action that contains upper - order curvature terms ( f ( R ) ) by using a combination of analytical techniques and mathematical simulations . We see how to obtain exact solutions for the background expansion history of these models at early periods when the universe is dominated by radiation or material ; we also generate approximate analytic expressions applicable on all scales during the early - time accelerated phase fueled by black radiation .In particular , we find that there are two groups of feasible f ( R ) scenarios which can mimic the known cosmic acceleration without using any new degrees of liberty beyond those present in general relativity . The first class includes theories where the effective gravitational constant reduces as time go on ; this results to a phantom - like behavior defined by w < −1 .The second class consists of models where the effective gravitational coupling increases with time ; here one sees quintessence - like interactions with w > −1 . Finally , we explain some observational tests that might be used to distinguish between various types of f ( R ) models .",
        "rewrite_text": "We investigate the cosmological evolution of modified gravity theories characterized by an action incorporating higher-order curvature terms (f(R)) through a combination of analytical methods and mathematical simulations. Our work focuses on deriving exact solutions for the background expansion history of these models during the early universe, particularly when it is dominated by radiation or matter. Additionally, we create approximate analytical expressions that are applicable across all scales during the early accelerated phase driven by black radiation. Notably, we identify two distinct groups of viable f(R) scenarios that can replicate the observed cosmic acceleration without introducing new degrees of freedom beyond those established in general relativity. The first group includes theories where the effective gravitational constant diminishes over time, leading to phantom-like behavior characterized by w < -1. The second group features models where the effective gravitational coupling increases with time, resulting in quintessence-like interactions with w > -1. Lastly, we discuss potential observational tests that could help differentiate between the various types of f(R) models.",
        "ori-fast-z-score": 0.6123724356957946,
        "water-fast-z-score": 6.531972647421809,
        "rewrite-fast-z-score": 0.6255432421712244
    },
    {
        "original_text": "In this work, we consider the problem of multiuser detection (MUD) for code division multiple access systems with time-varying channels. We propose an algorithm that jointly performs user identification and data detection by using a maximum likelihood criterion. The proposed method is based on the expectation-maximization (EM) algorithm which iteratively estimates both the channel coefficients and transmitted symbols. In order to reduce computational complexity, we also develop a low-complexity suboptimal MUD scheme. Numerical results show that our proposed algorithms outperform existing schemes under various scenarios. Index Terms-Data detection, EM algorithm, Multiuser detection, Time varying channels. 1 Introduction Code-division-multiple-access (CDMA) has been widely used as one of the most promising technologies for next-generation wireless communications due to its high spectral efficiency  1  . However, CDMA suffers from severe interference between users caused by multipath propagation  2  , especially when the number of active users increases  3  .\nTo mitigate inter-user interference, multiuser detectors have been developed  4  -  6  . Among them, linear multiuser detectors are attractive because they can be implemented easily at low cost  7  . Unfortunately, these detectors suffer from performance loss compared to optimal multiuser detectors  8  . To improve their performance, nonlinear multiuser detectors such as successive interference cancellation  9  or parallel interference cancellation  10  were introduced. These detectors require accurate knowledge about the received signals  11  . Therefore, blind multiuser detectors  12  -  14  were proposed to estimate unknown parameters without any training sequence  15  . Although blind multiuser detectors do not need prior information about the received signal, they usually perform worse than conventional multiuser detectors  16  .\nRecently, there has been growing interest in developing multiuser detectors for time-varying channels  17  -  20  . Since the channel varies over time, it becomes more difficult to detect the transmitted symbol accurately  21  . Moreover, if the channel changes rapidly, then the detector may fail completely  22  . Thus, it is important to design robust multiuser detectors against rapid channel variations  23  .",
        "watermark_text": "In this research , we investigate the question of multiuser tracking ( MUD ) for code division multiple entry systems with time - differing channels . We suggest an algorithm that together manages user identification and information detection by using a maximum likelihood threshold .The proposed approach is based on the expectation - maximization ( EM ) algorithm which iteratively generates both the channel coefficients and transmitted symbols . In order to reduce numerical complexity , we also build a small - complexity suboptimal MUD scheme .Numerical results show that our proposed methods outperform established algorithms under various circumstances . Index Terms - Data tracking , EM algorithm , Multiuser tracking , Time varying channels .1 Introduction Code - division - multiple - access ( CDMA ) has been widely adopted as one of the most attractive devices for next - class wireless communications thanks to its high spectral power 1 . However , CDMA suffers from severe interference between clients caused by multipath propagation 2 , particularly when the proportion of active consumers increases 3 .To mitigate inter - customer interference , multiuser detectors have been created 4 - 6 . Among them , continuous multiuser detectors are interesting because they can be deployed easily at low cost 7 .Unfortunately , these detectors suffer from performance loss compared to optimal multiuser detectors 8 . To increase their performance , nonlinear multiuser detectors such as consecutive interference cancellation 9 or parallel interference cancellation 10 were introduced .These detectors need accurate knowledge about the received messages 11 . Therefore , blind multiuser detectors 12 - 14 were developed to estimate unknown parameters without any testing sequence 15 .Although blind multiuser detectors do not require prior information about the received signal , they generally work worse than conventional multiuser detectors 16 . Recently , there has been growing interest in building multiuser detectors for time - differing channels 17 - 20 .Since the channel varies over time , it becomes more impossible to locate the transmitted expression accurately 21 . Moreover , if the channel shifts rapidly , then the sensor may fail entirely 22 .Thus , it is important to build robust multiuser detectors against sudden channel variations 23 .",
        "rewrite_text": "In this study, we explore the issue of multiuser detection (MUD) in code division multiple access (CDMA) systems that operate over time-varying channels. We propose an algorithm that effectively combines user identification and information detection using a maximum likelihood threshold. Our method is grounded in the expectation-maximization (EM) algorithm, which iteratively estimates both the channel coefficients and the transmitted symbols. To lower numerical complexity, we also develop a suboptimal MUD scheme with reduced complexity. Numerical simulations demonstrate that our proposed techniques outperform existing algorithms across various scenarios. \n\n**Index Terms**: Data tracking, EM algorithm, multiuser tracking, time-varying channels.\n\n1. **Introduction**: CDMA has become increasingly popular as a key technology for next-generation wireless communications due to its high spectral efficiency. However, it is challenged by significant interference between users resulting from multipath propagation, particularly as the number of active users rises. To combat inter-user interference, various multiuser detectors have been developed. Continuous multiuser detectors, in particular, are appealing due to their easy and cost-effective deployment. Unfortunately, these detectors tend to underperform compared to optimal multiuser detection methods. To enhance their performance, nonlinear multiuser detectors, such as successive interference cancellation and parallel interference cancellation, have been proposed; however, they require accurate knowledge of the received signals. Consequently, blind multiuser detectors have been designed to estimate unknown parameters without the need for a training sequence. Although they do not depend on prior knowledge of the signal, blind multiuser detectors typically perform less effectively than conventional multiuser detectors. Recently, there has been a surge of interest in developing multiuser detectors tailored for time-varying channels. The dynamic nature of these channels complicates the precise identification of the transmitted signals, and rapid channel variations can lead to complete detection failures. Thus, it is crucial to create robust multiuser detectors that can handle sudden changes in channel conditions.",
        "ori-fast-z-score": -1.0309670614335873,
        "water-fast-z-score": 8.751130125933031,
        "rewrite-fast-z-score": -0.07881104062391006
    },
    {
        "original_text": "We present exact solutions to the field equations of the Einstein-Gauss-Bonet (EGB) gravity with negative cosmological constant in 5D space-time. We find that there are three classes of black hole solutions depending on whether the Gauss-Bonnet coupling constant is positive or negative. The first class contains two types of static spherically symmetric black holes which have no horizons but possess naked singularities at their centers. In addition we also obtain another type of solution describing an asymptotically anti-de Sitter wormhole whose throat connects two asymptotic regions. Finally, by using the method developed recently by one of us, we construct a new type of solution representing a time-dependent spacetime horn. This work was supported by NSFC under Grant No. 10875030. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq . \nI. INTRODUCTORY REMARK\nThe discovery of gravitational waves has opened up a new window into our understanding of gravitation  1  , especially when it comes to testing general relativity  2  . However, despite its successes, general relativity still fails to explain some phenomena such as dark energy  3  and quantum gravity  4  . Therefore, many alternative theories of gravity were proposed over the years  5  .\nOne of these alternatives is the so-called Einstein-Gauss-Bonnet (EGB) gravity  6  -  8  . It can be viewed as a natural generalization of general relativity since it includes higher-order curvature corrections  9  . Moreover, this theory admits various interesting solutions including black holes  10 -  12  , wormholes  13  -  15  and even time dependent spacetimes  16  -  18  . Recently, EGB gravity attracted much attention due to its possible role in explaining the accelerated expansion of the universe  19  -  21  .",
        "watermark_text": "We present precise solutions to the field equations of the Einstein - Gauss - Bonet ( EGB ) gravity with negative cosmological constant in 5D space - time . We see that there are three categories of black hole solutions depending on whether the Gauss - Bonnet coupling constant is positive or negative .The first class includes two forms of static spherically symmetric black holes which have no horizons but possess naked singularities at their centers . In addition we also obtain another type of solution describing an asymptotically anti - de Sitter wormhole whose throat connects two asymptotic areas .Finally , by using the method developed ago by one of us , we create a new kind of solution representing a time - dependent spacetime horn . This project was supported by NSFC under Grant No .10875030 . PACS dates : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq .I . INTRODUCTORY REMARK The observation of gravitational waves has opened up a new window into our knowing of gravitation 1 , particularly when it comes to proving general relativity 2 .However , despite its successes , universal relativity also fails to explain some phenomena such as dark energy 3 and quantum gravitational 4 . Therefore , various alternative theories of gravitational were offered over the years 5 .One of these solutions is the so - called Einstein - Gauss - Bonnet ( EGB ) gravity 6 - 8 . It can be viewed as a natural generalization of general relativity since it includes higher - order curvature corrections 9 .Moreover , this theory admits various exciting solutions namely black holes 10 - 12 , wormholes 13 - 15 and even period based spacetimes 16 - 18 . Recently , EGB gravitational attracted much attention due to its potential importance in understanding the advanced expansion of the universe 19 - 21 .",
        "rewrite_text": "We provide accurate solutions to the field equations of Einstein-Gauss-Bonnet (EGB) gravity with a negative cosmological constant in five-dimensional spacetime. Our analysis reveals three distinct categories of black hole solutions, which depend on whether the Gauss-Bonnet coupling constant is positive or negative. The first category consists of two types of static, spherically symmetric black holes that lack horizons and instead feature naked singularities at their centers. Additionally, we derive a different solution that characterizes an asymptotically anti-de Sitter wormhole, with a throat connecting two asymptotic regions. Finally, employing a method previously developed by one of us, we construct a novel solution representing a time-dependent spacetime horn. This project was supported by the NSFC under Grant No. 10875030. PACS numbers: 04.20.q, 11.10.z, 98.80.Cq. \n\nINTRODUCTORY REMARKS: The detection of gravitational waves has provided new insights into our understanding of gravitation, especially in validating general relativity. However, despite its successes, general relativity falls short of explaining certain phenomena such as dark energy and quantum gravity. This has led to the proposal of various alternative theories of gravity over the years. One such theory is Einstein-Gauss-Bonnet (EGB) gravity, which serves as a natural extension of general relativity by incorporating higher-order curvature corrections. Moreover, this theory supports a range of intriguing solutions, including black holes, wormholes, and even periodic spacetimes. Recently, EGB gravity has garnered significant attention due to its potential relevance in comprehending the accelerated expansion of the universe.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 7.061788191316445,
        "rewrite-fast-z-score": -1.1917080461366747
    },
    {
        "original_text": "We study the dynamic properties of molecular motors by using burnt-bridge models, which are simple kinetic Monte Carlo simulations for motor proteins such as kinesin and myosin V. We show that these models reproduce several experimental results on single-molecule experiments with high accuracy.  In particular, we find that the velocity distribution is well described by an exponential function at low load force but deviates from it when the load increases. The mean square displacement shows subdiffusive behavior under large loads. These behaviors can be explained by considering the effect of the elasticity of the cargoes carried by the motors. Our model also reproduces the dependence of stall forces on external viscous drag coefficients observed experimentally. Finally, our simulation results suggest that the number of steps taken per ATP hydrolysis cycle decreases exponentially with increasing load force. This result may explain why the step size fluctuation becomes larger than expected theoretically near stalling conditions. \nI. INTRODUCTIO N\nMolecular motors play important roles in many biological processes including muscle contraction  1  , vesicle transport  2  , chromosome segregation  3  , and cell division  4  . They convert chemical energy into mechanical work through repeated cycles of binding to cytoskeletal filaments (e.g., microtubules) and releasing them  5  .\nThe most extensively studied class of molecular motors is the kinesins  6  . Kinesins walk along microtubules toward their plus ends  7, 8  . Myosins move towards actin filaments  minus ends  9  . Both types of motors have been shown to take discrete steps  10 -12  . Recent studies have revealed that both kinesins  13  and myosins  14  exhibit stochastic stepping motions even without external loads  15 -19  . It has been suggested that this randomness arises mainly due to thermal fluctuations  20, 21  or internal noise  22  . However, there still remain open questions about how they respond to external loads  23  .\nIn order to understand the mechanism underlying the operation of molecular motors, various theoretical approaches have been developed so far  24  . Among those methods, kinetic Monte Carlo (KMC) simulations  25  provide useful information on",
        "watermark_text": "We research the dynamic characteristics of molecular motors by using burnt - bridge machines , which are simple kinetic Monte Carlo simulations for motor molecules such as kinesin and myosin V . We see that these models reproduce many experimental results on single - cell experiments with high sensitivity . In particular , we find that the velocity distribution is well described by an exponential vector at low load pressure but deviates from it when the load changes .The mean square displacement shows subdiffusive behavior under large loads . These habits can be described by using the impact of the elasticity of the cargoes carried by the motors .Our model also reproduces the relationship of stall forces on external viscous drag coefficients observed experimentally . Finally , our modeling results show that the number of steps took per ATP hydrolysis process reduces exponentially with expanding load pressure .This result may understand why the step height fluctuation gets wider than expected theoretically near stalling circumstances . I . INTRODUCTIO N Molecular motors play important roles in multiple physical processes including muscle contraction 1 , vesicle transport 2 , genome segregation 3 , and cellular division 4 .They convert chemical electricity into mechanical labor through continuous periods of binding to cytoskeletal filaments ( e . g . , microtubules ) and releasing them 5 . The most extensively studied class of molecular motors is the kinesins 6 .Kinesins walk along microtubules toward their plus ends 7 , 8 . Myosins walk towards actin filaments minus ends 9 .Both types of motors have been shown to take distinct phases 10 - 12 . Recent research have revealed that both kinesins 13 and myosins 14 demonstrate stochastic stepping motions even without external loads 15 - 19 .It has been proposed that this randomness comes mostly owing to heat fluctuations 20 , 21 or internal sound 22 . However , there still continue open questions about how they react to external loads 23 .In order to comprehend the process underlying the operation of molecular motors , various theoretical methods have been proposed so far 24 . Among those techniques , kinetic Monte Carlo ( KMC ) simulations 25 provide useful details on",
        "rewrite_text": "We investigate the dynamic properties of molecular motors through the use of burnt-bridge machines, which are straightforward kinetic Monte Carlo simulations for motor proteins like kinesin and myosin V. Our findings indicate that these models effectively replicate numerous experimental outcomes from single-cell studies with a high degree of sensitivity. Notably, we observe that the velocity distribution follows an exponential pattern at low load pressures, but diverges from this behavior as the load increases. Furthermore, the mean square displacement exhibits subdiffusive behavior under substantial loads, which we attribute to the elastic characteristics of the cargoes transported by the motors. Our model also aligns with the experimentally observed relationship between stall forces and external viscous drag coefficients. Importantly, our simulation results demonstrate that the number of steps taken per ATP hydrolysis decreases exponentially as the load pressure rises. This finding helps to elucidate why step height fluctuations tend to be more pronounced than theoretical predictions near stalling conditions. \n\nI. INTRODUCTION: Molecular motors are integral to various biological processes, including muscle contraction, vesicle transport, genome segregation, and cell division. They convert chemical energy into mechanical work through repetitive binding and releasing of cytoskeletal filaments, such as microtubules. Kinesins, a highly studied type of molecular motor, move along microtubules toward their plus ends, while myosins traverse toward the minus ends of actin filaments. Both motor types exhibit distinct movement phases. Recent studies have demonstrated that kinesins and myosins engage in stochastic stepping motions even in the absence of external loads. This randomness is thought to primarily arise from thermal fluctuations or internal vibrations. However, several questions remain regarding their responses to external loads. To better understand the mechanisms by which molecular motors operate, various theoretical approaches have been proposed, with kinetic Monte Carlo (KMC) simulations offering valuable insights.",
        "ori-fast-z-score": -0.9428090415820634,
        "water-fast-z-score": 7.747580267412529,
        "rewrite-fast-z-score": 0.727606875108999
    },
    {
        "original_text": "We study the ground state properties and excitations in the spin-1/2 Heisenberg antiferromagnet on the square lattice with nearest-neighbor interactions only, using exact diagonalization (ED) for small clusters up to 12 sites as well as density-matrix renormalization group (DMRG) calculations for larger systems. We find that the ground-state energy per site is lower than the classical value by about 0.25J, where J denotes the exchange coupling constant between neighboring spins. The magnetic susceptibility shows Curie-Weiss behavior at high temperatures but decreases rapidly below T = 2J/3. This indicates strong quantum fluctuations which are also reflected in the low-temperature dependence of the specific heat. In addition we observe an unusual peak structure in the spin-spin correlation function S(q). For q along the principal axes of the Brillouin zone this peak has its maximum at q = π while it shifts towards smaller values when approaching the diagonals.",
        "watermark_text": "We research the ground state properties and excitations in the spin - 1 / 2 Heisenberg antiferromagnet on the square lattice with nearest - neighbor interactions only , using accurate diagonalization ( ED ) for large clusters up to 12 locations as well as density - vector renormalization group ( DMRG ) estimates for larger systems . We see that the ground - state energy per site is lower than the classical value by about 0 . 25J , where J refers the transfer coupling constant between neighboring spins .The magnetic susceptibility displays Curie - Weiss behavior at high temperatures but decreases quickly below T = 2J / 3 . This implies deep quantum fluctuations which are also reflected in the small - temperature dependence of the specific heat .In addition we encounter an peculiar peak structure in the spin - spinning correlation function S ( q ) . For q along the primary axes of the Brillouin zone this peak has its highest at q = π while it shifts towards lesser values when approaching the diagonals .",
        "rewrite_text": "We investigate the ground state properties and excitations of the spin-1/2 Heisenberg antiferromagnet on a square lattice with nearest-neighbor interactions. This is achieved through precise diagonalization (ED) for large clusters of up to 12 sites, alongside density-matrix renormalization group (DMRG) calculations for larger systems. Our findings indicate that the ground-state energy per site is approximately 0.25J lower than the classical value, where J represents the coupling constant between adjacent spins. The magnetic susceptibility exhibits Curie-Weiss behavior at elevated temperatures but rapidly decreases below T = 2J/3, suggesting significant quantum fluctuations. These fluctuations are further illustrated by the weak temperature dependence of the specific heat. Furthermore, we observe an unusual peak structure in the spin-spin correlation function S(q). For values of q along the primary axes of the Brillouin zone, this peak reaches its maximum at q = π but shifts to lower values as it approaches the diagonal directions.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.8944271909999159
    },
    {
        "original_text": "The study of critical phenomena is one of the most important topics in statistical physics, and has been applied to many fields such as biology, sociology, computer science, etc.. In this talk I will introduce some recent results on critical phenomena in complex networks. The main idea behind these works is that we can use the concept of  network resilience  to characterize different phases of a system with respect to its robustness against external perturbations or internal failures. We show how to detect phase transitions by measuring the size fluctuations of the largest connected component (LCC) of an evolving network under random attacks. Furthermore, we propose a new method for detecting phase transitions based on the evolution of the LCC s diameter. Finally, we discuss possible extensions of our work to other types of networks beyond scale-free ones. This research was supported by NSFC grant No.11271063. Keywords: Critical phenomena; Complex networks; Phase transition",
        "watermark_text": "The investigation of critical processes is one of the most important topics in statistical mechanics , and has been used to many fields such as biology , sociology , computer science , etc . . In this talk I will explain some latest findings on critical processes in complex networks .The main idea behind these works is that we can using the idea of system resilience to characterize different stages of a system with regard to its robustness against external perturbations or internal failures . We see how to identify phase transitions by assessing the size fluctuations of the greatest connected component ( LCC ) of an evolving network under random attacks .Furthermore , we investigate a new method for detecting phase transitions based on the evolution of the LCC s diameter . Finally , we investigate possible extend of our work to other types of networks beyond size - safe ones .This research was supported by NSFC award No . 11271063 . Keywords : Critical phenomena ; Complex networks ; Phase transition",
        "rewrite_text": "The study of critical processes is a key focus in statistical mechanics and has applications across various fields, including biology, sociology, and computer science. In this presentation, I will discuss recent discoveries related to critical processes within complex networks. The core concept of these studies is utilizing system resilience to characterize the different stages of a system in relation to its robustness against external disturbances or internal failures. We demonstrate how to identify phase transitions by analyzing the size fluctuations of the largest connected component (LCC) in an evolving network subjected to random attacks. Additionally, we introduce a novel approach for detecting phase transitions based on the changes in the LCC's diameter. Finally, we explore potential extensions of our research to other types of networks beyond size-safe ones. This research was funded by NSFC Award No. 11271063. Keywords: Critical phenomena; Complex networks; Phase transition.",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 5.815230125149881,
        "rewrite-fast-z-score": 1.162476387438193
    },
    {
        "original_text": "We study the dynamics of an interacting tachyonic field in a spatially flat Friedmann-Robertson-Walker universe with zero spatial curvature, using numerical methods to solve the equations of motion.  We find that there are two distinct phases during which the energy density evolves differently; one phase is dominated by the kinetic energy of the fields while the other is dominated by their potential energies.  The transition between these phases occurs when the Hubble parameter becomes comparable to the mass scale associated with the interaction term.  During this transition period we observe oscillatory behavior in both the Hubble parameter and the energy densities of each individual field.  In addition, we find that the total energy density decreases more slowly than it would if only one component were present (i.e., either a single tachyonic or a single scalar field).  Finally, we show how our results can be used to construct viable inflationary models. We consider a model consisting of a tachyonic field coupled to another scalar field through a quartic self-interaction term.  Using numerical techniques, we examine the time-evolution of various quantities such as the Hubble parameter, the energy densities of each field individually, and the total energy density.  Our analysis reveals several interesting features including the presence of a transition region where the Hubble parameter becomes comparable...",
        "watermark_text": "We research the dynamics of an interacting tachyonic field in a spatially straight Friedmann - Robertson - Walker universe with zero spatial curvature , using numerical methods to correct the coefficients of movement . We see that there are two separate phases during which the power concentration evolves differently ; one phase is dominated by the kinetic power of the fields while the other is dominated by their potential energies .The shift between these stages occurs when the Hubble parameter becomes comparable to the mass scale involved with the interaction term . During this shift period we study oscillatory behavior in both the Hubble parameter and the power densities of each individual field .In addition , we find that the total energy density decreases more slowly than it would if only one element were found ( i . e . , either a single tachyonic or a single scalar field ) . Finally , we show how our findings can be used to build feasible inflationary theories .We consider a theory consisting of a tachyonic field coupled to another scalar field through a quartic self - interaction term . Using numerical technology , we investigate the period - evolve of several quantities such as the Hubble parameter , the electricity densities of each field individually , and the total energy density .Our study reveals numerous interesting features including the presence of a transition region where the Hubble parameter becomes identical . . .",
        "rewrite_text": "We investigate the dynamics of an interacting tachyonic field within a spatially flat Friedmann-Robertson-Walker universe, employing numerical methods to refine the motion coefficients. Our analysis reveals two distinct phases in the evolution of power concentration: one phase is primarily influenced by the kinetic energy of the fields, while the other is dominated by their potential energies. The transition between these phases occurs when the Hubble parameter approaches the mass scale associated with the interaction term. During this transitional period, we observe oscillatory behavior in both the Hubble parameter and the power densities of each field. Additionally, we find that the total energy density decreases at a slower rate than it would if only a single component (either a solitary tachyonic or scalar field) were present. Furthermore, we demonstrate how our results can contribute to the development of viable inflationary theories. We examine a model consisting of a tachyonic field coupled to another scalar field via a quartic self-interaction term. Utilizing numerical techniques, we analyze the time evolution of various quantities, including the Hubble parameter, the energy densities of each field, and the overall energy density. Our research uncovers several intriguing features, including a transition region where the Hubble parameter converges.",
        "ori-fast-z-score": -0.5940885257860046,
        "water-fast-z-score": 5.671711984196938,
        "rewrite-fast-z-score": -0.618852747755276
    },
    {
        "original_text": "We report on neutron scattering experiments performed to study magnetic excitations in an underdoped cuprate superconductor, YBa2Cu3Ox (x = 6.35). We observe that the intensity and linewidth of the low-energy spin waves decrease with increasing temperature up to T* ~ 150 K, which is higher than Tc by about 50 K. The observed behavior can be explained within the framework of the spin-fermion model if one assumes that the spin-wave lifetime decreases rapidly at temperatures close to T* due to the decay into fermionic quasiparticles. This interpretation implies that the pseudogap opens already below T* as suggested previously. \n \n Introduction \n \n In recent years there has been considerable interest in studying the properties of high-temperature superconductors using neutron scattering techniques  1-5 . Neutron scattering allows us not only to investigate the static structure factor S(Q) but also dynamic correlations such as phonons or magnons  6 . It was found recently  7-9  that the low energy spin wave spectrum in optimally doped YBa2Cu3O3 displays unusual features compared to conventional metals. For example, it exhibits a strong dispersion anisotropy along different crystallographic directions  8  and shows significant deviations from the usual linear dependence between the inverse spin wave velocity and momentum  9 . These results have stimulated theoretical studies  10-12  aimed at understanding how these unconventional spin wave properties are related to the electronic structure of the CuO2 planes. However, little attention has so far been paid to the effect of doping on the spin wave dynamics. Here we present new experimental data obtained on an underdoped sample of YBa2Cu3OX (x= 6.35), where x denotes the oxygen content  13 . Our main goal is to explore whether the spin wave properties change significantly when going away from optimal doping towards lower values of x.",
        "watermark_text": "We report on particle scattering experiments conducted to study magnetic excitations in an underdoped cuprate superconductor , YBa2Cu3Ox ( x = 6 . 35 ) . We see that the strength and linewidth of the high - energy spinning waves reduce with advancing heat up to T * ~ 150 K , which is higher than Tc by about 50 K . The observed behavior can be described within the framework of the spin - fermion theory if one suppose that the spin - wave life falls swiftly at conditions close to T * due to the decay into fermionic quasiparticles .This interpretation means that the pseudogap opens already below T * as suggested previously . Introduction In recent years there has been substantial interest in investigating the properties of high - temperature superconductors using neutron scattering methods 1 - 5 .Neutron diffusion allows us not only to examine the static structure parameter S ( Q ) but also dynamic correlations such as phonons or magnons 6 . It was shown ago 7 - 9 that the reduced intensity spin wave spectrum in optimally doped YBa2Cu3O3 exhibits unusual characteristics compared to conventional metals .For instance , it displays a powerful dispersion anisotropy along various crystallographic directions 8 and shows significant deviations from the usual linear dependence between the inverse spinning wave velocity and momentum 9 . These data have stimulated theoretical experiments 10 - 12 aiming at studying how these unconventional momentum wave properties are related to the electronic properties of the CuO2 planes .However , little attention has so far been paid to the impact of doping on the spin wave behavior . Here we present new empirical data derived on an underdoped specimen of YBa2Cu3OX ( x = 6 . 35 ) , where x denotes the oxygen composition 13 .Our main goal is to examine whether the spin wave properties change considerably when going away from efficient doping towards lower values of x .",
        "rewrite_text": "We present findings from particle scattering experiments aimed at investigating magnetic excitations in the underdoped cuprate superconductor YBa2Cu3Ox (where x = 6.35). Our results indicate that both the intensity and linewidth of high-energy spin waves decrease as temperature rises, reaching T* ≈ 150 K, which is approximately 50 K above the critical temperature (Tc). This observed phenomenon can be explained through the spin-fermion theory, assuming that the lifespan of spin waves diminishes rapidly near T* due to their decay into fermionic quasiparticles. This interpretation suggests that the pseudogap begins to open at temperatures below T*, consistent with prior theories. \n\nIntroduction: Recent years have seen significant interest in exploring the properties of high-temperature superconductors via neutron scattering techniques. Neutron scattering not only allows for the examination of static structural parameters S(Q) but also dynamic correlations such as phonons and magnons. Previous studies have indicated that the reduced intensity of the spin wave spectrum in optimally doped YBa2Cu3O3 reveals peculiar characteristics that set it apart from conventional metals. For instance, it exhibits pronounced dispersion anisotropy along various crystallographic directions and shows notable deviations from the typical linear relationship between the inverse spin wave velocity and momentum. This data has prompted theoretical inquiries focused on understanding the relationship between these unconventional momentum wave properties and the electronic characteristics of the CuO2 planes. However, the impact of doping on spin wave behavior has received relatively little attention thus far. In this study, we provide new empirical data obtained from an underdoped sample of YBa2Cu3OX (x = 6.35), with the aim of assessing whether the spin wave properties experience significant changes when deviating from optimal doping towards lower values of x.",
        "ori-fast-z-score": -2.151657414559676,
        "water-fast-z-score": 6.627104836843802,
        "rewrite-fast-z-score": -0.08606629658238704
    },
    {
        "original_text": "We present the results of three-dimensional MHD simulations that show how magnetic fields can be amplified by shocks in molecular clouds, and lead to the formation of dense filaments with high mass-to-flux ratios. The initial conditions are based on observations of nearby star-forming regions. We find that shock compression leads to an increase in density and temperature at the post-shock region. This causes the gas pressure gradient across the shock front to decrease rapidly as time progresses. As a result, the field lines become more tangled due to turbulent motions induced by the shock wave. In addition, we observe that the magnetic energy is transferred into kinetic energy through Alfvén waves generated behind the shock fronts. Finally, we demonstrate that these processes cause the magnetic flux-to-mass ratio to increase significantly within the shocked region. \n \n Keywords: Magnetic fields, Shocks, Star formation, Turbulence \n \n 1. Introduction \n \n Molecular clouds play important roles in star formation (SF) because they provide the material for stars to form out of. However, it remains unclear what physical mechanisms drive SF inside molecular clouds. One possible mechanism involves supersonic turbulence driven by supernovae explosions or stellar winds (Mac Low & Klessen 2004). Another possibility is that large-scale gravitational collapse may trigger localised fragmentation leading to the formation of dense cores which then evolve into protostars (Larson 1978; Bonnell et al. 1997) . It has been suggested that both scenarios could operate simultaneously during different stages of evolution of molecular clouds (Krumholz 2014). \n \n Recent observational studies have shown that many young massive stars are associated with filamentary structures observed in infrared dust emission maps (André et al. 2010; Peretto et al. 2013 ). These filaments often appear to be aligned along magnetic field directions inferred from polarisation measurements (Chapman et al. 2011) , suggesting that magnetic fields might play an important role in regulating the dynamics of such systems. Indeed, theoretical models suggest that magnetic fields can affect the stability properties of self-gravitating clouds against global collapse (Mouschovias 1976; Tomis",
        "watermark_text": "We present the conclusion of three - dimensional MHD simulations that demonstrate how magnetic waves can be amplified by shocks in molecular clouds , and lead to the formation of dense filaments with high mass - to - flux proportions . The initial conditions are based on observations of nearby star - creating areas .We see that shock compression contributes to an increase in volume and heat at the post - shock zone . This leads the gas pressure slope across the shock front to reduce rapidly as time progresses .As a result , the field lines become more twisted due to chaotic motions created by the shock wave . In addition , we determine that the magnetic energy is transferred into kinetic power through Alfvén currents produced behind the shock fronts .Finally , we prove that these mechanisms create the magnetic flux - to - mass ratio to expand significantly within the shocked region . Keywords : Magnetic fields , Shocks , Star formation , Turbulence 1 .Introduction Molecular clouds play crucial roles in star formation ( SF ) because they provide the material for stars to form out of . However , it remains unsure what physical mechanisms drive SF inside molecular clouds .One potential mechanism involves supersonic turbulence driven by supernovae explosions or stellar winds ( Mac Low & Klessen 2004 ) . Another possibility is that high - scale gravitational failure may generate localised fragmentation leading to the formation of dense cores which then evolve into protostars ( Larson 1978 ; Bonnell et al .1997 ) . It has been proposed that both scenarios could operate simultaneously during various phases of evolved of molecular clouds ( Krumholz 2014 ) .Recent observational investigations have shown that several young massive stars are related with filamentary structures discovered in infrared dust emission images ( André et al . 2010 ; Peretto et al .2013 ) . These filaments often seem to be aligned along magnetic force directions inferred from polarisation observations ( Chapman et al .2011 ) , showing that magnetic waves might play an important role in controlling the dynamics of such systems . Indeed , theoretical theories indicate that magnetic waves can affect the stability properties of self - gravitating clouds against global failure ( Mouschovias 1976 ; Tomis",
        "rewrite_text": "We present the findings from three-dimensional MHD simulations that illustrate how magnetic waves can be enhanced by shocks in molecular clouds, leading to the development of dense filaments characterized by high mass-to-flux ratios. Our initial conditions are grounded in observations of nearby star-forming regions. Our results indicate that shock compression results in increased volume and temperature in the post-shock zone. Consequently, the gas pressure gradient across the shock front decreases rapidly over time. This dynamic results in the twisting of magnetic field lines due to the chaotic motions induced by the shock wave. Additionally, we find that magnetic energy is converted into kinetic energy via Alfvén currents generated behind the shock fronts. Ultimately, our findings demonstrate that these processes significantly enhance the magnetic flux-to-mass ratio in the shocked region. \n\nKeywords: Magnetic fields, Shocks, Star formation, Turbulence\n\n1. Introduction\nMolecular clouds are essential to star formation (SF) as they provide the raw materials from which stars emerge. However, the specific physical mechanisms that facilitate SF within these clouds remain unclear. One proposed mechanism is supersonic turbulence resulting from supernova explosions or stellar winds (Mac Low & Klessen 2004). Another possibility involves large-scale gravitational instability, which may cause localized fragmentation that results in the formation of dense cores that develop into protostars (Larson 1978; Bonnell et al. 1997). It has been postulated that both processes could operate concurrently during different stages of molecular cloud evolution (Krumholz 2014). Recent observational studies have indicated that numerous young massive stars are associated with filamentary structures identified in infrared dust emission images (André et al. 2010; Peretto et al. 2013). These filaments often appear to be aligned with the directions of magnetic forces inferred from polarization observations (Chapman et al. 2011), suggesting that magnetic waves may significantly influence the dynamics of these systems. In fact, theoretical models suggest that magnetic waves can alter the stability of self-gravitating clouds against global collapse (Mouschovias 1976; Tomisaka 2002).",
        "ori-fast-z-score": 0.07602859212697055,
        "water-fast-z-score": 7.732600044504815,
        "rewrite-fast-z-score": 0.914991421995628
    },
    {
        "original_text": "We present an alternative description of the electron in terms of its position and velocity, which is based on the idea that it moves along a helical trajectory around the nucleus. The new approach leads to a simple analytical expression for the energy levels of the helium atom as well as for the wave functions corresponding to these states. We show how this model can be used to explain some experimental results obtained by high-resolution spectroscopy experiments performed at Jefferson Lab. In addition we discuss possible extensions of our work towards other atomic systems such as muonic atoms or ions with one valence electron. Helium has been studied extensively over many decades both experimentally and theoretically. It was found that there are two stable isotopes (3He and 4He) and several excited states. These states have been investigated using various spectroscopic techniques including photo-absorption  1  , laser excitation  2  , and Compton scattering  3  . However, despite all efforts made so far, no satisfactory explanation exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound  4  .\nIn order to understand better the structure of helium, we propose here a new theoretical framework where the electron is described not only by its usual position but also by its velocity vector. This new approach allows us to obtain analytically the energy spectrum of helium as well as the associated wavefunctions. Our formalism is inspired by the so-called Bohmian mechanics  5  , which describes particles moving along trajectories instead of following classical equations of motions  6  .",
        "watermark_text": "We introduce an different characterization of the electron in terms of its position and speed , which is based on the idea that it travels along a helical trajectory around the nucleus . The modern perspective results to a simple analytical expression for the power concentrations of the helium atom as well as for the wave functions associated to these states .We see how this description can be used to explain some experimental results acquired by high - resolution spectroscopy investigations undertaken at Jefferson Lab . In addition we explain possible extend of our work towards other nuclear systems such as muonic atoms or ions with one valence electron .Helium has been studied frequently over numerous years both experimentally and theoretically . It was shown that there are two stable isotopes ( 3He and 4He ) and many excited states .These states have been investigated using numerous spectroscopic techniques including photo - absorption 1 , laser excitation 2 , and Compton absorption 3 . However , despite all efforts made so far , no satisfactory excuse exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound 4 .In order to explain better the composition of helium , we undertake here a new theoretical framework where the electron is characterized not only by its traditional position but also by its velocity tensor . This new approach allows us to obtain analytically the power spectrum of helium as well as the associated wavefunctions .Our formalism is influenced by the so - called Bohmian theory 5 , which explains particles moving along trajectories rather of following classical equations of motions 6 .",
        "rewrite_text": "We present a novel characterization of the electron in terms of its position and velocity, grounded in the concept of its helical motion around the nucleus. This modern viewpoint leads to a straightforward analytical expression for the power distribution of the helium atom, as well as for the wave functions related to these states. We demonstrate how this framework can elucidate experimental findings from high-resolution spectroscopy conducted at Jefferson Lab. Additionally, we propose potential extensions of our research to other nuclear systems, such as muonic atoms or ions with a single valence electron. Helium has long been a focal point of both experimental and theoretical inquiry, revealing two stable isotopes, \\( ^3\\text{He} \\) and \\( ^4\\text{He} \\), along with a variety of excited states. These states have been examined through various spectroscopic techniques, including photo-absorption, laser excitation, and Compton absorption. However, despite extensive research, a satisfactory explanation for why the ground state of \\( ^3\\text{He} \\) is unbound while that of \\( ^4\\text{He} \\) is bound remains elusive. To delve deeper into the nature of helium, we introduce a new theoretical framework that characterizes the electron not only by its traditional position but also by its velocity tensor. This innovative approach enables us to analytically derive the power spectrum of helium and the corresponding wavefunctions. Our formalism draws inspiration from Bohmian theory, which posits that particles move along trajectories rather than adhering strictly to classical equations of motion.",
        "ori-fast-z-score": -0.27975144247209416,
        "water-fast-z-score": 7.243550686553699,
        "rewrite-fast-z-score": 1.8542101386022132
    },
    {
        "original_text": "We present an algorithm for decomposing functions defined over quadratic surfaces in three dimensions, such as the surface of a sphere or ellipsoid, into multipole expansions. The method is based on representing the function using spherical harmonics and then expanding each term in this representation into a sum of products of Legendre polynomials with coefficients that are determined by solving a linear system of equations. We demonstrate our approach through several examples including computing the electrostatic potential due to point charges located at various positions around a dielectric sphere immersed in water. Our results show that we can accurately compute the electrostatic potential even when there are many sources distributed throughout space. This work was supported by NSF grant DMS-0852653 (CAREER). Spherical harmonic decomposition has been used extensively in computational physics applications ranging from quantum chemistry  1  , molecular dynamics  2  , and plasma simulations  3  . In these applications, one often needs to represent a given function f(r) defined over some domain Ω in terms of its expansion coefficients C lm :",
        "watermark_text": "We present an algorithm for decomposing functions defined over quadratic spheres in three dimensions , such as the surface of a sphere or ellipsoid , into multipole expansions . The method is based on representing the function using spherical harmonics and then increasing each term in this representation into a sum of products of Legendre polynomials with coefficients that are decided by solving a linear network of equations .We showed our approach through several examples namely calculation the electrostatic potential due to point charges situated at numerous positions around a dielectric sphere immersed in water . Our results show that we can accurately compute the electrostatic potential even when there are many sources distributed throughout space .This work was supported by NSF grant DMS - 0852653 ( CAREER ) . Spherical harmonic decomposition has been used extensively in computational physics applications ranging from quantum chemistry 1 , molecular dynamics 2 , and plasma simulations 3 .In these uses , one frequently needs to depict a given function f ( r ) established over some domain Ω in terms of its expansion function C lm :",
        "rewrite_text": "We introduce an algorithm designed to decompose functions defined on quadratic surfaces in three-dimensional space, such as the surfaces of spheres or ellipsoids, into multipole expansions. Our approach involves representing the function with spherical harmonics and then transforming each term in this representation into a summation of products of Legendre polynomials, with coefficients determined by solving a system of linear equations. We demonstrated our method through several examples, including the calculation of the electrostatic potential generated by point charges placed at various locations around a dielectric sphere immersed in water. Our findings indicate that we can accurately compute the electrostatic potential, even in scenarios with multiple distributed sources. This research was funded by NSF grant DMS-0852653 (CAREER). Spherical harmonic decomposition has found extensive application in computational physics, covering areas such as quantum chemistry, molecular dynamics, and plasma simulations. In these contexts, it is often necessary to express a given function f(r) defined over a domain Ω in terms of its expansion function C_lm.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 3.2627549126854696,
        "rewrite-fast-z-score": 0.6708203932499369
    },
    {
        "original_text": "The vapor pressure, solubility in water, and interfacial tension between oil and water are important parameters for understanding the behavior of crude oils during their production or transport through pipelines.  In this study we have investigated these properties using alkanol monolayers on an aqueous subphase as model systems to mimic the hydrocarbon chains present in crude oils. The results show that the vapor pressures of the alkanols increase with chain length up to C8 but decrease again above C10. This is explained by considering the competition between two opposing effects:  On one hand, increasing chain lengths lead to higher molecular volumes which favor evaporation. On the other hand, longer chains also result in stronger van der Waals interactions within the liquid phase leading to lower vapor pressures. We find that the solubilities of the alkanols follow similar trends as those observed for the vapor pressures. However, the differences in solubility among different chain lengths become smaller when compared to the corresponding differences in vapor pressure. Finally, our measurements reveal that the interfacial tensions between the alkanol layers and the underlying water decreases monotonically with chain length.",
        "watermark_text": "The vapor tension , solubility in water , and interfacial tension between petroleum and water are important characteristics for studying the activity of crude oils during their production or delivery through pipelines . In this study we have analyzed these characteristics utilizing alkanol monolayers on an aqueous subphase as simulation structures to mimic the hydrocarbon chains present in crude oils .The results show that the liquid pressures of the alkanols increase with chain length up to C8 but decrease again above C10 . This is explained by using the competition between two conflicting factors : On one hand , increasing chain lengths result to higher molecular volumes which favor evaporation .On the other hand , wider chains also lead in heavier van der Waals bonds within the liquid phase leading to smaller liquid pressures . We see that the solubilities of the alkanols follow similar trends as those observed for the liquid pressures .However , the differences in solubility among different chain lengths become smaller when compared to the associated changes in vapor tension . Finally , our measurements reveal that the interfacial pressures between the alkanol layers and the underlying water reduces monotonically with chain depth .",
        "rewrite_text": "The vapor tension, water solubility, and interfacial tension between petroleum and water are key factors in understanding the behavior of crude oils during extraction or transportation through pipelines. This study investigates these factors using alkanol monolayers on an aqueous subphase to simulate the hydrocarbon chains found in crude oils. Our findings indicate that the liquid pressures of the alkanols rise with increasing chain length up to C8, but then decline for chains longer than C10. This phenomenon can be attributed to the interplay of two opposing forces: longer chains increase molecular volume, which promotes evaporation, while they also create stronger van der Waals forces within the liquid phase, resulting in lower liquid pressures. Similar trends are observed in the solubility of the alkanols, although the variations in solubility across different chain lengths are less pronounced compared to the corresponding changes in vapor tension. Additionally, our measurements indicate that the interfacial pressures between the alkanol layers and the water decrease steadily with increased chain depth.",
        "ori-fast-z-score": 0.21081851067789195,
        "water-fast-z-score": 6.324555320336758,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We derive bounds on the absolute values of all entries in the up- and down-quark mass matrices, using only information about the CKM-matrix and current experimental data for the masses of quarks. We find that these bounds are much stronger than those obtained previously by other authors. \n \n The results presented here can be used as input parameters for future studies of CP violation within the Standard Model or its extensions. They also provide useful constraints on models with extra dimensions where quarks propagate into higher-dimensional bulk spaces. \nI. INTRODUCTORY REMARK\nThe Cabibbo-Kobayashi-Maskawa (CKM)  1  quark mixing matrix V is an unitary 3 x 3 complex matrix which describes how quarks mix among themselves after electroweak symmetry breaking. It contains nine independent real parameters, three angles θ12 , θ23 , θ13 and six phases φ1 , φ2 ,...",
        "watermark_text": "We derive bounds on the absolute values of all entries in the up - and down - quark mass matrices , using only data about the CKM - matrix and current experimental evidence for the masses of quarks . We see that these limits are greatly strengthened than those achieved previously by other researchers .The results presented here can be used as input parameters for future research of CP violation within the Standard Model or its extended . They especially offer useful limitations on models with extra dimensions where quarks propagate into larger - dimensional bulk spaces .I . INTRODUCTORY REMARK The Cabibbo - Kobayashi - Maskawa ( CKM ) 1 quark mixing function V is an unitary 3 x 3 complex graph which explains how quarks blend among themselves after electroweak symmetry breaking .It contains nine independent real functions , three curves θ12 , θ23 , θ13 and six phases φ1 , φ2 , . . .",
        "rewrite_text": "We establish bounds on the absolute values of all entries in the up and down quark mass matrices by utilizing only the CKM matrix data and the latest experimental evidence regarding quark masses. Our findings demonstrate significantly stronger limits compared to those obtained by previous researchers. These results can serve as input parameters for future studies on CP violation within both the Standard Model and its extensions. They provide particularly valuable constraints for models that incorporate extra dimensions, where quarks interact within higher-dimensional bulk spaces. \n\nI. INTRODUCTORY REMARKS\n\nThe Cabibbo-Kobayashi-Maskawa (CKM) matrix is a unitary \\(3 \\times 3\\) complex matrix that describes the mixing of quarks following electroweak symmetry breaking. It comprises nine independent real parameters: three mixing angles \\(\\theta_{12}\\), \\(\\theta_{23}\\), \\(\\theta_{13}\\), and six CP-violating phases \\(\\phi_1, \\phi_2, \\ldots\\).",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 4.635863249727653,
        "rewrite-fast-z-score": -0.254000254000381
    },
    {
        "original_text": "We study the geometry and topology of generalized Lagrangian submanifolds (GLSMs) in complex symplectic manifolds, focusing on GLSMs that are special Lagrangians with respect to some Kähler form. We show how these can be constructed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles. In particular we consider the case where the base is a partial flag variety. This leads us to define new families of Calabi-Yau varieties which have been studied by physicists recently. These varieties are obtained by taking products of Grassmannian manifolds or their quotients by finite groups. The main results of this thesis are:  1. A construction of GLSMs using parabolic Higgs bundles.  2. An explicit description of the cohomology ring of the total space of a vector bundle associated to a parabolic Higgs bundle.  3. A proof of mirror symmetry between two different types of GLSMs defined above when the base is a product of Grassmannians.",
        "watermark_text": "We research the topology and topology of generalized Lagrangian submanifolds ( GLSMs ) in complex symplectic manifolds , concentrating on GLSMs that are special Lagrangians with regard to some Kähler form . We see how these can be formed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles .In particular we define the case where the base is a partial flag variety . This leads us to define novel families of Calabi - Yau extensions which have been studied by physicists recently .These varieties are derived by take products of Grassmannian manifolds or their quotients by finite groups . The main results of this dissertation are : 1 .A design of GLSMs involving parabolic Higgs bundles . 2 .An exact description of the cohomology ring of the total space of a vector bundle related to a parabolic Higgs bundle . 3 .A proof of mirror symmetry between two different kinds of GLSMs provided above when the base is a product of Grassmannians .",
        "rewrite_text": "We investigate the topology and geometry of generalized Lagrangian submanifolds (GLSMs) within complex symplectic manifolds, focusing specifically on those GLSMs that are special Lagrangians with respect to a given Kähler form. Our analysis reveals how these submanifolds can be expressed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles. Notably, we examine the scenario where the base space is a partial flag variety, which enables us to introduce new families of Calabi-Yau extensions that have recently garnered interest from physicists. These varieties arise from products of Grassmannian manifolds or their quotients by finite groups. The primary contributions of this dissertation include: 1. A formulation of GLSMs involving parabolic Higgs bundles, 2. A precise description of the cohomology ring of the total space of a vector bundle associated with a parabolic Higgs bundle, and 3. A demonstration of mirror symmetry between two distinct types of GLSMs when the base is a product of Grassmannians.",
        "ori-fast-z-score": 0.39735970711951313,
        "water-fast-z-score": 4.900769721140662,
        "rewrite-fast-z-score": 1.0327955589886444
    },
    {
        "original_text": "We present the results of an optimization study for future galaxy surveys, aimed at constraining cosmological parameters in the context of general relativity (GR) and alternative theories of gravitation. We consider two different classes of experiments: weak lensing tomography with Euclid-like specifications, and 21cm intensity mapping observations with SKA1-LOW-like specifications. In both cases we assume that systematics can be controlled down to percent level accuracy. For each experiment we compute the Fisher matrix associated with the measurement of several relevant observables as functions of the underlying cosmology. The resulting covariance matrices are then used to perform parameter forecasts using Monte Carlo Markov Chains. Our main findings are:  - Weak lensing tomography is able to provide competitive constraints on w0 and wa when combined with Planck data.  - Intensity mapping experiments will not significantly improve our knowledge about the equation-of-state of dark energy beyond what has already been achieved by current CMB measurements.",
        "watermark_text": "We present the conclusion of an optimization investigation for future universe surveys , aiming at constraining cosmological values in the context of general relativity ( GR ) and new theories of gravitation . We consider two different categories of studies : soft lensing tomography with Euclid - like standards , and 21cm intensity projection measurements with SKA1 - LOW - like requirements .In both cases we suppose that systematics can be regulated down to percent level precision . For each experiment we compute the Fisher vector associated with the observation of several applicable observables as functions of the fundamental cosmology .The resulting covariance matrices are then utilized to conduct parameter forecasts using Monte Carlo Markov Chains . Our main results are : - Weak lensing tomography is ability to provide competitive limitations on w0 and wa when coupled with Planck information .- Intensity mapping observations will not considerably enhance our information about the equation - of - state of dark energy beyond what has already been achieved by current CMB observations .",
        "rewrite_text": "We present the findings of an optimization study aimed at future surveys of the universe, with the goal of constraining cosmological parameters in the framework of general relativity (GR) and emerging gravitational theories. Our investigation involves two primary types of studies: soft lensing tomography using Euclid-like criteria and 21cm intensity mapping measurements adhering to SKA1-LOW standards. In both scenarios, we assume that systematic uncertainties can be controlled to a precision of one percent. For each experiment, we calculate the Fisher vectors corresponding to the observation of several relevant observables as they relate to fundamental cosmological models. The resulting covariance matrices are then applied to perform parameter forecasts through Monte Carlo Markov Chains. Our key findings are as follows: weak lensing tomography can offer competitive constraints on the parameters w0 and wa when combined with data from Planck; however, intensity mapping observations are unlikely to significantly improve our understanding of the dark energy equation of state compared to what current CMB data already provide.",
        "ori-fast-z-score": -2.215646837627989,
        "water-fast-z-score": 5.908391567007971,
        "rewrite-fast-z-score": -0.5698028822981898
    },
    {
        "original_text": "We study the dynamics of an asymmetric exclusion process with two species on a ring, where particles can hop to their right or left neighboring site and are subject to hard-core repulsion. We show that for any initial condition there exists a unique stationary state which is characterized by a density profile depending only on the distance between sites. In particular we find that this profile decays exponentially fast as one moves away from the origin. This result implies that the system exhibits dynamic screening, i.e., correlations decay exponentially fast at large distances even though the underlying microscopic model does not have translational invariance. The proof relies on a combination of techniques from probability theory (in particular martingale methods) and functional analysis. Our results hold both for finite systems and infinite lattices. \nI. INTRODUCTORY REMARK\nIn recent years much attention has been devoted to studying nonequilibrium steady states of driven lattice gases  1  . These models describe interacting particle systems evolving according to stochastic rules such that detailed balance cannot be satisfied globally  2  , but nevertheless they exhibit interesting macroscopic behavior  3  .\nOne class of these models consists of so-called exclusion processes  4  describing particles moving along a regular lattice under mutual exclusion constraints  5  . For example, consider a chain of L sites labeled by integers 1, ..., L, each occupied by either zero or one particle. Particles may jump to the right or left neighboring site provided it is empty  6  . If all jumps occur independently then the resulting Markov process satisfies detailed balance with respect to some product measure  7, 8  . However if the rates depend on the number of particles occupying adjacent sites  9  then detailed balance breaks down  10  . Despite this lack of equilibrium properties many of these models still display non-trivial features reminiscent of those observed in thermal equilibrium  11  .",
        "watermark_text": "We work the dynamics of an asymmetric exclusion cycle with two organisms on a ring , where ions can jump to their right or left neighboring place and are subject to rough - core repulsion . We see that for any initial situation there exists a unique stationary state which is characterized by a density profile depending only on the distance between locations .In particular we find that this profile decays exponentially rapidly as one moves away from the origin . This result suggests that the process exhibits dynamic filtering , i . e . , correlations decay exponentially rapidly at large distances even though the underlying microscopic theory does not have translational invariance .The proof uses on a combination of techniques from likelihood analysis ( in instance martingale models ) and functional analysis . Our results hold both for finite systems and infinite lattices .I . INTRODUCTORY REMARK In recent years much attention has been focused to investigating nonequilibrium steady states of driven lattice gases 1 .These systems represent interacting particle structures operating due to stochastic laws such that detailed balance cannot be satisfied globally 2 , but still they show exciting macroscopic behavior 3 . One class of these models includes of so - called exclusion mechanisms 4 describing particles moving along a regular lattice under mutual exclusion constraints 5 .For instance , consider a network of L locations labeled by integers 1 , . . . , L , each inhabited by either zero or one particle . Particles must hop to the right or left neighboring area provided it is vacant 6 .If all jumps happen independently then the resulting Markov process satisfies detailed balance with regard to some product measure 7 , 8 . However if the rates differ on the quantity of particles occupying adjoining sites 9 then detailed balance breaks down 10 .Despite this lack of stability properties many of these models remain show non - simple details resembling of those observed in heat equilibrium 11 .",
        "rewrite_text": "We examine the dynamics of an asymmetric exclusion process involving two organisms on a ring, where ions can jump to adjacent positions on either the right or left and are influenced by hard-core repulsion. Our analysis reveals that for any initial configuration, there exists a unique stationary state characterized by a density profile that depends solely on the distance between locations. Notably, we observe that this profile decreases exponentially as one moves away from the origin. This finding implies that the process exhibits dynamic filtering, meaning that correlations diminish exponentially fast at large distances, even though the fundamental microscopic theory lacks translational invariance. The proof employs a combination of techniques from likelihood analysis, particularly martingale models, and functional analysis. Our findings apply to both finite systems and infinite lattices.\n\n**I. INTRODUCTORY REMARK**  \nIn recent years, there has been significant focus on investigating nonequilibrium steady states of driven lattice gases. These systems are comprised of interacting particle configurations governed by stochastic laws, leading to a situation where detailed balance cannot be maintained globally, yet they still exhibit intriguing macroscopic behaviors. One category of these models involves exclusion mechanisms, which describe the movement of particles along a regular lattice under mutual exclusion constraints. For example, consider a network of L sites, labeled by integers 1 to L, each occupied by either zero or one particle. Particles can hop to a neighboring slot on the right or left only if it is unoccupied. When all jumps are independent, the resulting Markov process adheres to detailed balance concerning a specific product measure. However, if the jump rates depend on the occupancy of adjacent sites, detailed balance breaks down. Despite this instability, many of these models continue to exhibit complex behaviors reminiscent of those seen in thermal equilibrium.",
        "ori-fast-z-score": 0.39904344223381105,
        "water-fast-z-score": 8.96717661308488,
        "rewrite-fast-z-score": 2.0344711469278987
    },
    {
        "original_text": "We consider the wiretap channel model in which an eavesdropper can obtain feedback about its observations and use it to improve future decoding attempts. We show that, for any positive integer , there exists a sequence of (2, ) codes such that each code has rate at least . This result is obtained by combining polar coding techniques with ideas from successive cancellation list decoding. The proof relies on a new converse bound for the secrecy capacity region of the wiretap channel with feedback. Finally, we present numerical results illustrating our achievability scheme. In this work, we study the wiretap channel model introduced by Wyner  1  . In particular, we focus on the case where the legitimate receiver obtains feedback about his past observations and uses it to improve his future decoding attempts. Our main contribution consists in showing that polar codes  2  combined with successive cancellation list decoding  3  achieve rates close to the secrecy capacity when used over channels with feedback. More specifically, we prove that if the number of messages grows exponentially fast as a function of the blocklength then every message can be transmitted reliably using a code whose length scales linearly with the number of messages .\nThe remainder of this paper is organized as follows. Section II introduces some notation and definitions. Section III presents our main theorem. Section IV provides a sketch of the proof of Theorem 3.1. Finally, Section V concludes the paper.",
        "watermark_text": "We consider the wiretap network theory in which an eavesdropper can obtain feedback about its measurements and use it to assist future decoding attempts . We see that , for any positive integer , there exists a sequence of ( 2 , ) codes such that each code has rate at least .This result is found by combining polar compression techniques with ideas from successive cancellation list decoding . The proof uses on a new converse bound for the secrecy ability area of the wiretap network with feedback .Finally , we present numerical findings illustrating our achievability scheme . In this research , we study the wiretap channel theory introduced by Wyner 1 .In particular , we focus on the case where the legitimate receiver obtains input about his past discoveries and using it to improve his future decoding attempts . Our main effort consists in discovering that polar codes 2 combined with successive cancellation list decoding 3 achieve speeds close to the silence ability when utilized over networks with feedback .More specifically , we prove that if the quantity of transmissions grows exponentially rapid as a function of the blocklength then every message can be delivered reliably using a code whose width scales linearly with the number of transmissions . The remainder of this paper is organized as follows .Section II offers some terminology and definitions . Section III presents our major theorem .Section IV offers a sketch of the proof of Theorem 3 . 1 . Finally , Section V concludes the paper .",
        "rewrite_text": "In this study, we explore the theory of wiretap networks, where an eavesdropper can receive feedback on their measurements to enhance future decoding efforts. We demonstrate that for any positive integer, there exists a sequence of (2, γ) codes, each having a rate of at least γ. This finding combines polar compression techniques with concepts from successive cancellation list decoding. The proof relies on a novel converse bound concerning the secrecy capacity region of wiretap networks with feedback. Additionally, we present numerical results that validate our proposed achievability scheme. Our focus aligns with the wiretap channel theory established by Wyner, particularly emphasizing scenarios where the legitimate receiver utilizes knowledge from previous observations to refine future decoding attempts. Our primary contribution is showing that polar codes, when paired with successive cancellation list decoding, can achieve rates close to the secrecy capacity in feedback-enabled networks. Specifically, we prove that if the number of transmissions increases exponentially with respect to the block length, then every message can be reliably transmitted using a code whose length scales linearly with the number of transmissions. The paper is structured as follows: Section II provides definitions and terminology, Section III details our main theorem, Section IV outlines a proof sketch for Theorem 3.1, and Section V concludes the discussion.",
        "ori-fast-z-score": 0.2873478855663454,
        "water-fast-z-score": 6.674238124719146,
        "rewrite-fast-z-score": -0.5827715174143585
    },
    {
        "original_text": "We present new observations of intervening metal systems at z ~ 1.5-2.0 towards the gamma-ray burst (GRB) 050525A, which is one of only two GRBs with spectroscopic redshifts known to date. We detect absorption lines due to Mg ii λλ2796, 2803, Feii λ2382, 2344+2600, Ciii λ977, Siiv λ1394, 1403, Ovi λ1032, 1038, Nv λ1239, 1243, Lyα, and Lyβ associated with an absorber at z = 2.01 ± 0.02. This system has log NHI/cm−2 = 19.6 +0.2 −0.1 , corresponding to a total hydrogen column density of 5 × 1020 cm−2 . It also shows strong low-ionization transitions such as Al iii λ1854, 1854 + 1862, and S iv λ1063, 1073 that are not seen in typical high-redshift absorbers. These features suggest that this absorber may be similar to those found along quasar sightlines.",
        "watermark_text": "We report new images of intervening metal systems at z ~ 1 . 5 - 2 . 0 towards the gamma - ray flare ( GRB ) 050525A , which is one of only two GRBs with spectroscopic redshifts discovered to date . We detect absorption tracks due to Mg ii λλ2796 , 2803 , Feii λ2382 , 2344 + 2600 , Ciii λ977 , Siiv λ1394 , 1403 , Ovi λ1032 , 1038 , Nv λ1239 , 1243 , Lyα , and Lyβ associated with an absorber at z = 2 . 01 ± 0 . 02 .This system has log NHI / cm−2 = 19 . 6 + 0 . 2 −0 . 1 , equivalent to a total hydrogen row density of 5 × 1020 cm−2 . It additionally shows bright low - ionization transitions such as Al iii λ1854 , 1854 + 1862 , and S iv λ1063 , 1073 that are not seen in standard high - redshift absorbers .These features indicate that this absorber could be similar to those observed along quasar sightlines .",
        "rewrite_text": "We present new observations of intervening metal systems at redshifts approximately 1.5 to 2.0 associated with the gamma-ray burst (GRB) 050525A, which is notable as one of only two GRBs with known spectroscopic redshifts thus far. Our analysis reveals absorption features from various transitions, including Mg ii λλ2796, 2803, Fe ii λ2382, 2344 + 2600, C iii λ977, Si iv λ1394, 1403, O vi λ1032, 1038, N v λ1239, 1243, as well as Lyman-alpha (Lyα) and Lyman-beta (Lyβ), linked to an absorber with a redshift of z = 2.01 ± 0.02. This system has a logarithmic hydrogen column density of log NHI / cm−2 = 19.6 +0.2 −0.1, which corresponds to a total hydrogen density of 5 × 10^20 cm−2. Additionally, it exhibits prominent low-ionization transitions such as Al iii λ1854, 1854 + 1862, and S iv λ1063, 1073, which are typically absent in conventional high-redshift absorbers. These characteristics suggest that this absorber may be analogous to those observed along quasar sightlines.",
        "ori-fast-z-score": -1.9466570535691505,
        "water-fast-z-score": 2.7221786146864813,
        "rewrite-fast-z-score": -2.4961508830135313
    },
    {
        "original_text": "Glutamatergic synapses are the most abundant excitatory connections in the brain, but how they form is not well understood. Here we show that glutamate receptors (GluRs) can be delivered directly to synaptic sites by lateral diffusion on the plasma membrane or indirectly via endocytosis followed by recycling endosomes. We used single-particle tracking with total internal reflection fluorescence microscopy to follow GluR2-containing AMPA receptors tagged with quantum dots at hippocampal neuron dendrites. The results reveal two distinct trafficking routes for receptor delivery to synapses. First, receptors diffuse laterally along the surface until encountering an existing synapse where they become trapped. Second, receptors enter endosomal vesicles after endocytosis and then recycle back to the cell surface through Rab11-positive recycling endosomes before being targeted to new synapses. These findings provide insights into mechanisms underlying formation of functional neuronal circuits during development as well as plasticity of mature neurons.",
        "watermark_text": "Glutamatergic synapses are the most widespread excitatory connections in the brain , but how they shape is not well understood . Here we show that glutamate receptors ( GluRs ) can be delivered directly to synaptic sites by vertical diffusion on the plasma membrane or indirectly via endocytosis followed by recycling endosomes .We utilized single - particle tracking with total internal reflection fluorescence microscopy to follow GluR2 - containing AMPA proteins tagged with quantum dots at hippocampal neuron dendrites . The results show two different transport ways for receptor delivery to synapses .First , receptors diffuse laterally along the surface until encountering an established synapse where they become trapped . Second , receptors enter endosomal vesicles after endocytosis and then recycle back to the cell surface through Rab11 - positive recycling endosomes before being directed to fresh synapses .These studies provide insights into mechanisms governing formation of functional neuronal systems during development as well as plasticity of mature neurons .",
        "rewrite_text": "Glutamatergic synapses represent the most prevalent excitatory connections in the brain, yet their formation is not fully understood. In this study, we demonstrate that glutamate receptors (GluRs) can reach synaptic locations through two distinct methods: direct lateral diffusion along the plasma membrane or indirect delivery via endocytosis followed by recycling through endosomes. We employed single-particle tracking with total internal reflection fluorescence microscopy to monitor GluR2-containing AMPA receptors, which were labeled with quantum dots, at dendrites of hippocampal neurons. Our findings reveal two pathways for receptor delivery to synapses. The first pathway involves lateral diffusion where receptors move along the surface until they reach an established synapse, where they become localized. The second pathway entails receptors being internalized into endosomal vesicles through endocytosis, followed by recycling back to the cell surface via Rab11-positive recycling endosomes, ultimately targeting new synapses. These insights elucidate the mechanisms that influence the development of functional neuronal networks and the plasticity of mature neurons.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 3.75,
        "rewrite-fast-z-score": 1.2701705922171767
    },
    {
        "original_text": "We present the results on the unique type Ib supernova (SN) 2005bf in NGC 6946, which is one of only two SNe with an unambiguous association to a gamma-ray burst (GRB). The light curve and spectral evolution are compared with those of other GRB-associated SNe as well as normal SNe Ia and IIb. We find that SN 2005bf shows several unusual features among these objects. In particular, it has a very slow decline rate during its nebular phase, indicating that the ejecta mass may be much larger than expected for ordinary core-collapse SNe. This suggests that the progenitor star was massive enough to form a black hole directly without going through a neutron-star stage. If this is true, we expect that the central engine of the associated GRB would have been powered by accretion onto such a black hole rather than by radioactive decay energy. Our analysis also indicates that the explosion site should be located close to the center of the host galaxy, suggesting that the progenitor system might have experienced significant natal kick before the explosion. \n \n Keywords: Gamma-ray bursts",
        "watermark_text": "We present the results on the unusual type Ib supernova ( SN ) 2005bf in NGC 6946 , which is one of only two SNe with an unambiguous association to a gamma - ray burst ( GRB ) . The light curve and spectral evolution are compared with those of other GRB - associated SNe as well as normal SNe Ia and IIb .We see that SN 2005bf demonstrates several unique features among these objects . In particular , it has a very slow trend rate during its nebular phase , showing that the ejecta density might be much larger than expected for normal core - collapse SNe .This implies that the progenitor star was massive enough to form a black hole directly without going through a neutron - star stage . If this is true , we expect that the main motor of the associated GRB would have been driven by accretion onto such a black hole rather than by radioactive decay electricity .Our study also shows that the explosion area should be found nearby to the center of the host universe , showing that the progenitor system might have experienced significant natal kick before the explosion . Keywords : Gamma - ray bursts",
        "rewrite_text": "We provide an analysis of the unique Type Ib supernova (SN) 2005bf in NGC 6946, notable for being one of only two supernovae with a clear link to a gamma-ray burst (GRB). We compare its light curve and spectral evolution with those of other GRB-associated supernovae, as well as typical SNe Ia and IIb. Our observations reveal several distinctive characteristics of SN 2005bf. Notably, it exhibits an unusually slow decline during its nebular phase, suggesting that the ejecta density may be significantly higher than what is typically observed in standard core-collapse supernovae. This finding implies that the progenitor star was massive enough to collapse directly into a black hole, bypassing the neutron star phase. If this hypothesis is accurate, we anticipate that the primary mechanism driving the associated GRB was accretion onto the black hole, rather than energy generated by radioactive decay. Additionally, our analysis indicates that the explosion site is located near the center of the host galaxy, suggesting that the progenitor system likely experienced a considerable natal kick before the explosion. Keywords: Gamma-ray bursts.",
        "ori-fast-z-score": 0.1125087900926024,
        "water-fast-z-score": 5.287913134352312,
        "rewrite-fast-z-score": -0.7875615306482168
    },
    {
        "original_text": "We study how classical and quantum uncertainty affect the price dynamics in an incomplete information setting, where agents have access to different sources of information about the underlying state variable. We show that when there is no common knowledge among traders on the true value of the state variable, they may disagree on its expected future evolution even if all are rational and risk-neutral. This disagreement leads to fluctuations in prices which can be amplified by the presence of noise traders who trade based solely on their private signals. In this case, we find that the stock returns exhibit volatility clustering and fat tails similar to those observed empirically. Finally, we demonstrate that these effects persist for both classical and quantum states with non-Gaussian statistics. The results presented here provide new insights into the role played by uncertainty in shaping the statistical properties of asset returns. They also suggest possible avenues for further research aimed at understanding the origin of such phenomena within more realistic models of trading behavior.",
        "watermark_text": "We research how classical and quantum uncertainty influence the price dynamics in an incomplete information context , where agents have access to different sources of information about the underlying state variable . We see that when there is no shared information among traders on the true value of the state variable , they may differ on its expected past development even if all are realistic and chance - neutral .This dispute leads to fluctuations in prices which can be amplified by the presence of noise traders who trade based primarily on their private signals . In this instance , we find that the stock returns display volatility clustering and lean tails identical to those observed empirically .Finally , we prove that these consequences persist for both classical and quantum states with non - Gaussian statistics . The results presented here provide fresh insights into the part played by uncertainty in shaping the statistical characteristics of investment returns .They also suggest possible avenues for further studies aimed at studying the origin of such patterns within more realistic descriptions of market behavior .",
        "rewrite_text": "We investigate how classical and quantum uncertainty affect price dynamics within a context of incomplete information, where agents have access to varying sources of information regarding the underlying state variable. Our findings indicate that in the absence of shared information among traders about the true value of the state variable, they may hold differing views on its expected past performance, even when all traders are realistic and neutral regarding chance. This disagreement can lead to price fluctuations, which may be exacerbated by the presence of noise traders who base their trading decisions primarily on private signals. In this scenario, we observe that stock returns exhibit volatility clustering and lean tails, characteristics that align with empirical observations. Furthermore, we demonstrate that these effects persist across both classical and quantum states exhibiting non-Gaussian statistics. The results we present offer new insights into the role of uncertainty in influencing the statistical properties of investment returns. They also open up avenues for further research aimed at exploring the origins of such patterns in more realistic market behavior frameworks.",
        "ori-fast-z-score": -0.7504787743864564,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": -0.618852747755276
    },
    {
        "original_text": "We present an algorithm to solve numerically the time-dependent equations describing the growth of ionized regions in the early universe, based on the Weighted Essentially Non-Oscillatory (WENO) scheme. The numerical solution is obtained by solving simultaneously two coupled partial differential equations that describe the evolution of the ionization fraction and temperature fields inside each cell of a grid covering the computational domain. We show how this method can be used to study the reionization process driven by UV photons emitted by galaxies. In particular we compare our results with those obtained using the Smooth Particle Hydrodynamics technique. Our tests indicate that both methods are able to reproduce qualitatively similar results but there are some quantitative differences between them which may have important consequences when studying the statistical properties of the IGM during the reionization epoch. \n \n Keywords: Numerical Methods, Computational Science, Cosmology, Reionization",
        "watermark_text": "We present an algorithm to solve numerically the time - dependent equations explaining the development of ionized areas in the early universe , using on the Weighted Essentially Non - Oscillatory ( WENO ) scheme . The mathematical solution is found by solving simultaneously two coupled partial differential equations that explain the evolution of the ionization fraction and heat fields inside each cell of a grid covering the theoretical domain .We see how this technology can be used to study the reionization process driven by UV photons generated by galaxies . In particular we compare our findings with those achieved using the Smooth Particle Hydrodynamics technique .Our tests show that both approaches are able to predict qualitatively identical outcome but there are some quantitative variations between them which would have important implications when examining the empirical features of the IGM during the reionization epoch . Keywords : Numerical Methods , Computational Science , Cosmology , Reionization",
        "rewrite_text": "We introduce a numerical algorithm designed to address the time-dependent equations that describe the emergence of ionized regions in the early universe, employing the Weighted Essentially Non-Oscillatory (WENO) scheme. The solution is obtained by simultaneously solving two coupled partial differential equations that characterize the evolution of both the ionization fraction and heat fields within each cell of a grid traversing the theoretical domain. This approach allows us to investigate the reionization process driven by UV photons emitted from galaxies. We specifically compare our results with those derived from the Smooth Particle Hydrodynamics technique. Our evaluations indicate that, while both methods yield qualitatively similar results, there are notable quantitative differences that could significantly influence our understanding of the empirical characteristics of the intergalactic medium (IGM) during the reionization epoch. Keywords: Numerical Methods, Computational Science, Cosmology, Reionization.",
        "ori-fast-z-score": -2.1766269588592317,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": -1.0504514628777804
    },
    {
        "original_text": "In this work, we propose an approach to human identification based on the analysis of image attributes and their relationships with each other. We use a set of visual features that are extracted by applying state-of-the-art computer vision techniques over images in order to represent them as vectors of numerical values. These feature vectors can be used to train machine learning algorithms such as Support Vector Machines (SVMs) or Random Forests (RF). In addition, we also consider the relationship between these features using Graphical Models (GM), which allow us to learn how they interact with one another. The proposed method is evaluated against two different datasets containing face images captured under controlled conditions. Our results show that our system outperforms existing approaches when identifying individuals across multiple sessions. This research was supported by the National Science Foundation through awards IIS-1253153 and CNS-1527225. In this work, we propose a novel approach to identify humans based on the analysis of their facial appearance. To do so, we extract several visual features from faces using state-of-the-art computer vision methods. Then, we model the interactions among those features using graphical models. Finally, we evaluate the performance of our method against two publicly available databases.",
        "watermark_text": "In this project , we develop an way to human identification relying on the examination of visual attributes and their connections with each other . We use a group of visual elements that are derived by using state - of - the - art computer vision techniques over images in order to represent them as matrices of numerical values .These feature vectors can be used to train machine understanding algorithms such as Support Vector Machines ( SVMs ) or Random Forests ( RF ) . In addition , we also consider the relationship between these characteristics utilizing Graphical Models ( GM ) , which allow us to study how they interact with one another .The proposed approach is evaluated against two different datasets featuring face photos taken under controlled circumstances . Our results show that our system outperforms previous techniques when identifying persons across multiple sessions .This research was supported by the National Science Foundation through awards IIS - 1253153 and CNS - 1527225 . In this research , we propose a innovative method to identify humans based on the evaluation of their facial form .To do so , we extract many graphical features from faces utilizing state - of - the - art computer vision methods . Then , we study the interactions among those characteristics utilizing graphical descriptions .Finally , we assess the performance of our technique against two publicly accessible data .",
        "rewrite_text": "In this project, we have developed a novel method for human identification that focuses on analyzing visual attributes and their interrelationships. We utilize a collection of visual elements extracted from images using cutting-edge computer vision techniques, representing these elements as matrices of numerical values. These feature vectors can be employed to train machine learning algorithms such as Support Vector Machines (SVMs) or Random Forests (RF). Furthermore, we explore the relationships among these features through Graphical Models (GM), allowing us to investigate their interactions. Our proposed approach has been tested on two distinct datasets containing face images taken in controlled environments. The results indicate that our system surpasses previous methods in identifying individuals across multiple sessions. This research was funded by the National Science Foundation under awards IIS-1253153 and CNS-1527225. In our study, we introduce an innovative approach to human identification by analyzing facial features. We extract a range of graphical features from faces using advanced computer vision techniques and examine the interactions among these features through graphical representations. Lastly, we evaluate the effectiveness of our method on two publicly available datasets.",
        "ori-fast-z-score": -1.507556722888818,
        "water-fast-z-score": 6.4,
        "rewrite-fast-z-score": -0.8340576562282991
    },
    {
        "original_text": "We revisit the slow relaxation in a model glass-forming system, namely a binary mixture of Lennard-Jones particles interacting via a Weeks-Chandler-Andersen potential with an additional repulsive shoulder at short distances. The main goal is to investigate whether this simple model can reproduce some of the features observed experimentally for silicate melts and glasses such as the stretched exponential decay of the intermediate scattering function or the presence of two distinct time scales in the structural relaxation process. To that end we perform molecular dynamics (MD) and Monte Carlo (MC) computer simulations on systems containing up to N = 1000 particles. In particular, by means of MC simulations we are able to study much larger systems than those accessible through MD runs. Our results show that both methods give rise to similar relaxation patterns but there exist subtle differences between them which may be relevant when comparing theory and experiment. Finally, we discuss how our findings compare with previous theoretical predictions based on mode-coupling theories.",
        "watermark_text": "We revisit the slow relaxation in a model glass - creating scheme , namely a binary mixture of Lennard - Jones atoms interacting via a Weeks - Chandler - Andersen current with an additional repulsive shoulder at short distances . The main goal is to examine whether this straightforward model can mimic some of the properties observed experimentally for silicate melts and glasses such as the stretched exponential decay of the intermediate absorption parameter or the presence of two separate time scales in the structural relaxation process .To that end we perform molecular mechanics ( MD ) and Monte Carlo ( MC ) computer simulations on structures containing up to N = 1000 particles . In particular , by means of MC simulations we are able to study considerably wider structures than those accessible through MD runs .Our results show that both approaches give rise to similar relax patterns but there remain simple changes between them which may be applicable when comparing theory and observation . Finally , we explain how our findings compare with previous conceptual predictions based on phase - correlation experiments .",
        "rewrite_text": "We revisit the slow relaxation characteristics in a model for glass formation, specifically a binary mixture of Lennard-Jones atoms that interact through a Weeks-Chandler-Andersen potential, supplemented by an additional repulsive shoulder for short distances. Our primary objective is to determine whether this straightforward model can replicate certain experimentally observed properties of silicate melts and glasses, such as the stretched exponential decay of the intermediate absorption parameter and the existence of two distinct time scales in the structural relaxation process. To achieve this, we conduct molecular dynamics (MD) and Monte Carlo (MC) simulations on systems comprising up to N = 1000 particles. Notably, our MC simulations enable us to explore significantly larger systems than those analyzed via MD. Our findings indicate that both methods produce similar relaxation patterns; however, there are notable differences between them that may be relevant for comparing theoretical predictions with experimental observations. Lastly, we discuss how our results align with earlier conceptual predictions derived from phase-correlation experiments.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 5.888888888888889,
        "rewrite-fast-z-score": -0.5698028822981898
    },
    {
        "original_text": "We present an analysis of data obtained with the Chandra X-ray Observatory to search for evidence that dark matter particles are being annihilated by collisions at the Galactic Center (GC). We find no significant excess emission above backgrounds, and we set upper limits on the fluxes of gamma rays produced by such processes. These results can be used to constrain models of particle physics beyond the Standard Model which predict the existence of new stable massive particles whose interactions produce gamma-ray signatures similar to those expected from dark matter annihilation products. \n \n The GC is one of the most promising targets for indirect searches for dark matter because it contains large amounts of dark matter concentrated within a small volume. In addition, there may exist astrophysical objects near the GC capable of accelerating cosmic ray protons up to energies high enough to produce gamma-rays through pion decay or inverse Compton scattering off ambient photons. However, these sources could also contribute significantly to any observed gamma-ray signal associated with dark matter annihilation.",
        "watermark_text": "We present an assessment of evidence generated with the Chandra X - ray Observatory to search for indication that dark matter elements are being annihilated by collisions at the Galactic Center ( GC ) . We see no considerable residual emission above backgrounds , and we setting lower limits on the fluxes of gamma radiation generated by such processes .These data can be used to constrain models of particle science beyond the Standard Model which predict the existence of new stable massive bodies whose interactions generate gamma - ray signatures identical to those expected from dark matter annihilation products . The GC is one of the most attractive sites for indirect searches for black material because it contains large quantities of bright particles concentrated within a small quantity .In addition , there may contain astrophysical objects near the GC capable of accelerating cosmic ray protons up to energies high enough to produce gamma - rays through pion decay or inverse Compton absorption off ambient photons . However , these sources might additionally contribute considerably to any observed gamma - ray signal associated with bright matter annihilation .",
        "rewrite_text": "We provide an evaluation of evidence collected by the Chandra X-ray Observatory in our search for signs of dark matter annihilation due to collisions at the Galactic Center (GC). Our findings reveal no significant residual emissions above the background levels, leading us to establish lower limits on the flux of gamma radiation produced by these potential processes. This information can help constrain theories of particle physics that extend beyond the Standard Model, which anticipate the existence of new, stable massive entities whose interactions create gamma-ray signatures similar to those expected from dark matter annihilation products. The GC is particularly promising for indirect searches for dark matter, as it harbors substantial concentrations of bright particles in a relatively small area. Furthermore, there may be astrophysical objects near the GC that can accelerate cosmic ray protons to energy levels sufficient to generate gamma rays through pion decay or inverse Compton scattering with surrounding photons. However, these sources could also significantly contribute to any observed gamma-ray signals related to dark matter annihilation.",
        "ori-fast-z-score": 1.7056057308448833,
        "water-fast-z-score": 6.609222207023923,
        "rewrite-fast-z-score": 2.1081851067789197
    },
    {
        "original_text": "We report on the magnetic properties of self-assembled arrays of cobalt (Co) hollow spheres, which are prepared by an electrochemical deposition method onto carbon-coated copper grids. The samples show anisotropic behavior in their hysteresis loops at room temperature as well as superparamagnetic characteristics above 300 K. We also find that the coercivity decreases rapidly when the applied field is parallel to the chain direction but remains almost unchanged for fields perpendicular to it. This indicates that the easy axis lies along the chain direction. In addition, we observe that the remanent magnetization increases gradually up to about 100 Oe after annealing the sample at 400 °C under vacuum conditions. These results suggest that the observed anisotropy originates mainly from shape effects rather than inter-particle interactions. \n \n Keywords: Anisotropy, Cobalt, Hollow sphere, Self-assembling, Superparamagnetism",
        "watermark_text": "We report on the magnetic properties of self - assembled arrays of cobalt ( Co ) hollow spheres , which are prepared by an electrochemical deposition technology onto carbon - wrapped copper grids . The samples show anisotropic dynamics in their hysteresis loops at room temperature as well as superparamagnetic qualities above 300 K . We additionally find that the coercivity decreases quickly when the introduced field is parallel to the chain direction but remains virtually unchanged for fields perpendicular to it .This implies that the easy axis lies along the chain direction . In addition , we determine that the remanent magnetization increases slowly up to about 100 Oe after annealing the sample at 400 °C under vacuum environments .These data suggest that the seen anisotropy derives mainly from shape impacts rather than inter - particle processes . Keywords : Anisotropy , Cobalt , Hollow sphere , Self - assembling , Superparamagnetism",
        "rewrite_text": "We present our findings on the magnetic properties of self-assembled arrays of cobalt (Co) hollow spheres, which are fabricated using electrochemical deposition on carbon-coated copper grids. The samples exhibit anisotropic behavior in their hysteresis loops at room temperature and demonstrate superparamagnetic characteristics at temperatures exceeding 300 K. Furthermore, we observe that the coercivity decreases rapidly when the applied field is aligned with the chain direction, while it remains largely unchanged when the field is perpendicular. This indicates that the easy axis is oriented along the chain direction. Additionally, after annealing the samples at 400 °C in a vacuum, we find that the remanent magnetization gradually rises to approximately 100 Oe. These results suggest that the observed anisotropy is primarily due to shape effects rather than inter-particle interactions. \n\nKeywords: Anisotropy, Cobalt, Hollow spheres, Self-assembly, Superparamagnetism",
        "ori-fast-z-score": 0.6401843996644799,
        "water-fast-z-score": 5.505585837114527,
        "rewrite-fast-z-score": 0.3721042037676254
    },
    {
        "original_text": "The Stochastic Loewner Evolution (SLE) is an important tool in the study of random curves and fractals, especially those arising as scaling limits of discrete models such as lattice paths or self-avoiding walks.  The SLE trace can be thought of as a continuous version of Brownian motion with drift; it has been shown to have connections to many other fields including quantum gravity, string theory, statistical mechanics, probability theory, mathematical physics, number theory, and computer science.   In this article we will give a brief introduction to the basic concepts behind the SLE process, along with some examples that illustrate its use. We also provide references for further reading on the subject. For more information about the SLE process see the following articles:  http://arxiv.org/abs/math/9906028 http://arxiv.org/sabs/0909.0366 http://arxiv.org/10.1103/PhysRevE.71.026110",
        "watermark_text": "The Stochastic Loewner Evolution ( SLE ) is an important tool in the study of random curves and fractals , particularly those arose as scaling limits of linear models such as lattice trails or self - escaping tours . The SLE trace can be thought of as a continuous version of Brownian movement with drift ; it has been shown to have link to many other fields including quantum gravitational , string theory , statistical mechanics , probability theory , mathematical science , number theory , and computer science .In this article we will provide a brief introduction to the fundamental concepts behind the SLE method , along with some examples that highlight its use . We additionally offer references for further reading on the subject .For more information about the SLE method see the following articles : www : / / arxiv . org / abs / math / 9906028 http : / / arxiv . org / sabs / 0909 . 0366 www : / / arxiv . org / 10 . 1103 / PhysRevE . 71 . 026110",
        "rewrite_text": "The Stochastic Loewner Evolution (SLE) is a crucial tool for investigating random curves and fractals, especially those that emerge as scaling limits of linear models like lattice trails or self-avoiding walks. The SLE trace can be interpreted as a continuous analogue of Brownian motion with drift. Its relevance has been demonstrated across various fields, including quantum gravity, string theory, statistical mechanics, probability theory, mathematics, number theory, and computer science. In this article, we will present a concise overview of the key concepts underlying the SLE method, accompanied by examples that showcase its applications. We will also provide references for those interested in exploring the topic further. For additional information about the SLE method, please refer to the following articles: www://arxiv.org/abs/math/9906028, http://arxiv.org/abs/0909.0366, and www://arxiv.org/10.1103/PhysRevE.71.026110.",
        "ori-fast-z-score": 0.24618298195866545,
        "water-fast-z-score": 5.25,
        "rewrite-fast-z-score": 2.0465595024580763
    },
    {
        "original_text": "Muon-catalyzed fusion (MCF) is an exotic nuclear reaction that can be used to produce energy in future reactors, but it requires extremely pure hydrogen gas as fuel.  The MuCap experiment at TRIUMF has developed and tested a novel system for producing ultra-pure hydrogen using liquid helium cryogenic distillation followed by two stages of molecular sieves.  This system produces up to 1 liter per minute with less than 10 parts-per-trillion impurities.  It will provide enough clean hydrogen gas to operate the MuCap experiment until 2020 when the next generation of experiments are expected to begin taking data. A circulating hydrogen ultra-high purification system was designed and built for the MuCap experiment at TRI-UMF. Liquid helium cryogenic distillation is combined with two stages of molecular sieve beds to achieve high purity levels required for MCF research. The system provides up to one litre of purified hydrogen per minute with less than ten parts-per-trillion impurity content.",
        "watermark_text": "Muon - catalyzed fusion ( MCF ) is an exotic radioactive reaction that can be used to produce energy in future reactors , but it costs extremely solid hydrogen energy as fuel . The MuCap project at TRIUMF has constructed and demonstrated a innovative system for producing ultra - pure hydrogen utilizing liquid helium cryogenic distillation preceded by two stages of molecular sieves .This system creates up to 1 liter per minute with fewer than 10 parts - per - trillion impurities . It will provide enough clean hydrogen energy to run the MuCap project until 2020 when the new generation of studies are expected to begin take data .A circulating hydrogen super - high purification system was developed and developed for the MuCap project at TRI - UMF . Liquid helium cryogenic distillation is combined with two stages of molecular sieve beds to achieve high purity levels needed for MCF research .The system provides up to one litre of purified fuel per moment with fewer than ten components - per - trillion impurity content .",
        "rewrite_text": "Muon-catalyzed fusion (MCF) is a unique radioactive process that holds promise for generating energy in future reactors, though it requires highly pure solid hydrogen as fuel. At TRIUMF, the MuCap project has successfully developed an innovative system for producing ultra-pure hydrogen through a process of liquid helium cryogenic distillation followed by two stages of molecular sieves. This advanced system is capable of generating up to one liter of hydrogen per minute with less than ten parts per trillion in impurities. It is expected to supply the clean hydrogen needed for the MuCap project until 2020, when a new generation of experiments is anticipated to begin data collection. A highly efficient hydrogen purification system was specifically designed for the MuCap project, utilizing liquid helium cryogenic distillation in combination with two stages of molecular sieve beds to achieve the necessary purity levels for MCF research.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": 0.8307471607356973
    },
    {
        "original_text": "We present an analytical method to determine the physical properties (size, mass, and density) of exomoons using only their light curves in transits. We show that this method is robust against uncertainties in the planet s orbital parameters by applying it to simulated data sets with different values for the semi-major axis, eccentricity, inclination angle, argument of periastron, longitude of ascending node, mean anomaly at epoch, and time of passage through periastron. The results are compared to those obtained when fitting directly for these six orbital elements as well as the moon-to-host radius ratio and moon phase function simultaneously. Our analysis shows that our new method can be used to obtain accurate estimates of the moon s physical characteristics even if its orbit has significant eccentricities or inclinations. \n \n Keywords: Exoplanet, Moon, Transit Timing Variations, Photometry \n \n Transiting planets have been found around more than 1000 stars so far1. Many of them exhibit periodic dimming events caused by moons2-5. These moons may play important roles in planetary evolution6-8 but they cannot be detected via direct imaging techniques because of their small sizes9-11. Therefore, we need other methods to study their physical properties12-14. In particular, the detection of moons around extrasolar giant planets would provide valuable information about how such systems form15-17. \n \n Here we propose a novel approach to estimate the physical characteristics of exomoons based on their light curves alone18-20. This method does not require any prior knowledge of the planet s orbital parameters21-24. It also allows us to detect moons whose orbits are highly inclined25-27 and/or eccentric28-30 relative to the plane of the sky31-33. Moreover, it works equally well whether the moon is tidally locked34-36 or free-rotating37-39. Finally, it provides reliable measurements of the moon s size, mass, and bulk density40-42. \n \n To demonstrate the feasibility of our method, we apply it to simulated data sets generated under various conditions43-45. We find that our technique yields accurate estimates of",
        "watermark_text": "We present an analytical method to estimate the physical properties ( size , mass , and density ) of exomoons utilizing only their light curves in transits . We see that this technology is robust against uncertainties in the planet s orbital variables by using it to modeled information sets with various estimates for the semi - major axis , eccentricity , inclination angle , argument of periastron , longitude of ascending node , mean anomaly at epoch , and period of passage through periastron .The results are compared to those achieved when fitting directly for these six orbital elements as well as the lunar - to - host radius ratio and moon phase equation simultaneously . Our study shows that our new method can be used to obtain precise estimates of the lunar s physical qualities even if its orbit has significant eccentricities or inclinations .Keywords : Exoplanet , Moon , Transit Timing Variations , Photometry Transiting planets have been detected around more than 1000 stars so far1 . Many of them exhibit periodic dimming events produced by moons2 - 5 .These moons might play vital functions in planetary evolution6 - 8 but they cannot be identified via direct scanning techniques because of their tiny sizes9 - 11 . Therefore , we require other methods to study their physical properties12 - 14 .In particular , the observation of moons around extrasolar giant worlds may provide valuable info about how such systems form15 - 17 . Here we undertake a new approach to estimate the physical qualities of exomoons depending on their light curves alone18 - 20 .This method does not require any earlier knowledge of the planet s orbital parameters21 - 24 . It additionally lets us to identify moons whose orbits are extremely inclined25 - 27 and / or eccentric28 - 30 relative to the plane of the sky31 - 33 .Moreover , it works extremely good whether the lunar is tidally locked34 - 36 or free - rotating37 - 39 . Finally , it gives reliable measurements of the lunar s diameter , mass , and bulk density40 - 42 .To show the feasibility of our technique , we apply it to modeled information sets generated under various conditions43 - 45 . We see that our technique yields good estimates of",
        "rewrite_text": "We introduce an analytical method for estimating the physical properties—size, mass, and density—of exomoons using only their transit light curves. Our technology demonstrates robustness against uncertainties in the orbital parameters of the host planet by applying it to simulated datasets with varying estimates for the semi-major axis, eccentricity, inclination angle, argument of periastron, longitude of the ascending node, mean anomaly at epoch, and the period of periastron passage. We compare our results to those obtained by directly fitting these six orbital elements, along with the ratio of the moon's radius to that of its host and the moon phase equation, simultaneously. Our findings indicate that this new method can provide accurate estimations of the exomoon's physical characteristics, even when its orbit exhibits significant eccentricities or inclinations. \n\nKeywords: Exoplanet, Moon, Transit Timing Variations, Photometry\n\nTo date, more than 1,000 transiting planets have been identified around various stars. Many of these systems display periodic dimming events caused by the presence of moons. These moons may play crucial roles in planetary evolution, yet their small sizes impede identification through direct observational techniques. Thus, alternative methods are necessary for studying their physical properties. In particular, detecting moons orbiting extrasolar giant planets could yield valuable insights into the formation of such systems. In this paper, we propose a novel approach that relies solely on analyzing light curves to estimate the physical qualities of exomoons. This technique does not necessitate prior knowledge of the planet's orbital parameters and allows for the detection of moons with highly inclined or eccentric orbits relative to the plane of the sky. Furthermore, it proves effective whether the moon is tidally locked or free-rotating, providing reliable measurements of the moon's diameter, mass, and bulk density. To demonstrate the viability of our method, we apply it to simulated datasets generated under varying conditions and find that it produces favorable estimations of the aforementioned parameters.",
        "ori-fast-z-score": -0.4931969619160719,
        "water-fast-z-score": 8.713146327183937,
        "rewrite-fast-z-score": 0.6405126152203485
    },
    {
        "original_text": "We present an analytical model to study the effect of mass loading feedback on particle concentration and enstrophy in fully developed turbulence. The cascade process is modeled by using a set of coupled ordinary differential equations, which are derived based on dimensional analysis and Kolmogorov s similarity hypothesis. We show that the presence of particles can significantly affect both the energy transfer rate between different scales as well as the dissipation rates at small scales. In particular, we find that the total amount of energy transferred into smaller scales decreases when there exists significant mass loading feedback. This result suggests that the presence of heavy particles may lead to reduced turbulent mixing efficiency. Finally, our results also indicate that the effects of mass loading feedback become more pronounced if the Stokes number increases or the initial volume fraction of particles becomes larger. Our findings provide useful insights into understanding how heavy particles influence the dynamics of fluid flows. C \nAuthor(s): Yi-Chun Chen , Shih-Chieh Hwang , Chia-Hui Wu , Yu-Ting Lin , Ming-Yuan Liu , Chao-Lin Wang , Jie-Sheng Huang , Wen-Ju Tsai , Tzi-Chao Chan , Chin-Fa Lee , Kuo-Yang Chang , Chung-Ming Yeh , Yuan-Kang Chiou , Chien-Nan Chu , Cheng-Wei Hsieh , Chien-Wen Lu , Chien-Chung Wu , Chien-Shu Chen , Chien-Chin Wu , Chien-Chin Yang , Chien-Chin Lai , Chien-Chin Su , Chien-Chin Hung , Chien-Chin Chen , Chien-Ching Wu , Chien-Ching Tai , Chien-Ching Li , Chien-Ching Sun , Chien-Ching Liang , Chien-Ching Chen , Chien-Chong Wu , Chien-Chung Chen , Chien-Chung Lai , Chien-Chung Su , Chien-Chung Hung , Chien-Chung",
        "watermark_text": "We present an analytical theory to study the impact of mass displacement feedback on particle concentration and enstrophy in fully developed turbulence . The cascade process is modeled by using a setting of coupled ordinary differential coefficients , which are derived based on dimensional evaluation and Kolmogorov s similarity hypothesis .We see that the presence of particles can significantly affect both the power transfer frequency between various scales as well as the dissipation levels at small scales . In particular , we find that the total quantity of power sent into smaller scales decreases when there exists significant volume displacement feedback .This result suggests that the presence of hard particles may contribute to reduced fluid mixing efficiency . Finally , our findings also suggest that the effects of mass displacement feedback become more pronounced if the Stokes number increases or the initial volume fraction of molecules changes larger .Our findings provide useful insights into studying how heavy grains alter the dynamics of fluid flows . C Author ( s ) : Yi - Chun Chen , Shih - Chieh Hwang , Chia - Hui Wu , Yu - Ting Lin , Ming - Yuan Liu , Chao - Lin Wang , Jie - Sheng Huang , Wen - Ju Tsai , Tzi - Chao Chan , Chin - Fa Lee , Kuo - Yang Chang , Chung - Ming Yeh , Yuan - Kang Chiou , Chien - Nan Chu , Cheng - Wei Hsieh , Chien - Wen Lu , Chien - Chung Wu , Chien - Shu Chen , Chien - Chin Wu , Chien - Chin Yang , Chien - Chin Lai , Chien - Chin Su , Chien - Chin Hung , Chien - Chin Chen , Chien - Ching Wu , Chien - Ching Tai , Chien - Ching Li , Chien - Ching Sun , Chien - Ching Liang , Chien - Ching Chen , Chien - Chong Wu , Chien - Chung Chen , Chien - Chung Lai , Chien - Chung Su , Chien - Chung Hung , Chien - Chung",
        "rewrite_text": "We introduce an analytical framework to investigate the effects of mass displacement feedback on particle concentration and enstrophy in fully developed turbulence. The cascade mechanism is modeled using a set of coupled ordinary differential equations, derived through dimensional analysis and Kolmogorov’s similarity hypothesis. Our results demonstrate that the presence of particles can significantly influence both the power transfer frequency across different scales and the levels of dissipation at smaller scales. Notably, we observe that the total power directed towards smaller scales diminishes when substantial volume displacement feedback is present. This finding implies that hard particles may reduce the efficiency of fluid mixing. Additionally, our results indicate that the impact of mass displacement feedback becomes more pronounced with an increase in the Stokes number or a larger initial volume fraction of the particles. Overall, our research offers valuable insights into how heavy grains affect the dynamics of fluid flows. \n\nAuthors: Yi-Chun Chen, Shih-Chieh Hwang, Chia-Hui Wu, Yu-Ting Lin, Ming-Yuan Liu, Chao-Lin Wang, Jie-Sheng Huang, Wen-Ju Tsai, Tzi-Chao Chan, Chin-Fa Lee, Kuo-Yang Chang, Chung-Ming Yeh, Yuan-Kang Chiou, Chien-Nan Chu, Cheng-Wei Hsieh, Chien-Wen Lu, Chien-Chung Wu, Chien-Shu Chen, Chien-Chin Wu, Chien-Chin Yang, Chien-Chin Lai, Chien-Chin Su, Chien-Chin Hung, Chien-Chin Chen, Chien-Ching Wu, Chien-Ching Tai, Chien-Ching Li, Chien-Ching Sun, Chien-Ching Liang, Chien-Ching Chen, Chien-Chong Wu, Chien-Chung Chen, Chien-Chung Lai, Chien-Chung Su, Chien-Chung Hung.",
        "ori-fast-z-score": 0.7844645405527362,
        "water-fast-z-score": 7.060180864974626,
        "rewrite-fast-z-score": 2.154554539378824
    },
    {
        "original_text": "We present an algorithm for quantum search that is based on the Grover s algorithm and uses only two qubits to represent one item in the database, which can be viewed as a binary number. The algorithm has been implemented using IBM Q Experience simulator with four different databases containing up to 16 items each. We have also compared our results against those obtained by running Grover s original algorithm on the same datasets. Our experimental results show that the proposed algorithm performs better than its classical counterpart when searching through small databases (up to 8 items). However, it becomes less efficient if we increase the size of the database beyond this limit. This work was supported by the Australian Research Council Discovery Project DP160103745. In recent years there has been significant interest in developing algorithms for performing quantum searches over large data sets  1  . These algorithms are expected to find applications in areas such as machine learning  2  , pattern recognition  3  , computer vision  4  , bioinformatics  5  , etc., where they will allow us to solve problems faster or more accurately  6  .\nIn general, these algorithms use N qubits to encode M elements in the database  7, 8  . For example, Grover s algorithm  9  requires O( √ N/M ) iterations to find any single element out of M elements encoded into N qubits  10  . It should be noted here that the number of required iterations increases exponentially with respect to both N and M  11  . Therefore, these algorithms become inefficient when dealing with very large databases  12  .",
        "watermark_text": "We introduce an algorithm for quantum search that is based on the Grover s algorithm and using only two qubits to represent one element in the database , which can be viewed as a binary value . The algorithm has been deployed using IBM Q Experience simulator with four different databases containing up to 16 artifacts each .We have already compared our findings against those achieved by running Grover s original method on the same datasets . Our research results show that the suggested method performs better than its classical rival when looking through tiny data ( up to 8 objects ) .However , it becomes slower efficient if we increase the length of the database beyond this limit . This research was supported by the Australian Research Council Discovery Project DP160103745 .In recent years there has been significant interest in building methods for performing quantum searches over large data sets 1 . These methods are expected to find uses in areas such as machine computing 2 , image learning 3 , computer vision 4 , bioinformatics 5 , etc . , where they will let us to solve issues quicker or more accurately 6 .In general , these algorithms use N qubits to encode M elements in the database 7 , 8 . For instance , Grover s algorithm 9 requires O ( √ N / M ) iterations to find any single element out of M elements stored into N qubits 10 .It should be mentioned here that the number of required iterations increases exponentially with regard to both N and M 11 . Therefore , these algorithms get inefficient when dealing with very huge databases 12 .",
        "rewrite_text": "We present a quantum search algorithm inspired by Grover's algorithm, which utilizes only two qubits to represent a single element from the database, effectively operating as a binary value. This algorithm was tested on the IBM Q Experience simulator across four distinct databases, each containing up to 16 items. Our findings have been compared to the results obtained from Grover's original method applied to the same datasets. The results indicate that our proposed method outperforms its classical counterpart for small datasets (up to 8 items); however, its efficiency declines as the database size increases beyond this threshold. This research was funded by the Australian Research Council Discovery Project DP160103745. Recently, there has been a growing interest in developing quantum search techniques for large datasets. These techniques are anticipated to be beneficial in various domains such as machine learning, image processing, computer vision, and bioinformatics, enabling faster and more accurate problem-solving. Typically, these algorithms employ N qubits to encode M elements in the database. For instance, Grover's algorithm necessitates O(√N/M) iterations to locate any single element among M elements stored in N qubits. It is important to note that the required number of iterations increases exponentially with respect to both N and M, leading to inefficiencies when handling very large databases.",
        "ori-fast-z-score": -0.19802950859533489,
        "water-fast-z-score": 7.389969586232199,
        "rewrite-fast-z-score": 0.5241424183609592
    },
    {
        "original_text": "Circinus X-1 is an X-ray binary system with a neutron star and its companion, which has been observed in many wavelengths ranging from radio to gamma-ray bands. The source shows periodic dipping activity at X-ray energies that are caused by obscuration of the central X-ray emitting region due to matter falling onto the accretion disk around the compact object. In this work we present results obtained using data collected during two different observational campaigns carried out with Suzaku satellite (from 2005 to 2007) and INTEGRAL/IBIS telescope (from 2003 to 2009). We have analyzed the spectral properties of the source for both observations separately as well as combined together. Our analysis reveals that the spectrum can be described by a combination of several components such as: blackbody emission from the neutron star surface; Comptonized component produced by hot plasma surrounding the neutron star; reflection component originating from reprocessing of hard radiation emitted by the central X-ray source into softer photons; iron line feature arising from fluorescence of cold material located close to the neutron star.",
        "watermark_text": "Circinus X - 1 is an X - ray binary system with a neutron star and its companion , which has been observed in multiple wavelengths ranging from radio to gamma - ray bands . The source shows irregular dipping activity at X - ray energies that are created by obscuration of the main X - ray emitting area owing to matter falling onto the accretion disk around the compact body .In this project we present results collected using data taken during two different observational campaigns carried out with Suzaku spacecraft ( from 2005 to 2007 ) and INTEGRAL / IBIS telescope ( from 2003 to 2009 ) . We have analyzed the spectral properties of the origin for both surveys independently as well as combined together .Our study reveals that the spectrum can be described by a combination of several parts such as : blackbody emission from the neutron star surface ; Comptonized component produced by hot plasma surrounding the neutron star ; reflection portion arising from reprocessing of hard radiation emitted by the main X - ray source into stronger photons ; iron line feature arising from fluorescence of cold matter located close to the neutron star .",
        "rewrite_text": "Circinus X-1 is an X-ray binary system consisting of a neutron star and its companion. This system has been observed across a range of wavelengths, from radio to gamma rays. It exhibits irregular dipping activity in X-ray energies, which is caused by the obscuration of the primary X-ray emission area due to matter falling onto the accretion disk around the neutron star. In this project, we present findings from data collected during two separate observational campaigns: one using the Suzaku spacecraft between 2005 and 2007, and the other with the INTEGRAL/IBIS telescope from 2003 to 2009. We analyzed the spectral characteristics of both surveys both independently and in combination. Our research indicates that the spectrum can be described by several components, including: blackbody emission from the surface of the neutron star; a Comptonized component produced by hot plasma surrounding the neutron star; a reflection component resulting from the reprocessing of hard radiation emitted by the main X-ray source into higher-energy photons; and an iron line feature arising from the fluorescence of cold matter located near the neutron star.",
        "ori-fast-z-score": 1.801996396010812,
        "water-fast-z-score": 6.041987916036252,
        "rewrite-fast-z-score": 2.27776980709589
    },
    {
        "original_text": "We present an analysis of the basis-set dependence of post-Hartree-Fock (HF) corrections to atomization energies for small molecules, using explicitly correlated Gaussian functions and extrapolation techniques.  We show that the correlation energy contribution is more sensitive than the HF energy to the choice of basis sets used in calculations. The results are compared with those obtained by other authors who have studied this problem previously. Finally we discuss how these findings can be applied to improve the accuracy of thermochemical data calculated at the CCSD(T) level. In recent years there has been considerable interest in improving the accuracy of theoretical predictions of thermochemical properties such as enthalpies of formation or heats of reaction. These quantities are often determined experimentally but it would clearly be useful if they could also be predicted theoretically. One approach which has proved successful involves calculating the total electronic energy E tot  n  of a molecule within some chosen approximation n to quantum mechanics, where n = 1 corresponds to Hartree-Fock theory and n = 2 to second-order Møller-Plesset perturbation theory (MP2). Corrections beyond MP2 may then be estimated either by performing higher-level ab initio calculations on smaller subsets of atoms  1  , or alternatively by fitting empirical parameters to experimental data  2  .",
        "watermark_text": "We present an assessment of the basis - set dependence of post - Hartree - Fock ( HF ) corrections to atomization energies for little molecules , using explicitly coupled Gaussian functions and extrapolation methods . We see that the correlation power contribution is more sensitive than the HF power to the selection of basis sets involved in calculations .The results are compared with those acquired by other researchers who have researched this question previously . Finally we talk how these results can be applied to improve the accuracy of thermochemical data calculated at the CCSD ( T ) level .In recent years there has been substantial interest in improving the accuracy of theoretical estimates of thermochemical properties such as enthalpies of formation or heats of reaction . These quantities are often determined experimentally but it would clearly be valuable if they could also be determined theoretically .One approach which has found successful means measuring the total electronic energy E tot n of a molecule within some selected approximation n to quantum mechanics , where n = 1 corresponds to Hartree - Fock physics and n = 2 to second - order Møller - Plesset perturbation theory ( MP2 ) . Corrections beyond MP2 might then be estimated either by performing higher - grade ab initio analyses on smaller subsets of atoms 1 , or alternatively by fitting experimental parameters to experimental data 2 .",
        "rewrite_text": "We provide an evaluation of the dependence of post-Hartree-Fock (HF) corrections on basis sets for small molecules, utilizing explicitly coupled Gaussian functions and extrapolation techniques. Our findings indicate that the contribution from correlation effects is more sensitive to the choice of basis sets than the HF contributions in our calculations. We compare our results with those obtained from previous studies on this topic. Additionally, we discuss how these findings can be leveraged to enhance the accuracy of thermochemical data computed at the CCSD(T) level. Recently, there has been significant interest in refining the theoretical predictions of thermochemical properties, such as enthalpies of formation and reaction heats. While these values are typically measured experimentally, it would be beneficial to also derive them through theoretical approaches. One effective method is to measure the total electronic energy, E_tot(n), of a molecule using a specific approximation n to quantum mechanics, where n=1 corresponds to Hartree-Fock theory and n=2 to second-order Møller-Plesset perturbation theory (MP2). Corrections beyond MP2 may be estimated either through higher-level ab initio analyses on smaller groups of atoms or by fitting experimental parameters to experimental data.",
        "ori-fast-z-score": -0.7035264706814485,
        "water-fast-z-score": 5.050762722761053,
        "rewrite-fast-z-score": -0.21320071635561041
    },
    {
        "original_text": "We present new observations of three protostellar cores in Orion B, which are among the youngest known prestellar objects. The data were obtained with the Submillimeter Array (SMA) at 1.3 mm and 0.87 mm wavelengths as part of an ongoing survey to study the earliest stages of star formation. We find that all three sources show evidence for infall motions on scales ranging between 1000 AU and 10000 AU. In addition, we detect outflows associated with two of these sources. These results suggest that the first phase of star formation is characterized by rapid mass accretion onto the central object through a circumstellar disk. This process may be triggered by gravitational collapse or magneto-rotational instabilities within the core. Finally, our SMA maps reveal extended emission around each source, which could represent either flattened envelopes or disks surrounding the protostar. Our findings provide important constraints on theoretical models describing the early evolution of young stellar objects.",
        "watermark_text": "We report new studies of three protostellar cores in Orion B , which are among the youngest known prestellar objects . The data were obtained with the Submillimeter Array ( SMA ) at 1 . 3 cm and 0 . 87 mm wavelengths as part of an continuing survey to study the earliest periods of star formation .We see that all three reports show proof for infall motions on scales ranging between 1000 AU and 10000 AU . In addition , we find outflows associated with two of these sources .These data suggest that the first phase of star formation is characterized by rapid mass accretion onto the main object through a circumstellar disk . This process may be triggered by gravitational failure or magneto - rotational instabilities within the core .Finally , our SMA images produce extended emission around each source , which could indicate either flattened envelopes or disks circling the protostar . Our findings provide important restrictions on theoretical theories describing the early evolved of young stellar bodies .",
        "rewrite_text": "We present new research on three protostellar cores in Orion B, which are among the youngest known prestellar objects. The observations were made using the Submillimeter Array (SMA) at wavelengths of 1.3 cm and 0.87 mm as part of an ongoing survey aimed at investigating the earliest stages of star formation. All three cores exhibit evidence of infall motions occurring on scales between 1,000 AU and 10,000 AU. Additionally, we observe outflows associated with two of these sources. This data suggests that the initial phase of star formation is characterized by rapid mass accretion onto the central object via a circumstellar disk. This process may be initiated by gravitational collapse or magneto-rotational instabilities within the core. Furthermore, our SMA images reveal extended emission surrounding each source, which may indicate either flattened envelopes or disks orbiting the protostar. Our findings impose significant constraints on theoretical models that describe the early evolution of young stellar objects.",
        "ori-fast-z-score": 0.47809144373375745,
        "water-fast-z-score": 5.976143046671968,
        "rewrite-fast-z-score": 0.8427009716003844
    },
    {
        "original_text": "The structure and dynamics of lipid membranes are important for many biological processes, such as cell division or protein transport across the membrane. In this work we use atomic force microscopy to investigate the structural properties of stacked layers of phospholipids in water. We find that these structures form spontaneously on mica surfaces at room temperature within minutes after adding the lipids into solution. The height profiles show that the thicknesses of the individual layers vary between 1 nm and 2 nm depending on their composition. By analyzing the lateral diffusion coefficients of single molecules with respect to time, we can determine whether they are mobile or immobile. Our results indicate that the mobility is strongly dependent on the number of layers present in each stack. For example, while most of the molecules in one layer diffuse freely over large distances, those in two layers exhibit only small displacements perpendicular to the surface. This behavior suggests that the mobility decreases significantly when more than one layer forms.",
        "watermark_text": "The structure and dynamics of lipid membranes are important for numerous biological events , such as cell division or protein transport across the membrane . In this research we utilize atomic force microscopy to examine the structural structure of piled sheets of phospholipids in water .We see that these structures create spontaneously on mica surfaces at room temperature within moments after addition the lipids into solution . The height profiles indicate that the thicknesses of the individual layers varies between 1 nm and 2 nm depending on their composition .By analyzing the longitudinal diffusion coefficients of single particles with regard to time , we can determine whether they are portable or immobile . Our results show that the mobility is strongly dependent on the quantity of elements contained in each stack .For instance , while most of the molecules in one layer diffuse widely over large distances , those in two sheets experience only tiny displacements parallel to the surface . This phenomenon suggests that the mobility decreases dramatically when more than one surface exists .",
        "rewrite_text": "The structure and dynamics of lipid membranes play a crucial role in various biological processes, including cell division and the transport of proteins across membranes. In this study, we employ atomic force microscopy to investigate the structural characteristics of stacked phospholipid sheets in water. We observe that these structures spontaneously form on mica surfaces at room temperature within moments of the lipids being added to the solution. Height profiling reveals that the thickness of the individual layers ranges from 1 nm to 2 nm, depending on their composition. By examining the longitudinal diffusion coefficients of individual particles over time, we can assess their mobility. Our findings indicate that mobility is significantly influenced by the number of elements in each stack. For example, while most molecules in a single layer can diffuse over large distances, those in two layers display only minor displacements parallel to the surface. This phenomenon implies a substantial reduction in mobility when multiple layers are present.",
        "ori-fast-z-score": -0.7977240352174656,
        "water-fast-z-score": 5.8119893994415355,
        "rewrite-fast-z-score": 0.3418817293789138
    },
    {
        "original_text": "We study the ground state properties of the spin-1/2 square lattice with nearest-neighbor antiferromagnetic interactions and vacancies, using exact diagonalization (ED) on finite clusters up to 12x12 sites. We find that for low concentrations of vacancies there is no significant change in the magnetic order parameter or the energy gap between singlet and triplet excitations as compared to the pure system. However, we observe an increase in the density of states at zero energy when increasing the concentration of vacancies. This effect can be explained by considering the formation of bound pairs of vacancies which are localized around each other due to their mutual interaction. The binding energies of these pairs depend strongly on the distance between them but only weakly on the size of the cluster considered. In addition, we show how this behavior changes if one considers next-nearest neighbor interactions instead of nearest-neighbor ones. Finally, we discuss possible experimental realizations of our results.",
        "watermark_text": "We research the ground state properties of the spin - 1 / 2 square lattice with nearest - neighbor antiferromagnetic interactions and vacancies , using accurate diagonalization ( ED ) on polynomial clusters up to 12x12 places . We see that for low levels of vacancies there is no major shift in the magnetic order parameter or the power gap between singlet and triplet excitations as compared to the pure system .However , we perceive an increase in the density of states at zero energy when increasing the density of vacancies . This phenomenon can be described by using the formation of bound pairs of vacancies which are localized around each other owing to their mutual interaction .The activation energies of these pairs depend greatly on the distance between them but only weakly on the length of the cluster considered . In addition , we show how this behavior changes if one considers next - nearest neighbor interactions instead of nearest - neighbor ones .Finally , we explain possible experimental realizations of our findings .",
        "rewrite_text": "We investigate the ground state properties of a spin-1/2 square lattice with nearest-neighbor antiferromagnetic interactions and vacancies, utilizing precise diagonalization (ED) on polynomial clusters of up to 12x12 sites. Our findings indicate that, at low vacancy levels, there is no significant change in the magnetic order parameter or the energy gap between singlet and triplet excitations when compared to the pure system. However, we observe an increase in the density of states at zero energy as the vacancy density rises. This behavior can be attributed to the formation of bound pairs of vacancies that localize due to their mutual interactions. The activation energies of these pairs vary significantly with their separation but are only weakly influenced by the size of the cluster analyzed. Additionally, we discuss how this behavior alters when next-nearest neighbor interactions are considered instead of nearest-neighbor interactions. Finally, we propose potential experimental implementations of our results.",
        "ori-fast-z-score": 0.8307471607356973,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 1.7556172079419585
    },
    {
        "original_text": "We present the results of our investigation into accretion disk continuum emission in black hole candidates (BHCs). We have developed an analytical model for calculating the spectrum emitted by a thin, optically thick accretion disk around a Schwarzschild black hole and applied it to several BHCs with known mass functions. The observed spectra are well reproduced when we assume that the inner edge of the disk is located at 6 gravitational radii. This result suggests that the standard thin disk model can be used as a good approximation for modeling the X-ray continuum emission of these objects. \n \n Keywords: Black holes -- Spectroscopy -- X-rays -- Modeling -- Accretion disks -- Emission lines -- Broad-band spectral energy distribution -- Luminosity function -- Mass measurement -- Stellar-mass black holes -- Supermassive black holes -- Active galactic nuclei -- Quasars -- Cosmic evolution \n \n \n \n 1 Introduction \n \n In recent years there has been considerable progress made towards understanding the physical processes occurring near supermassive black holes (SMBH) in active galactic nuclei (AGN), quasars, and other similar systems. These studies rely on observations of the broad-band spectral energy distributions (SEDs) of SMBHs over many decades in frequency space. However, because of their enormous distances, direct measurements of the intrinsic luminosities of most AGNs are not possible. Instead, one must use indirect methods such as reverberation mapping or statistical correlations between various properties of AGNs to determine their luminosities. For example, if one knows how much light passes through some region of interest within an AGN then one may calculate its luminosity using simple geometric arguments. Alternatively, if one knows the distance to an AGN then one could measure its absolute magnitude directly. Unfortunately, both of these approaches require detailed knowledge about the structure of the emitting regions which cannot currently be obtained observationally. Therefore, in order to make accurate estimates of the luminosities of distant AGNs, one needs to develop models capable of reproducing the observed SEDs of nearby AGNs.",
        "watermark_text": "We present the conclusion of our inquiry into accretion disk continuum emission in black hole candidates ( BHCs ) . We have developed an analytical theory for determining the spectrum emitted by a thin , optically dense accretion disk around a Schwarzschild red hole and applied it to several BHCs with reported mass distributions .The observed spectra are better illustrated when we suppose that the inner corner of the disk is situated at 6 gravitational radii . This result suggests that the standard narrow disk model can be used as a better approximation for modeling the X - ray continuum emission of these objects .Keywords : Black holes - - Spectroscopy - - X - rays - - Modeling - - Accretion disks - - Emission lines - - Broad - band spectral energy distribution - - Luminosity function - - Mass measurement - - Stellar - mass black holes - - Supermassive black holes - - Active galactic nuclei - - Quasars - - Cosmic evolution 1 Introduction In recent years there has been increased progress conducted towards exploring the physical processes occurring near supermassive black holes ( SMBH ) in active galactic nuclei ( AGN ) , quasars , and other similar systems . These studies rely on observations of the broad - band spectral energy distributions ( SEDs ) of SMBHs over many decades in frequency space .However , because of their huge altitudes , direct measurements of the intrinsic luminosities of most AGNs are not required . Rather , one must use indirect tools such as reverberation projection or statistical correlations between various properties of AGNs to obtain their luminosities .For instance , if one remembers how many light passes through some region of interest within an AGN then one may calculate its luminosity taking simple geometric arguments . Alternatively , if one understands the distance to an AGN then one might estimate its absolute magnitude directly .Unfortunately , both of these perspectives need rigorous knowledge about the composition of the emitting regions which lacks already be obtained observationally . Therefore , in order to make accurate calculations of the luminosities of distant AGNs , one needs to develop models capable of reproducing the known SEDs of neighbouring AGNs .",
        "rewrite_text": "We conclude our investigation into the continuum emission of accretion disks surrounding black hole candidates (BHCs). Our work centers around an analytical framework designed to calculate the spectrum emitted by a thin, optically dense accretion disk encircling a Schwarzschild black hole, which we have applied to various BHCs with established mass distributions. Our findings indicate that the observed spectra are more accurately represented when we assume the inner edge of the disk is located at 6 gravitational radii. This outcome implies that the conventional narrow disk model serves as a more effective approximation for the X-ray continuum emission of these entities.\n\n**Keywords**: Black holes, Spectroscopy, X-rays, Modeling, Accretion disks, Emission lines, Broad-band spectral energy distribution, Luminosity function, Mass measurement, Stellar-mass black holes, Supermassive black holes, Active galactic nuclei, Quasars, Cosmic evolution.\n\n**1 Introduction**  \nIn recent years, significant advancements have been made in understanding the physical processes occurring near supermassive black holes (SMBHs) in active galactic nuclei (AGN), quasars, and similar systems. These investigations are primarily based on observations of the broad-band spectral energy distributions (SEDs) of SMBHs across a wide frequency range. However, due to their considerable distances, direct measurements of the intrinsic luminosities of most AGNs are often unfeasible. Instead, researchers utilize indirect methods, such as reverberation mapping or statistical correlations between different AGN properties, to infer their luminosities. For example, by considering the number of light crossings in a specific region within an AGN, one can estimate its luminosity using basic geometric principles. Alternatively, if the distance to an AGN is known, its absolute magnitude can be directly calculated. Unfortunately, both approaches require a thorough understanding of the emitting regions’ composition, which is often not accessible through current observational methods. Therefore, to accurately calculate the luminosities of distant AGNs, it is crucial to develop models that can effectively reproduce the known SEDs of nearby AGNs.",
        "ori-fast-z-score": -0.9733285267845753,
        "water-fast-z-score": 5.028864055053639,
        "rewrite-fast-z-score": -1.150792911137501
    },
    {
        "original_text": "We propose an improved metric on the space of worldsheet sigma model couplings that is suitable to study gradient renormalization group flows beyond first order in perturbation theory. The new metric has several advantages over previous proposals, including manifestly positive kinetic terms and no need for additional counterterms at higher orders. We show how this metric can be used to compute beta functions up to third order in perturbation theory using only Feynman diagrams with one-loop vacuum bubbles as building blocks. This allows us to obtain results for the beta function of the dilaton coupling to the Ricci scalar which are consistent with those obtained by other methods but have not been previously available due to technical difficulties. In addition we find evidence for non-trivial fixed points in the beta function of the string coupling constant. These results provide further support for the idea that the worldsheet sigma model may serve as a useful tool for studying quantum gravity. Introduction: It was recently shown  1  that the worldsheet sigma-model (WSSM) provides a powerful framework for investigating quantum gravity via its connection to the gravitational path integral  2  . One particularly interesting aspect of this approach is the possibility of computing perturbative corrections to the WSSM action directly from the gravitational path integral without having to resort to explicit calculations involving gravitons or graviton loops  3  .\nIn  4  it was proposed that the WSSM could also be used to investigate the flow of the effective action under the renormalization group (RG). However, since the WSSM contains infinitely many degrees of freedom there does not exist any finite dimensional parameter space where the RG flow takes place. Instead, the RG flow must take place along some infinite-dimensional trajectory through the space of all possible actions. To make progress towards understanding such trajectories it would be helpful if one were able to define a sensible metric on the space of WSSM actions so that distances between different actions could be measured. Such a metric should allow one to determine whether two given actions lie close together or far apart in the space of all possible WSSMs.",
        "watermark_text": "We suggest an better metric on the space of worldsheet sigma model couplings that is suitable to study gradient renormalization group flows beyond first order in perturbation theory . The updated metric has numerous benefits over past proposals , notably manifestly strong kinetic terms and no requirement for additional counterterms at higher orders .We see how this metric can be used to compute beta functions up to third order in perturbation theory employing only Feynman diagrams with one - ring vacuum bubbles as building blocks . This enables us to obtain results for the beta function of the dilaton coupling to the Ricci scalar which are compatible with those achieved by other methods but have not been previously available owing to technical problems .In addition we find proof for non - trivial fixing points in the beta function of the string coupling constant . These conclusions provide further evidence for the idea that the worldsheet sigma approach may serve as a helpful resource for studying quantum gravitational .Introduction : It was recently shown 1 that the worldsheet sigma - model ( WSSM ) presents a powerful framework for investigating quantum gravitational via its connection to the gravitational path integral 2 . One especially interesting aspect of this methodology is the prospect of computing perturbative corrections to the WSSM action directly from the gravitational path integral without having to resort to explicit experiments concerning gravitons or graviton loops 3 .In 4 it was suggested that the WSSM could also be used to examine the flow of the effective act under the renormalization group ( RG ) . However , since the WSSM contains infinitely many degrees of liberty there does not exist any finite dimensional parameter area where the RG flow takes place .Instead , the RG flow must take place along some infinite - dimensional trajectory through the space of all possible actions . To build progress towards studying such trajectories it would be nice if one were trying to define a practical metric on the space of WSSM actions so that lengths between multiple movements could be measured .Such a metric should enable one to estimate whether two given actions sit close together or far separated in the space of all possible WSSMs .",
        "rewrite_text": "We propose an improved metric for the space of worldsheet sigma model couplings, which is suited for investigating gradient renormalization group flows beyond the first order in perturbation theory. This revised metric offers several advantages over previous proposals, particularly its clear representation of strong kinetic terms and the elimination of the need for additional counterterms at higher orders. We demonstrate how this metric facilitates the computation of beta functions up to third order in perturbation theory, relying solely on Feynman diagrams with one-ring vacuum bubbles as components. This approach allows us to derive results for the beta function of the dilaton coupling to the Ricci scalar that align with findings from other methods, which were previously unattainable due to technical difficulties. Moreover, we provide evidence for non-trivial fixed points in the beta function of the string coupling constant. These results support the notion that the worldsheet sigma model framework may be an effective tool for exploring quantum gravity. \n\nIntroduction: Recent work has shown that the worldsheet sigma model (WSSM) is a robust framework for examining quantum gravity through its association with the gravitational path integral. One particularly intriguing aspect of this framework is the potential to compute perturbative corrections to the WSSM action directly from the gravitational path integral, bypassing the need for explicit experiments involving gravitons or graviton loops. It has also been suggested that the WSSM could be employed to analyze the flow of the effective action under the renormalization group (RG). However, due to the infinite number of degrees of freedom in the WSSM, there is no finite-dimensional parameter space for the RG flow to occur; instead, the flow must traverse an infinite-dimensional trajectory in the space of all conceivable actions. To advance the study of such trajectories, it would be beneficial to define a practical metric for the space of WSSM actions, allowing for the measurement of distances between various points and helping to determine whether two specific actions are closely related or widely separated within the WSSM landscape.",
        "ori-fast-z-score": 0.7715167498104595,
        "water-fast-z-score": 8.33543833063041,
        "rewrite-fast-z-score": 0.5551361100027009
    },
    {
        "original_text": "We introduce the notion of dynamical objects associated to cohomologically expanding maps, and prove that they are equivalent to the usual ones in many cases.  We also show how these new objects can be used to study the dynamics of such maps on infinite-dimensional spaces. Let X be an infinite dimensional Banach space with norm . For each integer n ≥ 1 we define the open ball B(n) = {x ∈ X : x < n}. A map T : X → X is said to be cohomologically expanding if there exists some constant C > 0 so that for all integers m, n ≥ 1 one has  diam (T −m (B(n))) ≤ Cn. In this case it follows easily that T satisfies the following properties:\n(1)  T is continuous; \n(2)  T is surjective; \nThe main result of our work shows that under certain conditions, the existence of a dynamical object implies the existence of another one which behaves well when restricted to finite-dimensional subspaces. \nLet us now recall what a dynamical object is. Given any point x ∈ X , let O(x) denote the orbit of x; i.e., O(x) := {T k (x), k ∈ Z}. The set O(x) equipped with the metric dO defined by dO((x1, x2)) = sup{d(x1, x2), x1 ∈ O(x2), x2 ∈ O(x1)} becomes a compact metric space called the orbital space at x. If T is cohomologically expanding then every orbital space is homeomorphic to a Cantor set.",
        "watermark_text": "We introduce the notion of dynamical images attached to cohomologically extended spaces , and prove that they are analogous to the usual ones in many situations . We also demonstrate how these new objects can be used to study the dynamics of such maps on infinite - dimensional spaces .Let X be an infinite dimensional Banach space with norm . For each integer n ≥ 1 we define the open ball B ( n ) = { x ∈ X : x < n } .A mapping T : X → X is said to be cohomologically extended if there exists some variable C > 0 so that for all numbers m , n ≥ 1 one has diam ( T −m ( B ( n ) ) ) ≤ Cn . In this instance it appears easily that T satisfies the following properties : ( 1 ) T is continuous ; ( 2 ) T is surjective ; The main consequence of our work proves that under certain conditions , the existence of a dynamical object suggests the existence of another one which behaves well when confined to finite - dimensional subspaces .Let us now recall what a dynamical object is . Given any point x ∈ X , let O ( x ) define the orbit of x ; i . e . , O ( x ) : = { T k ( x ) , k ∈ Z } .The set O ( x ) equipped with the metric dO defined by dO ( ( x1 , x2 ) ) = sup { d ( x1 , x2 ) , x1 ∈ O ( x2 ) , x2 ∈ O ( x1 ) } becomes a compact metric space termed the orbital space at x . If T is cohomologically extended then every orbital space is homeomorphic to a Cantor set .",
        "rewrite_text": "We present the concept of dynamical images associated with cohomologically extended spaces and demonstrate their similarity to conventional dynamical images in various contexts. Furthermore, we illustrate how these new constructs can facilitate the analysis of dynamics in mappings within infinite-dimensional spaces. Let \\( X \\) represent an infinite-dimensional Banach space with norm \\(\\|\\cdot\\|\\). For each integer \\( n \\geq 1 \\), we define the open ball \\( B(n) = \\{ x \\in X : \\|x\\| < n \\} \\). A mapping \\( T : X \\to X \\) is considered cohomologically extended if there exists a constant \\( C > 0 \\) such that for all integers \\( m, n \\geq 1 \\), the diameter satisfies \\( \\text{diam}(T^{-m}(B(n))) \\leq Cn \\). In this case, it is straightforward to show that \\( T \\) possesses the following properties: (1) \\( T \\) is continuous; and (2) \\( T \\) is surjective. A key outcome of our work is that under certain conditions, the presence of a dynamical object implies the existence of another that behaves well when restricted to finite-dimensional subspaces. To clarify what we mean by a dynamical object, for any point \\( x \\in X \\), we define the orbit of \\( x \\) as \\( O(x) = \\{ T^k(x) : k \\in \\mathbb{Z} \\} \\). When equipped with the metric \\( dO \\) defined by \\( dO((x_1, x_2)) = \\sup\\{ d(x_1, x_2) : x_1 \\in O(x_2), x_2 \\in O(x_1) \\} \\), the set \\( O(x) \\) becomes a compact metric space known as the orbital space at \\( x \\). If \\( T \\) is cohomologically extended, every orbital space is homeomorphic to a Cantor set.",
        "ori-fast-z-score": 0.6123724356957946,
        "water-fast-z-score": 4.569057743101286,
        "rewrite-fast-z-score": 0.20851441405707477
    },
    {
        "original_text": "We present evidence for dark matter annihilation in the cosmic microwave background (CMB) haze, which is an excess emission at large angles with respect to the Galactic center that was first detected by Wilkinson Microwave Anisotropy Probe (WMAP). We use data from Planck and Fermi Large Area Telescope (LAT), as well as new measurements of the CMB temperature anisotropies made using the Atacama Cosmology Telescope (ACT).\nThe observed spectrum of this signal can be explained if it originates from dark matter particles with masses between 1 GeV and 10 TeV, annihilating into pairs of photons or leptons. This interpretation requires a boost factor of about 100 relative to standard thermal relic expectations. \n \n If confirmed, our results would provide strong support for models where dark matter self-annihilates into Standard Model particles. They also have important implications on the nature of dark matter itself, since they require either non-thermal production mechanisms or additional interactions beyond those predicted within the minimal supersymmetric extension of the Standard Model.",
        "watermark_text": "We present evidence for black material annihilation in the cosmic microwave background ( CMB ) fog , which is an excess emission at large angles with regard to the Galactic center that was first detected by Wilkinson Microwave Anisotropy Probe ( WMAP ) . We use data from Planck and Fermi Large Area Telescope ( LAT ) , as well as new studies of the CMB altitude anisotropies made using the Atacama Cosmology Telescope ( ACT ) .The observed spectrum of this signal can be understood if it originates from dark matter molecules with masses between 1 GeV and 10 TeV , annihilating into sets of photons or leptons . This interpretation needs a boost factor of about 100 compared to standard temperature relic estimates .If confirmed , our findings would offer strong evidence for models where bright particles self - annihilates into Standard Model particles . They especially have important implications on the nature of bright matter itself , since they demand either non - temperature generation pathways or additional interactions beyond those predicted within the minimal supersymmetric extension of the Standard Model .",
        "rewrite_text": "We provide evidence for the annihilation of dark matter in the cosmic microwave background (CMB) fog, characterized by an excess of emissions at large angles from the Galactic center that was initially observed by the Wilkinson Microwave Anisotropy Probe (WMAP). Our analysis incorporates data from the Planck satellite and the Fermi Large Area Telescope (LAT), along with new investigations of CMB altitude anisotropies conducted by the Atacama Cosmology Telescope (ACT). The spectrum of this signal can be explained by dark matter particles with masses ranging from 1 GeV to 10 TeV, which annihilate into groups of photons or leptons. This explanation necessitates a boost factor of approximately 100 compared to standard relic temperature estimates. If validated, our results would provide compelling evidence for scenarios in which bright particles self-annihilate into Standard Model particles. This holds particular significance for understanding the nature of dark matter, as it requires either non-thermal production mechanisms or additional interactions that extend beyond the predictions of the minimal supersymmetric extension of the Standard Model.",
        "ori-fast-z-score": 0.7276068751089989,
        "water-fast-z-score": 5.578319375835658,
        "rewrite-fast-z-score": -0.24253562503633297
    },
    {
        "original_text": "We report on the detection and characterization of microwave continuum emission from air shower plasmas using data collected by the LOPES experiment in Germany during 2004-2006. The observed signal is consistent with that expected for coherent Cherenkov radiation emitted by relativistic electrons accelerated to energies up to 100 MeV within the showers, as predicted by theory. We find no evidence for any significant contribution from incoherent synchrotron or bremsstrahlung processes. These results provide new insights into the physics of cosmic ray interactions at high energy. They also demonstrate the potential utility of radio techniques for studying atmospheric phenomena such as thunderstorms. \n \n Keywords: Cosmic rays, Radio waves, Air showers, Coherence, Synchrotron radiation \n \n \n \n 1 Introduction \n \n In recent years there has been growing interest in developing novel methods for detecting ultra-high-energy (UHE) cosmic rays based upon their interaction with Earth s atmosphere  1  . One promising technique involves measuring the radio-frequency (RF) emission produced when UHE particles interact with molecules in the upper atmosphere  2  , which can be detected remotely over large areas  3  .\n \nThe most prominent feature of this RF emission is an intense broadband pulse lasting several microseconds  4  . This pulse arises because the charged particle cascade generated by each primary cosmic ray interacts strongly with the geomagnetic field, causing it to emit coherently across a wide range of frequencies  5  . However, other mechanisms may contribute significantly to the total RF emission  6  . \n \n Here we present observations made with the Low-Frequency Array (LOFAR), one component of the International LOFAR Telescope  7  . Our analysis focuses primarily on measurements taken between 2004 and 2006 with the Long Wavelength Array (LWA)  8  , a phased array consisting of 144 dual-polarized dipole antennas operating at wavelengths ranging from 10 m to 80 m  9  . During these three years, LWA was deployed near Karthaus Township, Germany  10  , where it recorded signals from more than 20 million cosmic-ray-induced air showers  11  .",
        "watermark_text": "We report on the characterization and description of microwave continuum emission from air spray plasmas using data taken by the LOPES experiment in Germany during 2004 - 2006 . The observed light is compatible with that expected for coherent Cherenkov radiation emitted by relativistic electrons accelerated to energies up to 100 MeV within the showers , as predicted by theory .We see no evidence for any large contribution from incoherent synchrotron or bremsstrahlung interactions . These data provide fresh insights into the physics of cosmic ray interactions at high energy .They especially demonstrate the possibilities utility of radio methods for studying air events such as thunderstorms . Keywords : Cosmic rays , Radio beams , Air showers , Coherence , Synchrotron emission 1 Introduction In recent years there has been growing interest in establishing novel techniques for detecting ultra - large - energy ( UHE ) cosmic rays based upon their association with Earth s atmosphere 1 .One promising hypothesis includes monitoring the television - frequency ( RF ) emission generated when UHE ions contact with compounds in the higher atmosphere 2 , which can be identified remotely over large areas 3 . The most notable feature of this RF absorption is an intense broadband wave lasting several microseconds 4 .This wave exists because the charged particle cascade generated by each main cosmic ray interacts heavily with the geomagnetic field , forcing it to emit coherently across a broad variety of rates 5 . However , other mechanisms may contribute greatly to the total RF radiation 6 .Here we present observations made with the Low - Frequency Array ( LOFAR ) , one element of the International LOFAR Telescope 7 . Our study consists primarily on observations made between 2004 and 2006 with the Long Wavelength Array ( LWA ) 8 , a phased array consisting of 144 multiple - polarized dipole antennas active at wavelengths ranging from 10 m to 80 m 9 .During these three years , LWA was deployed near Karthaus Township , Germany 10 , where it recorded transmissions from more than 20 million cosmic - ray - caused air showers 11 .",
        "rewrite_text": "We present our findings on the characterization and analysis of microwave continuum emission from air spray plasmas, utilizing data gathered from the LOPES experiment in Germany between 2004 and 2006. The detected emissions align with the theoretical expectations for coherent Cherenkov radiation generated by relativistic electrons that are accelerated to energies reaching up to 100 MeV within cosmic ray showers. No significant contributions from incoherent synchrotron or bremsstrahlung interactions were observed. These findings enhance our understanding of cosmic ray interactions at high energies and highlight the potential of radio methods for investigating atmospheric phenomena, such as thunderstorms. \n\n**Keywords:** Cosmic rays, Radio emissions, Air showers, Coherence, Synchrotron radiation.\n\n**1. Introduction**  \nIn recent years, there has been an increasing interest in developing innovative techniques for detecting ultra-high-energy (UHE) cosmic rays through their interactions with Earth’s atmosphere. One promising approach involves monitoring the radio frequency (RF) emissions generated when UHE ions collide with atmospheric components, which can be detected over extensive areas. A key characteristic of this RF signal is the presence of an intense broadband wave that lasts several microseconds. This phenomenon occurs due to the charged particle cascade produced by each primary cosmic ray interacting significantly with the geomagnetic field, leading to coherent emissions across a wide range of frequencies. However, other mechanisms may also significantly contribute to the total RF radiation. \n\nIn this study, we present observations acquired with the Low-Frequency Array (LOFAR), a component of the International LOFAR Telescope. Our analysis is primarily based on data collected between 2004 and 2006 using the Long Wavelength Array (LWA), which comprises 144 multi-polarized dipole antennas operating at wavelengths from 10 m to 80 m. Over these three years, the LWA, located near Karthaus Township, Germany, successfully recorded transmissions from over 20 million air showers caused by cosmic rays.",
        "ori-fast-z-score": -1.4631270419005797,
        "water-fast-z-score": 8.060433501697915,
        "rewrite-fast-z-score": -0.3481553119113957
    },
    {
        "original_text": "We present the first results on the use of infrared calcium lines to determine stellar metallicities in open clusters and field stars, based on high-resolution spectra obtained with the CRIRES spectrograph at VLT/UT1 (ESO). We find that the equivalent widths of the two strongest components of the infrared Ca II triplet are strongly correlated with  Fe/H  for both cluster members and field stars.  The calibration is valid over a wide range of temperatures, including those typical of red giants. This method can be used to obtain accurate estimates of the iron abundance even when only low resolution data are available. It also provides an alternative way to estimate distances using parallaxes measured by space missions such as Hipparcos or Gaia. Keywords: Calcium, Metallicity, Red giant branch star, Open cluster, Infrared spectrum, Iron abundance, Distance determination, Space mission, High-Resolution spectroscopy",
        "watermark_text": "We present the first findings on the using of laser calcium bands to estimate stellar metallicities in open complexes and field stars , using on wide - resolution spectra obtained with the CRIRES spectrograph at VLT / UT1 ( ESO ) . We see that the equivalent widths of the two strongest elements of the infrared Ca II triplet are strongly correlated with Fe / H for both cluster groups and field stars .The calibration is valid over a broad variety of conditions , particularly those common of red giants . This method can be used to obtain precise estimates of the metal density even when only low resolution data are available .It additionally offers an additional means to estimate distances using parallaxes measured by space missions such as Hipparcos or Gaia . Keywords : Calcium , Metallicity , Red giant branch star , Open cluster , Infrared spectrum , Iron concentration , Distance finding , Space mission , High - Resolution spectroscopy",
        "rewrite_text": "We present our initial findings on the application of laser calcium bands for estimating stellar metallicities in open clusters and field stars, utilizing wide-resolution spectra obtained from the CRIRES spectrograph at the VLT/UT1 (ESO). Our results indicate a strong correlation between the equivalent widths of the two most prominent elements in the infrared Ca II triplet and Fe/H for both clusters and field stars. This calibration is applicable under a wide range of conditions, particularly those typical of red giants. This technique allows for accurate estimates of metallic density, even when only low-resolution data is available. Furthermore, it provides an additional method for estimating distances based on parallaxes measured by space missions like Hipparcos or Gaia. Keywords: Calcium, Metallicity, Red giant branch star, Open cluster, Infrared spectrum, Iron concentration, Distance measurement, Space mission, High-Resolution spectroscopy.",
        "ori-fast-z-score": -1.099524999206747,
        "water-fast-z-score": 5.0089472186085136,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "We consider the problem of energy-efficient modulation for downlink transmissions over code-division multiple-access (CDMA) networks, where each user has an individual delay constraint and is equipped with a rechargeable battery that can be charged by harvesting ambient radio-frequency signals. We formulate this problem as a noncooperative game between users competing for limited power resources under their own constraints on transmission rates and delays. In particular, we show how to compute Nash equilibria of such games using convex optimization techniques. Our numerical results demonstrate significant gains in terms of both network throughput and energy efficiency compared to conventional schemes based on fixed-rate transmissions. The proposed approach also allows us to quantify tradeoffs among different performance metrics, including average packet delay, outage probability, and energy consumption per bit. This work was supported by NSF grants CNS-0932040 and CCF-0939370. \n \n Code available at http://arxiv.org/abs/1206.5481",
        "watermark_text": "We consider the issue of electricity - efficient modulation for downlink transmissions over code - division multiple - access ( CDMA ) networks , where each consumer has an individual delay constraint and is equipped with a rechargeable battery that can be charged by collecting ambient broadcast - frequency signals . We formulate this situation as a noncooperative contest between operators competing for limited power assets under their own restrictions on transmission rates and delays .In particular , we study how to compute Nash equilibria of such players utilizing convex optimization schemes . Our numerical findings show considerable gains in terms of both network throughput and energy efficiency compared to conventional systems relying on fixed - frequency transmissions .The proposed approach also enables us to quantify tradeoffs among different performance metrics , notably mean packet delay , outage likelihood , and energy consumption per bit . This effort was supported by NSF grants CNS - 0932040 and CCF - 0939370 .Code available at http : / / arxiv . org / abs / 1206 . 5481",
        "rewrite_text": "We examine the challenge of energy-efficient modulation for downlink transmissions in code-division multiple access (CDMA) networks, where each user faces individual delay constraints and has a rechargeable battery that can be recharged by capturing ambient broadcast-frequency signals. This scenario is modeled as a noncooperative competition among operators vying for limited power resources, each subject to their own transmission rate and delay limitations. Specifically, we explore how to calculate Nash equilibria for these participants using convex optimization techniques. Our numerical results indicate significant improvements in network throughput and energy efficiency when compared to traditional systems that use fixed-frequency transmissions. Additionally, our proposed method allows us to assess trade-offs among various performance metrics, including average packet delay, outage probability, and energy consumption per bit. This research was funded by NSF grants CNS-0932040 and CCF-0939370. The code is available at http://arxiv.org/abs/1206.5481.",
        "ori-fast-z-score": -1.0392304845413263,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": -1.585187847802434
    },
    {
        "original_text": "We study the UV behavior of gauged chiral supergravities in six dimensions, focusing on their possible supersymmetric AdS 7 vacua. We show that these theories are not free at high energies but instead have an interacting fixed point with a finite number of degrees of freedom. This is achieved by using the recently developed technique of holographic renormalization to compute the beta functions of the gauge coupling constants as well as those associated with higher-derivative terms in the action. The latter can be used to stabilize the scalar potential against quantum corrections. In particular we find that there exists a large class of models which admit metastable de Sitter solutions. These results provide further evidence for the existence of stable non-supersymmetric AdS 7 vacuua in this context. Introduction: Recently it has been shown  1  that certain classes of N = 1 superconformal field theories (SCFTs) in four dimensions may be realized via compactifications of type IIA string theory on Calabi-Yau threefolds X 3 . It was also found  2  that such constructions generically lead to massive gravitons in five dimensions whose masses scale like M 2 grav ∝ V −3 , where V denotes the volume of X 3 . As a result one expects that the effective gravitational constant G 5 will run logarithmically with energy  3  .\nIn  4  it was suggested that this running could be stopped if one considers non-perturbative effects due to Euclidean D3-branes wrapping special Lagrangian cycles L ∈ H 4 (X 3 ; Z). Indeed, it turns out that the corresponding instanton contributions generate a term proportional to R ∧ R in the lowenergy effective action  5  . If this term dominates over other contributions then the resulting vacuum solution should correspond to anti-de Sitter space  6  . Moreover, since the instanton contribution scales like e −1/g s , where g s denotes the string coupling constant, one finds that the radius of curvature of the anti-de Sitter space decreases exponentially fast when approaching weak coupling  7, 8  . Thus, in order to obtain a phenomen",
        "watermark_text": "We research the UV interactions of gauged chiral supergravities in six dimensions , concentrating on their possible supersymmetric AdS 7 vacua . We see that these theories are not free at high energies but instead have an interacting fixed point with a finite number of degrees of liberty .This is achieved by using the recently advanced technique of holographic renormalization to compute the beta functions of the gauge interaction constants as well as those associated with higher - derivative terms in the action . The latter can be used to stabilize the scalar current against quantum corrections .In particular we find that there exists a large class of models which admit metastable de Sitter solutions . These data provide further evidence for the existence of stable non - supersymmetric AdS 7 vacuua in this context .Introduction : Recently it has been shown 1 that particular categories of N = 1 superconformal field theories ( SCFTs ) in four dimensions may be realized via compactifications of type IIA string theory on Calabi - Yau threefolds X 3 . It was also discovered 2 that such constructions generically lead to massive gravitons in five dimensions whose masses range like M 2 grav [UNK] V −3 , where V denotes the volume of X 3 .As a result one expects that the effective gravitational constant G 5 will go logarithmically with energy 3 . In 4 it was suggested that this run could be halted if one considers non - perturbative properties due to Euclidean D3 - branes wrapping special Lagrangian curves L ∈ H 4 ( X 3 ; Z ) .Indeed , it turns out that the associated instanton contributions create a word proportional to R ∧ R in the lowenergy efficient action 5 . If this term dominates over other contributions then the resulting vacuum problem should approximate to anti - de Sitter space 6 .Moreover , since the instanton contribution varies like e −1 / g s , where g s indicates the string coupling constant , one discovers that the radius of curvature of the anti - de Sitter space reduces exponentially rapidly when approaching weak interaction 7 , 8 . Thus , in order to obtain a phenomen",
        "rewrite_text": "We investigate the ultraviolet (UV) interactions of gauged chiral supergravities in six dimensions, focusing on their potential supersymmetric AdS₇ vacua. Our findings reveal that these theories are not free at high energies; rather, they exhibit an interacting fixed point characterized by a finite number of degrees of freedom. This analysis employs the recently developed method of holographic renormalization to calculate the beta functions for the gauge coupling constants and those linked to higher-derivative terms in the action. The latter play a crucial role in stabilizing the scalar current against quantum corrections. Notably, we identify a substantial class of models that possess metastable de Sitter solutions. This evidence strengthens the case for the existence of stable non-supersymmetric AdS₇ vacua within this framework. \n\nIntroduction: Recent research has demonstrated that specific categories of N = 1 superconformal field theories (SCFTs) in four dimensions can be realized through compactifications of type IIA string theory on Calabi-Yau threefolds X₃. It was also uncovered that these constructions typically result in massive gravitons in five dimensions, with their masses behaving like M²₍₉₋₊₋₃₋₋ₖ₌₁₋₇₋₏₌₉₏₂₋₁₎, where V represents the volume of X₃. Consequently, one anticipates that the effective gravitational constant G₅ varies logarithmically with energy. Moreover, it has been suggested that this running could be mitigated by considering non-perturbative effects arising from Euclidean D3-branes wrapping special Lagrangian curves L ∈ H₄(X₃; Z). Indeed, it turns out that the associated instanton contributions yield a term proportional to R ∧ R in the low-energy effective action. If this term dominates over other contributions, the resulting vacuum structure should closely resemble anti-de Sitter space. Additionally, since the instanton contribution decreases like e^(-1/gₛ), where gₛ represents the string coupling constant, we find that the radius of curvature of the anti-de Sitter space diminishes exponentially as one approaches weak coupling. Consequently, to achieve a phenomenological understanding, it's essential to analyze these dynamics further.",
        "ori-fast-z-score": -0.26413527189768715,
        "water-fast-z-score": 5.396407334626633,
        "rewrite-fast-z-score": 0.7863336509949341
    },
    {
        "original_text": "We report the discovery of an unusual object in the outer solar system that is likely to be composed primarily of ice particles. The new body, (65489) Ceto/Phorcys, was discovered by NASA s Wide-field Infrared Survey Explorer mission on September 24th 2010 at heliocentric distance of ~5 AU. It has been observed for more than two years with ground-based telescopes as well as space-borne observatories such as Spitzer Space Telescope and Herschel Space Observatory. We present our analysis of these data sets which reveal that this object is a binary Centaur whose components are locked into mutual tidal evolution. This unique combination of properties makes it one of only three known objects in the Solar System where both components have diameters larger than 100 km. Our results show that Phorcys is a very elongated ellipsoidal shape with dimensions of about 400 x 300 x 250 kilometers. Its surface temperature is estimated to be around -100 degrees Celsius.",
        "watermark_text": "We report the discovery of an unique body in the outer solar system that is probably to be composed primarily of ice particles . The newest body , ( 65489 ) Ceto / Phorcys , was discovered by NASA s Wide - field Infrared Survey Explorer mission on September 24th 2010 at heliocentric height of ~ 5 AU .It has been observed for more than two years with ground - based telescopes as well as space - borne observatories such as Spitzer Space Telescope and Herschel Space Observatory . We present our analysis of these information sets which confirm that this body is a binary Centaur whose components are locked into mutual tidal evolution .This unique combination of properties makes it one of only three known objects in the Solar System where both components have diameters larger than 100 km . Our results show that Phorcys is a very elongated ellipsoidal structure with sizes of about 400 x 300 x 250 meter .Its surface temperature is predicted to be around - 100 degrees Celsius .",
        "rewrite_text": "We announce the discovery of a unique celestial body in the outer solar system, likely composed predominantly of ice particles. The latest addition, (65489) Ceto/Phorcys, was identified by NASA's Wide-field Infrared Survey Explorer mission on September 24, 2010, at a heliocentric distance of approximately 5 AU. It has been observed for over two years using both ground-based telescopes and space-based observatories, including the Spitzer Space Telescope and the Herschel Space Observatory. In this report, we present our analysis of these datasets, which confirm that this body is a binary Centaur with its components engaged in mutual tidal evolution. This distinctive characteristic makes it one of only three known objects in the Solar System where both components exceed 100 km in diameter. Our findings indicate that Phorcys is an elongated ellipsoidal structure measuring roughly 400 x 300 x 250 meters, with a predicted surface temperature of around -100 degrees Celsius.",
        "ori-fast-z-score": 1.8382900600361156,
        "water-fast-z-score": 5.252257314388902,
        "rewrite-fast-z-score": 2.4327007187250236
    },
    {
        "original_text": "We report on our analysis of the Suzaku observation of the z = 1.55 quasar, RBS 315 (RA=00h45m53.6s; DEC=-36d19 59.6 ). The source is detected with an average 2-10 keV flux of 4 x 10^-13 erg cm-2 s-1 corresponding to a luminosity of 3 x 10^44 erg s-1 at this redshift. We find that the spectrum can be well fitted by a power law model modified by Galactic absorption plus reflection component using pexrav model in XSPEC. This gives photon index Γ=1.9 +0.2 -0.1 , reflection fraction f=0.7 +1.0 -1.3 . The observed 0.5-7 keV band luminosity is found to be 5x10^43 erg/sec which corresponds to Eddington ratio L/L edd =0.01-0.03 assuming black hole mass M BH ~10 9 M sun .",
        "watermark_text": "We report on our analysis of the Suzaku observation of the z = 1 . 55 quasar , RBS 315 ( RA = 00h45m53 . 6s ; DEC = - 36d19 59 . 6 ) . The source is detected with an estimated 2 - 10 keV flux of 4 x 10 ^ - 13 erg centimetres - 2 s - 1 resulting to a luminosity of 3 x 10 ^ 44 erg s - 1 at this redshift .We see that the spectrum can be well fitted by a power law theory derived by Galactic absorption plus reflection factor used pexrav system in XSPEC . This gives photon index Γ = 1 . 9 + 0 . 2 - 0 . 1 , absorption proportion f = 0 . 7 + 1 . 0 - 1 . 3 .The observed 0 . 5 - 7 keV band luminosity is found to be 5x10 ^ 43 erg / sec which equals to Eddington ratio L / L edd = 0 . 01 - 0 . 03 assuming black hole weight M BH ~ 10 9 M sun .",
        "rewrite_text": "We present our analysis of the Suzaku observation of the quasar RBS 315, located at a redshift of z = 1.55 (RA = 00h45m53.6s; DEC = -36d19m59.6s). The source is detected with an estimated 2-10 keV flux of 4 x 10^-13 erg cm^-2 s^-1, corresponding to a luminosity of 3 x 10^44 erg s^-1 at this redshift. Our findings indicate that the spectrum is well explained by a power-law model, taking into account Galactic absorption and the reflection factor derived from the pexrav model in XSPEC. This yields a photon index of Γ = 1.9 +0.2 -0.1 and an absorption fraction of f = 0.7 +1.0 -1.3. The observed luminosity in the 0.5-7 keV band is measured to be 5 x 10^43 erg/s, translating to an Eddington ratio of L / Ledd = 0.01 - 0.03, assuming a black hole mass of MBH ~ 10^9 M☉.",
        "ori-fast-z-score": -0.31622776601683794,
        "water-fast-z-score": 2.846049894151541,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "In this work, we propose an adaptive service provisioning scheme to provide quality-of-service (QoS) guarantees and maximize the network utility by jointly optimizing resource allocation at both base stations (BSs) and mobile users (MUs). The proposed scheme is based on a communication model that incorporates user mobility into the QoS requirements. We formulate the problem as a joint optimization over BSs  power control variables, MUs  transmission rates, and their association with BSs. To solve it efficiently, we first decompose the original problem into two subproblems: one for each BS and another for all MUs. Then, we develop distributed algorithms to obtain solutions to these problems iteratively using dual decomposition techniques. Finally, simulation results show that our proposed algorithm can achieve better performance than existing schemes under various system settings. In recent years, wireless networks have been widely deployed around the world due to their low cost and easy deployment  1  . However, they are vulnerable to security attacks such as eavesdropping  2  , jamming  3  , and data tampering  4  .\nTo enhance the security level of wireless communications, physical layer security has attracted much attention recently  5  -  8  . Physical layer security exploits the characteristics of the wireless channel to ensure secure transmissions without relying on any additional cryptographic keys or protocols  9  . It was shown in  10  that if the legitimate transmitter-receiver pair shares no common information about the statistical properties of the channels between them and other potential eavesdroppers, then perfect secrecy cannot be achieved even when there exists infinite number of antennas at the transmitter side. Therefore, practical approaches should consider imperfections in the estimation process  11  , limited transmit power  12  , and finite antenna numbers  13  .",
        "watermark_text": "In this project , we propose an adaptive system provisioning scheme to provide quality - of - service ( QoS ) guarantees and maximize the service utility by jointly optimizing resource allocation at both base places ( BSs ) and portable consumers ( MUs ) . The proposed system is based on a communication plan that incorporates user connectivity into the QoS specifications .We formulate the question as a joint optimization over BSs power control factors , MUs transmission rates , and their association with BSs . To solve it easily , we first decompose the original problem into two subproblems : one for each BS and another for all MUs .Then , we develop dispersed schemes to obtain answers to these problems iteratively utilizing double decomposition techniques . Finally , simulation data demonstrate that our proposed algorithm can attain better performance than existing strategies under various system situations .In recent years , wireless networks have been widely deployed around the world thanks to their low cost and easy deployment 1 . However , they are susceptible to security attacks such as eavesdropping 2 , jamming 3 , and information tampering 4 .To increase the protection quality of mobile communications , physical layer security has garnered considerable scrutiny recently 5 - 8 . Physical layer security exploits the attributes of the wireless network to ensure safe communication without relying on any additional cryptographic codes or technologies 9 .It was shown in 10 that if the legitimate transmitter - receiver pair shares no shared information about the statistical characteristics of the channels between them and other potential eavesdroppers , then perfect secrecy fails be obtained even when there exists infinite quantity of antennas at the receiver side . Therefore , practical techniques should consider imperfections in the estimation method 11 , limited transmit energy 12 , and finite antenna numbers 13 .",
        "rewrite_text": "In this project, we introduce an adaptive system provisioning scheme aimed at ensuring quality-of-service (QoS) guarantees while maximizing service utility by optimizing resource allocation for both base stations (BSs) and mobile users (MUs). Our proposed system employs a communication strategy that integrates user connectivity into the QoS criteria. We frame the problem as a joint optimization involving power control for BSs, transmission rates for MUs, and their connections to BSs. To facilitate a manageable solution, we decompose the main problem into two subproblems: one dedicated to each BS and the other encompassing all MUs. We then create decentralized schemes to iteratively address these subproblems using dual decomposition techniques. Simulation results reveal that our proposed algorithm outperforms existing methods across a range of system scenarios. Recently, the widespread deployment of wireless networks has been facilitated by their low cost and ease of installation. However, these networks are vulnerable to security threats such as eavesdropping, jamming, and data tampering. To enhance the security of mobile communications, there has been significant interest in physical layer security. This approach leverages the characteristics of wireless networks to facilitate secure communication without the need for additional cryptographic mechanisms. Research has indicated that if a legitimate transmitter-receiver pair lacks shared knowledge about the statistical properties of their channels relative to potential eavesdroppers, achieving perfect secrecy can be impossible, even with an infinite number of antennas at the receiver. Therefore, practical solutions must account for estimation imperfections, limited transmission power, and finite numbers of antennas.",
        "ori-fast-z-score": 0.5895063447465633,
        "water-fast-z-score": 7.7754191435023525,
        "rewrite-fast-z-score": 1.172170525067662
    },
    {
        "original_text": "SDSS J121811+465501 is an extremely faint galaxy discovered in the Sloan Digital Sky Survey (SDSS). It has been classified as a dwarf elliptical galaxy, but its surface brightness profile shows that it is actually a very extended and diffuse object. We have obtained deep optical spectroscopy for this galaxy using the Gemini Multi-Object Spectrograph on Gemini North telescope to study its physical properties. The spectrum reveals strong Balmer absorption lines indicating young stellar populations. Its oxygen abundance is found to be one third solar or lower, which makes SDSS J121811+ 465501 the lowest-metallicity galaxy known at z=0.1. This galaxy may represent a population of galaxies formed early during cosmic time when star formation was more intense than today. Keywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness",
        "watermark_text": "SDSS J121811 + 465501 is an incredibly faint galaxy discovered in the Sloan Digital Sky Survey ( SDSS ) . It has been classified as a dwarf elliptical galaxy , but its surface brightness profile reveals that it is actually a very extended and diffuse object .We have achieved dark optical spectroscopy for this galaxy using the Gemini Multi - Object Spectrograph on Gemini North telescope to study its physical properties . The spectrum reveals large Balmer spectral lines suggesting young stellar regions .Its oxygen abundance is found to be one third solar or smaller , which makes SDSS J121811 + 465501 the smallest - metallicity galaxy known at z = 0 . 1 . This galaxy might represent a population of galaxies formed early during cosmic time when galaxy formation was more intense than tomorrow .Keywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness",
        "rewrite_text": "SDSS J121811 + 465501 is an incredibly faint galaxy identified in the Sloan Digital Sky Survey (SDSS). Although it has been classified as a dwarf elliptical galaxy, its surface brightness profile indicates that it is actually a highly extended and diffuse object. We have performed dark optical spectroscopy on this galaxy using the Gemini Multi-Object Spectrograph with the Gemini North telescope to explore its physical characteristics. The spectrum displays prominent Balmer spectral lines, suggesting the presence of young stellar regions. Its oxygen abundance is found to be one-third solar or less, making SDSS J121811 + 465501 the galaxy with the lowest metallicity known at z = 0.1. This galaxy may represent a class of galaxies that formed during the early stages of cosmic evolution, a time when galaxy formation was more active than it is today.  \nKeywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 4.810702354423639,
        "rewrite-fast-z-score": 1.5491933384829668
    },
    {
        "original_text": "We consider the problem of linear regression with measurement error, where the errors are assumed to be normally distributed and uncorrelated between different measurements but correlated within each measurement set (i.e., for one given star). We derive an expression for the covariance matrix of the estimated parameters which is valid even if the number of data points per measurement set differs significantly among sets. This allows us to study how the accuracy of the results depends on the distribution of the number of observations over all stars. The derived formula can also be used as a tool for optimizing observational strategies by minimizing the uncertainty of the resulting estimates. In particular we show that it may be advantageous to use only a small fraction of available data points when estimating the coefficients of the model function while using most or all data points for determining its first few derivatives. Finally, we apply our method to simulated data obtained from the Hipparcos astrometric satellite mission.",
        "watermark_text": "We consider the question of linear regression with measurement loss , where the errors are expected to be usually spread and uncorrelated between various measurements but correlated within each measurement set ( i . e . , for one given star ) . We derive an expression for the covariance matrix of the expected variables which is valid even if the proportion of data points per observation set differs greatly among setting .This enables us to study how the accuracy of the results varies on the distribution of the proportion of measurements over all stars . The derived formula can also be used as a tool for optimizing observational strategies by minimizing the uncertainty of the resulting estimates .In particular we prove that it could be advantageous to use only a small fraction of available data points when estimating the coefficients of the model function while using most or all information points for determining its initial few derivatives . Finally , we apply our technique to simulated images obtained from the Hipparcos astrometric satellite mission .",
        "rewrite_text": "We explore the issue of linear regression in the presence of measurement loss, where errors are generally assumed to be normally distributed and uncorrelated across different measurements, but correlated within the same measurement set (for example, for a specific star). We derive a formula for the covariance matrix of the expected variables, which remains valid even when the number of data points per observation set varies significantly across different settings. This allows us to examine how the distribution of measurement proportions across all stars affects the accuracy of the results. Additionally, our derived formula serves as a tool for optimizing observational strategies by minimizing the uncertainty of the resulting estimates. Notably, we demonstrate that it can be beneficial to utilize only a small subset of available data points for estimating the model coefficients, while employing a larger proportion of all available data to calculate the initial few derivatives. Lastly, we apply our methodology to simulated images from the Hipparcos astrometric satellite mission.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": 0.6793662204867574
    },
    {
        "original_text": "We present an analytical model for the growth, radial drift and fragmentation of dust grains in protoplanetary disks that evolve under the combined effects of viscous accretion onto the central star and photoevaporation by external radiation fields. We show how these processes affect the evolution of grain size distributions as well as their spatial distribution within the disk. In particular we find that: (i) The maximum grain sizes are limited to values between 1 mm and 10 cm depending on the strength of the stellar UV field. (ii) Grains grow faster at larger distances from the star due to lower gas densities and higher temperatures. (iii) Fragmentation is more efficient closer to the star where the local pressure maxima lead to enhanced collisional velocities.  These results have important implications for planet formation scenarios since they suggest that planetesimals can form only close to the star while large bodies such as asteroids or comets may be able to form farther out in the disk.",
        "watermark_text": "We present an analytical theory for the development , radial drift and fragmentation of dust grains in protoplanetary disks that develop under the combined influences of viscous accretion onto the main star and photoevaporation by external emission fields . We see how these mechanisms affect the evolution of grain length distributions as well as their temporal distribution within the disk .In particular we find that : ( i ) The maximum grain sizes are limited to values between 1 mm and 10 mm depending on the strength of the stellar UV field . ( ii ) Grains grow better at larger distances from the star due to higher gas densities and larger temperatures .( iii ) Fragmentation is more efficient closer to the star where the local pressure maxima lead to greater collisional velocities . These conclusions have important implications for planet development schemes since they show that planetesimals can form only close to the star while huge bodies such as asteroids or comets might be possible to form farther out in the disk .",
        "rewrite_text": "We introduce a theoretical framework for understanding the growth, radial movement, and fragmentation of dust grains in protoplanetary disks, influenced by both viscous accretion processes toward the central star and photoevaporation from external radiation sources. Our analysis reveals how these factors shape the evolution of grain size distributions and their temporal arrangement within the disk. Notably, we find that: (i) the maximum sizes of grains are constrained to a range of 1 mm to 10 mm, contingent on the intensity of the stellar UV radiation; (ii) grain growth is more pronounced at greater distances from the star due to elevated gas densities and temperatures; and (iii) fragmentation occurs more readily near the star, as local pressure peaks lead to increased collision velocities. These findings have significant implications for planet formation theories, suggesting that planetesimals can only form in proximity to the star, while larger bodies, such as asteroids or comets, may develop further out in the disk.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 4.50333209967908,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results of our analysis on blazars detected by both the Wilkinson Microwave Anisotropy Probe (WMAP) satellite and the Swift observatory in the first year of operation, 2004-05. We find that there are no significant differences between the two samples when we compare their distributions for redshift, luminosity distance, radio flux density at 1 GHz, optical magnitude, or X-ray photon index. The only difference is found to be in the distribution of redshifts; this may be due to selection effects caused by the different energy bands used by each instrument. \n \n Keywords: Blazar, Swift, WMAP, survey, cosmology, statistics, gamma-ray bursts, galaxy clusters, dark matter, dark energy, neutrino mass, cosmic microwave background radiation, anisotropies, large-scale structure, gravitational lensing, relativistic jets, quasar, active galactic nuclei",
        "watermark_text": "We present the conclusion of our analysis on blazars detected by both the Wilkinson Microwave Anisotropy Probe ( WMAP ) satellite and the Swift telescope in the first year of operation , 2004 - 05 . We see that there are no considerable changes between the two specimens when we compare their distributions for redshift , luminosity distance , radio flux concentration at 1 GHz , optical magnitude , or X - ray photon index .The only difference is found to be in the distribution of redshifts ; this might be due to choice influences resulting by the different energy bands used by each instrument . Keywords : Blazar , Swift , WMAP , survey , cosmology , statistics , gamma - ray clusters , galaxy rings , soft material , soft energy , neutrino mass , cosmic microwave background radiation , anisotropies , large - scale structure , gravity lensing , relativistic jets , quasar , active galactic nuclei",
        "rewrite_text": "We present the findings from our analysis of blazars identified by both the Wilkinson Microwave Anisotropy Probe (WMAP) satellite and the Swift telescope during their initial year of operation, 2004-2005. Our comparison of various parameters—including redshift, luminosity distance, radio flux at 1 GHz, optical magnitude, and X-ray photon index—reveals no significant differences between the two datasets. The sole notable distinction lies in the distribution of redshifts, which may be influenced by the different energy bands utilized by each instrument. \n\nKeywords: Blazar, Swift, WMAP, survey, cosmology, statistics, gamma-ray clusters, galaxy rings, soft material, soft energy, neutrino mass, cosmic microwave background radiation, anisotropies, large-scale structure, gravitational lensing, relativistic jets, quasar, active galactic nuclei.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 3.3113308926626095,
        "rewrite-fast-z-score": -0.39056673294247163
    },
    {
        "original_text": "We present the mid-infrared (MIR) spectrum of the central region in the Virgo galaxy cluster, obtained with Spitzer/IRS at high spatial resolution. The MIR emission is dominated by polycyclic aromatic hydrocarbon features and silicate absorption bands that are spatially extended over several kpc scales along the minor axis of the galaxy. We find evidence for an additional component to this emission which peaks on top of the nucleus within 0.5 arcsec (0.1 pc). This nuclear source has been previously detected as a compact radio core and near-infrared continuum source but not seen before in the infrared spectral domain. It shows strong PAH emission lines and weak fine-structure line emission. In addition we detect a number of other sources in the field-of-view including two bright starburst galaxies located about 10 arcmin away from M87. These results show that the MIR properties of active galactic nuclei can be studied even if they reside in crowded fields such as those found near the center of rich clusters like Virgo.",
        "watermark_text": "We present the mid - infrared ( MIR ) spectrum of the central region in the Virgo star cluster , obtained with Spitzer / IRS at high spatial resolution . The MIR emission is dominated by polycyclic aromatic hydrocarbon characteristics and silicate emission bands that are spatially extended over several kpc scales along the minor axis of the constellation .We get confirmation for an additional element to this emission which peaks on top of the nucleus within 0 . 5 arcsec ( 0 . 1 pc ) . This nuclear source has been previously observed as a compact radio core and near - infrared continuum source but not seen before in the infrared spectral domain .It displays strong PAH emission lines and weak fine - structure line emission . In addition we locate a number of other sources in the field - of - view including two bright starburst objects located about 10 arcmin away from M87 .These data demonstrate that the MIR properties of active galactic nuclei can be examined even if they live in busy fields such as those contained near the center of rich clusters like Virgo .",
        "rewrite_text": "We present the mid-infrared (MIR) spectrum of the central region of the Virgo star cluster, captured with high spatial resolution using the Spitzer/IRS. The MIR emission is primarily characterized by polycyclic aromatic hydrocarbons and silicate emission bands, which extend across several kiloparsecs along the minor axis of the cluster. Additionally, we identify a distinct component of this emission that peaks within 0.5 arcseconds (approximately 0.1 parsec) of the nucleus. This nuclear source has previously been detected as a compact radio core and a near-infrared continuum source, but this is its first observation in the infrared spectral range. It exhibits strong PAH emission lines and faint fine-structure line emissions. Furthermore, we identify several other sources within the field of view, including two prominent starburst galaxies located around 10 arcminutes from M87. These findings illustrate that the MIR characteristics of active galactic nuclei can be investigated even in densely populated fields such as those found near the centers of rich clusters like Virgo.",
        "ori-fast-z-score": 1.1322770341445956,
        "water-fast-z-score": 5.20847435706514,
        "rewrite-fast-z-score": 1.585187847802434
    },
    {
        "original_text": "Image registration is an important problem in medical imaging and computer vision, where the goal is to find a transformation that aligns two or more images taken at different times and/or by different sensors. In this work we present evolutionary optimisation methods for template based image registration problems. We consider both rigid and non-rigid transformations between images. The proposed algorithms are tested on synthetic data as well as real world datasets including brain MRI scans and CT angiography (CTA) volumes. Our results show that our approach outperforms state-of-the-art techniques in terms of accuracy while being computationally efficient. This research was supported by EPSRC grant EP/N014560/1. Keywords: Evolutionary Computation, Registration, Non-Rigid Transformation, Rigid Transformation, Brain Imaging, Computer Vision. 1 Introduction Image registration is one of the most fundamental tasks in many areas such as medical imaging  1  , remote sensing  2  , video processing  3  , etc., which aims to find a spatial transformation T that maps each point x ∈ Ω1 =  0, 1 d into its corresponding location y = Tx ∈ Ω2 =  0, 1 d in another image I(y). Here d denotes the dimension of the space. For example, if T1 and T2 denote two consecutive time points in a dynamic sequence of images then finding the optimal transformation T would allow us to track the movement of objects over time  4  . Similarly, if S1 and S2 represent two views of the same scene captured using cameras with slightly differing orientations then registering these images will help us fuse information across multiple viewpoints  5  .\nIn recent years there has been significant interest in developing fast and accurate registration algorithms  6  -  8  . However, despite considerable progress made towards solving this challenging problem  9  -  11  , it remains unsolved due to several factors including large number of degrees of freedom involved  12  , presence of noise  13  , partial occlusions  14  , lack of feature correspondence  15  , etc..",
        "watermark_text": "Image registration is an important challenge in medical imaging and computer vision , where the objective is to find a transformation that aligns two or more images took at different times and / or by various sensors . In this research we present evolutionary optimisation methods for template based image registration problems .We consider both stiff and non - flexible transformations between images . The proposed methods are tested on synthetic information as well as real life datasets including brain MRI scans and CT angiography ( CTA ) volumes .Our results show that our approach outperforms state - of - the - art methods in terms of precision while being computationally effective . This research was supported by EPSRC award EP / N014560 / 1 .Keywords : Evolutionary Computation , Registration , Non - Rigid Transformation , Rigid Transformation , Brain Imaging , Computer Vision . 1 Introduction Image registration is one of the most important responsibilities in different areas such as hospital photography 1 , remote sensing 2 , television recording 3 , etc . , which aims to find a spatial mapping T that mapped each point x ∈ Ω1 = 0 , 1 d into its corresponding location y = Tx ∈ Ω2 = 0 , 1 d in another image I ( y ) .Here d indicates the dimension of the space . For instance , if T1 and T2 denote two consecutive time points in a dynamic sequence of pictures then finding the ideal conversion T would enable us to track the movement of items over time 4 .Similarly , if S1 and S2 represent two perspectives of the same scene captured using cameras with slightly differing orientations then registering these images will assist us fuse information across multiple viewpoints 5 . In recent years there has been significant interest in building fast and precise registered methods 6 - 8 .However , despite considerable progress made towards solving this dangerous problem 9 - 11 , it remains unsolved due to several considerations including huge amount of degrees of freedom required 12 , presence of noise 13 , partial occlusions 14 , absence of feature relations 15 , etc . .",
        "rewrite_text": "Image registration poses a significant challenge in the fields of medical imaging and computer vision, focusing on determining a transformation that aligns two or more images captured at different times or by various sensors. In this study, we introduce evolutionary optimization techniques designed for template-based image registration tasks. We examine both rigid and non-rigid transformations between images. Our proposed methods are tested on synthetic datasets as well as real-world data, including brain MRI scans and CT angiography (CTA) volumes. Our findings indicate that our approach surpasses state-of-the-art methods concerning accuracy while maintaining computational efficiency. This research is funded by the EPSRC under award EP/N014560/1. \n\nKeywords: Evolutionary Computation, Registration, Non-Rigid Transformation, Rigid Transformation, Brain Imaging, Computer Vision. \n\n1. Introduction  \nImage registration is a critical function across various domains, such as medical imaging, remote sensing, and television broadcasting, striving to establish a spatial mapping \\( T \\) that aligns each point \\( x \\in \\Omega_1 = [0, 1]^d \\) with its corresponding location \\( y = Tx \\in \\Omega_2 = [0, 1]^d \\) in another image \\( I(y) \\). Here, \\( d \\) represents the dimensionality of the space. For instance, if \\( T_1 \\) and \\( T_2 \\) correspond to two adjacent time points within a dynamic image sequence, identifying the optimal transformation \\( T \\) will enable the tracking of objects’ movements over time. Similarly, if \\( S_1 \\) and \\( S_2 \\) denote two views of the same scene captured with cameras at slightly different angles, registering these images will facilitate the integration of information from multiple perspectives. In recent years, there has been growing interest in developing fast and accurate registration methods. Nonetheless, despite substantial advancements in addressing this challenging problem, it remains unresolved due to factors such as the high number of degrees of freedom required, the presence of noise, partial occlusions, and lacking feature correspondences.",
        "ori-fast-z-score": 1.3620104492139977,
        "water-fast-z-score": 8.00044325013193,
        "rewrite-fast-z-score": 0.9263671131731709
    },
    {
        "original_text": "We study the dynamics of open quantum systems in one dimension, focusing on their ability to generate entanglement between distant sites and how this is affected by decoherence.  We consider two different models for the system-environment interaction: (i) an environment that couples locally with each site; and (ii) an environment that couples globally with all sites simultaneously. In both cases we find that there are regimes where the system can be driven into highly entangled states even when it starts out unentangled or only weakly entangled. This occurs because the environment acts as a source of noise which drives the system towards its ground state. The effect of local coupling is more pronounced than global coupling since the former allows for faster relaxation timescales. Finally, we show that these results hold true also if the initial state has some degree of spatial correlations. Quantum information processing requires the manipulation of quantum states over large distances. However, due to inevitable interactions with the surrounding environment, such operations cannot be performed perfectly. Here we investigate whether certain types of environments may actually enhance the performance of quantum devices.",
        "watermark_text": "We research the dynamics of open quantum systems in one dimension , concentrating on their potential to create entanglement between distant objects and how this is affected by decoherence . We consider two different models for the system - landscape interaction : ( i ) an environment that pairs locally with each site ; and ( ii ) an environment that pairs internationally with all locations simultaneously .In both cases we find that there are regimes where the system can be pushed into extremely entangled states especially when it comes out unentangled or only lightly entangled . This occurs because the surroundings serves as a source of noise which drives the system towards its ground state .The impact of local coupling is more pronounced than worldwide coupling since the former provides for quicker recovery timescales . Finally , we prove that these results hold true also if the first state has some degree of spatial correlations .Quantum knowledge processing requires the processing of quantum states over large distances . However , owing to inherent interactions with the nearby surroundings , such operations unable be performed properly .Here we investigate whether particular kinds of conditions might actually increase the performance of quantum devices .",
        "rewrite_text": "We investigate the dynamics of open quantum systems in one dimension, focusing on their ability to generate entanglement between distant objects and the influence of decoherence on this process. We examine two distinct models for the system: (i) an environment that couples locally to each site, and (ii) an environment that couples globally to all locations at once. In both scenarios, we discover regions where the system can achieve highly entangled states, particularly starting from a point of low or no entanglement. This phenomenon occurs because the environment acts as a noise source that drives the system toward its ground state. The effects of local coupling are found to be more significant than those of global coupling, as the former allows for faster recovery timescales. Furthermore, we demonstrate that our findings remain valid even when the initial state possesses some degree of spatial correlations. Quantum information processing necessitates the manipulation of quantum states over extended distances; however, due to intrinsic interactions with the local environment, such processes often face challenges. In this work, we explore whether certain conditions can enhance the performance of quantum devices.",
        "ori-fast-z-score": -0.5129891760425771,
        "water-fast-z-score": 6.807380225308036,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new high angular resolution observations of the massive protostellar system, Orion Source I (OSI), obtained with ALMA in Band 7 and 9 at an average spatial resolution of 0.3 arcsec. We detect emission lines of SiO(5-4) and HCO+(4-3). The observed line profiles are consistent with those expected for Keplerian rotation around a central object of mass ~10 Msun. Using these results we derive physical parameters such as disk inclination angle, radius, temperature, density structure etc., which can be used to test theoretical models of circumstellar disks. In addition, we find that the kinematics of the innermost region probed by our data is dominated by infall motions rather than outflowing gas. This suggests that OSI may have recently undergone rapid accretion onto its central star. Finally, we also report detection of two compact continuum sources within the primary beam of the telescope.",
        "watermark_text": "We report new high angular resolution measurements of the huge protostellar body , Orion Source I ( OSI ) , obtained with ALMA in Band 7 and 9 at an estimated spatial resolution of 0 . 3 arcsec . We detect emission lines of SiO ( 5 - 4 ) and HCO + ( 4 - 3 ) .The observed line profiles are compatible with those expected for Keplerian rotation around a central object of mass ~ 10 Msun . Using these results we derive physical factors such as disk inclination angle , diameter , temperature , density structure etc . , which can be used to test theoretical theories of circumstellar disks .In addition , we find that the kinematics of the innermost region probed by our information is dominated by infall dynamics rather than outflowing gas . This implies that OSI may have subsequently undergone sudden accretion onto its primary star .Finally , we also report discovery of two compact continuum sources within the primary beam of the observatory .",
        "rewrite_text": "We present new high-angular-resolution measurements of the massive protostellar object Orion Source I (OSI), acquired using ALMA in Bands 7 and 9, achieving an estimated spatial resolution of 0.3 arcseconds. Emission lines of SiO (5-4) and HCO+ (4-3) have been detected, with the observed line profiles consistent with expectations for Keplerian rotation around a central mass of approximately 10 M☉. From these findings, we derive several physical parameters, including the disk's inclination angle, diameter, temperature, and density structure, which can be utilized to evaluate theoretical models of circumstellar disks. Furthermore, our observations indicate that the kinematics in the innermost region are primarily influenced by infall dynamics rather than outflowing gas, suggesting that OSI may have experienced a significant accretion event onto its central star. Lastly, we also report the discovery of two compact continuum sources located within the primary beam of the observatory.",
        "ori-fast-z-score": 1.116312611302876,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": 1.697749375254331
    },
    {
        "original_text": "We present the equation of state (EoS) for isospinasymmetric nuclear matter within the framework of relativistic mean field theory, including both nucleons and hyperons as well as their interactions via meson exchange. We consider two different parameterizations of the EoS: NL3* and TM1. The former one includes non-linear terms in the scalar self-interaction potential while the latter has been fitted to reproduce properties of finite nuclei. In addition we also study the effect on the EoS when using the so-called  chiral limit  instead of the standard values for the coupling constants between baryons and mesons. This work is motivated by recent experimental results obtained at GSI Darmstadt which suggest that the symmetry energy may be much softer than previously thought. It will allow us to make predictions about the composition of neutron stars and its dependence on the density profile inside these objects. Finally, we compare our results with those obtained recently by other authors.",
        "watermark_text": "We present the equation of state ( EoS ) for isospinasymmetric nuclear material within the framework of relativistic mean field theory , comprising both nucleons and hyperons as well as their interactions via meson exchange . We consider two different parameterizations of the EoS : NL3 * and TM1 .The first one includes non - linear terms in the scalar self - interaction potential while the former has been fitted to reproduce properties of finite nuclei . In addition we also study the impact on the EoS when using the so - called chiral limitation rather of the standard expressions for the interaction constants between baryons and mesons .This research is prompted by recent experimental studies obtained at GSI Darmstadt which propose that the symmetry power could be much softer than previously thought . It will provide us to make predictions about the composition of neutron galaxies and its dependence on the density profile inside these objects .Finally , we compare our findings with those achieved lately by other published .",
        "rewrite_text": "We present the equation of state (EoS) for isospin-asymmetric nuclear matter using the framework of relativistic mean field theory, which incorporates both nucleons and hyperons along with their meson-exchange interactions. We examine two distinct parameterizations of the EoS: NL3* and TM1. The NL3* parameterization includes nonlinear terms in the scalar self-interaction potential, while TM1 has been calibrated to accurately reflect the properties of finite nuclei. Additionally, we investigate the effects on the EoS when employing the so-called chiral limit instead of the conventional formulations for the interaction constants between baryons and mesons. This research is motivated by recent experimental findings from GSI Darmstadt, which suggest that the symmetry energy may be significantly softer than previously anticipated. Our study aims to make predictions concerning the composition of neutron stars and how it varies with the density profile within these objects. Finally, we will compare our results with those from other recent publications.",
        "ori-fast-z-score": 0.12216944435630522,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 0.254000254000381
    },
    {
        "original_text": "We present an analysis of the stability of planetary systems in which protoplanetary embryos evolve under oligarchy, i.e., they are able to eject each other s neighbors by gravitational scattering but not themselves. We find that this process leads to rapid growth of the largest embryo until it reaches its isolation mass (the minimum mass required for runaway accretion). The system then evolves into either a single planet or two planets with comparable masses depending on how close the initial conditions were to instability. This evolution is very different than what happens when all bodies grow simultaneously; in particular, we show that there can be multiple stable outcomes even if the initial conditions are identical. Our results suggest that the formation of terrestrial planets may have proceeded through several stages including oligarchy before reaching their final state as observed today. In addition, our work provides new insights about the origin of Mercury-like planets. Protoplanetary embryos form in circumstellar disks around young stars and undergo mutual gravitational interactions during their growth phase. These interactions lead to orbital migration and dynamical instabilities such as collisions between neighboring embryos. If these processes occur frequently enough, only one body will survive at the end of the growth stage leaving behind a planetary system consisting of just one planet. However, recent studies indicate that many planetary systems contain more than one planet suggesting that some mechanism must exist to prevent complete destruction of the system. Here we study the possibility that protoplanetary embryos follow a hierarchical evolutionary path where they first grow hierarchically via gravitational scattering followed by runaway accretion once the largest embryo has reached its isolation mass. Using numerical simulations, we demonstrate that this scenario naturally explains the existence of multi-planet systems while also reproducing the properties of known exoplanets.",
        "watermark_text": "We present an assessment of the stability of planetary networks in which protoplanetary embryos grow under oligarchy , i . e . , they are able to eject each other s neighbors by gravitational scattering but not themselves . We see that this process results to rapid growth of the greatest embryo until it hits its isolation volume ( the minimum mass needed for runaway accretion ) .The system then evolves into either a single planet or two planets with similar masses depending on how close the early conditions were to instability . This evolution is very different than what happens when all bodies grow simultaneously ; in particular , we prove that there can be several stable outcomes even if the first conditions are matched .Our results show that the formation of terrestrial worlds may have continued through several stages including oligarchy before reaching their final condition as found today . In addition , our work offer additional information about the origin of Mercury - like planets .Protoplanetary embryos form in circumstellar disks around new stars and undergo mutual gravitational interactions during their development period . These interactions result to orbital movement and dynamical instabilities such as collisions between neighboring embryos .If these mechanisms occur frequently enough , only one body will survive at the end of the development period leaving behind a planetary system consisting of just one planet . However , recent studies demonstrate that several planetary complexes include more than one planet suggesting that some method may arise to resist total destruction of the system .Here we study the prospect that protoplanetary embryos continue a hierarchical evolutionary course where they originally grow hierarchically via gravitational waves followed by runaway accretion once the greatest embryo has reached its isolation volume . Using numerical simulations , we prove that this situation naturally explains the existence of dual - planet systems while actually reproducing the properties of known exoplanets .",
        "rewrite_text": "We provide a review of the stability of planetary networks in which protoplanetary embryos experience oligarchic growth. In this scenario, these embryos can eject their neighboring bodies through gravitational scattering, while remaining intact themselves. Our analysis reveals that this leads to the rapid growth of the largest embryo until it reaches its isolation volume, the minimum mass required for runaway accretion. Subsequently, the system evolves into either a single planet or a pair of planets with comparable masses, depending on the proximity of initial conditions to instability. This developmental pathway contrasts sharply with scenarios where all bodies grow simultaneously; notably, we demonstrate that even with identical initial conditions, multiple stable outcomes are possible. Our findings suggest that the formation of terrestrial planets likely proceeded through several stages, including an oligarchic phase, before arriving at their current configurations. Furthermore, our research sheds light on the origins of Mercury-like planets. Protoplanetary embryos form within circumstellar disks surrounding new stars and engage in gravitational interactions throughout their growth period. These interactions can lead to orbital shifts and dynamic instabilities, including collisions between adjacent embryos. If these events occur frequently, it is likely that only one body will survive by the end of the growth phase, resulting in a planetary system composed of a single planet. However, recent research indicates that various planetary systems contain multiple planets, suggesting that some mechanisms might prevent the complete destruction of the system. In this study, we explore the possibility that protoplanetary embryos follow a hierarchical evolutionary trajectory, initially growing via gravitational interactions and later undergoing runaway accretion once the largest embryo achieves its isolation volume. Through numerical simulations, we demonstrate that this scenario naturally accounts for the existence of dual-planet systems while accurately reflecting the characteristics of known exoplanets.",
        "ori-fast-z-score": -0.8333333333333334,
        "water-fast-z-score": 7.166666666666667,
        "rewrite-fast-z-score": 0.8951673046482753
    },
    {
        "original_text": "We use hydrodynamic simulations to study how proto-clusters grow and evolve into galaxy clusters, focusing on their baryon content at high redshifts (z > 5). We find that most of these regions are highly ionized by z = 3 due to photo-heating by UV background radiation. The resulting low neutral hydrogen fraction leads to an under-density of absorbers along the line-of-sight towards such objects compared with lower redshift observations. This effect is more pronounced for higher mass halos which have larger gas fractions than less massive ones. Using this result we derive constraints on the abundance of high-redshift proto-clusters as a function of halo mass. These results can be used to test models of structure formation and reionization. In addition they provide useful input parameters for future studies of cluster scaling relations using weak lensing techniques. \n \n Keywords: Hydrogen ionization state, Galaxy Cluster, Reionization",
        "watermark_text": "We use hydrodynamic simulations to study how proto - galaxies grow and evolve into star clusters , concentrating on their baryon concentration at high redshifts ( z > 5 ) . We see that most of these regions are extremely ionized by z = 3 due to photo - heating by UV background radiation .The resulting lowered neutral hydrogen proportion leads to an under - density of absorbers along the line - of - view towards such objects compared with higher redshift observations . This phenomenon is more pronounced for greater weight halos which have larger gas fractions than less massive ones .Using this effect we derive restrictions on the availability of high - redshift proto - complexes as a function of halo mass . These conclusions can be used to test models of structure development and reionization .In addition they give valuable input parameters for future research of cluster scaling relations utilizing weak lensing methods . Keywords : Hydrogen ionization state , Galaxy Cluster , Reionization",
        "rewrite_text": "We employ hydrodynamic simulations to investigate the growth and evolution of proto-galaxies into star clusters, focusing on their baryonic concentration at high redshifts (z > 5). Our findings reveal that by z = 3, most of these regions are highly ionized due to photo-heating from the UV background radiation. This increased ionization results in a reduced proportion of neutral hydrogen, causing a lower density of absorbers in the line of sight towards such objects compared to observations at higher redshifts. This effect is more significant in more massive halos, which possess larger gas fractions than their less massive counterparts. By analyzing this phenomenon, we establish constraints on the abundance of high-redshift proto-complexes as a function of halo mass. These insights can be utilized to evaluate models of structure formation and reionization. Furthermore, they provide important parameters for future studies on cluster scaling relations through weak lensing techniques. Keywords: Hydrogen ionization state, Galaxy Cluster, Reionization.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.5778737935111105,
        "rewrite-fast-z-score": 0.11867816581938533
    },
    {
        "original_text": "We study holographically the confinement/deconfinement phase transitions of strongly coupled gauge theories on curved spaces by using the gravity dual with dilaton and axion fields, which is obtained as an exact solution to Einstein-Maxwell-dilaton-axion system in five dimensions. We find that the critical temperature for deconfinement decreases when we increase the curvature radius at fixed chemical potential or charge density. This result implies that the effect of gravitational backreaction becomes more important near the horizon than far away from it. In addition, we show that the critical temperature increases monotonously with increasing chemical potential (or charge density) at fixed curvature radius. The results are consistent with those found in flat space-time. Finally, we discuss how our results can be applied to QGP produced in heavy ion collisions. PACS: 04.50.+h, 11.10.Wx, 12.38.Mh",
        "watermark_text": "We research holographically the confinement / deconfinement phase transitions of highly coupled gauge theories on curved spaces by using the gravity twin with dilaton and axion fields , which is found as an precise answer to Einstein - Maxwell - dilaton - axion system in five dimensions . We see that the key temperature for deconfinement decreases when we increase the curvature length at fixed molecular potential or charge density .This result suggests that the impact of gravitational backreaction becomes more important near the horizon than far away from it . In addition , we find that the critical temperature increases monotonously with rising chemical potential ( or charge density ) at fixed curvature length .The results are compatible with those observed in flat space - time . Finally , we talk how our findings can be applied to QGP produced in heavy ion collisions .PACS: 04.50.+h, 11.10.Wx, 12.38.Mh",
        "rewrite_text": "We conduct holographic research into the confinement/deconfinement phase transitions of highly coupled gauge theories in curved spacetime, utilizing a gravity dual that incorporates dilaton and axion fields. This approach provides a precise solution to the Einstein-Maxwell-dilaton-axion system in five dimensions. Our findings indicate that the critical temperature for deconfinement decreases as the curvature length increases, while the molecular potential or charge density remains constant. This outcome implies that the effects of gravitational backreaction are more significant near the horizon compared to regions further away. Additionally, we observe that the critical temperature rises steadily with an increase in chemical potential (or charge density) at fixed curvature length, and these results align with those obtained in flat spacetime. Lastly, we discuss the implications of our results for understanding the quark-gluon plasma (QGP) formed in heavy ion collisions. PACS: 04.50.+h, 11.10.Wx, 12.38.Mh",
        "ori-fast-z-score": 0.6201736729460423,
        "water-fast-z-score": 5.333493587335964,
        "rewrite-fast-z-score": -0.12216944435630522
    },
    {
        "original_text": "We study the existence and uniqueness of equilibrium states for some classes of potentials in the presence of an external magnetic field, using results on the asymptotic behavior of solutions to Schrödinger equations obtained by J. Bourgain and M. Goldstein.  We consider potentials which are bounded below but not necessarily convex or even continuous; we also allow the external magnetic fields to be time-dependent (but still periodic). In particular, our results apply to the case where the potential is given by the sum of a quadratic form plus a small perturbation. The main tool used here is the so-called ``Bourgain-Goldstein method  , which consists in studying the evolution of the Wigner transform of the solution to the Schrödinger equation under the action of the unitary group generated by the corresponding Hamiltonian. \nIntroduction\n\nThe purpose of this article is to prove that there exists at most one equilibrium state associated to certain classes of potentials in the context of quantum mechanics when these potentials interact with an external magnetic field. This result was first proved by D. Ruelle  Ru  , who showed that if V : T d → R satisfies suitable conditions then there exists exactly one equilibrium measure µV . These assumptions include the fact that V should have superlinear growth at infinity and satisfy a condition known as ``uniform ellipticity  . However, it turns out that many interesting examples do not fall into this category. For example, let us mention the following two examples:  • If V = −|x| 2 + |y| 2 , then V does not grow faster than linearly at infinity.",
        "watermark_text": "We explore the existence and uniqueness of equilibrium states for some categories of potentials in the presence of an external magnetic force , using findings on the asymptotic nature of solutions to Schrödinger coefficients derived by J . Bourgain and M . Goldstein . We consider potentials which are bounded below but not necessarily convex or even continuous ; we also consider the external magnetic fields to be time - dependent ( but still periodic ) .In particular , our findings apply to the case where the potential is given by the sum of a quadratic form plus a small perturbation . The main technique applied here is the so - called ` ` Bourgain - Goldstein method , which consists in examining the evolution of the Wigner transform of the solution to the Schrödinger equation under the action of the unitary group produced by the associated Hamiltonian .Introduction The purpose of this page is to prove that there exists at most one equilibrium state associated to specified classes of potentials in the context of quantum mechanics when these potentials behave with an external magnetic force . This result was first proved by D . Ruelle Ru , who demonstrated that if V : T d → R satisfies suitable conditions then there exists precisely one equilibrium measure µV .These assumptions involve the fact that V should have superlinear development at infinity and meet a condition called as ` ` uniform ellipticity . However , it turns out that several interesting instances do not drop into this class .For instance , let us note the following two examples : • If V = − | x | 2 + | y | 2 , then V does not grow rapid than linearly at infinity .",
        "rewrite_text": "We investigate the existence and uniqueness of equilibrium states for specific categories of potentials influenced by an external magnetic field. Our approach leverages the asymptotic behavior of solutions to Schrödinger coefficients as established by J. Bourgain and M. Goldstein. We focus on potentials that are bounded below, though they need not be convex or continuous, and allow for time-dependent (but still periodic) external magnetic fields. Notably, our results encompass cases where the potential is represented as the sum of a quadratic form plus a minor perturbation. The primary methodology utilized in this study is the \"Bourgain-Goldstein method,\" which involves analyzing the evolution of the Wigner transform of the solution to the Schrödinger equation under the influence of the unitary group generated by the corresponding Hamiltonian. \n\n**Introduction**: This page aims to demonstrate that there is at most one equilibrium state associated with certain classes of potentials within quantum mechanics when these potentials interact with an external magnetic field. This outcome was initially proven by D. Ruelle Ru, who established that if V: Td → R meets specific conditions, then there exists a unique equilibrium measure µV. These conditions include V exhibiting superlinear growth at infinity and satisfying a requirement known as \"uniform ellipticity.\" However, it becomes apparent that several intriguing instances fall outside this classification. For example, consider the case where V = −|x|^2 + |y|^2; in this scenario, V does not exhibit growth faster than linear as it approaches infinity.",
        "ori-fast-z-score": 0.19611613513818404,
        "water-fast-z-score": 6.148170459575759,
        "rewrite-fast-z-score": -0.6
    },
    {
        "original_text": "We present the fundamental plane (FP) for galaxy clusters detected in the Planck survey at 143 GHz, based on their X-ray luminosity Lx , temperature Tx and SZ flux Y500 . The FP is defined as log(Y500 ) = α + βlog(Tx /Lx ), where we find that the best-fit values are  α = 0.92 ± 0.01 and β = 1.27 ± 0.02 with an intrinsic scatter of σint = 0.10 ± 0.03 dex.  We compare our results to previous studies using different cluster samples and methods. Our sample consists of 31 massive clusters selected by applying cuts in mass M500 > 5 × 1014 h−1 70 M⊙ and redshift z < 0.3. These clusters have been observed with XMM-Newton and Chandra satellites and also with ground-based telescopes such as APEX-SZ or Bolocam.",
        "watermark_text": "We present the fundamental plane ( FP ) for galaxy galaxies found in the Planck survey at 143 GHz , using on their X - ray luminosity Lx , temperature Tx and SZ flux Y500 . The FP is calculated as log ( Y500 ) = β + βlog ( Tx / Lx ) , where we find that the best - fitting values are α = 0 . 92 ± 0 . 01 and β = 1 . 27 ± 0 . 02 with an intrinsic scatter of σint = 0 . 10 ± 0 . 03 dex .We match our results to previous studies using separate cluster samples and methods . Our sample consists of 31 massive clusters selected by applying cuts in mass M500 > 5 × 1014 h−1 70 [UNK] and redshift z < 0 . 3 .These clusters have been observed with XMM-Newton and Chandra satellites and also with ground-based telescopes such as APEX-SZ or Bolocam.",
        "rewrite_text": "We present the fundamental plane (FP) for galaxies identified in the Planck survey at 143 GHz, utilizing their X-ray luminosity (Lx), temperature (Tx), and SZ flux (Y500). The FP is defined by the equation log(Y500) = β + β log(Tx / Lx). Our analysis yields best-fitting parameters of α = 0.92 ± 0.01 and β = 1.27 ± 0.02, with an intrinsic scatter of σ_int = 0.10 ± 0.03 dex. We compare our findings with prior studies that employed different cluster samples and methodologies. The sample consists of 31 massive clusters, selected with criteria of M500 > 5 × 10^14 h^−1_70 and z < 0.3. These clusters have been observed using XMM-Newton and Chandra satellites, as well as ground-based observatories like APEX-SZ and Bolocam.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.3094010767585034,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results of our analysis of pulsar data obtained with the LOFAR telescope in the Netherlands, which is part of the Low Frequency Array (LOFAR). The observations were carried out at frequencies between 10 MHz and 120 MHz using the High Band Antenna (HBA) system. We have detected two new types of pulsars - one that emits bursts of radio waves lasting several seconds and another whose pulses are modulated by an additional signal. In addition to these discoveries we also report on the detection of previously unknown phenomena associated with known pulsars. \n \n Pulsars are rapidly rotating neutron stars emitting beams of electromagnetic radiation across the entire visible universe. They can be observed over many decades as they spin down due to their magnetic dipole field losing energy into space. This causes them to slow down gradually until they stop completely after about ten billion years. As well as being extremely stable clocks for measuring time intervals, pulsars provide information about fundamental physics such as gravity, quantum electrodynamics and general relativity.",
        "watermark_text": "We present the conclusion of our analysis of pulsar information obtained with the LOFAR observatory in the Netherlands , which is part of the Low Frequency Array ( LOFAR ) . The surveys were carried out at speeds between 10 MHz and 120 MHz use the High Band Antenna ( HBA ) scheme .We have discovered two new types of pulsars - first that emits flashes of radio beams lasting several seconds and another whose pulses are modulated by an additional signal . In addition to these discoveries we also report on the discovery of previously unidentified behaviour associated with reported pulsars .Pulsars are rapidly spinning neutron stars emitting waves of electromagnetic radiation across the entire visible world . They can be viewed over numerous centuries as they rotate down due to their magnetic dipole field losing power into space .This forces them to halt down gradually until they stop completely after about ten billion decades . As well as being extremely reliable clocks for determining time periods , pulsars provide information about basic physics such as gravity , quantum electrodynamics and general relativity .",
        "rewrite_text": "We conclude our analysis of pulsar data collected at the LOFAR observatory in the Netherlands, part of the Low Frequency Array (LOFAR). The surveys were conducted at frequencies ranging from 10 MHz to 120 MHz using the High Band Antenna (HBA) configuration. Our research has led to the discovery of two new types of pulsars: one type emits bursts of radio waves lasting several seconds, while the other has pulses that are modulated by an additional signal. Additionally, we report the observation of previously unrecognized behavior associated with known pulsars. Pulsars are rapidly rotating neutron stars that emit waves of electromagnetic radiation across the observable universe. They can be observed over many centuries as their rotation slows due to the loss of energy from their magnetic dipole fields. This process causes them to gradually come to a halt, eventually ceasing to emit after approximately ten billion years. Besides serving as incredibly precise timekeepers, pulsars also provide valuable insights into fundamental physics, including gravity, quantum electrodynamics, and general relativity.",
        "ori-fast-z-score": -1.4814874939752933,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": 0.11704114719613057
    },
    {
        "original_text": "We present the results of our analysis of molecular gas mass estimates based on CO and HCN observations in nearby galaxies, using data obtained with the IRAM 30m telescope. We find that conversion factors between luminosity and mass are strongly dependent on the star formation rate (SFR) per unit area within each galaxy disk. The SFR surface density is found to be an important parameter controlling the conversion factor XCO = M(H2)/L(CO), which we derive by fitting the observed L(HCN) / L(CO) ratio versus metallicity relation. For low values of ΣSFR < 1M⊙ yr-1 kpc-2 , corresponding to quiescent disks or nuclear regions dominated by old stellar populations, we obtain XCO ≈ 2 × 10 20 cm−2 K−1 km−1 s. This value increases up to XCO ≈ 5×10 20 cm−2 K−1km−1s at high ΣSFR > 3M⊙yr-1kpc-2 . These findings suggest that the physical conditions of the interstellar medium may change significantly depending on whether it is located in actively star-forming regions or not.",
        "watermark_text": "We publish the conclusion of our analysis of molecular gas mass estimates based on CO and HCN measurements in nearby galaxies , using data derived with the IRAM 30m telescope . We see that conversion factors between luminosity and mass are strongly dependent on the star formation rate ( SFR ) per unit area within each galaxy disk .The SFR ground density is found to be an important function regulating the transformation parameter XCO = M ( H2 ) / L ( CO ) , which we derive by fitting the seen L ( HCN ) / L ( CO ) ratio versus metallicity relation . For low values of ΣSFR < [UNK] yr - 1 kpc - 2 , equivalent to quiescent disks or atomic regions dominated by ancient stars populations , we obtain XCO ≈ 2 × 10 20 cm−2 K−1 km−1 s . This value rises up to XCO ≈ 5×10 20 cm−2 K−1km−1s at high ΣSFR > [UNK] - 1kpc - 2 .These studies imply that the physical conditions of the interstellar material may change considerably depending on whether it is situated in actively star - creating areas or not .",
        "rewrite_text": "We present the conclusions from our analysis of molecular gas mass estimates based on CO and HCN measurements in nearby galaxies, utilizing data obtained from the IRAM 30m telescope. Our findings indicate that the conversion factors between luminosity and mass are significantly influenced by the star formation rate (SFR) per unit area within each galaxy's disk. We discovered that the SFR surface density plays a crucial role in regulating the transformation parameter XCO = M(H2)/L(CO), which we calculate by fitting the observed L(HCN)/L(CO) ratio against metallicity. For low values of ΣSFR < [UNK] yr⁻¹ kpc⁻², typical of quiescent disks or regions dominated by older stellar populations, we derive XCO ≈ 2 × 10²⁰ cm⁻² K⁻¹ km⁻¹ s. In contrast, this value increases to XCO ≈ 5 × 10²⁰ cm⁻² K⁻¹ km⁻¹ s for high ΣSFR > [UNK] yr⁻¹ kpc⁻². These results suggest that the physical conditions of interstellar material can vary significantly depending on its location in relation to active star formation areas.",
        "ori-fast-z-score": 0.8307471607356973,
        "water-fast-z-score": 5.815230125149881,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study the wetting properties of a fluid in contact with an attractive wall, using a simple model for which we can perform exact calculations. The system consists of particles interacting via a hard-core repulsion and a short-range attraction that decays exponentially fast at large distances. We show how to calculate exactly the density profile near the wall as well as the surface tension between the liquid phase and the gas phase. In particular, we find that there is no true equilibrium state corresponding to complete wetting by the liquid phase; instead, the interface becomes rough when the temperature decreases below some critical value T*. This phenomenon occurs because the exponential tail of the interaction potential leads to strong fluctuations in the number of particles adsorbed onto the wall. These fluctuations are responsible for the non-analytic behavior observed both in the density profile and in the surface tension. \n \n Introduction \n \n Wetting phenomena occur whenever two phases coexist in contact with each other  1  . For example, water droplets spread over glass surfaces due to capillary forces  2  , while oil spreads out on top of water  3  . A particularly interesting situation arises if one of these phases has a lower dimensionality than the others  4  . Indeed, this may lead to new types of transitions such as those occurring in systems where a thin film coexists with its vapor  5  or in confined geometries  6  . \n \n Here we consider a simple model describing the wetting properties of fluids in contact with walls  7, 8  . Our results suggest that even though the interactions decay rapidly away from the wall, they still give rise to nontrivial effects. More specifically, our analysis shows that the presence of a wall induces strong fluctuations in the number Nw of particles adsorbed on it  9  . As a result, the interface separating the liquid phase (containing all particles) from the gas phase (containing none) becomes rough  10  when the temperature drops below a certain threshold T*. Below T*, the average distance between neighboring particles increases significantly so that the interface acquires a fractal structure  11  . \nModel description\n\nThe system under consideration consists of N identical",
        "watermark_text": "We research the wetting properties of a fluid in contact with an attractive wall , using a simple simulation for which we can conduct accurate calculations . The system consists of molecules bonding via a hard - core repulsion and a small - range attraction that decays exponentially rapidly at large distances .We see how to estimate exactly the density profile near the wall as well as the surface friction between the liquid phase and the gas phase . In particular , we find that there is no true equilibrium state analogous to complete wetting by the liquid phase ; instead , the interface becomes rough when the temperature rises below some significant value T * .This phenomenon occurs because the exponential tail of the interaction potential leads to powerful fluctuations in the quantity of molecules adsorbed onto the wall . These fluctuations are responsible for the non - analytic activity observed both in the density profile and in the surface tension .Introduction Wetting processes arise whenever two phases coexist in contact with each other 1 . For instance , freshwater droplets distributed over glazed surfaces thanks to capillary forces 2 , while petroleum spreads out on top of water 3 .A notably important scenario occurs if one of these stages has a smaller dimensionality than the others 4 . Indeed , this might lead to novel sorts of transitions such as those occurring in systems where a thin glass coexists with its vapor 5 or in confined geometries 6 .Here we imagine a simple simulation explaining the wetting properties of fluids in contact with structures 7 , 8 . Our results propose that even though the interactions decay greatly back from the wall , they still give rise to nontrivial effects .More specifically , our analysis shows that the presence of a barrier induces strong fluctuations in the number Nw of molecules adsorbed on it 9 . As a result , the interface separating the liquid phase ( containing all ions ) from the gas phase ( containing nobody ) gets rough 10 when the temperature falls below a certain level T * .Below T * , the average distance between neighboring particles grows significantly so that the interface acquires a fractal structure 11 . Model description The system under consideration consists of N identical",
        "rewrite_text": "We investigate the wetting characteristics of a fluid interacting with an attractive wall through a straightforward simulation that allows for precise calculations. The system is composed of molecules that interact via a hard-core repulsion and a short-range attraction that decreases rapidly at larger distances. Our findings enable us to accurately estimate the density profile near the wall as well as the surface friction between the liquid and gas phases. Notably, we discover that there is no true equilibrium state comparable to complete wetting by the liquid phase; instead, the interface becomes rough when the temperature drops below a critical value, T*. This behavior arises because the exponential decay of the interaction potential results in significant fluctuations in the number of molecules adsorbed on the wall. These fluctuations contribute to the non-analytic features observed in both the density profile and surface tension.\n\n**Introduction**  \nWetting phenomena occur whenever two phases coexist in contact. For example, freshwater droplets can rest on glazed surfaces due to capillary forces, while petroleum floats on water. An important scenario arises when one of these phases has lower dimensionality than the others, potentially leading to novel transition types, such as those observed in systems where a thin glass layer is in equilibrium with its vapor, or in confined geometries. In this work, we propose a simple simulation to elucidate the wetting properties of fluids in contact with structural interfaces. Our results indicate that even though interactions diminish significantly with distance from the wall, they still produce remarkable effects. More specifically, our analysis reveals that the presence of a barrier generates strong fluctuations in the number of molecules adsorbed onto it. Consequently, the interface separating the liquid phase (which contains all ions) from the gas phase (which contains none) becomes rough when the temperature falls below a threshold temperature, T*. Below this critical temperature, the average distance between neighboring particles increases considerably, giving the interface a fractal character.\n\n**Model Description**  \nThe system under analysis comprises N identical molecules.",
        "ori-fast-z-score": 0.5482823149915702,
        "water-fast-z-score": 8.227195572487128,
        "rewrite-fast-z-score": 2.413346726900721
    },
    {
        "original_text": "The magnetic reconnection is one of the most important processes for understanding many phenomena observed in space and laboratory plasmas, such as solar flares, magnetospheric substorms, sawtooth crashes in tokamaks etc.. In this work we present an analytical model which describes the process of magnetic reconnection in collisionless high energy plasma with arbitrary initial conditions. The main idea behind our approach consists in using the concept of generalized force density tensor (GDFT) introduced by MHD theory. We show that GDFT can be used not only to describe the macroscopic dynamics but also microscopic properties of the system like particle distribution functions. Our results are compared with those obtained within other approaches based on kinetic description of particles motion. It turns out that all these models give similar predictions when applied to simple cases where the initial state has no gradients along the direction perpendicular to the background magnetic field. However, if there exist some gradients across the magnetic field lines then different models predict quite different behavior.",
        "watermark_text": "The magnetic reconnection is one of the most important processes for studying many phenomena observed in space and lab plasmas , such as sun flares , magnetospheric substorms , sawtooth crashes in tokamaks etc . . In this research we present an analytical theory which explains the process of magnetic reconnection in collisionless high energy gas with arbitrary initial conditions .The main idea behind our approach consists in utilizing the notion of generalized force density tensor ( GDFT ) developed by MHD physics . We see that GDFT can be used not only to explain the macroscopic behavior but also microscopic characteristics of the process like particle distribution functions .Our results are compared with those achieved within other methods based on kinetic representation of particles movement . It turns out that all these models make comparable predictions when applied to simple instances where the first state has no gradients along the direction perpendicular to the background magnetic force .However , if there exist some gradients across the magnetic force lines then various models predict quite different properties .",
        "rewrite_text": "Magnetic reconnection is a crucial process for understanding various phenomena observed in both space and laboratory plasmas, including solar flares, magnetospheric substorms, and sawtooth crashes in tokamaks. In this study, we introduce an analytical theory that describes magnetic reconnection in collisionless high-energy gases with arbitrary initial conditions. Our approach leverages the concept of the generalized force density tensor (GDFT), as developed in magnetohydrodynamics (MHD). We demonstrate that GDFT is effective not only in elucidating macroscopic behavior but also in capturing microscopic characteristics, such as particle distribution functions. Our findings are compared with those from alternative methods based on kinetic representations of particle dynamics. We discover that these models yield comparable predictions in simpler cases where the initial state lacks gradients perpendicular to the background magnetic field. However, when gradients are present across the magnetic field lines, the models diverge in their predictions of various properties.",
        "ori-fast-z-score": -0.7777777777777778,
        "water-fast-z-score": 5.74243935589202,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "We present the results of our survey for southern M-dwarf binaries with separations between 0.1 AU and 10 AU, using data obtained by the Wide Field Camera 3 (WFC3) on board HST as part of program GO-12775. We find that about half of all systems are unresolved or only marginally resolved at these distances. The fraction of close binaries is higher among lower mass objects than it is among more massive ones; we estimate that this difference may be due to observational biases rather than intrinsic differences in formation mechanisms. \n \n In addition, we have identified several new candidate companions around known nearby ultracool dwarfs: GJ 436Bb, GJ 581Cc, GJ 674Ab, and GJ 758Aa. These candidates will require further observations before their status can be confirmed. Finally, we report the discovery of two previously unknown substellar objects: 2MASS J03552385+1133413AB and 2MASS J16252618-2434439AB.",
        "watermark_text": "We present the conclusion of our survey for southern M - dwarf binaries with separations between 0 . 1 AU and 10 AU , using data acquired by the Wide Field Camera 3 ( WFC3 ) on board HST as part of series GO - 12775 . We see that about half of all systems are unresolved or only marginally resolved at these distances .The percentage of close binaries is higher among lower weight objects than it is among more massive ones ; we estimate that this distinction might be due to observational biases rather than intrinsic differences in formation patterns . In addition , we have discovered several novel candidate companions around suspected local ultracool dwarfs : GJ 436Bb , GJ 581Cc , GJ 674Ab , and GJ 758Aa .These candidates will take further observations before their status can be verified . Finally , we publish the discovery of two formerly unidentified substellar objects : 2MASS J03552385 + 1133413AB and 2MASS J16252618 - 2434439AB .",
        "rewrite_text": "We present the findings from our survey of southern M-dwarf binaries with separations ranging from 0.1 AU to 10 AU, utilizing data collected by the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST) as part of program GO-12775. Our analysis reveals that approximately half of these systems are either unresolved or only marginally resolved at such distances. We observe a higher percentage of close binaries among less massive stars compared to their more massive counterparts; we hypothesize that this disparity may stem from observational biases rather than inherent differences in their formation processes. Additionally, we have identified several new candidate companions around potential local ultracool dwarfs, including GJ 436Bb, GJ 581Cc, GJ 674Ab, and GJ 758Aa. These candidates will require further observations to confirm their status. Lastly, we report the discovery of two previously unidentified substellar objects: 2MASS J03552385 + 1133413AB and 2MASS J16252618 - 2434439AB.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.181036310903636,
        "rewrite-fast-z-score": -1.632993161855452
    },
    {
        "original_text": "We present an efficient numerical scheme to solve the incompressible Navierstokes (NS) equations by using the lattice Boltzmann method with the Inverse Kinetic Approach (IKA). The IKA is based on the idea that the NS equation can be recovered as the equilibrium state in the Chapman-Enskog expansion, and it has been successfully applied to various fluid dynamics problems. We show how this concept can be implemented into the LBM framework. Numerical results are presented to demonstrate the accuracy and efficiency of our proposed algorithm. Finally we discuss some possible extensions of the current work. Keywords: Lattice Boltzmann Method; Inverse Kinetic Approximation; Incompressible Navier-Stokes; Computational Fluid Dynamics. 1 Introduction The lattice Boltzmann method (LBM), originally developed by Frisch et al  1  , is one of the most promising approaches to computational fluid dynamics (CFD). It is particularly suitable for parallel computing due to its inherent locality  2  . Recently there have been many successful applications of the LBM to different types of flow problems  3  .\nThe basic idea behind the LBM is to represent the distribution function f(x,t) at each node x of a regular grid by a finite number of particles moving along discrete velocities c i = ciΔt/Δx, where Δx and Δt denote respectively the spatial and temporal resolutions  4  . Then the evolution of these particles is governed by the following equation: \nwhere τ denotes the relaxation time which controls the rate of approaching towards the equilibrium distribution function f eq i\n. By choosing appropriate values of τ, the macroscopic quantities such as density ρ and velocity u can be obtained through moments of the distribution function:",
        "watermark_text": "We present an efficient numerical system to solve the incompressible Navierstokes ( NS ) equations by using the lattice Boltzmann technique with the Inverse Kinetic Approach ( IKA ) . The IKA is based on the idea that the NS equation can be recovered as the equilibrium state in the Chapman - Enskog expansion , and it has been successfully application to numerous fluid theory issues .We see how this concept can be applied into the LBM framework . Numerical results are presented to indicate the accuracy and efficiency of our proposed algorithm .Finally we explain some possible extensions of the present work . Keywords : Lattice Boltzmann Method ; Inverse Kinetic Approximation ; Incompressible Navier - Stokes ; Computational Fluid Dynamics .1 Introduction The lattice Boltzmann technique ( LBM ) , previously developed by Frisch et al 1 , is one of the most attractive approaches to computational liquid dynamics ( CFD ) . It is especially suitable for concurrent processing due to its inherent locality 2 .Recently there have been many successful uses of the LBM to different kinds of flow questions 3 . The basic idea behind the LBM is to represent the distribution map g ( x , t ) at each node h of a regular grid by a finite number of atoms moved along continuous velocities c i = ciΔt / Δx , where Δx and Δt denote respectively the spatial and spatial resolutions 4 .Then the evolution of these objects is governed by the following equation : where τ represents the relaxation time which influences the speed of moving towards the equilibrium distribution function f eq i . By selecting appropriate values of σ , the macroscopic parameters such as density w and speed v can be obtained through moments of the distribution function :",
        "rewrite_text": "We introduce an effective numerical method for solving the incompressible Navier-Stokes (NS) equations, utilizing the lattice Boltzmann method (LBM) in conjunction with the Inverse Kinetic Approach (IKA). The IKA operates on the principle that the NS equations can be derived as the equilibrium state in the Chapman-Enskog expansion and has proven to be effective in addressing a variety of fluid dynamics problems. This work demonstrates how the IKA can be integrated within the LBM framework. We present numerical results that highlight the accuracy and efficiency of our proposed algorithm and discuss potential extensions of this research. \n\nKeywords: Lattice Boltzmann Method; Inverse Kinetic Approximation; Incompressible Navier-Stokes; Computational Fluid Dynamics. \n\n1 Introduction: The lattice Boltzmann method (LBM), initially developed by Frisch et al., is a highly effective approach for computational fluid dynamics (CFD). Its inherent locality makes it particularly well-suited for concurrent processing. In recent times, LBM has been successfully applied to a variety of flow-related problems. The core idea of LBM is to represent the distribution function \\( g(x, t) \\) at each node of a structured grid through a finite number of particles moving with continuous velocities \\( c_i = c_i \\Delta t / \\Delta x \\), where \\( \\Delta x \\) and \\( \\Delta t \\) denote the spatial and temporal resolutions, respectively. The evolution of these particles is described by an equation where \\( \\tau \\) is the relaxation time that dictates how quickly the system approaches the equilibrium distribution function \\( f_{eq}^i \\). By choosing appropriate values for \\( \\sigma \\), macroscopic parameters such as density \\( \\rho \\) and velocity \\( v \\) can be derived from the moments of the distribution function.",
        "ori-fast-z-score": -2.3312620206007844,
        "water-fast-z-score": 4.786988013071282,
        "rewrite-fast-z-score": -1.4110813025753959
    },
    {
        "original_text": "We present new results on the kinematics and dynamics of spiral arm streaming motions in the nearby grand-design galaxy NGC 5194 (M51). We use high-resolution near-IR integral field spectroscopy obtained with SINFONI at VLT to study the gas velocity fields, line-of-sight velocities, and mass surface density distributions along two different lines of sight through the central region of this interacting system. The main goal is to investigate whether or not there are systematic differences between these two viewing angles that could be related to an underlying gravitational potential perturbation caused by the companion galaxy NGC 5195. Our analysis shows that both sides of the nuclear ring exhibit similar large-scale streaming patterns which can be traced out to distances of about 1 kpc into the disk plane. However, we find significant deviations from circular rotation within the inner 100 pc radius around the nucleus. These non-circular motions appear to be dominated by radial inflow towards the center as well as counter-rotating components perpendicular to the major axis of the galactic bar.",
        "watermark_text": "We report new data on the kinematics and dynamics of spiral arm streaming movements in the nearby grand - design galaxy NGC 5194 ( M51 ) . We use large - resolution near - IR integral field spectroscopy derived with SINFONI at VLT to study the gas velocity fields , line - of - view velocities , and mass surface velocity distributions along two different lines of view through the main region of this interacting system .The main goal is to examine whether or not there are systematic differences between these two viewing angles that might be connected to an underlying gravitational potential perturbation caused by the companion galaxy NGC 5195 . Our study shows that both sides of the nuclear ring show identical large - scale drifting patterns which can be traced out to distances of about 1 kpc into the disk plane .However , we find considerable deviations from circular rotation within the inner 100 pc radius around the nucleus . These non - circular movements appear to be dominated by radial inflow towards the center as well as anti - spinning components parallel to the main axis of the galactic bar .",
        "rewrite_text": "We present new findings on the kinematics and dynamics of spiral arm movements in the nearby grand-design galaxy NGC 5194 (M51). Utilizing high-resolution near-infrared integral field spectroscopy from SINFONI at the VLT, we analyze gas velocity fields, line-of-sight velocities, and mass surface velocity distributions along two distinct lines of sight within the main region of this interacting galaxy system. Our primary objective is to determine whether systematic differences exist between these two perspectives, potentially linked to gravitational disturbances from the companion galaxy NGC 5195. Our results indicate that both sides of the nuclear ring exhibit similar large-scale drift patterns, which extend approximately 1 kpc into the disk plane. However, we observe significant deviations from circular rotation within the inner 100 pc surrounding the nucleus. These non-circular motions appear to be predominantly influenced by radial inflow toward the center, along with anti-spinning components aligned with the main axis of the galactic bar.",
        "ori-fast-z-score": 1.0660035817780522,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": 1.4269353798659745
    },
    {
        "original_text": "We present new optical/NIR data for GRB 060206, which show that its X-ray to radio afterglow is well described by an achromatic break at tbreak = 1.3 days followed by a power-law decay with index -1.2 (Fν ∝ t-1.2). The lack of any spectral evolution across this break suggests it was caused by energy injection into the blast wave. We find no evidence for dust extinction along our line-of-sight; however we cannot rule out significant reddening due to host galaxy dust. Our results are consistent with previous claims that achromatic breaks observed in many other bursts may be explained as being due to late-time energy injections rather than jet-break effects. \n \n Keywords: Gamma-ray burst, Afterglow emission, Energy injection, Jet break, Redshift measurement \n \n INTRODUCTION \n \n In recent years there has been growing interest in understanding how gamma ray bursts (GRBs) produce their broadband electromagnetic radiation. This effort has led to several successful models describing the prompt phase of GRB emission (see e.g., Piran 2005; Zhang 2007), but less progress on explaining the origin of the afterglow component. A key feature of most afterglows is the presence of a steepening or  jet break  in the light curve around one day postburst (Rhoads 1999) . Such breaks have traditionally been interpreted as marking the time when the relativistic ejecta becomes optically thin to synchrotron self-absorption, causing the flux density to drop rapidly. However, some authors argue that such breaks can also arise if the ejecta undergoes continued energy input following the initial explosion (e.g., Kumar & Panaitescu 2000; Granot et al. 2001; Chevalier & Li 2000) , while others suggest that they could instead result from changes in the geometry of the emitting region (e.g., Racusin et al. 2008 ). An alternative explanation for these breaks invokes interstellar scintillation (Goodman 1997; Goodman & Narayan 2006 ) - a phenomenon",
        "watermark_text": "We present new optical / NIR data for GRB 060206 , which show that its X - ray to radio afterglow is well described by an achromatic crack at tbreak = 1 . 3 days preceded by a power - law decay with index - 1 . 2 ( Fν [UNK] t - 1 . 2 ) . The absence of any spectral evolution across this break suggests it was produced by energy flow into the explosion wave .We see no evidence for powder extinction along our line - of - seeing ; however we cannot leave out significant reddening due to host universe material . Our results are compatible with previous accounts that achromatic breaks found in many other bursts perhaps be understood as being result to late - time energy injections rather than jet - break interactions .Keywords : Gamma - ray flare , Afterglow emission , Energy injection , Jet break , Redshift measurement INTRODUCTION In past decades there has been growing interest in understanding how gamma ray clusters ( GRBs ) output their broadband electromagnetic radiation . This effort has led to several successful theories describing the prompt stage of GRB absorption ( saw e . g . , Piran 2005 ; Zhang 2007 ) , but less progress on explaining the origin of the afterglow component .A key feature of most afterglows is the formation of a steepening or jet break in the light spiral around one day postburst ( Rhoads 1999 ) . Such breaks have traditionally been viewed as indicating the period when the relativistic ejecta becomes optically thin to synchrotron self - absorption , forcing the flux concentration to fall swiftly .However , some writers argue that such breaks can also arise if the ejecta undergoes continued energy source following the first blast ( e . g . , Kumar & Panaitescu 2000 ; Granot et al . 2001 ; Chevalier & Li 2000 ) , while others suggest that they may rather result from alterations in the topology of the emitting area ( e . g . , Racusin et al .2008 ) . An alternative theory for these breaks invokes interstellar scintillation ( Goodman 1997 ; Goodman & Narayan 2006 ) - a phenomenon",
        "rewrite_text": "We present new optical and near-infrared data for GRB 060206, demonstrating that its X-ray to radio afterglow is accurately characterized by an achromatic break at t_break = 1.3 days, following a power-law decay with an index of -1.2 (Fν ∝ t^-1.2). The lack of spectral variability around this break indicates that it resulted from energy transfer into the explosion wave. We do not find evidence of dust extinction along our line of sight; however, we cannot rule out significant reddening due to material from the host galaxy. Our findings align with previous observations suggesting that achromatic breaks observed in many other bursts may be interpreted as a consequence of late-time energy injections rather than jet-break interactions. \n\n**Keywords**: Gamma-ray bursts, Afterglow emission, Energy injection, Jet break, Redshift measurement\n\n**INTRODUCTION**: In recent decades, there has been increasing interest in understanding the mechanisms through which gamma-ray bursts (GRBs) emit their broadband electromagnetic radiation. This pursuit has resulted in several successful theories explaining the prompt phase of GRB absorption (see Piran 2005; Zhang 2007), but there has been less advancement in clarifying the origin of the afterglow component. A prominent feature of most afterglows is the appearance of a steepening or jet break in the light curve approximately one day after the burst (Rhoads 1999). Traditionally, these breaks have been interpreted as the moment when the relativistic ejecta becomes optically thin to synchrotron self-absorption, leading to a rapid decline in flux. However, some researchers argue that such breaks can also occur due to ongoing energy extraction following the initial explosion (e.g., Kumar & Panaitescu 2000; Granot et al. 2001; Chevalier & Li 2000), while others suggest they might arise from changes in the geometry of the emitting region (e.g., Racusin et al. 2008). An alternative model for these breaks involves interstellar scintillation (Goodman 1997; Goodman & Narayan 2006).",
        "ori-fast-z-score": -0.33567254331867563,
        "water-fast-z-score": 7.72046849632954,
        "rewrite-fast-z-score": -0.1690308509457033
    },
    {
        "original_text": "We present the first measurement of the supermassive black hole (SMBH) mass function for both early-(elliptical/S0, Sa-Sb) and latetype (Scd-Sm) galaxies using data from the Millennium Galaxy Catalogue (MGC). We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations. Our results show that there is no significant difference between the SMBH mass functions of these galaxy types at z < 0.1. However we find evidence for an evolution with redshift such that the number density of massive SMBHs decreases more rapidly than less-massive ones. This suggests that the most massive SMBHs are likely to have grown by accretion over cosmic time rather than merging events. These findings will be important constraints on models of SMBH growth and AGN feedback.",
        "watermark_text": "We present the first measurement of the supermassive black hole ( SMBH ) mass distribution for both late - ( elliptical / S0 , Sa - Sb ) and latetype ( Scd - Sm ) clusters using data from the Millennium Galaxy Catalogue ( MGC ) . We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations .Our results show that there is no major variation between the SMBH mass distributions of these galaxy forms at z < 0 . 1 . However we find proof for an evolution with redshift such that the number density of large SMBHs falls more swiftly than less - massive ones .This implies that the most gigantic SMBHs are likely to have expanded by accretion over universe time rather than joining events . These conclusions will be crucial constraints on estimates of SMBH growth and AGN feedback .",
        "rewrite_text": "We present the inaugural measurement of the supermassive black hole (SMBH) mass distribution for both late-type and early-type galaxy clusters (including elliptical/S0 and Sa-Sb types, as well as Scd-Sm types), using data from the Millennium Galaxy Catalogue (MGC). Our analysis employs two distinct approaches to determine SMBH masses: stellar velocity dispersion measurements and bulge luminosity scaling relations. Our findings indicate that there is no significant difference in the SMBH mass distributions across these galaxy types at redshift z < 0.1. However, we observe evidence of redshift evolution, showing that the number density of massive SMBHs declines more rapidly than that of their less massive counterparts. This suggests that the most massive SMBHs are likely to have grown through accretion over cosmic time, rather than through merger events. These insights will serve as important constraints for understanding SMBH growth and AGN feedback mechanisms.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": -1.2309149097933272
    },
    {
        "original_text": "We present the results of our analysis on the statistical properties of dust FIR emission in nearby galaxies, based on data obtained by ISO and Spitzer space telescopes. We find that the distribution function of dust FIR luminosity is well described by a log-normal form with an exponential tail at high luminosities. The mean value of the logarithmic luminosity dispersion for all samples considered here is 0.3 dex (factor of 2). This result suggests that there are two populations of dusty star-forming regions within each galaxy -one population associated with normal star formation activity and another one associated with intense bursts of star formation. Our study also shows that the fraction of galaxies containing such extreme objects increases towards higher redshifts. These findings have important implications for understanding the physical processes responsible for the evolution of distant galaxies as well as their contribution to the cosmic infrared background radiation. \n \n Keywords: Infrared, Galaxy",
        "watermark_text": "We present the conclusion of our analysis on the statistical characteristics of dust FIR pollution in nearby galaxies , using on evidence derived by ISO and Spitzer space telescopes . We see that the distribution function of dust FIR luminosity is well described by a log - normal shape with an exponential tail at high luminosities .The mean value of the logarithmic luminosity dispersion for all specimens considered here is 0 . 3 dex ( factor of 2 ) . This result suggests that there are two communities of dusty star - creating areas within each galaxy - one community associated with normal star formation activity and another one linked with aggressive bursts of galaxy formation .Our study also shows that the fraction of stars bearing such extreme objects increases towards higher redshifts . These studies have important implications for studying the physical processes responsible for the evolution of distant galaxies as also as their impact to the cosmic infrared background radiation .Keywords: Infrared, Galaxy",
        "rewrite_text": "We conclude our analysis of the statistical characteristics of dust FIR pollution in nearby galaxies, utilizing data from the ISO and Spitzer space telescopes. Our findings indicate that the distribution function of dust FIR luminosity closely follows a log-normal distribution with an exponential tail at higher luminosities. The average value of the logarithmic luminosity dispersion across all samples analyzed is 0.3 dex (equivalent to a factor of 2). This implies the existence of two distinct communities of dusty star-forming regions within each galaxy: one associated with standard star formation activity and another linked to intense bursts of galaxy formation. Additionally, our study reveals that the proportion of stars with such extreme properties rises at greater redshifts. These findings have significant implications for understanding the physical processes that drive the evolution of distant galaxies and their contributions to the cosmic infrared background radiation. \nKeywords: Infrared, Galaxy",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 5.427092530382482,
        "rewrite-fast-z-score": 1.3054598240132387
    },
    {
        "original_text": "We present the Hamiltonian formulation for general relativity with matter fields on an arbitrary spacetime manifold, including both classical and quantum aspects. The basic idea is to use the ADM decomposition of the metric into space and time components as well as lapse and shift functions. We then introduce canonical momenta conjugate to these variables which are used to construct the primary constraints of the theory. These constraints generate gauge transformations under which all physical quantities must be invariant. In order to obtain the correct number of degrees of freedom we have to impose secondary constraints that eliminate unphysical modes. Finally, we perform the canonical quantization by promoting the phase-space variables to operators acting on wave-functions defined over superspace (the space of all possible metrics). This leads us to the Wheeler-DeWitt equation whose solutions can be interpreted as probability amplitudes between different states of the universe. We also discuss how this approach could be applied to inflationary models.",
        "watermark_text": "We introduce the Hamiltonian formulation for general relativity with matter fields on an arbitrary spacetime manifold , covering both classical and quantum aspects . The basic idea is to use the ADM decomposition of the metric into space and time parts as well as lapse and shift variables .We then introduce canonical momenta conjugate to these parameters which are applied to build the primary constraints of the theory . These restrictions produce gauge transformations under which all physical components must be invariant .In order to obtain the appropriate number of degrees of liberty we have to introduce secondary constraints that eliminate unphysical modes . Finally , we perform the canonical quantization by expanding the phase - space variables to functions acted on wave - functions defined over superspace ( the space of all possible metrics ) .This leads us to the Wheeler - DeWitt equation whose solutions can be interpreted as probability amplitudes between various states of the universe . We also discuss how this methodology possible be applied to inflationary theories .",
        "rewrite_text": "We present the Hamiltonian formulation of general relativity that incorporates matter fields on an arbitrary spacetime manifold, addressing both classical and quantum perspectives. The central concept involves the ADM decomposition of the metric into spatial and temporal components, along with lapse and shift variables. We then define the canonical momenta conjugate to these parameters, which facilitate the establishment of the primary constraints of the theory. These constraints lead to gauge transformations, under which all physical quantities must remain invariant. To achieve the correct number of degrees of freedom, we introduce secondary constraints to eliminate unphysical modes. Ultimately, we carry out canonical quantization by expanding the phase-space variables into functions that act on wave functions defined in superspace—the space of all possible metrics. This process culminates in the Wheeler-DeWitt equation, whose solutions can be interpreted as probability amplitudes connecting different states of the universe. Additionally, we explore the potential application of this framework to inflationary theories.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 3.849741916091625,
        "rewrite-fast-z-score": 1.3416407864998738
    },
    {
        "original_text": "We propose an algorithm for network tomography that is able to reconstruct the internal structure of a network by using only one-dimensional (1-D) measurements, i.e., link counts between pairs of nodes in the network. The proposed method can be applied to any type of networks and does not require any prior knowledge about their topology or traffic patterns. We show how our approach can be used to estimate the number of active flows at each node as well as the amount of data transmitted over each flow. Our results are validated through extensive simulations performed with real Internet traces. Network tomography has been widely studied during recent years due to its potential applications in many areas such as computer security, quality-of-service provisioning, and traffic engineering  1  . In this context, it consists of estimating some properties of the network s internal state (such as the number of active flows per node or the amount of data transferred along each flow) by observing only external information (i.e., link-level statistics). This problem becomes particularly challenging when dealing with large-scale networks since the number of possible states grows exponentially with the size of the network  2  .\nIn order to overcome these limitations, several approaches have been recently proposed which exploit specific characteristics of the underlying network  3  , e.g., sparsity  4  -  6  , symmetry  7  , or regularity  8  . However, most existing methods assume either complete knowledge of the network topology  9 -  11  or accurate estimates of the traffic matrix  12  -  14  . Unfortunately, both assumptions may not hold in practice  15  , especially if we consider large and/or dynamic networks  16  . For example, in IP-based networks, the exact location of routers cannot always be determined  17  while the traffic matrix is usually unknown  18  . Moreover, even if the network topology were known, collecting all necessary information would still be impractical because of scalability issues  19  . Finally, obtaining accurate estimates of the traffic...",
        "watermark_text": "We suggest an algorithm for network tomography that is able to reconstruct the internal structure of a network by using only one - dimensional ( 1 - D ) observations , i . e . , link counts between pairs of nodes in the network . The proposed approach can be applied to any type of networks and does not require any earlier knowledge about their topology or traffic behavior .We see how our approach can be used to estimate the total of active flows at each node as also as the quantity of content conveyed over each flow . Our results are validated through ongoing simulations conducted with real Internet traces .Network tomography has been widely explored during recent years owing to its potential applications in different areas such as data security , quality - of - service provisioning , and route management 1 . In this context , it consists of estimating some properties of the organization s internal state ( such as the number of active flows per node or the quantity of content transferred along each flow ) by observing only external information ( i . e . , link - level statistics ) .This problem remains particularly challenging when dealing with large - scale networks since the number of possible states expands exponentially with the length of the network 2 . In try to overcome these limitations , various approaches have been lately advocated which use particular features of the underlying network 3 , e . g . , sparsity 4 - 6 , symmetry 7 , or regularity 8 .However , most existing techniques assume either complete understanding of the traffic topology 9 - 11 or accurate calculations of the traffic matrix 12 - 14 . Unfortunately , both conclusions may not hold in practice 15 , particularly if we study huge and / or dynamic networks 16 .For instance , in IP - based networks , the exact location of routers never always be determined 17 while the traffic matrix is usually unknown 18 . Moreover , even if the network topology were known , compiling all necessary data would still be impractical because of scalability concerns 19 .Finally , obtaining adequate figures of the traffic . . .",
        "rewrite_text": "We propose a network tomography algorithm capable of reconstructing the internal structure of a network using only one-dimensional (1-D) observations, such as link counts between pairs of nodes. This approach is versatile and applicable to various types of networks, requiring no prior knowledge of their topology or traffic patterns. Our method enables the estimation of the total number of active flows at each node, as well as the volume of content carried by each flow. We validate our results through ongoing simulations utilizing real Internet traffic data. Network tomography has gained significant attention in recent years due to its potential applications in areas such as data security, quality of service, and route management. It involves estimating the internal properties of an organization's state—such as the count of active flows at nodes or the amount of content transferred—by utilizing only external information, namely link-level statistics. This task becomes particularly challenging in large-scale networks, as the number of potential states increases exponentially with network size. To address these challenges, recent methods have leveraged specific characteristics of the underlying networks, including sparsity, symmetry, and regularity. However, most existing methods either presume a complete understanding of the traffic topology or rely on precise traffic matrix computations. In practice, these assumptions can be problematic, especially for large and dynamic networks. For example, in IP-based networks, determining the exact locations of routers is often impractical, and traffic matrices are typically unknown. Even if network topology is available, collecting all the requisite data can be unfeasible due to scalability issues. Ultimately, achieving accurate metrics for traffic remains a complex challenge.",
        "ori-fast-z-score": -0.8723567442899586,
        "water-fast-z-score": 6.899548795747854,
        "rewrite-fast-z-score": 0.6620847108818944
    },
    {
        "original_text": "We propose an algorithm to select the injection point in hybrid networks, which is based on local information and can be implemented with low overheads. The proposed algorithm has two phases. First, it elects one node as the injection point by using only local information. Second, if there are multiple nodes that have been elected as the injection points, then these nodes will negotiate among themselves to determine exactly one injection point. We prove that our algorithm always selects at least one node as the injection points when all nodes are reachable from each other. Furthermore, we show through simulations that our algorithm performs well even under various network conditions such as dynamic topologies or limited communication ranges. Keywords: Distributed system, Localization, Self-organization, Wireless sensor network (WSN), Data dissemination, Energy efficiency, Routing protocol, Location-based routing, Clustering, Load balancing, Traffic engineering, Internet service provider",
        "watermark_text": "We suggest an algorithm to select the injection point in hybrid networks , which is based on local information and can be deployed with minimum overheads . The proposed algorithm has two phases .First , it elects one node as the injection center by using only local information . Second , if there are multiple nodes that have been chosen as the injection points , then these nodes will agree among themselves to select exactly one injection point .We showed that our algorithm often selects at least one node as the injection points when all nodes are reachable from each other . Furthermore , we demonstrated through simulations that our algorithm performs better even under various network conditions such as dynamic topologies or restricted transmission ranges .Keywords : Distributed system , Localization , Self - organization , Wireless sensor channel ( WSN ) , Data dissemination , Energy quality , Routing protocol , Location - based scheduling , Clustering , Load balancing , Traffic management , Internet service provider",
        "rewrite_text": "We propose an algorithm designed for selecting the injection point in hybrid networks. This algorithm relies on local information and is implemented with minimal overhead. It consists of two phases: first, it designates one node as the injection center based solely on local data; second, if multiple nodes are identified as potential injection points, these nodes will collaborate to choose a single injection point. Our findings indicate that the algorithm successfully identifies at least one injection point when all nodes are interconnected. Moreover, simulations have shown that our algorithm outperforms others, maintaining effectiveness across various network conditions, including dynamic topologies and limited transmission ranges. \n\nKeywords: Distributed systems, Localization, Self-organization, Wireless Sensor Networks (WSN), Data dissemination, Energy efficiency, Routing protocols, Location-based scheduling, Clustering, Load balancing, Traffic management, Internet service providers.",
        "ori-fast-z-score": -0.25,
        "water-fast-z-score": 4.0,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We present new high resolution (R = λ/Δλ ~ 20,000) far-ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE), as well as archival Hubble Space Telescope (HST) data for the hot white dwarf central star in the planetary nebula Sh2-216. The FUSE spectrum shows numerous absorption lines due to highly ionized species such as C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. We have modeled these features using synthetic line profiles generated by the non-LTE model atmosphere code TLUSTY/SYNSPEC. Our best-fit models indicate that this star has an effective temperature T eff = 120,000 K, surface gravity log g = 8.0, mass M = 0.6M☉ , radius R = 0.01R☉ , and is surrounded by a shell of material with density n(He II)/n(He I) = 1.5 x 10-3 .",
        "watermark_text": "We use new high resolution ( R = λ / Δλ ~ 20 , 000 ) far - ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer ( FUSE ) , as well as archival Hubble Space Telescope ( HST ) statistics for the cool white dwarf central star in the planetary nebula Sh2 - 216 . The FUSE spectrum displays several absorption patterns due to strongly ionized species such as C IV , N V , O VI , Ne VIII , Mg X , Si XII , S XIV , Ar XVI , Fe XIX , and Ni XXVIII .We have analyzed these characteristics utilizing artificial line profiles generated by the non - LTE model atmosphere code TLUSTY / SYNSPEC . Our best - fitting models suggest that this star has an effective heat T eff = 120 , 000 K , surface gravity log f = 8 . 0 , mass M = 0 . 6M☉ , diameter R = 0 . 01R☉ , and is enclosed by a shell of material with volume n ( He II ) / n ( He I ) = 1 . 5 x 10 - 3 .",
        "rewrite_text": "We conducted an analysis using new high-resolution far-ultraviolet spectra (R = λ / Δλ ~ 20,000) obtained from the Far Ultraviolet Spectroscopic Explorer (FUSE), along with archival Hubble Space Telescope (HST) data, for the central cool white dwarf star in the planetary nebula Sh2-216. The FUSE spectrum reveals several absorption lines corresponding to strongly ionized species, including C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. We analyzed these features using artificial line profiles created by the non-LTE model atmosphere code TLUSTY/SYNSPEC. Our best-fitting models indicate that this star has an effective temperature (T_eff) of 120,000 K, a surface gravity of log f = 8.0, a mass of M = 0.6M☉, a diameter of R = 0.01R☉, and is surrounded by a shell of material with a volume ratio of n(He II)/n(He I) = 1.5 x 10^-3.",
        "ori-fast-z-score": -1.5109662034355793,
        "water-fast-z-score": 3.6055512754639896,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "The Earth s rotation axis is not fixed, but wobbles around its mean position with respect to distant stars (polar motion). The amplitude and phase of this wobble are determined by many factors including tidal forces between the Earth and other bodies such as the Sun and Moon.  Free Core Nutation (FCN) refers to an oscillation of the Earth s spin axis that occurs at a period of about 430 days. This phenomenon has been observed for more than 50 years using ground-based radio telescopes observing the positions of celestial objects near the celestial poles. In this study we use data spanning over 30 years obtained from two observatories located on opposite sides of the globe to investigate how FCN varies spatially across the surface of the Earth. We find that there exists significant differences in both the amplitudes and phases of the FCN signal recorded at each site. These results suggest that the FCN signal may be modulated by local geophysical effects which vary across the Earth s surface.",
        "watermark_text": "The Earth s rotation axis is not fixed , but wobbles around its average position with regard to distant stars ( polar motion ) . The amplitude and phase of this wobble are decided by many processes including tidal forces between the Earth and other bodies such as the Sun and Moon .Free Core Nutation ( FCN ) refers to an oscillation of the Earth s spin axis that happens at a period of about 430 days . This phenomenon has been observed for more than 50 weeks using ground - based radio telescopes observing the places of heavenly objects near the astronomical poles .In this study we utilize evidence covering over 30 years derived from two observatories situated on opposite ends of the globe to examine how FCN varies spatially across the surface of the Earth . We see that there exists significant variations in both the amplitudes and phases of the FCN signal recorded at each site .These data suggest that the FCN response possibly be modulated by regional geophysical influences which varies across the Earth s surface .",
        "rewrite_text": "The Earth's rotational axis is not stationary; instead, it wobbles around an average position in relation to distant stars, a phenomenon known as polar motion. This wobble's amplitude and phase are influenced by various factors, including tidal forces exerted by the Sun and Moon on the Earth. Free Core Nutation (FCN) refers to an oscillation of the Earth's spin axis that occurs over a period of approximately 430 days. This phenomenon has been observed for over 50 weeks using ground-based radio telescopes that track celestial objects near the astronomical poles. In this study, we analyze data spanning more than 30 years from two observatories located at opposite ends of the globe to investigate the spatial variation of FCN across the Earth's surface. Our findings reveal significant differences in both the amplitudes and phases of the FCN signals recorded at each site, indicating that regional geophysical influences may modulate the FCN response differently across the Earth's surface.",
        "ori-fast-z-score": -2.0768805540571886,
        "water-fast-z-score": 4.764608329895903,
        "rewrite-fast-z-score": 0.6108472217815261
    },
    {
        "original_text": "We present an approach to robust multi-cellular developmental design that combines the advantages of both bottom-up and top-down approaches, while avoiding their respective disadvantages. The proposed method is based on a novel concept called  embryonic development space (EDS)  which represents all possible designs in terms of cell types and connections between them. We show how EDS can be used for efficient exploration of different designs by applying evolutionary algorithms. Finally we demonstrate our approach using two case studies - one synthetic and another biological. Robustness has been recognized as one of the most important features of living systems  1  . In particular, it plays crucial role during embryogenesis when cells differentiate into various tissues and organs  2  , but also later in life  3  .\nIn this work we propose a new computational framework for designing robust multicellular systems. Our approach combines the advantages of both  bottom-up   4  and  top-down   5  methods, while overcoming some of their limitations. Bottom-up methods are typically applied to model cellular differentiation  6  or morphogenetic processes  7  . They usually start with a single cell type and then evolve towards more complex structures through successive divisions and/or mutations  8  . Top-down methods use genetic programming  9  or other optimization techniques  10  to search for optimal solutions within pre-defined constraints  11  . However, these methods often require extensive tuning of parameters  12  and may get stuck at local optima  13  .\nOur approach uses a novel concept called  embryo-",
        "watermark_text": "We suggest an proposal to robust multi - cell developmental architecture that combines the advantages of both bottom - up and bottom - down approaches , while eliminating their different disadvantages . The proposed approach is based on a new theory called embryonic development space ( EDS ) which includes all possible designs in terms of gene groups and links between them .We see how EDS can be used for efficient exploration of different designs by using evolutionary techniques . Finally we prove our approach using two case studies - one natural and another biological .Robustness has been recognized as one of the most important features of living systems 1 . In particular , it serves crucial role during embryogenesis when cells transform into various tissues and tissues 2 , but also subsequently in life 3 .In this project we develop a new computational framework for constructing robust multicellular systems . Our solution combines the advantages of both bottom - up 4 and bottom - down 5 methods , while overcoming some of their limitations .Bottom - up methods are typically utilized to model cell differentiation 6 or morphogenetic pathways 7 . They often beginning with a single cell type and then evolve towards more sophisticated types through consecutive divisions and / or variants 8 .Top - down methods using genetic programming 9 or other optimization tools 10 to search for efficient answers within pre - defined constraints 11 . However , these algorithms often use extensive tuned of constraints 12 and may come stuck at local optima 13 .Our concept employs a novel concept called embryo -",
        "rewrite_text": "We propose a robust multi-cell developmental architecture that harnesses the strengths of both bottom-up and top-down methodologies while addressing their respective shortcomings. This approach is grounded in a new theory known as embryonic development space (EDS), which encompasses all potential designs in terms of gene groups and their interconnections. We demonstrate how EDS can facilitate the efficient exploration of various designs through the use of evolutionary techniques. To validate our approach, we present two case studies—one from the natural world and another from the biological sphere. Robustness is recognized as a critical characteristic of living systems, playing an essential role during embryogenesis when cells differentiate into various tissues, as well as throughout later stages of life. In this project, we have developed a novel computational framework aimed at constructing robust multicellular systems. Our solution integrates the benefits of both bottom-up and top-down methods while addressing some of their constraints. Bottom-up methods typically focus on modeling cell differentiation or morphogenetic pathways, starting with a single cell type that evolves into more complex forms through successive divisions or variations. Conversely, top-down methods leverage genetic programming or other optimization tools to identify efficient solutions within preset constraints. However, these algorithms often require extensive tuning of constraints and can become trapped in local optima. Our concept introduces an innovative idea known as embryo-...",
        "ori-fast-z-score": 0.3621429841700741,
        "water-fast-z-score": 9.0,
        "rewrite-fast-z-score": 2.060488785479727
    },
    {
        "original_text": "We present the results on X-ray luminosity, temperature, gas mass fraction and YX (the product of these three quantities) for a sample of 62 galaxy clusters observed with XMM-Newton. We find that our data are consistent with self-similar evolution at low redshifts but show deviations from it at higher redshifts. The deviation is more pronounced when we use the SZ signal to estimate cluster masses instead of using the X-ray temperatures. This may be due to non-thermal pressure support or biases introduced by different selection effects between X-ray and SZ surveys. Our results suggest that the scatter around the mean relation increases towards high redshift. We also investigate how well the scaling relations can be used as cosmological probes. Using the current observational constraints on the Hubble constant H0 = 73 km s-1 Mpc-1 , matter density parameter Omega_m = 0.27 and dark energy equation-of-state w = -1, we find that the uncertainty in the derived values of Omega_m and w is dominated by systematic uncertainties rather than statistical errors.",
        "watermark_text": "We present the results on X - ray luminosity , temperature , gas mass fraction and YX ( the product of these three quantities ) for a sample of 62 galaxy galaxies found with XMM - Newton . We see that our statistics are compatible with self - similar development at low redshifts but display deviations from it at higher redshifts .The deviation is more pronounced when we using the SZ signal to estimate cluster masses rather of using the X - ray temperatures . This might be due to non - cooling stress support or biases created by various selection effects between X - ray and SZ measurements .Our results propose that the scatter around the mean relation rises towards high redshift . We additionally observe how best the scaling relations can be used as cosmological probes .Using the present observational restrictions on the Hubble constant H0 = 73 km s - 1 Mpc - 1 , matter density variable Omega _ m = 0 . 27 and dark energy equation - of - state w = - 1 , we find that the uncertainty in the derived values of Omega _ m and v is dominated by systematic uncertainties rather than statistical mistakes .",
        "rewrite_text": "We present the results regarding X-ray luminosity, temperature, gas mass fraction, and YX (the product of these three quantities) for a sample of 62 galaxy clusters identified using XMM-Newton. Our findings indicate that the statistics align with self-similar evolution at low redshifts; however, deviations become apparent at higher redshifts. These discrepancies are more pronounced when we estimate cluster masses using the SZ signal instead of X-ray temperatures, potentially due to non-cooling timescale support or biases arising from differing selection effects between X-ray and SZ measurements. Our results suggest that the scatter around the mean relationship increases at higher redshifts. Additionally, we explore how these scaling relations can serve as effective cosmological probes. Based on current observational constraints—specifically, a Hubble constant of H0 = 73 km s^-1 Mpc^-1, a matter density parameter of Omega_m = 0.27, and a dark energy equation of state w = -1—we find that the uncertainties in the derived values of Omega_m and v are primarily due to systematic uncertainties rather than statistical errors.",
        "ori-fast-z-score": 0.11396057645963795,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": 1.8888888888888888
    },
    {
        "original_text": "The SNO+ experiment is designed to measure the neutrino fluxes in the energy range between 1 MeV and 20 MeV, with an expected sensitivity at low energies comparable to that achieved by Super-Kamiokande (SK). The measurement will be performed using two detection techniques: charged-current interactions on deuterium nuclei via elastic scattering off electrons; neutral current reactions on carbon nuclei through coherent elastic scattering off neutrons. \n \n In this work we present results for the predicted rates of these processes as well as their uncertainties based on state-of-the-art Standard Solar Models (SSMs) and nuclear cross sections. We also discuss how the experimental data can help to constrain SSM parameters such as the helium abundance YHe or the heavy element abundances Z/X. Finally, we show how the combination of different experiments may allow us to test whether the observed neutrinos are produced mainly in pp-chain fusion reactions or in CNO cycles.",
        "watermark_text": "The SNO + experiment is designed to measure the neutrino fluxes in the power range between 1 MeV and 20 MeV , with an anticipated sensitivity at low energies comparable to that attained by Super - Kamiokande ( SK ) . The measurement will be performed using two detection methods : charged - current interactions on deuterium nuclei via elastic scattering off electrons ; neutral current reactions on carbon atoms through coherent elastic scattering off neutrons .In this research we present results for the expected rates of these mechanisms as well as their uncertainties according on state - of - the - art Standard Solar Models ( SSMs ) and reactor cross sections . We additionally discuss how the empirical data can help to constrain SSM characteristics such as the helium abundance YHe or the heavy atom abundances Z / X .Finally , we show how the combination of different experiments could enable us to test whether the seen neutrinos are produced chiefly in pp - chain fusion events or in CNO cycles .",
        "rewrite_text": "The SNO+ experiment aims to measure neutrino fluxes within the energy range of 1 MeV to 20 MeV, with an expected sensitivity at lower energies comparable to that of Super-Kamiokande (SK). The measurements will utilize two detection methods: charged-current interactions on deuterium nuclei through elastic scattering with electrons, and neutral-current interactions on carbon atoms via coherent elastic scattering with neutrons. In this research, we present anticipated rates for these mechanisms along with their uncertainties, based on advanced Standard Solar Models (SSMs) and reactor cross sections. We also explore how the resulting empirical data can refine the characteristics of SSMs, such as helium abundance (YHe) and heavy element abundances (Z/X). Lastly, we demonstrate how integrating data from various experiments could help determine whether the observed neutrinos predominantly originate from pp-chain fusion events or from CNO cycles.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 5.658135095031152,
        "rewrite-fast-z-score": 2.5
    },
    {
        "original_text": "We present an accurate analytical model for the description of optical precursors in ultrashort laser pulses propagating through dispersive media, which is based on the concept of nonlinear phase modulation by self-phase-modulation (SPM) and cross-phase-modulation (XPM). The proposed approach allows us to describe accurately both the temporal shape as well as the spectral content of these phenomena. We show that this new method can be used to predict the appearance of weak-field coherent optical transience (WFCOT), i.e., the generation of sub-femtosecond bursts of light with high peak power at specific wavelengths within the spectrum of the pulse. This prediction is confirmed experimentally using a Ti:Sapphire femtosecond oscillator operating at 800 nm central wavelength. Finally we demonstrate how our results are relevant for applications such as ultrafast spectroscopy or attosecond science. \n \n Optical precursors have been observed since the early days of ultrafast optics  1–3  . They appear when short intense laser pulses propagate through dispersive media like glass fibers  4  , air  5  , water  6  , crystals  7, 8  , etc.. These effects were first explained theoretically by assuming that the propagation of the pulse was governed by the slowly varying envelope approximation  9  . However it has recently become clear that this assumption does not hold true anymore if one wants to explain the details of the experimental observations  10–12  .\n \nIn order to overcome this limitation several authors have developed more sophisticated models  13–19  . In particular, the so-called generalized nonlinear Schrödinger equation (GNLSE)  20, 21  has proven very useful because it takes into account all orders of dispersion  22  , self-steepening  23  , third-order dispersion  24  , Raman scattering  25  , stimulated Brillouin scattering  26  , self-frequency shift  27  , plasma defocusing  28  , gain saturation  29  , and other higher-order effects  30  . \n \nHowever, despite its successes, there still remain some discrepancies between theory and experiment  31  . For example, the GNLSE predicts that the intensity profile of the precursor should always exhibit a smooth bell-shaped structure  32 ",
        "watermark_text": "We present an accurate analytical explanation for the description of optical precursors in ultrashort laser pulses propagating through dispersive media , which is based on the idea of nonlinear phase modulation by self - phase - modulation ( SPM ) and cross - phase - modulation ( XPM ) . The proposed approach allows us to explain precisely both the temporal shape as well as the spectral content of these phenomena .We suggest that this new method can be used to predict the appearance of weak - field unified optical transience ( WFCOT ) , i . e . , the generation of sub - femtosecond bursts of light with high peak power at defined wavelengths within the spectrum of the pulse . This prediction is reported experimentally utilizing a Ti : Sapphire femtosecond oscillator running at 800 nm central frequency .Finally we prove how our findings are applicable for applications such as ultrafast spectroscopy or attosecond research . Optical precursors have been observed since the early days of ultrafast optics 1 – 3 .They arise when short intense laser messages propagate through dispersive media like glass fibers 4 , air 5 , water 6 , particles 7 , 8 , etc . . These effects were first explained theoretically by assuming that the propagation of the signal was regulated by the slowly varying envelope approximation 9 .However it has recently become clear that this claim does not stand true anymore if one wants to explain the details of the empirical observations 10 – 12 . In try to overcome this limitation many writers have developed more sophisticated models 13 – 19 .In particular , the so - called generalized nonlinear Schrödinger equation ( GNLSE ) 20 , 21 has proven very useful because it takes into consideration all orders of dispersion 22 , self - steepening 23 , fourth - order dispersion 24 , Raman absorption 25 , stimulated Brillouin absorption 26 , self - frequency shift 27 , plasma defocusing 28 , gain saturation 29 , and other higher - order effects 30 . However , despite its successes , there still continue some discrepancies between theoretical and experiment 31 .For instance , the GNLSE predicts that the intensity profile of the precursor should always exhibit a straight bell - shaped structure 32",
        "rewrite_text": "We provide a precise analytical framework for understanding optical precursors in ultrashort laser pulses as they traverse dispersive media. This explanation is grounded in the concepts of self-phase modulation (SPM) and cross-phase modulation (XPM). Our approach effectively accounts for both the temporal characteristics and the spectral composition of these phenomena. We propose that this innovative method can facilitate predictions regarding weak-field unified optical transience (WFCOT), which refers to the generation of sub-femtosecond light bursts with high peak power at specific wavelengths within the pulse spectrum. This prediction is supported by experimental data obtained from a Ti:Sapphire femtosecond oscillator operating at an 800 nm central frequency. Moreover, we demonstrate the applicability of our findings to fields such as ultrafast spectroscopy and attosecond research. Optical precursors have been documented since the inception of ultrafast optics, manifesting when intense laser pulses propagate through various dispersive media, including glass fibers, air, water, and particles. Early theoretical models explained these effects using the slowly varying envelope approximation; however, recent evidence suggests that this approach is insufficient in capturing the intricacies of empirical observations. In response to this challenge, researchers have developed more advanced models, notably the generalized nonlinear Schrödinger equation (GNLSE), which incorporates multiple dispersion effects, self-steepening, higher-order dispersion, and non-linear interactions. Despite its effectiveness, discrepancies between theoretical predictions and experimental results persist, such as the GNLSE's prediction that the intensity profile of the precursor should always display a symmetric bell-shaped structure.",
        "ori-fast-z-score": 2.1053798026662975,
        "water-fast-z-score": 7.888304767988878,
        "rewrite-fast-z-score": -0.3651483716701107
    },
    {
        "original_text": "We present the results of long-term numerical simulations of binary black hole (BBH) evolution, including gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption. We focus on binaries with total mass M = 100-1000M⊙ that evolve through collisional nuclear environments at high redshifts z > 10. Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds. In particular we investigate whether these systems can reach masses above 1000M⊙ before merging within a Hubble time. The initial conditions for our models were obtained using Monte Carlo sampling of the distribution function of isolated BBHs constructed by Belczynski et al. (2010) . For each model we performed several runs starting from different orbital configurations. All calculations were carried out assuming circular orbits. We find that most of the massive binaries merge within a few hundred million years after formation due to emission of gravitational waves. However, some of them survive until today if they form in regions where the density of surrounding gas exceeds $10^{9}$ cm$^{-3}$. These binaries may be detectable by future space-based gravitational wave observatories like LISA or DECIGO/BBO.",
        "watermark_text": "We present the conclusion of long - term numerical simulations of binary dark hole ( BBH ) development , notably gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption . We focus on binaries with total mass M = 100 - [UNK] that develop through collisional nuclear habitats at high redshifts z > 10 .Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds . In particular we investigate whether these systems can reach masses above [UNK] before merging within a Hubble time .The initial conditions for our models were obtained using Monte Carlo analysis of the distribution function of isolated BBHs generated by Belczynski et al . ( 2010 ) .For each model we performed numerous runs beginning from varying orbital locations . All calculations were carried out assuming circular orbits .We see that most of the huge binaries dissolve within a few hundred million months after formed due to emission of gravitational waves . However , some of them remain until today if they exist in areas where the density of neighbouring gas approaches $ 10 ^ { 9 } $ cm $ ^ { - 3 } $ .These binaries may be detectable by future space - based gravity wave observatories like LISA or DECIGO / BBO .",
        "rewrite_text": "We present the findings from long-term numerical simulations of the evolution of binary black holes (BBHs), particularly examining the effects of gravitational radiation reaction and general relativistic phenomena such as frame dragging and tidal disruption. Our focus is on BBHs with a total mass \\( M = 100 - [UNK] \\) that form in collisional nuclear environments at high redshifts \\( z > 10 \\). The primary aim of our research is to understand how BBHs can accumulate mass through accretion during their early evolutionary phases, particularly when they are enveloped by dense gas clouds. Specifically, we explore whether these systems can reach masses exceeding \\( [UNK] \\) before merging within a Hubble time. The initial conditions for our models were derived from a Monte Carlo analysis of the distribution function of isolated BBHs, as generated by Belczynski et al. (2010). For each model, we conducted multiple simulations starting from various orbital configurations, assuming circular orbits throughout. Our results indicate that most of these large binary systems disband within a few hundred million years of formation due to the emission of gravitational waves. However, some remain intact to this day if they are situated in regions where the gas density is approximately \\( 10^9 \\, \\text{cm}^{-3} \\). These binaries could potentially be detected by upcoming space-based gravitational wave observatories such as LISA or DECIGO/BBO.",
        "ori-fast-z-score": 0.5129891760425771,
        "water-fast-z-score": 5.437685266051317,
        "rewrite-fast-z-score": 1.0101525445522108
    },
    {
        "original_text": "The Axion Dark Matter Search (ADMX) experiment is designed to search for dark matter in the form of axions, which are hypothetical particles predicted by theories beyond the Standard Model.  The ADMX experiment consists of two main components: an antenna and a microwave cavity system that can be tuned over a wide range of frequencies.   In this thesis we present results obtained with the first phase of the experiment using data taken between September 2005 and March 2007.    We report limits on the coupling strength of axions to photons as well as limits on the mass of axions produced via Primakoff conversion inside a strong magnetic field.  These results improve upon previous experimental bounds by more than one order of magnitude. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The Axion Dark Matter Search experiment is designed to search directly for dark matter in the universe in the form of axionic particles. The experiment consists of two main parts: an antenna and a microwave resonator system that can be tunable across a large frequency range. In this dissertation I will discuss our recent results from the first stage of the experiment.",
        "watermark_text": "The Axion Dark Matter Search ( ADMX ) experiment is designed to search for black material in the form of axions , which are hypothetical particles expected by theories beyond the Standard Model . The ADMX experiment consists of two principal portions : an antenna and a microwave cavity network that can be tuned over a broad variety of frequencies .In this dissertation we present results derived with the first phase of the program including data taken between September 2005 and March 2007 . We report limits on the interaction strength of axions to photons as well as limits on the mass of axions produced via Primakoff transformation inside a powerful magnetic force .These data improve upon former empirical bounds by more than one order of magnitude . This research was done under the auspices of the U . S Department of Energy by Lawrence Livermore National Laboratory under Contract DE - AC52 - 07NA27344 .The Axion Dark Matter Search experiment is designed to search specifically for black material in the universe in the form of axionic particles . The project consists of two principal parts : an antenna and a microwave resonator system that can be tunable across a large frequency spectrum .In this dissertation I will explore our latest findings from the first phase of the project .",
        "rewrite_text": "The Axion Dark Matter Search (ADMX) experiment aims to detect dark matter in the form of axions, which are theoretical particles predicted by theories beyond the Standard Model. The ADMX setup comprises two main components: an antenna and a microwave cavity system that can be adjusted across a wide range of frequencies. In this dissertation, we present findings from the initial phase of the project, focusing on data collected between September 2005 and March 2007. We provide new limits on the interaction strength between axions and photons, as well as constraints on the mass of axions generated through Primakoff processes within a strong magnetic field. Our results significantly enhance previous empirical limits by over an order of magnitude. This research was conducted under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory, under Contract DE-AC52-07NA27344.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 6.413001035278336,
        "rewrite-fast-z-score": -2.0225995873897262
    },
    {
        "original_text": "We propose that gamma-ray bursts (GRBs) are the result of collisions between primordial black holes and stars in globular clusters, which occur at rates predicted by current models for GRB production.  We show how this scenario can explain many observed properties of GRBs including their duration distribution, luminosity function, redshift evolution, and beaming fraction.   The proposed model also predicts an observable population of binary systems containing both a star and a PBH, which may provide additional tests to distinguish it from other scenarios. Gamma-ray bursts (GRBs; see Figure 1 ) are intense flashes of high-energy radiation lasting only milliseconds up to several minutes  1  . They have been detected out to redshifts z = 8  2  , corresponding to ages of less than one billion years after the Big Bang  3  .\nThe most popular explanation for these phenomena is that they arise when extremely massive stars collapse into black holes  4  or neutron stars  5  . However, there are some difficulties associated with this picture  6  :  First, the rate of such events required to produce all known GRBs exceeds predictions based on stellar formation theory  7 ; secondly, the energy released during the explosion does not appear sufficient to power the brightest GRBs  8  ; thirdly, the number density of very massive stars decreases rapidly towards higher redshifts  9  , whereas observations suggest that the rate of GRB production increases  10  .  Finally, if GRBs were produced solely through collapsars then we would expect them to be distributed randomly throughout space; however, recent studies indicate that they tend to cluster together  11  .\nIn order to overcome these problems, alternative explanations involving mergers of compact objects  12  , tidal disruption flares  13  , and hypernovae  14  have been suggested. In addition,...",
        "watermark_text": "We suggest that gamma - ray clusters ( GRBs ) are the result of collisions between primordial black holes and stars in globular complexes , which occur at levels predicted by current scenarios for GRB development . We see how this situation can describe several observed properties of GRBs notably their duration distribution , luminosity function , redshift development , and beaming fraction .The proposed theory even predicts an observable population of binary systems featuring both a star and a PBH , which would offer additional studies to distinguish it from other scenarios . Gamma - ray bursts ( GRBs ; view Figure 1 ) are intense pulses of high - energy rays lasting only milliseconds up to several seconds 1 .They have been detected out to redshifts z = 8 2 , equivalent to periods of fewer than one billion days after the Big Bang 3 . The most popular reason for these phenomena is that they occur when immensely massive galaxies fall into black holes 4 or neutron galaxies 5 .However , there are some difficulties related with this picture 6 : First , the frequency of such events required to produce all known GRBs increases assumptions based on stellar formation theory 7 ; secondly , the electricity created during the explosion does not appear adequate to power the brightest GRBs 8 ; thirdly , the number density of very huge stars reduces rapidly towards higher redshifts 9 , whereas observations suggest that the frequency of GRB generation rises 10 . Finally , if GRBs were produced solely through collapsars then we would expect them to be spread randomly throughout space ; however , recent studies confirm that they tend to group together 11 .In try to overcome these problems , alternative theories involving mergers of compact objects 12 , tidal disruption flares 13 , and hypernovae 14 have been proposed . In addition , . . .",
        "rewrite_text": "We propose that gamma-ray bursts (GRBs) originate from collisions between primordial black holes (PBHs) and stars within globular clusters, occurring at rates consistent with current GRB formation models. This explanation accounts for several observed characteristics of GRBs, such as their duration distribution, luminosity function, redshift evolution, and beaming fraction. The theory also predicts the existence of observable binary systems containing both a star and a PBH, which could facilitate further research to differentiate this model from others. Gamma-ray bursts (GRBs) are intense emissions of high-energy radiation that last from milliseconds to several seconds. They have been detected at redshifts up to z = 8, representing times less than a billion years after the Big Bang. The most widely accepted explanation for GRBs is their occurrence during the collapse of massive galaxies into black holes or neutron stars. However, this model presents several challenges: first, the frequency of such events needed to account for all known GRBs relies on assumptions derived from stellar formation theories; second, the energy generated by these explosions may not be sufficient to explain the brightest GRBs; third, the density of extremely large stars significantly decreases at higher redshifts, while observations indicate an increase in GRB occurrences; finally, if GRBs solely resulted from collapsars, they would be expected to be uniformly distributed in space, yet recent studies show they tend to cluster together. To address these challenges, alternative hypotheses involving the mergers of compact objects, tidal disruption events, and hypernovae have been proposed. In addition...",
        "ori-fast-z-score": 0.3481553119113957,
        "water-fast-z-score": 8.87796045374059,
        "rewrite-fast-z-score": 1.3525044520011484
    },
    {
        "original_text": "We study the effects of noise on spatially extended systems by using an extension of the concept of nonequilibrium potential (NEP). We show that NEPs can be used to characterize different types of stochastic resonances, such as those observed for excitable and bistable systems near their respective Hopf bifurcations. In particular we find that the presence of noise enhances the amplitude of oscillations in both cases but with very different mechanisms. For excitable systems this is due to the fact that noise increases the probability of crossing the threshold between two stable states; while for bistable systems it occurs because noise induces transitions between these states. Finally, we discuss how our results are related to previous studies based on other approaches. Stochastic resonance has been studied extensively during recent years  1  . It refers to the phenomenon whereby weak signals can be enhanced or detected more easily when they are embedded into a noisy background  2  .\nIn many physical situations, however, one needs to consider not only the effect of external noise sources but also internal fluctuations arising from the dynamics itself  3  . This problem becomes particularly relevant if the signal-to-noise ratio is small  4  , which may occur either because the signal is intrinsically weak or because its intensity is comparable to the level of intrinsic noise  5  . Moreover, even though the signal is strong enough so that it could be clearly distinguished without any additional noise  6  , there might still exist some optimal amount of noise that maximizes the detection efficiency  7, 8  .",
        "watermark_text": "We research the effects of noise on spatially extended systems by using an extension of the idea of nonequilibrium potential ( NEP ) . We see that NEPs can be used to characterize many kinds of stochastic resonances , such as those observed for excitable and bistable systems near their different Hopf bifurcations .In particular we find that the presence of noise enhances the frequency of oscillations in both cases but with very different mechanisms . For excitable systems this is due to the fact that noise changes the probability of reaching the threshold between two stable states ; while for bistable systems it appears because sound induces transitions between these states .Finally , we talk how our findings are related to previous research based on other methods . Stochastic resonance has been studied significantly during recent seasons 1 .It refers to the situation whereby soft signals can be enhanced or detected more easily when they are lodged into a loud background 2 . In many physical conditions , however , one needs to consider not only the impact of external sound sources but also internal fluctuations originating from the dynamics itself 3 .This problem appears particularly relevant if the signal - to - noise proportion is tiny 4 , which may happen either because the signal is intrinsically weak or because its brightness is equal to the level of intrinsic noise 5 . Moreover , even though the signal is strong enough so that it could be obviously differentiated without any additional noise 6 , there might nevertheless exist some optimal level of noise that maximizes the detection efficiency 7 , 8 .",
        "rewrite_text": "We investigate the impact of noise on spatially extended systems by employing an extension of the concept of nonequilibrium potential (NEP). Our findings show that NEPs can effectively characterize various types of stochastic resonance, such as those found in excitable and bistable systems near their respective Hopf bifurcations. Notably, we observe that noise increases the frequency of oscillations in both types of systems, albeit through different mechanisms. In excitable systems, noise affects the likelihood of reaching the threshold between two stable states, while in bistable systems, it facilitates transitions between these states. Additionally, we discuss how our results connect to previous studies using alternative approaches. Stochastic resonance has garnered considerable attention in recent years, referring to the phenomenon where weak signals can be amplified or more readily detected amid a noisy background. However, in many physical scenarios, it is crucial to account for not only external noise sources but also the internal fluctuations arising from the system's dynamics. This issue becomes particularly significant when the signal-to-noise ratio is low, which can occur due to an inherently weak signal or when the signal strength matches the level of intrinsic noise. Furthermore, even in cases where the signal is sufficiently strong to be clearly distinguished without any added noise, there may exist an optimal noise level that enhances detection efficiency.",
        "ori-fast-z-score": 0.38138503569823695,
        "water-fast-z-score": 6.928853368993243,
        "rewrite-fast-z-score": -0.29002094671369905
    },
    {
        "original_text": "The Sloan Digital Sky Survey (SDSS) is an ambitious project to map one quarter of the sky in five photometric bands, and obtain spectra for over 100 million galaxies, quasars, stars, and other celestial objects.  The survey has been completed on two thirds of its original footprint; it will be complete by 2008. This talk describes some of the science that can be done using this data set. It also discusses how astronomers are working together to make use of these vast resources. In particular I describe my own work on studying galaxy clusters as dark matter halos, measuring their masses through gravitational lensing effects, and determining their evolution across cosmic time. Finally, I discuss plans for future surveys which will build upon SDSS s successes. The Sloan Digital Sky Survey is an ambitious project to measure the positions and redshifts of more than 100 million galaxies, quasARS, stars, and other objects in our Universe.",
        "watermark_text": "The Sloan Digital Sky Survey ( SDSS ) is an ambitious project to map one quarter of the heavens in five photometric bands , and collect spectra for over 100 million galaxies , quasars , stars , and other celestial entities . The survey has been completed on two thirds of its initial footprint ; it will be full by 2008 .This discussion describes some of the science that can be performed using this data set . It especially discusses how astronomers are working united to make using of these enormous resources .In particular I outline my own research on studying galaxy galaxies as dark matter halos , measuring their masses through gravity lensing effects , and determining their development across universe time . Finally , I discuss projects for future surveys which will build upon SDSS s successes .The Sloan Digital Sky Survey is an ambitious project to measure the places and redshifts of more than 100 million galaxies , quasARS , stars , and other objects in our Universe .",
        "rewrite_text": "The Sloan Digital Sky Survey (SDSS) is a groundbreaking initiative aimed at mapping a quarter of the sky across five photometric bands and gathering spectral data for over 100 million galaxies, quasars, stars, and other celestial objects. While the survey has completed two-thirds of its initial coverage, the full project is expected to be completed by 2008. This discussion highlights some of the scientific research that can be conducted using this extensive dataset, emphasizing the collaborative efforts of astronomers to leverage these vast resources. I also present my own research on studying galaxies as dark matter halos, including how we measure their masses using gravitational lensing effects and analyze their evolution over cosmic time. Finally, I will discuss future survey projects aimed at building on the achievements of the SDSS. The Sloan Digital Sky Survey represents a monumental effort to measure the positions and redshifts of over 100 million galaxies, quasars, stars, and other objects in our universe.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 6.604006604009906,
        "rewrite-fast-z-score": 1.5650160901149996
    },
    {
        "original_text": "BiFeO3 is an antiferromagnetic insulator with high Curie temperature (TC = 1103 K) that exhibits ferroelectricity at room temperature when doped or under pressure. Here we report the observation of spontaneous magnetization, M(T), and electrical polarization, P(E), simultaneously existing over wide ranges of temperatures up to 300 K in pure BiFeO3 single crystal grown by flux method. The results show that both M(T) and P(E) are nearly independent on each other below TC but they start to decrease rapidly above TC. We also found that the magnitude of M(T) decreases faster than that of P(E). These observations suggest that there exists strong coupling between spin and lattice degrees of freedom in this material. \n \n Keywords: Room-temperature multiferroic materials, BiFeO3, Electric polarization, Magnetic properties, Flux growth technique.",
        "watermark_text": "BiFeO3 is an antiferromagnetic insulator with high Curie temperature ( TC = 1103 K ) that exhibits ferroelectricity at room temperature when doped or under pressure . Here we document the observation of induced magnetization , M ( T ) , and electrical polarization , P ( E ) , independently operating over broad ranges of conditions up to 300 K in pure BiFeO3 single crystal grown by flux technology .The results show that both M ( T ) and P ( E ) are nearly independent on each other below TC but they start to decline rapidly above TC . We additionally found that the magnitude of M ( T ) decreases quicker than that of P ( E ) .These measurements suggest that there exists strong coupling between spin and lattice degrees of liberty in this material . Keywords : Room - temperature multiferroic materials , BiFeO3 , Electric polarization , Magnetic properties , Flux growth technique .",
        "rewrite_text": "BiFeO3 is an antiferromagnetic insulator with a high Curie temperature (TC = 1103 K) that displays ferroelectric properties at room temperature when doped or subjected to pressure. In this study, we report on the observation of induced magnetization, M(T), and electrical polarization, P(E), both functioning independently over a wide range of conditions up to 300 K in pure BiFeO3 single crystals grown by flux technology. Our findings indicate that below TC, M(T) and P(E) exhibit almost no mutual dependence, though both diminish rapidly above TC. Furthermore, we observed that the rate at which M(T) decreases is faster than that of P(E). These results imply a significant coupling between the spin and lattice degrees of freedom in this material. Keywords: Room-temperature multiferroic materials, BiFeO3, electric polarization, magnetic properties, flux growth technique.",
        "ori-fast-z-score": 0.14285714285714285,
        "water-fast-z-score": 3.857142857142857,
        "rewrite-fast-z-score": 0.6868028197434451
    },
    {
        "original_text": "We present the results of an investigation into the physical properties of weak Mg II absorbers (WAs) in quasar spectra, using high-resolution spectroscopy and photoionization modeling. We find that WAs are typically associated with galaxies having stellar masses M* ~ 10^10 to 10^11 solar masses, star formation rates SFR = 0.1-10 Msun/yr, and metallicities Z = 0.2 - 1 times solar metallicity. The majority of these systems have low ionization parameters log U < -2.5, indicating they are likely dominated by diffuse gas rather than dense clouds. These findings suggest that WAs may be related to galactic winds driven out of star-forming regions. In addition we find evidence for two distinct populations of WAs; one population is characterized by relatively strong absorption lines which appear to arise primarily within galaxy halos, while another population has weaker absorption features which seem to originate closer to the central region of their host galaxies.",
        "watermark_text": "We present the conclusion of an research into the physical properties of weak Mg II absorbers ( WAs ) in quasar spectra , using high - resolution spectroscopy and photoionization mapping . We see that WAs are typically associated with galaxies having stellar masses M * ~ 10 ^ 10 to 10 ^ 11 solar masses , star formation rates SFR = 0 . 1 - 10 Msun / yr , and metallicities Z = 0 . 2 - 1 twice solar metallicity .The majority of these systems have lowest ionization factors log U < - 2 . 5 , showing they are likely dominated by diffuse plasma rather than dense clouds . These studies imply that WAs might be due to galactic winds driven out of galaxy - creating areas .In addition we find proof for two separate populations of WAs ; one community is characterized by relatively strong absorption patterns which appear to arise principally within star halos , while another population has less absorption elements which appear to arise farther to the inner region of their host galaxies .",
        "rewrite_text": "We conclude our research on the physical properties of weak Mg II absorbers (WAs) found in quasar spectra, utilizing high-resolution spectroscopy and photoionization mapping. Our findings indicate that WAs are generally linked with galaxies that have stellar masses ranging from M* ~ 10^10 to 10^11 solar masses, star formation rates (SFR) between 0.1 and 10 M☉/yr, and metallicities (Z) between 0.2 and 1 times solar metallicity. Most of these systems exhibit low ionization parameters, with log U < -2.5, suggesting they are primarily influenced by diffuse plasma rather than dense clouds. These observations imply that WAs may be associated with galactic winds expelled from star-forming regions. Furthermore, we identify two distinct populations of WAs: one group is characterized by relatively strong absorption features, predominantly arising within star halos, while the other group displays weaker absorption signatures emanating from the inner regions of their host galaxies.",
        "ori-fast-z-score": -0.47809144373375745,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 1.099524999206747
    },
    {
        "original_text": "We present an algorithm to find galaxy groups using photometric redshifts, which is based on the Voronoi tessellation technique (VT). The VT method has been widely used for identifying clusters of galaxies with spectroscopic redshifts but it had not yet been applied to identify galaxy groups with photometric redshifts. We use the Sloan Digital Sky Survey data release five (SDSS DR5) as our input sample. Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available. In this work we have identified more than 12000 galaxy groups at 0 < z < 0.3. These groups contain about 30000 member galaxies. We also provide a catalog containing basic information such as positions, magnitudes, colors, and photometric redshifts for all these groups. \n \n Keywords: Galaxy Group, Photometric Redshift",
        "watermark_text": "We present an algorithm to find galaxy groups using photometric redshifts , which is based on the Voronoi tessellation method ( VT ) . The VT method has been widely using for finding clusters of stars with spectroscopic redshifts but it had not already been used to identify galaxy groups with photometric redshifts .We use the Sloan Digital Sky Survey data release five ( SDSS DR5 ) as our input data . Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available .In this research we have discovered more than 12000 star groups at 0 < z < 0 . 3 . These groups contain about 30000 member galaxies .We additionally offer a list containing essential information such as positions , magnitudes , colors , and photometric redshifts for all these groups . Keywords : Galaxy Group , Photometric Redshift",
        "rewrite_text": "We introduce an algorithm for identifying galaxy groups utilizing photometric redshifts, which is grounded in the Voronoi tessellation (VT) method. Although the VT method has been extensively employed to discover clusters of stars with spectroscopic redshifts, its application in identifying galaxy groups using photometric redshifts has been unprecedented. For our analysis, we utilize data from the Sloan Digital Sky Survey data release five (SDSS DR5). Our findings demonstrate that the VT method is effective in detecting galaxy groups, even when only photometric redshifts are accessible. In this study, we have uncovered over 12,000 star groups within the redshift range of 0 < z < 0.3, comprising approximately 30,000 member galaxies. Furthermore, we provide a comprehensive list containing key information such as positions, magnitudes, colors, and photometric redshifts for all identified groups. Keywords: Galaxy Group, Photometric Redshift.",
        "ori-fast-z-score": -1.2602520756252087,
        "water-fast-z-score": 3.2206441932644223,
        "rewrite-fast-z-score": 1.2362450755382013
    },
    {
        "original_text": "In this work, we consider a multiple-input-multiple-output (MIMO) system where each transmitter has limited feedback information about its channel state to the receiver. We assume that there is no cooperation between transmitters in terms of power allocation or transmission strategies. Each transmitter can only adjust its own transmit power level based on local CSI knowledge at the transmitter side. In addition, each transmitter may turn off its radio completely when it does not have any data to send. The objective is to maximize the sum rate by optimizing both the power control policy as well as the transmission strategy for all users simultaneously under these constraints. \n \n First, we derive an upper bound on the achievable sum-rate using finite-rate feedback assuming Gaussian codebooks. Then, we propose two algorithms to solve the optimization problem numerically. Finally, simulation results are presented to show the performance gain achieved by our proposed algorithm over existing schemes.",
        "watermark_text": "In this research , we study a multiple - input - multiple - output ( MIMO ) scheme where each antenna has restricted feedback info about its channel state to the receiver . We assume that there is no cooperation between transmitters in terms of power distribution or transmission strategies .Each antenna can only alter its own transmit energy level based on local CSI information at the transmitter side . In addition , each antenna could switch off its broadcast completely when it does not have any info to carry .The goal is to maximize the sum frequency by optimizing both the power control strategy as well as the transmission strategy for all users simultaneously under these requirements . First , we derive an upper bound on the achievable sum - speed using finite - frequency feedback assuming Gaussian codebooks .Then , we propose two strategies to solve the algorithms issue numerically . Finally , simulation data are presented to see the performance gain achieved by our proposed algorithm over existing strategies .",
        "rewrite_text": "In this study, we explore a multiple-input multiple-output (MIMO) system in which each antenna possesses limited feedback regarding its channel state to the receiver. We work under the assumption that the transmitters do not collaborate in terms of power distribution or transmission strategies. Each antenna can only adjust its own transmission power based on local Channel State Information (CSI) available at its end. Moreover, if an antenna lacks data to transmit, it has the option to turn off its transmission entirely. Our objective is to maximize the total frequency by optimizing both power control and transmission strategies for all users concurrently within these constraints. Initially, we establish an upper limit on the achievable sum rate using finite frequency feedback with Gaussian codebooks. Subsequently, we introduce two numerical strategies to address the algorithmic challenges. Finally, we provide simulation results to demonstrate the performance improvements offered by our proposed algorithm compared to existing methods.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 6.114295984380816,
        "rewrite-fast-z-score": 1.585187847802434
    },
    {
        "original_text": "The vortex dynamics is studied numerically for fractal cluster structure (FCCS) superconductor by solving time-dependent Ginzburg-Landau equations under an external magnetic field. The FCCS has been proposed as one possible candidate to explain the origin of high-Tc cuprates, and it consists of randomly distributed clusters which are connected each other via Josephson coupling. We find that the resistivity increases rapidly when the applied current exceeds some threshold value Ic(H), where H denotes the strength of the external magnetic field. This behavior can be understood by considering the motion of vortices inside the clusters. In addition, we show that the critical current density Jc decreases gradually with increasing temperature T . Finally, we discuss how these results may be relevant to experiments on high-Tc cuprate superconductors. PACS: 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k",
        "watermark_text": "The vortex dynamics is studied numerically for fractal cluster structure ( FCCS ) superconductor by modeling period - dependent Ginzburg - Landau equations under an external magnetic force . The FCCS has been proposed as one possible candidate to explain the origin of high - Tc cuprates , and it consists of randomly distributed clusters which are connected each other via Josephson coupling .We see that the resistivity increases quickly when the introduced current reaches some threshold factor Ic ( H ) , where H represents the strength of the external magnetic force . This phenomenon can be understood by analyzing the movement of vortices inside the clusters .In addition , we show that the critical current density Jc falls gradually with rising heat T . Finally , we talk how these results may be applicable to experiments on high - Tc cuprate superconductors .PACS: 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k",
        "rewrite_text": "The numerical study of vortex dynamics in fractal cluster structure (FCCS) superconductors is conducted by simulating periodic Ginzburg-Landau equations under an external magnetic field. The FCCS model has been suggested as a potential explanation for the origins of high-temperature cuprates, characterized by randomly distributed clusters interconnected through Josephson coupling. Our findings indicate that resistivity increases sharply once the applied current exceeds a critical threshold Ic(H), where H denotes the intensity of the external magnetic field. This behavior can be elucidated by examining the movement of vortices within the clusters. Furthermore, we observe that the critical current density Jc gradually decreases as temperature T rises. Lastly, we discuss the implications of these results for experiments involving high-temperature cuprate superconductors. PACS: 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k.",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 5.252257314388902,
        "rewrite-fast-z-score": 1.0504514628777804
    },
    {
        "original_text": "The Kuiper belt is the source region for most short period comets, but its formation remains poorly understood. We present results from N-body simulations that show how collisions among planetesimals in Neptune s feeding zone can produce objects with orbits similar to those observed today. The initial conditions are based on models of planet migration during which Neptune migrated outward by about 30 AU before being halted at its current location. Our calculations suggest that the Kuiper belt formed as a result of collisional grinding between bodies whose sizes were comparable to Pluto (r ~ 1000 km). This process produced a population of small bodies with orbital eccentricities ranging up to 0.3. Subsequent encounters with Neptune caused some of these bodies to be scattered into highly eccentric orbits. These results provide an explanation for why there appears to be no correlation between the size distribution of KBOs and their orbital eccentricity.",
        "watermark_text": "The Kuiper belt is the origin region for most short period comets , but its formation appears poorly known . We publish results from N - bodies simulations that demonstrate how collisions among planetesimals in Neptune s feeding area can generate objects with orbits similar to those observed currently .The initial conditions are based on estimates of planet migration during which Neptune migrated outward by about 30 AU before being stopped at its current site . Our calculations suggest that the Kuiper belt developed as a outcome of collisional grinding between bodies whose sizes were analogous to Pluto ( r ~ 1000 kilometers ) .This process produced a population of tiny bodies with orbital eccentricities ranging up to 0 . 3 . Subsequent experiences with Neptune resulted some of these objects to be scattered into extremely eccentric orbits .These data provide an reason for why there seems to be no correlation between the size distribution of KBOs and their orbital eccentricity .",
        "rewrite_text": "The Kuiper Belt serves as the source region for the majority of short-period comets, yet its formation process remains poorly understood. In our study, we present findings from N-body simulations that illustrate how collisions among planetesimals in the vicinity of Neptune can create objects with orbits resembling those currently observed. The initial conditions are informed by estimates of planetary migration, during which Neptune moved outward approximately 30 AU before stabilizing in its present location. Our results indicate that the Kuiper Belt formed as a result of collisional grinding among bodies comparable in size to Pluto (around 1,000 kilometers in radius). This interaction led to the creation of a population of small bodies with orbital eccentricities of up to 0.3. Subsequent dynamics involving Neptune scattered some of these objects into highly eccentric orbits. These findings offer an explanation for the observed lack of correlation between the size distribution of Kuiper Belt Objects (KBOs) and their orbital eccentricities.",
        "ori-fast-z-score": -0.9847319278346618,
        "water-fast-z-score": 5.333493587335964,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study the effect of temperature dependent shape anisotropy in an exchange coupled system consisting of two identical uniaxial single domain particles, one being magnetically softer than the other and both having their easy axes parallel to each other. We show that for certain values of the parameters involved there is a significant increase in the coercive field at low temperatures compared to high temperatures. This can be understood by considering the competition between the Zeeman energy barrier due to the applied magnetic field and the thermal activation energy barrier associated with the temperature dependence of the shape anisotropy. \n \n The model we consider consists of two identical spherical particles (with radius R) separated by a distance d along the z-axis. Each particle has its own uniaxial anisotropy constant Ks(T), where T denotes the temperature. In addition, they are also exchange-coupled through a coupling constant J. For simplicity, we assume that the anisotropy constants have the same functional form as given below, \n \n Ks = K1 + K2 tanh -(T/Tc) ,\n \nwhere Tc is some characteristic temperature scale which determines how rapidly the anisotropy changes with temperature.",
        "watermark_text": "We explore the impact of temperature dependent shape anisotropy in an exchange coupled system consisting of two different uniaxial single domain particles , one being magnetically softer than the other and both having their easy axes aligned to each other . We see that for particular values of the variables required there is a substantial rise in the coercive field at low temperatures relative to low temperatures .This can be understood by analyzing the competition between the Zeeman power barrier resulting to the introduced magnetic force and the thermal activation energy barrier associated with the temperature dependence of the shape anisotropy . The model we define consists of two equal spherical atoms ( with diameter R ) connected by a length d along the z - axis .Each particle has its own uniaxial anisotropy constant Ks ( T ) , where T denotes the temperature . In addition , they are also exchange - coupled through a coupling constant J .For simplicity , we suppose that the anisotropy constants have the same functional shape as provided below , Ks = K1 + K2 tanh - ( T / Tc ) , where Tc is some characteristic temperature scale which determines how swiftly the anisotropy changes with temperature .",
        "rewrite_text": "We investigate the effects of temperature-dependent shape anisotropy in an exchange-coupled system composed of two distinct uniaxial single-domain particles, with one being magnetically softer than the other and both having aligned easy axes. Our findings show that for specific values of the relevant parameters, there is a significant increase in the coercive field at low temperatures compared to higher temperatures. This phenomenon can be explained by examining the interplay between the Zeeman energy barrier, which arises from the applied magnetic force, and the thermal activation energy barrier that is influenced by the temperature-dependent shape anisotropy. The model we propose includes two identical spherical particles with a diameter of R, connected by a distance d along the z-axis. Each particle possesses its own temperature-dependent uniaxial anisotropy constant, Ks(T). Furthermore, the particles are exchange-coupled with a coupling constant J. For ease of analysis, we assume that the anisotropy constants share a similar functional form as shown below: Ks = K1 + K2 tanh(-T / Tc), where Tc represents a characteristic temperature that indicates how rapidly the anisotropy changes with temperature.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 5.590169943749474,
        "rewrite-fast-z-score": 0.5423261445466404
    },
    {
        "original_text": "We present an exact quantum-mechanical treatment for the dynamics of open systems in which the system is coupled to many harmonic oscillators representing its surrounding environment.  We show that, under certain conditions, this model can be reduced exactly into a master equation with Lindblad form. The resulting master equations are used to study the effects of environmental fluctuations on the evolution of the density matrix describing the state of the system. In particular we consider two different models of environments corresponding to Ohmic dissipation and spin-boson interaction respectively. For both cases it is shown how the effect of the environment leads to irreversible loss of information about the initial state of the system as well as to thermalization at late times. Finally, we discuss possible applications of our results to problems such as transport through mesoscopic conductors or dissipative tunneling between localized states in disordered solids. Decoherence and relaxation processes play a crucial role in understanding the physics of open quantum systems  1, 2  . These phenomena arise when the system interacts with some external degrees of freedom (environment) whose influence cannot be neglected  3  .\nIn recent years there has been considerable interest in developing theoretical methods capable of treating these effects beyond the perturbative regime  4  . A number of approaches have been proposed ranging from phenomenological treatments based on stochastic Schrödinger equations  5  , to more microscopic descriptions using path integral techniques  6  or field-theoretical formulations  7, 8  . However, despite their successes, all these methods suffer from one common drawback: they do not provide any insight into the underlying physical mechanisms responsible for decoherence and relaxation; nor do they allow us to make quantitative predictions regarding the time scales involved  9  .\nRecently, several authors  10 -12  have suggested that the problem may be tackled within the framework of quantum mechanics itself. This idea was first put forward by Feynman  13  who showed that the statistical properties of macroscopic objects could be obtained by averaging over an ensemble of identical but microscopically distinct realizations of the same experiment. More recently, Leggett  14  introduced a method...",
        "watermark_text": "We present an precise quantum - mechanical explanation for the dynamics of open systems in which the system is linked to many harmonic oscillators describing its surrounding environment . We see that , under certain conditions , this description can be reduced exactly into a master equation with Lindblad form .The resulting master equations are using to study the effects of environmental fluctuations on the evolution of the density graph explaining the state of the system . In particular we investigate two different models of environments corresponding to Ohmic dissipation and spin - boson collision respectively .For both cases it is demonstrated how the impact of the surroundings leads to irreversible loss of information about the first state of the system as well as to thermalization at late times . Finally , we explain possible applied of our findings to problems such as transport through mesoscopic conductors or dissipative tunneling between localized states in disordered solids .Decoherence and relaxation processes take a crucial role in understanding the physics of open quantum systems 1 , 2 . These phenomena arise when the system interacts with some external degrees of autonomy ( surroundings ) whose influence cannot be forgotten 3 .In recent years there has been substantial interest in pursuing theoretical methods suitable of addressing these phenomena beyond the perturbative regime 4 . A variety of methods have been proposed ranging from phenomenological treatments based on stochastic Schrödinger coefficients 5 , to more microscopic descriptions using path integral methods 6 or field - theory formulations 7 , 8 .However , despite their successes , all these algorithms suffer from one common drawback : they do not offer any insight into the fundamental physical mechanisms involved for decoherence and relaxation ; nor do they allow us to make quantitative predictions regarding the period scales involved 9 . Recently , various scientists 10 - 12 have suggested that the issue may be tackled within the framework of quantum mechanics itself .This idea was first put forward by Feynman 13 who demonstrated that the empirical qualities of macroscopic objects may be obtained by averaging over an ensemble of different but microscopically different realizations of the same experiment . More recently , Leggett 14 proposed a technique . . .",
        "rewrite_text": "We provide a precise quantum-mechanical framework for understanding the dynamics of open systems connected to numerous harmonic oscillators that model their surrounding environment. Our analysis shows that, under specific conditions, this framework can be accurately reduced to a master equation in Lindblad form. The resulting master equations are employed to examine how environmental fluctuations influence the evolution of the density matrix, which represents the system's state. In particular, we explore two distinct environmental models: one representing Ohmic dissipation and the other corresponding to spin-boson interactions. In both scenarios, we demonstrate that the influence of the surrounding environment results in an irreversible loss of information about the system's initial state, leading to thermalization over time. Furthermore, we discuss potential applications of our findings to issues such as transport in mesoscopic conductors or dissipative tunneling between localized states in disordered solids. Understanding decoherence and relaxation processes is essential for grasping the physics of open quantum systems. These phenomena occur when the system interacts with external degrees of freedom (the environment), whose effects are significant. Recently, there has been increased interest in exploring theoretical approaches that address these phenomena beyond the perturbative regime. A range of methodologies has been proposed, from phenomenological models based on stochastic Schrödinger equations to more microscopic treatments using path integral methods or field-theoretical approaches. However, despite their successes, these methods share a common limitation: they fail to provide insights into the fundamental physical mechanisms behind decoherence and relaxation, and they do not facilitate quantitative predictions regarding the timescales involved. Recently, several researchers have suggested that this challenge might be addressed within the framework of quantum mechanics itself. This notion was originally proposed by Feynman, who illustrated that the observable properties of macroscopic objects could be derived by averaging over an ensemble of microscopically distinct realizations of the same experiment. More recently, Leggett introduced a technique…",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.905694150420948,
        "rewrite-fast-z-score": 2.2861904265976327
    },
    {
        "original_text": "We present new results on the properties and evolution of early-type galaxies (ETGs) in the Coma cluster, based on deep HST/ACS imaging data obtained as part of our ongoing survey for faint globular clusters associated with ETGs.  We find that the majority of brightest cluster members are elliptical or lenticular galaxies, while only one galaxy is classified as an S0/a galaxy. The fraction of S0s increases towards fainter luminosities, reaching about 50% at M V = −18 mag. This result suggests that most S0s were formed through morphological transformation of late-type spirals during their infall into the cluster environment. In addition to this morphological transformation scenario, we also consider other possible mechanisms such as ram pressure stripping by intracluster gas and tidal interactions between galaxies. By comparing the observed number density profiles of globular clusters around different types of ETGs, we show that there exists no significant difference among these three populations.",
        "watermark_text": "We report new data on the properties and evolution of early - class objects ( ETGs ) in the Coma cluster , using on deep HST / ACS scanning data acquired as part of our ongoing search for faint globular galaxies involved with ETGs . We see that the majority of brightest cluster elements are elliptical or lenticular galaxies , while only one galaxy is categorized as an S0 / a galaxy .The percentage of S0s increases towards fainter luminosities , increasing about 50 % at M V = −18 mag . This result suggests that most S0s were created through morphological transformation of late - class spirals during their infall into the cluster environment .In addition to this morphological transformation situation , we also consider other possible processes such as ram temperature stripping by intracluster gas and tidal interactions between galaxies . By matching the reported number density characteristics of globular complexes around different kinds of ETGs , we prove that there exists no major variation among these three communities .",
        "rewrite_text": "We present new findings on the properties and evolution of early-type galaxies (ETGs) within the Coma cluster, utilizing deep HST/ACS imaging data obtained as part of our ongoing investigation into faint globular galaxies associated with ETGs. Our observations reveal that the majority of the brightest cluster members are elliptical or lenticular galaxies, with only one classified as an S0/a galaxy. Notably, the fraction of S0 galaxies increases among fainter luminosities, rising by approximately 50% at M_V = -18 mag. This trend indicates that many S0s likely originated from the morphological transformation of late-type spirals as they fell into the cluster environment. In addition to this morphological evolution, we explore other potential mechanisms such as ram pressure stripping by intracluster gas and tidal interactions among galaxies. Through a comparison of the number density characteristics of globular clusters surrounding various types of ETGs, we find no significant differences among these three groups.",
        "ori-fast-z-score": -1.4770978917519928,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": 0.12216944435630522
    },
    {
        "original_text": "En este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una solución exacta del campo gravitacional generado por un cuerpo esférico con simetría axial, que corresponde al caso más simple de agujero negro no rotante. Se muestra que los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmológicas involucradas. Además, se presenta una nueva clase de soluciones exactas para el problema de Einstein-Klein-Gordon en espacios homogéneos e isótropos. Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energía escalar. Finalmente, se discute brevemente cómo estas soluciones pueden ser utilizadas como fuentes de radiación gravitatoria coherente. En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano  en el espacio-tiempo descrito por una solución exacta correspondiente al campo gravitacional generado por una esfera con simetría axial. Esta solución corresponde al caso más sencillo posible de agujero negro sin rotación. Mostramos que estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmológica involucradas.  También presentamos una nueva clase de soluciones exáctas para el problema de Klein-Gordon-Einstein en espacios homogéneos isotrópicos. Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energía escalara. Por último discutimos brevemente cómo estas soluciones pueden utilizarse como fuentes de radiación gravitacional coherente.",
        "watermark_text": "En este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una solución exacta en campo gravitacional generado por un cuerpo esférico con simetría axial , se corresponde un caso más simple de agujero negro no rotante . Se muestra que los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmológicas involucradas .Además , se presenta una nueva clase de soluciones exactas en su problema de Einstein - Klein - Gordon en espacios homogéneos e isótropos . Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energía escalar .Finalmente , se discute brevemente cómo estas soluciones pueden ser utilizadas como fuentes de radiación gravitatoria coherente . En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano en el espacio - tiempo descrito por una solución exacta correspondiente un campo gravitacional generado por una esfera con simetría axial .Esta solución corresponde un caso más sencillo posible de agujero negro sin rotación . Mostramos para estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmológica involucradas .También presentamos una nueva clase de soluciones exáctas en su problema de Klein - Gordon - Einstein en espacios homogéneos isotrópicos . Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energía escalara .Por último discutimos brevemente cómo estas soluciones pueden utilizarse como fuentes de radiación gravitacional coherente.",
        "rewrite_text": "En este estudio se analiza la existencia y estabilidad de agujeros de gusano en el espacio-tiempo, basado en una solución exacta que describe un campo gravitacional generado por un cuerpo esférico con simetría axial, lo cual representa un caso más simple de un agujero negro no rotante. Se demuestra que estos agujeros de gusano son estables bajo ciertas condiciones relativas a las constantes cosmológicas involucradas. Además, se introduce una nueva clase de soluciones exactas para el problema de Einstein-Klein-Gordon en espacios homogéneos e isótropos. Estas soluciones se relacionan con ondas escalares estacionarias que se encuentran alrededor de un punto singular donde existe una densidad infinita de energía escalar. Por último, se discute brevemente cómo estas soluciones pueden servir como fuentes de radiación gravitacional coherente.",
        "ori-fast-z-score": -0.9284766908852594,
        "water-fast-z-score": 2.5018511664883785,
        "rewrite-fast-z-score": 0.8164965809277261
    },
    {
        "original_text": "We reply to the comments by A. M. Braden and J. P. Dowling in their Comment  1  . We show that our results are robust against different choices for the fitting range, and we discuss how this is related to the choice of the initial state wave function used in Ref.  2  .\nPACS numbers: 11.10.Wx, 12.20.Ds, 13.25.Gv Quantum tunneling has been studied extensively since its discovery more than half century ago  3  , but it remains an active area of research  4  . In particular, recent experiments have shown that quantum tunneling can be observed even at room temperature  5  . Theoretically, there exist two types of tunneling processes  6  : extrinsic tunneling which occurs when particles move through barriers created by external potentials  7, 8  ; and intrinsic tunneling where particles tunnel between degenerate states without any potential barrier  9  . Intrinsic tunneling plays important roles in many physical systems such as molecular vibrations  10  , nuclear fission  11  , Josephson junctions  12  , Bose-Einstein condensates  13  , and semiconductor superlattices  14  . However, distinguishing intrinsic tunneling from other effects experimentally still poses great challenges  15  .",
        "watermark_text": "We respond to the remarks by A . M . Braden and J . P . Dowling in their Comment 1 . We see that our findings are robust against different decisions for the fitting range , and we explain how this is related to the selection of the first state wave system employed in Ref .2 . PACS scores : 11 . 10 . Wx , 12 . 20 . Ds , 13 . 25 . Gv Quantum tunneling has been studied thoroughly since its discovery more than quarter century ago 3 , but it remains an active area of research 4 .In particular , recent experiments have shown that molecular tunneling can be experienced even at room temperature 5 . Theoretically , there exist two forms of tunneling processes 6 : extrinsic tunneling which occurs when molecules travel through barriers created by external potentials 7 , 8 ; and intrinsic tunneling where ions tunnel between degenerate states without any potential barrier 9 .Intrinsic tunneling performs important roles in different physical structures such as chemical vibrations 10 , nuclear fission 11 , Josephson junctions 12 , Bose - Einstein condensates 13 , and semiconductor superlattices 14 . However , distinguishing intrinsic tunneling from other effects experimentally nevertheless remains big challenges 15 .",
        "rewrite_text": "We address the comments made by A. M. Braden and J. P. Dowling in their Comment 1. Our results are shown to be consistent across various choices for the fitting range, and we clarify how this relates to the selection of the initial state wave system used in Ref. 2. PACS codes: 11.10.Wx, 12.20.Ds, 13.25.Gv. Quantum tunneling has been extensively studied since its discovery over twenty-five years ago, yet it continues to be a vibrant area of research. Recent experiments have demonstrated that molecular tunneling can occur even at room temperature. Theoretically, two types of tunneling processes are recognized: extrinsic tunneling, which takes place when molecules navigate barriers created by external potentials, and intrinsic tunneling, where ions tunnel between degenerate states without any potential barriers. Intrinsic tunneling plays significant roles in various physical phenomena, including chemical vibrations, nuclear fission, Josephson junctions, Bose-Einstein condensates, and semiconductor superlattices. However, experimentally differentiating intrinsic tunneling from other effects remains a substantial challenge.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 5.741963884746346,
        "rewrite-fast-z-score": 0.24618298195866545
    },
    {
        "original_text": "We study the phase behavior and structure of binary mixtures composed of soft repulsive spheres with attractive depletants, which are modeled as hard-spheres that interact only via excluded volume interactions. We find that these systems exhibit rich phase diagrams including gas-liquid coexistence at low temperatures for all compositions studied here (0.25 < f < 0.75), where f is the fraction of particles made up by the smaller species. The liquid-gas binodal lines shift to higher pressures upon increasing the size ratio between the two components. For large size ratios we observe an additional fluid-fluid transition line along which both fluids have similar densities but different structures. This new fluid state has been observed experimentally in colloidal suspensions containing nonadsorbing polymer chains. Our results show good agreement with experimental data on colloid-polymer mixtures over wide ranges of temperature, pressure, and composition. \nI. INTRODUCTIO N\nThe presence of small particles can dramatically affect the properties of larger ones through depletion forces  1  . These effects play important roles in many physical phenomena such as protein crystallization  2  , gelation  3  , and sedimentation  4  .\nDepending on their sizes relative to each other, the mixture may be either miscible or immiscible  5  . In addition, there exist regions of metastability  6  and even multiple phases  7, 8  . A number of theoretical studies  9  -  11  have investigated the effect of depletion attractions on the phase diagram of simple model systems. However, most of them focused on idealized models neglecting hydrodynamic interactions  12  , finite-size effects  13  , polydispersity  14  , and particle shape  15  . Only recently did some authors  16  take into account more realistic features like Brownian motion  17  , electrostatic repulsion  18  , and van der Waals attraction  19  . Despite this progress, it remains difficult to predict the exact location of the critical point  20  due to strong correlations  21  among the particles  22  . Moreover, the influence of depletion forces on the structural  23  and dynamical  24  properties of complex fluids still needs further investigation  25  .\nIn recent years, experiments  26 ",
        "watermark_text": "We research the phase response and shape of binary mixtures consisting of soft repulsive balls with interesting depletants , which are modeled as hard - spheres that interact only via excluded volume interactions . We see that these systems exhibit rich phase diagrams including gas - fluid coexistence at low temperatures for all compositions studied here ( 0 . 25 < f < 0 . 75 ) , where f is the fraction of molecules making up by the smaller species .The gas - gas binodal lines shift to higher pressures upon increasing the height factor between the two parts . For large size ratios we study an additional liquid - fluid transition line along which both gases have equal densities but different structures .This new fluid state has been observed experimentally in colloidal suspensions containing nonadsorbing polymer complexes . Our results show good agreement with experimental evidence on colloid - polymer mixtures over broad ranges of temperature , pressure , and composition .I . INTRODUCTIO N The appearance of tiny particles can dramatically impact the properties of bigger ones through depletion forces 1 . These effects play essential roles in different physical phenomena such as protein crystallization 2 , gelation 3 , and sedimentation 4 .Depending on their sizes comparative to each other , the mixture might be either miscible or immiscible 5 . In addition , there exist zones of metastability 6 and even multiple components 7 , 8 .A variety of theoretical investigations 9 - 11 have explored the impact of depletion attractions on the phase diagram of simple model structures . However , most of them focused on idealized models neglecting hydrodynamic interactions 12 , finite - length effects 13 , polydispersity 14 , and electron shape 15 .Only lately did some writers 16 taking into consideration more realistic characteristics like Brownian movement 17 , electrostatic repulsion 18 , and van der Waals attraction 19 . Despite this progress , it remains impossible to predict the exact location of the critical position 20 due to powerful correlations 21 among the ions 22 .Moreover , the impact of depletion forces on the structural 23 and dynamical 24 properties of complex fluids already requires further investigation 25 . In recent years , observations 26",
        "rewrite_text": "We investigate the phase response and structural characteristics of binary mixtures comprised of soft repulsive particles paired with intriguing depletants, represented as hard spheres that interact solely through excluded volume effects. Our findings reveal that these systems display complex phase diagrams, including gas-fluid coexistence at low temperatures across all examined compositions (0.25 < f < 0.75), where f denotes the proportion of the smaller species. As the height factor between the two components increases, we observe a shift in the gas-gas binodal lines toward higher pressures. For significant size ratios, we also identify an additional liquid-fluid transition line where both gaseous components maintain equal densities while exhibiting distinct structures. This novel fluid state has been experimentally documented in colloidal suspensions containing non-adsorbing polymer complexes. Our results align well with experimental data concerning colloid-polymer mixtures across a wide spectrum of temperature, pressure, and composition.\n\n**Introduction**: The presence of small particles can significantly influence the properties of larger particles through depletion forces. These effects are crucial in various physical phenomena, including protein crystallization, gelation, and sedimentation. Depending on the relative sizes of the components, mixtures can be either miscible or immiscible. Furthermore, there are regions of metastability and even multiple components present. Numerous theoretical studies have examined the effects of depletion attractions on the phase diagrams of simplified model systems. However, many of these studies have relied on idealized models, overlooking critical factors such as hydrodynamic interactions, finite-length effects, polydispersity, and particle shape. Recently, some researchers have begun to incorporate more realistic factors, including Brownian motion, electrostatic repulsion, and van der Waals attraction. Despite these advancements, accurately predicting the exact position of the critical point remains challenging due to strong correlations among ions. Additionally, the impact of depletion forces on the structural and dynamical properties of complex fluids invites further exploration. In recent years, there have been significant observations in this area.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 8.009821015753255,
        "rewrite-fast-z-score": 2.880446093397635
    },
    {
        "original_text": "The detection and study of cosmic rays is one of the main goals for imaging atmospheric Cherenkov telescopes (IACTs). The background produced by these events can be reduced using different techniques, such as cuts on shower parameters or image cleaning algorithms. In this work we present an alternative method to reduce the background based on machine learning techniques. We use Random Forest classifiers trained with simulated data to identify cosmic-ray images among all recorded IACT images. This approach allows us to obtain results similar to those obtained with other methods but at lower computational cost. Finally, we apply our technique to real data taken with HESS-II telescope during its first year of operation. Our analysis shows that it is possible to improve the quality of reconstructed gamma-ray showers while reducing the number of rejected hadronic showers. \n \n Keywords: Machine Learning; Cosmic Ray Identification; Image Cleaning; Gamma-ray Astrophysics; Hadronic Shower",
        "watermark_text": "The observation and investigation of cosmic rays is one of the main goals for imaging atmospheric Cherenkov telescopes ( IACTs ) . The background formed by these events can be reduced use different methods , such as cuts on shower parameters or image cleaning algorithms .In this project we present an additional method to reduce the background based on machine learning techniques . We use Random Forest classifiers trained with simulated evidence to identify cosmic - ray pictures among all collected IACT images .This method enables us to obtain results comparable to those acquired with other methods but at lower mathematical price . Finally , we apply our technique to real information taken with HESS - II telescope during its initial season of operation .Our study shows that it is possible to upgrade the performance of reconstructed alpha - ray showers while reducing the quantity of rejected hadronic showers . Keywords : Machine Learning ; Cosmic Ray Identification ; Image Cleaning ; Gamma - ray Astrophysics ; Hadronic Shower",
        "rewrite_text": "The observation and analysis of cosmic rays is a primary objective of imaging atmospheric Cherenkov telescopes (IACTs). Various techniques, such as applying cuts on shower parameters and implementing image cleaning algorithms, can help mitigate the background created by these events. In this project, we introduce an innovative approach for background reduction using machine learning techniques. Specifically, we employ Random Forest classifiers trained on simulated data to distinguish cosmic-ray images from the numerous IACT images collected. This method yields results that are comparable to those obtained through traditional techniques, but with a lower computational cost. We further validate our approach using real data acquired from the HESS-II telescope during its initial operational season. Our findings demonstrate that it is feasible to enhance the performance of reconstructed cosmic-ray showers while minimizing the number of rejected hadronic showers. \n\nKeywords: Machine Learning; Cosmic Ray Identification; Image Cleaning; Gamma-ray Astrophysics; Hadronic Shower.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 0.5773502691896257
    },
    {
        "original_text": "We present BVRI surface photometry for isolated spiral galaxies in the nearby universe (0.01 < z < 0.1). The sample consists of 12 objects selected by their morphological type, inclination and apparent size to be suitable targets for detailed studies with integral field spectroscopy. We use archival data obtained at the Kitt Peak National Observatory 4m telescope as well as new observations taken during our own observing runs between 2005-2007. Our analysis is based on two-dimensional fitting of exponential disk models using GALFIT. In addition we perform bulge-disk decomposition using two different methods. First, we fit Sérsic profiles to both components simultaneously. Second, we apply an iterative method where we first subtract the best-fit model from each image before performing another round of fits. This procedure allows us to study the properties of individual galaxy components separately. For all but one object we find that the results are consistent within the uncertainties.",
        "watermark_text": "We use BVRI ground photometry for isolated spiral galaxies in the nearby universe ( 0 . 01 < z < 0 . 1 ) . The sample consists of 12 objects chose by their morphological class , inclination and apparent size to be suitable destinations for detailed analyses with integral field spectroscopy .We use archival measurements obtained at the Kitt Peak National Observatory 4m telescope as well as additional observations made during our own observing runs between 2005 - 2007 . Our study is based on two - dimensional fitting of exponential disk maps using GALFIT .In addition we perform bulge - disk transformation use two different methods . First , we fit Sérsic profiles to both components simultaneously .Second , we apply an iterative method where we first subtract the best - fitting model from each map before performing another round of fits . This procedure allows us to study the properties of different universe components separately .For all but one object we find that the results are compatible within the uncertainties .",
        "rewrite_text": "We utilize BVRI ground photometry for isolated spiral galaxies located in the nearby universe (0.01 < z < 0.1). Our sample comprises 12 galaxies selected based on their morphological classification, inclination, and apparent size, making them ideal candidates for comprehensive analyses using integral field spectroscopy. We incorporate archival data from the Kitt Peak National Observatory's 4m telescope, along with additional observations collected during our own observing sessions from 2005 to 2007. Our analysis employs two-dimensional fitting of exponential disk models through GALFIT. Furthermore, we conduct bulge-disk transformations utilizing two distinct methodologies. The first method involves fitting Sérsic profiles to both components simultaneously, while the second employs an iterative approach wherein we first subtract the best-fitting model from each map before performing further fits. This technique enables us to examine the characteristics of various components of the universe in isolation. For all but one galaxy in our sample, we find that the results are consistent within the margins of uncertainty.",
        "ori-fast-z-score": 1.1785113019775793,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 0.9058216273156765
    },
    {
        "original_text": "We study effects of the axial U(1) A anomaly in the two-flavor lattice QCD with Wilson fermions at finite temperature and density by using the Polyakov-loop extended Nambu-Jona-Lasinio (PNJL) model. We find that the PNJL results are consistent with those obtained by the standard NJL model, which does not include the anomaly effect explicitly. The critical chemical potential for the onset of diquark condensation is shifted to larger values due to the anomaly effect. This shift becomes smaller as we increase the current quark mass. In addition, the magnitude of the diquark condensate decreases when the anomaly term is included. \n \n PACS numbers: 11.10.Wx, 12.38.Gc, 14.40.Nd \n \n Introduction: It has been known that the axial U(1) \n \n \n \n Here mq denotes the current quark mass.",
        "watermark_text": "We research effects of the axial U ( 1 ) A anomaly in the two - flavor lattice QCD with Wilson fermions at finite temperature and density by using the Polyakov - loop enhanced Nambu - Jona - Lasinio ( PNJL ) model . We see that the PNJL results are compatible with those achieved by the standard NJL theory , which does not include the anomaly effect explicitly .The vital chemical potential for the emergence of diquark condensation is shifted to larger estimates due to the anomaly effect . This shift gets smaller as we increase the current quark mass .In addition , the magnitude of the diquark condensate reduces when the anomaly word is included . PACS codes : 11 . 10 . Wx , 12 . 38 . Gc , 14 . 40 . Nd Introduction : It has been known that the axial U ( 1 ) Here mq indicates the present quark mass .",
        "rewrite_text": "We investigate the impact of the axial U(1)A anomaly in two-flavor lattice QCD using Wilson fermions at finite temperature and density, applying the Polyakov-loop enhanced Nambu-Jona-Lasinio (PNJL) model. Our findings indicate that the results obtained from the PNJL model are consistent with those derived from the standard NJL theory, which does not explicitly factor in the anomaly. The critical chemical potential for the formation of diquark condensation is found to increase due to the effects of the anomaly, with this increase becoming less pronounced as the current quark mass is raised. Furthermore, the size of the diquark condensate decreases when the anomaly is taken into account. PACS codes: 11.10.Wx, 12.38.Gc, 14.40.Nd. Introduction: It is well-known that the axial U(1)A symmetry... Here, mq refers to the current quark mass.",
        "ori-fast-z-score": -0.4375949744936837,
        "water-fast-z-score": 4.813544719430521,
        "rewrite-fast-z-score": 0.9801960588196068
    },
    {
        "original_text": "The Standard Model (SM) is an extremely successful theory, but it leaves many questions unanswered about physics at very high energies. In particular, there are no known fundamental principles that can explain why the SM has three generations of quarks and leptons with such different masses or how gravity fits into this picture. Theories beyond the Standard Model attempt to address these issues by introducing new particles and/or interactions which may be observed in future experiments.  Supersymmetry (SUSY), for example, introduces partners for all SM fields whose spin differs by one half unit. These partner states have identical gauge quantum numbers as their SM counterparts, so they could mix with them if SUSY were broken at low energy scales. This mixing would lead to deviations from SM predictions for observables like cross sections and decay rates. Many extensions of the Standard Model also predict new phenomena associated with extra dimensions of space-time. For instance, theories based on string/M-theory often contain additional spatial dimensions compactified down to tiny sizes. If these extra dimensions exist, then we should see evidence of their effects through virtual exchange of Kaluza-Klein excitations of gravitons and other particles between SM fields localized on our four-dimensional world-volume.",
        "watermark_text": "The Standard Model ( SM ) is an incredibly successful theory , but it leaves many issues unanswered about physics at very high energies . In particular , there are no available fundamental principles that can describe why the SM has three generations of quarks and leptons with such distinct masses or how gravity fits into this picture .Theories beyond the Standard Model attempt to alleviate these problems by introducing additional particles and / or relationships which would be found in future research . Supersymmetry ( SUSY ) , for example , creates partners for all SM fields whose spin varies by one half unit .These partner states have equal gauge quantum values as their SM counterparts , so they may blend with them if SUSY were breaking at low power scales . This blending would result to deviations from SM predictions for observables like cross sections and decay rates .Many modifications of the Standard Model also predict new concepts associated with extra dimensions of space - time . For instance , theories based on string / M - theory often contain extra spatial dimensions compactified down to small sizes .If these extra dimensions exist , then we should see evidence of their influence through virtual exchange of Kaluza - Klein excitations of gravitons and other particles between SM fields confined on our four - dimensional world - volume .",
        "rewrite_text": "The Standard Model (SM) is a remarkably successful framework, yet it leaves numerous questions unresolved regarding physics at very high energies. Notably, there are no fundamental principles explaining why the SM includes three generations of quarks and leptons with such varied masses or how gravity integrates into the overall picture. Theories that extend beyond the Standard Model seek to address these issues by introducing new particles and relationships that could emerge from future investigations. For instance, Supersymmetry (SUSY) posits the existence of partner particles for all SM fields, differing in spin by half a unit. These partner states share identical gauge quantum values with their SM counterparts, potentially allowing them to merge if SUSY were to break at lower energy scales. This merging could lead to observable deviations from SM predictions in areas such as cross sections and decay rates. Additionally, many extensions of the Standard Model suggest the existence of new ideas related to extra dimensions of spacetime. Theories rooted in string/M-theory, for example, often incorporate additional spatial dimensions that are compactified to tiny scales. If these extra dimensions are real, we should expect to see their effects through the virtual exchange of Kaluza-Klein excitations of gravitons and other particles among SM fields that are confined to our four-dimensional universe.",
        "ori-fast-z-score": 1.2247448713915892,
        "water-fast-z-score": 7.005888539421972,
        "rewrite-fast-z-score": 1.116880781646981
    },
    {
        "original_text": "We present an algorithm that determines whether or not there exists a quantum circuit with a given number of qubits and gates, such that the corresponding time-independent Hamiltonian is realizable by a physical system in which each energy level has at most one excited state.  We also show how to find all possible circuits if they exist. Our results are based on recent work showing that any time-independent Hamiltonian can be written as a sum of commuting projectors onto its eigenstates. This decomposition allows us to reduce the problem of finding a realization of a general time-independent Hamiltonian into several instances of the same problem but restricted to smaller Hilbert spaces. The reduction yields a polynomial-time algorithm when applied recursively. Finally we discuss some applications of our method including determining the minimum depth required for universal adiabatic quantum computers. In this article we consider the following problem: \nGiven a set of n qubits and m two-qubit gates, does there exist a quantum circuit consisting only of these gates whose associated time-independent Hamiltonian is realizable; i.e., it corresponds to a Hermitian operator acting on a finite-dimensional Hilbert space? If so, what is the smallest circuit size needed?\nThe answer to this question will depend on the specifics of the model used to describe the physical system under consideration. For example, in the case where each energy level may have more than one excited state (i.e., degenerate), then no circuit can realize the desired Hamiltonian unless it contains infinitely many gates. On the other hand, if each energy level has exactly one excited state (i..",
        "watermark_text": "We present an algorithm that decide whether or not there exists a quantum circuit with a given number of qubits and gates , such that the associated time - independent Hamiltonian is realizable by a physical system in which each energy level has at most one excited state . We also study how to find all possible circuits if they exist .Our results are based on current work showing that any time - independent Hamiltonian can be written as a sum of commuting projectors onto its eigenstates . This decomposition allows us to reduce the question of finding a realization of a general time - independent Hamiltonian into numerous instances of the same problem but restricted to smaller Hilbert spaces .The reduction gives a polynomial - time algorithm when applied recursively . Finally we explain some applications of our technique including calculating the minimum depth needed for universal adiabatic quantum computers .In this article we investigate the following issue : Given a setting of n qubits and m two - qubit gates , does there exist a quantum circuit consisting only of these gates whose associated time - independent Hamiltonian is realizable ; i . e . , it corresponds to a Hermitian operator acting on a finite - dimensional Hilbert space ? If so , what is the smallest circuit length needed ?The answer to this question will depend on the specifics of the model used to explain the physical system under consideration . For instance , in the case where each energy level may have more than one excited state ( i . e . , degenerate ) , then no circuit can realize the desired Hamiltonian unless it contains infinitely many gates .On the other hand , if each energy level has precisely one excited state ( i . .",
        "rewrite_text": "We introduce an algorithm that determines whether a quantum circuit can be constructed using a specified number of qubits and gates, such that the corresponding time-independent Hamiltonian can be realized by a physical system where each energy level includes at most one excited state. Additionally, we explore methods to identify all potential circuits, should they exist. Our findings build on recent research that demonstrates every time-independent Hamiltonian can be expressed as a sum of commuting projectors onto its eigenstates. This decomposition allows us to transform the challenge of finding a realization for a general time-independent Hamiltonian into multiple instances of the same problem, albeit in reduced Hilbert spaces. By applying this reduction recursively, we can achieve a polynomial-time algorithm. We also discuss several applications of our approach, such as determining the minimum depth required for universal adiabatic quantum computations. In this paper, we address the following question: Given \\(n\\) qubits and \\(m\\) two-qubit gates, does there exist a quantum circuit composed solely of these gates, whose associated time-independent Hamiltonian is realizable, meaning it corresponds to a Hermitian operator acting within a finite-dimensional Hilbert space? Furthermore, we investigate the minimum circuit length necessary for such realization. The answer to this query will vary based on the specifics of the model used to describe the physical system in question. For example, if energy levels can accommodate multiple excited states (i.e., if they are degenerate), a circuit cannot realize the desired Hamiltonian unless it contains infinitely many gates. Conversely, if each energy level has exactly one excited state, then...",
        "ori-fast-z-score": 2.3539293971054818,
        "water-fast-z-score": 6.024948132556827,
        "rewrite-fast-z-score": 0.26620695282483414
    },
    {
        "original_text": "We present an analysis of the alignments between galaxy spins and tidal fields in real space, using data from the Two Mass Redshfit Survey (TMRS). We find that galaxies are preferentially aligned perpendicular to their local tidal field on scales larger than 1 Mpc/h. This alignment is stronger for more massive galaxies at higher redshifts. The observed spin-tide correlation can be explained by the effect of gravitational torques exerted by large-scale structures during the formation process of these galaxies. Our results suggest that this mechanism may play an important role in shaping galactic angular momenta. These findings have implications for understanding how dark matter halos acquire their angular momentum as well as for interpreting observations of cosmic shear statistics. Introduction: Galaxies form within overdense regions of the universe where they experience strong gravitational interactions with other objects such as neighboring galaxies or clusters of galaxies. During the formation process, these interactions induce gravitational torques which affect the orientation of the galactic angular momentum vector. In turn, the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction processes. Therefore, it has been suggested that the shape distribution of galaxies could provide information about the origin of galactic angular momentums (e.g., Catelan & Theuns 1996; Lee et al. 2008) . However, observational studies show conflicting results regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors  positions (see e.g., Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 , for recent works).\nIn order to understand the physical mechanisms responsible for determining the directions of galactic angular momentas, we need to study the statistical properties of galaxy spin distributions over large volumes of the universe. Recent surveys like Sloan Digital Sky Survey (SDSS) allow us to measure galaxy orientations accurately enough to perform such analyses. For example, Lee et al. (2008) used SDSS DR4 data to investigate the alignments between galaxy spin vectors and their nearest neighbor s position angles. They found no",
        "watermark_text": "We present an assessment of the alignments between galaxy spins and tidal fields in real space , using data from the Two Mass Redshfit Survey ( TMRS ) . We see that galaxies are preferentially aligned perpendicular to their nearby tidal field on scales bigger than 1 Mpc / h .This alignment is strengthened for more massive galaxies at higher redshifts . The observed spinning - tide relationship can be described by the impact of gravitational torques exerted by large - scale structures during the formation period of these objects .Our results propose that this mechanism may play an important role in shaping galactic angular momenta . These studies have consequences for studying how dark matter halos acquire their angular velocity as also as for interpreting observations of universe shear trends .Introduction : Galaxies shape within overdense regions of the universe where they encounter strong gravitational interactions with other structures such as nearby galaxies or rows of clusters . During the formation period , these interactions initiate gravitational torques which affect the orientation of the galactic angular velocity tensor .In turn , the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction mechanisms . Therefore , it has been proposed that the form distribution of stars could give information about the origin of galactic angular momentums ( e . g . , Catelan & Theuns 1996 ; Lee et al .2008 ) . However , observational analyses suggest conflicting findings regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors places ( saw e . g . , Faltenbacher et al .2002 ; Bailin et al . 2005 ; Paz et al .2008 ; Codis et al . 2012 , for recent works ) .In order to comprehend the physical mechanisms involved for determining the directions of galactic angular momentas , we require to study the statistical characteristics of galaxy spin distributions over large quantities of the universe . Recent surveys like Sloan Digital Sky Survey ( SDSS ) able us to measure galaxy orientations correctly sufficiently to conduct such analyses .For instance , Lee et al . ( 2008 ) used SDSS DR4 data to examine the alignments between galaxy spin vectors and their closest neighbor s position angles .They found no",
        "rewrite_text": "We provide an evaluation of the alignments between galaxy spins and tidal fields in real space, utilizing data from the Two Mass Redshift Survey (TMRS). Our findings indicate that galaxies tend to align perpendicularly to their nearby tidal fields on scales larger than 1 Mpc/h. This alignment is more pronounced in more massive galaxies at higher redshifts. The relationship between spinning galaxies and tidal forces can be attributed to the gravitational torques exerted by large-scale structures during their formation period. Our results suggest that this mechanism could significantly influence the angular momentum of galaxies. These findings have implications for understanding how dark matter halos gain angular velocity and for interpreting trends in cosmic shear observations. \n\nIntroduction: Galaxies form in overdense regions of the universe, where they experience strong gravitational interactions with adjacent structures, such as neighboring galaxies and clusters. During the formation phase, these interactions generate gravitational torques that alter the orientation of the galactic angular velocity tensor. Consequently, the orientation of galactic angular momenta plays a crucial role in shaping galaxies through dynamic friction mechanisms. As such, it has been suggested that the distribution of stellar forms could provide insights into the origins of galactic angular momenta (e.g., Catelan & Theuns 1996; Lee et al. 2008). Nonetheless, observational studies have produced mixed results regarding the existence of a preferred orientation for galaxy spin axes in relation to their neighbors (see, for instance, Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 for recent studies). To understand the physical mechanisms that determine the directions of galactic angular momenta, it is essential to examine the statistical properties of galaxy spin distributions across large volumes of the universe. Recent surveys, such as the Sloan Digital Sky Survey (SDSS), enable us to accurately measure galaxy orientations for such analyses. For example, Lee et al. (2008) utilized SDSS DR4 data to investigate the alignments between galaxy spin vectors and the position angles of their nearest neighbors, and they found no significant correlation.",
        "ori-fast-z-score": -0.8512055557875505,
        "water-fast-z-score": 7.140584836498262,
        "rewrite-fast-z-score": 0.2349781349963872
    },
    {
        "original_text": "We present an equation-of-state (EOS) model for solar matter that is based on the path integral formalism and includes quantum nuclear effects in the form of shell corrections, which are calculated using realistic nuclear interactions. The EOS covers densities ranging from 0 to 1.5 times normal density at temperatures between 10^6 K and 5×10^8 K. We compare our results against those obtained by other authors who used different methods or approximations. Our new EOS agrees well with previous calculations within their respective domains of validity but extends these into previously unexplored regions. In particular we find that the pressure increases more rapidly than predicted by standard models when approaching the center of the Sun. This leads to higher central temperatures and lower radii compared to standard models. These differences may be important for understanding the structure of stars like the Sun as well as for modeling stellar evolution. \n \n Keywords: Solar interior",
        "watermark_text": "We create an equation - of - state ( EOS ) model for solar material that is based on the path integral formalism and incorporates quantum nuclear effects in the form of shell corrections , which are measured using realistic nuclear interactions . The EOS includes densities extending from 0 to 1 . 5 times normal density at levels between 10 ^ 6 K and 5×10 ^ 8 K . We match our findings against those achieved by other researchers who used various methods or approximations .Our current EOS follows well with previous analyses within their different domains of validity but extends these into formerly unexplored regions . In particular we find that the pressure changes more fast than expected by traditional models when approaching the center of the Sun .This leads to higher central temperatures and less radii compared to standard models . These changes may be crucial for knowledge the composition of stars like the Sun as also as for modeling stellar evolution .Keywords: Solar interior",
        "rewrite_text": "We develop an equation-of-state (EOS) model for solar material, utilizing the path integral formalism to incorporate quantum nuclear effects through shell corrections, which are assessed using realistic nuclear interactions. The EOS spans densities from 0 to 1.5 times the typical density, across temperature ranges of 10^6 K to 5×10^8 K. We compare our results with those from other researchers who employed different methodologies or approximations. Our EOS is consistent with prior analyses within their respective domains of validity, while extending the exploration into previously uncharted territories. Notably, we observe that pressure increases more rapidly than traditional models predict as one approaches the Sun's core, resulting in higher central temperatures and smaller radii compared to standard models. These findings may be significant for understanding the composition of stars like the Sun and for modeling stellar evolution. Keywords: Solar interior.",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": -0.7276068751089989
    },
    {
        "original_text": "We present new results on the faint-end slope and evolution of the luminosity function (LF) for optically-selected quasars in the redshift range 0.5 < z < 2.2, based on the VIMOS-VLT Deep Survey (VVDS). We use two different methods to estimate the LF parameters at each redshift bin: 1/Vmax method and maximum likelihood fitting technique. Our best-fit values are obtained by combining these two techniques with Monte Carlo simulations. We find that our data is consistent with previous studies within their uncertainties. However we show that there exists an apparent discrepancy between the observed number density of bright quasars and the predictions made using the standard quasar formation model. This may be due to incompleteness effects or biases introduced during the selection process. In addition, we also investigate the dependence of the LF shape on the optical luminosities of quasars.",
        "watermark_text": "We report new data on the faint - end slope and evolution of the luminosity function ( LF ) for optically - selected quasars in the redshift region 0 . 5 < z < 2 . 2 , using on the VIMOS - VLT Deep Survey ( VVDS ) . We use two different methods to estimate the LF variables at each redshift bin : 1 / Vmax method and greatest probability fitting technique .Our best - fitting values are derived by combining these two strategies with Monte Carlo simulations . We see that our information is compatible with previous research within their uncertainties .However we prove that there exists an apparent discrepancy between the expected number density of bright quasars and the assumptions produced using the standard quasar structure model . This might be due to incompleteness effects or biases created during the selection phase .In addition , we also investigate the relationship of the LF formation on the optical luminosities of quasars .",
        "rewrite_text": "We present new findings on the faint-end slope and evolution of the luminosity function (LF) for optically-selected quasars in the redshift range of 0.5 < z < 2.2, utilizing data from the VIMOS-VLT Deep Survey (VVDS). To estimate LF parameters for each redshift bin, we employ two distinct methods: the 1/Vmax technique and the greatest probability fitting method. Our best-fitting values are obtained by integrating these approaches with Monte Carlo simulations. Our results align with previous studies within their respective uncertainties. However, we identify an apparent discrepancy between the anticipated number density of bright quasars and the predictions generated by the standard quasar structure model, which may stem from selection biases or incompleteness effects. Additionally, we explore the relationship between the formation of the LF and the optical luminosities of quasars.",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 6.713171133426189,
        "rewrite-fast-z-score": 1.632993161855452
    },
    {
        "original_text": "We present results on polarized radio emission in the field surrounding the galaxy cluster Abell 2218, observed with the Australia Telescope Compact Array (ATCA) at 1.4 GHz and 4.8 GHz. We detect 16 compact sources above 5 mJy beam-1 at both frequencies; all but one are unresolved or marginally resolved by our observations. The majority have fractional linear polarization between 10% and 20%, while two objects show higher values up to 40%. All detected sources appear to be associated with galaxies within the central region of Abell 2218. In addition we find evidence for diffuse polarized emission around the brightest member of this galaxy cluster. This is likely due to synchrotron radiation produced by relativistic electrons accelerated in shocks driven into the intracluster medium during multiple mergers that occurred over time scales ranging from 10 Myr to several Gyrs ago. Our data also reveal an extended halo-like structure which surrounds the entire galaxy cluster.",
        "watermark_text": "We report findings on polarized radio emission in the field surrounding the galaxy cluster Abell 2218 , detected with the Australia Telescope Compact Array ( ATCA ) at 1 . 4 GHz and 4 . 8 GHz . We detect 16 compact sources above 5 mJy beam - 1 at both frequencies ; all but one are unresolved or marginally resolved by our observations .The majority have fractional linear polarization between 10 % and 20 % , while two bodies display larger ratings up to 40 % . All detected sources appear to be identified with galaxies within the inner region of Abell 2218 .In addition we find data for diffuse polarized emission around the brightest part of this galaxy cluster . This is probably due to synchrotron emission generated by relativistic electrons accelerated in shocks driven into the intracluster medium during various mergers that occurred over time ranges ranging from 10 Myr to several Gyrs ago .Our data also suggest an extended halo - like structure which covers the entire galaxy cluster .",
        "rewrite_text": "We present findings on polarized radio emission in the vicinity of the galaxy cluster Abell 2218, observed using the Australia Telescope Compact Array (ATCA) at frequencies of 1.4 GHz and 4.8 GHz. Our observations reveal 16 compact sources exceeding 5 mJy beam^-1 at both frequencies; nearly all of these sources are either unresolved or only marginally resolved. Most of the sources exhibit fractional linear polarization in the range of 10% to 20%, while two sources show higher polarization levels reaching up to 40%. All detected sources are associated with galaxies located in the inner region of Abell 2218. Additionally, we report the presence of diffuse polarized emission around the brightest area of the galaxy cluster, likely attributed to synchrotron emission from relativistic electrons that have been accelerated by shocks interacting with the intracluster medium during various mergers that occurred over timescales from 10 million years to several billion years ago. Our findings also indicate the existence of an extended halo-like structure encompassing the entire galaxy cluster.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 4.6475800154489,
        "rewrite-fast-z-score": 0.3721042037676254
    },
    {
        "original_text": "We report on an exceptional flare detected by Swift/XRT at t ~ 1 day post-burst, which lasted for more than 100 ks (~20 hr). The flare was followed up with observations performed with XMM-Newton/EPIC-pn between 2.5 days to 3 months post-burst. We find that this flare is best described as a superposition of two components: one component lasting about 50 ks peaking around 10^-3 s and another component lasting about 70 ks peaking around 5 x 10^4 s. Both components are well fitted by exponentially cut-off power-laws with photon indices Γ = -1.6 ± 0.1 and -2.2 ± 0.3 respectively. No significant spectral evolution during either of these flares has been found. This flare is among the most energetic ever seen in any gamma-ray burst.",
        "watermark_text": "We report on an exceptional burst detected by Swift / XRT at t ~ 1 day post - flare , which occurred for more than 100 ks ( ~ 20 hr ) . The flare was followed up with observations performed with XMM - Newton / EPIC - pn between 2 . 5 weeks to 3 weeks following - flare .We see that this flare is better depicted as a superposition of two parts : one element lasting about 50 ks peaking roughly 10 ^ - 3 s and another component lasting about 70 ks peaking roughly 5 x 10 ^ 4 s . Both components are better fitted by exponentially drop - off power - laws with photon indices Γ = - 1 . 6 ± 0 . 1 and - 2 . 2 ± 0 . 3 respectively . No notable spectral evolution during either of these flares has been determined .This burst is among the most intense yet shown in any gamma - ray flare .",
        "rewrite_text": "We present an extraordinary burst detected by Swift/XRT approximately one day after the flare, which lasted for over 100 ks (around 20 hours). Follow-up observations were conducted using XMM-Newton/EPIC-pn between 2.5 and 3 weeks after the initial flare. Our analysis indicates that this flare is best represented as a combination of two distinct components: one lasting about 50 ks with a peak around 10^-3 seconds, and another lasting approximately 70 ks with a peak near 5 x 10^4 seconds. Both components are more accurately described by exponentially decaying power laws, with photon indices of Γ = -1.6 ± 0.1 for the first and Γ = -2.2 ± 0.3 for the second. We observed no significant spectral evolution during either of the flare components. This burst ranks among the most intense recorded in any gamma-ray flare.",
        "ori-fast-z-score": -0.8320502943378437,
        "water-fast-z-score": 4.900980294098034,
        "rewrite-fast-z-score": 1.0886621079036347
    },
    {
        "original_text": "We study theoretically the influence of longitudinal magnetic fields in coronal loops on the damping rate of small amplitude prominence oscillations. We find that for sufficiently large values of the longitudinal magnetic field, the damping is dominated by resonant absorption and it increases with increasing longitudinal magnetic field strength. For smaller values of the longitudinal magnetic fields we find that the damping is due to phase mixing between Alfvén waves propagating along neighboring flux tubes. In this case the damping decreases as the longitudinal magnetic field becomes weaker. The results are compared with observations made at the Solar Ultraviolet Measurements of Emitted Radiation (SUMER) instrument aboard SOHO spacecraft. It has been known since the early 1980s that many prominences exhibit periodic transverse displacements which can be observed both in Hα images and in EUV lines formed higher up in the atmosphere than Hα . These motions have periods ranging from several minutes down to about one minute and amplitudes typically less than 100 km s-1 .\nTheoretical models suggest that these oscillations may be driven by slow magnetoacoustic waves trapped inside the prominence body or by fast kink modes excited by photospheric flows (see e.g., Oliver & Ballester 1994; Terradas et al. 2002) . However, there is still no consensus regarding what causes them.",
        "watermark_text": "We explore theoretically the impact of longitudinal magnetic fields in coronal loops on the damping rate of tiny intensity prominence oscillations . We see that for enough large values of the longitudinal magnetic field , the damping is dominated by resonant absorption and it changes with increasing transverse magnetic field intensity .For smaller values of the longitudinal magnetic fields we find that the damping is due to phase mixing between Alfvén currents propagating along neighboring flux tubes . In this situation the damping decreases as the longitudinal magnetic force gets smaller .The results are compared with observations made at the Solar Ultraviolet Measurements of Emitted Radiation ( SUMER ) experiment aboard SOHO satellites . It has been known since the early 1980s that several prominences exhibit periodic longitudinal displacements which can be viewed both in Hα images and in EUV bands formed higher up in the atmosphere than Hα .These motions have periods ranging from several seconds down to about one minute and amplitudes typically less than 100 km s - 1 . Theoretical models suggest that these oscillations must be caused by fast magnetoacoustic waves trapped inside the prominence body or by fast kink modes excited by photospheric flows ( see e . g . , Oliver & Ballester 1994 ; Terradas et al .2002 ) . However , there is nevertheless no agreed regarding what causes them .",
        "rewrite_text": "We theoretically investigate how longitudinal magnetic fields in coronal loops affect the damping rate of small intensity oscillations in prominences. Our analysis shows that when the longitudinal magnetic field is sufficiently strong, the damping is primarily governed by resonant absorption, which varies with increasing transverse magnetic field strength. Conversely, when the longitudinal magnetic fields are weaker, damping arises from phase mixing of Alfvén waves traveling through adjacent flux tubes, leading to a decrease in damping as the longitudinal magnetic force weakens. We compare our findings with observations from the Solar Ultraviolet Measurements of Emitted Radiation (SUMER) experiment on the SOHO satellite. Since the early 1980s, it has been observed that several prominences display periodic longitudinal displacements, detectable in Hα images as well as in EUV bands located higher in the solar atmosphere than Hα. These oscillations have periods ranging from several seconds to about a minute, with amplitudes typically below 100 km/s. Theoretical models indicate that these oscillations are likely driven by fast magnetoacoustic waves trapped within the prominence or by fast kink modes induced by photospheric flows (see, for example, Oliver & Ballester 1994; Terradas et al. 2002). However, there is still no consensus on the exact mechanisms responsible for these phenomena.",
        "ori-fast-z-score": 0.7181848464596079,
        "water-fast-z-score": 5.569674729797484,
        "rewrite-fast-z-score": 2.5584085962673253
    },
    {
        "original_text": "We present results on flame evolution during type Ia supernova (SN) explosions, based on two-dimensional hydrodynamic simulations with detailed nuclear reaction networks for both deflagrations and detonations. We find that the transition to detonation is triggered by shock-induced turbulent mixing at densities around 10$^{9}$ g/cm$^3$. The resulting detonation wave propagates through the entire white dwarf star within about 1 s after ignition. In this scenario, the observed light curve can be reproduced if we assume an initial central density of 2 x $10^8$ g/cm$^3$ or higher. For lower values of the central density, the explosion fails to produce enough 56Ni to explain observations. This work was supported by NASA grant NNX10AD03G. Keywords: Flame propagation, Hydrodynamics, Nuclear burning, Supernovae, White dwarfs. Subject headings: Nucleosynthesis",
        "watermark_text": "We report findings on flame evolution during type Ia supernova ( SN ) exploded , using on two - dimensional hydrodynamic simulations with comprehensive nuclear response systems for both deflagrations and detonations . We see that the shift to detonation is caused by shock - caused turbulent mixing at densities around 10 $ ^ { 9 } $ h / cm $ ^ 3 $ .The produced detonation radiation propagates through the entire white dwarf star within about 1 s after explosion . In this situation , the observed light curve can be reproduced if we suppose an initial central density of 2 x $ 10 ^ 8 $ h / cm $ ^ 3 $ or greater .For lower values of the central density , the explosion fails to produce enough 56Ni to explain observations . This research was supported by NASA grant NNX10AD03G .Keywords : Flame transmission , Hydrodynamics , Nuclear burning , Supernovae , White dwarfs . Subject headings : Nucleosynthesis",
        "rewrite_text": "We present our findings on the evolution of flames during type Ia supernovae (SN), based on two-dimensional hydrodynamic simulations with detailed nuclear response mechanisms for both deflagrations and detonations. Our results indicate that the transition to detonation occurs due to turbulent mixing caused by shock waves at densities around 10$^{9}$ h/cm$^{3}$. The resulting detonation radiation spreads throughout the white dwarf star within approximately one second after the explosion. To replicate the observed light curve, we propose that the initial central density must be at least 2 x 10$^{8}$ h/cm$^{3}$ or higher. For central densities below this threshold, the explosion does not generate enough 56Ni to align with observational data. This research was funded by NASA grant NNX10AD03G. Keywords: Flame propagation, Hydrodynamics, Nuclear burning, Supernovae, White dwarfs. Subject headings: Nucleosynthesis.",
        "ori-fast-z-score": -0.2672612419124244,
        "water-fast-z-score": 5.258758927213289,
        "rewrite-fast-z-score": 0.8017837257372732
    },
    {
        "original_text": "We report the observation of dynamic condensation of water vapor on crack tips during slow fracture experiments performed under vacuum conditions (10-6 mbar) and low temperature (77 K). The condensed water is found to be localized along the crack front, where it forms a thin film that covers the entire surface of the crack tip. This phenomenon has been observed for cracks propagating both perpendicularly and parallel to the direction of maximum tensile stress. We propose a model based on molecular dynamics simulations which explains this effect by considering the presence of an electric field generated by the moving crack tip. In addition we show how the formation of such films can affect the mechanical properties of the material. Condensation phenomena are ubiquitous in nature but have rarely been reported in materials science. Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses. These results were obtained using a combination of optical microscopy techniques with environmental scanning electron microscopy (ESEM), Raman spectroscopy and infrared reflection absorption spectroscopy (IRAS).",
        "watermark_text": "We report the observation of dynamic condensation of liquid vapor on break tips during slow fracture studies performed under vacuum environments ( 10 - 6 mbar ) and low heat ( 77 K ) . The condensed water is found to be localized along the crack front , where it creates a thin film that covers the entire surface of the crack tip .This phenomenon has been observed for faults propagating both perpendicularly and parallel to the direction of maximum tensile tension . We suggest a theory based on protein mechanics simulations which explains this effect by assuming the presence of an electric field produced by the moved crack edge .In addition we study how the formation of such films can affect the structural properties of the material . Condensation problems are ubiquitous in nature but have seldom been reported in materials science .Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses . These conclusions were obtained using a combination of optical microscopy methods with environmental scan electron microscopy ( ESEM ) , Raman spectroscopy and infrared reflection detection spectroscopy ( IRAS ) .",
        "rewrite_text": "We present our findings on the dynamic condensation of liquid vapor at the tips of cracks during slow fracture experiments conducted in vacuum conditions (10^-6 mbar) and at low temperatures (77 K). This condensed water is observed to accumulate along the crack front, forming a thin film that blankets the entire surface of the crack tip. This phenomenon occurs in faults propagating both perpendicular and parallel to the direction of maximum tensile stress. We propose a theoretical framework based on protein mechanics simulations, which attributes this effect to an electric field generated by the moving crack edge. Furthermore, we investigate how the formation of these films influences the material's structural properties. While condensation phenomena are commonly found in nature, they are rarely documented in materials science. Our experimental evidence demonstrates that water condenses on crack surfaces as they advance through fused silica glasses. These results were achieved through a combination of optical microscopy, environmental scanning electron microscopy (ESEM), Raman spectroscopy, and infrared reflection absorption spectroscopy (IRAS).",
        "ori-fast-z-score": -0.32539568672798425,
        "water-fast-z-score": 5.1759731137650435,
        "rewrite-fast-z-score": 1.6464638998453551
    },
    {
        "original_text": "We study lattice chiral gauge theories with Wilson fermions in four dimensions, focusing on their phase structure at finite temperature T . We show that there is no spontaneous breaking of parity (P) or time-reversal symmetry (T ) for any value of the bare quark mass m0 if the number Nf of flavors satisfies Nf > 2. This result implies that the theory does not have an order parameter associated to P and/or T , which are spontaneously broken by the standard model. In particular, we find that the spectrum contains two degenerate Dirac fermion species corresponding to left-handed and right-handed quarks, respectively. These fermions can be identified as mirror fermions because they transform into each other under reflection about one spatial axis. The existence of these mirror fermions leads to interesting consequences such as the absence of flavor changing neutral currents mediated by gluons. \n \n Introduction \n \n Chiral gauge theories play important roles both theoretically and phenomenologically. They provide a natural framework for describing low-energy phenomena involving hadrons  1  . On the other hand, it has been suggested recently that some extensions of the Standard Model may contain extra space-time symmetries beyond Poincaré invariance  2  . It would then be very useful to develop techniques to analyze the possible effects of such new symmetries on physical observables  3  .\n \nIn this Letter, we consider a class of chiral gauge theories defined on a Euclidean spacetime lattice  4  . Our main interest lies in studying how the presence of additional discrete symmetries affects the phase diagram of the system. For simplicity, let us first focus on the case where only parity (P), charge conjugation (C), and time reversal (T ) transformations act nontrivially on fields  5  . Then, the action S = d4 x L(U; ψ,ψ) should satisfy the following conditions  6  :",
        "watermark_text": "We research lattice chiral gauge physics with Wilson fermions in four dimensions , concentrating on their phase shape at finite temperature T . We see that there is no premature breaking of parity ( P ) or time - reversal symmetry ( T ) for any value of the bare quark mass m0 if the number Nf of flavors satisfies Nf > 2 .This result means that the model does not have an rank parameter identified to P and / or T , which are spontaneously shattered by the standard description . In particular , we find that the spectrum contains two degenerate Dirac fermion varieties corresponding to right - handed and left - handed quarks , respectively .These fermions can be identified as mirror fermions because they change into each other under reflection about one spatial axis . The existence of these mirror fermions contributes to useful consequences such as the absence of flavor changing neutral currents mediated by gluons .Introduction Chiral gauge fields take key roles both theoretically and phenomenologically . They offer a natural framework for describing low - energy phenomena involving hadrons 1 .On the other hand , it has been proposed lately that some extensions of the Standard Model possibly possess extra space - time symmetries beyond Poincaré invariance 2 . It would then be very useful to develop techniques to analyze the possible effects of such new symmetries on natural observables 3 .In this Letter , we investigate a class of chiral gauge fields formulated on a Euclidean spacetime lattice 4 . Our main interest lies in examining how the presence of added discrete symmetries affects the phase diagram of the system .For simplicity , let us first focus on the case where only parity ( P ) , charge conjugation ( C ) , and period correction ( T ) transformations act nontrivially on fields 5 . Then , the operation S = d4 x L ( U ; ψ , ψ ) should satisfy the following assumptions 6 :",
        "rewrite_text": "We investigate lattice chiral gauge physics featuring Wilson fermions in four dimensions, with a particular focus on their phase structure at finite temperature T. Our findings indicate that parity (P) and time-reversal symmetry (T) do not undergo premature breaking for any value of the bare quark mass \\( m_0 \\), provided the number of flavors \\( N_f \\) satisfies \\( N_f > 2 \\). This outcome suggests that the model lacks a rank parameter associated with P and/or T, which are typically spontaneously broken in the standard framework. Notably, our analysis reveals a spectrum comprising two degenerate types of Dirac fermions, representing right-handed and left-handed quarks, respectively. These fermions can be classified as mirror fermions, as they transform into one another when reflected across a spatial axis. The existence of these mirror fermions has significant implications, such as the absence of flavor-changing neutral currents mediated by gluons.\n\nIn our research, chiral gauge fields play crucial theoretical and phenomenological roles. They provide a natural framework for elucidating low-energy phenomena related to hadrons. Recent proposals suggest that certain extensions of the Standard Model may possess additional space-time symmetries beyond Poincaré invariance. Therefore, developing methodologies to explore the implications of these new symmetries on physical observables is of great importance. In this Letter, we examine a specific class of chiral gauge fields defined on a Euclidean spacetime lattice. Our primary focus is to understand how the introduction of extra discrete symmetries influences the phase diagram of the system. To simplify the analysis, we first consider the scenario in which only parity (P), charge conjugation (C), and time (T) transformations have a nontrivial effect on the fields. Consequently, the operation \\( S = \\int d^4 x \\, L(U; \\psi, \\psi) \\) must adhere to the following assumptions:",
        "ori-fast-z-score": 0.2727272727272727,
        "water-fast-z-score": 6.636363636363637,
        "rewrite-fast-z-score": 0.8049844718999243
    },
    {
        "original_text": "We present chemical abundances for stars in three halo globular clusters (M92, M15, and M68) based on high-resolution spectroscopy obtained with HIRES at Keck Observatory. We find that these clusters have very similar abundance patterns to those observed among field halo stars; they are all metal-poor, α-enhanced systems. The mean  Fe/H  values we derive range between -2.3 dex and -1.9 dex depending upon which cluster is considered.  These results confirm previous suggestions that there may be no significant differences between the properties of the stellar populations found within individual open or globular star clusters versus those found in the general Galactic halo population. In addition, our derived ages suggest that these clusters formed early during the formation history of the Galaxy. Finally, we compare the kinematics of these clusters to those of other nearby halo objects such as dwarf galaxies and ultra-faint dwarfs.",
        "watermark_text": "We create molecular abundances for stars in three halo globular complexes ( M92 , M15 , and M68 ) based on wide - resolution spectroscopy acquired with HIRES at Keck Observatory . We see that these complexes have very identical abundance patterns to those observed among field halo stars ; they are all iron - weak , α - enhanced systems .The mean Fe / H values we derive range between - 2 . 3 dex and - 1 . 9 dex vary upon which cluster is regarded . These data confirm preceding suggestions that there may be no considerable variations between the properties of the stellar groups found within individual open or globular star clusters versus those observed in the overall Galactic halo community .In addition , our derived ages suggest that these groups formed earliest during the formation history of the Galaxy . Finally , we compare the kinematics of these clusters to those of other nearby halo objects such as dwarf stars and ultra - faint dwarfs .",
        "rewrite_text": "We have determined the molecular abundances for stars in three halo globular clusters—M92, M15, and M68—using high-resolution spectroscopy collected with HIRES at the Keck Observatory. Our analysis reveals that these clusters exhibit abundance patterns strikingly similar to those found in field halo stars; all are characterized as iron-poor and alpha-enhanced systems. The average Fe/H values we calculated range from -2.3 dex to -1.9 dex, depending on the specific cluster analyzed. These findings support earlier hypotheses that there are no significant differences in the stellar properties between the various open or globular star clusters and the broader Galactic halo population. Furthermore, our age estimates indicate that these clusters formed among the earliest stars in the Galaxy's formation history. Lastly, we compare the kinematics of these clusters with those of other nearby halo entities, including dwarf stars and ultra-faint dwarf galaxies.",
        "ori-fast-z-score": -1.75,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": 0.24618298195866545
    },
    {
        "original_text": "We present an algorithm for the construction of a discrete computer network in which each node is connected to all nodes within its distance, and also to some other nodes at larger distances. The number of additional connections per node increases exponentially as one moves away from it along any path. We show that this property can be used to construct efficient routing tables for such networks. Our results are based on a new concept called  expanding dimension . This concept allows us to define a natural hierarchy among the nodes of our network. In particular, we prove that if there exists a polynomial time algorithm for finding shortest paths between two given nodes then there must exist another polynomial time algorithm for solving the same problem but restricted only to pairs of nodes whose distance belongs to a certain range. Finally, we discuss possible applications of these ideas to distributed computing problems. Keywords: Computer network, Expanding Dimension, Hierarchy, Routing",
        "watermark_text": "We present an algorithm for the creation of a discrete computer network in which each node is linked to all nodes within its distance , and also to some other connections at larger distances . The amount of added connections per node increases exponentially as one moves away from it along any path .We see that this property can be used to build efficient routing charts for such connections . Our results are based on a new notion called expanding dimension .This concept allows us to define a natural hierarchy among the nodes of our system . In particular , we prove that if there exists a polynomial time algorithm for finding shortest links between two given nodes then there need arise another polynomial time algorithm for solving the same difficulty but restricted only to pairs of vertices whose distance belongs to a certain range .Finally , we study possible applied of these ideas to dispersed computing problems . Keywords : Computer network , Expanding Dimension , Hierarchy , Routing",
        "rewrite_text": "We introduce an algorithm designed for constructing a discrete computer network where each node connects to all nearby nodes within a specified distance, in addition to selecting some connections that span greater distances. The number of supplementary connections for each node grows exponentially as one moves further away along any given path. This characteristic can be leveraged to create effective routing charts for such connections. Our findings are grounded in a novel concept known as expanding dimension, which enables us to establish a natural hierarchy among the network's nodes. Notably, we demonstrate that if a polynomial time algorithm exists for determining the shortest paths between two specified nodes, then it follows that there is also a polynomial time algorithm for addressing the same problem, limited solely to pairs of vertices within a certain distance range. Lastly, we explore potential applications of these principles in distributed computing scenarios. \nKeywords: Computer Network, Expanding Dimension, Hierarchy, Routing.",
        "ori-fast-z-score": 1.4814874939752933,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": 0.7977240352174656
    },
    {
        "original_text": "We study heavy quarkonium transitions to hybrid mesons by using the multipole expansion method within the framework of nonrelativistic potential models. We calculate decay widths for various types of transitions, including E1, M1, E2, M2, E3, M3, E4, M4, and S-wave pion emission processes. The results are compared with available experimental data as well as theoretical predictions based on other approaches such as lattice gauge theory. \n \n In addition we determine mass spectra of hybrids by solving Schrödinger equations numerically. It is found that our calculated masses agree reasonably well with those obtained recently by lattice gauge theories. Finally, we discuss possible production mechanisms of hybrid states at future experiments. This work was supported by the National Natural Science Foundation of China under Grant No. 10675040. We thank Professors J. H. Chen and C. T. Chan for helpful discussions.",
        "watermark_text": "We research heavy quarkonium interactions to hybrid mesons by using the multipole expansion method within the framework of nonrelativistic potential methods . We calculate decay widths for various types of transitions , notably E1 , M1 , E2 , M2 , E3 , M3 , E4 , M4 , and S - wave pion emission cycles .The results are compared with existing experimental evidence as well as conceptual predictions based on other methods such as lattice gauge theory . In addition we determine mass spectra of hybrids by solving Schrödinger coefficients numerically .It is found that our measured masses agree reasonably well with those achieved lately by lattice gauge experiments . Finally , we explain possible generation systems of hybrid states at possible experiments .This project was supported by the National Natural Science Foundation of China under Grant No . 10675040 .We praise Professors J . H . Chen and C . T . Chan for helpful talks .",
        "rewrite_text": "We investigate heavy quarkonium interactions to explore hybrid mesons by employing the multipole expansion method within the nonrelativistic potential framework. Our calculations focus on decay widths for various transition types, including E1, M1, E2, M2, E3, M3, E4, M4, and S-wave pion emission cycles. We compare our results with existing experimental data and theoretical predictions from alternative approaches, such as lattice gauge theory. Additionally, we determine the mass spectra of hybrids through numerical solutions of Schrödinger equations. Our findings indicate that the masses we obtained align reasonably well with recent results from lattice gauge experiments. Finally, we discuss potential production mechanisms for hybrid states in forthcoming experiments. This research was supported by the National Natural Science Foundation of China under Grant No. 10675040, and we extend our gratitude to Professors J. H. Chen and C. T. Chan for their valuable discussions.",
        "ori-fast-z-score": -0.254000254000381,
        "water-fast-z-score": 5.08000508000762,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We present 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves for the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1. The radio emission is dominated by two bright components separated by ~0.5 arcsec at all epochs observed between 2005 January and 2007 December. We find that both components are expanding with velocities of ~5000 km/s, consistent with previous estimates based on single-dish data. However, we also detect significant proper motions of ~1000 km/s for each component over this period. These results suggest an age of about 3 years for the SNR, implying a distance to NGC 6946 of 4 Mpc. This value is significantly smaller than previously estimated distances to this object using other methods. Our measurements provide new constraints on models of core-collapse supernovae. \n \n Keywords: Supernova remnants",
        "watermark_text": "We create 8 . 4 GHz Very Long Baseline Interferometry ( VLBI ) images and light angles for the supernova remnant ( SNR ) associated with the Type IIb supernova SN2004et , which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1 . The radio emission is dominated by two bright components differentiated by ~ 0 . 5 arcsec at all epochs observed between 2005 January and 2007 December .We see that both components are growing with velocities of ~ 5000 kilometres / s , consistent with previous estimates based on single - dish data . However , we also observe significant normal motions of ~ 1000 kilometers / s for each component over this time .These data suggest an age of about 3 years for the SNR , suggests a proximity to NGC 6946 of 4 Mpc . This value is significantly less than previously estimated distances to this body using other methods .Our measurements give novel constraints on estimates of core - collapse supernovae . Keywords : Supernova remnants",
        "rewrite_text": "We have produced 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves for the supernova remnant (SNR) associated with the Type IIb supernova SN 2004et, which erupted in the nearby spiral galaxy NGC 6946 on September 24, 2004, UT. The radio emission reveals two prominent components separated by approximately 0.5 arcseconds, observed consistently from January 2005 to December 2007. Both components exhibit growth at velocities around 5000 kilometers per second, aligning with prior estimates based on single-dish data. Additionally, we detect notable normal motions of about 1000 kilometers per second for each component throughout this period. These findings indicate that the SNR is roughly 3 years old and suggest that it is located about 4 Mpc from NGC 6946, a distance significantly less than previously estimated using other methods. Our measurements provide new constraints for predictions regarding core-collapse supernovae. Keywords: Supernova remnants.",
        "ori-fast-z-score": -0.6509445549041194,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": 0.762000762001143
    },
    {
        "original_text": "We present an ab initio study of the structure, energetics, and dynamics of water adsorbed on the MgO(001) surface in the submonolayer regime using density functional theory with van der Waals corrections. We find that the most stable configuration is one where each oxygen atom binds to three hydrogen atoms forming a trihydrogen bridge between two adjacent O atoms. The binding energy per molecule for this configuration is 1.6 eV. This value agrees well with previous theoretical results obtained within the generalized gradient approximation but disagrees significantly with experimental values which are typically larger by about 0.5-0.7 eV. Our calculations show that the discrepancy can be attributed mainly to the neglect of dispersion interactions in earlier studies. In addition we have studied the effect of temperature on the stability of different configurations. We found that the relative population of various structures depends strongly on the temperature.",
        "watermark_text": "We present an ab initio investigation of the composition , energetics , and dynamics of water adsorbed on the MgO ( 001 ) surface in the submonolayer regime using density functional theory with van der Waals corrections . We see that the most stable configuration is one where each oxygen element connects to three hydrogen atoms forming a trihydrogen bridge between two adjacent O atoms .The binding energy per molecule for this configuration is 1 . 6 eV . This value agrees well with previous conceptual conclusions derived within the generalized gradient formulation but disagrees substantially with theoretical values which are typically greater by about 0 . 5 - 0 . 7 eV .Our calculations show that the discrepancy can be due mainly to the neglect of dispersion interactions in earlier methods . In addition we have researched the impact of temperature on the stability of different configurations .We determined that the relative population of several structures varies strongly on the temperature .",
        "rewrite_text": "We conduct an ab initio study of the composition, energetics, and dynamics of water adsorbed onto the MgO (001) surface in the submonolayer regime, employing density functional theory with van der Waals corrections. Our findings indicate that the most stable configuration is characterized by each oxygen atom forming connections with three hydrogen atoms, resulting in a trihydrogen bridge linking two neighboring oxygen atoms. The binding energy per molecule for this configuration is 1.6 eV, which aligns well with previous conceptual insights obtained using the generalized gradient approximation, but significantly diverges from theoretical values that are typically 0.5 to 0.7 eV higher. We attribute this discrepancy largely to the exclusion of dispersion interactions in earlier approaches. Additionally, we investigate the effect of temperature on the stability of various configurations, revealing that the relative populations of different structures are highly sensitive to temperature variations.",
        "ori-fast-z-score": 0.8551861104941365,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": 1.9402850002906638
    },
    {
        "original_text": "We study the sound propagation through a one-dimensional (1D) Bose-Einstein condensed gas trapped by an optical lattice potential and interacting with each other via contact interactions. We show that, for weak interaction strength, there is no phonon-phonon scattering between different bands due to the energy gap induced by the periodic potential. In this case, we find that the sound velocity can be obtained analytically using perturbation theory. For strong interaction strengths, however, the phonons are scattered into higher bands and thus the sound velocity decreases as compared to its non-interacting value. The results agree well with numerical calculations based on the Gross-Pitaevskii equation. PACS numbers: 03.75.Dg, 05.30.Jp, 37.10.Gh \nI. INTRODUCTIO N\nThe properties of superfluid helium have been studied extensively since it was discovered more than half century ago  1  . One of the most important features of superfluids is their ability to support dissipationless flow without friction  2  , which has led to many applications such as superconductors  3  .\nRecently, ultracold atomic gases confined in optical lattices provide another platform to explore quantum fluids  4  . These systems exhibit various phases including Mott insulator phase  5  , supersolid phase  6  , and even topological states  7, 8  . Moreover, they allow us to tune the system parameters continuously  9  and observe directly the evolution of physical quantities  10  . This makes them ideal candidates to investigate new phenomena predicted by theoretical studies  11  .\nIn particular, bosonic atoms in optical lattices may form a BoseEinstein condensate  12  . It is known that these condensates behave like superfluids  13  . Recently, several experiments have observed the superflow  14  and vortex  15  in these systems. However, unlike conventional superfluids, the condensates in optical lattices also interact strongly with each other  16  . Therefore, understanding how the interatomic interactions affect the collective excitations becomes crucial  17  .\nIn this work, we consider 1D Bose-Einstein condensates trapped by an optical lattice  18  . By solving the",
        "watermark_text": "We test the music transmission through a one - dimensional ( 1D ) Bose - Einstein condensed gas trapped by an optical lattice potential and evolving with each other via contact interactions . We see that , for weak interaction strength , there is no phonon - phonon absorption between various groups owing to the power gap induced by the periodic potential .In this situation , we find that the audio speed can be obtained analytically using perturbation theory . For strong coupling strengths , however , the phonons are scattered into greater bands and therefore the audio speed reduces as compared to its non - interacting function .The results agree well with numerical measurements based on the Gross - Pitaevskii equation . PACS quantities : 03 . 75 . Dg , 05 . 30 . Jp , 37 . 10 . Gh I . INTRODUCTIO N The properties of superfluid helium have been studied thoroughly since it was known more than quarter century ago 1 .One of the most important features of superfluids is their ability to support dissipationless flow without tension 2 , which has led to many applications such as superconductors 3 . Recently , ultracold atomic fluids confined in laser lattices offers another platform to study quantum fluids 4 .These systems exhibit several stages namely Mott insulator stage 5 , supersolid phase 6 , and even topological states 7 , 8 . Moreover , they allow us to balance the process variables continuously 9 and observe directly the evolution of physical quantities 10 .This gives them ideal candidates to examine novel processes proposed by theoretical experiments 11 . In particular , bosonic atoms in optical lattices might form a BoseEinstein condensate 12 .It is known that these condensates behave like superfluids 13 . Recently , various observations have noted the superflow 14 and vortex 15 in these systems .However , unlike conventional superfluids , the condensates in laser lattices additionally interact heavily with each other 16 . Therefore , studying how the interatomic interactions impact the collective excitations remains crucial 17 .In this study , we solve 1D Bose - Einstein condensates trapped by an optical lattice 18 . By solving the",
        "rewrite_text": "We investigate the transmission of music through a one-dimensional (1D) Bose-Einstein condensate gas, which is confined by an optical lattice potential and interacts via contact interactions. Our findings indicate that at weak interaction strength, phonon-phonon absorption between different groups is absent due to the power gap created by the periodic potential. Under these conditions, we can derive the sound speed analytically using perturbation theory. In contrast, at strong coupling strengths, phonons scatter into higher bands, leading to a reduction in sound speed compared to the function of non-interacting phonons. These findings align well with numerical simulations based on the Gross-Pitaevskii equation. PACS codes: 03.75.Dg, 05.30.Jp, 37.10.Gh. \n\nINTRODUCTION: The properties of superfluid helium have been extensively researched since its discovery over twenty-five years ago. One of the key characteristics of superfluids is their capability to maintain dissipationless flow without tension, which has resulted in numerous applications, including in superconductors. Recently, ultracold atomic fluids trapped in laser lattices have emerged as a new platform for studying quantum fluids. These systems exhibit several distinct phases, such as the Mott insulator phase, supersolid phase, and even topological states. Furthermore, they enable us to continuously adjust process parameters and directly observe the evolution of physical quantities, making them ideal candidates for exploring novel theoretical processes. In particular, bosonic atoms in optical lattices can form Bose-Einstein condensates, which are known to behave similarly to superfluids. Recent observations have reported instances of superflow and vortex formation in these systems. However, unlike traditional superfluids, condensates in optical lattices experience significant interatomic interactions, making it essential to study the effects of these interactions on collective excitations. Thus, in this study, we address the dynamics of 1D Bose-Einstein condensates trapped in an optical lattice by solving the relevant equations.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.736345975703701,
        "rewrite-fast-z-score": 2.2132669799727465
    },
    {
        "original_text": "We present an analysis of the gravitational waveforms emitted by two neutron stars orbiting each other, and show that they can be used to detect violations of Lorentz invariance (LI). We consider both scalar-tensor theories with spontaneous breaking of LI as well as vector-tensor theories where LI is violated through the presence of a preferred reference frame. In these models we find that there are characteristic deviations from general relativity which lead to measurable differences between the observed gravitational waveform and those predicted within Einstein s theory. \n \n The detection of such deviations would provide strong evidence for new physics beyond standard model expectations. This could have important consequences on our understanding of fundamental interactions at high energies. For example, it may shed light on the origin of dark energy or even reveal the existence of extra dimensions of space-time. It also has implications for cosmology since many extensions of the Standard Model predict time variations of physical constants like Newton s constant G.",
        "watermark_text": "We present an assessment of the gravitational waveforms emitted by two neutron stars orbiting each other , and find that they can be used to identify violations of Lorentz invariance ( LI ) . We consider both scalar - vector models with spontaneous breaking of LI as well as vector - vector models where LI is enforced through the presence of a preferred source frame .In these models we find that there are characteristic deviations from general relativity which lead to measurable differences between the expected gravitational waveform and those predicted within Einstein s theory . The diagnosis of such deviations might give strong evidence for future physics beyond standard theory expectations .This might have important implications on our knowing of fundamental interactions at high energies . For instance , it could cast light on the origin of dark energy or actually expose the existence of extra dimensions of space - time .It additionally has implications for cosmology since several extensions of the Standard Model predict time variations of physical constants like Newton s constant G .",
        "rewrite_text": "We evaluate the gravitational waveforms produced by two neutron stars in orbit around each other and discover that these waveforms can help identify violations of Lorentz invariance (LI). Our analysis encompasses both scalar-vector models with spontaneous LI breaking and vector-vector models where LI is upheld by a preferred source frame. Our findings reveal distinct deviations from general relativity, resulting in measurable differences between the gravitational waveforms predicted by Einstein's theory and those generated by these alternative models. Detecting such deviations could provide compelling evidence for physics that extends beyond the current standard theories, potentially shedding light on fundamental interactions at high energies. For example, it may help illuminate the origins of dark energy or reveal extra dimensions of space-time. Furthermore, this has significant implications for cosmology, as various extensions of the Standard Model suggest that physical constants, such as Newton's constant G, might vary over time.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "We present the chemical properties (metallicity, abundance ratios) for a sample of dwarf galaxies in the Local Volume with known distances and star formation histories. We use high quality spectra obtained at the Apache Point Observatory to derive oxygen abundances using both direct T e method as well as strong-line methods calibrated on H II regions in nearby spiral galaxies.  The derived metallicities range between 12+log(O/H)=7.6-8.2 dex. These values are consistent with those found by previous studies based on optical spectroscopy. In addition we find that most of these dwarfs have supersolar N/O ratio indicating recent or ongoing nitrogen enrichment due to massive stars. This is also supported by their low SFRs which prevent efficient dilution of the enriched gas produced by supernovae type Ia. Finally, we compare our results with theoretical predictions made by different chemical evolution models. Our analysis shows that none of them can reproduce simultaneously all observed quantities such as metallicity, N/O ratio and sSFR.",
        "watermark_text": "We present the chemical properties ( metallicity , abundance proportions ) for a sample of dwarf stars in the Local Volume with established altitudes and galaxy formation histories . We use large quality spectra obtained at the Apache Point Observatory to derive gas abundances using both direct T e method as also as bright - line methods calibrated on H II regions in nearby spiral clusters .The derived metallicities range between 12 + log ( O / H ) = 7 . 6 - 8 . 2 dex . These values are compatible with those shown by earlier surveys based on optical spectroscopy .In addition we find that most of these dwarfs have supersolar N / O ratio indicating new or ongoing nitrogen enrichment due to massive stars . This is also supported by their low SFRs which prevent efficient dilution of the enriched gas created by supernovae class Ia .Finally , we compare our findings with theoretical estimates made by various chemical evolution models . Our investigation reveals that none of them can predict simultaneously all observed properties such as metallicity , N / O ratio and sSFR .",
        "rewrite_text": "We present the chemical properties, specifically metallicity and abundance ratios, of a sample of dwarf stars located in the Local Volume, taking into account their established altitudes and histories of galaxy formation. By utilizing high-quality spectra from the Apache Point Observatory, we determined gas abundances through both the direct T_e method and bright-line methods that are calibrated using H II regions from nearby spiral clusters. The obtained metallicities range from 12 + log (O/H) = 7.6 to 8.2 dex, aligning well with values reported in earlier optical spectroscopic surveys. Additionally, we find that most of these dwarf stars exhibit supersolar N/O ratios, suggesting recent or ongoing nitrogen enrichment attributed to massive stars. This observation is further supported by their low star formation rates (SFRs), which hinder the effective dilution of enriched gas from Type Ia supernovae. Lastly, we compare our results with theoretical predictions from various chemical evolution models, revealing that none of these models successfully account for all observed properties, including metallicity, N/O ratio, and specific star formation rate (sSFR).",
        "ori-fast-z-score": -0.9058216273156765,
        "water-fast-z-score": 4.612860393796698,
        "rewrite-fast-z-score": 0.6469966392206304
    },
    {
        "original_text": "We present the Nobeyama CO (J=1-0) atlas for nearby spiral galaxies, which is based on our previous work by Kuno et al. (1995) . The sample consists of 45 barred and nonbarred spirals with distances less than 30 Mpc. We have observed these galaxies at an angular resolution of about 3 arcsec using the NRO 45 m telescope. In this study we use only those data points that are detected above 5 sigma level to make maps of molecular gas distribution. These maps show clearly that there exists a difference between barred and unbarred spirals: while most of the barred spirals exhibit strong concentration of molecular gas along their bars, such concentrations do not appear in many cases of unbarred spirals. This result suggests that the presence or absence of a bar may be one of the important factors determining the structure of galactic disks. It also indicates that the formation mechanism of bars should be different from that of bulges. \n \n\nThe authors acknowledge support from Grants-in-Aid for Scientific Research (No. 08540349), Ministry of Education, Science and Culture, Japan.",
        "watermark_text": "We present the Nobeyama CO ( J = 1 - 0 ) atlas for nearby spiral galaxies , which is based on our previous research by Kuno et al . ( 1995 ) .The sample consists of 45 barred and nonbarred spirals with distances lower than 30 Mpc . We have discovered these objects at an angular resolution of about 3 arcsec use the NRO 45 m observatory .In this study we using only those data points that are detected above 5 sigma level to make projections of molecular gas distribution . These surveys demonstrate obviously that there exists a difference between barred and unbarred spirals : while most of the barred spirals exhibit strong amount of molecular vapor along their bars , such concentrations do not appear in large cases of unbarred spirals .This result suggests that the presence or lack of a bar may be one of the important factors regulating the composition of galactic disks . It additionally indicates that the formation system of rings should be changed from that of bulges .The authors acknowledge support from Grants - in - Aid for Scientific Research ( No . 08540349 ) , Ministry of Education , Science and Culture , Japan .",
        "rewrite_text": "We present the Nobeyama CO (J = 1 - 0) atlas for nearby spiral galaxies, building upon our earlier work by Kuno et al. (1995). The sample comprises 45 barred and non-barred spiral galaxies located within 30 Mpc. Utilizing the NRO 45 m observatory, we have identified these objects with an angular resolution of approximately 3 arcseconds. In this investigation, we focus solely on data points detected above the 5 sigma level to project the distribution of molecular gas. Our surveys clearly indicate a distinction between barred and unbarred spirals: while most barred spirals exhibit significant concentrations of molecular gas along their bars, such concentrations are typically absent in many unbarred spirals. This finding suggests that the presence or absence of a bar may play a crucial role in determining the composition of galactic disks, and it implies that the formation mechanisms of rings may differ from those of bulges. The authors gratefully acknowledge the support of Grants-in-Aid for Scientific Research (No. 08540349) from the Ministry of Education, Science, and Culture of Japan.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 4.365641250653994,
        "rewrite-fast-z-score": -0.5933908290969266
    },
    {
        "original_text": "We present an analysis of the kinematics and dynamics of galaxies within galaxy clusters, using data obtained with the Hubble Space Telescope (HST). We use this information to measure the degree of radial bias in the distribution of cluster member velocities as well as their spatial correlation function. The results are compared against predictions made by cosmological N-body simulations that include both baryonic gas and collisionless dark matter particles. Our main conclusions are:  1) Galaxy clusters exhibit significant deviations from isotropic dynamical equilibrium; 2) These deviations can be explained if we assume that the dark matter component has a radially biased velocity dispersion tensor; 3) This result implies that the dark matter halos surrounding individual galaxies have similar shapes but different orientations relative to each other. In addition, our measurements provide new constraints on the mass-to-light ratio for galaxy clusters. Using HST observations of four nearby galaxy clusters, we find evidence that the dark matter component exhibits a strong radial bias in its velocity dispersion tensor.",
        "watermark_text": "We present an assessment of the kinematics and dynamics of clusters within galaxy clusters , using data acquired with the Hubble Space Telescope ( HST ) . We use this data to measure the degree of radial bias in the distribution of cluster member velocities as well as their spatial correlation function .The results are compared against models done by cosmological N - bodies simulations that include both baryonic gas and collisionless dark matter molecules . Our main results are : 1 ) Galaxy clusters exhibit substantial deviations from isotropic dynamical balance ; 2 ) These deviations can be described if we suppose that the dark matter component has a radially biased speed dispersion matrix ; 3 ) This result suggests that the dark matter halos adjoining individual stars have related shapes but different orientations relative to each other .In addition , our measurements give novel constraints on the mass - to - light percentage for galaxy regions . Using HST observations of four nearby galaxy galaxies , we find proof that the dark matter component displays a powerful radial bias in its velocity dispersion matrix .",
        "rewrite_text": "We present an evaluation of the kinematics and dynamics of clusters within galaxy clusters, utilizing data obtained from the Hubble Space Telescope (HST). This data allows us to assess the extent of radial bias in the distribution of cluster member velocities and their spatial correlation function. Our findings are contrasted with models derived from cosmological N-body simulations that consider both baryonic gas and collisionless dark matter. Our key conclusions are: 1) Galaxy clusters show significant deviations from isotropic dynamical equilibrium; 2) These deviations can be accounted for by assuming that the dark matter component exhibits a radially biased velocity dispersion matrix; 3) This indicates that the dark matter halos surrounding individual stars have similar shapes but vary in orientation relative to one another. Moreover, our measurements provide new constraints on the mass-to-light ratios in galaxy regions. Through HST observations of four nearby galaxies, we find evidence that the dark matter component exhibits a strong radial bias in its velocity dispersion matrix.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 6.184165460191406,
        "rewrite-fast-z-score": -0.22645540682891913
    },
    {
        "original_text": "We present an exact solution to the classical equations of motion in two dimensions, which is interpreted as describing a rotating black hole with angular momentum J = M . The metric has the form ds2 = −dt2+(1+cosh2r)dθ2−r2dr2,\nwhere r and θ are polar coordinates on the plane. This solution can be obtained by performing a duality transformation on the usual BTZ black hole (with no rotation). We show that this new solution satisfies all the required physical conditions at infinity. In particular we find that it describes a regular black hole horizon located atr+ = √3M , where M is the mass parameter appearing in the original BTZ solution. Finally, we discuss some possible generalizations of our results. Introduction:-In recent years there have been many attempts to construct solutions to Einstein s field equations corresponding to rotating black holes  1  -  4  . One particularly interesting class of such solutions was found by Bañados, Teitelboim and Zanelli (BTZ), who showed how one could obtain a static black hole solution in three dimensional anti-de Sitter space-time  5  .\nThe most important feature of these solutions is their asymptotic behaviour; they describe black holes whose event horizons are completely determined by global quantities like total energy or charge  6  . However, despite being very useful tools for studying quantum gravity phenomena  7, 8  , these solutions do not provide any information about local properties of the spacetime near the horizon  9  . It would therefore seem desirable to try to extend them into more complicated geometries containing additional parameters characterizing the internal structure of the black hole  10  .\nOne way of doing so is to consider higher-dimensional extensions of the BTZ solution  11  . Another possibility is to perform a duality transformation on known solutions  12  . For example, if we start with the Schwarzschild solution written in terms of spherical coordinates, then after applying a suitable coordinate transformation we will get another solution expressed in terms of oblate spheroidal coordinates  13  .",
        "watermark_text": "We present an precise answer to the classical equations of movement in two dimensions , which is interpreted as representing a rotating black hole with angular velocity J = M . The metric has the form ds2 = −dt2 + ( 1 + cosh2r ) dθ2−r2dr2 , where p and θ are polar coordinates on the plane .This solution can be obtained by performing a duality conversion on the usual BTZ dark hole ( with no rotation ) . We see that this new solution satisfies all the necessary physical conditions at infinity .In particular we find that it describes a regular dark hole horizon located atr + = √3M , where M is the mass vector appearing in the previous BTZ solution . Finally , we explain some possible generalizations of our findings .Introduction : - In recent history there have been many efforts to build solutions to Einstein s field equations corresponding to spinning black holes 1 - 4 . One especially interesting class of such solve was seen by Bañados , Teitelboim and Zanelli ( BTZ ) , who demonstrated how one might obtain a static black hole solution in three dimensional anti - de Sitter space - time 5 .The most important feature of these solutions is their asymptotic behaviour ; they describe brown holes whose event horizons are completely determined by global quantities like total energy or charge 6 . However , despite being very useful techniques for studying quantum gravitational dynamics 7 , 8 , these solutions do not offer any info about local characteristics of the spacetime near the horizon 9 .It would therefore appear desirable to work to extend them into more complicated geometries containing extra constraints characterizing the internal structure of the dark hole 10 . One method of doing so is to consider higher - dimensional applications of the BTZ solution 11 .Another possibility is to conduct a duality conversion on known solutions 12 . For instance , if we start with the Schwarzschild solution written in terms of spherical coordinates , then after applying a suitable coordinate transformation we will get another solution expressed in terms of oblate spheroidal coordinates 13 .",
        "rewrite_text": "We provide a precise solution to the classical equations of motion in two dimensions, which can be interpreted as depicting a rotating black hole with angular momentum \\( J = M \\). The metric takes the form \\( ds^2 = -dt^2 + (1 + \\cosh^2 r) d\\theta^2 - r^2 dr^2 \\), where \\( r \\) and \\( \\theta \\) are polar coordinates in the plane. This solution is derived through a duality transformation applied to the conventional BTZ black hole (which has no rotation). Notably, this new solution meets all essential physical conditions at infinity, specifically describing a regular black hole horizon located at \\( r_+ = \\sqrt{3}M \\), with \\( M \\) representing the mass parameter from the original BTZ solution. We also discuss potential generalizations of our results. \n\n**Introduction:** In recent years, significant efforts have been made to construct solutions to Einstein's field equations that correspond to spinning black holes. One particularly notable class of solutions was developed by Bañados, Teitelboim, and Zanelli (BTZ), who illustrated how to derive a stationary black hole solution in three-dimensional anti-de Sitter space-time. The key aspect of these solutions lies in their asymptotic behavior; they describe black holes whose event horizons are determined entirely by global parameters such as total energy or charge. Nevertheless, while these approaches are effective for exploring quantum gravitational dynamics, they do not provide insights into the local properties of spacetime near the horizon. Consequently, there is a strong interest in extending these solutions to more complex geometries that incorporate additional constraints reflecting the internal structure of the black hole. One approach to achieve this is by examining higher-dimensional versions of the BTZ solution. Alternatively, one might explore duality transformations applied to known solutions. For example, starting from the Schwarzschild solution expressed in spherical coordinates and then applying an appropriate coordinate transformation can result in a different solution in oblate spheroidal coordinates.",
        "ori-fast-z-score": 1.150792911137501,
        "water-fast-z-score": 8.27424239724231,
        "rewrite-fast-z-score": 0.6488856845230502
    },
    {
        "original_text": "The extraction of fresh water and energy from the atmosphere is proposed as an alternative to conventional sources, which are limited in supply or environmentally damaging.  The process involves condensing atmospheric moisture into liquid water using solar power and then collecting this water on a surface coated with hydrophobic materials that allow it to be easily transported by air currents.   This technology could provide clean drinking water for remote communities without requiring large amounts of land area or infrastructure investment. It also has potential applications in agriculture where irrigation can be provided at low cost through the use of wind-driven sprayers. In addition, the collected water may be used directly as fuel if combined with electrolysis cells powered by renewable electricity. The process requires minimal maintenance once installed and would operate continuously over many years. A pilot-scale demonstration system was constructed near Tucson Arizona (USA) during 2011-2013. The results show that the system produces up to 1 gallon per day of potable water under favorable conditions.",
        "watermark_text": "The extraction of fresh water and energy from the air is proposed as an alternative to conventional sources , which are limited in supply or environmentally destructive . The method means condensing ambient rainfall into liquid water use solar electricity and then collecting this water on a surface packed with hydrophobic materials that enable it to be easily carried by air waves .This system could supply fresh water freshwater for isolated communities without using big amounts of property area or structural capital . It additionally has potential applications in farming where irrigation can be provided at low cost through the using of wind - powered sprayers .In addition , the stored water may be used directly as fuel if combined with electrolysis panels driven by renewable energy . The method needs minimal repair once implemented and might run constantly over numerous years .A pilot - scale test system was constructed near Tucson Arizona ( USA ) during 2011 - 2013 . The results show that the scheme produces up to 1 gallon per day of potable liquid under favorable conditions .",
        "rewrite_text": "The process of extracting fresh water and energy from the air is suggested as an alternative to traditional sources, which are often limited in availability or harmful to the environment. This method involves condensing ambient moisture into liquid water using solar energy, then collecting it on a surface covered with hydrophobic materials that allow it to be easily transported by air currents. This system has the potential to provide fresh water to remote communities without requiring extensive land use or significant infrastructure investment. Additionally, it could be utilized in agriculture, offering low-cost irrigation through wind-powered sprayers. Furthermore, the collected water can be used as fuel when paired with electrolysis panels powered by renewable energy. Once established, this method requires minimal maintenance and can operate continuously for many years. A pilot-scale system was developed near Tucson, Arizona, between 2011 and 2013, demonstrating that it can produce up to 1 gallon of potable water per day under favorable conditions.",
        "ori-fast-z-score": -0.8340576562282991,
        "water-fast-z-score": 7.715033320111767,
        "rewrite-fast-z-score": 1.193117518002609
    },
    {
        "original_text": "We have analyzed the redshifts of two samples of active galactic nuclei (AGNs) with different luminosities and found evidence for intrinsic redshift components in both cases.  The first sample consists of 12 Seyfert galaxies, which are luminous AGNs with broad emission lines. We find that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose amplitude is correlated with the widths of the broad emission lines. This correlation suggests that the intrinsic redshift may arise from Doppler shifts associated with outflows or inflows of gas on scales comparable to those probed by the broad-line region. The second sample contains low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. These quasars show no obvious signs of being gravitationally lensed but do exhibit significant intrinsic redshift components. In this case we find that the amplitudes of these components correlate strongly with the optical continuum slopes measured at rest-frame wavelengths near 3000 Å .",
        "watermark_text": "We have analyzed the redshifts of two specimens of active galactic nuclei ( AGNs ) with varying luminosities and found proof for intrinsic redshift components in both cases . The first specimen consists of 12 Seyfert galaxies , which are luminous AGNs with broad absorption paths .We see that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose intensity is associated with the widths of the broad emission lines . This correlation suggests that the intrinsic redshift may arise from Doppler movements related with outflows or inflows of gas on scales similar to those probed by the broad - line region .The second survey features small - luminosity quasars chosen from the Sloan Digital Sky Survey Data Release 4 quasar catalog . These quasars exhibit no evident indication of being gravitationally lensed but do exhibit substantial intrinsic redshift components .In this situation we find that the amplitudes of these systems correlate strongly with the optical continuum curves observed at rest - mirror wavelengths near 3000 Å .",
        "rewrite_text": "We have examined the redshifts of two samples of active galactic nuclei (AGNs) with differing luminosities and discovered evidence of intrinsic redshift components in both instances. The first sample includes 12 Seyfert galaxies, which are bright AGNs characterized by broad absorption features. Our analysis shows that their observed redshifts can be separated into an extrinsic component caused by gravitational lensing from foreground objects, and an intrinsic component related to the widths of the broad emission lines. This relationship indicates that the intrinsic redshift may stem from Doppler effects linked to gas outflows or inflows on scales comparable to those in the broad-line region. The second sample consists of low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 catalog. While these quasars do not show clear signs of gravitational lensing, they do exhibit significant intrinsic redshift components. In this case, we find a strong correlation between the amplitudes of these systems and the optical continuum curves observed at rest-frame wavelengths around 3000 Å.",
        "ori-fast-z-score": 0.4923659639173309,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": 0.11867816581938533
    },
    {
        "original_text": "The measurement calculus is an extension to the standard quantum mechanics formalism that allows for measurements on composite systems, and it has been used in several recent works as well as in this thesis.  The main idea behind the measurement calculus is to consider all possible outcomes of a measurement process as separate states of the system being measured instead of just one outcome (as done by von Neumann). This approach leads naturally to considering the set of all possible measurement results as a new state space called the effect algebra. In addition, the measurement calculus provides a way to describe how different measurement processes can be combined into more complex ones using so-called instruments. Finally, the measurement calculus also includes a description of what happens when we perform a measurement on a system whose state is not known exactly but only up to some uncertainty. The measurement calculus was first introduced by Aharonov et al., and since then there have been many papers written about its properties and applications.",
        "watermark_text": "The measurement calculus is an addition to the standard quantum mechanics formalism that enables for measurements on composite systems , and it has been used in multiple recent works as well as in this dissertation . The main idea behind the observation calculus is to consider all possible outcomes of a measurement process as separate states of the process being measured instead of just one outcome ( as done by von Neumann ) .This method results naturally to defining the group of all possible measurement results as a new state space termed the effect algebra . In addition , the observation calculus provides a way to explain how various measurement processes can be merged into more sophisticated ones utilizing so - called instruments .Finally , the observation calculus also contains a description of what happens when we perform a measurement on a system whose state is not known exactly but only up to some uncertainty . The measurement calculus was first developed by Aharonov et al . , and since then there have been many books written about its properties and use .",
        "rewrite_text": "The measurement calculus is an extension of the standard quantum mechanics framework, allowing for measurements on composite systems. It has been utilized in various recent studies, including this dissertation. The central concept of the observation calculus is to treat all potential outcomes of a measurement process as distinct states of the system being observed, differing from von Neumann's approach that considers only one outcome. This perspective naturally leads to the establishment of a new state space, called the effect algebra, which encompasses all possible measurement results. Moreover, the observation calculus offers a framework to integrate different measurement processes into more complex ones through the use of so-called instruments. Additionally, it provides a description of the measurement outcomes when the system's state is only partially known, subject to some degree of uncertainty. The measurement calculus was initially proposed by Aharonov et al., and numerous books have since been published discussing its properties and applications.",
        "ori-fast-z-score": 1.3093073414159544,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 0.5423261445466404
    },
    {
        "original_text": "We study nonequilibrium spin dependent transport properties of normal-metal-superconductor (NS) and ferromagnet-superconductor (FS) hybrid systems by using the quasiclassical theory for diffusive conductors with spin-orbit scattering. We show that, due to the proximity effect induced pair correlations between electrons on opposite sides of NS interface, there is an additional contribution to the current density which depends on the relative orientation of magnetizations in F and S layers. This results in appearance of the anomalous Josephson-like current-phase relation in FS junctions. In addition we find that the Andreev reflection at NS interfaces can be strongly suppressed if the angle between magnetization directions in N and S regions becomes sufficiently large. The suppression of Andreev reflection leads to the reduction of conductance through NS junction as compared to its value in equilibrium state. \n \n Finally, we discuss possible experimental realizations of our predictions.",
        "watermark_text": "We research nonequilibrium spin dependent transport properties of normal - copper - superconductor ( NS ) and ferromagnet - superconductor ( FS ) hybrid systems by using the quasiclassical principle for diffusive conductors with spin - orbit scattering . We see that , owing to the proximity effect induced pair correlations between electrons on opposite ends of NS interface , there is an additional contribution to the current density which depends on the relative position of magnetizations in F and S layers .This results in appearance of the anomalous Josephson - like current - phase connection in FS junctions . In addition we find that the Andreev reflection at NS interfaces can be highly suppressed if the angle between magnetization directions in N and S areas becomes enough large .The disruption of Andreev reflection results to the reduction of conductance through NS intersection as compared to its value in equilibrium state . Finally , we explain possible experimental realizations of our predictions .",
        "rewrite_text": "We investigate the nonequilibrium spin-dependent transport characteristics of normal-copper-superconductor (NS) and ferromagnet-superconductor (FS) hybrid systems, utilizing the quasiclassical approach for diffusive conductors with spin-orbit scattering. Our findings reveal that the proximity effect generates pair correlations between electrons at opposite ends of the NS interface, leading to an additional component in the current density that is influenced by the relative orientations of the magnetizations in the ferromagnetic (F) and superconducting (S) layers. This interaction results in an anomalous Josephson-like current-phase relationship in FS junctions. Furthermore, we discover that Andreev reflection at NS interfaces can be significantly suppressed when the angle between the magnetization directions in the normal (N) and superconducting (S) regions becomes sufficiently large. This suppression of Andreev reflection leads to a decrease in conductance through the NS junction compared to its equilibrium value. Lastly, we discuss potential experimental implementations of our findings.",
        "ori-fast-z-score": 1.3858697343671664,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 0.508000508000762
    },
    {
        "original_text": "We study the cycle structure of elements in braid groups, and prove that any element can be written as a product of cycles whose lengths are bounded by some constant depending only on the number of strands. This is an extension to higher dimensions of results obtained for free groups by Magnus and Karrass-Solitar. \nTheorem 1 (Cycle decomposition). Let G = B_n(K) be a braid group with n strands over K. Then every g ∈ G has a unique expression as a product of disjoint cycles c_1, ..., c_k such that: \n1. The length of each cycle ci satisfies |ci| ≤ 2^{log_2 n} + 3.\n2. Each cycle ci contains at most one generator t_i which does not commute with its inverse.\n3. If there exists no generator t_i satisfying condition 2 then k = 0.\n4. No two generators appear simultaneously in different cycles.\n5. Every generator appears exactly once in each cycle it belongs to.\n6. For all i, |c_1 · ... · c_(i-1)| < |c_i|.",
        "watermark_text": "We research the cycle composition of elements in braid families , and prove that any element can be written as a product of cycles whose distances are bounded by some constant depending only on the number of strands . This is an addition to higher dimensions of findings obtained for free families by Magnus and Karrass - Solitar .Theorem 1 ( Cycle decomposition ) . Let G = B _ n ( K ) be a braid set with n chains over K . Then every g ∈ G has a unique expression as a product of disjoint paths c _ 1 , . . . , b _ k such that : 1 .The span of each cycle ci satisfies | ci | ≤ 2 ^ { log _ 2 n } + 3 . 2 .Each cycle ci comprises at most one generator t _ i which does not commute with its inverse . 3 .If there exists no generator t _ i satisfying condition 2 then k = 0 . 4 .No two generators appear separately in different cycles . 5 .Every generator appears precisely once in each cycle it belongs to . 6 .For all i , | c _ 1 · . . . · c _ ( i - 1 ) | < | c _ i | .",
        "rewrite_text": "We investigate the cycle composition of elements in braid families and demonstrate that each element can be expressed as a product of cycles with distances constrained by a constant that solely depends on the number of strands. This extends the results previously established for free families by Magnus and Karrass-Solitar to higher dimensions. Theorem 1 (Cycle Decomposition). Let \\( G = B_n(K) \\) be a braid group with \\( n \\) strands over \\( K \\). Then, for every \\( g \\in G \\), there is a unique representation as a product of disjoint cycles \\( c_1, \\ldots, c_k \\) such that: 1. The length of each cycle \\( c_i \\) satisfies \\( |c_i| \\leq 2^{\\log_2 n} + 3 \\). 2. Each cycle \\( c_i \\) contains at most one generator \\( t_i \\) that does not commute with its inverse. 3. If there is no generator \\( t_i \\) fulfilling condition 2, then \\( k = 0 \\). 4. No two generators appear separately in different cycles. 5. Each generator appears exactly once in every cycle it belongs to. 6. For all \\( i \\), \\( |c_1 \\cdots c_{i-1}| < |c_i| \\).",
        "ori-fast-z-score": -2.0768805540571886,
        "water-fast-z-score": 2.215646837627989,
        "rewrite-fast-z-score": -1.0
    },
    {
        "original_text": "The authors present an analysis of the stochasticity inherent to biochemical reactions, and its effects on metabolic networks. They show that this noise can be reduced by increasing enzyme concentrations or decreasing reaction rates; however, these strategies are limited because they may lead to other undesirable consequences such as increased production costs for enzymes or decreased growth rate due to slower metabolism. The authors also discuss how their results could help explain why some organisms have evolved mechanisms to reduce the amount of noise in their metabolic processes (e.g., gene regulation). Finally, they suggest possible extensions of their work including studying more complex models with multiple species participating in each reaction. This article is available from: http://arxiv.org/abs/1306.5481 . Stochastic fluctuations in metabolic pathways  Authors: Yi-Chun Chen , Shih-Chieh Hwang , Chia-Hui Wu , Yu-Ting Lin , Ming-Jerng Wang , Wen-Yuan Lee , Jyh-Ming Huang , Chin-Lung Chang , Yuan-Chao Tsai , Wei-Hsien Yang , Kuo-Feng Yeh , Chung-I Wu , Tzi-Chin Chan , Cheng-Yang Liu , Chao-Kuang Chiang , Chien-Nan Chu , Chien-Wen Lu , Chien-Chi Lai , Chien-Shuu Chen , Chien-Chi Hsieh , Chien-Chi Wu , Chien-Chi Hung , Chien-Chi Li , Chien-Chi Su , Chien-Chi Liao , Chien-Chi Chen , Chien-Chiang Wu , Chien-Chiang Tai , Chien-Chiang Liang , Chien-Chiang Sun , Chien-Chiang Wei , Chien-Chiang Chen , Chien-Chang Wu , Chien-Chang Tai , Chien-Chang Liang , Chien-Chang Sun , Chien-Chang Wei , Chien-Chang Chen , Chien-Cheng Wu , Chien-Cheng Tai , Chien-Cheng Liang , Chien-Cheng Sun , Chien-Cheng Wei , Chien-Cheng Chen , Chien-Ch",
        "watermark_text": "The authors present an assessment of the stochasticity inherent to biochemical reactions , and its consequences on metabolic networks . They show that this noise can be reduced by expanding substrate concentrations or decreasing reaction rates ; however , these schemes are limited because they may contribute to other undesirable consequences such as reduced production expenses for enzymes or improved development pace leading to slower metabolism .The authors additionally discuss how their results could assist explain why some organisms have adapted processes to reduce the quantity of noise in their metabolic processes ( e . g . , gene control ) . Finally , they propose could extend of their studies namely studying more sophisticated models with many taxa participating in each synthesis .This page is accessible from : www : / / arxiv . org / abs / 1306 . 5481 . Stochastic fluctuations in metabolic mechanisms Authors : Yi - Chun Chen , Shih - Chieh Hwang , Chia - Hui Wu , Yu - Ting Lin , Ming - Jerng Wang , Wen - Yuan Lee , Jyh - Ming Huang , Chin - Lung Chang , Yuan - Chao Tsai , Wei - Hsien Yang , Kuo - Feng Yeh , Chung - I Wu , Tzi - Chin Chan , Cheng - Yang Liu , Chao - Kuang Chiang , Chien - Nan Chu , Chien - Wen Lu , Chien - Chi Lai , Chien - Shuu Chen , Chien - Chi Hsieh , Chien - Chi Wu , Chien - Chi Hung , Chien - Chi Li , Chien - Chi Su , Chien - Chi Liao , Chien - Chi Chen , Chien - Chiang Wu , Chien - Chiang Tai , Chien - Chiang Liang , Chien - Chiang Sun , Chien - Chiang Wei , Chien - Chiang Chen , Chien - Chang Wu , Chien - Chang Tai , Chien - Chang Liang , Chien - Chang Sun , Chien - Chang Wei , Chien - Chang Chen , Chien - Cheng Wu , Chien - Cheng Tai , Chien - Cheng Liang , Chien - Cheng Sun , Chien - Cheng Wei , Chien - Cheng Chen , Chien - Ch",
        "rewrite_text": "The authors provide an evaluation of the inherent stochasticity in biochemical reactions and its impact on metabolic networks. They demonstrate that this variability can be mitigated by increasing substrate concentrations or lowering reaction rates. However, these methods have limitations, as they may lead to other negative effects, such as decreased enzyme production efficiency or slower metabolic rates. Additionally, the authors explore how their findings may help explain why certain organisms have evolved mechanisms to minimize noise in their metabolic processes, such as through gene regulation. Finally, they suggest extending their research to more complex models that include multiple taxa involved in each synthesis. For further details, please visit: www://arxiv.org/abs/1306.5481. The paper, titled \"Stochastic Fluctuations in Metabolic Mechanisms,\" is authored by Yi-Chun Chen and colleagues.",
        "ori-fast-z-score": 1.0392304845413263,
        "water-fast-z-score": 8.075839156533009,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We introduce the notion of covering homology, which is an extension of ordinary homology to infinite graphs and simplicial complexes. The main idea behind this concept is that we consider not only finite subcomplexes but also their infinite analogues - so-called coverings. We show how covering homology can be used for studying properties of infinite graphs such as amenability or hyperbolicity. In particular, we prove that if a graph has infinitely many ends then its first covering homology group vanishes. This result generalizes the classical theorem by Stallings on groups with infinitely many ends. Covering homology provides us with new tools for investigating various classes of infinite objects in mathematics. It turns out that it is closely related to other concepts like equivariant homology theory (in the case when our object admits actions of some group) or higher-order algebraic K-theory. Finally, we present several examples illustrating applications of covering homology.",
        "watermark_text": "We introduce the notion of covering homology , which is an extension of regular homology to infinite graphs and simplicial complexes . The main idea behind this concept is that we treat not only finite subcomplexes but also their infinite analogues - so - called coverings .We see how covering homology can be used for studying characteristics of infinite graphs such as amenability or hyperbolicity . In particular , we prove that if a diagram has infinitely many ends then its first covering homology group vanishes .This result generalizes the classical principle by Stallings on groups with infinitely many ends . Covering homology provides us with novel techniques for investigating different categories of infinite concepts in mathematics .It turns out that it is closely related to other concepts like equivariant homology theory ( in the case when our object accepts actions of some group ) or greater - order algebraic K - theory . Finally , we present many examples illustrating applications of covering homology .",
        "rewrite_text": "We introduce the concept of covering homology, which serves as an extension of traditional homology for infinite graphs and simplicial complexes. The central premise of this idea is to consider not only finite subcomplexes but also their infinite counterparts, known as coverings. We demonstrate how covering homology can be employed to investigate properties of infinite graphs, such as amenability and hyperbolicity. Specifically, we establish that if a diagram has infinitely many ends, its first covering homology group is trivial. This finding extends Stallings' classical theorem concerning groups with infinitely many ends. Covering homology equips us with innovative methods for exploring various infinite constructs in mathematics. Notably, it has significant connections to other theories, such as equivariant homology theory (when the object in question is acted upon by a group) and higher-order algebraic K-theory. Lastly, we provide numerous examples that highlight the applications of covering homology.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 3.8450767722654624,
        "rewrite-fast-z-score": 0.24618298195866545
    },
    {
        "original_text": "We present new molecular opacity tables that include all relevant molecules in cool, carbon-rich stellar envelopes and are valid over a wide range of temperatures (T = 1000 - 10000 K), densities (ρ = 10 −10 -10 6 g/cm 3 ) and compositions (C/O=0.5-2). The calculations were performed with the state-of-the-art ab initio line-by-line radiative transfer code SPECTRUM using extensive laboratory data on molecular lines as well as theoretical predictions based on quantum chemical methods. We have calculated synthetic spectra for several model atmospheres representative of red giant branch (RGB) and asymptotic giant branch (AGB) stars to demonstrate how our new opacity tables affect their structure and evolution. Our results show that the inclusion of additional species such as SiO, TiO, VO, FeH, MgS, NaCl, CaF, AlO, CrH, MnS, CoO, NiO, ZnS, ZrO, BaO, LaO etc., which are not included in previous studies, leads to significant changes in the atmospheric structure and consequently affects the predicted surface abundances of CNO elements during the third dredge-up phase.",
        "watermark_text": "We create novel molecular opacity lists that include all relevant molecules in cold , carbon - rich stellar envelopes and are applicable over a broad variety of temperatures ( T = 1000 - 10000 K ) , densities ( ρ = 10 −10 - 10 6 g / cm 3 ) and compositions ( C / O = 0 . 5 - 2 ) . The calculations were performed with the state - of - the - art ab initio line - by - line radiative transfer code SPECTRUM combining extensive research data on chemical lines as well as conceptual predictions based on quantum chemical techniques .We have adjusted synthetic spectra for various model atmospheres representative of red dwarf branch ( RGB ) and asymptotic giant branch ( AGB ) stars to indicate how our new opacity tables affect their structure and evolution . Our results show that the inclusion of added species such as SiO , TiO , VO , FeH , MgS , NaCl , CaF , AlO , CrH , MnS , CoO , NiO , ZnS , ZrO , BaO , LaO etc . , which are not mentioned in earlier analyses , leads to significant improvements in the atmospheric composition and consequently affects the expected exterior abundances of CNO compounds during the third dredge - up process .",
        "rewrite_text": "We have developed new molecular opacity lists that encompass all relevant molecules found in cold, carbon-rich stellar envelopes, suitable for a wide range of temperatures (T = 1000 - 10000 K), densities (ρ = 10^−10 - 10^6 g/cm^3), and compositions (C/O = 0.5 - 2). Our calculations utilize the advanced ab initio line-by-line radiative transfer code SPECTRUM, which integrates comprehensive research data on chemical lines with theoretical predictions derived from quantum chemical methods. We have modified synthetic spectra for various model atmospheres representative of red giant branch (RGB) and asymptotic giant branch (AGB) stars to illustrate how our new opacity tables influence their structure and evolution. Our findings indicate that the inclusion of additional species such as SiO, TiO, VO, FeH, MgS, NaCl, CaF, AlO, CrH, MnS, CoO, NiO, ZnS, ZrO, BaO, LaO, and others, which were not considered in previous studies, leads to significant enhancements in atmospheric composition. This, in turn, impacts the expected exterior abundances of CNO elements during the third dredge-up process.",
        "ori-fast-z-score": -1.585187847802434,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We present the relativistic second order perturbation theory for fluids in curved space-time with arbitrary number of components. We derive the general expression for the energy-momentum tensor at first order in perturbations as well as its trace-free part which is responsible for gravitational waves generation. The evolution equations are derived by projecting the conservation law onto the background 4-velocity vector field. In particular we show that the presence of anisotropic stress leads to an additional source term in the equation governing the evolution of scalar modes. Finally, we discuss how our formalism can be applied to study different physical situations such as inflationary models or dark matter halos formation. Cosmology has been revolutionized over the past decade thanks to precision measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation  1  . These observations have provided us with detailed information about the early universe and allowed to test fundamental physics on very large scales  2  .\nThe standard model of cosmology assumes that the universe consists of several interacting components including cold dark matter (CDM), baryons, photons, neutrinos etc.. Each component evolves according to some set of hydrodynamical equations describing their dynamics  3  . However, these equations cannot be solved analytically even if one neglects all interactions between particles  4  , so numerical simulations are required  5  . On the other hand, analytical solutions exist only under certain approximations  6  . For example, it was shown recently  7, 8  that the effect of pressure gradients may lead to significant corrections to the growth rate of density perturbations during the late stages of structure formation  9  .",
        "watermark_text": "We introduce the relativistic second order perturbation theory for fluids in curved space - time with arbitrary number of components . We derive the general expression for the power - momentum tensor at first order in perturbations as well as its trace - free portion which is responsible for gravitational waves generation .The evolution equations are derived by projecting the conservation law onto the background 4 - velocity vector field . In particular we find that the presence of anisotropic stress leads to an additional source term in the equation regulating the evolution of scalar modes .Finally , we talk how our formalism can be applied to study various physical conditions such as inflationary theories or black particle halos formation . Cosmology has been revolutionized over the previous decade courtesy to accurate measurements of temperature fluctuations in the cosmic microwave background ( CMB ) radiation 1 .These measurements have provided us with comprehensive information about the early universe and enable to test fundamental theory on very huge scales 2 . The conventional model of cosmology assumes that the universe consists of several interacting components namely cold bright matter ( CDM ) , baryons , photons , neutrinos etc . . Each component evolves due to some setting of hydrodynamical equations explaining their mechanics 3 .However , these equations never be answered analytically even if one neglects all interactions between particles 4 , so numerical simulations are required 5 . On the other hand , analytical solutions arise only under certain approximations 6 .For instance , it was shown ago 7 , 8 that the impact of force gradients might lead to significant corrections to the development time of density perturbations during the late stages of formation formation 9 .",
        "rewrite_text": "We present a relativistic second-order perturbation theory for fluids in curved spacetime, accommodating an arbitrary number of components. We derive the general expression for the power-momentum tensor at first-order perturbations, along with its trace-free part, which is responsible for the generation of gravitational waves. The evolution equations are obtained by projecting the conservation law onto the background four-velocity vector field. Notably, we find that the presence of anisotropic stress introduces an additional source term in the equation governing the evolution of scalar modes. We also discuss how our framework can be utilized to explore various physical scenarios, such as inflationary models and the formation of black particle halos. The field of cosmology has undergone a transformation in the last decade, largely owing to precise measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation. These measurements have provided us with detailed insights into the early universe and allowed us to test fundamental theories on vast scales. The standard cosmological model posits that the universe comprises several interacting components, including cold dark matter (CDM), baryons, photons, and neutrinos, each evolving according to hydrodynamical equations that dictate their dynamics. However, these equations generally cannot be solved analytically, even when interactions between particles are disregarded, necessitating numerical simulations. In contrast, analytical solutions can only be derived under specific approximations. For example, previous studies have demonstrated that force gradients can significantly affect the development time of density perturbations during the later stages of their formation.",
        "ori-fast-z-score": -2.1213203435596424,
        "water-fast-z-score": 5.1946603473211805,
        "rewrite-fast-z-score": -0.1796053020267749
    },
    {
        "original_text": "Dans cet article, nous donnons une nouvelle preuve de la classification bimeromorphee des varietes kahleriennes compactes en utilisant les orbifolds speciaux. Nous montrons que si X est un orbifold special alors le groupe fondamental de X s identifie au groupe fondamental d une variete kahlerienne compacte Y . _SPLIT_ En particulier, on obtient que tout orbifold special admet une resolution symplectique. On montre aussi que toute application holomorphe entre deux orbifolds speciaux induit par passage aux groupes fondamentaux une application holomorphe sur leurs resolutions respectives. Enfin, on donne quelques exemples d orbifolds speciaux qui ne sont pas resolubles. Keywords: Orbifoldes spéciaux, Variétés Kähleriennes Compactes, Groupes fondamentaux, Résolutions symplectiques. 1 Introduction Soit X un espace analytique complexe connexe muni d un feuilletage holomorphe F . Un tel objet sera appelé dans ce texte un orbifold spécial (voir  V ). Dans cette note, nous allons donner une nouvelle démonstration du théorème suivant dû à Verbitsky : Théorème 0.1. (Verbitsky) Tout orbifold spécial admet une résolution symplectique.",
        "watermark_text": "Dans cet article , nous donnons une nouvelle preuve de la classification bimeromorphee des varietes kahleriennes compactes en utilisant les orbifolds speciaux . Nous montrons que si X est un orbifold special alors la groupe fondamental de X s identifie au groupe fondamental d la variete kahlerienne compacte Y ._ SPLIT _ En particulier , on obtient que tout orbifold special admet une resolution symplectique . On montre aussi que toute application holomorphe entre deux orbifolds speciaux induit par passage des groupes fondamentaux la applied holomorphe sur leurs resolutions respectives .Enfin, on donne quelques exemples d orbifolds speciaux qui ne sont pas resolubles.Keywords: Orbifoldes spéciaux, Variétés Kähleriennes Compactes, Groupes fondamentaux, Résolutions symplectiques.1 Introduction Soit X un espace analytique complexe connexe muni d en feuilletage holomorphe F . Un tel objet sera appelé dans ce texte un orbifold spécial ( voir V ) .Dans cette report , nous allons donner à nouvelle démonstration du théorème suivant dû à Verbitsky : Théorème 0 . 1 . ( Verbitsky ) Tout orbifold spécial admet une résolution symplectique .",
        "rewrite_text": "Dans cet article, nous proposons une nouvelle démonstration de la classification bimeromorphe des variétés Kähleriennes compactes en utilisant les orbifolds spéciaux. Nous établissons que si X est un orbifold spécial, alors son groupe fondamental coïncide avec celui de la variété Kählerienne compacte Y. En particulier, cela implique que tout orbifold spécial possède une résolution symplectique. Nous démontrons également que toute application holomorphe entre deux orbifolds spéciaux entraîne, par le biais des groupes fondamentaux, une application holomorphe sur leurs résolutions respectives. Enfin, nous fournissons quelques exemples d'orbifolds spéciaux qui ne peuvent pas être résolus. \n\nMots-clés : Orbifolds spéciaux, Variétés Kähleriennes compactes, Groupes fondamentaux, Résolutions symplectiques.\n\n1 Introduction\n\nSoit X un espace analytique complexe connexe doté d'un feuilletage holomorphe F. Cet objet sera désigné dans cet article comme un orbifold spécial (voir V). Dans ce rapport, nous allons présenter une nouvelle démonstration du théorème suivant, attribué à Verbitsky : \n\nThéorème 0.1 (Verbitsky) : Tout orbifold spécial admet une résolution symplectique.",
        "ori-fast-z-score": -0.1690308509457033,
        "water-fast-z-score": 2.263009527424072,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "We present an analysis of all known short-period cataclysmic variables (CVs) in which we find that most systems have orbital periods longer than 3 hrs and are dominated by SW Sex stars, while those with shorter periods tend to be AM Her binaries. We show that this dichotomy is consistent with theoretical predictions for the evolution of CVs driven by angular momentum loss via gravitational radiation. The observed distribution of orbital periods can also be explained if there exists a minimum period below which no CVs exist due to magnetic braking. This result has important implications on our understanding of how CVs evolve towards shorter orbital periods. Cataclysmic Variables (CVs), interacting binary star systems consisting of a white dwarf primary accreting matter from its low-mass companion through Roche lobe overflow, are among the best studied classes of close binary stars. They provide unique opportunities to study many aspects of astrophysics such as stellar structure and evolution, mass transfer processes, nuclear burning at high temperatures, and relativistic effects near compact objects. In particular, they offer insights into the formation mechanisms of both single and double degenerate white dwarfs, the progenitors of Type Ia supernovae.",
        "watermark_text": "We present an assessment of all known short - period cataclysmic variables ( CVs ) in which we find that most components have orbital periods longer than 3 hrs and are dominated by SW Sex stars , while those with shorter cycles seem to be AM Her binaries . We suggest that this dichotomy is compatible with theoretical estimates for the evolution of CVs caused by angular velocity loss via gravitational rays .The observed pattern of orbital periods can also be described if there exists a minimum period below which no CVs occur due to magnetic braking . This result has crucial consequences on our understanding of how CVs develop towards shorter orbital periods .Cataclysmic Variables ( CVs ) , interacting binary star systems composed of a white dwarf secondary accreting matter from its low - weight sister through Roche lobe overflow , are among the best researched groups of close binary stars . They offer distinct options to study many aspects of astrophysics such as stellar formation and evolution , mass transfer mechanisms , nuclear burning at high temperatures , and relativistic effects near compact galaxies .In particular , they give insights into the formation patterns of both single and double degenerate white dwarfs , the progenitors of Type Ia supernovae .",
        "rewrite_text": "We provide an evaluation of all known short-period cataclysmic variables (CVs), revealing that the majority have orbital periods exceeding 3 hours, primarily characterized by SW Sex stars. In contrast, those with shorter periods appear to be AM Her binaries. We propose that this distinction aligns with theoretical models of CV evolution, which consider angular momentum loss due to gravitational waves. Furthermore, the observed distribution of orbital periods may indicate a minimum period below which no CVs exist, potentially due to magnetic braking effects. This finding has significant implications for our understanding of how CVs evolve toward shorter orbital periods. Cataclysmic variables, which are interacting binary star systems consisting of a white dwarf that accretes matter from its lower-mass companion through Roche lobe overflow, represent one of the most studied categories of close binary stars. They provide valuable opportunities to investigate various astrophysical phenomena, including stellar formation and evolution, mass transfer processes, high-temperature nuclear burning, and relativistic effects in proximity to compact objects. Specifically, they shed light on the formation mechanisms of both single and double degenerate white dwarfs, the progenitors of Type Ia supernovae.",
        "ori-fast-z-score": 0.618852747755276,
        "water-fast-z-score": 5.982243228301002,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We present the results of an analysis of galaxy cluster data in terms of their gravitational lensing properties and X-ray emission, with particular emphasis on the comparison between observed and predicted values for the mass-to-light ratio M/L. We find that the best-fit value of this quantity is consistent with the predictions based on standard CDM models if one assumes that most of the baryonic component of these systems resides within galaxies rather than being distributed throughout the intracluster medium (ICM). This result suggests that the ICM may be heated by some mechanism other than gravity alone. \n \n Keywords: Galaxy cluster, Dark Matter Halo, Gravitational Lensing, Mass-to-Light Ratio, X-Ray Emission \n \n \n \n 1 Introduction \n \n The study of galaxy clusters has been instrumental to our understanding of cosmology over the past few decades. In fact, it was through observations of galaxy clusters that we first discovered evidence supporting the existence of non-baryonic dark matter  1  . Today, galaxy clusters are still used extensively to test theories about structure formation  2  , and they provide important constraints on cosmological parameters such as the Hubble constant  3  or the equation-of-state parameter w  4  . \n \n However, despite all its successes, there remain several open questions regarding galaxy clusters which have yet to be answered satisfactorily. For example, while current observational techniques allow us to measure accurately the total amount of light emitted by a galaxy cluster, it remains difficult to determine how much of this light comes from stars inside individual galaxies versus diffuse gas located outside them  5  . Similarly, although we can estimate fairly well the total gravitating mass of a galaxy cluster using various methods  6  , it is not clear what fraction of this mass is associated with visible objects like galaxies  7, 8  . Finally, even though we know that galaxy clusters contain large amounts of hot plasma  9  , it is unclear whether this material is gravitationally bound to the system  10  .\n \nIn order to address these issues, we will use two different datasets obtained from the Chandra Observatory  11  : the sample of galaxy clusters studied by Vikhlinin et",
        "watermark_text": "We present the conclusion of an assessment of galaxy cluster data in terms of their gravitational lensing behavior and X - ray radiation , with particular emphasis on the comparison between seen and anticipated readings for the mass - to - light value M / L . We see that the best - fitting value of this quantity is compatible with the estimates based on normal CDM models if one suppose that most of the baryonic core of these systems resides within stars rather than being dispersed throughout the intracluster medium ( ICM ) .This result suggests that the ICM could be heated by some process other than gravity alone . Keywords : Galaxy cluster , Dark Matter Halo , Gravitational Lensing , Mass - to - Light Ratio , X - Ray Emission 1 Introduction The investigation of galaxy galaxies has been instrumental to our understanding of cosmology over the previous few years .In indeed , it was through observations of galaxy clusters that we first discovered evidence proving the existence of non - baryonic black material 1 . Today , galaxy rings are still used heavily to test models about structure formation 2 , and they give important restrictions on cosmological factors such as the Hubble constant 3 or the equation - of - state variable f 4 .However , despite all its victories , there remain many open questions regarding galaxy clusters which have yet to be answered satisfactorily . For instance , while contemporary observational techniques permit us to measure correctly the total amount of light emitted by a galaxy cluster , it remains impossible to predict how many of this light originates from stars inside individual stars vs diffuse gas located outside them 5 .Similarly , although we can calculate fairly good the total gravitating mass of a galaxy cluster using numerous technique 6 , it is not clear what fraction of this mass is associated with visible objects like stars 7 , 8 . Finally , even though we know that galaxy regions include significant amounts of bright plasma 9 , it is uncertain whether this material is gravitationally bound to the system 10 .In order to meet these problems , we will use two different datasets obtained from the Chandra Observatory 11 : the sample of galaxy regions investigated by Vikhlinin et",
        "rewrite_text": "We present the findings from an analysis of galaxy cluster data, focusing on their gravitational lensing characteristics and X-ray emissions. Our primary goal is to compare observed versus expected mass-to-light ratio (M/L) values. The results indicate that the optimal M/L value aligns with estimates based on standard cold dark matter (CDM) models, provided we assume that a significant portion of the baryonic matter within these clusters is concentrated in stars rather than distributed throughout the intracluster medium (ICM). This observation implies that there may be processes other than gravity contributing to the heating of the ICM. \n\nKeywords: Galaxy cluster, Dark Matter Halo, Gravitational Lensing, Mass-to-Light Ratio, X-Ray Emission.\n\n1. Introduction\n\nThe study of galaxy clusters has significantly advanced our understanding of cosmology over recent years. In fact, our early evidence for non-baryonic dark matter emerged from observations of galaxy clusters. Today, galaxy clusters continue to serve as critical testbeds for theories of structure formation, providing vital constraints on cosmological parameters such as the Hubble constant and the equation of state parameter. However, despite these advances, many questions regarding galaxy clusters remain unresolved. For example, while modern observational techniques enable accurate measurements of the total light emitted by a galaxy cluster, it is still challenging to determine how much of that light is produced by individual stars compared to the diffuse gas surrounding them. Similarly, though we can estimate the total gravitational mass of a galaxy cluster through various methods, the proportion of this mass that corresponds to visible objects, such as stars, is still uncertain. Lastly, although we recognize that galaxy clusters contain substantial amounts of luminous plasma, it remains unclear whether this plasma is gravitationally bound to the system. To address these issues, we will utilize two distinct datasets obtained from the Chandra Observatory, including the sample of galaxy clusters analyzed by Vikhlinin et al.",
        "ori-fast-z-score": -1.5811388300841895,
        "water-fast-z-score": 8.171011693711216,
        "rewrite-fast-z-score": -1.9215378456610455
    },
    {
        "original_text": "We present new maps of the circumstellar SiO (v=1, v=2) masers around the Mira variable R Leo obtained with the Very Long Baseline Array at 22 GHz and 43 GHz. The observations were carried out on September 24th 2004 using all ten antennas available for VLBA operation during that time period. We detect two distinct groups of masers: one group is located near the star s position as determined by optical astrometry; another group lies about 0.5 arcsec to the south-west of this location. Both groups are associated with an extended bipolar structure seen in previous single-dish observations. This structure has been interpreted as a shell-like envelope surrounding the central star. Our results show that both groups of masers trace different parts of this shell-like structure. In addition we find evidence for a third component which may be related to the presence of a companion object. \n \n Keywords: Masers",
        "watermark_text": "We report new maps of the circumstellar SiO ( v = 1 , v = 2 ) masers around the Mira variable R Leo achieved with the Very Long Baseline Array at 22 GHz and 43 GHz . The surveys were carried out on September 24th 2004 utilizing all ten antennas used for VLBA operation during that time time .We detect two different bands of masers : one group is situated near the star s position as determined by optical astrometry ; another group lies about 0 . 5 arcsec to the south - west of this location . Both groups are identified with an extended bipolar structure seen in earlier single - dish measurements .This structure has been interpreted as a shell - like envelope surrounding the main star . Our results show that both bands of masers trace various parts of this shell - like structure .In addition we find proof for a third element which may be connected to the presence of a companion object . Keywords : Masers",
        "rewrite_text": "We present new maps of the circumstellar SiO (v = 1, v = 2) masers surrounding the Mira variable R Leo, obtained using the Very Long Baseline Array at frequencies of 22 GHz and 43 GHz. The observations were conducted on September 24, 2004, utilizing all ten antennas available for VLBA operations at that time. We identified two distinct groups of masers: one group is located close to the star’s position as determined by optical astrometry, while another group is approximately 0.5 arcseconds to the southwest of this position. Both groups correspond to an extended bipolar structure observed in previous single-dish measurements, which has been interpreted as a shell-like envelope enveloping the main star. Our findings indicate that both maser groups trace different regions of this shell-like structure. Additionally, we provide evidence for a third component that may be related to the presence of a companion object. Keywords: Masers.",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 5.588005588008382,
        "rewrite-fast-z-score": 1.016001016001524
    },
    {
        "original_text": "We present an evolutionary algorithm for the problem of mesh numbering, which is to assign integers in the range  0, ..., n-1  to all vertices of a given graph G = (V, E) such that adjacent vertices have consecutive numbers and no two edges share the same pair of endpoints with the same order.  The proposed method uses a population-based approach where each individual represents a valid solution by means of a permutation vector. We show how this representation can be used to efficiently compute fitness values using only local information. In addition we propose several genetic operators to explore the search space. Finally, we report on preliminary results obtained by applying our method to some well-known benchmark instances. Keywords: Graphs, Optimization problems, Evolutionary algorithms, Genetic algorithms. 1 Introduction A common task when working with graphs is to label their nodes or edges with unique identifiers. This process is known as node or edge numbering respectively. For example, it may be required to number the nodes of a road network so that every path between any two points has a unique sequence of labels. Another application arises in circuit design, where one needs to assign unique addresses to components of electronic circuits.",
        "watermark_text": "We present an evolutionary algorithm for the question of mesh numbering , which is to assign integers in the range 0 , . . . , k - 1 to all edges of a given graph G = ( V , E ) such that adjacent vertices have consecutive numbers and no two edges occupy the same pair of endpoints with the same order . The proposed approach utilizes a population - based approach where each individual represents a valid answer by means of a permutation matrix .We see how this representation can be used to easily compute fitness values utilizing only local information . In addition we propose several genetic functions to examine the search space .Finally , we publish on preliminary results acquired by using our technique to some well - famous benchmark instances . Keywords : Graphs , Optimization problems , Evolutionary algorithms , Genetic algorithms .1 Introduction A typical task when dealing with graphs is to label their edges or edges with special identifiers . This process is known as node or edge numbering respectively .For instance , it could be required to count the nodes of a street system so that every path between any two points has a unique string of tags . Another application exists in circuit design , where one needs to give unique addresses to components of electronic circuits .",
        "rewrite_text": "We introduce an evolutionary algorithm designed to tackle the problem of mesh numbering, which involves assigning integers within the range of 0 to k - 1 to all edges of a given graph G = (V, E), ensuring that adjacent vertices receive consecutive numbers and that no two edges share the same ordered pair of endpoints. Our approach employs a population-based strategy, with each individual representing a valid solution through a permutation matrix. This representation allows for the straightforward calculation of fitness values using only local information. Additionally, we propose several genetic operations to explore the search space effectively. Finally, we present preliminary results obtained by applying our method to several well-known benchmark instances. \n\nKeywords: Graphs, Optimization problems, Evolutionary algorithms, Genetic algorithms.\n\n1 Introduction\nLabeling edges or nodes in graphs is a common task, referred to as edge or node numbering, respectively. For example, one might need to assign unique tags to the nodes in a roadway system so that each route between any two points has a distinct sequence of identifiers. Another application can be found in circuit design, where unique addresses must be assigned to components of electronic circuits.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 5.273697108112943,
        "rewrite-fast-z-score": 0.4
    },
    {
        "original_text": "We report on the experimental demonstration of frequency upshift in two colliding laser pulses using a relativistically flying mirror (RFM). The RFM is realized as an ultrathin foil accelerated to high velocities by intense femtosecond laser pulses focused onto it at grazing incidence angle. We show that the collision between the counter-propagating laser pulses leads to the generation of new frequencies, which are shifted towards higher values compared to those generated without the presence of the RFM. This effect can be explained within the framework of nonlinear optics and quantum electrodynamics. Our results demonstrate the possibility for generating high-energy photons via collisions of laser pulses in vacuum. These findings may have important implications for future applications such as particle acceleration or gamma-ray sources based on table-top experiments. \n \n In recent years there has been growing interest in studying the interaction of ultra-intense lasers with matter under extreme conditions  1  . One particular area of research focuses on the investigation of novel phenomena associated with the propagation of light in vacuum  2  , where the effects of strong field QED  3  become relevant  4  . For example, the emission of energetic electrons  5  and positrons  6  into vacuum was observed experimentally  7-9  when intense laser pulses were focused onto thin foils  10  . Moreover, the production of energetic photons  11  and pairs  12  in vacuum was predicted theoretically  13-15  .\n \nIn this Letter we present our experimental study of another interesting phenomenon related to the propagation of light in vacuo -the so-called relativistic tennis  16  . It consists of two counterpropagating laser pulses interacting with each other inside a vacuum chamber  17  . When these pulses collide they generate new frequencies  18  , which are shifted towards higher energies  19  . This effect occurs due to the fact that the electric fields of both pulses add coherently  20  leading to the formation of a standing wave pattern  21  . As a result, the intensity of the standing wave increases significantly  22  causing the appearance of new frequencies  23  . \n \n Here we report on the first experimental observation of the relativistic tennis effect  24  . To achieve this goal, we used a relativistically flying mirror  25  , which",
        "watermark_text": "We report on the empirical demonstration of rate upshift in two colliding laser pulses using a relativistically flying reflection ( RFM ) . The RFM is realized as an ultrathin foil enhanced to large velocities by intense femtosecond laser pulses focused onto it at grazing incidence angle .We see that the interaction between the counter - propagating optical pulses gives to the generation of new frequencies , which are shifted towards higher values compared to those generated without the presence of the RFM . This phenomenon can be described within the framework of nonlinear optics and quantum electrodynamics .Our results show the idea for generating high - energy photons via collisions of laser pulses in vacuum . These studies might have important implications for future applications such as particle gravity or gamma - ray sources based on table - top research .In recent years there has been growing interest in investigating the interaction of ultra - intense lasers with matter under extreme circumstances 1 . One particular area of research focuses on the exploration of new events associated with the propagation of light in vacuum 2 , where the effects of bright force QED 3 become relevant 4 .For instance , the emission of active electrons 5 and positrons 6 into vacuum was seen experimentally 7 - 9 when strong laser pulses were focused onto thin foils 10 . Moreover , the production of energetic photons 11 and pairs 12 in vacuum was anticipated theoretically 13 - 15 .In this Letter we present our experimental discussion of another important process related to the propagation of light in vacuo - the so - called relativistic tennis 16 . It consists of two counterpropagating light beams interacting with each other inside a vacuum chamber 17 .When these pulses collide they generate additional frequencies 18 , which are shifted towards higher energies 19 . This phenomenon occurs due to the fact that the electric forces of both pulses add coherently 20 resulting to the formation of a standing wave pattern 21 .As a result , the strength of the running wave increases substantially 22 resulting the appearance of new frequencies 23 . Here we note on the first experimental measurement of the relativistic tennis phenomenon 24 .To achieve this goal , we using a relativistically flying reflection 25 , which",
        "rewrite_text": "We present empirical evidence of a rate upshift resulting from the collision of two laser pulses, facilitated by a relativistically flying reflector (RFM). The RFM is implemented as an ultra-thin foil accelerated to high velocities by intense femtosecond laser pulses focused at a grazing incidence angle. Our observations reveal that the interaction between the counter-propagating optical pulses leads to the generation of new frequencies, which are shifted to higher values compared to those produced in the absence of the RFM. This occurrence can be understood in the context of nonlinear optics and quantum electrodynamics. Our findings suggest a method for producing high-energy photons through the collision of laser pulses in a vacuum. This research could have significant implications for future applications, such as particle gravity studies or compact gamma-ray sources. Recently, interest has surged in examining how ultra-intense lasers interact with matter under extreme conditions. One significant area of focus is the exploration of phenomena associated with light propagation in vacuum, where the effects of intense quantum electrodynamics are pertinent. For example, experimental observations have noted the emission of active electrons and positrons into vacuum when strong laser pulses are directed at thin foils. Additionally, the theoretical anticipation of energetic photon and pair production in vacuum has been discussed. In this letter, we explore another crucial process connected to light propagation in vacuo known as relativistic tennis. This process involves two counterpropagating light beams interacting within a vacuum chamber. When these pulses collide, they produce additional frequencies that are shifted to higher energy levels. This phenomenon arises because the electric fields of the two pulses coherently combine, creating a standing wave pattern. Consequently, the intensity of the running wave is significantly enhanced, leading to the emergence of new frequencies. We proudly present the first experimental measurement of the relativistic tennis phenomenon, achieved through the use of a relativistically flying reflector.",
        "ori-fast-z-score": 0.8563488385776753,
        "water-fast-z-score": 7.96486185631891,
        "rewrite-fast-z-score": 2.7583864218368523
    },
    {
        "original_text": "We study the gravitational field generated by a massive scalar point source moving on an equatorial circular geodesic around a Schwarzschild black hole, and we evolve it numerically in two spatial dimensions (2+1) with the puncture method. We find that the perturbation is dominated by a single mode which grows exponentially as time goes on. The growth rate agrees well with the prediction based on quasinormal modes for this system. This result suggests that the exponential growth may be related to the instability of the scalar field near the horizon. In addition, we also show that the amplitude of the growing mode decreases rapidly when the mass of the scalar field increases. Finally, we discuss possible applications of our results to astrophysical phenomena such as gamma-ray bursts. \n \n Introduction \n \n Black holes are among the most fascinating objects predicted by general relativity. They have been studied extensively both theoretically and observationally over many decades  1  . One important aspect of their physics concerns how particles move close to them  2  , especially those that can escape from the black hole s gravity  3  . It has recently become clear that there exist some interesting physical processes taking place very close to the event horizon  4  -  6  . For example, if one considers a charged particle falling into a Reissner-Nordström black hole, then its motion will be unstable due to the so-called  photon sphere effect   7, 8  . If the charge of the particle is sufficiently large, then the particle will eventually fall into the black hole after emitting photons  9  . Another interesting phenomenon occurs when a neutral particle falls into a Kerr black hole  10  . Here again, the motion becomes unstable because of the existence of the photon sphere  11  . However, unlike the case of a Reissner-Norström black hole, the emitted radiation now contains not only photons but also gravitons  12  . \n \n In recent years, much attention has been paid to studying the dynamics of fields outside black holes  13  -  17  . In particular, the problem of finding the spectrum of quasi-normal modes (QNMs), i.e., the characteristic frequencies at",
        "watermark_text": "We test the gravitational field produced by a huge scalar point source rotating on an equatorial circular geodesic around a Schwarzschild black hole , and we evolve it numerically in two spatial dimensions ( 2 + 1 ) with the puncture method . We see that the perturbation is dominated by a single mode which increases exponentially as time went on .The growth speed agrees well with the observation based on quasinormal modes for this scheme . This result suggests that the exponential growth could be connected to the instability of the scalar field near the horizon .In addition , we also demonstrate that the frequency of the increasing mode decreases quickly when the mass of the scalar field increases . Finally , we explain possible applied of our findings to astrophysical processes such as gamma - ray bursts .Introduction Black holes are among the most beautiful objects anticipated by general relativity . They have been studied frequently both theoretically and observationally over numerous centuries 1 .One important element of their physics matters how particles moving nearer to them 2 , particularly those that can escape from the dark hole s gravity 3 . It has recently become clear that there exist some interesting physical processes take place very close to the event horizon 4 - 6 .For instance , if one considers a charged particle falling into a Reissner - Nordström black hole , then its motion will be unstable due to the so - called photon sphere phenomenon 7 , 8 . If the charge of the particle is sufficiently huge , then the particle will eventually go into the dark hole after emitting photons 9 .Another important process occurs when a neutral element goes into a Kerr black hole 10 . Here again , the movement becomes unstable because of the existence of the photon sphere 11 .However , unlike the case of a Reissner - Norström black hole , the emitted radiation now contains not only photons but also gravitons 12 . In recent years , much attention has been paid to researching the dynamics of fields outside brown holes 13 - 17 .In particular , the question of finding the spectrum of quasi - normal frequencies ( QNMs ) , i . e . , the typical frequencies at",
        "rewrite_text": "We investigate the gravitational field generated by a massive scalar point source that is rotating along an equatorial circular geodesic around a Schwarzschild black hole, employing a numerical approach in two spatial dimensions (2 + 1) using the puncture method. Our findings reveal that the perturbation is predominantly characterized by a single mode, which exhibits exponential growth over time. The rate of this growth aligns well with observations derived from quasinormal modes within this framework. This observation implies that the exponential increase may be linked to the instability of the scalar field in the vicinity of the event horizon. Furthermore, we demonstrate that the frequency of this growing mode rapidly decreases as the mass of the scalar field increases. Lastly, we discuss the potential applications of our results to astrophysical phenomena, such as gamma-ray bursts.\n\n**Introduction**\nBlack holes are among the most fascinating entities predicted by general relativity, attracting extensive theoretical and observational research over the centuries. A key aspect of their physics involves the behavior of particles in close proximity to them, particularly those that can escape their gravitational influence. Recent studies have revealed intriguing physical processes occurring near the event horizon. For example, a charged particle falling into a Reissner-Nordström black hole experiences instability in its motion due to the presence of a photon sphere. If the particle's charge is sufficiently large, it will ultimately be captured by the black hole after radiating photons. Similarly, when a neutral particle approaches a Kerr black hole, its trajectory becomes unstable due to the photon sphere's influence. However, in contrast to the Reissner-Nordström scenario, the emitted radiation from this interaction includes not only photons but also gravitons. In recent years, there has been a growing focus on the dynamics of fields outside black holes, particularly concerning the investigation of the spectrum of quasinormal frequencies (QNMs), which represent the characteristic frequencies associated with these systems.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 7.526023228839096,
        "rewrite-fast-z-score": 0.24413653763134782
    },
    {
        "original_text": "We study the effect of measurement noise on the Markov property for stochastic processes with continuous state spaces and discrete time steps. We show that, under certain conditions, the noisy process is still Markovian if its transition probabilities are modified by an exponential factor depending only on the noise level. This result can be used to derive efficient algorithms for computing the stationary distribution of such processes. The results presented here generalize previous work on this topic which was restricted to finite-state-space models. In addition we provide examples illustrating how our theory applies to several important classes of stochastic processes including diffusion processes, autoregressive moving average (ARMA) processes, and hidden Markov models. \nI. INTRODUCTORY REMARK\nThe main goal of this article is to present some new theoretical results about the effects of measurement noise on the statistical behavior of stochastic processes. These results will then be applied to develop efficient numerical methods for estimating the stationary distributions of various types of stochastic processes. Our approach relies heavily on recent advances made in the field of nonlinear filtering  1  , where it has been shown that many interesting problems related to estimation or prediction can often be solved efficiently using techniques based on the concept of particle filters  2  .",
        "watermark_text": "We test the impact of monitoring interference on the Markov property for stochastic systems with continuous state spaces and finite period steps . We see that , under certain conditions , the noisy process is nevertheless Markovian if its transition probabilities are modified by an exponential factor depending only on the noise rate .This result can be used to derive elegant techniques for modeling the stationary distribution of such processes . The results presented here generalize past work on this topic which was confined to finite - state - space models .In addition we provide examples illustrating how our theory applies to several important classes of stochastic systems including diffusion processes , autoregressive moving average ( ARMA ) events , and hidden Markov models . I .INTRODUCTORY REMARK The main goal of this page is to provide some fresh theoretical results about the effects of measurement noise on the statistical behavior of stochastic systems . These conclusions will then be applied to develop able numerical models for estimating the stationary distributions of several kinds of stochastic systems .Our solution draws highly on current developments made in the field of nonlinear routing 1 , where it has been shown that several interesting problems related to estimation or prediction can often be answered easily using techniques based on the idea of particle filters 2 .",
        "rewrite_text": "We examine how monitoring interference affects the Markov property in stochastic systems with continuous state spaces and finite time steps. Our findings indicate that, under certain conditions, the noisy process can still be considered Markovian if its transition probabilities are adjusted by an exponential factor that depends solely on the noise rate. This insight enables the development of effective methods for modeling the stationary distributions of such processes. Our results extend previous research, which was limited to finite-state-space models. Furthermore, we provide examples that demonstrate our theory's applicability to several significant categories of stochastic systems, including diffusion processes, autoregressive moving average (ARMA) events, and hidden Markov models.\n\nI. INTRODUCTORY REMARK\n\nThe primary objective of this section is to present new theoretical findings regarding the influence of measurement noise on the statistical behavior of stochastic systems. These insights will subsequently be utilized to create practical numerical models for estimating the stationary distributions of various types of stochastic systems. Our approach builds significantly on recent advancements in the field of nonlinear routing, where it has been shown that many intriguing problems related to estimation or prediction can often be addressed effectively using techniques derived from the concept of particle filters.",
        "ori-fast-z-score": 0.29002094671369905,
        "water-fast-z-score": 7.057176370033344,
        "rewrite-fast-z-score": 2.6943012562182536
    },
    {
        "original_text": "The influence of pulsed magnetic fields on the relaxation processes in HTSC was investigated by measuring the temperature dependence of resistance and Hall coefficient for samples with different oxygen content (d = 0, 1). The results show that the application of pulsed magnetic fields leads to an increase in the resistivity and Hall mobility of the sample with d = 0. This effect is explained as due to the appearance of additional scattering centers caused by defects formed during the process of magnetization reversal. In contrast, no significant changes were observed in the case of the sample with d=1. It can be assumed that this difference is associated with the presence of structural disordering in the crystal lattice of the latter compound. \n \n Keywords: High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defects Formation, Magnetoresistance, Hall Effect. Introduction \n \n Investigation of relaxation phenomena in high temperature superconductors under the action of pulsed external magnetic fields has been attracting considerable attention recently  1 - 5  . These studies are important both for understanding the physics of these materials and for practical applications  6  -  8  . \n \n In particular, it should be noted that the investigation of relaxation processes in HTSCs allows one to study the dynamics of defect formation  9  , which plays an important role in determining their transport properties  10  . At present there are several models describing the mechanism of defect generation  11  -  13  . However, none of them takes into account the possibility of defect formation induced by the action of pulsed fields  14  . \nExperimental details\n\nIn our work we used single crystals of two compounds with different oxygen content: HoBa 2 Cu 3 O 7−δ (HBS) and YBa 2 Cu 3 O 6+δ (YBS), grown using the floating zone method  15  . The oxygen concentration in the samples was determined by iodometric titration  16  . The typical size of the samples was about 5 × 4 mm 2 . The measurements were carried out in liquid helium cryostats equipped with pulse magnets  17  . The maximum value of the magnetic induction reached up to B max =",
        "watermark_text": "The impact of pulsed magnetic fields on the relaxation processes in HTSC was investigated by monitoring the temperature dependence of resistance and Hall coefficient for specimens with various oxygen composition ( d = 0 , 1 ) . The results show that the introduction of pulsed magnetic fields leads to an increase in the resistivity and Hall velocity of the sample with d = 0 .This phenomenon is understood as owing to the appearance of added scattering centers caused by defects formed during the process of magnetization reversal . In comparison , no major changes were detected in the case of the sample with d = 1 .It can be assumed that this contrast is associated with the presence of structural disordering in the crystal structures of the latter chemical . Keywords : High - Tc Superconductor , Pulsed Magnetic Field , Relaxation Processes , Defects Formation , Magnetoresistance , Hall Effect .Introduction Investigation of relaxation effects in high heat superconductors under the action of pulsed external magnetic waves has been drawing greater notice lately 1 - 5 . These studies are important both for studying the physics of these structures and for useful use 6 - 8 .In particular , it should be mentioned that the examination of relaxation processes in HTSCs allows one to study the dynamics of defect form 9 , which plays an important role in establishing their transport properties 10 . At currently there are several models explaining the process of defect generation 11 - 13 .However , none of them took into consideration the danger of defect development caused by the activity of pulsed fields 14 . Experimental details In our work we using single crystals of two compounds with varying oxygen composition : HoBa 2 Cu 3 O 7−δ ( HBS ) and YBa 2 Cu 3 O 6 + δ ( YBS ) , grown using the sliding zone method 15 .The oxygen composition in the tests was calculated by iodometric titration 16 . The typical size of the samples was about 5 × 4 mm 2 .The measurements were carried out in pure helium cryostats fitted with pulse magnets 17 . The maximum value of the magnetic induction reached up to B max =",
        "rewrite_text": "The effects of pulsed magnetic fields on the relaxation processes in high-temperature superconductors (HTSC) were examined by assessing the temperature dependence of resistance and Hall coefficient in specimens with different oxygen compositions (d = 0, 1). The findings indicated that the application of pulsed magnetic fields resulted in increased resistivity and Hall velocity for the sample with d = 0. This increase is attributed to the emergence of additional scattering centers arising from defects generated during the magnetization reversal process. In contrast, no significant changes were observed in the sample with d = 1, suggesting that this disparity may be linked to the structural disorder present in the crystal structure of that particular compound.\n\n**Keywords:** High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defect Formation, Magnetoresistance, Hall Effect.\n\n**Introduction**: Recent studies have increasingly focused on the relaxation effects in high-temperature superconductors under the influence of pulsed external magnetic fields. These investigations are critical for understanding the physics of these materials and their practical applications. Notably, studying the relaxation processes in HTSCs provides insights into the dynamics of defect formation, which plays a crucial role in determining their transport properties. Although several models have been proposed to explain defect generation, none have adequately accounted for the potential risks of defect development resulting from pulsed field activity.\n\n**Experimental Details**: In this study, we utilized single crystals of two compounds with varying oxygen compositions: HoBa2Cu3O7−δ (HBS) and YBa2Cu3O6+δ (YBS), which were synthesized using the sliding zone method. The oxygen composition in the samples was determined via iodometric titration. The typical sample size was approximately 5 × 4 mm². Measurements were conducted in pure helium cryostats equipped with pulse magnets, achieving a maximum magnetic induction value of B_max =",
        "ori-fast-z-score": -1.1832159566199232,
        "water-fast-z-score": 7.268326590665242,
        "rewrite-fast-z-score": 1.2004900959975617
    },
    {
        "original_text": "We present an analysis of the most massive cluster known to be part of the Great Attractor, based on new spectroscopic data obtained with VLT/VIMOS and archival Chandra X-ray Observatory observations. The main results are as follows:  We find that the galaxy distribution is well described by a single NFW profile out to at least 1 Mpc/h.  Using this mass model we derive a virial mass for the cluster of Mvir = 2.1 x 1015M⊙/h70 within R200 = 0.9 h-1 70 Mpc/h. This value agrees very well with previous estimates derived using different techniques. We also show that the velocity dispersion profile can be fitted with a simple β-model up to r ~ 0.7R200. Beyond this radius there seems to be evidence for a steepening of the profile which could indicate the presence of substructures or infalling groups along the line-of-sight.",
        "watermark_text": "We present an assessment of the most large cluster known to be part of the Great Attractor , using on new spectroscopic data acquired with VLT / VIMOS and archival Chandra X - ray Observatory surveys . The main results are as follows : We see that the galaxy distribution is well described by a single NFW profile out to at least 1 Mpc / h .Using this mass estimate we derive a virial mass for the cluster of Mvir = 2 . 1 x [UNK] / h70 within R200 = 0 . 9 h - 1 70 Mpc / h . This value agrees very well with previous calculated derived using separate techniques .We also find that the velocity dispersion model can be fit with a simple β - model up to r ~ 0 . 7R200 . Beyond this radius there seems to be data for a steepening of the profile which could indicate the formation of substructures or infalling bands along the line - of - view .",
        "rewrite_text": "We present an assessment of the largest known cluster within the Great Attractor, utilizing new spectroscopic data obtained through VLT/VIMOS and archival surveys from the Chandra X-ray Observatory. The key findings are as follows: The distribution of galaxies is accurately represented by a single Navarro-Frenk-White (NFW) profile extending out to at least 1 Mpc/h. Using this mass estimate, we calculate a virial mass for the cluster of Mvir = 2.1 x [UNK]/h70 within R200 = 0.9 h -1 70 Mpc/h. This value is in strong agreement with previously derived estimates using various methods. Additionally, our analysis shows that the velocity dispersion can be effectively modeled with a simple β-model up to approximately r ~ 0.7R200. Beyond this point, the data suggest a steepening of the profile, which may indicate the presence of substructures or infalling groups along the line of sight.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 5.680375574437544,
        "rewrite-fast-z-score": 0.5163977794943222
    },
    {
        "original_text": "The geometry of time is the study of how space-time evolves in time.  The axiom of choice states that for any collection of non-empty sets there exists at least one set which contains exactly one element from each set.   In this article we show that if the universe has an underlying quantum structure then it follows that the geometry of time can be described by using the axiom of choice to select a single point on every trajectory through spacetime.   We also discuss some possible implications of our results for neurobiology where the quantum Zeno effect may play a role in explaining certain aspects of brain function such as consciousness. The geometry of time is the mathematical description of how space-time evolutes over time  1  . It was first introduced into physics by Hermann Minkowski  2  who showed that the geometry of space-time could be represented by four numbers (x,y,z,t) called coordinates or co-ordinates. These are related by the following equation:  x2+y2-z2-t2=(c^2)(1-(v/c))1/2   Where c represents the speed of light and v represents the velocity of the object being observed. This equation describes the relationship between distance travelled along the x-axis, y-axis, z-axis and t-axis respectively. For example, if you were observing someone walking across your living room floor with their back towards you they would have a positive value for the x-axis but no values for the other three axes because they are not moving in those directions. If however you were watching them walk away from you they would have negative values for all three axes except the x-axis since they are still travelling forward in that direction. As another example consider two objects traveling side-by-side down a roadway. They will both travel at the same speed so their velocities will be equal. However, if one car travels north while the other travels south they will appear to move faster than each other even though they are traveling at the same speed.",
        "watermark_text": "The geometry of time is the study of how space - time evolves in time . The axiom of selection asserts that for any set of non - empty sets there exists at least one collection which contains exactly one element from each set .In this article we prove that if the universe has an underlying quantum model then it follows that the topology of time can be described by using the axiom of choice to select a single position on every trajectory through spacetime . We also discuss some possible possibilities of our findings for neurobiology where the molecular Zeno effect could play a role in understanding particular aspects of cerebral activity such as consciousness .The geometry of time is the geometric description of how space - time evolutes over time 1 . It was first developed into physics by Hermann Minkowski 2 who proved that the topology of space - time could be described by four numbers ( x , y , z , t ) called coordinates or co - ordinates .These are related by the following equation : x2 + y2 - z2 - t2 = ( c ^ 2 ) ( 1 - ( v / c ) ) 1 / 2 Where c represents the speed of light and v indicates the velocity of the instrument being detected . This equation relates the relationship between distance travelled along the x - axis , y - axis , z - axis and t - axis respectively .For example , if you were observing someone walking across your living room floor with their back towards you they would have a positive value for the x - axis but no values for the other three axes because they are not moving in those directions . If however you were watching them walk away from you they may have negative values for all three axes except the x - axis since they are still travelling forward in that direction .As another example consider two bodies going side - by - side down a street . They will both travel at the same speed so their velocities will be equal .However , if one vehicle goes north while the other travels south they will appear to move faster than each other even though they are traveling at the same speed .",
        "rewrite_text": "The geometry of time examines the evolution of spacetime over time. According to the axiom of selection, for any collection of non-empty sets, there exists at least one subset that includes exactly one element from each set. In this article, we demonstrate that if the universe operates under an underlying quantum model, then the topology of time can be characterized by employing the axiom of choice to identify a unique position along every trajectory through spacetime. We also explore the implications of our findings for neurobiology, particularly regarding the molecular Zeno effect and its potential role in elucidating certain aspects of cerebral activity, such as consciousness. The geometry of time, providing a geometric framework for understanding the evolution of spacetime, was first articulated in the context of physics by Hermann Minkowski. He established that the topology of spacetime could be represented by four coordinates (x, y, z, t). These coordinates are interrelated through the equation: x² + y² - z² - t² = c²(1 - (v/c))¹/², where c symbolizes the speed of light and v denotes the velocity of the observed instrument. This equation describes how distance traveled is connected across the x, y, z, and t axes. For instance, if you observe someone walking across your living room with their back to you, they would register a positive value along the x-axis, while having no values for the other axes as they are not moving in those directions. Conversely, if you watch them walk away from you, they might register negative values for all three axes except the x-axis, as they continue to advance forward in that direction. To illustrate further, consider two vehicles traveling side by side down a street at the same speed. Despite their equal velocities, if one turns north and the other south, they will appear to move faster relative to each other, even though both are maintaining the same speed.",
        "ori-fast-z-score": 0.6575959492214292,
        "water-fast-z-score": 5.816546635685188,
        "rewrite-fast-z-score": 1.7149858514250882
    },
    {
        "original_text": "We consider universal quantization for distributed estimation over noisy channels, where feedback is available at both ends and communication rates are constrained by an average power constraint. We propose a novel scheme that achieves the optimal rate-distortion tradeoff under this setting. The proposed scheme consists of two parts: (i) A quantizer design based on the Lloyd-Max algorithm; (ii) An encoding-decoding strategy using successive refinement coding to achieve the desired distortion level while satisfying the average power constraints. Our results show that the proposed scheme can be implemented efficiently even when the number of sensors grows large. In addition, we provide numerical examples to illustrate our theoretical findings. Finally, we discuss some possible extensions of our work. This article has been accepted for publication in IEEE Transactions on Information Theory. \nThe final authenticated version is available online at: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6627571",
        "watermark_text": "We consider universal quantization for distributed estimation over noisy channels , where feedback is provided at both ends and communication frequencies are constrained by an estimated power constraint . We suggest a new method that achieves the ideal rate - noise tradeoff under this setting .The proposed system contains of two parts : ( i ) A quantizer design using on the Lloyd - Max algorithm ; ( ii ) An encoding - decoding strategy using successive refinement sequences to achieve the desired distortion height while fulfilling the average power limitations . Our results show that the suggested system can be executed easily even when the number of measurements grows large .In addition , we provide numerical examples to illustrate our theoretical results . Finally , we explain some possible extensions of our work .This section has been accepted for published in IEEE Transactions on Information Theory . The final authenticated copy is accessible online at : www : / / ieeexplore . ieee . org / xpls / abs _ all . jsp ? arnumber = 6627571",
        "rewrite_text": "We explore universal quantization for distributed estimation over noisy channels with feedback available at both ends, while adhering to constraints on communication frequencies caused by power limitations. Our proposed method achieves an optimal balance between rate and noise in this context. The system consists of two main components: (i) a quantizer designed using the Lloyd-Max algorithm, and (ii) an encoding-decoding strategy that employs successive refinement sequences to meet the desired distortion level while satisfying average power constraints. Our findings demonstrate that this system can be effectively implemented even as the number of measurements increases significantly. We also present numerical examples to support our theoretical results and discuss potential extensions of our work. This section has been accepted for publication in IEEE Transactions on Information Theory, and the final authenticated version is available online at: www://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6627571.",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 5.5778737935111105,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "In this work, we propose a novel cross-layer scheme to improve the performance of distributed wireless ad hoc networks (DWAHNs). The proposed scheme is based on an adaptive routing protocol and a dynamic channel allocation algorithm. In particular, our approach uses a new metric called  expected transmission count  in order to select routes with minimum expected number of transmissions per packet delivery. Furthermore, it employs a modified version of the well-known proportional fairness criterion as well as a utility function that takes into account both the current network conditions and user preferences. Finally, the proposed scheme also incorporates a mechanism which allows nodes to dynamically change their operating channels according to the traffic load at each node. Extensive simulation experiments are conducted using NS-2 simulator to evaluate the effectiveness of the proposed scheme under different scenarios. Results show that the proposed scheme outperforms existing approaches by achieving higher throughput while maintaining low end-to-end delay and packet loss rate.",
        "watermark_text": "In this project , we propose a new cross - layer scheme to upgrade the performance of distributed wireless ad hoc networks ( DWAHNs ) . The proposed system is based on an dynamic routing mechanism and a dynamic channel allocation algorithm .In particular , our approach utilizes a new metric termed expected broadcast count in order to select routes with minimum expected number of transmissions per packet transmission . Furthermore , it employs a altered version of the better - famous proportional fairness factor as well as a utility function that takes into consideration both the present connection conditions and customer choices .Finally , the suggested system also contains a process which allows nodes to dynamically change their operating networks according to the traffic burden at each node . Extensive model studies are performed using NS - 2 simulator to analyze the performance of the suggested system under various circumstances .Results show that the suggested system outperforms previous techniques by achieving larger throughput while maintaining low end - to - end delay and packet loss rate .",
        "rewrite_text": "In this project, we present a novel cross-layer approach aimed at enhancing the performance of distributed wireless ad hoc networks (DWAHNs). Our proposed system incorporates a dynamic routing mechanism alongside a dynamic channel allocation algorithm. Specifically, we introduce a new metric called expected broadcast count, which helps in selecting routes that minimize the expected number of transmissions required per packet. Additionally, our strategy features an adapted version of the well-known proportional fairness factor and a utility function that considers both current connection conditions and user preferences. Moreover, the system includes a mechanism that enables nodes to dynamically adjust their operating networks based on the traffic load at each node. We conduct comprehensive model studies using the NS-2 simulator to evaluate the performance of our system under various scenarios. The results demonstrate that our system significantly surpasses previous methods, achieving higher throughput while effectively maintaining low end-to-end delay and reduced packet loss rates.",
        "ori-fast-z-score": -0.9138115486202573,
        "water-fast-z-score": 6.869037302955033,
        "rewrite-fast-z-score": 1.6329931618554523
    },
    {
        "original_text": "Epitaxial thin films of the multiferroic compound Bi2FeCrO 6 were grown on (001)-oriented SrTiO3 substrates by pulsed laser deposition at 750 °C in an oxygen partial pressure of 0.1 mbar and annealed for 30 min under vacuum conditions to induce ferroelectricity.  The structural properties of these epitaxial films are investigated using X-ray diffraction, transmission electron microscopy, scanning probe techniques as well as Raman spectroscopy. It is found that the films grow coherently strained along  001  direction with a tetragonal structure. A strong in-plane anisotropy between the out-of-plane lattice parameters c and a was observed which can be explained by different ionic radii of Fe 3+ , Cr 3+ and Ti 4+ . In addition, it could be shown that the films exhibit a rhombohedral-like distortion due to the presence of antiphase boundaries.",
        "watermark_text": "Epitaxial thin sheets of the multiferroic compound Bi2FeCrO 6 were cultivated on ( 001 ) - directed SrTiO3 substrates by pulsed infrared deposition at 750 °C in an oxygen partial pressure of 0 . 1 mbar and annealed for 30 min under vacuum environments to create ferroelectricity . The structural properties of these epitaxial films are examined utilizing X - ray diffraction , transmission electron microscopy , scanning probe methods as well as Raman spectroscopy .It is found that the films increase coherently strained along 001 direction with a tetragonal shape . A strong in - plane anisotropy between the out - of - plane lattice parameters c and a was seen which can be described by various ionic radii of Fe 3 + , Cr 3 + and Ti 4 + .In addition , it could be shown that the films show a rhombohedral - like degradation due to the presence of antiphase borders .",
        "rewrite_text": "Epitaxial thin films of the multiferroic compound Bi2FeCrO6 were grown on (001)-oriented SrTiO3 substrates using pulsed infrared deposition at 750 °C with an oxygen partial pressure of 0.1 mbar. These films were subsequently annealed for 30 minutes in a vacuum to induce ferroelectricity. The structural characteristics of the epitaxial layers were analyzed through X-ray diffraction, transmission electron microscopy, scanning probe techniques, and Raman spectroscopy. The results reveal that the films exhibit coherent strain along the 001 direction, adopting a tetragonal structure. A significant in-plane anisotropy was observed between the out-of-plane lattice parameters c and a, which can be attributed to the differing ionic radii of Fe3+, Cr3+, and Ti4+. Additionally, the films demonstrated a rhombohedral-like distortion caused by the presence of antiphase boundaries.",
        "ori-fast-z-score": -1.9051586888313607,
        "water-fast-z-score": 2.9938207967349952,
        "rewrite-fast-z-score": -2.721655269759087
    },
    {
        "original_text": "We study the geometry of harmonic maps into spheres with values in vector bundles over Riemann surfaces. We prove that if such a map is not constant then it has no critical points outside its singular set (Theorem 1). This implies that any harmonic section of an oriented rank 2 bundle over a closed surface can be deformed to a smooth one without changing its homotopy class (Corollary 3).\nIn particular we show how this result leads to new proofs for some results about instantons on 4-dimensional manifolds due to Donaldson  D1  ,  D2  . In fact our proof gives more information than those given by Donaldson s arguments since it allows us to control the behavior of the harmonic section near its singularities. Finally we give examples showing that these results are sharp. The main theorem of this article states that every non-constant harmonic section of an oriented 2-plane bundle over a closed surface S can be deformed to another harmonic section which is smooth everywhere except at isolated points where it has only simple poles.",
        "watermark_text": "We research the topology of harmonic maps into spheres with values in vector bundles over Riemann spheres . We prove that if such a mapping is not zero then it has no important points outside its singular set ( Theorem 1 ) .This implies that any harmonic section of an oriented rank 2 bundle over a closed surface can be deformed to a smooth one without altering its homotopy class ( Corollary 3 ) . In particular we tell how this consequence leads to novel proofs for some results about instantons on 4 - dimensional manifolds due to Donaldson D1 , D2 .In reality our proof provides more information than those given by Donaldson s statements since it allows us to affect the response of the chord section near its singularities . Finally we give examples demonstrating that these results are sharp .The main theorem of this page states that every non - constant harmonic section of an oriented 2 - plane bundle over a closed surface S can be deformed to another harmonic section which is continuous everywhere except at isolated points where it has only simple poles .",
        "rewrite_text": "We investigate the topology of harmonic maps that take values in vector bundles over Riemann spheres and map into spheres. We demonstrate that if such a mapping is non-zero, then it lacks critical points outside its singular set (Theorem 1). This leads to the conclusion that any harmonic section of an oriented rank 2 bundle over a closed surface can be smoothly deformed while preserving its homotopy class (Corollary 3). Notably, this result provides new proofs for certain assertions regarding instantons on four-dimensional manifolds, as shown in the work of Donaldson (D1, D2). Our proof offers more detailed insights than Donaldson's original findings, as it enables us to manipulate the behavior of the chord section near its singularities. Lastly, we present examples that illustrate the sharpness of these results. The primary theorem asserts that every non-constant harmonic section of an oriented 2-plane bundle over a closed surface S can be transformed into another harmonic section that remains continuous everywhere except at isolated points where it features only simple poles.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.103161130233569,
        "rewrite-fast-z-score": 1.709408646894569
    },
    {
        "original_text": "We present new observations at 1.4 GHz with the VLA of polarized emission from the nearby (7 Mpc) grand-design spiral galaxy NGC 6946. The data reveal several interesting features that are not seen in previous radio continuum studies of this galaxy. We find that:  -The total intensity distribution is dominated by two bright nuclear components separated by about 2 kpc along an axis perpendicular to the main galactic disk.  -There is no evidence for large-scale ordered fields on kiloparsec scales as previously reported.   -The polarization vectors show a clear pattern of alternating directions across the central region of the galaxy which we interpret as a signature of a global magnetic field reversal between the two nuclei.  -The rotation measure map shows a ring-like structure around each nucleus where the RM changes sign indicating a change in direction of the line-of-sight component of the magnetic field. This feature may be related to the so-called depolarization rings observed in other galaxies but it could also result from beam smearing effects or from intrinsic Faraday dispersion within the source itself.  -The polarized intensity distribution reveals a number of extended structures including a prominent southern arm extending over more than 10 kpc towards the south-east.",
        "watermark_text": "We present new images at 1 . 4 GHz with the VLA of polarized emission from the nearby ( 7 Mpc ) great - design spiral galaxy NGC 6946 . The data reveal numerous interesting features that are not seen in earlier radio continuum experiments of this galaxy .We see that : - The total magnitude distribution is dominated by two faint nuclear elements divided by about 2 kpc along an axis adjacent to the main galactic disk . - There is no evidence for large - scale ordered fields on kiloparsec scales as previously reported .- The polarization vectors display a clear sequence of alternating directions across the central region of the galaxy which we perceive as a signature of a global magnetic force reversal between the two nuclei . - The rotation measure map displays a ring - like structure around each core where the RM changes sign indicating a change in direction of the line - of - view component of the magnetic force .This characteristic could be connected to the so - called depolarization belts detected in other stars but it could also occur from light smearing effects or from intrinsic Faraday dispersion within the source itself . - The polarized intensity distribution reveals a number of extended features including a major southern arm reaching over more than 10 kpc towards the south - west .",
        "rewrite_text": "We present new 1.4 GHz images of polarized emission from the nearby grand-design spiral galaxy NGC 6946, located just 7 Mpc away, using the VLA. Our data uncover several intriguing features that were not observed in previous radio continuum studies of this galaxy. We find that: - The total magnitude distribution is primarily influenced by two faint nuclear components separated by approximately 2 kpc along an axis close to the main galactic disk. - Contrary to earlier reports, there is no evidence of large-scale ordered magnetic fields on kiloparsec scales. - The polarization vectors exhibit a distinct pattern of alternating directions across the galaxy's central region, which we interpret as an indication of a global magnetic field reversal between the two nuclei. - The rotation measure map reveals a ring-like structure around each core where the RM changes sign, suggesting a reversal in the direction of the line-of-sight component of the magnetic field. This feature may be related to the so-called depolarization belts observed in other stars, or it could arise from light smearing effects or intrinsic Faraday dispersion within the source itself. - Additionally, the distribution of polarized intensity shows several extended features, including a prominent southern arm that extends over 10 kpc toward the southwest.",
        "ori-fast-z-score": 0.6,
        "water-fast-z-score": 6.8,
        "rewrite-fast-z-score": 0.9805806756909202
    },
    {
        "original_text": "We report on the valley dependent optoelectronic properties in monolayer WSe2, which is an inversion symmetry breaking semiconductor with strong spin-orbit coupling and large exciton binding energy. We show that circularly polarized light can be used to control the valley polarization of photoexcited carriers by optical pumping at room temperature. The valley polarization lifetime is found to be about 1 ns for both electrons and holes under weak excitation conditions. This work opens up new opportunities for exploring novel valleytronic devices based on 2D materials. \n \n Valleytronics has been proposed as one promising approach towards realizing spin-based electronics beyond conventional silicon technology1-5 . Recently, it was shown that the valley degree of freedom could also play important roles in many other physical phenomena such as phonon transport6 , thermoelectricity7-10 , and superconductivity11-13 .\n \n \n Monolayer transition metal dichalcogenides (TMDCs) are emerging two-dimensional semiconductors14-17 with broken inversion symmetry18-20 due to their unique layered structure21-23 . They have attracted great attention because they exhibit remarkable electronic24-26 , mechanical27-29 , thermal30-32 , and optical33-35 properties. Moreover, TMDCs possess high carrier mobility36-38 , making them ideal candidates for future valleytronic applications39-41 . \n \n Here we demonstrate valley-dependent optoelectronic properties of monolayer WSe2 using time-resolved photoluminescence spectroscopy42-45 . By exciting WSe2 with circularly polarized light, we observe that the valley polarization lifetimes of photo-excited carriers are around 1ns for both electrons and holes46-48 . Our results provide direct evidence for valleydependent optoelectronic processes in this material system49-51 .",
        "watermark_text": "We report on the valley dependent optoelectronic properties in monolayer WSe2 , which is an inversion symmetry breaking semiconductor with powerful spin - orbit bonding and large exciton bound energy . We see that circularly polarized light can be used to affect the valley polarization of photoexcited carriers by optical pumping at room temperature .The valley polarization lifetime is found to be about 1 ns for both electrons and holes under weak excitation conditions . This research provides up new opportunities for studying novel valleytronic technologies based on 2D materials .Valleytronics has been proposed as one promising alternative towards developing spin - based computing beyond traditional silicon technology1 - 5 . Recently , it was shown that the valley degree of liberty might actually hold important roles in many other physical phenomena such as phonon transport6 , thermoelectricity7 - 10 , and superconductivity11 - 13 .Monolayer transition metal dichalcogenides ( TMDCs ) are emerging two - dimensional semiconductors14 - 17 with broken inversion symmetry18 - 20 due to their different layered structure21 - 23 . They have garnered great popularity because they show remarkable electronic24 - 26 , mechanical27 - 29 , thermal30 - 32 , and optical33 - 35 qualities .Moreover , TMDCs contain high carrier mobility36 - 38 , making them ideal candidates for future valleytronic applications39 - 41 . Here we prove valley - dependent optoelectronic properties of monolayer WSe2 utilizing period - resolved photoluminescence spectroscopy42 - 45 .By exciting WSe2 with circularly polarized light , we determine that the valley polarization lifetimes of photo - excited carriers are around 1ns for both electrons and holes46 - 48 . Our results represent direct data for valleydependent optoelectronic processes in this solid system49 - 51 .",
        "rewrite_text": "We present our findings on the valley-dependent optoelectronic properties of monolayer WSe2, a semiconductor characterized by broken inversion symmetry, strong spin-orbit coupling, and a significant exciton binding energy. Our results demonstrate that circularly polarized light can effectively influence the valley polarization of photoexcited carriers through optical pumping at room temperature. We observe that the valley polarization lifetime is approximately 1 ns for both electrons and holes under weak excitation conditions. This research paves the way for exploring innovative valleytronic technologies based on two-dimensional materials. Valleytronics has emerged as a promising alternative for advancing spin-based computing beyond conventional silicon technologies. Furthermore, recent studies have indicated that the valley degree of freedom may play crucial roles in various other physical phenomena, including phonon transport, thermoelectricity, and superconductivity. Monolayer transition metal dichalcogenides (TMDCs) are emerging two-dimensional semiconductors with broken inversion symmetry, attributed to their distinct layered structure. These materials have gained significant attention due to their exceptional electronic, mechanical, thermal, and optical properties, along with high carrier mobility, which makes them ideal candidates for future valleytronic applications. In this work, we demonstrate the valley-dependent optoelectronic properties of monolayer WSe2 using time-resolved photoluminescence spectroscopy. By exciting WSe2 with circularly polarized light, we find that the valley polarization lifetimes of photoexcited carriers are around 1 ns for both electrons and holes. Our findings provide direct evidence of valley-dependent optoelectronic processes in this solid-state system.",
        "ori-fast-z-score": 0.4879500364742666,
        "water-fast-z-score": 7.060180864974626,
        "rewrite-fast-z-score": 1.8107149208503706
    },
    {
        "original_text": "The dynamics-based approach is an emerging method for studying terrestrial exoplanets, which are planets with masses similar to that of Earth orbiting other stars in the solar system.  The main goal of this research is to study how these planets form and evolve over time.   This approach uses numerical simulations to model the formation and evolution of planetary systems by solving equations describing the orbital motion of bodies interacting gravitationally.   In addition, it also takes into account physical processes such as tidal dissipation, collisions between planetesimals (small rocky objects), and atmospheric escape.    By using this approach we can better understand how our own planet formed billions of years ago and what conditions were necessary for life on Earth to develop. Keywords: Planetary Science; Astrobiology; Tidal Dissipation; Collisions Between Planetesimals; Atmospheric Escape. Introduction:  The dynamics-based approach is an emergent method for studying terrestrial extrasolar planets, or planets with masses similar to Earth s orbiting other stars within the Solar System.  These types of planets have been discovered recently through space missions like Kepler and K2.  The main goal of the dynamics-based approach is to study how these worlds form and evolve over time.  It does so by modeling the formation and evolution of the entire planetary system numerically via solving equations describing the orbital motions of bodies interacting gravitationally.  Additionally, it incorporates physical processes including tidal dissipation, collisions among planetesimals (smaller rocky objects) and atmospheric escape.  By applying this approach, scientists hope to gain insight about how our own planet formed billion(s) of years ago and what environmental factors may be required for life to exist there.",
        "watermark_text": "The dynamics - based approach is an evolving technique for studying terrestrial exoplanets , which are stars with masses similar to that of Earth orbiting other stars in the solar system . The main goal of this research is to study how these planets form and evolve over time .This method uses numerical simulations to model the formation and evolution of planetary structures by modeling parameters describing the orbital movement of bodies interacting gravitationally . In addition , it also took into consideration physical processes such as tidal dissipation , collisions between planetesimals ( small rocky objects ) , and atmospheric release .By using this methodology we can help realize how our own planet developed billions of years previously and what circumstances were required for people on Earth to develop . Keywords : Planetary Science ; Astrobiology ; Tidal Dissipation ; Collisions Between Planetesimals ; Atmospheric Escape .Introduction : The dynamics - based alternative is an emergent technique for studying terrestrial extrasolar stars , or worlds with masses similar to Earth s orbiting other stars within the Solar System . These kind of stars have been detected lately through space missions like Kepler and K2 .The main goal of the dynamics - based theory is to study how these worlds create and evolve over time . It does so by modeling the formation and evolution of the entire celestial system numerically via solving equations explaining the orbital motions of bodies interacting gravitationally .Additionally , it combines physical processes including tidal dissipation , collisions among planetesimals ( smaller rocky objects ) and atmospheric escape . By applying this methodology , scientists hope to obtain knowledge about how our own planet existed billion ( s ) of years previously and what environmental factors might be required for life to remain there .",
        "rewrite_text": "The dynamics-based approach is an innovative technique for investigating terrestrial exoplanets, which are planets with masses comparable to Earth that orbit stars beyond our solar system. The primary objective of this research is to explore the formation and evolution of these planets over time. This method employs numerical simulations to represent the development of planetary systems by analyzing parameters related to the gravitational interactions of orbiting bodies. Additionally, it considers various physical processes, such as tidal dissipation, collisions between planetesimals (small rocky bodies), and atmospheric escape. Utilizing this approach, researchers aim to gain insights into the development of our own planet billions of years ago and the conditions necessary for the emergence of life on Earth. \n\nKeywords: Planetary Science; Astrobiology; Tidal Dissipation; Collisions Between Planetesimals; Atmospheric Escape. \n\nIntroduction: The dynamics-based method is an emerging technique for the study of terrestrial extrasolar planets, or worlds with masses akin to Earth's, that orbit stars outside our solar system. Recently, these types of planets have been identified through space missions such as Kepler and K2. The main aim of the dynamics-based framework is to investigate how these worlds form and change over time. This is achieved by numerically modeling the formation and evolution of celestial systems through equations that describe the orbital motions of gravitationally interacting bodies. Furthermore, it incorporates physical processes such as tidal dissipation, collisions among planetesimals (smaller rocky objects), and atmospheric escape. By employing this methodology, scientists hope to uncover how our planet came to be billions of years ago and what environmental conditions could be necessary for life to thrive.",
        "ori-fast-z-score": 1.6859773678906163,
        "water-fast-z-score": 8.419756985347481,
        "rewrite-fast-z-score": 0.6260990336999411
    },
    {
        "original_text": "We study the floating phase in the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). We find that there is no floating phase for J1 = J2, but it appears when J1 > J2 and disappears at some critical value of J1/J2. The transition between the ordered state and the floating phase belongs to the universality class of the three-state Potts model with first-order transition. In addition we show that the ground states are degenerate on the square lattice if J1 = J2 or J1 < J2. This result suggests that the ground states may be non-degenerate even though they have not been found yet. \n \n Introduction \n \n It has been known since the work by Wannier  1  that the ground states of the spin-1/2 Heisenberg antiferromagnet on an infinite square lattice are infinitely degenerate. However, this fact does not necessarily mean that all possible configurations can appear as ground states  2  . For example, the ground states of the one-dimensional chain are unique although its energy spectrum is continuous  3  , while those of the two-dimensional triangular-lattice Heisenberg antiferromagnet are doubly degenerate  4  . \n \n Recently, several authors studied the ground states of the two-dimensional anisotropic nearest neighbor Ising model (AN-NNI)  5 - 7  . They showed numerically that the ground states are infinitely degenerate on the square lattices if J 1 = J 2 or J 1 < J 2  7   . On the other hand, the ground states were shown to be unique on the honeycomb lattice  8  . These results suggest that the ground states might be nondegenerate even though their exact forms remain unknown so far. \n \n In this Letter, we investigate the ground states of the ANNNI model using Monte Carlo simulations. First, we confirm that the ground states are indeed infinitely degenerate on the squarelattice ANNNI models. Then, we examine whether these ground states are unique or not. Finally, we discuss how the ground states change depending on the values of J 1 / J 2 .\n \n Ground States of the Square-Lattice",
        "watermark_text": "We explore the floating mode in the two - dimensional anisotropic closest - neighbor Ising model ( ANNNI ) . We see that there is no floating mode for J1 = J2 , but it appears when J1 > J2 and vanished at some significant value of J1 / J2 .The shift between the ordered state and the floating stage belongs to the universality category of the three - state Potts model with first - order transition . In addition we prove that the ground states are degenerate on the square lattice if J1 = J2 or J1 < J2 .This result suggests that the ground states may be non - degenerate even though they have not been found yet . Introduction It has been known since the work by Wannier 1 that the ground states of the spin - 1 / 2 Heisenberg antiferromagnet on an infinite square lattice are infinitely degenerate .However , this fact does not necessarily mean that all possible configurations can emerge as ground states 2 . For instance , the ground states of the one - dimensional network are distinct although its energy spectrum is continuous 3 , while those of the two - dimensional triangular - lattice Heisenberg antiferromagnet are doubly degenerate 4 .Recently , various scientists examined the ground states of the two - dimensional anisotropic closest neighbor Ising model ( AN - NNI ) 5 - 7 . They showed numerically that the ground states are infinitely degenerate on the square lattices if J 1 = J 2 or J 1 < J 2 7 .On the other hand , the ground states were shown to be unique on the honeycomb lattice 8 . These conclusions show that the ground groups may be nondegenerate even though their exact forms remain uncertain so far .In this Letter , we investigate the ground states of the ANNNI theory using Monte Carlo simulations . First , we prove that the ground states are indeed infinitely degenerate on the squarelattice ANNNI models .Then , we investigate whether these ground fields are distinct or not . Finally , we explain how the ground states change based on the values of J 1 / J 2 .Ground States of the Square-Lattice",
        "rewrite_text": "We investigate the floating mode in the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). Our findings indicate that no floating mode exists when J1 equals J2. However, a floating mode emerges when J1 exceeds J2 and disappears at a notable ratio of J1/J2. The transition between the ordered state and the floating regime falls within the universality class of the three-state Potts model, characterized by a first-order transition. Furthermore, we demonstrate that the ground states on a square lattice are degenerate when J1 equals J2 or when J1 is less than J2. This suggests the possibility of non-degenerate ground states, even though the specific configurations have yet to be identified. \n\nIt has long been established, since Wannier's work, that the ground states of the spin-1/2 Heisenberg antiferromagnet on an infinite square lattice are infinitely degenerate. However, this does not imply that all configurations can serve as ground states. For example, the ground states of a one-dimensional system are unique despite having a continuous energy spectrum, whereas those of the two-dimensional triangular-lattice Heisenberg antiferromagnet exhibit double degeneracy. Recently, various researchers have explored the ground states of the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). Their numerical analysis showed that these ground states are infinitely degenerate on square lattices when J1 equals J2 or when J1 is less than J2. Conversely, on the honeycomb lattice, the ground states were found to be unique. These findings suggest that, while the precise configurations of the ground states remain uncertain, they could potentially be non-degenerate. \n\nIn this Letter, we utilize Monte Carlo simulations to further examine the ground states of the ANNNI model. We first confirm the infinite degeneracy of the ground states in the square-lattice ANNNI models. Next, we assess whether these ground states are distinct or not. Finally, we discuss how the ground states evolve with varying values of J1/J2.",
        "ori-fast-z-score": -3.07821536544563,
        "water-fast-z-score": 2.434508013602067,
        "rewrite-fast-z-score": -3.064523510731495
    },
    {
        "original_text": "We present optical variability measurements for infrared power law-selected galaxies and X-ray sources in the Chandra Deep Field South (CDFS). We use data obtained with the Hubble Space Telescope s Advanced Camera for Surveys to measure photometric redshifts, rest-frame absolute magnitudes, stellar masses, star formation rates, and specific star-formation rates for these objects over an eight-year baseline. The sample consists of 16,000 galaxies at 0 < z < 5 selected by their mid-infrared colors using Spitzer/IRAC observations as well as 1,500 X-ray point sources detected in deep Chandra observations. We find that both galaxy samples show significant levels of intrinsic variation on timescales ranging from days to years. For example, we detect more than 50% of our IRAC-selected galaxies at 3.6 microns and 80% at 4.5 microns showing >0.1 mag variations between epochs separated by one year or less. These results are consistent with previous studies which have found similar levels of variability among optically-selected quasars. However, we also find evidence suggesting that this level of variability is not driven solely by AGN activity but may be associated with other physical processes such as mergers and/or interactions within the host galaxy itself.",
        "watermark_text": "We present optical variability observations for laser power law - selected galaxies and X - ray sources in the Chandra Deep Field South ( CDFS ) . We use data acquired with the Hubble Space Telescope s Advanced Camera for Surveys to measure photometric redshifts , rest - frame absolute magnitudes , stellar masses , sun formation rates , and particular galaxy - formation rates for these objects over an eight - month baseline .The sample consists of 16 , 000 galaxies at 0 < z < 5 selected by their mid - infrared colors using Spitzer / IRAC measurements as also as 1 , 500 X - ray point sources detected in deep Chandra measurements . We see that both star samples show considerable rates of intrinsic variation on timescales ranging from hours to decades .For instance , we find more than 50 % of our IRAC - selected galaxies at 3 . 6 microns and 80 % at 4 . 5 microns showing > 0 . 1 mag variations between epochs separated by one decade or less . These conclusions are compatible with previous research which have discovered similar rates of variability among optically - selected quasars .However , we also find proof suggesting that this level of variability is not driven solely by AGN activity but might be involved with other structural processes such as mergers and / or relationships within the host universe itself .",
        "rewrite_text": "We provide observations of optical variability for galaxies and X-ray sources selected based on their laser power law properties in the Chandra Deep Field South (CDFS). Using data from the Hubble Space Telescope's Advanced Camera for Surveys, we have measured photometric redshifts, rest-frame absolute magnitudes, stellar masses, star formation rates, and specific galaxy-formation rates for these objects over an eight-month period. The sample includes 16,000 galaxies with redshifts ranging from 0 to 5, selected by their mid-infrared colors through Spitzer/IRAC measurements, along with 1,500 X-ray point sources identified in deep Chandra observations. Our findings reveal significant intrinsic variability in both samples over timescales from hours to decades. For example, we observe that over 50% of the IRAC-selected galaxies at 3.6 microns and 80% at 4.5 microns exhibit variations greater than 0.1 magnitudes across epochs spaced by one decade or less. These results align with previous studies that have noted similar variability rates among optically-selected quasars. However, we also provide evidence suggesting that this degree of variability may not be solely attributed to AGN activity, but could also involve other structural processes, such as mergers and interactions within the host universe.",
        "ori-fast-z-score": -1.2939932784412609,
        "water-fast-z-score": 5.682196434640312,
        "rewrite-fast-z-score": 0.32539568672798425
    },
    {
        "original_text": "We study the stability properties of longitudinal flows in straight and curved magnetic flux tubes using linearized ideal MHD equations. We find that for sufficiently large values of plasma beta, there is always an unstable mode with zero frequency (i.e., static) which grows exponentially fast at small wavenumbers. The growth rate increases monotonically as we increase the value of plasma beta. \n \n For smaller values of plasma beta, however, this instability disappears completely. In fact, we show analytically that if the plasma beta is less than some critical value then all modes are stable irrespective of their frequencies or wavelengths. This result agrees well with our numerical simulations. Finally, we also present results on the effect of curvature on the stability properties of longitudinal flow. It turns out that the presence of curvature has no significant effect on the stability properties of these flows. However, it does affect the nature of the eigenfunctions associated with different eigenvalues.",
        "watermark_text": "We research the stability properties of longitudinal streams in straight and curved magnetic flux tubes using linearized ideal MHD equations . We see that for enough large values of plasma beta , there is usually an weak mode with zero frequency ( i . e . , static ) which increases exponentially rapidly at small wavenumbers .The growth speed increases monotonically as we increase the value of plasma beta . For lower values of plasma beta , however , this instability disappears entirely .In fact , we prove analytically that if the plasma beta is less than some essential value then all modes are stable irrespective of their frequencies or wavelengths . This result agrees well with our numerical simulations .Finally , we also present results on the impact of curvature on the stability properties of longitudinal stream . It turns out that the presence of curvature has no considerable impact on the stability properties of these flows .However , it does affect the nature of the eigenfunctions associated with various eigenvalues .",
        "rewrite_text": "We investigate the stability characteristics of longitudinal streams in both straight and curved magnetic flux tubes by employing linearized ideal magnetohydrodynamics (MHD) equations. Our findings reveal that when the plasma beta is sufficiently large, there is typically a weak mode with zero frequency (i.e., static) that grows exponentially at small wavenumbers. The rate of growth increases steadily with higher values of plasma beta. Conversely, at lower plasma beta values, this instability completely vanishes. We demonstrate analytically that if plasma beta falls below a certain critical threshold, all modes remain stable regardless of their frequencies or wavelengths. This theoretical result aligns well with our numerical simulations. Additionally, we examine the effect of curvature on the stability properties of longitudinal streams, discovering that while curvature does not significantly affect their stability, it does influence the nature of the eigenfunctions linked to various eigenvalues.",
        "ori-fast-z-score": 1.0256451881367414,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": 3.3941932686877867
    },
    {
        "original_text": "We present the results of our second survey to find very low mass binaries using lucky imaging with the Palomar Observatory 1.2 m telescope and the PALM-3000 camera. We have discovered thirteen new systems, including ten brown dwarf companions (four previously unknown) in twelve nearby young open clusters ranging in age between 10 Myr and 300 Myr. The masses range from 0.03-0.10 M . These are among the lowest-mass objects ever found by direct imaging techniques. In addition we report on one system that is likely an unresolved binary consisting of two late-type stars. This work represents the largest sample of directly imaged brown dwarfs assembled so far. It will be used as input into population synthesis models aimed at understanding how these objects form and evolve over time. \n \n Keywords: Brown Dwarf, Open Cluster, Direct Imaging, Lucky Imaging, Nearby Stars",
        "watermark_text": "We present the results of our second survey to find very low weight binaries using lucky imaging with the Palomar Observatory 1 . 2 m camera and the PALM - 3000 sensor . We have discovered thirteen new systems , including ten brown giant companions ( four originally unknown ) in twelve nearby young open complexes ranging in age between 10 Myr and 300 Myr .The masses range from 0 . 03 - 0 . 10 M . These are among the smallest - weight objects ever found by direct observation techniques .In addition we publish on one system that is probably an unresolved binary composed of two late - class stars . This project represents the greatest sample of specifically imaged brown dwarfs assembled so far .It will be used as input into population analysis models targeted at studying how these objects formation and evolve over time . Keywords : Brown Dwarf , Open Cluster , Direct Imaging , Lucky Imaging , Nearby Stars",
        "rewrite_text": "We present the findings from our second survey aimed at identifying very low-mass binary systems, utilizing lucky imaging techniques with the Palomar Observatory's 1.2-meter camera and the PALM-3000 sensor. Our efforts have led to the discovery of thirteen new systems, including ten brown dwarf companions—four of which were previously unknown—located in twelve nearby young open clusters with ages ranging from 10 million to 300 million years. The masses of these objects vary between 0.03 and 0.10 times that of the Sun, making them among the lowest-mass entities detected through direct observation methods. Additionally, we report on a system that is likely an unresolved binary consisting of two late-type stars. This project constitutes the largest collection of imaged brown dwarfs to date and will serve as valuable input for population analysis models focused on understanding the formation and evolution of these objects over time. Keywords: Brown Dwarf, Open Cluster, Direct Imaging, Lucky Imaging, Nearby Stars.",
        "ori-fast-z-score": -0.8551861104941365,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": -0.3375263702778072
    },
    {
        "original_text": "We present the results of our numerical simulations of accretion disk annuli in which radiation pressure is comparable to gas pressure, but not dominant.  We find that for such disks there are two distinct regimes depending on whether or not the luminosity is dominated by advection (i.e., Ladv/Lvisc ≫ 1). In one regime we find that the temperature profile has a power-law form T ∝ r^{-(3/2)}, while in another it follows a more complicated dependence on radius. The latter case occurs when the luminosity is dominated either by viscous dissipation or by advection. For both cases, however, the radial velocity profiles have similar shapes. Finally, we show how these results can be used to explain observed properties of X-ray binaries. Subject headings: Black holes -accretion disks -X-ray binaries: general -Accretion, accretion disks",
        "watermark_text": "We present the results of our numerical simulations of accretion cone annuli in which radiation volume is analogous to liquid temperature , but not dominant . We see that for such disks there are two different regimes depending on whether or not the luminosity is dominated by advection ( i . e . , Ladv / Lvisc [UNK] 1 ) .In one regime we find that the temperature profile has a power - law form T [UNK] r ^ { - ( 3 / 2 ) } , while in another it takes a more complicated dependence on radius . The latter example happens when the luminosity is dominated either by viscous dissipation or by advection .For both cases , however , the radial speed profiles have parallel patterns . Finally , we show how these results can be used to explain observed properties of X - ray binaries .Subject headings : Black holes - accretion disks - X - ray binaries : general - Accretion , accretion disks",
        "rewrite_text": "We present the findings from our numerical simulations of accretion cone annuli, where the radiation volume corresponds to liquid temperature but is not the dominant factor. Our study reveals two distinct regimes based on whether the luminosity is primarily driven by advection (i.e., Ladv / Lvisc < 1). In one regime, the temperature profile exhibits a power-law form, T ∝ r^{-(3/2)}, while in the other regime, it shows a more complex relationship with radius. This complexity arises when the luminosity is predominantly influenced by either viscous dissipation or advection. Nevertheless, in both scenarios, the profiles of radial velocity display similar patterns. Finally, we demonstrate how our findings can help elucidate the observed characteristics of X-ray binaries. Subject headings: Black holes - accretion disks - X-ray binaries: general - Accretion, accretion disks.",
        "ori-fast-z-score": -0.2773500981126146,
        "water-fast-z-score": 2.6605321596532185,
        "rewrite-fast-z-score": 0.2721655269759087
    },
    {
        "original_text": "The book Nurturing Breakthroughs by James Watson and Peter Winkler is about how to create breakthroughs in science, technology, engineering or mathematics (STEM). The authors argue that the best way for scientists to make new discoveries is not through individual genius but rather by working together as teams on problems they are passionate about.  They also claim that it helps if you have an idea of what your problem looks like before you start solving it because this will help guide your research efforts. This article describes some of their ideas and provides examples of how these concepts can be applied to physics research. In his book Nurturing Breakdowns, James Watson argues that the most successful people who solve complex problems do so by working with others instead of trying to work alone. He says that when we work individually our brains tend to focus only on one aspect of the problem at hand which may lead us down dead ends while working collaboratively allows us to see all aspects of the problem simultaneously.",
        "watermark_text": "The book Nurturing Breakthroughs by James Watson and Peter Winkler is about how to create breakthroughs in science , technology , engineering or math ( STEM ) . The authors argue that the best method for researchers to make fresh findings is not through individual genius but rather by acting together as teams on problems they are loving about .They also claim that it assists if you have an idea of what your problem looks like before you start investigating it because this will assist guide your study efforts . This page describes some of their ideas and provides examples of how these concepts can be applied to physics studies .In his book Nurturing Breakdowns , James Watson says that the most talented people who solution complex challenges do so by working with others rather of trying to work alone . He said that when we study collectively our mind tend to reflect only on one element of the issue at hand which would guide us down dead ends while working collaboratively allows us to see all aspects of the issue simultaneously .",
        "rewrite_text": "The book \"Nurturing Breakthroughs\" by James Watson and Peter Winkler explores how to achieve advancements in STEM fields—science, technology, engineering, and mathematics. The authors contend that successful research is best accomplished through teamwork, rather than relying solely on individual brilliance. They emphasize that having a clear understanding of the problem before beginning research can significantly enhance the direction and effectiveness of your efforts. This overview highlights some of their ideas and illustrates how these principles can be applied to the study of physics. In \"Nurturing Breakdowns,\" Watson argues that the most capable individuals tackle complex problems collaboratively, rather than in isolation. He notes that when we work together, our minds can address multiple facets of an issue at once, rather than fixating on a single aspect, which can lead to unproductive outcomes.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 5.888972745734182,
        "rewrite-fast-z-score": -0.13483997249264842
    },
    {
        "original_text": "We propose to probe the nature of dark energy by correlating the morphological properties of galaxies in the nearby universe (z < 0.1) as measured by the Sloan Digital Sky Survey (SDSS), with those at higher redshifts, z ~ 1, obtained from the Wilkinson Microwave Anisotropy Probe (WMAP). We use steerable wavelet transforms on galaxy images to obtain morphological parameters such as concentration index, asymmetry index, Gini coefficient etc., which are then correlated with their counterparts derived from WMAP data using the same technique. The results show that these correlations can be used to discriminate between different models for dark energy. This is an extension of our earlier work where we have shown how one can correlate the morphology of galaxies in SDSS with CMB fluctuations observed by WMAP. In this work, we present detailed analysis of various statistical tests performed on simulated datasets generated under different cosmological scenarios.",
        "watermark_text": "We suggest to probe the nature of dark energy by correlating the morphological properties of galaxies in the nearby universe ( z < 0 . 1 ) as measured by the Sloan Digital Sky Survey ( SDSS ) , with those at higher redshifts , z ~ 1 , obtained from the Wilkinson Microwave Anisotropy Probe ( WMAP ) . We use steerable wavelet transforms on star photographs to obtain morphological characteristics such as concentration index , asymmetry index , Gini coefficient etc . , which are then correlated with their counterparts obtained from WMAP information using the same technique .The results show that these correlations can be used to discriminate between various models for black energy . This is an extension of our earlier paper where we have shown how one can correlate the morphology of stars in SDSS with CMB fluctuations detected by WMAP .In this research , we present detailed analysis of several statistical tests administered on simulated datasets generated under various cosmological environments .",
        "rewrite_text": "We propose investigating the nature of dark energy by examining the morphological properties of galaxies in the nearby universe (z < 0.1) as recorded by the Sloan Digital Sky Survey (SDSS), and comparing them with those at higher redshifts (z ~ 1) derived from the Wilkinson Microwave Anisotropy Probe (WMAP). Utilizing steerable wavelet transforms on images of stars, we extract morphological features such as the concentration index, asymmetry index, and Gini coefficient. These features are then correlated with their counterparts obtained from WMAP data through the same methodology. Our findings indicate that these correlations can effectively differentiate among various dark energy models. This work expands on our previous study, which demonstrated how to correlate the morphology of stars in SDSS with cosmic microwave background fluctuations observed by WMAP. In this paper, we provide a comprehensive analysis of several statistical tests conducted on simulated datasets produced under different cosmological scenarios.",
        "ori-fast-z-score": 1.3363062095621219,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": 0.8962581595302719
    },
    {
        "original_text": "The objective was to evaluate the potential use of Monoksa dorsiplana as an alternative biological control agent against Pseudopachymeria sp. (Bruchidae). The parasitoids were obtained in laboratory and released on P.sp. eggs laid by females collected at different locations in Brazil, Argentina and Paraguay. Egg parasitism ranged between 0.5 and 88% depending on location. Parasitized eggs hatched after 7 days under controlled conditions. Males emerged first followed by females. Female longevity varied according to temperature ranging from 11 to 21 days at 25 °C; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C. Females oviposited for up to three weeks when fed with honey solution. This species is considered highly suitable for mass production because it has high reproductive capacity and short life cycle. It can be used successfully in integrated pest management programs aimed at reducing damage caused by this insect pest.",
        "watermark_text": "The goal was to examine the possibilities application of Monoksa dorsiplana as an alternative bio control drug against Pseudopachymeria sp . ( Bruchidae ) .The parasitoids were obtained in laboratory and captured on P . sp . eggs laid by females collected at different places in Brazil , Argentina and Paraguay .Egg parasitism ranged between 0 . 5 and 88 % depending on location . Parasitized nests hatched after 7 days under regulated conditions .Males appeared first followed by females . Female longevity varied based to heat ranging from 11 to 21 weeks at 25 °C ; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C .Females oviposited for up to three weeks when fed with honey solution . This species is regarded highly suited for mass production because it has large sexual capacity and low life cycle .It can be used successfully in effective pest management projects designed at decreasing damage suffered by this insect pest .",
        "rewrite_text": "The objective of this study was to evaluate the potential use of Monoksa dorsiplana as an alternative biocontrol agent against Pseudopachymeria sp. (Bruchidae). The parasitoids were sourced from laboratory cultures and collected from eggs laid by females captured in various locations across Brazil, Argentina, and Paraguay. The rate of egg parasitism varied significantly by location, ranging from 0.5% to 88%. Under controlled conditions, the hatching of parasitized eggs occurred after 7 days, with males emerging first, followed by females. Female lifespan was influenced by temperature, lasting between 11 to 21 weeks at 25 °C, 14 to 23 days at 20 °C, and 16 to 27 days at 15 °C. When provided with a honey solution, females were able to oviposit for up to three weeks. This species is considered highly suitable for mass production due to its large reproductive capacity and short life cycle, making it a viable option for pest management initiatives aimed at reducing the damage caused by this pest species.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": 0.35603449745815596
    },
    {
        "original_text": "We present an overview on supersymmetric grand unified theories (SUSY-GUT), their connection to neutrino masses via seesaw mechanisms as well as dark matter candidates in these models. We discuss how GUT scale physics can be probed at future colliders such as LHC or ILC. Finally we give some examples for specific realizations within SO(10) and E6 gauge groups. Supersymmetry is one of the most promising extensions beyond the Standard Model which addresses many open questions like the hierarchy problem between electroweak and Planck scales, unification of forces etc.. In addition it provides a natural candidate for cold dark matter -the lightest neutralino. The minimal supersymmetric standard model (MSSM) has been studied extensively over the last two decades but suffers from several shortcomings. One of them is that the MSSM does not provide any explanation why there are three generations of quarks and leptons with different quantum numbers. Grand Unified Theories (GUTs) address this issue by postulating that all known particles including those of the third generation belong to multiplets of larger symmetry group than SU(3)xSU(2)xU(1). This leads naturally to relations among coupling constants and fermion mass matrices. Another shortcoming of the MSSM is that it cannot explain small neutrino masses observed experimentally. However, if R-parity is broken then Majorana neutrinos may acquire tiny masses through see-saw mechanism. These new states could also contribute significantly to the relic density of dark matter.",
        "watermark_text": "We bring an overview on supersymmetric grand unified fields ( SUSY - GUT ) , their connection to neutrino masses via seesaw processes as well as dark matter candidates in these models . We discuss how GUT scale physics can be probed at possible colliders such as LHC or ILC .Finally we give some examples for specific realizations within SO ( 10 ) and E6 gauge bands . Supersymmetry is one of the most promising extensions beyond the Standard Model which answers many open questions like the hierarchy problem between electroweak and Planck scales , unification of forces etc . .In addition it gives a natural candidate for cold dark matter - the lightest neutralino . The minimal supersymmetric standard theory ( MSSM ) has been studied thoroughly over the last two decades but suffers from several shortcomings .One of them is that the MSSM does not offer any evidence why there are three generations of quarks and leptons with varying quantum numbers . Grand Unified Theories ( GUTs ) address this question by postulating that all known objects including those of the third generation belong to multiplets of bigger symmetry class than SU ( 3 ) xSU ( 2 ) xU ( 1 ) .This leads naturally to relations among coupling constants and fermion mass matrices . Another shortcoming of the MSSM is that it lacks explain little neutrino volumes discovered experimentally .However , if R - parity is shattered then Majorana neutrinos might acquire small masses through see - saw phenomenon . These new states could also contribute considerably to the relic volume of dark matter .",
        "rewrite_text": "We provide an overview of supersymmetric grand unified theories (SUSY-GUTs), exploring their connections to neutrino masses through seesaw mechanisms and identifying potential dark matter candidates within these frameworks. We examine how physics at the GUT scale can be investigated at future colliders such as the LHC or ILC. Additionally, we present examples of specific realizations within the SO(10) and E6 gauge groups. Supersymmetry is regarded as one of the most promising extensions of the Standard Model, addressing several unresolved issues, such as the hierarchy problem between the electroweak and Planck scales, as well as the unification of forces. Moreover, it naturally provides a candidate for cold dark matter: the lightest neutralino. The minimal supersymmetric standard model (MSSM) has been extensively studied over the past twenty years but has several limitations. One significant issue is that the MSSM does not explain why there are three generations of quarks and leptons with differing quantum numbers. Grand Unified Theories tackle this question by suggesting that all known particles, including those from the third generation, are part of multiplets belonging to a larger symmetry group than SU(3) × SU(2) × U(1). This framework leads to inherent relationships among coupling constants and fermion mass matrices. Another limitation of the MSSM is its inability to account for the small neutrino masses observed experimentally. However, if R-parity is violated, Majorana neutrinos may acquire small masses through the seesaw mechanism, and these new states could significantly contribute to the relic abundance of dark matter.",
        "ori-fast-z-score": 0.7258661863112977,
        "water-fast-z-score": 6.25846794771944,
        "rewrite-fast-z-score": -0.09950371902099892
    },
    {
        "original_text": "The effect of silver and indium on the magnetic properties, electrical resistivity (ER) and Hall coefficient (R H ) has been investigated in La 2/3 Ca 1/3 Mn O 3 . The results show that ER decreases with increasing temperature for all samples while R H increases with decreasing temperature. It is found that both silver and indium doping decrease T C , increase J c and enhance pinning force density F p . Silver doped sample shows higher values of J c than indium doped one at low temperatures but lower value at high temperatures.  These results are explained by considering different effects of silver and indium ions on the microstructure as well as their influence on oxygen vacancies concentration. This work was supported by the National Natural Science Foundation of China under Grant No. 50571040. We would like to thank Prof. Y. M. Wu for his help during this research. Abstract: In this study we have prepared two series of La 2/3 Ca 1/3 MnO 3 :Ag and La 2/3 Ca 1/3 MnO3 :In polycrystalline composite materials using solid state reaction method. X-ray powder diffraction patterns confirm single phase formation without any impurity peaks. The structural parameters such as lattice constant, unit cell volume and bond length were calculated from XRD data. The dc magnetization measurements reveal that Curie temperature (Tc), critical current density (Jc) and pinning force density (Fp) decrease with increasing amount of silver or indium content.",
        "watermark_text": "The impact of silver and indium on the magnetic properties , thermal resistivity ( ER ) and Hall coefficient ( R H ) has been investigated in La 2 / 3 Ca 1 / 3 Mn O 3 . The results show that ER decreases with expanding temperature for all specimens while R H increases with varying temperature .It is found that both silver and indium doping change T C , enhance J c and enhance pinning power concentration F p . Silver doped specimen shows better levels of J c than indium doped one at low temperatures but smaller value at high temperatures .These data are explained by examining different impacts of silver and indium ions on the microstructure as also as their influence on oxygen vacancies density . This research was supported by the National Natural Science Foundation of China under Grant No .50571040.We would like to thank Prof. Y. M. Wu for his help during this research.Abstract : In this study we have formed two sequence of La 2 / 3 Ca 1 / 3 MnO 3 : Ag and La 2 / 3 Ca 1 / 3 MnO3 : In polycrystalline composite materials utilizing solid state process method . X - ray powder diffraction patterns indicate single phase form without any impurity peaks .The structural values such as structure constant , unit cell size and bond length were calculated from XRD information . The dc magnetization calculations reveal that Curie temperature ( Tc ) , vital current density ( Jc ) and pinning power concentration ( Fp ) decline with varying amount of silver or indium content .",
        "rewrite_text": "The influence of silver and indium on the magnetic properties, thermal resistivity (ER), and Hall coefficient (RH) in La2/3Ca1/3MnO3 has been studied. The findings indicate that ER decreases with increasing temperature across all samples, while RH increases with temperature variations. It was observed that doping with both silver and indium alters the Curie temperature (TC), improves the critical current density (JC), and increases the pinning power concentration (Fp). Silver-doped samples exhibit higher JC values than indium-doped samples at low temperatures, but lower values at elevated temperatures. These results are attributed to the differing effects of silver and indium ions on the microstructure and their impact on the density of oxygen vacancies. This research was funded by the National Natural Science Foundation of China under Grant No. 50571040. We extend our gratitude to Prof. Y. M. Wu for his assistance during this study. \n\nAbstract: In this study, we have synthesized two series of polycrystalline composite materials, La2/3Ca1/3MnO3:Ag and La2/3Ca1/3MnO3:In, using a solid-state processing method. X-ray powder diffraction patterns indicate a single-phase formation without any impurity peaks. Structural parameters, including the structure constant, unit cell dimensions, and bond lengths, were derived from the XRD data. The direct current magnetization measurements show that the Curie temperature (TC), critical current density (JC), and pinning power concentration (Fp) decrease with increasing amounts of silver or indium content.",
        "ori-fast-z-score": 0.8081220356417685,
        "water-fast-z-score": 7.677159338596802,
        "rewrite-fast-z-score": 1.3199500146737049
    },
    {
        "original_text": "We study the properties of Bose-Einstein condensates (BECs) with an arbitrary number N of particles trapped by a harmonic oscillator and exposed to a disordered external potential. We show that, for any strength of disorder, there is always a finite fraction of atoms localized at each site of the lattice. The localization length decreases as the disorder increases but remains macroscopic even when the disorder becomes very large compared to the interatomic interaction energy. This result holds true both in one dimension and higher dimensions.  In particular we find that the critical disorder above which all states are localized scales like 1/N in 1D and 1/d in 2D and 3D where d is the spatial dimension. Our results provide a microscopic understanding of recent experiments on ultracold atomic gases in optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases of bosons or fermions confined in periodic potentials  1  . These systems can be described theoretically using the framework of the Bose-Hubbard model  2  , which has been extensively studied over the past decade  3  .\nIn this work we consider the case of a gas of interacting bosons in a disordered potential. Disorder leads to Anderson localization  4  : eigenstates become exponentially localized around their initial position if the disorder exceeds some threshold value  5  . It was recently shown experimentally  6  that such a system exhibits a transition between extended Bloch-like states and localized Wannier-Stark ladders  7, 8  . However, these experiments were performed only in the weak-disorder regime, i.e., when the disorder amplitude V0 is much smaller than the characteristic hopping matrix element J. Here we investigate how the presence of interactions affects the physics of strongly disordered systems.",
        "watermark_text": "We explore the properties of Bose - Einstein condensates ( BECs ) with an arbitrary number N of atoms trapped by a harmonic oscillator and exposed to a disordered external potential . We see that , for any strength of disorder , there is usually a finite fraction of atoms confined at each site of the crystal .The localization width decreases as the disorder advances but continues macroscopic even when the disorder becomes very huge compared to the interatomic interaction power . This result holds true both in one dimension and upper dimensions .In particular we find that the fundamental disorder above which all states are localized scales like 1 / N in 1D and 1 / d in 2D and 3D where d is the spatial dimension . Our results present a microscopic understanding of recent experiments on ultracold atomic gases in optical lattices .Introduction : - Recent research developments have enabled it able to create quantum degenerate gases of bosons or fermions localized in periodic potentials 1 . These systems can be described theoretically utilizing the framework of the Bose - Hubbard theory 2 , which has been heavily explored over the previous decade 3 .In this research we study the case of a gas of interacting bosons in a disordered potential . Disorder leads to Anderson localization 4 : eigenstates grow exponentially localized around their early position if the disorder approaches some threshold parameter 5 .It was recently shown experimentally 6 that such a system displays a shift between advanced Bloch - like states and localized Wannier - Stark ladders 7 , 8 . However , these experiments were performed only in the weak - disturbance regime , i . e . , when the disorder amplitude V0 is much smaller than the typical hopping matrix element J .Here we investigate how the presence of interactions affects the physics of highly disordered systems .",
        "rewrite_text": "We investigate the characteristics of Bose-Einstein condensates (BECs) composed of an arbitrary number N of atoms confined within a harmonic oscillator and influenced by a disordered external potential. Our findings indicate that, regardless of the disorder strength, a finite fraction of atoms typically remains localized at each site of the crystal. Although the localization width decreases with increasing disorder, it remains macroscopic even when the disorder becomes substantially larger than the interatomic interaction strength. This observation is consistent across one and higher dimensions. Notably, we discover that the critical disorder level above which all states become localized scales as 1/N in one dimension and as 1/d in two and three dimensions, where d represents the spatial dimension. Our results provide a microscopic framework that enhances our understanding of recent experiments involving ultracold atomic gases in optical lattices. \n\nIntroduction: Recent advancements have made it possible to create quantum degenerate gases of either bosons or fermions that are localized in periodic potentials. These systems can be described using the Bose-Hubbard model, which has been the focus of extensive research over the past decade. In this study, we examine a gas of interacting bosons subjected to a disordered potential. Disorder can lead to Anderson localization, where the eigenstates become exponentially localized around their initial positions if the disorder exceeds a certain threshold. Recent experimental evidence has shown that such systems exhibit a transition between extended Bloch-like states and localized Wannier-Stark ladders. However, these experiments have primarily been conducted in the weak-disorder regime, where the disorder amplitude V0 is significantly smaller than the typical hopping matrix element J. Here, we explore how interactions influence the behavior of strongly disordered systems.",
        "ori-fast-z-score": 0.09166984970282113,
        "water-fast-z-score": 6.937819061732104,
        "rewrite-fast-z-score": 0.4472135954999579
    },
    {
        "original_text": "We study the statistical properties of surface fluctuations for different values of the lateral size L and height H of the system, using numerical simulations on square lattices with periodic boundary conditions. We find that the probability density function (PDF) of the local slope angle θ is well described by an exponential decay at large angles, while it displays power-law tails at small slopes. The crossover between these two regimes occurs around θ = 0.5π. In addition to this behavior, we observe that the PDFs are strongly dependent upon both L and H. This dependence can be explained as finite-size effects: when L or H becomes smaller than some characteristic length scale, the PDFs become broader due to the presence of more rare events. Finally, we show how our results compare favorably with experimental data obtained from scanning tunneling microscopy measurements performed on Si(111). PACS numbers: 68.35.Bs",
        "watermark_text": "We research the statistical characteristics of fluid fluctuations for different values of the longitudinal length L and elevation H of the system , using numerical simulations on square lattices with periodic border conditions . We see that the probability density function ( PDF ) of the local slope angle θ is well described by an exponential decay at large angles , while it displays power - law tails at small elevations .The crossover between these two regimes occurs around θ = 0 . 5π . In addition to this behavior , we find that the PDFs are strongly dependent upon both L and H . This reliance can be described as finite - length effects : when L or H becomes lower than some characteristic length range , the PDFs get broader owing to the presence of more unusual events .Finally , we prove how our findings compare favorably with experimental evidence derived from scanning tunneling microscopy observations performed on Si ( 111 ) . PACS codes : 68 . 35 . Bs",
        "rewrite_text": "We investigate the statistical properties of fluid fluctuations for various longitudinal lengths (L) and elevations (H) of the system through numerical simulations on square lattices with periodic boundary conditions. Our analysis reveals that the probability density function (PDF) of the local slope angle (θ) is aptly described by an exponential decay for large angles, while exhibiting power-law tails at smaller elevations. The transition between these two behaviors occurs around θ = 0.5π. Additionally, we observe that the PDFs are significantly influenced by both L and H. This dependence can be understood as finite-length effects, as reductions in L or H below a characteristic length scale lead to broader PDFs due to an increase in atypical events. Finally, we demonstrate that our results align well with experimental data obtained from scanning tunneling microscopy studies conducted on Si (111). PACS codes: 68.35.Bs.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 5.658135095031152,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The aim of this book is to provide an overview on the state-of-the-art in on-line condition monitoring and fault diagnosis for industrial processes, with special emphasis on computational intelligence techniques such as neural networks (NNs), fuzzy logic systems (FLS) or evolutionary algorithms (EAs). The book covers both theoretical aspects and practical applications of these methods. It also discusses some recent advances in intelligent sensor technologies that are essential for successful implementation of on-line condition monitoring schemes. This book will be useful not only for researchers but also for engineers who want to apply computational intelligence techniques into their own research work. Contents include:  Chapter 1: Introduction to On-line Condition Monitoring.  Chapter 2: Intelligent Sensors for On-line Condition Monitoring.  Chapters 3-7: Neural Networks for Fault Diagnosis.  Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.   Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.   ...",
        "watermark_text": "The goal of this paper is to provide an overview on the state - of - the - art in on - line condition monitoring and failure detection for industrial systems , with special emphasis on computational intelligence techniques such as neural systems ( NNs ) , fuzzy logic systems ( FLS ) or ecological computers ( EAs ) . The text encompasses both theoretical components and useful users of these algorithms .It additionally outlines some latest advances in intelligent detection systems that are essential for successful implementation of on - line condition monitoring schemes . This book will be valuable not only for researchers but also for designers who desire to apply computational intelligence techniques into their own research effort .Contents comprise : Chapter 1 : Introduction to On - line Condition Monitoring . Chapter 2 : Intelligent Sensors for On - line Condition Monitoring .Chapters 3-7: Neural Networks for Fault Diagnosis.Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis....",
        "rewrite_text": "This paper aims to present a comprehensive overview of the current advancements in online condition monitoring and failure detection for industrial systems, with a particular focus on computational intelligence techniques such as neural networks (NNs), fuzzy logic systems (FLS), and evolutionary algorithms (EAs). It includes both theoretical aspects and practical applications of these algorithms, while also highlighting recent progress in intelligent detection systems that are crucial for the effective implementation of online condition monitoring solutions. This book is intended for both researchers and designers looking to integrate computational intelligence techniques into their work. The contents include: Chapter 1: Introduction to Online Condition Monitoring; Chapter 2: Intelligent Sensors for Online Condition Monitoring; Chapters 3-7: Neural Networks for Fault Diagnosis; Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis; Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.75,
        "rewrite-fast-z-score": 1.8073922282301278
    },
    {
        "original_text": "We present results on interface widths in liquid-gas systems, obtained by molecular dynamics simulations with different potentials. We find that the potential should be able to reproduce both bulk properties (e.g., density) as well as surface tension at coexistence conditions simultaneously. The latter requirement is not fulfilled by standard Lennard-Jones or hard-sphere models. In order to overcome this problem we propose an alternative model which reproduces all relevant physical quantities within their experimental uncertainty. This new model can also be used to study other phenomena like capillary waves. \n \n Introduction \n \n Interfaces are ubiquitous in nature and play important roles in many processes ranging from phase separation  1  over wetting  2  to crystal growth  3  . They have been studied extensively using computer simulations  4  , but it has proven difficult to obtain reliable data due to finite size effects  5  . These arise because interfaces are typically only one particle thick  6  so that they cannot be simulated directly. Instead, periodic boundary conditions must be applied  7, 8  leading to artificial interactions between images of the same interface  9  . As a result, the measured interfacial width depends strongly on system size  10  .\n \nIn recent years there has been considerable progress towards understanding these finite-size effects  11  . It was shown that the dependence of the interfacial width w on the linear dimension L of the system follows a power law  12  :",
        "watermark_text": "We report findings on interface widths in liquid - gas systems , obtained by molecular mechanics simulations with various potentials . We see that the potential should be possible to capture both bulk properties ( e . g . , temperature ) as well as surface tension at coexistence situations simultaneously .The latter requirement is not satisfied by traditional Lennard - Jones or hard - sphere theories . In order to overcome this situation we undertake an additional model which reproduces all relevant physical components within their observation uncertainty .This new model can also be used to study other processes like capillary currents . Introduction Interfaces are ubiquitous in nature and play essential roles in multiple processes ranging from phase splitting 1 over wetting 2 to crystal growth 3 .They have been studied frequently using computer simulations 4 , but it has proven impossible to obtain reliable data due to finite diameter effects 5 . These occur because interfaces are typically only one particle thick 6 so that they cannot be simulated directly .Instead , periodic border conditions must be applied 7 , 8 causing to artificial interactions between images of the same interface 9 . As a consequently , the measured interfacial width depends strongly on system width 10 .In recent years there has been substantial advances towards studying these finite - length effects 11 . It was shown that the dependence of the interfacial width w on the linear dimension L of the system follows a power law 12 :",
        "rewrite_text": "We present our findings on interface widths in liquid-gas systems, derived from molecular mechanics simulations utilizing various potentials. Our results indicate that an effective potential must be capable of simultaneously capturing both bulk properties, such as temperature, and surface tension in coexistence scenarios. Unfortunately, conventional Lennard-Jones or hard-sphere models do not fulfill this second criterion. To address this limitation, we have developed an additional model that accurately reproduces all relevant physical aspects within their observational uncertainties. This new model is also applicable for investigating other processes, such as capillary currents.\n\n**Introduction**  \nInterfaces are pervasive in nature and play crucial roles in a variety of processes, including phase separation, wetting, and crystal growth. Although interfaces have been extensively studied through computer simulations, obtaining reliable data remains challenging due to finite diameter effects. These effects arise because interfaces are generally only one particle thick, making them difficult to simulate directly. Instead, periodic boundary conditions are employed, which can lead to artificial interactions between different images of the same interface. Consequently, the measured interfacial width is highly dependent on the width of the system. In recent years, significant progress has been made in understanding these finite-length effects, demonstrating that the relationship between interfacial width (w) and the linear dimension (L) of the system follows a power law.",
        "ori-fast-z-score": 0.09245003270420485,
        "water-fast-z-score": 6.1941521911817246,
        "rewrite-fast-z-score": 0.647150228929434
    },
    {
        "original_text": "The discovery of the first X-ray pulsar, Scorpius X1 (Sco-X1), by Giacconi et al. (1962) opened up an exciting new field for astrophysics and led to the development of many important concepts such as accretion disks around compact objects.  The study of these systems has been revolutionized with the launch of Chandra and XMM-Newton observatories which have allowed us to probe their physical properties on unprecedented spatial scales.   In this review we will discuss some recent results obtained using these satellites that shed light on how neutron stars are formed and evolve within low-mass binary systems. We will also present our current understanding of the physics behind the formation of relativistic jets observed in several classes of X-ray binaries. Finally, we will briefly describe the prospects offered by future missions like XEUS or Constellation-X for studying these fascinating sources. Keywords: Accretion disk - Compact object - Relativistic jet -X-ray binary",
        "watermark_text": "The observation of the first X - ray pulsar , Scorpius X1 ( Sco - X1 ) , by Giacconi et al . ( 1962 ) opened up an exciting new area for astrophysics and led to the development of several important concepts such as accretion disks around compact galaxies .The investigation of these systems has been revolutionized with the launch of Chandra and XMM - Newton observatories which have permitted us to probe their physical properties on unprecedented spatial scales . In this review we will explore some latest findings obtained using these satellites that shed light on how neutron galaxies are created and evolve within low - weight binary complexes .We will also discuss our latest understanding of the physics behind the formation of relativistic jets discovered in multiple types of X - ray binaries . Finally , we will briefly outline the possibilities offered by future flights like XEUS or Constellation - X for studying these fascinating sources .Keywords : Accretion wheel - Compact object - Relativistic jet - X - ray binary",
        "rewrite_text": "The discovery of the first X-ray pulsar, Scorpius X-1 (Sco-X1), by Giacconi et al. in 1962 opened an exciting new frontier in astrophysics and led to the development of several important concepts, including accretion disks surrounding compact objects. The investigation of these systems has been transformed by the launch of the Chandra and XMM-Newton observatories, which have allowed us to examine their physical properties with unprecedented precision. In this review, we will discuss recent findings from these satellites that illuminate the formation and evolution of neutron stars within low-mass binary systems. We will also delve into our current understanding of the physics behind the formation of relativistic jets observed in various types of X-ray binaries. Lastly, we will briefly outline the potential that future missions like XEUS or Constellation-X hold for further studying these captivating sources. \n\nKeywords: Accretion disk, Compact object, Relativistic jet, X-ray binary.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 5.2532861073211246,
        "rewrite-fast-z-score": 0.8682431421244593
    },
    {
        "original_text": "We have performed first-principles calculations to investigate the characteristics of phonon transmission across an epitaxial interface between Si and Ge, which are important for thermoelectric applications. We found that the phonon transmission is strongly suppressed at low frequencies due to the mismatch in acoustic impedance between two materials. The suppression becomes more significant as the thickness of Ge layer decreases. In addition, we observed that the phonon transmission exhibits strong anisotropy with respect to incident angle. These results suggest that it may be possible to control thermal transport by tuning the structure of epitaxial interfaces. Epitaxial interfaces play crucial roles in determining physical properties such as electrical conductivity  1  , optical reflectivity  2  , mechanical strength  3  , and thermal conductivity  4  . For example, recent studies on superlattices show that the thermal conductance can be reduced significantly compared to bulk values  5, 6  .\nIn this work, we focus on phonons because they dominate heat conduction in solids  7, 8  . Phonon scattering at epitaxial interfaces has been studied extensively using molecular dynamics (MD) simulations  9  or kinetic theory  10  . However, these approaches cannot provide detailed information about phonon transmission across interfaces since they do not take into account atomic interactions explicitly  11  . On the other hand, density functional theory (DFT), which describes electronic states based on quantum mechanics  12  , allows us to calculate phonon transmission coefficients directly  13  . Therefore, DFT-based methods are suitable for investigating phonon transmission across epi-",
        "watermark_text": "We have done first - principles measurements to examine the properties of phonon communication across an epitaxial connection between Si and Ge , which are important for thermoelectric use . We identified that the phonon communication is strongly restrained at low frequencies owing to the mismatch in sound impedance between two materials .The suppression gets more considerable as the height of Ge coating decreases . In addition , we reported that the phonon propagation displays good anisotropy with regard to incident angle .These data suggest that it could be possible to affect heat transport by tuning the structure of epitaxial interfaces . Epitaxial interfaces play crucial roles in determining physical properties such as mechanical conductivity 1 , optical reflectivity 2 , thermal strength 3 , and thermal conductivity 4 .For instance , recent studies on superlattices indicate that the thermal conductance can be reduced greatly compared to bulk values 5 , 6 . In this research , we focus on phonons because they govern heat conduction in solids 7 , 8 .Phonon splitting at epitaxial interfaces has been studied thoroughly using chemical mechanics ( MD ) simulations 9 or kinetic theory 10 . However , these perspectives cannot offer comprehensive information about phonon communication across interfaces since they do not take into consideration atomic interactions explicitly 11 .On the other hand , density functional theory ( DFT ) , which explains electronic states based on quantum mechanics 12 , allows us to estimate phonon communication equations directly 13 . Therefore , DFT - based methods are suitable for investigating phonon communication across epi -",
        "rewrite_text": "We conducted first-principles measurements to investigate the properties of phonon communication across an epitaxial interface between silicon (Si) and germanium (Ge), which are significant for thermoelectric applications. Our findings indicate that phonon communication is greatly restricted at low frequencies due to the mismatch in acoustic impedance between the two materials. This suppression becomes more pronounced as the thickness of the Ge layer decreases. Additionally, we observed that phonon propagation exhibits notable anisotropy concerning the incident angle. These results imply that it may be possible to influence heat transport by modifying the structure of epitaxial interfaces. Such interfaces are crucial in determining various physical properties, including mechanical conductivity, optical reflectivity, thermal strength, and thermal conductivity. For example, recent investigations into superlattices have shown that thermal conductance can be significantly lower than that of bulk materials. In this study, we concentrate on phonons, as they are key to heat conduction in solids. Phonon splitting at epitaxial interfaces has been extensively analyzed using molecular dynamics (MD) simulations or kinetic theory. However, these methods may not provide a complete understanding of phonon communication across interfaces because they do not explicitly consider atomic interactions. In contrast, density functional theory (DFT), which describes electronic states based on quantum mechanics, enables us to derive phonon communication equations directly. Thus, DFT-based approaches are well-suited for investigating phonon communication across epitaxial interfaces.",
        "ori-fast-z-score": -0.7559289460184544,
        "water-fast-z-score": 7.878011364745742,
        "rewrite-fast-z-score": -0.09407208683835973
    },
    {
        "original_text": "We present an exact expression for the pressure tensor of a fluid confined by any external potential, which is valid at all temperatures. The result can be obtained as a special case of the virial expansion for the grand canonical partition function. We show that this expression reduces to known results when applied to specific potentials such as harmonic traps or periodic lattices. Finally we apply our general formula to calculate the equation of state of a gas of fermions with attractive interactions in two dimensions. In particular, we find that the system undergoes a phase transition into a superfluid state below some critical temperature Tc. This work was supported by NSF grant PHY-0456747 (M.A.) . \nI. INTRODUCTORY REMARK\nThe thermodynamic properties of many-body systems are often studied using statistical mechanics methods  1  , where one considers ensembles of particles interacting via a given potential energy V(r). For example, if the particles interact through short-range forces only, then it is possible to derive expressions for various physical quantities like density profiles  2  , compressibility  3  , heat capacity  4  , etc., starting from the microscopic definition of entropy S = -k B ln Z, where k B is Boltzmann s constant and Z is the partition function defined as:",
        "watermark_text": "We introduce an precise representation for the pressure tensor of a fluid confined by any external potential , which is valid at all temperatures . The result can be obtained as a special case of the virial expansion for the grand canonical partition function .We see that this formula reduces to known results when applied to unique potentials such as vibration trapping or periodic lattices . Finally we apply our general formula to estimate the equation of state of a gas of fermions with interesting interactions in two dimensions .In particular , we find that the system undergoes a phase shift into a superfluid state below some significant heat Tc . This research was supported by NSF grant PHY - 0456747 ( M . A . ).I.INTRODUCTORY REMARK The thermodynamic properties of several - bodies systems are often researched employing statistical mechanics methods 1 , where one studies ensembles of molecules evolving via a given potential energy V ( r ) . For instance , if the atoms interact through short - range forces only , then it is possible to derive expressions for various mechanical parameters like density profiles 2 , compressibility 3 , temperature strength 4 , etc . , beginning from the microscopic definition of entropy S = - k B ln Z , where k B is Boltzmann s constant and Z is the partition function characterized as :",
        "rewrite_text": "We present a precise formulation for the pressure tensor of a fluid constrained by any external potential, applicable across all temperatures. This result emerges as a specific case of the virial expansion for the grand canonical partition function. Notably, our formula simplifies to established outcomes when considered for distinct potentials, such as vibrational traps or periodic lattices. Additionally, we utilize our general formula to approximate the equation of state for a gas of fermions exhibiting intriguing interactions in two dimensions. Specifically, we observe that the system transitions into a superfluid state below a significant critical temperature, denoted as Tc. This research was funded by NSF grant PHY - 0456747 (M. A.). \n\n**I. INTRODUCTORY REMARK**  \nThe thermodynamic behaviors of various many-body systems are frequently explored using statistical mechanics techniques, where researchers analyze ensembles of molecules interacting through a specified potential energy \\( V(r) \\). For instance, when atoms interact solely via short-range forces, it becomes feasible to derive expressions for mechanical parameters such as density profiles, compressibility, and temperature strength, commencing from the microscopic definition of entropy \\( S = -k_B \\ln Z \\), where \\( k_B \\) represents Boltzmann's constant and \\( Z \\) is the partition function defined as:",
        "ori-fast-z-score": 0.8251369970070347,
        "water-fast-z-score": 6.325405337855594,
        "rewrite-fast-z-score": 0.618852747755276
    },
    {
        "original_text": "We study the dependence of galaxy isophotal structure parameters, such as the Sersic index n, effective radius Re, axis ratio q, position angle PA, and surface brightness SB, on environment (local density) and nuclear activity (AGN luminosity). We use a sample of early-type galaxies selected by their colors from the Sloan Digital Sky Survey Data Release 6 (SDSS DR6), which contains about 1 million objects with spectroscopic redshifts between 0 < z < 0.3. The local density around each galaxy was estimated using its nearest neighbors within a projected distance rp = 20h-1 Mpc and a velocity difference |v| = 1000 km s-1. For our analysis we used only those galaxies that have no nearby companions brighter than them by more than one magnitude to avoid any possible contamination due to tidal interactions or mergers. \n \n In order to investigate how these structural properties depend on environment and nuclear activity, we divided our sample into four different subsamples based on the values of local density and AGN luminosity: low-density/low-luminosity active galactic nuclei (LLAGNs), high-density/high-luminosity active Galactic Nuclei (HLAGNs), low-density/high-luminous inactive galaxies (LHIGGs), and high-density/low-luminous inactive galaxies(HLIGGs). \n \n Our results show that LLAGNs are generally rounder and less concentrated compared to HLAGNs. This suggests that LLAGNs may be undergoing morphological transformations driven by environmental effects and/or internal processes associated with black hole growth. On average, LHIGGs appear to be rounder but slightly less concentrated than HLIGGs. However, there appears to be an overlap among all four samples for most of the structural parameters considered here.",
        "watermark_text": "We research the dependence of galaxy isophotal shape parameters , such as the Sersic index h , effective radius Re , axis proportion r , point angle PA , and surface brightness SB , on climate ( local concentration ) and nuclear activity ( AGN luminosity ) . We use a sample of early - class stars selected by their colors from the Sloan Digital Sky Survey Data Release 6 ( SDSS DR6 ) , which contains about 1 million bodies with spectroscopic redshifts between 0 < z < 0 . 3 .The local concentration around each galaxy was calculated using its closest neighbors within a projected diameter rp = 20h - 1 Mpc and a speed difference | v | = 1000 cm s - 1 . For our analysis we using only those galaxies that have no nearby friends brighter than them by more than one magnitude to eliminate any likely infection due to tidal interactions or mergers .In order to investigate how these structural properties depend on environment and nuclear activity , we divided our sample into four different subsamples based on the values of local density and AGN luminosity : low - density / low - luminosity active galactic nuclei ( LLAGNs ) , high - density / high - luminosity active Galactic Nuclei ( HLAGNs ) , low - density / high - luminous inactive galaxies ( LHIGGs ) , and high - density / low - luminous inactive galaxies ( HLIGGs ) . Our results show that LLAGNs are generally rounder and less concentrated compared to HLAGNs .This implies that LLAGNs might be experiencing morphological transformations motivated by ecological effects and / or internal mechanisms associated with black hole growth . On average , LHIGGs occur to be rounder but little less concentrated than HLIGGs .However , there seems to be an interchange among all four samples for most of the structural values discussed here .",
        "rewrite_text": "We investigate how the shape parameters of galaxy isophotes—specifically the Sersic index (h), effective radius (Re), axis ratio (r), position angle (PA), and surface brightness (SB)—are influenced by environmental factors like local density and nuclear activity measured by AGN luminosity. Our study utilizes a sample of early-type galaxies selected by their colors from the Sloan Digital Sky Survey Data Release 6 (SDSS DR6), which includes approximately one million galaxies with spectroscopic redshifts ranging from 0 to 0.3. To assess the local density around each galaxy, we calculated the concentration based on its closest neighbors within a projected diameter of rp = 20 h⁻¹ Mpc and a velocity difference of |v| = 1000 km/s. We focused exclusively on galaxies that do not have nearby companions brighter than them by more than one magnitude to minimize potential complications from tidal interactions or mergers. To explore the relationship between structural properties, environmental influences, and nuclear activity, we categorized our sample into four distinct subsamples according to local density and AGN luminosity: low-density/low-luminosity active galactic nuclei (LLAGNs), high-density/high-luminosity active galactic nuclei (HLAGNs), low-density/high-luminosity inactive galaxies (LHIGGs), and high-density/low-luminosity inactive galaxies (HLIGGs). Our findings indicate that LLAGNs tend to be rounder and less concentrated than HLAGNs, suggesting that LLAGNs may be undergoing morphological changes driven by environmental effects or internal processes linked to black hole growth. On average, LHIGGs appear to be rounder but slightly less concentrated compared to HLAGNs. However, we observe a degree of interchangeability among the four groups regarding the structural parameters examined.",
        "ori-fast-z-score": -1.6431676725154982,
        "water-fast-z-score": 5.363636363636363,
        "rewrite-fast-z-score": -0.08804509063256238
    },
    {
        "original_text": "We present parallax measurements for four short-period (P orb < 80 min) AM CVn stars using the Fine Guidance Sensor on board HST, which are accurate to better than 1% in distance. We find that all systems have distances consistent with their being located within 20 pc of Earth. The derived absolute magnitudes range between MV = 12.5 and 14.1 mag, corresponding to luminosities of 1030 - 1600 L⊙ . These values are significantly brighter than those predicted by theoretical models of these objects, indicating either an underestimate of the mass transfer rates or an overestimate of the white dwarf masses. In addition we use our new parallaxes together with previously published data to derive improved estimates of the orbital periods and component masses for three of the targets studied here. Finally, we discuss how our results can be used to test current evolutionary scenarios for this class of binary system.",
        "watermark_text": "We present parallax observations for four short - duration ( P orb < 80 min ) AM CVn stars using the Fine Guidance Sensor on board HST , which are accurate to good than 1 % in distance . We see that all systems have distances consistent with their being located within 20 pc of Earth .The derived absolute magnitudes range between MV = 12 . 5 and 14 . 1 mag , corresponding to luminosities of 1030 - 1600 [UNK] . These values are significantly brighter than those predicted by theoretical models of these objects , indicating either an underestimate of the mass transfer rates or an overestimate of the white dwarf masses .In addition we utilize our new parallaxes together with former reported information to derive improved models of the orbital periods and component masses for three of the targets examined here . Finally , we explain how our findings can be used to test recent evolutionary scenarios for this class of binary system .",
        "rewrite_text": "We present parallax measurements for four short-duration (P_orb < 80 min) AM CVn stars, obtained using the Fine Guidance Sensor on the Hubble Space Telescope (HST), achieving an accuracy of better than 1% in distance estimates. Our results indicate that all systems are located within 20 parsecs of Earth. The calculated absolute magnitudes range from MV = 12.5 to 14.1 mag, reflecting luminosities between 10^30 and 10^31 erg/s. These luminosity values are significantly higher than those predicted by theoretical models for these objects, suggesting that current estimations of mass transfer rates may be too low or that white dwarf masses may be overestimated. Furthermore, we integrate our new parallax data with previously reported measurements to refine models of the orbital periods and component masses for three of the systems examined. Lastly, we discuss how our findings can serve to evaluate recent evolutionary scenarios pertaining to this type of binary system.",
        "ori-fast-z-score": 0.254000254000381,
        "water-fast-z-score": 3.556003556005334,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "We introduce the concept of generalized conditional random fields (GCRFs) and show how they can be used to model arbitrary probability distributions over structured data sets, such as sequences or trees.  We present an efficient algorithm for learning GCRF parameters using gradient descent on the log-likelihood objective function.   Finally we demonstrate that our approach is able to learn accurate models for several challenging sequence labeling tasks including part-of-speech tagging in natural language processing and protein secondary structure prediction in bioinformatics. Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) are undirected graphical models which have been successfully applied to many problems involving sequential data, e.g. (Sha & Pereira, 2003) . In this work, we propose Generalized Conditional Random Fields (GCRFs), a generalization of CRFs which allows us to represent any distribution over structured data sets like sequences or trees. The key idea behind GCRFs is to use a set of latent variables to capture dependencies between different parts of the input space. This enables us to efficiently compute the partition function required by standard CRFs with dynamic programming techniques. Furthermore, it also makes it possible to train GCRFs using gradient-based methods similar to those employed for Maximum Entropy Markov Models (MEMMs). To evaluate the performance of our method, we apply it to two important applications: part-of-speech taggin",
        "watermark_text": "We introduce the idea of generalized conditional random fields ( GCRFs ) and explain how they can be used to model arbitrary likelihood distributions over structured data sets , such as sequences or trees . We introduce an efficient algorithm for learning GCRF variables using gradient descent on the log - likelihood objective function .Finally we prove that our approach is ability to acquire precise models for numerous challenging gene labeling challenges including whole - of - voice tagging in natural language processing and protein secondary structure prediction in bioinformatics . Conditional Random Fields ( CRFs ) ( Lafferty et al . , 2001 ) are undirected numerical models which have been successfully applied to many difficulties involving sequential data , e . g .( Sha & Pereira , 2003 ) . In this research , we propose Generalized Conditional Random Fields ( GCRFs ) , a generalization of CRFs which allows us to model any distribution over structured data sets like sequences or trees .The main idea behind GCRFs is to use a setting of latent variables to capture dependencies between various parts of the input space . This enables us to easily compute the partition function required by traditional CRFs with dynamic programming tools .Furthermore , it also makes it able to train GCRFs using gradient - based methods similar to those utilized for Maximum Entropy Markov Models ( MEMMs ) . To assess the performance of our technique , we apply it to two essential applications : whole - of - voice taggin",
        "rewrite_text": "We present Generalized Conditional Random Fields (GCRFs) and describe their utility in modeling complex likelihood distributions over structured datasets, such as sequences and trees. Our approach includes an efficient algorithm for learning the parameters of GCRFs through gradient descent applied to the log-likelihood objective function. Furthermore, we demonstrate that our method can effectively capture accurate models across various challenging gene labeling tasks, including whole-voice tagging in natural language processing and protein secondary structure prediction in bioinformatics. Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models that have proven effective for various sequential data challenges, as highlighted by Sha & Pereira (2003). In this work, we introduce GCRFs as an extension of CRFs that allows for the modeling of any distribution over structured datasets such as sequences or trees. The core concept of GCRFs involves utilizing latent variables to represent dependencies among different components of the input space. This approach facilitates the computation of the partition function needed for traditional CRFs through dynamic programming techniques. Additionally, it enables the training of GCRFs using gradient-based methods similar to those applied in Maximum Entropy Markov Models (MEMMs). To evaluate the performance of our method, we apply it to two important applications: whole-voice tagging and another critical task in the field.",
        "ori-fast-z-score": -0.4833682445228318,
        "water-fast-z-score": 6.735753140545634,
        "rewrite-fast-z-score": 0.30151134457776363
    },
    {
        "original_text": "We present new radial velocity measurements for the short-period binary system WZ Sge, which show that its orbital period is decreasing at an average rate of  _ P = -1.3 x 10^(-7) d/s.  We use these data to derive dynamical mass estimates for both components and find M_1 = 0.85 ± 0.05M_sun and M_2 = 0.65 ± 0.04M_sun. The primary star has evolved off the main sequence but still retains some hydrogen in its atmosphere; it is therefore classified as a subdwarf B (sdB). Our results are consistent with previous determinations based on photometric observations. However, our analysis provides more accurate values because we have used higher quality spectroscopic data than were available previously. In addition, we have been able to determine the individual masses rather than just their ratio. This work was supported by NASA grant NAG5-13523.",
        "watermark_text": "We present new radial speed measurements for the short - period binary system WZ Sge , which show that its orbital period is decreasing at an average rate of _ P = - 1 . 3 x 10 ^ ( - 7 ) d / s . We use these information to derive dynamical mass estimates for both components and find M _ 1 = 0 . 85 ± 0 . 05M _ sun and M _ 2 = 0 . 65 ± 0 . 04M _ sun .The main star has evolution off the main sequence but still retains some hydrogen in its atmosphere ; it is consequently listed as a subdwarf B ( sdB ) . Our results are compatible with previous determinations based on photometric studies .However , our analysis provides more accurate figures because we have utilized higher reliability spectroscopic data than were offered previously . In addition , we have been able to predict the individual masses rather than just their ratio .This project was supported by NASA gift NAG5 - 13523 .",
        "rewrite_text": "We report new radial speed measurements for the short-period binary system WZ Sge, indicating that its orbital period is decreasing at an average rate of _ P = - 1.3 x 10 ^ ( - 7 ) days/second. Using this data, we derive dynamical mass estimates for both stars, finding M _ 1 = 0.85 ± 0.05 M _ sun and M _ 2 = 0.65 ± 0.04 M _ sun. The primary star has evolved off the main sequence but still retains some hydrogen in its atmosphere, classifying it as a subdwarf B (sdB) star. Our findings align with previous estimates from photometric studies; however, our analysis offers more precise values due to the use of higher-quality spectroscopic data. Furthermore, we have succeeded in determining the individual masses rather than merely their ratio. This research was supported by NASA grant NAG5 - 13523.",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": 1.6378460497066512
    },
    {
        "original_text": "The intramolecular structure of polymers is studied by means of the molecular dynamics simulation method for several model systems with different chain lengths, degrees of branching, and types of interactions between monomers. It has been found that the intramolecular correlation function can be represented as a sum of two terms corresponding to short- and long-range contributions. The latter term describes the contribution of segments separated by distances larger than the average distance between neighboring chains. This term depends on the number density of chains and their length. In addition, it was shown that this term decreases exponentially at large separations. The dependence of the first four moments of the segmental size distribution on temperature and pressure are calculated using the results obtained previously. These dependences were compared with experimental data available in literature. Good agreement between theory and experiment is observed. \n \n Keywords: Polymer melt, Intramolecular structure, Correlation functions, Segmental size distribution",
        "watermark_text": "The intramolecular structure of polymers is studied by means of the molecular dynamics simulation method for numerous model models with various chain lengths , degrees of branching , and types of interactions between monomers . It has been shown that the intramolecular relationship value can be described as a sum of two terms corresponding to short - and long - range contributions .The latter term describes the contribution of segments apart by distances bigger than the average distance between neighboring lines . This term relies on the number density of chains and their length .In addition , it was shown that this term drops exponentially at large separations . The dependence of the first four moments of the segmental size distribution on temperature and pressure are measured using the results derived earlier .These dependences were compared with observation information available in literature . Good agreement between theoretical and experiment is observed .Keywords : Polymer melt , Intramolecular structure , Correlation properties , Segmental size distribution",
        "rewrite_text": "The intramolecular structure of polymers is investigated using molecular dynamics simulations across various model configurations that differ in chain lengths, degrees of branching, and the types of interactions between monomers. It has been demonstrated that the value of intramolecular relationships can be represented as the sum of two components: one for short-range interactions and another for long-range contributions. The long-range component accounts for segments that are separated by distances greater than the average distance between neighboring chains and depends on chain number density and length. Furthermore, this long-range term decreases exponentially at larger separations. The first four moments of the segmental size distribution are analyzed in relation to temperature and pressure, utilizing previously derived results. These analyses are compared with available observational data from the literature, showing a strong agreement between theoretical predictions and experimental findings. Keywords: Polymer melt, Intramolecular structure, Correlation properties, Segmental size distribution.",
        "ori-fast-z-score": 0.7977240352174656,
        "water-fast-z-score": 4.817730411281796,
        "rewrite-fast-z-score": 0.562543950463012
    },
    {
        "original_text": "We present new results on the nature and geometry of the compact object in the gamma-ray binary system LS I +61 303, based on observations with the INTEGRAL satellite. We find that the source is variable at all wavelengths studied here (radio to hard X-rays), but shows no evidence for orbital modulation or eclipses. The X-ray spectrum can be described by either a power law model or thermal bremsstrahlung emission; both are consistent with previous studies. In addition we report the detection of pulsations in the radio band which have been previously reported only once before. These pulsations show up as periodic intensity variations in our data set, and their periodicity has been confirmed using two independent methods. Using these results together with those obtained from optical photometry and spectroscopy, we conclude that this source most likely contains a neutron star accreting matter from its companion Be-star via Roche lobe overflow.",
        "watermark_text": "We report new data on the nature and morphology of the compact body in the gamma - ray binary system LS I + 61 303 , based on observations with the INTEGRAL satellite . We see that the origin is varying at all wavelengths explored here ( radio to soft X - radiation ) , but gives no evidence for orbital modulation or eclipses .The X - ray signal can be described by either a power law description or heating bremsstrahlung emission ; both are compatible with previous research . In addition we study the observation of pulsations in the radio band which have been previously reported only once before .These pulsations appear up as periodic intensity variations in our information pool , and their periodicity has been confirmed using two independent methods . Using these results together with those acquired from optical photometry and spectroscopy , we determine that this source most likely contains a neutron star accreting matter from its companion Be - star via Roche lobe overflow .",
        "rewrite_text": "We present new findings on the characteristics and structure of the compact object in the gamma-ray binary system LS I + 61 303, derived from observations conducted with the INTEGRAL satellite. Our analysis reveals that the source exhibits variability across all wavelengths studied, ranging from radio to soft X-rays, without any indications of orbital modulation or eclipses. The X-ray emissions can be modeled using either a power law or heating bremsstrahlung emission, both of which align with prior research. Furthermore, we investigate the radio pulsations that have been reported only once before. These pulsations manifest as periodic intensity fluctuations in our data, and their periodic nature has been validated through two independent techniques. Combining these findings with data from optical photometry and spectroscopy, we conclude that this source likely consists of a neutron star that is accreting material from its Be-star companion via Roche lobe overflow.",
        "ori-fast-z-score": -0.36650833306891567,
        "water-fast-z-score": 5.2532861073211246,
        "rewrite-fast-z-score": -0.5163977794943222
    },
    {
        "original_text": "We study the dynamics of an open system, which is composed by a two-level atom interacting with a single-mode cavity field and driven by a classical laser source. We show that this simple model can be used to describe the behavior of a quantum information processor (QIP) based on trapped ions or atoms coupled to optical cavities. In particular we find signatures of decoherence in QIPs due to spontaneous emission noise. The results are obtained using exact numerical solutions for the master equation describing our model. Quantum information processing has been proposed as one possible application of quantum mechanics  1  . A number of experimental realizations have already been achieved  2  , but it remains unclear how practical these devices will become  3  .\nIn order to understand better what kind of problems may arise when implementing such schemes experimentally, it would be useful to develop models that allow us to investigate the effects of different types of errors  4  . Here we consider a very simple model consisting of a two-level atom interacting resonantly with a single mode of an electromagnetic field inside a cavity  5  . This system could represent either a trapped ion  6  or an atom coupled to an optical cavity  7, 8  . It is well known that if there were no losses present then the state of the system at any time t > 0 would simply be given by |ψ(t) = e −iωat/2 cos ω c t + θ(0) / √ 2 where ω a and ω c are respectively the atomic and cavity frequencies  9  . However, in practice both the atom-cavity coupling strength g and the decay rate κ associated with the cavity field are finite so that the evolution of the system becomes more complicated  10  .",
        "watermark_text": "We explore the dynamics of an open network , which is composed by a two - level atom interacting with a single - mode cavity field and driven by a traditional beam source . We see that this straightforward model can be used to explain the response of a quantum information processor ( QIP ) based on trapped ions or atoms connected to laser cavities .In particular we find signatures of decoherence in QIPs due to spontaneous emission interference . The results are derived using correct mathematical solutions for the master equation representing our model .Quantum knowledge processing has been proposed as one possible application of quantum mechanics 1 . A variety of research realizations have already been achieved 2 , but it remains unsure how practical these machines will become 3 .In order to explain better what sort of problems might arise when executing such schemes experimentally , it would be beneficial to develop models that enable us to examine the effects of different kinds of errors 4 . Here we study a very simple study consisting of a two - level atom interacting resonantly with a single mode of an electromagnetic field inside a cavity 5 .This system could represent either a trapped ion 6 or an element coupled to an optical cavity 7 , 8 . It is well established that if there were no losses present then the state of the system at any time t > 0 would merely be described by | ψ ( t ) = e −iωat / 2 cos ω c t + θ ( 0 ) / √ 2 where α a and ω c are respectively the atomic and cavity frequencies 9 .However , in practice both the atom - cavity coupling strength g and the decay rate κ involved with the cavity field are finite so that the evolution of the system gets more complicated 10 .",
        "rewrite_text": "We investigate the dynamics of an open network comprising a two-level atom that interacts with a single-mode cavity field, driven by a conventional beam source. Our straightforward model effectively illustrates the behavior of quantum information processors (QIPs) utilizing trapped ions or atoms linked to laser cavities. Notably, we identify signatures of decoherence in QIPs stemming from spontaneous emission interference. The findings are obtained through precise mathematical solutions to the master equation governing our model. Quantum knowledge processing has been proposed as a potential application of quantum mechanics, and while various research implementations have been achieved, the practicality of these machines remains uncertain. To better understand the potential issues that may arise during experimental execution, it is advantageous to develop models that allow us to investigate the impacts of different types of errors. In this study, we focus on a simple scenario involving a resonantly interacting two-level atom and a single mode of an electromagnetic field within a cavity. This system could represent a trapped ion or an atom coupled to an optical cavity. In an ideal scenario without losses, the state of the system at any time \\( t > 0 \\) is described by \\( | \\psi(t) \\rangle = e^{-i\\omega_a t/2} \\cos(\\omega_c t + \\theta(0))/\\sqrt{2} \\), where \\( \\alpha_a \\) and \\( \\omega_c \\) are the atomic and cavity frequencies, respectively. However, practical considerations show that both the atom-cavity coupling strength \\( g \\) and the decay rate \\( \\kappa \\) of the cavity field are finite, complicating the system's evolution.",
        "ori-fast-z-score": -0.25630729731502827,
        "water-fast-z-score": 6.46954963376649,
        "rewrite-fast-z-score": 1.975658322294524
    },
    {
        "original_text": "We study the unitarity properties of an effective field theory describing the interactions between gluons and quarks in QCD at high energies, where perturbation theory is not applicable anymore. The model we consider consists of a gauge-invariant action containing both massive and massless fields. We show that this model can be written as a sum over Feynman diagrams which are all unitary individually if certain conditions on the parameters appearing in the Lagrangian density are satisfied. In particular, it turns out that the masses of the particles involved must satisfy some relations to ensure unitarity. Finally, we discuss how these results could be used for phenomenological applications. PACS numbers: 11.10.Wx, 12.38.Qk, 13 .60.Hb \nI. INTRODUCTORY REMAR K S\nThe Standard Model (SM) describes successfully most experimental data available today  1  , but its validity has been tested only up to energies of about 1 TeV  2  . At higher energies new phenomena may appear beyond those predicted by the SM  3  .\nIn order to describe such effects one usually considers extensions of the SM  4  or models based on effective theories  5  . Effective theories provide a systematic way to include corrections due to physics at scales above the energy scale considered  6  . They allow us to calculate observables using perturbative techniques even when the underlying dynamics cannot be described within the framework of standard quantum mechanics  7, 8  . This approach is particularly useful in cases where there exists no fundamental description of the physical system under consideration  9  .\nOne example of an effective theory is Quantum Chromodynamics (QCD), the theory of strong interactions  10  . It predicts the existence of hadrons made of quarks and gluons  11  . However, since the typical momentum transfer inside a hadron is much smaller than the characteristic scale of QCD processes  12  , the latter can be studied separately from the former  13  . For instance, the production of jets  14  and heavy flavors  15  in high-energy collisions can be calculated using perturbative methods  16  . On the other hand, the interaction among partons  17 ",
        "watermark_text": "We explore the unitarity properties of an efficient field model explaining the interactions between gluons and quarks in QCD at high energies , where perturbation theory is not applicable nowadays . The model we treat consists of a gauge - invariant action incorporating both giant and massless fields .We see that this description can be written as a sum over Feynman diagrams which are all unitary individually if certain conditions on the variables appearing in the Lagrangian density are fulfilled . In particular , it turns out that the masses of the interactions involved must satisfy some relations to ensure unitarity .Finally , we explain how these results could be used for phenomenological purposes . PACS codes : 11 . 10 . Wx , 12 . 38 . Qk , 13 . 60 . Hb I .INTRODUCTORY REMAR K S The Standard Model ( SM ) presents successfully most empirical data available today 1 , but its authenticity has been tested only up to energies of about 1 TeV 2 . At higher energies new concepts might appear beyond those predicted by the SM 3 .In order to explain such effects one usually uses extensions of the SM 4 or models built on effective models 5 . Effective models seek a comprehensive way to consider corrections due to physics at scales above the energy scale discussed 6 .They allow us to estimate observables using perturbative methodology even when the fundamental dynamics cannot be described within the framework of standard quantum mechanics 7 , 8 . This method is especially suitable in cases where there exists no basic description of the physical system under consideration 9 .One example of an effective theory is Quantum Chromodynamics ( QCD ) , the model of strong interactions 10 . It predicts the existence of hadrons made of quarks and gluons 11 .However , since the typical velocity transfer inside a hadron is much smaller than the typical scale of QCD processes 12 , the latter can be analyzed separately from the former 13 . For instance , the production of jets 14 and dark flavors 15 in high - energy collisions can be determined use perturbative methods 16 .On the other hand , the interaction among partons 17",
        "rewrite_text": "We investigate the unitarity properties of an effective field model that describes the interactions between gluons and quarks in Quantum Chromodynamics (QCD) at high energies, where perturbation theory is no longer applicable. The model we analyze features a gauge-invariant action that includes both giant and massless fields. Our findings indicate that this framework can be expressed as a sum over Feynman diagrams, each of which maintains unitarity, provided that specific conditions regarding the variables in the Lagrangian density are met. Notably, the masses of the interactions must satisfy particular relations to uphold unitarity. Finally, we discuss how these insights could be utilized for phenomenological applications. \n\nThe Standard Model (SM) successfully describes most available empirical data, but its validity has only been verified up to energies around 1 TeV. Beyond this threshold, new phenomena may emerge that are not accounted for by the SM. To address these effects, researchers often look to extensions of the SM or effective models. These effective models aim to provide a holistic approach to incorporate corrections arising from physics at scales exceeding those currently under consideration. They enable the estimation of observables using a perturbative approach even when the fundamental dynamics elude conventional quantum mechanics. This methodology is particularly advantageous when a fundamental description of the physical system is absent. Quantum Chromodynamics (QCD), which governs strong interactions, serves as a prime example of an effective theory. It predicts the formation of hadrons composed of quarks and gluons. However, because the typical velocity transfer within a hadron is much less than the characteristic scale of QCD processes, these processes can be analyzed independently. For instance, the production of jets and dark flavors in high-energy collisions can be effectively studied using perturbative methods. Conversely, the interactions among partons present additional complexities.",
        "ori-fast-z-score": -0.08481889296799709,
        "water-fast-z-score": 7.7754191435023525,
        "rewrite-fast-z-score": 1.0524696231684352
    },
    {
        "original_text": "We present an exact expression for the pair correlation functions in terms of a free energy functional, valid both above and below the nematic transition temperature T_N. The functional is obtained by minimizing the grand potential with respect to all possible orientations of the director field n(r). We show that this approach leads to results which are consistent with those obtained using Onsager s theory at high temperatures (T_T>T_N), but also provides new insight into the structure of the nematic phase near its critical point. In particular we find that the orientational order parameter S exhibits a non-monotonic behavior as a function of density, while the positional correlations remain short ranged throughout the entire range of densities studied here. Finally, our analysis shows that the nematic phase can be viewed as a superposition of two different liquid phases characterized by distinct values of the orientational order parameter. PACS numbers: 64.70.Pq",
        "watermark_text": "We present an precise representation for the pair correlation functions in terms of a free energy functional , valid both above and below the nematic transition temperature T _ N . The functional is found by minimizing the grand potential with regard to all possible orientations of the director field n ( r ) .We see that this methodology leads to findings which are compatible with those achieved using Onsager s principle at high heating ( T _ T > T _ N ) , but also provides new insight into the formation of the nematic phase near its critical position . In particular we find that the orientational order parameter S exhibits a non - monotonic activity as a function of density , while the positional correlations remain small ranged throughout the entire range of densities explored here .Finally , our analysis shows that the nematic phase can be viewed as a superposition of two different fluid stages characterized by separate values of the orientational order parameter . PACS codes : 64 . 70 . Pq",
        "rewrite_text": "We present a precise representation of pair correlation functions in terms of a free energy functional, applicable both above and below the nematic transition temperature \\( T_N \\). This functional is derived by minimizing the grand potential with respect to all potential orientations of the director field \\( n(r) \\). Our approach yields results consistent with those obtained through Onsager's principle at elevated temperatures (\\( T_T > T_N \\)), while also offering new insights into the formation of the nematic phase near its critical point. Notably, we observe that the orientational order parameter \\( S \\) displays a non-monotonic behavior as a function of density, whereas positional correlations remain short-ranged across the entire density range examined. Ultimately, our analysis reveals that the nematic phase can be interpreted as a superposition of two distinct fluid states characterized by different values of the orientational order parameter. PACS codes: 64.70.Pq",
        "ori-fast-z-score": 0.3611575592573076,
        "water-fast-z-score": 4.780914437337574,
        "rewrite-fast-z-score": 0.6108472217815261
    },
    {
        "original_text": "We study the dynamics of three agent games with two strategies each, where agents are connected by an underlying network and play pairwise interactions according to their strategy choices. We show that for any initial state there is always at least one absorbing state in which all agents have the same strategy choice. In addition we find that if the number of nodes with either strategy exceeds 1 then this state can be reached within finite time. Finally, we provide bounds on how fast such convergence occurs as well as conditions under which it will occur exponentially quickly. The results presented here generalize previous work done on two-agent evolutionary games to multi-agent systems. Evolutionary game theory has been used extensively over the past decade to model competition between different species or individuals competing for limited resources  1  . A common approach taken when modeling these types of problems is to consider a population consisting of many interacting agents who choose among several possible strategies  2  , and then use mathematical tools developed in statistical physics  3  to analyze the resulting system behavior  4  .\nIn recent years researchers have begun studying more complex models involving multiple populations  5  , spatial structure  6  , and heterogeneous environments  7, 8  . However, most existing research focuses only on two-player games  9  , while less attention has been paid to multi-agent systems  10  . Here we present new results showing that even simple multi-agent systems exhibit richer dynamical behaviors than previously thought  11  .",
        "watermark_text": "We explore the dynamics of three agent games with two strategies each , where agents are connected by an underlying network and play pairwise relationships according to their strategy choices . We see that for any initial state there is usually at least one absorbing state in which all agents have the same strategy selection .In addition we find that if the number of nodes with either strategy approaches 1 then this state can be reached within finite period . Finally , we provide bounds on how rapid such convergence occurs as well as conditions under which it will occur exponentially rapidly .The results presented here generalize past work done on two - agent evolutionary games to multi - agent systems . Evolutionary game theory has been used heavily over the previous decade to model competition between various populations or individuals competing for limited supplies 1 .A typical approach took when solving these kinds of problems is to consider a population consisting of several interacting agents who decide among various possible strategies 2 , and then use numerical tools developed in statistical physics 3 to analyze the resulting system interaction 4 . In recent years investigators have begun investigating more sophisticated models involving several populations 5 , temporal formation 6 , and heterogeneous environments 7 , 8 .However , most existing study emphasis only on two - player players 9 , while worse attention has been paid to multi - agent systems 10 . Here we present new data demonstrating that even basic multi - agent systems exhibit richer dynamical traits than previously thought 11 .",
        "rewrite_text": "We examine the dynamics of three-agent games, each with two strategies, where agents are linked by an underlying network and engage in pairwise interactions based on their chosen strategies. Our findings indicate that for nearly any initial configuration, there is typically at least one absorbing state where all agents adopt the same strategy. Moreover, we discover that if the number of agents employing either strategy approaches one, this absorbing state can be attained in a finite timeframe. We also establish boundaries on the speed of convergence and the conditions under which this convergence occurs exponentially rapidly. The results presented here extend previous research on two-agent evolutionary games to multi-agent systems. Over the past decade, evolutionary game theory has been extensively utilized to model competition among various populations or individuals vying for limited resources. A common approach to tackle these problems involves examining a population of interacting agents that choose from multiple strategies and applying numerical methods from statistical physics to analyze the resulting interactions. Recently, researchers have begun to explore more complex models that include multiple populations, temporal dynamics, and heterogeneous environments. However, the majority of existing studies focus predominantly on two-player scenarios, with less attention given to multi-agent systems. This study provides new evidence that even simple multi-agent systems display more complex dynamical behaviors than previously recognized.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.789028582272215,
        "rewrite-fast-z-score": 1.590990257669732
    },
    {
        "original_text": "We present an algorithm for the numerical evaluation of Feynman diagrams with arbitrary numbers of external particles and internal loops, which is based on the concept of  partonic subdiagrams . The method allows to perform calculations in QCD beyond leading order accuracy without any approximations or assumptions about the kinematics of the process under consideration. We demonstrate its applicability by calculating the next-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. In this talk we will discuss how one can gain analytic control over parton showers using the concept of  partons  as fundamental degrees of freedom. This approach has been developed recently within the framework of Soft-Collinear Effective Theory (SCET)  1  . It provides a systematic way to resum large logarithms associated with collinear splittings into multiple jets  2  , thereby improving our understanding of jet physics  3  .\nThe basic idea behind SCET is that physical observables are described by matrix elements involving soft and/or collinear fields only  4  . These fields have nontrivial transformation properties under boosts along the beam axis  5  . They allow us to separate hard interactions from soft radiation  6  . As a result, it becomes possible to systematically factorize contributions to scattering amplitudes into  hard functions  describing short-distance dynamics  7, 8  and  semi-hard functions  encoding information about the emission of soft gluons  9  .",
        "watermark_text": "We present an algorithm for the numerical identification of Feynman diagrams with arbitrary numbers of external particles and internal loops , which is based on the idea of partonic subdiagrams . The method enables to conduct measurements in QCD beyond trailing order accuracy without any approximations or assumptions about the kinematics of the process under consideration .We test its applicability by calculating the second - to - leading - order corrections to the production cross section of large quarks at hadron colliders . In this talk we will explore how one can obtain analytic control over parton showers using the idea of partons as essential degrees of liberty .This method has been constructed recently within the framework of Soft - Collinear Effective Theory ( SCET ) 1 . It provides a comprehensive way to resum big logarithms associated with collinear splittings into multiple jets 2 , thereby improving our appreciation of flight mechanics 3 .The basic idea behind SCET is that physical observables are explained by matrix elements featuring soft and / or collinear fields only 4 . These fields have nontrivial transformation qualities under boosts along the laser axis 5 .They allow us to separate hard interactions from soft light 6 . As a result , it becomes possible to deliberately factorize contributions to scattering amplitudes into hard functions describing short - distance dynamics 7 , 8 and semi - hard functions encoding information about the emission of deep gluons 9 .",
        "rewrite_text": "We introduce an algorithm for the numerical identification of Feynman diagrams that can accommodate any number of external particles and internal loops, utilizing the concept of partonic subdiagrams. This approach allows for precision measurements in QCD beyond leading-order accuracy without requiring any approximations or assumptions regarding the kinematics of the processes studied. To validate this method, we compute the second-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. In this presentation, we will discuss how one can achieve analytic control over parton showers by treating partons as fundamental degrees of freedom. This method has recently been developed within the framework of Soft-Collinear Effective Theory (SCET). It offers a robust means to sum large logarithms linked to collinear splittings into multiple jets, enhancing our understanding of flight dynamics. The core principle of SCET is that physical observables can be represented by matrix elements that involve only soft and/or collinear fields. These fields exhibit complex transformation properties under boosts along the laser axis, enabling a clear separation of hard interactions from soft light. Consequently, it becomes feasible to systematically factor contributions to scattering amplitudes into hard functions that describe short-distance dynamics and semi-hard functions that encode information about the emission of soft gluons.",
        "ori-fast-z-score": 0.5129891760425771,
        "water-fast-z-score": 6.874054958970533,
        "rewrite-fast-z-score": 1.3199500146737049
    },
    {
        "original_text": "We present an alternative method to the usual Feynman path integral description for calculating the probability amplitudes in quantum walk models, based on the concept of scattering states and their associated S-matrix elements. We show that this new formalism allows us to obtain exact results for several interesting cases where standard methods fail or are not applicable. In particular we consider two different types of boundary conditions at one end of the chain (the origin) which lead to completely different behaviour of the system as time evolves. The first type is known as Dirichlet boundary condition, corresponding to reflecting particles back into the origin after they have left it once; while the second case corresponds to absorbing particles when they reach the origin. For both these cases we calculate exactly the evolution operator over all times t > 0 using our new method. Finally, by applying the inverse Fourier transform to the evolution operator we can recover the full probability distribution function of finding the walker at any position x along the chain at time t.",
        "watermark_text": "We introduce an additional method to the usual Feynman path integral description for determining the probability amplitudes in particle walk models , using on the idea of scattering states and their accompanying S - vector elements . We see that this new formalism allows us to obtain exact findings for numerous interesting cases where standard methods fail or are not applicable .In particular we define two different kinds of boundary conditions at one end of the chain ( the origin ) which lead to totally distinct behaviour of the system as time evolves . The first sort is known as Dirichlet boundary relation , corresponding to reflecting molecules back into the origin after they have left it once ; while the second case corresponds to absorbing particles when they reach the origin .For both these circumstances we estimate exactly the evolution function over all times t > 0 using our new method . Finally , by using the inverse Fourier integral to the evolution function we can extract the full probability distribution function of finding the walker at any point x along the chain at time t .",
        "rewrite_text": "We present a novel approach to the traditional Feynman path integral framework for calculating probability amplitudes in particle walk models, based on the concept of scattering states and their associated S-matrix elements. This new formalism enables us to achieve precise results for a variety of intriguing scenarios where conventional methods may be insufficient or inapplicable. Specifically, we introduce two distinct types of boundary conditions at one end of the chain (the origin), which result in markedly different behaviors as time progresses. The first type, known as Dirichlet boundary conditions, involves reflecting particles back into the origin after they have exited. In contrast, the second type pertains to absorbing particles upon their arrival at the origin. For both situations, we precisely calculate the evolution function for all times \\( t > 0 \\) using our new methodology. Ultimately, through the application of the inverse Fourier transform to the evolution function, we can derive the complete probability distribution for locating the walker at any position \\( x \\) along the chain at time \\( t \\).",
        "ori-fast-z-score": 1.2649110640673518,
        "water-fast-z-score": 5.902918298980975,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "We present the results for the equation of state (EoS) in hot Quantum Chromodynamics (QCD). We use two different approaches to solve numerically the lattice QCD EoS at finite temperature, namely the Taylor expansion method and the integral method. The latter is based on an exact representation of the pressure as a function of energy density using Padé approximants. In addition we also study the dependence of the EoS on the number of flavors Nf . Finally, we compare our numerical results with those obtained by other authors within various theoretical frameworks. Our main conclusions are that both methods give consistent results which agree well with previous calculations performed in the literature. Moreover, it turns out that the inclusion of strange quarks has only minor effects on the thermodynamic quantities considered here. \n \n Keywords: Equation of State; Heavy Ion Collisions; Lattice QCD; Relativistic Hydrodynamics",
        "watermark_text": "We present the results for the equation of state ( EoS ) in hot Quantum Chromodynamics ( QCD ) . We use two different methods to solve numerically the lattice QCD EoS at finite temperature , namely the Taylor expansion method and the integral method .The latter is based on an precise representation of the pressure as a function of energy density using Padé approximants . In addition we also study the dependence of the EoS on the quantity of flavors Nf .Finally , we compare our numerical findings with those achieved by other researchers within various theoretical frameworks . Our main results are that both approaches give consistent conclusions which agree well with previous analyses performed in the literature .Moreover , it turns out that the introduction of odd quarks has only minor impacts on the thermodynamic quantities considered here . Keywords : Equation of State ; Heavy Ion Collisions ; Lattice QCD ; Relativistic Hydrodynamics",
        "rewrite_text": "We present our findings on the equation of state (EoS) in hot Quantum Chromodynamics (QCD). To numerically solve the lattice QCD EoS at finite temperatures, we employ two distinct methods: the Taylor expansion method and the integral method. The integral method utilizes an accurate representation of pressure as a function of energy density through Padé approximants. Additionally, we investigate how the EoS varies with the number of flavors, denoted as Nf. Ultimately, we compare our numerical results with those obtained by other researchers using different theoretical approaches. Our key findings indicate that both methodologies yield consistent results that align well with existing analyses in the literature. Furthermore, we find that the introduction of odd quarks has minimal effects on the thermodynamic quantities examined. Keywords: Equation of State; Heavy Ion Collisions; Lattice QCD; Relativistic Hydrodynamics.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 4.03585624040554,
        "rewrite-fast-z-score": 0.8017837257372732
    },
    {
        "original_text": "The authors present an evaluation framework to assess personal archiving strategies in the context of internet-based information, and apply it to two case studies. The first is based on a survey conducted among researchers at the University of Southampton; the second focuses on the activities of one individual researcher who has been collecting data about his research field over several years. Both cases are used to illustrate how different types of archives can be evaluated using this approach. This article was published as part of the Proceedings of the 1st International Conference on Digital Preservation (ICDP-1), held September 24-27, 2002 in New York City. It may be freely reproduced by anyone wishing to do so provided that proper credit is given to the author(s) and citation details are included. Copyright permission requests should be addressed to: RightsLink@copyright.gov. The authors present an evaluation framework which they use to assess personal archiving initiatives in the context of internet- based information. They then apply their method to two case studies - one focusing on a group of researchers at the University of Southamptonshire, UK, and another concentrating on the activities of one particular individual researcher.",
        "watermark_text": "The authors present an assessment framework to examine personal archiving strategies in the context of internet - based information , and application it to two case studies . The first is based on a survey conducted among researchers at the University of Southampton ; the second focuses on the actions of one individual scientist who has been collecting data about his scientific area over ten years .Both cases are applied to illustrate how various types of archives can be evaluated using this methodology . This section was publication as part of the Proceedings of the 1st International Conference on Digital Preservation ( ICDP - 1 ) , conducted September 24 - 27 , 2002 in New York City .It might be freely copied by anyone wishing to do so provided that proper credit is given to the writer ( s ) and citation notes are provided . Copyright permit applications should be addressed to : RightsLink @ copyright . gov .The authors present an assessment framework which they use to examine personal archiving activities in the context of internet - based information . They then use their method to two case studies - one focusing on a group of studies at the University of Southamptonshire , UK , and another concentrating on the actions of one particular individual scientist .",
        "rewrite_text": "The authors introduce an assessment framework designed to evaluate personal archiving strategies within the realm of internet-based information, applying it to two distinct case studies. The first case study involves a survey administered to researchers at the University of Southampton, while the second focuses on the activities of an individual scientist who has gathered data in his scientific field over a span of ten years. These cases demonstrate how different types of archives can be assessed using the proposed methodology. This section was published in the Proceedings of the 1st International Conference on Digital Preservation (ICDP-1), held from September 24 to 27, 2002, in New York City. It may be freely copied by anyone, provided that appropriate credit is given to the authors and citation notes are included. Requests for copyright permissions should be directed to RightsLink@copyright.gov. The authors also highlight the utilization of their framework to analyze personal archiving practices, particularly through case studies from the University of Southampton and the work of a specific individual scientist.",
        "ori-fast-z-score": 1.091089451179962,
        "water-fast-z-score": 8.292279828967711,
        "rewrite-fast-z-score": 1.1470786693528088
    },
    {
        "original_text": "We present an extension of the Continuous Time Random Walk (CTRW) formalism to describe non-Markovian processes in terms of a stochastic Liouville equation with memory kernel. The latter is obtained by solving the corresponding Fokker-Planck equation for the probability distribution function, which can be done analytically only under certain conditions on the form of the memory kernel. We show that this problem can be overcome by introducing a new concept of  stochastic propagator , which allows us to obtain exact solutions even when the memory kernel does not have any particular functional dependence on time or space variables. Finally we discuss how our results are related to previous works based on fractional calculus techniques. In recent years there has been growing interest in studying systems whose dynamics cannot be described within the standard framework of Markovian processes  1  . This is mainly due to the fact that many physical phenomena exhibit complex temporal behavior characterized by power law relaxation  2  , aging  3  , intermittency  4  , etc., all of them being typical features of nonMarkovian processes  5  .\nIn order to deal with these problems it was proposed  6  to use the so-called Continuous Time Random Walk (CWTRW) formalism  7, 8  . Within such a description one considers a system evolving continuously in time according to some deterministic laws but interrupted at random times by jumps between different states. These jumps occur as a consequence of interactions with other degrees of freedom, e.g. phonons  9  , electrons  10  , photons  11  , etc.. As shown in Ref.  12  , the CWTRW formalism provides a very general description of non-Markovian dynamics since it includes both discrete state models  13  and fractional diffusion equations  14  as special cases. However, despite its great flexibility, the application of the CWTRW formal-",
        "watermark_text": "We introduce an extension of the Continuous Time Random Walk ( CTRW ) formalism to explain non - Markovian systems in terms of a stochastic Liouville equation with memory kernel . The latter is found by solving the analogous Fokker - Planck formula for the probability distribution function , which can be performed analytically only under certain conditions on the form of the storage kernel .We see that this question can be overcome by using a new notion of stochastic propagator , which allows us to obtain exact solutions even when the memory kernel does not have any specific functional dependence on time or space factors . Finally we explain how our findings are related to previous works based on fractional calculus techniques .In recent years there has been growing interest in investigating systems whose dynamics cannot be described within the standard structure of Markovian mechanisms 1 . This is mainly owing to the fact that several physical phenomena experience complex temporal activity described by power law contraction 2 , aging 3 , intermittency 4 , etc . , all of them being normal features of nonMarkovian mechanisms 5 .In try to deal with these problems it was suggested 6 to use the so - called Continuous Time Random Walk ( CWTRW ) formalism 7 , 8 . Within such a description one sees a system evolving continuously in time according to some deterministic laws but halted at random times by jumps between various states .These moves occur as a outcome of interactions with other degrees of freedom , e . g . phonons 9 , electrons 10 , photons 11 , etc . . As seen in Ref .12 , the CWTRW formalism gives a very general explanation of non - Markovian physics since it includes both discrete state models 13 and fractional diffusion expressions 14 as special cases . However , despite its great flexibility , the implementation of the CWTRW formal -",
        "rewrite_text": "We present an expansion of the Continuous Time Random Walk (CTRW) framework to elucidate non-Markovian systems through a stochastic Liouville equation that incorporates a memory kernel. This memory kernel is derived by solving the corresponding Fokker-Planck equation for the probability distribution function, which can be analytically handled only under specific conditions regarding the form of the storage kernel. We demonstrate that this limitation can be addressed by introducing a novel concept of stochastic propagator, enabling us to derive exact solutions even when the memory kernel lacks any explicit dependence on time or spatial variables. Additionally, we discuss how our results connect to prior studies that utilize fractional calculus techniques. Recent years have seen an increased focus on exploring systems whose dynamics do not conform to the conventional framework of Markovian processes. This interest is primarily driven by the observation that many physical phenomena exhibit complex temporal behaviors described by power law contraction, aging, intermittency, and other characteristics that are inherent to non-Markovian mechanisms. To tackle these challenges, the Continuous Time Random Walk (CTRW) formalism has been proposed. In this model, a system evolves continuously over time according to deterministic laws, interrupted at random intervals by transitions between different states. These transitions result from interactions with various degrees of freedom, such as phonons, electrons, and photons. As noted in previous work, the CTRW framework provides a broad explanation of non-Markovian physics, encompassing both discrete state models and fractional diffusion expressions as particular instances. However, despite its significant versatility, the application of the CTRW formalism...",
        "ori-fast-z-score": -1.5787044347526527,
        "water-fast-z-score": 6.122759914971185,
        "rewrite-fast-z-score": 0.1796053020267749
    },
    {
        "original_text": "We study the dynamics of hard-core bosons on an optical lattice with random disorder and nearest-neighbor hopping, using exact diagonalization techniques. We find that there is a crossover between two different regimes as we increase the strength of disorder. In one regime (weak disorder), the system shows Anderson localization behavior; while in another regime (strong disorder) it exhibits Bose glass behavior. The transition point depends strongly on the filling fraction of particles per site. For low fillings, this transition occurs at relatively small values of disorder strengths. However, for higher fillings, the transition to the Bose glass phase takes place only when the disorder becomes very strong. This suggests that the presence of interactions can significantly affect the nature of the ground state of the system even if they are weak compared to other energy scales such as the bandwidth or the disorder strength. \n \n Introduction \n \n Disorder plays an important role in determining many properties of condensed matter systems. It has been shown recently that disorder can lead to interesting phenomena like quantum Hall effect  1  , metal-insulator transitions  2  , and superconductivity  3  . One of the most studied models which incorporates both disorder and interaction effects is the so-called Anderson model  4  . In its simplest form, this model describes non-interacting electrons moving through a disordered medium. Although the original formulation was restricted to electronic degrees of freedom, it has also been extended to describe various physical situations involving interacting particles  5  -  8  .\n \nIn recent years, ultracold atoms have emerged as promising candidates for simulating complex quantum mechanical problems  9  -  11  . These experiments provide us with unprecedented control over all relevant parameters of the problem under consideration  12  -  14  . Moreover, these systems allow us to explore new physics beyond what is possible in conventional solid-state materials  15  -  17  . Ultracold atomic gases trapped in optical lattices offer unique opportunities to investigate the interplay between disorder and interactions  18  -  20  . Recently, several experimental groups  21  -  23  have observed signatures of Anderson localization  24  in cold atom systems by studying the transport properties of the gas across the lattice.",
        "watermark_text": "We research the dynamics of hard - core bosons on an optical lattice with random disorder and nearest - neighbor hopping , using exact diagonalization techniques . We see that there is a crossover between two different regimes as we increase the strength of disorder .In one regime ( weak disorder ) , the system displays Anderson localization behavior ; while in another regime ( strong disorder ) it displays Bose glass behavior . The transition point varies strongly on the filling portion of molecules per site .For low fillings , this shift occurs at fairly little values of disorder strengths . However , for greater fillings , the shift to the Bose glass phase takes place only when the disorder becomes very strong .This implies that the presence of interactions can significantly affect the nature of the ground state of the system especially if they are weak compared to other energy scales such as the bandwidth or the disorder strength . Introduction Disorder plays an important role in establishing many properties of condensed matter structures .It has been shown lately that disturbance can lead to unusual phenomena like quantum Hall impact 1 , metal - insulator transitions 2 , and superconductivity 3 . One of the most studied models which includes both disorder and interaction influences is the so - called Anderson model 4 .In its simplest version , this description assumes non - interacting electrons moved through a disordered material . Although the first formulation was confined to electronic degrees of liberty , it has additionally been extended to explain different mechanical problems concerning interacting molecules 5 - 8 .In recent years , ultracold atoms have developed as hopeful candidates for simulating complex quantum mechanical problems 9 - 11 . These studies provide us with tremendous control over all relevant variables of the issue under consideration 12 - 14 .Moreover , these systems allow us to examine novel physics beyond what is allowed in standard solid - state materials 15 - 17 . Ultracold atomic atoms trapped in laser lattices provides unique possibilities to examine the interplay between disorder and interactions 18 - 20 .Recently , various experimental groups 21 - 23 have discovered signatures of Anderson localization 24 in cold atom systems by examining the travel properties of the gas across the lattice .",
        "rewrite_text": "We investigate the behavior of hard-core bosons on an optical lattice characterized by random disorder and nearest-neighbor hopping, utilizing exact diagonalization methods. Our findings reveal a crossover between two distinct regimes as the strength of disorder increases. In the weak disorder regime, the system exhibits Anderson localization, whereas in the strong disorder regime, it demonstrates characteristics of a Bose glass. Notably, the transition point is highly dependent on the filling fraction of molecules per site. At low fillings, the transition occurs at relatively low disorder strengths, whereas for higher fillings, the shift to the Bose glass phase only occurs when disorder reaches significant levels. This suggests that interactions can considerably influence the system's ground state, particularly when they are weak compared to other energy scales like bandwidth or disorder strength.\n\n**Introduction**  \nDisorder plays a crucial role in determining various properties of condensed matter systems. Recent studies have shown that disorder can lead to fascinating phenomena, such as the quantum Hall effect, metal-insulator transitions, and superconductivity. One of the most extensively studied models that incorporates both disorder and interaction effects is the Anderson model. In its simplest form, this model considers non-interacting electrons moving through a disordered medium. Although originally focused on electronic systems, it has been adapted to address various physical problems involving interacting particles. In recent years, ultracold atoms have emerged as promising candidates for simulating complex quantum mechanical phenomena. These systems provide exceptional control over all relevant parameters, allowing us to explore novel physics that is often inaccessible in conventional solid-state materials. Ultracold atoms trapped in optical lattices present unique opportunities to study the interplay between disorder and interactions. Recently, various experimental groups have reported evidence of Anderson localization in cold atom systems by analyzing the transport properties of gases across the lattice.",
        "ori-fast-z-score": -0.8315218406202999,
        "water-fast-z-score": 7.140637266026874,
        "rewrite-fast-z-score": 0.3086066999241838
    },
    {
        "original_text": "We present an analysis of archival XMM-Newton data for the Seyfert 1 galaxy, Mkr841 (NGC 4151). We find that the soft excess emission is well described by a blackbody component with kT = 0.16 keV and luminosity LBB ~ 1043 erg s-1. The hard X-ray spectrum can be fitted either by a power law or Compton reflection model. In both cases we detect strong relativistic Fe Kα lines at 6.4-6.7 keV which are broadened to FWHM ~ 1000 km/sec. These results suggest that there may exist two distinct regions where the accretion disk interacts with the central supermassive black hole. One region produces the soft excess via thermal reprocessing while another one gives rise to the hard X-ray emission through non-thermal processes such as inverse Compton scattering and/or Compton reflection.",
        "watermark_text": "We present an assessment of archival XMM - Newton data for the Seyfert 1 galaxy , Mkr841 ( NGC 4151 ) . We see that the soft excess emission is well described by a blackbody element with kT = 0 . 16 keV and luminosity LBB ~ 1043 erg s - 1 .The hard X - ray signal can be fit either by a power law or Compton absorption theory . In both cases we find strong relativistic Fe Kα bands at 6 . 4 - 6 . 7 keV which are broadened to FWHM ~ 1000 kilometers / sec .These data suggest that there may arise two separate areas where the accretion plasma interacts with the main supermassive black hole . One region delivers the deep excess via thermal reprocessing while another one offers rise to the hard X - ray radiation through non - thermal factors such as inverse Compton absorption and / or Compton absorption .",
        "rewrite_text": "We provide an analysis of archival XMM-Newton data for the Seyfert 1 galaxy Mkn 841 (NGC 4151). Our findings indicate that the soft excess emission can be accurately characterized by a blackbody component with a temperature of kT = 0.16 keV and a luminosity of LBB ~ 10^43 erg s^-1. The hard X-ray emission can be modeled using either a power law or Compton absorption theory. In both scenarios, we observe prominent relativistic Fe Kα lines in the range of 6.4 - 6.7 keV, which are broadened to a full width at half maximum (FWHM) of approximately 1000 kilometers/second. These observations suggest the presence of two distinct regions where the accretion plasma interacts with the central supermassive black hole. One region provides the soft excess through thermal reprocessing, while the other contributes to the hard X-ray emission via non-thermal processes such as inverse Compton scattering and/or Compton attenuation.",
        "ori-fast-z-score": -1.8203641092364127,
        "water-fast-z-score": 4.808326112068523,
        "rewrite-fast-z-score": -1.4569855927715483
    },
    {
        "original_text": "Epitaxial growth is the process by which atoms are deposited on top of an existing crystal structure, forming new layers that grow in registry with each other.  Epitaxy can be used to produce thin films and superlattices for electronic devices.   Graphene epitaxy refers to the deposition of carbon atoms onto a substrate such as silicon carbide (SiC) or gallium nitride (GaN).   The resulting material has many properties similar to those of single-layered graphene but also exhibits some differences due to interactions between the different atomic planes.   ... Graphene epitaxy is the deposition of carbon atoms on a substrate such as SiC or GaN. This results in a material with many properties similar to those found in single layer graphene, but there are also important differences arising from interlayer interactions. In this review we discuss recent progress towards understanding these effects using scanning probe microscopy techniques including STM, AFM and Raman spectroscopy.",
        "watermark_text": "Epitaxial growth is the process by which elements are deposited on top of an established crystal composition , forming fresh layers that grow in registry with each other . Epitaxy can be used to produce thin films and superlattices for electronic systems .Graphene epitaxy refers to the precipitation of carbon atoms onto a substrate such as silicon carbide ( SiC ) or gallium nitride ( GaN ) . The produced material has numerous characteristics similar to those of double - layered graphene but also exhibits some variations owing to interactions between the different atomic planes .. . . Graphene epitaxy is the precipitation of carbon atoms on a substrate such as SiC or GaN . This results in a material with many properties similar to those shown in single surface graphene , but there are also important differences arising from interlayer bonding .In this review we discuss latest advances towards studying these phenomena using scan probe microscopy methods notably STM , AFM and Raman spectroscopy .",
        "rewrite_text": "Epitaxial growth is the process where elements are deposited onto an existing crystal structure, creating new layers that align precisely with one another. This technique is utilized for the fabrication of thin films and superlattices in electronic applications. Specifically, graphene epitaxy involves the deposition of carbon atoms onto substrates like silicon carbide (SiC) or gallium nitride (GaN). The resulting material shares many properties with double-layered graphene but also shows distinct variations due to the interactions between different atomic layers. This review explores the latest advancements in understanding these phenomena through scanning probe microscopy techniques, including scanning tunneling microscopy (STM), atomic force microscopy (AFM), and Raman spectroscopy.",
        "ori-fast-z-score": 1.5650160901149996,
        "water-fast-z-score": 6.861993625888845,
        "rewrite-fast-z-score": 0.9801960588196068
    },
    {
        "original_text": "We present an extension to the classical lubrication theory that accounts for shear thinning effects on both the fluid viscosity and the friction coefficient, which are assumed to be functions of local pressure only.  The resulting model is applied to study the effect of non-Newtonian rheology on the elastohydrodynamic (EHD) behavior of a circular contact between two elastic solids with different surface roughnesses under pure rolling conditions.  We show that the presence of shear thinning leads to significant changes in the predicted load-carrying capacity as well as the distribution of the normal stress across the contact area compared to those obtained using Newtonian models.  In particular, we find that the maximum value of the dimensionless pressure increases significantly when the fluids exhibit strong shear thinning characteristics.  Moreover, our results indicate that the inclusion of shear thinning effects can lead to substantial reductions in the magnitude of the dimensionless tangential stresses at the centerline of the contact region.  Finally, it should be noted that the proposed theoretical framework may also be used to investigate other important phenomena such as thermal effects or mixed lubrication regimes.",
        "watermark_text": "We introduce an extension to the classical lubrication theory that accounts for shear thinning effects on both the liquid viscosity and the tension factor , which are assumed to be functions of local pressure only . The resulting theory is applied to study the impact of non - Newtonian rheology on the elastohydrodynamic ( EHD ) behavior of a circular contact between two elastic solids with varying surface roughnesses under pure sliding conditions .We see that the presence of shear thinning leads to significant improvements in the expected load - holding capacity as well as the spread of the standard pressure across the contact area compared to those achieved using Newtonian methods . In particular , we find that the maximum value of the dimensionless tension increases substantially when the fluids show strong shear thinning characteristics .Moreover , our findings show that the introduction of shear thinning effects can lead to substantial reductions in the severity of the dimensionless tangential stresses at the centerline of the contact region . Finally , it should be mentioned that the suggested theoretical framework would also be used to examine other vital effects such as heat effects or mixed lubrication regimes .",
        "rewrite_text": "We present an extension of classical lubrication theory that incorporates shear thinning effects on both liquid viscosity and the tension factor, treating them as functions of local pressure alone. This enhanced theory is applied to explore the influence of non-Newtonian rheology on the elastohydrodynamic (EHD) behavior of a circular contact between two elastic solids with varying surface roughnesses, particularly under pure sliding conditions. Our analysis reveals that shear thinning significantly enhances the expected load-holding capacity and influences the distribution of standard pressure across the contact area, compared to results obtained using Newtonian models. Notably, we observe a considerable increase in the maximum dimensionless tension value when dealing with fluids exhibiting strong shear thinning characteristics. Additionally, our results indicate that incorporating shear thinning effects can greatly reduce the magnitude of dimensionless tangential stresses at the centerline of the contact region. Finally, it is worth noting that this theoretical framework can also be utilized to investigate other important phenomena, including thermal effects and mixed lubrication regimes.",
        "ori-fast-z-score": -0.6469966392206304,
        "water-fast-z-score": 6.539886462510548,
        "rewrite-fast-z-score": 1.3937462952891333
    },
    {
        "original_text": "We study the propagation of surface waves in a layered medium with different signs of nonlinear coefficients and show that, under certain conditions, localized solutions exist which are similar to those known as gap solitons in periodic structures. The existence domain for these solutions is determined by solving an eigenvalue problem numerically. We also present results on the stability properties of such solutions against small perturbations. Surface wave localization can be observed experimentally using optical waveguide arrays or photonic crystals. In this work we consider the case when two layers have opposite signs of nonlinearities (e.g., one positive and another negative). This situation occurs naturally if the material parameters change sign across some interface between media. For example, it may happen near the boundary between materials with normal dispersion and anomalous dispersion. \n \n We demonstrate that there exists a class of localized solutions which resemble gap solitons in periodic systems. These solutions appear due to the interplay between linear and nonlinear effects. They exist only within a finite range of frequencies and decay exponentially away from their center point. Their amplitude depends strongly on the ratio of the amplitudes of the incident and reflected waves.",
        "watermark_text": "We test the propagation of surface waves in a layered medium with various signs of nonlinear coefficients and find that , under certain conditions , confined solutions arise which are comparable to those known as gap solitons in periodic complexes . The existence domain for these solutions is chosen by treating an eigenvalue problem numerically .We additionally report findings on the stability properties of such solutions against small perturbations . Surface wave localization can be viewed experimentally employing optical waveguide arrays or photonic clusters .In this research we imagine the case when two levels have different signs of nonlinearities ( e . g . , one positive and another negative ) . This condition occurs automatically if the material variables alter sign across some interface between media .For instance , it could happen near the boundary between materials with normal dispersion and anomalous dispersion . We suggest that there exists a class of localized solutions which mimic gap solitons in periodic systems .These solutions emerge due to the interplay between linear and nonlinear effects . They arise only within a finite range of frequencies and decay exponentially farther from their center position .Their amplitude depends strongly on the ratio of the amplitudes of the incident and reflected waves .",
        "rewrite_text": "We investigate the propagation of surface waves in a layered medium characterized by various signs of nonlinear coefficients. Our findings reveal that under specific conditions, confined solutions emerge that are akin to those known as gap solitons found in periodic structures. To determine the existence domain of these solutions, we approach the problem numerically by solving an eigenvalue problem. Furthermore, we examine the stability of these solutions against minor disturbances. Experimentally, surface wave localization can be observed using optical waveguide arrays or photonic clusters. In this study, we consider the scenario where two levels exhibit differing signs of nonlinearity (for example, one positive and the other negative). This situation naturally arises when the material properties change sign at an interface between two media, such as at the boundary between materials with normal and anomalous dispersion. We propose that a particular class of localized solutions exists that echoes the behavior of gap solitons in periodic systems. These solutions result from the interaction of linear and nonlinear effects and are found only within a limited frequency range, decaying exponentially with distance from their center. Their amplitude is significantly influenced by the ratio of the amplitudes of the incident and reflected waves.",
        "ori-fast-z-score": -1.0425720702853738,
        "water-fast-z-score": 4.79583152331272,
        "rewrite-fast-z-score": 0.7258661863112977
    },
    {
        "original_text": "We report the discovery and characterization of 33 new nearby white dwarf systems, including eight with trigonometric parallaxes measured by Gaia Data Release 2 (DR2). The sample includes six previously known binaries that were not included in DR2 because they are too faint for Gaia to resolve their components. We also present an analysis of the mass distribution of these newly discovered white dwarfs based on their photometric distances. This is the first time such a study has been performed using Gaia data alone. Our results show good agreement between our observed mass function and theoretical predictions. These findings demonstrate how Gaia can be used as a powerful tool to explore the local stellar population. Keywords: White dwarf - Galaxy - Parallax - Mass function - Gaia - Photometry - Binaries - Trigonometry - Distance scale - Astrometry - Stellar evolution - Galactic structure - Nearby stars",
        "watermark_text": "We report the discovery and identification of 33 new nearby white dwarf systems , including eight with trigonometric parallaxes measured by Gaia Data Release 2 ( DR2 ) . The sample comprises six formerly identified binaries that were not added in DR2 because they are too faint for Gaia to identify their components .We additionally offer an assessment of the mass distribution of these newly discovered white dwarfs based on their photometric ranges . This is the first time such a survey has been performed using Gaia data alone .Our results show good agreement between our measured mass behavior and theoretical estimates . These studies demonstrate how Gaia can be used as a powerful tool to examine the local stars community .Keywords : White dwarf - Galaxy - Parallax - Mass function - Gaia - Photometry - Binaries - Trigonometry - Distance scale - Astrometry - Stellar evolution - Galactic structure - Nearby stars",
        "rewrite_text": "We announce the discovery and classification of 33 new nearby white dwarf systems, including eight for which trigonometric parallaxes have been measured using Gaia Data Release 2 (DR2). Our sample also includes six previously identified binaries that were not included in DR2 due to their faintness, which made it difficult for Gaia to resolve their components. Additionally, we provide an analysis of the mass distribution of these newly identified white dwarfs based on their photometric data. This represents the first survey conducted solely with Gaia data. Our findings reveal a strong correlation between our measured mass characteristics and theoretical predictions. These studies highlight Gaia's capability as a powerful instrument for exploring the local stellar community. Keywords: White dwarf - Galaxy - Parallax - Mass function - Gaia - Photometry - Binaries - Trigonometry - Distance scale - Astrometry - Stellar evolution - Galactic structure - Nearby stars.",
        "ori-fast-z-score": 1.8073922282301278,
        "water-fast-z-score": 6.454972243679028,
        "rewrite-fast-z-score": 2.3937749957251055
    },
    {
        "original_text": "We study the Wightman functions and vacuum densities on a Z_2-symmetric, thick brane embedded in an anti-de Sitter (AdS) space-time with one extra dimension. We find that there are two types of solutions to the corresponding equations depending on whether or not the bulk mass is zero. In both cases we show how these quantities can be expressed as sums over modified Bessel functions. The results obtained here may have applications in quantum field theory at finite temperature and/or density. PACS: 11.10.Kk, 12.20.Ds, 98.80.Cq Keywords: Vacuum expectation value, Anti-de Sitter space time, Thick brane, Modified Bessel function. 1 Introduction An interesting feature of string theories is their ability to incorporate gravity into the fundamental description of nature. This has led to renewed interest in studying gravitational backgrounds which admit supersymmetry  1  . One such class of spacetimes is given by the so-called warped product spaces  2  , where the metric takes the form ds2 = e2A(y)(ημνdxμ dxν + dy 2 ),\n(1)\nwhere y denotes the coordinate along the extra dimension, A(y) is called the warp factor and ημν is the Minkowski metric. For example, if we consider the five-dimensional case then this corresponds to the Randall-Sundrum model  3  .\nIn recent years it was shown  4  -  8  that the presence of a nontrivial warp factor leads to new features in the physics associated with fields propagating in the bulk. These include modifications to the standard dispersion relations  9  , spontaneous symmetry breaking  10  , fermion localization  11  , etc.. It turns out  12  that the effects due to the warp factor depend crucially upon its behaviour near the boundary of the extra dimension. If the warp factor vanishes sufficiently rapidly at infinity then all physical observables will be identical to those computed using ordinary flat-space techniques. However, if the warp factor does not vanish fast enough then some novel phenomena occur.",
        "watermark_text": "We explore the Wightman functions and vacuum densities on a Z _ 2 - symmetric , thick brane embedded in an anti - de Sitter ( AdS ) space - time with one extra dimension . We see that there are two forms of solutions to the associated equations depending on whether or not the liquid weight is zero .In both cases we give how these quantities can be shown as sums over modified Bessel functions . The results derived here may have applications in quantum field theory at finite cooling and / or density .PACS : 11 . 10 . Kk , 12 . 20 . Ds , 98 . 80 . Cq Keywords : Vacuum expectation point , Anti - de Sitter space time , Thick brane , Modified Bessel relation . 1 Introduction An interesting feature of string theories is their power to insert gravitational into the fundamental description of nature .This has led to renewed emphasis in investigating gravitational backgrounds which admit supersymmetry 1 . One such family of spacetimes is given by the so - called warped product spaces 2 , where the metric takes the form ds2 = e2A ( y ) ( ημνdxμ dxν + dy 2 ) , ( 1 ) where y denotes the coordinate along the extra dimension , A ( y ) is dubbed the warp factor and ημν is the Minkowski metric .For instance , if we treat the five - dimensional case then this corresponds to the Randall - Sundrum model 3 . In recent years it was shown 4 - 8 that the presence of a nontrivial warp factor leads to novel features in the physics associated with fields propagating in the bulk .These include changes to the standard dispersion relations 9 , spontaneous symmetry breaking 10 , fermion localization 11 , etc . . It turns out 12 that the effects due to the warp factor rely crucially upon its behaviour near the boundary of the extra dimension .If the warp factor vanishes sufficiently quickly at infinity then all physical observables will be identical to those computed using ordinary flat - space methods . However , if the warp factor does not vanish fast enough then some interesting phenomena arise .",
        "rewrite_text": "We investigate the Wightman functions and vacuum densities on a Z_2-symmetric, thick brane situated within an anti-de Sitter (AdS) spacetime that includes an extra dimension. We find two types of solutions to the related equations, based on whether the liquid weight is zero or not. In both scenarios, we demonstrate that these quantities can be expressed as sums of modified Bessel functions. The results obtained may have implications for quantum field theory in contexts of finite temperature and/or density. PACS: 11.10.Kk, 12.20.Ds, 98.80.Cq. \n\n**Keywords:** Vacuum expectation value, Anti-de Sitter spacetime, Thick brane, Modified Bessel relation. \n\n**1 Introduction** One intriguing aspect of string theories is their ability to incorporate gravitational elements into the fundamental framework of nature, leading to a heightened focus on investigating gravitational backgrounds that support supersymmetry. A notable category of such spacetimes is represented by warped product spaces, characterized by the metric ds² = e²A(y)(ηₘₙdxⁱdxⁱ + dy²), where y denotes the coordinate of the extra dimension, A(y) is the warp factor, and ηₘₙ is the Minkowski metric. For example, in five dimensions, this corresponds to the Randall-Sundrum model. Recent studies have shown that the existence of a nontrivial warp factor introduces intriguing features in the behavior of fields propagating in the bulk, including modifications to standard dispersion relations, spontaneous symmetry breaking, and fermion localization. The effects attributed to the warp factor are significantly dependent on its behavior near the boundary of the extra dimension. If the warp factor diminishes rapidly at infinity, all physical observables will equate to those computed using conventional flat-space techniques. Conversely, if the warp factor does not decrease sufficiently fast, various interesting phenomena emerge.",
        "ori-fast-z-score": 0.8980265101338746,
        "water-fast-z-score": 6.2215204792052825,
        "rewrite-fast-z-score": 0.09245003270420485
    },
    {
        "original_text": "We present dynamical models for the Milky Way halo that include an arbitrary number of spherical shells, each characterized by its own density and velocity dispersion profiles. The model is based on Jeans  theorem applied to the phase-space distribution function (DF) in order to obtain the DF as well as the projected surface mass density along any line-of-sight. We show how this approach can be used to fit observational data such as those obtained from kinematic tracers or gravitational lensing measurements. In particular we apply our method to two different sets of observations: 1) A sample of RR Lyrae stars observed towards the Galactic bulge; 2) Gravitational lensing measurements towards the Bullet Cluster. Our results are compared against previous works using similar techniques but assuming either constant anisotropies across all radii or simple power-law radial dependences. We find that these assumptions lead to significant biases when fitting the data.",
        "watermark_text": "We create dynamical models for the Milky Way halo that include an arbitrary number of spherical shells , each described by its own density and frequency dispersion patterns . The model is based on Jeans theorem applied to the phase - space distribution relation ( DF ) in order to obtain the DF as well as the projected surface weight concentration along any line - of - view .We see how this methodology can be used to fit observational data such as those acquired from kinematic tracers or gravitational lensing observations . In particular we apply our technique to two different sets of measurements : 1 ) A specimen of RR Lyrae stars observed towards the Galactic bulge ; 2 ) Gravitational lensing observations towards the Bullet Cluster .Our results are compared against prior studies use similar techniques but adopting either constant anisotropies across all radii or straightforward power - law longitudinal dependences . We see that these assumptions lead to significant biases when fitting the information .",
        "rewrite_text": "We develop dynamic models for the Milky Way halo that can incorporate an arbitrary number of spherical shells, each characterized by its unique density and velocity dispersion profiles. Our approach applies Jeans theorem to the phase-space distribution function (DF), allowing us to determine the DF as well as the projected surface density along any line of sight. This methodology proves useful for fitting observational data, such as that obtained from kinematic tracers or gravitational lensing studies. Specifically, we apply our technique to two distinct sets of observations: 1) RR Lyrae stars located in the Galactic bulge; and 2) gravitational lensing data from the Bullet Cluster. We compare our findings to earlier research employing similar methods, which either assumed constant anisotropies at all radii or applied simple power-law dependencies. Our analysis reveals that these assumptions can introduce significant biases when interpreting the data.",
        "ori-fast-z-score": -0.6201736729460423,
        "water-fast-z-score": 4.589285179800713,
        "rewrite-fast-z-score": -0.6201736729460423
    },
    {
        "original_text": "We present new near-infrared (NIR) observations and analysis of the young binary system UZ Tau E, which is composed of two T Tauri stars with masses ~0.8 M⊙ separated by 0. ′′ 4. The NIR light curves show periodic variations that are consistent with ellipsoidal modulation due to tidal distortion of each star s photosphere as it orbits its companion. We also find evidence for an additional component to this variability; we interpret these data as indicating that one or both components of the binary undergoes periodic episodes of enhanced mass loss at periastron passage. This interpretation is supported by our detection of excess emission in the K-band spectrum during periods when the photometric flux decreases. Our results suggest that the circumstellar disks around each member of the binary have been truncated by their mutual gravitational interaction. In addition, we detect significant changes in the shape of the Hα line profile over time scales of days to weeks. These changes can be explained if there exists a region of high density gas surrounding the binary orbiting on timescales similar to those observed in the NIR light curve.",
        "watermark_text": "We report new near - infrared ( NIR ) observations and investigation of the small binary system UZ Tau E , which is composed of two T Tauri stars with masses ~ 0 . 8 [UNK] connected by 0 . ′ ′ 4 .The NIR light curves show continuous shifts that are compatible with ellipsoidal modulation owing to tidal manipulation of each star s photosphere as it orbits its companion . We additionally find proof for an additional element to this variability ; we view these information as indicating that one or both components of the binary undergoes frequent bouts of enhanced mass loss at periastron passage .This interpretation is backed by our screening of excess emission in the K - band spectrum during moments when the photometric flux drops . Our results propose that the circumstellar disks around each member of the binary have been truncated by their mutual gravitational interaction .In addition , we perceive considerable changes in the shape of the Hα line profile over time ranges of weeks to weeks . These changes can be described if there exists a region of high density gas covering the binary orbiting on timescales similar to those observed in the NIR light curve .",
        "rewrite_text": "We present new near-infrared (NIR) observations and an analysis of the small binary system UZ Tau E, which consists of two T Tauri stars, each with a mass of approximately 0.8 solar masses, separated by 0.4 arcseconds. The NIR light curves reveal continuous variations consistent with ellipsoidal modulation, caused by tidal interactions that affect the photospheres of each star as they orbit one another. Furthermore, we have identified additional factors contributing to this variability, suggesting that one or both stars experience episodes of increased mass loss at periastron. This interpretation is supported by our detection of excess K-band emission during periods when the photometric flux decreases. Our findings indicate that the circumstellar disks around each star have been truncated due to their gravitational interplay. Additionally, we observe significant variations in the shape of the Hα line profile over intervals of weeks. These changes can be explained by the presence of a high-density gas region surrounding the binary, evolving on timescales similar to those observed in the NIR light curves.",
        "ori-fast-z-score": -0.32539568672798425,
        "water-fast-z-score": 5.378448739494827,
        "rewrite-fast-z-score": 0.4588314677411235
    },
    {
        "original_text": "We report on four new quasars at redshifts z > 6, found in the Canada France Hawaii Telescope Legacy Survey (CFHTLS). The objects were selected as part of an ongoing survey for high-z quasars using photometric data obtained with CFHT and Spitzer Space Telescope. We present their optical to near-infrared SEDs, which are well fitted by composite quasar templates. Their luminosities range between 1.5 x 10^14 erg s-1 cm-2 and 2.1 x 10^15 erg s-1 cm-2 . These results show that there is still room for discovering very luminous quasars beyond redshift six. They also provide further evidence that supermassive black holes grew rapidly during this early phase of galaxy formation. Four quasars have been discovered at redshifts greater than 6 in the Canada France Hawaii telescope legacy survey (CFHTLS) by combining deep infrared observations taken with the Spitzer space telescope with optical data collected with the Canada France Hawaii telescope.",
        "watermark_text": "We report on four newest quasars at redshifts z > 6 , detected in the Canada France Hawaii Telescope Legacy Survey ( CFHTLS ) . The bodies were chosen as part of an continuing survey for high - z quasars using photometric data acquired with CFHT and Spitzer Space Telescope .We present their optical to near - infrared SEDs , which are well fitted by composite quasar templates . Their luminosities range between 1 . 5 x 10 ^ 14 erg s - 1 cm - 2 and 2 . 1 x 10 ^ 15 erg s - 1 cm - 2 .These data demonstrate that there is still space for producing very luminous quasars beyond redshift six . They also suggest further evidence that supermassive black holes grew rapidly during this earliest stage of galaxy formation .Four quasars have been detected at redshifts greater than 6 in the Canada France Hawaii telescope heritage survey ( CFHTLS ) by combining dark infrared observations made with the Spitzer space telescope with imaging information collected with the Canada France Hawaii telescope .",
        "rewrite_text": "We present four newly discovered quasars with redshifts exceeding 6, identified in the Canada France Hawaii Telescope Legacy Survey (CFHTLS). These quasars were selected as part of an ongoing investigation into high-redshift quasars, utilizing photometric data obtained from both the CFHT and the Spitzer Space Telescope. We provide their optical to near-infrared spectral energy distributions (SEDs), which align well with composite quasar templates. Their luminosities range from 1.5 x 10^14 erg s^-1 cm^-2 to 2.1 x 10^15 erg s^-1 cm^-2. This data indicates that there is still potential for the formation of extremely luminous quasars beyond redshift six, offering further support for the idea that supermassive black holes experienced rapid growth during the early stages of galaxy formation. The detection of these four quasars at redshifts greater than 6 was achieved by integrating infrared observations from the Spitzer Space Telescope with imaging data from the CFHT.",
        "ori-fast-z-score": -0.2672612419124244,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an analysis of the effects that mergers may have on scaling relations between black holes (BHs), galaxies, and other gravitationally bound systems. We use cosmological simulations to study how BH mass is related to galaxy properties in different merger histories. Our results show that mergers can significantly affect these relationships by increasing scatter at fixed luminosity or stellar velocity dispersion. This effect is strongest for low-mass galaxies with high specific star formation rates. In addition, we find that mergers tend to increase the average BH-to-galaxy mass ratio as well as the fraction of active galactic nuclei (AGNs) among massive galaxies. These findings are consistent with observations of AGN host galaxies. Finally, our results suggest that the observed correlation between supermassive BH masses and bulge properties could be driven primarily by the fact that both grow during major mergers. Keywords: Galaxy evolution; Supermassive black hole",
        "watermark_text": "We present an assessment of the effects that mergers might have on scaling relations between black holes ( BHs ) , galaxies , and other gravitationally bound structures . We use cosmological simulations to study how BH mass is related to galaxy structures in different merger histories .Our results show that mergers can significantly affect these interactions by expanding scatter at fixed luminosity or stellar velocity dispersion . This phenomenon is greatest for low - density nuclei with high specific star formation rates .In addition , we find that mergers prefer to raise the average BH - to - star mass ratio as well as the fraction of active galactic nuclei ( AGNs ) among giant galaxies . These conclusions are consistent with observations of AGN host galaxies .Finally , our findings show that the seen correlation between supermassive BH masses and bulge properties might be motivated primarily by the fact that both expand during major mergers . Keywords : Galaxy evolve ; Supermassive black hole",
        "rewrite_text": "We evaluate the impact of mergers on the scaling relationships among black holes (BHs), galaxies, and other gravitationally bound structures. Utilizing cosmological simulations, we investigate the correlation between BH mass and galaxy structures across various merger histories. Our findings indicate that mergers can significantly influence these relationships by increasing the scatter at fixed luminosity or stellar velocity dispersion, with the most pronounced effects observed in low-density nuclei that exhibit high specific star formation rates. Additionally, we discover that mergers tend to elevate both the average BH-to-star mass ratio and the occurrence of active galactic nuclei (AGNs) in massive galaxies. These results align with the observed characteristics of AGN host galaxies. Ultimately, our study suggests that the observed correlation between the masses of supermassive BHs and the properties of bulges may largely arise from the simultaneous expansion of both during major mergers. Keywords: Galaxy evolution; Supermassive black hole.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 5.5778737935111105,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "We present the results of our study on bimodality in galaxies and active galactic nuclei (AGN). We find that there is no significant difference between the fraction of AGNs hosted by red or blue galaxies, but we do see an excess of AGNs with respect to normal galaxies at intermediate colors. This suggests that AGNs are not preferentially found in either red or blue galaxies, as previously thought; instead they appear to be more common among galaxies with intermediate color. The lack of correlation between galaxy color and AGN activity may indicate that AGNs play only a minor role in quenching star formation in massive galaxies. Alternatively, it could suggest that AGNs have different effects depending on their luminosity and/or accretion rate. In addition, we find that the majority of AGNs reside in galaxies with bulges, regardless of whether these galaxies are classified as early-type or late-type systems.",
        "watermark_text": "We present the conclusion of our research on bimodality in galaxies and active galactic nuclei ( AGN ) . We see that there is no major variation between the fraction of AGNs hosted by red or blue clusters , but we do saw an surplus of AGNs with regard to normal galaxies at intermediate colors .This implies that AGNs are not preferentially found in either blue or blue stars , as previously thought ; however they appear to be more common among clusters with intermediate color . The absence of correlation between galaxy color and AGN activity may indicate that AGNs serve only a minor importance in quenching star formation in massive galaxies .Alternatively , it could indicate that AGNs have different impacts depending on their luminosity and / or accretion rate . In addition , we find that the majority of AGNs occur in galaxies with bulges , regardless of whether these objects are classified as early - class or late - class systems .",
        "rewrite_text": "We present the findings of our research on bimodality in galaxies and active galactic nuclei (AGN). Our analysis reveals that there is little difference in the fraction of AGNs found in red versus blue clusters. However, we did observe an excess of AGNs among galaxies with intermediate colors compared to normal galaxies. This suggests that AGNs are not predominantly associated with either blue or red galaxies, as was previously believed; rather, they are more frequently encountered in clusters with intermediate colors. The lack of a correlation between galaxy color and AGN activity may imply that AGNs play a relatively minor role in inhibiting star formation in massive galaxies. Alternatively, it could suggest that AGNs influence star formation differently based on their luminosity and/or accretion rate. Furthermore, our findings indicate that most AGNs are situated in galaxies with bulges, irrespective of whether those galaxies are classified as early-type or late-type.",
        "ori-fast-z-score": 0.8551861104941365,
        "water-fast-z-score": 6.305926250944657,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Magnetic separation is an important tool in biomedical research and clinical diagnostics, but it has been limited to macroscopic devices that are not suitable for point-of-care applications. Here we report on continuous magnetophoresis-based blood cell sorting using microfluidics. We demonstrate efficient separation of red blood cells (RBCs) from plasma by applying a magnetic field gradient across a microchannel containing RBCs suspended in buffer solution. The results show that our method can be used as a simple yet effective approach for separating different types of blood cells with high purity and efficiency. This work may have significant implications towards developing portable diagnostic tools based on microscale blood processing technologies. Magnetic separation techniques play an important role in many fields including medicine, biotechnology, environmental science, food industry etc.,  1  . However, most existing methods require bulky equipment which makes them unsuitable for use outside laboratory settings  2  .\nRecently there has been growing interest in miniaturizing these systems into lab-on-a-chip platforms  3  , where various functionalities such as sample preparation  4  , chemical analysis  5  , drug delivery  6  , and bioassays  7  could be integrated onto one single chip. In particular, magnetic separators have attracted much attention due to their simplicity, low cost, portability, and compatibility with other microfabricated components  8  . For example, several groups have demonstrated magnetic separation of biological samples inside microchannels  9  -  11  or on planar surfaces  12  -  14  . Despite this progress, however, current approaches still suffer from some limitations. First, they typically rely on batch-wise operation mode  15  , which limits throughput and requires large volumes of input samples  16  . Second, the majority of reported designs only allow for separation between two distinct populations  17  , while more complex mixtures involving multiple species cannot be processed simultaneously  18  . Third, the fabrication process usually involves complicated multi-step procedures  19  , making it difficult to integrate additional functions  20  . Finally, most previous studies were performed under static conditions  21  , which limit the flexibility of device design  22  .",
        "watermark_text": "Magnetic isolation is an important tool in biomedical research and medical diagnostics , but it has been limited to macroscopic devices that are not suitable for point - of - care applications . Here we study on rapid magnetophoresis - based blood cell sorting using microfluidics .We suggest efficient removal of red blood cells ( RBCs ) from fluid by using a magnetic current gradient across a microchannel containing RBCs held in buffer solution . The results show that our technique can be used as a simple yet effective methods for dividing different kinds of blood tissue with high purity and efficiency .This research could have considerable consequences towards developing portable diagnostic methods using on microscale blood extraction technologies . Magnetic isolation machines play an important role in multiple fields including medicine , biotechnology , environmental science , nutrition industry etc . , 1 .However , most existing techniques require bulky machinery which makes them unsuitable for use outside laboratory settings 2 . Recently there has been growing interest in miniaturizing these systems into lab - on - a - chip platforms 3 , where various functionalities such as specimen processing 4 , chemical analysis 5 , pharmaceutical production 6 , and bioassays 7 could be merged onto one single chip .In particular , magnetic separators have garnered considerable scrutiny due to their simplicity , low cost , portability , and compatibility with other microfabricated parts 8 . For instance , various groups have demonstrated magnetic separation of biological samples inside microchannels 9 - 11 or on planar materials 12 - 14 .Despite this progress , however , current approaches still suffer from some restrictions . First , they generally rely on batch - wise operation mode 15 , which reduces throughput and requires large quantities of input samples 16 .Second , the majority of reported prototypes only require for isolation between two separate populations 17 , while more sophisticated mixtures featuring multiple taxa unable be processed concurrently 18 . Third , the fabrication process usually includes complicated multi - phase techniques 19 , making it difficult to connect extra functions 20 .Finally , most prior studies were performed under static conditions 21 , which reduce the flexibility of device configuration 22 .",
        "rewrite_text": "Magnetic isolation is a crucial technique in biomedical research and medical diagnostics; however, it has primarily been confined to large-scale devices that are unsuitable for point-of-care applications. In this study, we investigate rapid magnetophoresis-based sorting of blood cells using microfluidics. We propose an effective method for removing red blood cells (RBCs) from fluid by applying a magnetic current gradient across a microchannel containing RBCs suspended in a buffer solution. Our findings demonstrate that this technique can serve as a straightforward yet efficient method for separating various types of blood tissues with high purity and effectiveness. This research could significantly impact the development of portable diagnostic methods utilizing microscale blood extraction technologies. Magnetic isolation devices are essential across numerous fields, including medicine, biotechnology, environmental science, and the nutrition industry. However, most existing methods depend on bulky equipment, making them impractical for use outside laboratory environments. Recently, there has been a surge of interest in miniaturizing these systems into lab-on-a-chip platforms, which can integrate various functions such as specimen processing, chemical analysis, pharmaceutical production, and bioassays onto a single chip. In particular, magnetic separators have attracted considerable attention for their simplicity, low cost, portability, and compatibility with other microfabricated components. Several research groups have successfully demonstrated magnetic separation of biological samples within microchannels or on planar surfaces. Despite these advancements, current methods still face several limitations. First, they typically operate in batch mode, which lowers throughput and necessitates larger sample quantities. Second, most prototypes are designed for isolating just two distinct populations and cannot efficiently process more complex mixtures with multiple cell types concurrently. Third, the fabrication processes often involve complicated multi-phase techniques, hindering the integration of additional functionalities. Finally, most previous studies have been conducted under static conditions, which limits the flexibility of device configurations.",
        "ori-fast-z-score": 0.5261522196019802,
        "water-fast-z-score": 9.838699100999074,
        "rewrite-fast-z-score": 1.7075311565539322
    },
    {
        "original_text": "We present Spitzer Infrared Spectrograph (IRS) observations of the nearby, interacting galaxy pair Mrk 297. The system consists of two galaxies separated by ~3 kpc in projection; one is an elliptical galaxy with a bright nuclear point source, while the other has a Seyfert 2 nucleus surrounded by extended emission lines. We detect several molecular hydrogen transitions including H$_2$ S(0), S(1), S(2), S(3), S(4), S(5), S(6), S(7), S(8), S(9), S(10), S(11), S(12), S(13), S(14), S(15), S(16), S(17), S(18), S(19), S(20). These are detected over a wide range of spatial scales ranging from <100 pc to >500 pc.",
        "watermark_text": "We present Spitzer Infrared Spectrograph ( IRS ) observations of the nearby , interacting galaxy pair Mrk 297 . The system contains of two galaxies linked by ~ 3 kpc in projection ; one is an elliptical galaxy with a bright nuclear point origin , while the other has a Seyfert 2 nucleus accompanied by extended emitted lines .We detect many molecular hydrogen changes including H $ _ 2 $ S ( 0 ) , S ( 1 ) , S ( 2 ) , S ( 3 ) , S ( 4 ) , S ( 5 ) , S ( 6 ) , S ( 7 ) , S ( 8 ) , S ( 9 ) , S ( 10 ) , S ( 11 ) , S ( 12 ) , S ( 13 ) , S ( 14 ) , S ( 15 ) , S ( 16 ) , S ( 17 ) , S ( 18 ) , S ( 19 ) , S ( 20 ) . These are detected over a broad variety of spatial scales extending from < 100 pc to > 500 pc .",
        "rewrite_text": "We present observations of the nearby interacting galaxy pair Mrk 297 using the Spitzer Infrared Spectrograph (IRS). This system comprises two galaxies connected by approximately 3 kpc in projection: one is an elliptical galaxy featuring a bright nuclear point source, while the other hosts a Seyfert 2 nucleus with extended emission lines. We identify numerous molecular hydrogen transitions, including H₂ S(0) through S(20). These transitions are detected across a wide range of spatial scales, from less than 100 pc to more than 500 pc.",
        "ori-fast-z-score": 0.5222329678670935,
        "water-fast-z-score": 4.351941398892446,
        "rewrite-fast-z-score": 0.8219949365267865
    },
    {
        "original_text": "The Large Area Telescope (LAT) on the Fermi Gamma-ray Space Telescope will be launched in June 2008, with an expected launch date of August 7, 2007.  The LAT is designed to detect gamma rays between 20 MeV and 300 GeV over a large field-of-view (2 steradians), making it possible for the first time to study the entire sky at these energies every three hours. This talk will discuss how supernova remnants are likely to appear as sources of high-energy emission when observed by this new instrument. In particular I will focus on:  - How many remnants should we expect to see? - What types of remnants might show up? - What can we learn about particle acceleration mechanisms? - What can we do to prepare for the observations? - Other topics that may come up during discussion. The Large Area Telescope (L AT ) on the Fermi Gamma-ray Sp ace T elescope w ill b e la unched i n J une 2 008 , wi th an ex pected l aunch d ate o f A ugust 7 , 2 007 .",
        "watermark_text": "The Large Area Telescope ( LAT ) on the Fermi Gamma - ray Space Telescope will be launched in June 2008 , with an anticipated flight date of August 7 , 2007 . The LAT is designed to identify gamma radiation between 20 MeV and 300 GeV over a large field - of - view ( 2 steradians ) , making it able for the first time to study the entire sky at these frequencies every three years .This discussion will explore how supernova remnants are likely to appear as causes of high - energy emission when observed by this new instrument . In particular I will focus on : - How many ruins should we expect to see ?- What kinds of remnants may show up ? - What can we find about particle particle processes ?- What can we do to organize for the discoveries ? - Other topics that might come up during discussion .The Large Area Telescope ( L AT ) on the Fermi Gamma - ray Sp ace T elescope l ill b e la unched i n J une 2 008 , wi th an Ex pected w aunch d ate o g A ugust 7 , 2 007 .",
        "rewrite_text": "The Large Area Telescope (LAT) aboard the Fermi Gamma-ray Space Telescope is set to launch in June 2008, with a preliminary flight date of August 7, 2007. The LAT is engineered to detect gamma radiation in the range of 20 MeV to 300 GeV across a vast field of view (2 steradians), allowing it to observe the entire sky at these frequencies every three years for the first time. This discussion will delve into how supernova remnants are likely to be sources of high-energy emissions when monitored by this cutting-edge instrument. Specifically, I will concentrate on: - The expected number of remnants we may observe, - The types of remnants that might be detected, - Insights we can gain about particle interactions, - Our preparedness for upcoming discoveries, - And other related topics that may arise during our conversation.",
        "ori-fast-z-score": 1.5882027766319677,
        "water-fast-z-score": 6.893123494842633,
        "rewrite-fast-z-score": 3.265986323710904
    },
    {
        "original_text": "We report on spectropolarimetric observations of the M8+M9 binary star LHS 1070A,B (GJ 436) with ESPaDOnS at CFHT. The two stars are separated by only 0. ′′ 1 and have been known to be magnetically active for many years. We find that both stars show significant circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields. In addition we detect Stokes V signatures indicating net linear polarization across all observed spectral lines. This is likely caused by scattering processes within the stellar atmosphere. Using our new data set together with previously published photometric measurements we derive rotation periods of P A = 3.6 ± 0.1 days and P B = 4.2 ± 0.3 days for the primary and secondary component respectively. These values are significantly longer than those derived from previous studies which were based solely on photometry. Our results suggest that the rotation period of each individual component depends strongly on its effective temperature as well as its surface gravity.",
        "watermark_text": "We report on spectropolarimetric studies of the M8 + M9 binary star LHS 1070A , B ( GJ 436 ) with ESPaDOnS at CFHT . The two stars are apart by only 0 .′ ′ 1 and have been known to be magnetically active for thousands decades . We see that both stars show considerable circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields .In addition we perceive Stokes V signatures suggesting net linear polarization across all observed spectral lines . This is probably due by scattering mechanisms within the stars air .Using our new data set combined with previously reported photometric surveys we derive rotation periods of P A = 3 . 6 ± 0 . 1 months and P B = 4 . 2 ± 0 . 3 days for the primary and secondary component respectively . These measurements are greatly longer than those generated from previous analyses which were based primarily on photometry .Our results show that the rotation cycle of each individual component relies highly on its effective heat as well as its surface gravity .",
        "rewrite_text": "We present spectropolarimetric observations of the binary star system LHS 1070A, B (GJ 436), comprised of M8 and M9 stars, using the ESPaDOnS instrument at CFHT. These two stars are separated by only 0.1 arcseconds and have been recognized as magnetically active for several decades. Our findings reveal significant circularly polarized emission lines, which indicate Zeeman splitting resulting from their magnetic fields. Additionally, we observe Stokes V signatures that suggest net linear polarization across all the spectral lines examined. This linear polarization likely arises from scattering mechanisms within the stars' atmospheres. By integrating our new dataset with existing photometric surveys, we determine rotation periods of P A = 3.6 ± 0.1 months for the primary component and P B = 4.2 ± 0.3 days for the secondary component. These rotation periods are significantly longer than those derived from prior analyses, which were primarily based on photometric data. Our results indicate that the rotation cycles of each component are strongly influenced by their effective temperature and surface gravity.",
        "ori-fast-z-score": -1.3949716649258315,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": 0.917662935482247
    },
    {
        "original_text": "We present an analysis of ellipsoidal oscillations induced in main-sequence stars by their substellar companions, and discuss prospects for detecting such signals with the Kepler mission. We find that these effects are detectable over a wide range of orbital periods (from 1 to 1000 days) and companion masses (0.1-10 MJ). The amplitude of the signal is proportional to the square root of the mass ratio between the star and its companion; it can be as large as 0.1% of the total flux at optical wavelengths. For typical parameters expected among planet-hosting systems, we estimate that this effect should produce a peak-to-peak variation in brightness on the order of 10 mmag or less. This level of precision will require several months of continuous observations using the Kepler spacecraft s photometer. However, if detected, the presence of such a signal would provide strong evidence for the existence of a low-mass stellar or planetary companion orbiting the primary star. \n \n Keywords: Ellipsoidal variations, Planetary system",
        "watermark_text": "We present an assessment of ellipsoidal oscillations induced in major - sequence stars by their substellar companions , and consider prospects for detecting such signals with the Kepler expedition . We see that these phenomena are detectable over a broad variety of orbital periods ( from 1 to 1000 days ) and companion masses ( 0 . 1 - 10 MJ ) .The amplitude of the signal is proportional to the square root of the mass ratio between the star and its companion ; it can be as big as 0 . 1 % of the total flux at optical wavelengths . For common parameters predicted among planet - hosting systems , we estimate that this effect should produce a peak - to - peak change in intensity on the order of 10 mmag or smaller .This level of precision will take many months of simultaneous measurements using the Kepler satellite s photometer . However , if detected , the presence of such a signal might give strong evidence for the existence of a small - density stellar or planetary companion orbiting the primary star .Keywords : Ellipsoidal variations , Planetary system",
        "rewrite_text": "We provide an evaluation of ellipsoidal oscillations caused by substellar companions in main-sequence stars and examine the potential for detecting these signals during the Kepler mission. Our analysis indicates that these phenomena can be detected across a wide range of orbital periods (from 1 to 1000 days) and companion masses (0.1 to 10 MJ). The signal's amplitude is proportional to the square root of the mass ratio between the star and its companion, potentially reaching up to 0.1% of the total flux at optical wavelengths. For typical parameters found in planet-hosting systems, we estimate that this effect could lead to peak-to-peak intensity changes on the order of 10 mmag or smaller. Achieving this level of precision will require many months of simultaneous observations using the photometer on the Kepler satellite. However, should such a signal be detected, it could provide compelling evidence of a small-density stellar or planetary companion orbiting the primary star. Keywords: Ellipsoidal variations, Planetary system.",
        "ori-fast-z-score": -1.62746694241347,
        "water-fast-z-score": 3.9524197172898554,
        "rewrite-fast-z-score": -0.3418817293789138
    },
    {
        "original_text": "We study the dynamics of a probabilistic cellular automaton (PCA) with two species, prey and predators, which interact in an oscillatory way. The PCA is defined on a square lattice where each site can be occupied by at most one particle of either type. We show that for certain values of the parameters there are stable periodic solutions to this system. These results are obtained using a mean field approximation method. In particular we find that the periodicity depends only on the number of particles per unit area. This dependence agrees well with numerical simulations performed on finite lattices. Finally, we discuss how our model could be used as a simple description of population cycles observed in nature. Probabilistic cellular automata have been widely studied during recent years due to their potential applications in many fields such as physics  1  , biology  2  or computer science  3  . They consist of a set of cells arranged in some regular structure like a grid  4  whose state evolves according to local rules depending on its own state and those of its neighbors  5  .\nIn this work we consider a two-dimensional probabilistic cellular automaton  6  consisting of N sites located on a square lattice L = Z 2 . Each cell i ∈ L has four possible states denoted by 0, 1, 2 and 3 corresponding respectively to empty space, prey, predator and dead. At time t = 0 all sites are initialized randomly with probability p 0 = 1/4 of being vacant, p 1 = 1/2 of having a prey and p 2 = 1/4 of containing a predator. Then, the evolution rule consists of applying simultaneously the following transition probabilities between consecutive times t and t + 1:",
        "watermark_text": "We work the dynamics of a probabilistic cellular automaton ( PCA ) with two species , prey and predators , which interact in an oscillatory way . The PCA is characterized on a square lattice where each site can be occupied by at most one particle of either type .We see that for particular values of the variables there are stable periodic answers to this scheme . These conclusions are derived using a mean field approximation algorithm .In particular we find that the periodicity varies only on the quantity of particles per unit area . This dependence agrees well with numerical simulations conducted on finite lattices .Finally , we explain how our model could be used as a simple explanation of population trends studied in nature . Probabilistic cellular automata have been widely explored during recent months thanks to their potential applications in different fields such as science 1 , chemistry 2 or computer science 3 .They consist of a group of cells located in some regular structure like a network 4 whose state evolves due to local rules depending on its own state and those of its relatives 5 . In this research we study a two - dimensional probabilistic cellular automaton 6 consisting of N sites located on a square lattice L = Z 2 .Each cell i ∈ L has four possible states denoted by 0 , 1 , 2 and 3 relating respectively to empty space , prey , predator and dead . At time t = 0 all locations are initialized randomly with probability p 0 = 1 / 4 of being empty , p 1 = 1 / 2 of having a predators and p 2 = 1 / 4 of featuring a hunter .Then , the evolution rule involves of using concurrently the following transfer probabilities between successive times t and t + 1 :",
        "rewrite_text": "We analyze the dynamics of a probabilistic cellular automaton (PCA) featuring two species: prey and predators, which interact in an oscillatory manner. The PCA operates on a square lattice where each site can hold at most one particle of either species. Our findings reveal that under specific parameter values, stable periodic solutions emerge within this framework. These results are obtained using a mean field approximation algorithm, and notably, we discover that the periodicity is influenced solely by the density of particles per unit area. This relationship aligns well with numerical simulations performed on finite lattices. Furthermore, we discuss how our model could serve as a simplified representation of population dynamics observed in nature. Recent months have seen extensive exploration of probabilistic cellular automata due to their potential applications across various fields, including science, chemistry, and computer science. These automata consist of a collection of cells arranged in a regular structure, such as a network, where the state of each cell evolves according to local rules based on its own state and the states of neighboring cells. In this study, we focus on a two-dimensional probabilistic cellular automaton that comprises N sites arranged on a square lattice L = Z². Each cell i ∈ L can exist in one of four states, represented by 0, 1, 2, and 3, corresponding to empty space, prey, predator, and dead, respectively. At time t = 0, the initial distribution of states is randomized, with a probability of p₀ = 1/4 for being empty, p₁ = 1/2 for having a predator, and p₂ = 1/4 for containing prey. The evolution of the system is governed by specific transfer probabilities that apply simultaneously between successive time steps t and t + 1:",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 6.15643073089126,
        "rewrite-fast-z-score": 1.84894690328381
    },
    {
        "original_text": "The solar cycle is an important phenomenon in space weather, which can have significant effects on Earth and its technological infrastructure. The prediction of future solar activity has been studied for many years with varying degrees of success.  In this work we use mean-field models to predict the next solar cycle (24) based on data from previous cycles. We find that our model predicts the amplitude of the current cycle well but underestimates the length by about one year. Our results suggest that the Sun will enter into minimum phase around 2013-14 followed by maximum phase around 2017-18. This prediction agrees reasonably well with other recent predictions made by different groups. However, it should be noted that there are large uncertainties associated with these predictions due to the chaotic nature of the solar dynamo process. Future observations may help reduce some of these uncertainties. Keywords: Space Weather, Solar Dynamo, Mean Field Modeling, Prediction",
        "watermark_text": "The solar cycle is an important feature in space weather , which can have considerable effects on Earth and its technological equipment . The prediction of potential solar activity has been studied for numerous years with varying degrees of progress .In this research we using mean - field estimates to predict the new sun cycle ( 24 ) based on evidence from previous periods . We see that our model predicts the frequency of the present cycle well but underestimates the length by about one year .Our results propose that the Sun will enter into minimum phase around 2013 - 14 followed by maximum phase around 2017 - 18 . This prediction agrees reasonably well with other recent predictions making by various groups .However , it should be mentioned that there are big uncertainties involved with these predictions attributed to the chaotic nature of the solar dynamo mechanism . Future discoveries may help decrease some of these uncertainties .Keywords: Space Weather, Solar Dynamo, Mean Field Modeling, Prediction",
        "rewrite_text": "The solar cycle plays a crucial role in space weather, impacting both Earth and its technological systems significantly. For many years, researchers have been working to predict potential solar activity, achieving varying levels of success. In this study, we utilize mean-field estimates to forecast the upcoming solar cycle (Cycle 24) by analyzing data from previous cycles. Our model accurately predicts the frequency of the current cycle, though it falls short in estimating its duration, underestimating it by approximately one year. We anticipate that the Sun will enter a minimum phase around 2013-2014, followed by a maximum phase around 2017-2018. This prediction aligns reasonably well with forecasts made by other research groups. However, it's important to note that significant uncertainties exist in these predictions due to the chaotic nature of the solar dynamo mechanism. Future advancements may help reduce some of these uncertainties. \n\nKeywords: Space Weather, Solar Dynamo, Mean Field Modeling, Prediction",
        "ori-fast-z-score": -2.121320343559643,
        "water-fast-z-score": 5.185449728701349,
        "rewrite-fast-z-score": 1.0681034923744679
    },
    {
        "original_text": "We present the results of our analysis on GRB 080916C, one of the most energetic bursts ever detected by Swift/BAT and Fermi/GBM. We find that this burst is consistent with being produced in an off-axis jet viewed at an angle θ ~ 60° to its axis. The observed light curve can be explained as emission from two components: (1) A bright component which peaks early during the prompt phase; it has a duration T90 = 1 s and a fluence Fγ = 2×10−6 erg cm−2. (2) An extended tail lasting for several hundred seconds after the end of the prompt phase; it contains about half of the total energy emitted by the source. Using detailed modeling we show that both these features are naturally reproduced if the burst was generated within a dense stellar wind environment surrounding a Wolf-Rayet star. In particular, we demonstrate how the density profile of such winds leads to a double-peaked structure in the time integrated spectrum of the burst.",
        "watermark_text": "We present the conclusion of our analysis on GRB 080916C , one of the most intense pulses ever observed by Swift / BAT and Fermi / GBM . We see that this burst is compatible with being produced in an off - axis jet viewed at an angle θ ~ 60° to its axis .The observed light spiral can be described as emission from two parts : ( 1 ) A bright component which peaks early during the prompt phase ; it has a duration T90 = 1 s and a fluence Fγ = 2×10−6 erg cm−2 . ( 2 ) An enhanced tail lasting for numerous hundred moments after the end of the prompt phase ; it contains about half of the total energy emitted by the source .Using detailed simulation we find that both these characteristics are naturally reconstructed if the explosion was generated within a dense stellar wind climate surrounding a Wolf - Rayet star . In particular , we prove how the density profile of such winds leads to a twin - peaked structure in the time integrated spectrum of the explosion .",
        "rewrite_text": "We present our analysis conclusions on GRB 080916C, which is among the most powerful bursts recorded by Swift/BAT and Fermi/GBM. Our findings suggest that this burst may have originated from an off-axis jet viewed from an angle of approximately θ ~ 60° to its axis. The observed light curve can be divided into two key components: (1) A bright peak occurring early in the prompt phase, characterized by a duration of T90 = 1 s and a fluence of Fγ = 2×10−6 erg cm−2, and (2) A prolonged tail lasting several hundred moments after the prompt phase, which accounts for about half of the total energy emitted by the source. Through detailed simulations, we demonstrate that both features can be explained if the explosion occurred within a dense stellar wind environment surrounding a Wolf-Rayet star. Specifically, we show how the density profile of such winds creates a twin-peaked structure in the time-integrated spectrum of the explosion.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 4.541868715470696,
        "rewrite-fast-z-score": 0.8551861104941365
    },
    {
        "original_text": "We present new near-infrared (NIR) observations for four Galactic bulge globular clusters: Terzan 5, Lilll1, UKS 1, and Terzan 4 obtained with the Near Infrared Camera and Multi-Object Spectrometer (NICMOS). The data were taken in two filters F160W and F222M during three orbits each at the Hubble Space Telescope (HST), as part of program GO-10775. We use these NIR images to derive accurate distances to all four clusters by comparing their observed magnitudes with those predicted using theoretical isochrones. Our results are consistent within uncertainties with previous distance estimates derived from optical photometric studies. For Terzan 5 we find d = 8.2 ± 0.3 kpc; for Liller 1: d = 7.7 ± 0.4 kpc; for UKS 1: d = 6.8 ± 0.5 kpc; and for Terzan 4: d = 9.0 ± 0.6 kpc.",
        "watermark_text": "We report new near - infrared ( NIR ) observations for four Galactic bulge globular complexes : Terzan 5 , Lilll1 , UKS 1 , and Terzan 4 obtained with the Near Infrared Camera and Multi - Object Spectrometer ( NICMOS ) . The data were took in two filters F160W and F222M during three orbits each at the Hubble Space Telescope ( HST ) , as part of series GO - 10775 .We use these NIR observations to derive exact distances to all four clusters by testing their observed magnitudes with those predicted using theoretical isochrones . Our results are compatible within uncertainties with previous diameter calculations derived from optical photometric studies .For Terzan 5 we find d = 8.2 ± 0.3 kpc; for Liller 1: d = 7.7 ± 0.4 kpc; for UKS 1: d = 6.8 ± 0.5 kpc; and for Terzan 4: d = 9.0 ± 0.6 kpc.",
        "rewrite_text": "We present new near-infrared (NIR) observations of four Galactic bulge globular clusters: Terzan 5, Liller 1, UKS 1, and Terzan 4, acquired using the Near Infrared Camera and Multi-Object Spectrometer (NICMOS). This data was collected across three orbits for each cluster at the Hubble Space Telescope (HST) in two filters, F160W and F222M, as part of program GO-10775. We utilize these NIR observations to determine precise distances to the clusters by comparing their observed magnitudes to those predicted by theoretical isochrones. Our findings align well within uncertainties with prior distance estimates obtained from optical photometric analyses. Specifically, we measure the distances as follows: for Terzan 5, d = 8.2 ± 0.3 kpc; for Liller 1, d = 7.7 ± 0.4 kpc; for UKS 1, d = 6.8 ± 0.5 kpc; and for Terzan 4, d = 9.0 ± 0.6 kpc.",
        "ori-fast-z-score": 1.6666666666666667,
        "water-fast-z-score": 4.666666666666667,
        "rewrite-fast-z-score": 2.0816659994661326
    },
    {
        "original_text": "The aim of this thesis is the study and development of computational methods for prolongation structures, which are used as tools in geometric analysis. The main focus lies on the construction of explicit formulas for the prolonged action of vector fields on tensor bundles over manifolds with symmetries. In particular we consider Lie groups acting by diffeomorphisms on Riemannian or pseudo-Riemannian manifolds. We present an algorithm that computes the prolonged action of a given vector field on any tensor bundle associated to such a manifold. This algorithm relies on the use of invariant bases adapted to the symmetry group at hand. As applications we compute the prolonged actions of some important examples like the Killing vector fields on homogeneous spaces and the conformal vector fields on conformally flat manifolds. Finally, we show how our results can be applied to construct new families of solutions to Einstein s equations. Keywords: Geometric Analysis, Manifold Symmetry Group",
        "watermark_text": "The goal of this dissertation is the development and evolution of computational procedures for prolongation structures , which are applied as tools in mathematical analysis . The main aim lies on the creation of explicit formulas for the prolonged operation of vector fields on tensor bundles over manifolds with symmetries .In particular we define Lie fields acted by diffeomorphisms on Riemannian or pseudo - Riemannian manifolds . We introduce an algorithm that computes the prolonged operation of a given tensor field on any tensor bundle corresponding to such a manifold .This method relies on the using of invariant bases adapted to the symmetry class at hand . As applications we compute the prolonged actions of some important examples like the Killing matrix fields on homogeneous spaces and the conformal vector spaces on conformally flat manifolds .Finally , we show how our findings can be applied to build modern classes of solutions to Einstein s equations . Keywords : Geometric Analysis , Manifold Symmetry Group",
        "rewrite_text": "The objective of this dissertation is to develop and advance computational techniques for prolongation structures, which serve as valuable tools in mathematical analysis. The primary focus is on deriving explicit formulas for the prolonged operation of vector fields on tensor bundles over manifolds exhibiting symmetries. Specifically, we define Lie fields influenced by diffeomorphisms on Riemannian and pseudo-Riemannian manifolds. Additionally, we introduce an algorithm designed to compute the prolonged operation of a given tensor field on any tensor bundle associated with such manifolds. This method utilizes invariant bases tailored to the specific symmetry class involved. As applications, we compute the prolonged actions of significant examples, including Killing matrix fields on homogeneous spaces and conformal vector fields on conformally flat manifolds. Ultimately, we demonstrate how our results can be utilized to construct modern classes of solutions to Einstein's equations. Keywords: Geometric Analysis, Manifold Symmetry Group.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an analysis of the distribution of gas, stars and dust in two nearby edge-on spirals with prominent bars (NGC 1365 and NGC 1530). We use high-resolution observations obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems. The main results are as follows:  - In both cases we find that the molecular hydrogen is concentrated on the leading edges of the bar, while atomic hydrogen follows closely the stellar light.  - The star formation rate peaks at the ends of the bar where the density of molecular hydrogen increases significantly. This suggests that the gravitational torques induced by the bar can trigger the collapse of dense clouds into new generations of young stars.  - The infrared emission associated with polycyclic aromatic hydrocarbons shows a clear correlation between the location of this component and the regions of active star formation. - The comparison of our data with hydrodynamical simulations indicates that the observed structure of the ISM may be explained if the bar potential has been able to drive significant amounts of cold gas towards its inner Lindblad resonance.",
        "watermark_text": "We present an assessment of the distribution of gas , stars and dust in two distant edge - on spirals with prominent bars ( NGC 1365 and NGC 1530 ) . We use large - resolution measurements obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems .The main results are as follows : - In both cases we find that the molecular hydrogen is confined on the led corners of the bar , while nuclear hydrogen takes closely the stellar radiation . - The galaxy formation rate peaks at the ends of the bar where the density of molecular hydrogen rises considerably .This implies that the gravitational torques induced by the bar can cause the decay of dense clouds into new generations of young stars . - The infrared absorption associated with polycyclic aromatic hydrocarbons indicates a clear correlation between the location of this constituent and the regions of active star formation .- The comparison of our information with hydrodynamical simulations indicates that the known composition of the ISM may be described if the bar potential has been able to drive considerable portions of cold gas towards its internal Lindblad resonance .",
        "rewrite_text": "We present an analysis of the distribution of gas, stars, and dust in two distant edge-on spiral galaxies with prominent bars: NGC 1365 and NGC 1530. Utilizing high-resolution measurements from the Herschel Space Observatory, we investigate the physical conditions of the interstellar medium in these systems. Our key findings are as follows: \n- In both galaxies, molecular hydrogen is concentrated at the leading edges of the bar, while the nuclear hydrogen closely follows the stellar radiation.\n- The star formation rate peaks at the ends of the bar, where there is a significant increase in molecular hydrogen density. This suggests that the gravitational torques generated by the bar can help collapse dense clouds into new generations of young stars.\n- The infrared absorption linked to polycyclic aromatic hydrocarbons shows a strong correlation with areas of active star formation.\n- Comparing our results with hydrodynamic simulations indicates that the known characteristics of the interstellar medium can be explained if the bar's gravitational potential has effectively driven substantial amounts of cold gas toward its internal Lindblad resonance.",
        "ori-fast-z-score": 1.4924050144892729,
        "water-fast-z-score": 6.88998622004134,
        "rewrite-fast-z-score": 2.156655464068768
    },
    {
        "original_text": "The statistical behavior of domain systems is studied by using the concept of entropy and its associated quantities, such as information content and mutual information.  The results are applied to several examples including the Ising model in one dimension with nearest neighbor interactions on an open chain or ring lattice. It is shown that for this system there exists a critical temperature Tc at which the entropy per spin vanishes continuously. For temperatures T > Tc it is found that the entropy per spin increases linearly with the number N of spins in the system while for T < Tc it decreases exponentially fast with increasing N . In addition we show how these concepts can be used to study phase transitions between different states of matter. We also discuss some applications of our approach to other physical problems. PACS: 05.45.-a; 05.60.Fh; 05.70.Jc; 06.20.Hv; 62.25.Kx",
        "watermark_text": "The statistical behavior of domain systems is studied by using the idea of entropy and its attendant parameters , such as data content and mutual information . The results are applied to several examples including the Ising model in one dimension with nearest friend interactions on an open chain or ring lattice .It is seen that for this scheme there exists a critical temperature Tc at which the entropy per spin vanishes constantly . For temperatures T > Tc it is found that the entropy per spin increases linearly with the number N of spinning in the system while for T < Tc it reduces exponentially rapidly with expanding N .In addition we explain how these concepts can be used to study phase transitions between various states of matter . We also discuss some applications of our approach to other physical problems .PACS: 05.45.-a; 05.60.Fh; 05.70.Jc; 06.20.Hv; 62.25.Kx",
        "rewrite_text": "The statistical behavior of domain systems is analyzed through the concept of entropy and its associated parameters, including data content and mutual information. These findings are demonstrated through various examples, such as the one-dimensional Ising model with nearest-neighbor interactions on an open chain or a ring lattice. It is observed that, within this framework, there exists a critical temperature \\( T_c \\) at which the entropy per spin consistently approaches zero. For temperatures above \\( T_c \\), the entropy per spin is found to increase linearly with the number \\( N \\) of spins in the system, while for temperatures below \\( T_c \\), it decreases exponentially as \\( N \\) increases. Additionally, we illustrate how these concepts can be employed to investigate phase transitions between different states of matter. We also explore several applications of our methodology to other physical issues. PACS: 05.45.-a; 05.60.Fh; 05.70.Jc; 06.20.Hv; 62.25.Kx",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 3.8729833462074166,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new ultraviolet (UV) observations of star formation rates (SFRs) for galaxies within 10 Mpc using GALEX data and compare these to SFRs derived from optical emission lines, infrared luminosities, radio continuum fluxes, and UV-optical colors. We find that all methods agree well with each other when applied to normal star-forming galaxies but disagree significantly on low-luminosity dwarf galaxies where dust extinction is significant. The scatter between different estimators increases at lower luminosities due primarily to differences in how they treat dust extinction. In addition we show that there are systematic offsets among some of the estimators which can be explained by aperture effects or calibration uncertainties. Finally, we use our sample to examine the relationship between galaxy mass and specific star formation rate as measured by various techniques. Our results suggest that the most reliable estimates of SFR come from combining multiple indicators rather than relying solely on one method.",
        "watermark_text": "We create latest ultraviolet ( UV ) observations of galaxy formation rates ( SFRs ) for galaxies within 10 Mpc using GALEX data and compare these to SFRs generated from optical emission lines , infrared luminosities , television continuum fluxes , and UV - optical colors . We see that all techniques work better with each other when applied to normal star - creating stars but disagree significantly on small - luminosity dwarf stars where dust extinction is substantial .The scatter between various estimators increases at lower luminosities due primarily to differences in how they treat dust extinction . In addition we find that there are systematic offsets among some of the estimators which can be described by lens effects or calibration uncertainties .Finally , we utilize our sample to examine the relationship between galaxy mass and particular galaxy formation rate as measured by various methods . Our results show that the most accurate calculations of SFR come from combining multiple indicators rather than relying solely on one method .",
        "rewrite_text": "We conduct the latest ultraviolet (UV) observations of star formation rates (SFRs) for galaxies within 10 Mpc using GALEX data, comparing these results to SFRs derived from optical emission lines, infrared luminosities, television continuum fluxes, and UV-optical colors. Our analysis reveals that all methods yield more consistent results when applied to normal star-forming galaxies, but they show significant discrepancies for low-luminosity dwarf stars, where dust extinction plays a major role. The variation among different estimators increases at lower luminosities, primarily due to differing approaches to dust extinction. Additionally, we identify systematic offsets among some estimators, which may be attributed to lens effects or calibration uncertainties. Finally, we examine the correlation between galaxy mass and specific star formation rates as assessed by various methods. Our findings indicate that the most reliable SFR estimates emerge from integrating multiple indicators rather than relying on a single method alone.",
        "ori-fast-z-score": 1.7320508075688772,
        "water-fast-z-score": 6.653056282246291,
        "rewrite-fast-z-score": 1.4444444444444444
    },
    {
        "original_text": "We have investigated how different assumptions about the velocity distribution function (VDF) affect the shape of the observed line profile in the solar corona, using an analytical model for the VDF that includes both isotropic thermal motions and anisotropic nonthermal motions. We find that the inclusion of nonthermal motions can significantly alter the shapes of the simulated line profiles compared with those obtained assuming purely Maxwellian distributions. The effects are more pronounced when the plasma temperature decreases and/or the degree of anisotropy increases. \n \n In particular, we show that the presence of nonthermal motions leads to significant asymmetries between the red-and blueshifted wings of the line profiles. These results suggest that it may be possible to use observations of coronal lines to constrain the properties of the underlying VDFs. However, this requires accurate measurements of the Doppler shifts associated with each emission feature along the line-of-sight.",
        "watermark_text": "We have analyzed how various assumptions about the velocity distribution relation ( VDF ) impact the morphology of the seen line profile in the sun corona , using an analytical theory for the VDF that contains both isotropic thermal motions and anisotropic nonthermal movements . We see that the introduction of nonthermal movements can significantly change the shapes of the simulated line profiles compared with those achieved assuming solely Maxwellian distributions .The effects are more pronounced when the plasma pressure drops and / or the degree of anisotropy changes . In particular , we find that the presence of nonthermal movements leads to significant asymmetries between the red - and blueshifted wings of the line profiles .These data suggest that it could be possible to use observations of coronal lines to constrain the properties of the intrinsic VDFs . However , this demands accurate measurements of the Doppler cycles identified with each emission feature along the line - of - view .",
        "rewrite_text": "We have investigated how different assumptions regarding the velocity distribution function (VDF) influence the morphology of solar coronal line profiles. Our analysis employs an analytical theory for the VDF that accounts for both isotropic thermal motions and anisotropic nonthermal movements. Our findings indicate that the inclusion of nonthermal motions can significantly alter the shapes of the simulated line profiles compared to those derived from Maxwellian distributions alone. These effects become more pronounced when there is a decrease in plasma pressure or when the degree of anisotropy varies. Notably, we observe that nonthermal movements introduce substantial asymmetries between the red- and blueshifted wings of the line profiles. These results imply that observations of coronal lines could help constrain the characteristics of the intrinsic VDFs. Nevertheless, this requires precise measurements of the Doppler shifts associated with each emission feature along the line of sight.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 6.621221919717306,
        "rewrite-fast-z-score": 0.36650833306891567
    },
    {
        "original_text": "We study the orbital evolution and stability properties of oligarchic co-orbitals in the Solar System, i.e., bodies with masses comparable to that of Jupiter which are trapped on orbits close to those of Neptune or Uranus for billions of years.  We show how these objects can be identified by their long-term dynamical behavior as well as by their current positions relative to Neptune s orbit. The existence of such bodies is confirmed by numerical integrations over timescales up to 10 billion years using the symplectic N-body code SyMBA. In addition we find that there exist at least two other stable regions where oligarchs may reside. These results suggest that the Solar System contains several dozen oligarchic co-orbitals:  - At least four known trans-Neptunian objects (Pluto, Charon, Haumea, Makemake) have been found to exhibit this type of dynamics; - There exists another region around 30 AU containing three additional bodies (Sedna, 2000 CR 105 , 2003 SQ 317 ); - Finally, our simulations indicate that there might also be an additional group of oligarchs located between 50-60 AU.",
        "watermark_text": "We research the orbital evolution and stability properties of oligarchic co - orbitals in the Solar System , i . e . , bodies with masses similar to that of Jupiter which are locked on orbits close to those of Neptune or Uranus for billions of years . We see how these objects can be identified by their long - term dynamical behavior as well as by their current positions close to Neptune s orbit .The formation of such objects is predicted by numerical integrations over timescales up to 10 billion years employing the symplectic N - bodies code SyMBA . In addition we find that there exist at least two other stable parts where oligarchs might live .These data suggest that the Solar System includes several several oligarchic co - orbitals : - At least four known trans - Neptunian planets ( Pluto , Charon , Haumea , Makemake ) have been seen to contain this form of dynamics ; - There exists another region around 30 AU holding three extra bodies ( Sedna , 2000 CR 105 , 2003 SQ 317 ) ; - Finally , our simulations confirm that there might additionally be an additional family of oligarchs located between 50 - 60 AU .",
        "rewrite_text": "We investigate the orbital evolution and stability characteristics of oligarchic co-orbitals within the Solar System—specifically, celestial bodies with masses comparable to Jupiter that have remained locked in orbits near those of Neptune or Uranus for billions of years. Our findings reveal that these objects can be identified by their long-term dynamical behaviors as well as their current proximity to Neptune's orbit. The formation of such bodies is supported by numerical simulations conducted over timescales of up to 10 billion years using the symplectic N-body code SyMBA. Furthermore, we have identified at least two additional stable regions where oligarchic bodies may exist. The evidence suggests that the Solar System hosts multiple oligarchic co-orbitals: at least four known trans-Neptunian objects (Pluto, Charon, Haumea, and Makemake) display this type of dynamical behavior; another region around 30 AU contains three additional bodies (Sedna, 2000 CR105, and 2003 SQ317); and our simulations propose the possibility of another family of oligarchs situated between 50 and 60 AU.",
        "ori-fast-z-score": -1.3643820804812932,
        "water-fast-z-score": 5.0854241181575475,
        "rewrite-fast-z-score": -1.016001016001524
    },
    {
        "original_text": "We study the phase structure of a surface model defined by an energy functional that consists of two competing terms, one favoring smooth surfaces and another penalizing their curvature fluctuations. The latter is modeled as a harmonic term in the local mean curvature. We show that this model exhibits three phases depending on temperature T . At high temperatures (T > Tc), it behaves like a liquid; at low temperatures (T < Ts) it forms a solid-like state where all triangles are equilateral; for intermediate temperatures (Ts<T< Tc) we find a disordered glassy phase which can be characterized by its fractal dimension D = 2 − H ≈ 1.7 ± 0.1. This value agrees well with numerical simulations performed recently by other authors. \n \n In addition to these results, our analysis also provides evidence for a first-order transition between the ordered and disordered states. Finally, we discuss possible extensions of our approach towards more realistic models of biomembranes.",
        "watermark_text": "We explore the phase composition of a surface model characterized by an energy functional that composed of two different terms , one favoring smooth surfaces and another penalizing their curvature fluctuations . The last is modeled as a harmonic term in the local average curvature .We see that this model shows three stages depending on temperature T . At high temperatures ( T > Tc ) , it behaves like a liquid ; at low temperatures ( T < Ts ) it creates a solid - like state where all triangles are equilateral ; for intermediate temperatures ( Ts < T < Tc ) we find a disordered glassy phase which can be described by its fractal dimension D = 2 − H ≈ 1 . 7 ± 0 . 1 .This value agrees well with numerical simulations conducted recently by other researchers . In addition to these results , our analysis even offers evidence for a first - order conversion between the ordered and disordered states .Finally , we discuss possible extensions of our approach towards more realistic descriptions of biomembranes .",
        "rewrite_text": "We investigate the phase composition of a surface model defined by an energy functional consisting of two distinct components: one that promotes smooth surfaces and another that penalizes variations in curvature. The latter is represented as a harmonic term in the local average curvature. Our findings indicate that the model exhibits three distinct phases based on temperature (T). At high temperatures (T > Tc), the system behaves like a liquid; at low temperatures (T < Ts), it transitions into a solid-like state characterized by equilateral triangles. For intermediate temperatures (Ts < T < Tc), we observe a disordered glassy phase, which can be represented by a fractal dimension of D = 2 - H ≈ 1.7 ± 0.1. This value is consistent with recent numerical simulations conducted by other researchers. Furthermore, our analysis provides evidence for a first-order transition between the ordered and disordered phases. Finally, we explore potential extensions of our approach to create more realistic models of biomembranes.",
        "ori-fast-z-score": -0.24618298195866545,
        "water-fast-z-score": 4.031591663758072,
        "rewrite-fast-z-score": 0.3611575592573076
    },
    {
        "original_text": "We consider an antenna combining scheme in which each user is equipped with multiple antennas and transmits its data to one base station (BS) using space-time coding techniques. The BS uses maximum ratio combining (MRC), zero forcing (ZF), or minimum mean square error (MMSE) receivers, depending on whether it has perfect channel state information (CSI). We derive closed-form expressions for the ergodic capacity achieved by this system under Rayleigh fading channels. Our results show that ZF performs better than MMSE when there are more users than transmit antennas at the BS; otherwise, MMSE outperforms ZF. In addition, we find that the performance gap between these two schemes decreases as the number of receive antennas increases. \n \n Keywords: Antenna combining, downlink channel, MIMO systems, Rayleigh fading channels, space time codes",
        "watermark_text": "We consider an radar combining system in which each user is furnished with many antennas and transmits its data to one base station ( BS ) used space - time coding techniques . The BS using maximum ratio combining ( MRC ) , zero forcing ( ZF ) , or lowest mean square error ( MMSE ) receivers , depending on whether it has good network state information ( CSI ) .We derive closed - form expressions for the ergodic strength achieved by this scheme under Rayleigh fading filters . Our results show that ZF performs better than MMSE when there are more users than receive antennas at the BS ; otherwise , MMSE outperforms ZF .In addition , we find that the performance difference between these two systems decreases as the quantity of receive antennas increases . Keywords : Antenna combining , downlink channel , MIMO systems , Rayleigh fading stations , space time codes",
        "rewrite_text": "We examine a radar combining system where each user is equipped with multiple antennas that transmit data to a single base station (BS) employing space-time coding techniques. Depending on the availability of sufficient channel state information (CSI), the BS utilizes maximum ratio combining (MRC), zero forcing (ZF), or minimum mean square error (MMSE) receivers. We derive closed-form expressions for the ergodic capacity achieved with this setup under Rayleigh fading conditions. Our findings indicate that ZF outperforms MMSE when the number of users exceeds the number of receive antennas at the BS; however, MMSE is superior when there are more receive antennas available. Additionally, the performance gap between these two methods narrows as the number of receive antennas increases. \n\nKeywords: Antenna combining, downlink channel, MIMO systems, Rayleigh fading, space-time codes.",
        "ori-fast-z-score": 0.254000254000381,
        "water-fast-z-score": 4.572004572006858,
        "rewrite-fast-z-score": -0.7385489458759964
    },
    {
        "original_text": "We present the results of our study on chemical composition, molecular line emission, dust properties, and thermal balance in dense cores with metallicities ranging between 1/100 solar to 1/10 000 solar. We find that the gas temperature decreases by about 10 K as the core density increases for all metallicities studied here (1/100-1/10 000 solar). The decrease is more rapid than predicted by current models which assume constant temperatures throughout the cloud evolution. This may be due to an increase in the importance of grain-surface chemistry relative to gas-phase reactions at higher densities. In addition we find evidence for significant depletion of carbon onto grains even at high metallicities such as Z = 1/10 000 solar. Our observations suggest that the critical density above which CO becomes optically thick depends strongly on metallicity. At lower metallicities this occurs at higher densities compared to higher metallicities. Finally, we show that the observed abundance ratios are consistent with those expected if the clouds were initially chemically enriched by supernovae type II explosions.",
        "watermark_text": "We present the conclusion of our research on chemical composition , molecular line emission , dust characteristics , and thermal balance in dense cores with metallicities ranging between 1 / 100 solar to 1 / 10 000 solar . We see that the gas temperature reduces by about 10 K as the core size grows for all metallicities researched here ( 1 / 100 - 1 / 10 000 solar ) .The reduction is more rapid than forecast by current scenarios which predict constant temperatures throughout the cloud evolution . This might be due to an increase in the importance of grain - boundary dynamics compared to liquid - phase processes at higher densities .In addition we find proof for significant depletion of carbon onto grains even at high metallicities such as Z = 1 / 10 000 solar . Our observations suggest that the critical mass above which CO becomes optically dense relies highly on metallicity .At lower metallicities this appears at higher densities compared to higher metallicities . Finally , we find that the seen concentrations proportions are compatible with those expected if the clouds were initially chemically enriched by supernovae class II explosions .",
        "rewrite_text": "We present the findings of our research examining the chemical composition, molecular line emission, dust characteristics, and thermal balance in dense cores with metallicities ranging from 1/100 to 1/10,000 of the solar value. Our results indicate that gas temperature decreases by approximately 10 K as core size increases across all studied metallicities (1/100 to 1/10,000 solar). This decline occurs more rapidly than current models predict, which suggest that temperatures remain constant throughout the evolution of clouds. This discrepancy may be attributed to the growing significance of grain-boundary dynamics relative to liquid-phase processes at higher densities. Furthermore, we observe substantial carbon depletion onto grains, even at elevated metallicities like Z = 1/10,000 solar. Our findings indicate that the critical mass at which CO becomes optically dense is highly dependent on metallicity; at lower metallicities, this transition occurs at greater densities compared to higher metallicities. Lastly, we find that the observed concentrations are consistent with expectations if the clouds were initially enriched chemically by Type II supernova explosions.",
        "ori-fast-z-score": 1.5652475842498528,
        "water-fast-z-score": 7.313071356019155,
        "rewrite-fast-z-score": 1.6666666666666667
    },
    {
        "original_text": "We consider forward stagewise regression (FSR) for linear models with nonnegative coefficients, which is an iterative procedure that adds variables to the model one at a time until some stopping criterion is met.  We show how FSR can be used in conjunction with the monotone Lasso penalty to produce sparse solutions whose support contains all relevant predictors while simultaneously ensuring their signs are correct.  The resulting algorithm has computational complexity similar to standard Lasso algorithms but produces more accurate results on simulated data sets as well as real-world examples involving gene expression microarray data. Forward Stagewise Regression (FSR), introduced by Frank & Friedman  1  , is an iterative procedure where each iteration consists of adding a single variable into the current set of selected features based on its contribution to the objective function. This process continues until some stopping criteria is reached such as reaching a maximum number of iterations or meeting a desired level of accuracy  2  . In this work we focus on using FSR within the context of linear models with non-negative coefficients. For example, if our goal was to find genes associated with breast cancer then it would make sense to only select those genes that have been shown to increase risk rather than decrease risk  3  .\nThe main advantage of FSR over other greedy selection methods like stepwise regression  4  is that it does not require any tuning parameters  5  . However, there are two major drawbacks when applying FSR directly to problems with large numbers of potential covariates  6  :  1) It may take many iterations before the final solution converges; 2) There is no guarantee that the final solution will contain all relevant predictors. To address these issues, Tibshirani et al.  7  proposed the Least Absolute Shrinkage and Selection Operator (Lasso). Lasso solves the following optimization problem:",
        "watermark_text": "We consider forward stagewise regression ( FSR ) for linear models with nonnegative parameters , which is an iterative method that adds variables to the model one at a time until some stops criterion is reached . We see how FSR can be used in partnership with the monotone Lasso penalty to produce sparse scenarios whose support includes all relevant predictors while simultaneously ensuring their signs are correct .The resulting algorithm has computational efficiency comparable to standard Lasso techniques but generates more accurate conclusions on virtual data sets as well as real - time examples involving gene sequence microarray data . Forward Stagewise Regression ( FSR ) , invented by Frank & Friedman 1 , is an iterative method where each iteration consists of adding a single variable into the present set of chosen characteristics depending on its contribution to the objective function .This process proceeds until some stopping requirements is reached such as reaching a maximum number of iterations or meeting a desired level of precision 2 . In this research we focus on implementing FSR within the context of linear models with non - negative coefficients .For instance , if our goal was to find genes associated with breast tumors then it would work sense to only select those genes that have been shown to raise risk rather than decrease risk 3 . The main advantage of FSR over other greedy selection schemes like stepwise regression 4 is that it does not require any tuning variables 5 .However , there are two principal drawbacks when applying FSR directly to problems with large numbers of potential covariates 6 : 1 ) It might took many iterations before the finished problem converges ; 2 ) There is no guarantee that the finished problem will include all relevant predictors . To address these problems , Tibshirani et al .7 proposed the Least Absolute Shrinkage and Selection Operator ( Lasso ) . Lasso solves the following optimization problem :",
        "rewrite_text": "We examine Forward Stagewise Regression (FSR) for linear models with nonnegative parameters, an iterative approach that introduces variables sequentially until a predetermined stopping criterion is met. We explore how FSR can be effectively combined with the monotone Lasso penalty to create sparse models that include all pertinent predictors while ensuring their signs are accurate. This approach yields computational efficiency similar to standard Lasso methods yet offers more precise outcomes on both synthetic datasets and real-time applications, such as gene sequence microarray data. Developed by Frank & Friedman, FSR involves adding one variable at each step based on its contribution to the objective function, continuing until a stopping condition is satisfied, like reaching a maximum iteration count or achieving a specified level of precision. Our focus in this study is applying FSR to linear models with nonnegative coefficients. For example, when identifying genes linked to breast tumors, it is logical to select only those genes that are associated with an increased risk rather than a decreased risk. A key advantage of FSR over other greedy selection methods like stepwise regression is its lack of a need for tuning parameters. However, two main challenges arise when applying FSR to problems with a large number of potential covariates: first, it may require many iterations before convergence, and second, there is no assurance that all relevant predictors will be included in the final model. To mitigate these issues, Tibshirani et al. introduced the Least Absolute Shrinkage and Selection Operator (Lasso), which addresses the following optimization problem:",
        "ori-fast-z-score": 1.30066495428618,
        "water-fast-z-score": 8.811457940929873,
        "rewrite-fast-z-score": 1.958260097304659
    },
    {
        "original_text": "We present the first measurement of single-transverse-spin asymmetries (SSA) for hadronic dijets produced at midrapidity in p+p collisions at sqrt(sNN) = 5.02 TeV using data collected by the CMS experiment during 2012 corresponding to an integrated luminosity of 2.3 fb-1 . The SSAs are extracted as functions of jet transverse momentum and rapidity, azimuthal angle between jets, and event centrality. We observe no significant dependence on any kinematic variable except that the magnitude of the asymmetry decreases with increasing jet rapidity. Our results are compared to theoretical predictions based on perturbative QCD calculations including higher-order corrections and parton distribution function uncertainties. \nThe measured values agree well within experimental and theoretical uncertainties. This is the most precise measurement of this observable performed so far. \n \n Introduction \n \n Single transverse-spin asymmetries have been observed in several processes involving polarized protons or neutrons  1  , such as inclusive pion production  2  , semi-inclusive deep-inelastic scattering  3  , Drell-Yan lepton pair production  4  , prompt photon production  5  , and direct photons  6  . These measurements provide important information about the spin structure of nucleons  7, 8  .\n \nIn particular, they can be used to test the validity of factorization theorems  9  which relate hard-scattering cross sections to partonic distributions inside the proton  10  . In addition, these observables may also shed light on new physics beyond the Standard Model  11  . \n \n For example, it has recently been suggested  12  that large single-spin asymmetries could arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high-energy pp collisions. Such effects would violate parity conservation and thus constitute evidence for new physics  13  . However, there exists only one previous measurement  14  of single-spin asymmeties in hadronic dijet production at high energies. That study was carried out at RHIC  15  where the center-of-mass energy per nucleon-nucleon collision √sNN=200 GeV is much lower",
        "watermark_text": "We present the first measurement of single - transverse - spinning asymmetries ( SSA ) for hadronic dijets created at midrapidity in p + p collisions at sqrt ( sNN ) = 5 . 02 TeV using data derived by the CMS experiment during 2012 corresponding to an unified luminosity of 2 . 3 fb - 1 . The SSAs are derived as functions of jet transverse momentum and rapidity , azimuthal angle between planes , and event centrality .We see no major dependence on any kinematic variable except that the magnitude of the asymmetry decreases with increasing jet rapidity . Our results are compared to theoretical estimates based on perturbative QCD calculations including higher - order corrections and parton distribution function uncertainties .The measured measures agree well within experimental and theoretical uncertainties . This is the most accurate calculation of this observable performed so far .Introduction Single transverse - spinning asymmetries have been observed in multiple processes involving polarized protons or neutrons 1 , such as inclusive pion production 2 , semi - inclusive dark - inelastic emission 3 , Drell - Yan lepton pair production 4 , prompt photon production 5 , and direct photons 6 . These measurements give important information about the spin composition of nucleons 7 , 8 .In particular , they can be used to test the legitimacy of factorization theorems 9 which compare hard - absorption cross sections to partonic distributions inside the proton 10 . In addition , these observables might additionally bring light on new science beyond the Standard Model 11 .For instance , it has recently been proposed 12 that wide single - spinning asymmetries may arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high - energy pp collisions . Such effects would violate parity conservation and therefore constitute evidence for recent science 13 .However , there exists only one previous measurement 14 of single - spinning asymmeties in hadronic dijet production at high energies . That experiment was carried out at RHIC 15 where the center - of - mass electricity per nucleon - nucleon collision √sNN = 200 GeV is much lower",
        "rewrite_text": "We report the first measurement of single transverse spinning asymmetries (SSA) for hadronic dijets produced at midrapidity in p + p collisions at \\( \\sqrt{s_{NN}} = 5.02 \\) TeV, utilizing data obtained from the CMS experiment during 2012, which corresponds to a total integrated luminosity of 2.3 fb\\(^{-1}\\). The SSAs are analyzed as functions of jet transverse momentum and rapidity, the azimuthal angle between the jets, and the event centrality. Our findings indicate no significant dependence on any kinematic variable, aside from a reduction in the asymmetry magnitude with increasing jet rapidity. We compare our results to theoretical predictions based on perturbative QCD calculations that incorporate higher-order corrections and uncertainties in parton distribution functions. The measured values are in good agreement within the uncertainties of both the experimental and theoretical frameworks. This represents the most precise assessment of this observable to date.\n\nIntroduction: Single transverse spinning asymmetries have been observed in various processes involving polarized protons or neutrons, including inclusive pion production, semi-inclusive deep inelastic scattering, Drell-Yan lepton pair production, prompt photon emission, and direct photon production. These measurements provide crucial insights into the spin structure of nucleons. Specifically, they serve to validate factorization theorems that relate hard scattering cross sections to partonic distributions within the proton. Additionally, these observables could shed light on new physics beyond the Standard Model. Recent theories suggest that significant single spinning asymmetries may result from the interference of distinct helicity amplitudes from quarks emitted by longitudinally polarized gluons in high-energy pp collisions, which would imply a violation of parity conservation and provide evidence for new phenomena. However, there has been only one previous measurement of single spinning asymmetries in hadronic dijet production at high energies, conducted at RHIC, where the center-of-mass energy per nucleon-nucleon collision (\\( \\sqrt{s_{NN}} \\)) is only 200 GeV, significantly lower than our current study.",
        "ori-fast-z-score": 0.43685202833051895,
        "water-fast-z-score": 7.077002858954407,
        "rewrite-fast-z-score": -0.17541160386140586
    },
    {
        "original_text": "We consider the problem of finding an optimal first-order reduced basis space in the context of model reduction using Proper Orthogonal Decomposition (POD). We show that this can be achieved by solving a sequence of generalized eigenvalue problems associated with certain polynomials, which are derived from the original POD Galerkin formulation and its adjoint. The proposed approach is illustrated on two numerical examples. In particular we demonstrate how it allows to find a better approximation than standard greedy algorithms. This work was supported by EPSRC grant EP/K034494/1. Model reduction techniques have been widely used over recent years as efficient tools for reducing computational complexity when simulating complex physical systems governed by partial differential equations (PDEs)  1  . One popular class of methods relies on Proper Orthogonal Decompostion (POD), where one seeks to approximate the solution u(x, t) ∈ R N of a PDE system by projecting onto a low-dimensional subspace spanned by a set of orthonormal functions {φi(x)}Ni=1  2  , i.e.,",
        "watermark_text": "We consider the question of finding an efficient first - order reduced basis set in the context of model reduction using Proper Orthogonal Decomposition ( POD ) . We see that this can be obtained by handling a sequence of generalized eigenvalue difficulties related with certain polynomials , which are derived from the previous POD Galerkin approach and its adjoint .The proposed approach is depicted on two numerical examples . In particular we prove how it allows to find a better approximation than conventional greedy algorithms .This research was supported by EPSRC award EP / K034494 / 1 . Model reduction techniques have been widely using over recent years as efficient techniques for lowering computational difficulty when simulating complex physical networks defined by partial differential equations ( PDEs ) 1 .One popular family of methods relies on Proper Orthogonal Decompostion ( POD ) , where one seeks to approximate the solution u ( x , t ) ∈ R N of a PDE system by projecting onto a small - dimensional subspace covered by a group of orthonormal maps { φi ( x ) } Ni = 1 2 , i . e . ,",
        "rewrite_text": "We explore the challenge of identifying an efficient first-order reduced basis set within the framework of model reduction via Proper Orthogonal Decomposition (POD). Our findings indicate that this can be achieved by addressing a series of generalized eigenvalue problems associated with specific polynomials derived from the earlier POD Galerkin method and its adjoint. We illustrate this proposed method through two numerical examples, demonstrating its ability to yield superior approximations compared to traditional greedy algorithms. This research received support from the EPSRC award EP/K034494/1. In recent years, model reduction techniques have gained popularity as effective methods for reducing computational complexity in simulating intricate physical systems governed by partial differential equations (PDEs). A prominent approach within this field is Proper Orthogonal Decomposition (POD), which aims to approximate the solution u(x, t) ∈ ℝ^N of a PDE system by projecting it onto a low-dimensional subspace spanned by a set of orthonormal functions {φ_i(x)} for i = 1 to N.",
        "ori-fast-z-score": 0.3418817293789138,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": -0.4588314677411235
    },
    {
        "original_text": "We report on our analysis of archival Chandra data for the galaxy cluster Sersic 159-03, which shows evidence for excess X-ray emission below 1 keV (the  soft-excess ). We find that this feature is not consistent with thermal bremsstrahlung or line emission associated with any known atomic species and conclude it must be due to some other process such as inverse Compton scattering by relativistic electrons. The observed spectrum can be fitted well using an absorbed power-law model plus a blackbody component at kT = 0.2 keV; however we show that this fit is statistically unacceptable when compared against more physically motivated models including a combination of Bremsstrahlung and inverse-Compton emission. In particular, we demonstrate that the inclusion of a second blackbody component improves the quality of the fits significantly over those obtained previously.  Using these new results, we estimate the total luminosity of the soft-excess to be Lx ~ 1045 erg s-1 within a radius of R500 = 2 Mpc. This value is comparable to the bolometric luminosities inferred for several nearby radio halos detected via their synchrotron emission.",
        "watermark_text": "We report on our analysis of archival Chandra data for the galaxy region Sersic 159 - 03 , which reveals proof for excess X - ray radiation below 1 keV ( the dark - excess ) . We see that this characteristic is not consistent with thermal bremsstrahlung or line emission associated with any observed atomic species and assume it must be due to some other mechanism such as inverse Compton absorption by relativistic electrons .The observed spectrum can be fitted well using an absorbed power - law model plus a blackbody component at kT = 0 . 2 keV ; however we show that this fit is statistically unacceptable when compared against more physically motivated models including a combination of Bremsstrahlung and inverse - Compton emission . In particular , we demonstrate that the inclusion of a second blackbody component improves the quality of the fits significantly over those obtained previously .Using these new data , we estimate the total luminosity of the soft - excess to be Lx ~ 1045 erg s - 1 within a diameter of R500 = 2 Mpc . This value is analogous to the bolometric luminosities inferred for numerous nearby radio halos detected via their synchrotron emission .",
        "rewrite_text": "We present our analysis of archival Chandra data for the galaxy region Sersic 159-03, which uncovers evidence of excess X-ray emission below 1 keV (referred to as the dark-excess). This feature does not align with thermal bremsstrahlung or line emissions from any detected atomic species, leading us to propose that it results from another mechanism, possibly inverse Compton scattering by relativistic electrons. The observed spectrum can be adequately modeled with an absorbed power-law along with a blackbody component at kT = 0.2 keV; however, we demonstrate that this model is statistically inadequate when compared to more physically relevant models that include a mix of bremsstrahlung and inverse Compton emissions. Notably, we show that adding a second blackbody component significantly enhances the fit quality compared to previous efforts. Using this updated data, we estimate the total luminosity of the soft-excess to be Lx ~ 10^45 erg s^-1 within a radius of R500 = 2 Mpc, a value comparable to the bolometric luminosities derived for many nearby radio halos observed through their synchrotron emissions.",
        "ori-fast-z-score": -1.9629909152447274,
        "water-fast-z-score": 1.835325870964494,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The purpose of this study is to examine whether or not there are any differences between the distributions of firm size and profitability, using data on Japanese firms for the period from 1971 to 2000. The results show that both firm size and profitability follow a log-normal distribution with different parameters. In addition, it was found that the growth rate of firm size follows Gibrat s Law while that of profitability does not. This suggests that the relationship between firm size and profitability may be explained by the fact that they have different underlying processes. Finally, we find evidence supporting the hypothesis that the process generating firm size has changed over time. We also find some support for the hypothesis that the process governing profitability has changed over time. These findings suggest that the relationship between firm-size and profitability can change over time depending upon changes in their respective underlying processes. Keywords: Firm Size Distribution, Profitability Distribution",
        "watermark_text": "The purpose of this study is to examine whether or not there are any variations between the distributions of firm size and profitability , using data on Japanese businesses for the period from 1971 to 2000 . The results show that both firm height and profitability follow a log - normal distribution with various variables .In addition , it was shown that the growth probability of firm size follows Gibrat s Law while that of profitability does not . This implies that the relationship between firm scale and profitability could be explained by the fact that they have different intrinsic processes .Finally , we find proof confirming the notion that the process generating firm size has altered over time . We additionally find some evidence for the notion that the process governing profitability has altered over time .These studies demonstrate that the relationship between firm - structure and profitability can shift over time depending upon changes in their different underlying mechanisms . Keywords : Firm Size Distribution , Profitability Distribution",
        "rewrite_text": "This study aims to investigate potential differences in the distributions of firm size and profitability using data from Japanese companies spanning the years 1971 to 2000. The findings indicate that both firm size and profitability adhere to a log-normal distribution influenced by various factors. Furthermore, the analysis reveals that the growth probability of firm size aligns with Gibrat's Law, while profitability does not. This suggests that the connection between firm size and profitability may stem from their distinct underlying processes. Additionally, the results provide evidence that the mechanisms driving firm size have evolved over time, and there is also some indication that the processes influencing profitability have changed. These findings illustrate that the relationship between firm structure and profitability can fluctuate over time as a result of shifts in their respective underlying factors. Keywords: Firm Size Distribution, Profitability Distribution.",
        "ori-fast-z-score": -1.0834726777719228,
        "water-fast-z-score": 5.019960159204453,
        "rewrite-fast-z-score": -0.629940788348712
    },
    {
        "original_text": "We prove that if an operator is adjointable, then it has dense range in its domain.  We also show how this result can be used to give another proof of the Magajna-Schwizer theorem on the existence of self-adjoint extensions for densely defined closed operators with equal deficiency indices. Let H be a Hilbert space over C. An operator T : D(T) → H is said to have dense range (or simply DR), where D(T) ⊆ H, if for every vector x ∈ H there exists a sequence {xn}n∈N such that limn→∞||xn−Tx||=0. In other words, the image of T contains no proper subspace of H. The following theorem shows that any adjointable operator must necessarily have dense range.   THEOREM 1.1. If T:D(T)⊆H→H is adjointable, then R(T∗)=D(T).",
        "watermark_text": "We showed that if an operator is adjointable , then it has dense range in its domain . We also demonstrate how this fact can be used to give another proved of the Magajna - Schwizer theorem on the existence of self - adjoint extensions for tightly defined closed operators with equal deficiency indices .Let H be a Hilbert space over C . An operator T : D ( T ) → H is said to have dense range ( or simply DR ) , where D ( T ) ⊆ H , if for every matrix x ∈ H there exists a sequence { xn } n∈N such that limn→∞ | | xn−Tx | | = 0 . In other words , the image of T contains no complete subspace of H . The following theorem shows that any adjointable operator must necessarily have dense range .THEOREM 1.1.If T:D(T)⊆H→H is adjointable, then R(T∗)=D(T).",
        "rewrite_text": "We have demonstrated that if an operator is adjointable, then its range is dense in its domain. Additionally, we illustrate how this result can be utilized to provide an alternative proof of the Magajna-Schweizer theorem, which states that self-adjoint extensions exist for tightly defined closed operators that possess equal deficiency indices. Let \\( H \\) be a Hilbert space over \\( \\mathbb{C} \\). An operator \\( T: D(T) \\to H \\) is said to have a dense range (or simply DR) when for every vector \\( x \\in H \\), there exists a sequence \\( \\{ x_n \\}_{n\\in\\mathbb{N}} \\) such that \\( \\lim_{n\\to\\infty} \\| x_n - Tx \\| = 0 \\). In this context, the image of \\( T \\) does not contain any closed subspace of \\( H \\). The following theorem establishes that any adjointable operator must indeed have a dense range: \n\n**THEOREM 1.1.** If \\( T: D(T) \\subseteq H \\to H \\) is adjointable, then \\( R(T^*) = D(T) \\).",
        "ori-fast-z-score": 0.29488391230979427,
        "water-fast-z-score": 2.06418738616856,
        "rewrite-fast-z-score": -1.0
    },
    {
        "original_text": "We present new results on the X-ray spectrum and variability properties of Mrk 509, one of the brightest Seyfert galaxies observed by XMM-Newton. We find that its 0.5-10 keV continuum is well described by an absorbed power law with Γ = 2.1 ± 0.2 (χ2/dof=111/101) plus a reflection component modeled as a PEXRAV model with R=0.7-1.0 and NH=10-23×1022 cm-2. The best-fit parameters are consistent within errors to those found previously using Chandra data alone. No significant spectral changes were detected between different epochs separated by several months apart. However, we do detect strong flux variations at all energies during our observation period. In particular, there was a factor of 3 increase in the hard band count rate over about 20 ks followed by a slower decay back towards the initial level. This behavior can be explained if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation.",
        "watermark_text": "We report new data on the X - ray spectrum and variability properties of Mrk 509 , one of the brightest Seyfert galaxies studied by XMM - Newton . We see that its 0 . 5 - 10 keV continuum is well described by an absorption power law with Γ = 2 . 1 ± 0 . 2 ( χ2 / dof = 111 / 101 ) plus a mirror element modeled as a PEXRAV model with R = 0 . 7 - 1 . 0 and NH = 10 - 23×1022 centimetres - 2 .The best - fitting values are compatible within errors to those identified previously used Chandra data alone . No meaningful spectral changes were detected between various epochs separated by many months separated .However , we do discover powerful flux variations at all energies during our observation term . In particular , there was a factor of 3 shift in the hard band count rate over about 20 ks followed by a slower decay forward towards the first rate .This phenomenon can be understood if the source has been caught in a transfer state where the accretion disk luminosity increased rapidly due to some distortion or perturbation .",
        "rewrite_text": "We present new data regarding the X-ray spectrum and variability characteristics of Mrk 509, one of the brightest Seyfert galaxies investigated by XMM-Newton. Our analysis indicates that the 0.5-10 keV continuum is well represented by an absorption power law with a spectral index of Γ = 2.1 ± 0.2 (χ²/dof = 111/101), along with a reflection component modeled using the PEXRAV framework, featuring R = 0.7 - 1.0 and NH = 10 - 23 × 10²² cm⁻². These best-fitting values are consistent, within uncertainties, with those determined from prior Chandra observations. No significant spectral changes were observed across different epochs spaced several months apart. However, we did note substantial flux variations across all energy bands during our observational period. Most notably, there was a threefold increase in the hard band count rate over approximately 20 ks, followed by a gradual decline back toward the initial rate. This behavior may suggest that the source is experiencing a transition phase where the luminosity from the accretion disk rapidly increased due to some perturbation or disturbance.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 6.527299120066193,
        "rewrite-fast-z-score": 0.3464101615137754
    },
    {
        "original_text": "We study the supersymmetry breaking patterns for vector multiplets transforming under non-adjoint representations of SO(N). We find that there are two distinct classes of theories, depending on whether or not the representation is real. In particular we show how to construct explicit examples with N = 1 and N = 2 supersymmetries which break all their supersymmetries spontaneously. Theories with adjoint matter fields can be obtained as special cases by taking appropriate limits. This work was supported in part by NSF grant PHY-0456735. Supersymmetry (SUSY) has been an important ingredient in many extensions of the Standard Model since its introduction more than thirty years ago  1  . It provides a natural solution to the hierarchy problem between the weak scale and the Planck scale  2  , while at the same time offering new ways to understand gauge coupling unification  3  .\nIn recent years it has become clear that SUSY must be broken if one wants to make contact with experiment  4  . However, despite much effort over several decades  5  -  8  , no fully satisfactory mechanism for spontaneous SUSY breaking exists yet  9  . One promising approach involves using supergravity  10  -  12  to generate soft terms  13  -  15  which then trigger SUSY breakdown  16  -  18  . Another possibility is to use extra dimensions  19  -  21  where SUSY is broken either explicitly  22  -  24  or spontaneously  25  -  27  via boundary conditions  28  -  30  . A third option is to consider models based on local symmetries  31  -  33  such as gauged  34  -  37  or global  38  -  41  SUSY.",
        "watermark_text": "We research the supersymmetry broken schemes for vector multiplets transforming under non - adjoint representations of SO ( N ) . We see that there are two different categories of theories , depending on whether or not the representation is real .In particular we prove how to build explicit examples with N = 1 and N = 2 supersymmetries which break all their supersymmetries spontaneously . Theories with adjoint matter varieties can be obtained as special cases by using appropriate restrictions .This project was supported in part by NSF grant PHY - 0456735 . Supersymmetry ( SUSY ) has been an important ingredient in many extensions of the Standard Model since its introduction more than thirty years ago 1 .It provides a natural solution to the ranking problem between the weakness scale and the Planck scale 2 , while at the same time providing new ways to comprehend gauge correlation unification 3 . In recent seasons it has become clear that SUSY must be broken if one wants to make contact with test 4 .However , despite much effort over numerous centuries 5 - 8 , no fully acceptable mechanism for premature SUSY broke exists yet 9 . One promising solution involves utilizing supergravity 10 - 12 to create soft terms 13 - 15 which then activate SUSY breakdown 16 - 18 .Another possibility is to use extra dimensions 19 - 21 where SUSY is beaten either explicitly 22 - 24 or spontaneously 25 - 27 via boundary rules 28 - 30 . A third possibility is to consider models built on local symmetries 31 - 33 such as gauged 34 - 37 or worldwide 38 - 41 SUSY .",
        "rewrite_text": "We investigate supersymmetry-breaking schemes for vector multiplets that transform under non-adjoint representations of SO(N). Our findings reveal two distinct categories of theories based on whether the representation is real or not. Specifically, we demonstrate how to construct explicit examples with N = 1 and N = 2 supersymmetries that spontaneously break all their supersymmetries. Theories incorporating adjoint matter can be perceived as special cases achieved through suitable restrictions. This research was partially funded by NSF grant PHY-0456735. Supersymmetry (SUSY) has played a crucial role in various extensions of the Standard Model since its introduction over thirty years ago. It offers a natural resolution to the hierarchy problem between the weak scale and the Planck scale, while also providing new insights into gauge coupling unification. Recently, it has become evident that SUSY must be broken in order to align with experimental results. However, despite extensive efforts over many years, no fully satisfactory mechanism for early SUSY breaking has been established. One promising approach involves using supergravity to generate soft terms that facilitate SUSY breaking. Alternatively, extra dimensions may be employed, where SUSY can be broken either explicitly or spontaneously through boundary conditions. Another avenue is to explore models based on local symmetries, such as gauged or global SUSY.",
        "ori-fast-z-score": -0.2,
        "water-fast-z-score": 6.8657566124489255,
        "rewrite-fast-z-score": 0.5076730825668095
    },
    {
        "original_text": "The Standard Model (SM) predicts the existence of new heavy particles, such as W and Z bosons that are responsible for weak interactions between quarks and leptons. The Large Hadron Collider experiments have searched for these particles in their data sets but no evidence has been found so far. In this work we present an analysis to look for heavy vector-like fermions decaying into pairs of charged or neutral gauge bosons using proton-proton collision data collected by ATLAS experiment during Run 1 period with center-of-mass energy √s=7 TeV corresponding to an integrated luminosity of 4.6 fb-1 . We consider two benchmark models where one is based on SU(2)LxU(1)Y gauge group while another model is based on SO(10). For both cases we perform a detailed simulation study to understand signal acceptance and background contributions. Using our results we set limits on production cross-section times branching ratio for different mass points ranging from 0.5 TeV to 3 TeV.",
        "watermark_text": "The Standard Model ( SM ) predicts the existence of new massive bodies , such as W and Z bosons that are responsible for weak interactions between quarks and leptons . The Large Hadron Collider experiments have searched for these ions in their information sets but no evidence has been detected so far .In this research we present an analysis to search for heavy vector - like fermions decaying into pairs of charged or neutral gauge bosons using proton - proton collision data taken by ATLAS program during Run 1 period with center - of - mass mass √s = 7 TeV corresponding to an integrated luminosity of 4 . 6 fb - 1 . We consider two benchmark versions where one is based on SU ( 2 ) LxU ( 1 ) Y gauge group while another model is based on SO ( 10 ) .For both cases we perform a detailed simulation study to explain signal availability and background contributions . Using our findings we setting limits on production cross - section times branching ratio for different mass points ranging from 0 . 5 TeV to 3 TeV .",
        "rewrite_text": "The Standard Model (SM) predicts the presence of new massive particles, such as the W and Z bosons, which mediate weak interactions between quarks and leptons. Experiments at the Large Hadron Collider have searched for these particles within their datasets, but no evidence has been found to date. In this study, we present an analysis aimed at detecting heavy vector-like fermions that decay into pairs of charged or neutral gauge bosons, utilizing proton-proton collision data collected by the ATLAS experiment during Run 1, with a center-of-mass energy of √s = 7 TeV and an integrated luminosity of 4.6 fb⁻¹. We consider two benchmark scenarios: one based on the SU(2)L × U(1)Y gauge group, and the other on SO(10). For both scenarios, we conduct a comprehensive simulation study to assess the signal significance and background contributions. Our findings allow us to set limits on the production cross-section times branching ratio for various mass points, ranging from 0.5 TeV to 3 TeV.",
        "ori-fast-z-score": 0.8551861104941365,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": 0.13018891098082389
    },
    {
        "original_text": "We study the stability of viscous shock waves for an ideal gas with viscosity and heat conduction, using the method of characteristics.  We show that if the initial data are sufficiently small then there exists a unique global solution to this problem which converges exponentially fast as t → ∞ towards the corresponding inviscid solution (the Rankine-Hugoniot condition). If the initial data have compact support we prove that the solution remains uniformly bounded on any finite time interval  0,T . Finally, under some additional assumptions about the initial data, we establish the exponential decay of solutions at infinity. The results obtained here generalize those known previously only for the case when the pressure p(ρ) = ρ γ , where 0 < γ ≤ 1.1. Viscous shock waves play important role in many physical phenomena such as combustion theory or plasma physics. In particular they appear in connection with the so-called Riemann problems describing the interaction between two different flows separated by a discontinuity surface. For example, one can consider the following problem:  Find a function u(x,t), x ∈ R n , t > 0 satisfying the system of equations",
        "watermark_text": "We research the stability of viscous blast currents for an suitable gas with viscosity and heat conduction , using the method of characteristics . We see that if the first data are sufficiently small then there exists a unique worldwide answer to this question which converges exponentially rapidly as t → ∞ towards the equivalent inviscid solving ( the Rankine - Hugoniot condition ) .If the first data have compact support we prove that the solve remains uniformly bounded on any finite time interval 0 , T . Finally , under some additional constraints about the initial data , we determine the exponential decay of solutions at infinity .The results derived here generalize those known previously only for the case when the pressure p ( ρ ) = ρ γ , where 0 < γ ≤ 1 . 1 . Viscous blast currents play important role in many mechanical phenomena such as combustion theory or plasma physics .In particular they appear in connection with the so - called Riemann problems regarding the interaction between two different flows divided by a discontinuity terrain . For instance , one can consider the following issue : Find a function v ( x , t ) , x ∈ R n , t > 0 satisfying the scheme of equations",
        "rewrite_text": "We investigate the stability of viscous blast currents for a suitable gas that possesses viscosity and heat conduction, utilizing the method of characteristics. Our findings indicate that when the initial data are sufficiently small, there exists a unique global solution that converges exponentially fast to the corresponding inviscid solution (as described by the Rankine-Hugoniot condition) as time approaches infinity. If the initial data have compact support, we demonstrate that the solution remains uniformly bounded over any finite time interval [0, T]. Furthermore, under certain additional conditions on the initial data, we establish the exponential decay of solutions as time goes to infinity. These results extend previous findings that were limited to the case when pressure p(ρ) = ρ^γ, with 0 < γ ≤ 1. Viscous blast currents are significant in various mechanical phenomena, including combustion theory and plasma physics. Notably, they are relevant to Riemann problems that address the interaction between two distinct flows separated by a discontinuity. For instance, we seek to find a function v(x, t), where x ∈ R^n and t > 0, that satisfies the following system of equations.",
        "ori-fast-z-score": -1.078327732034384,
        "water-fast-z-score": 5.314796216557077,
        "rewrite-fast-z-score": 0.105999788000636
    },
    {
        "original_text": "We present an exact hierarchical description of the reduced state and dynamics of open quantum systems in terms of a set of coupled equations, which we call the Hierarchy of Density Tensors (HDT). The HDT is derived by applying the Nakajima-Zwanzig projection operator technique to the von Neumann equation describing the evolution of the total system. We show that this approach allows one to retrieve all relevant information about the environment-induced decoherence process on arbitrary timescales. In particular, it provides access to the full spectrum of relaxation rates characterizing the decay of off-diagonal elements of the reduced density matrix as well as the stationary states reached at late times. As an example, we apply our formalism to study the dissipative spin-boson model with Ohmic dissipation. Our results are compared against numerical simulations based on the Quantum Monte Carlo Wavefunction method. \nI. INTRODUCTORY REMARK\nThe understanding of how macroscopic objects behave under the influence of their environments has been a central issue in physics since its very beginning  1, 2  . This problem becomes particularly challenging when dealing with complex many-body systems such as condensed matter or biological ones  3, 4  , where the number of degrees of freedom involved can be extremely large. A powerful theoretical tool to tackle these problems consists in studying the dynamics of the reduced state of the system of interest S conditioned upon some specific measurement performed over the environmental degrees of freedom E  5, 6  .\nIn recent years there have been several attempts to develop efficient methods to describe the time-evolution of the reduced state  7, 8  . Among them, the so-called Hierarchy of Density Matrices (HDM)  9  represents a promising alternative to other approaches  10, 11  due to its ability to capture non-Markovian effects  12  . However, despite being able to provide accurate predictions for short-time evolutions  13  , the HDM fails to reproduce correctly the asymptotic behavior of the system  14  . To overcome this limitation, here we introduce a new formulation of the HDM, called Hierarchy of Density...",
        "watermark_text": "We create an precise hierarchical description of the reduced state and dynamics of open quantum systems in terms of a setting of coupled equations , which we call the Hierarchy of Density Tensors ( HDT ) . The HDT is calculated by using the Nakajima - Zwanzig projection operator technique to the von Neumann equation explaining the evolution of the total system .We see that this methodology allows one to locate all relevant information about the environment - caused decoherence process on arbitrary timescales . In particular , it gives access to the full range of relaxation frequencies characterizing the decay of off - diagonal elements of the reduced density matrix as well as the stationary states reached at late times .As an instance , we apply our formalism to study the dissipative spin - boson theory with Ohmic dissipation . Our results are compared against numerical simulations based on the Quantum Monte Carlo Wavefunction method .I . INTRODUCTORY REMARK The knowledge of how macroscopic objects react under the impact of their environments has been a central topic in physics since its very beginning 1 , 2 .This problem appears particularly challenging when dealing with difficult large - bodies systems such as condensed matter or biological ones 3 , 4 , where the quantity of degrees of freedom employed can be extremely huge . A good experimental tool to tackle these problems involves in examining the dynamics of the reduced state of the system of interest S conditioned upon some specific assessment performed over the environmental degrees of liberty E 5 , 6 .In past decades there have been numerous attempts to develop able methods to explain the period - progression of the reduced state 7 , 8 . Among them , the so - called Hierarchy of Density Matrices ( HDM ) 9 offers a successful approach to other methods 10 , 11 due to its able to capture non - Markovian effects 12 .However , despite being able to provide accurate forecast for short - time evolutions 13 , the HDM fails to predict correctly the asymptotic behavior of the system 14 . To solve this limitation , here we incorporate a new implementation of the HDM , entitled Hierarchy of Density . . .",
        "rewrite_text": "We develop a precise hierarchical framework for analyzing the reduced state and dynamics of open quantum systems through a set of coupled equations, which we term the Hierarchy of Density Tensors (HDT). This framework is derived using the Nakajima-Zwanzig projection operator technique applied to the von Neumann equation that governs the evolution of the total system. Our approach effectively captures all pertinent information regarding the decoherence processes induced by the environment across various timescales. Specifically, it enables the determination of the full spectrum of relaxation frequencies that characterize the decay of off-diagonal elements in the reduced density matrix, as well as the stationary states attained at long times. As an illustrative example, we employ our formalism to investigate the dissipative spin-boson model with Ohmic dissipation, comparing our findings to numerical simulations conducted using the Quantum Monte Carlo Wavefunction method. \n\n**I. INTRODUCTORY REMARK**  \nUnderstanding how macroscopic objects respond to their environments has been a fundamental issue in physics since its inception. This challenge becomes particularly pronounced in dealing with complex systems such as condensed matter and biological entities, where the number of degrees of freedom can be exceedingly large. A valuable experimental strategy for tackling these challenges involves examining the dynamics of the system's reduced state, conditioned on specific measurements of the environmental degrees of freedom. Over the past decades, numerous efforts have been made to develop effective methods for describing the time evolution of the reduced state. One notable method, the Hierarchy of Density Matrices (HDM), offers significant advantages by effectively capturing non-Markovian effects. However, while the HDM can accurately predict short-time dynamics, it struggles to correctly forecast the system's asymptotic behavior. To address this limitation, we present an enhanced version of the HDM, referred to as the Hierarchy of Density.",
        "ori-fast-z-score": -0.40961596025952024,
        "water-fast-z-score": 7.675067860720625,
        "rewrite-fast-z-score": 1.01418510567422
    },
    {
        "original_text": "We present new spectroscopic observations for nine cataclysmic variable stars (CVs) obtained with the HIRES spectrograph on Keck I telescope in Hawaii, and compare them to previous results. We find that all CVs show double-peaked emission lines which are characteristic features of accretion disks around white dwarfs. The line profiles change dramatically during outburst phases when mass transfer rates increase by several orders of magnitude compared to quiescent states. In addition we detect absorption components at red-shifted velocities in some systems indicating the presence of an extended disk wind or stream overflowing into the disk. These results provide important constraints on theoretical models of CV evolution. \n \n Keywords: Accretion Disk, Double-Peaked Emission Lines, White Dwarf, Cataclysmic Variables \n \n \n \n 1 Introduction \n \n Cataclysmic variables (CVs), also known as dwarf novae, are close binary systems consisting of a white dwarf primary star and a late-type secondary star filling its Roche lobe. Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf where it forms an accretion disk surrounding the compact object. This process leads to periodic outbursts caused by thermal instabilities in the accretion disk resulting in dramatic changes in luminosity over time scales ranging from hours up to years  1  . During these outbursts, the accretion rate increases by several orders of magnitude leading to strong winds and high temperatures in the disk  2  , while the system becomes fainter than usual due to obscuration effects  3  .\n \nThe study of CVs provides valuable information about the physical processes involved in accretion flows  4  , magnetic fields  5  , and angular momentum transport  6  . Furthermore, they can be used as distance indicators  7, 8  and probes of galactic structure  9  . \n \n 2 Observations & Data Reduction \n \n Our sample consists of 9 CVs observed between 2004 and 2007 using the High Resolution Echelle Spectrometer (HIRES)  10  mounted on the 10 m Keck I telescope located on Mauna Kea",
        "watermark_text": "We introduce novel spectroscopic observations for nine cataclysmic variable stars ( CVs ) obtained with the HIRES spectrograph on Keck I telescope in Hawaii , and compare them to previous findings . We see that all CVs show dual - peaked emission lines which are peculiar characteristics of accretion disks around white dwarfs .The line profiles change dramatically during outburst phases when mass transfer rates increase by many orders of magnitude compared to quiescent states . In addition we perceive absorption elements at red - shifted velocities in some systems suggesting the presence of an extended disk dust or stream overflowing into the disk .These conclusions provide important restrictions on theoretical theories of CV evolution . Keywords : Accretion Disk , Double - Peaked Emission Lines , White Dwarf , Cataclysmic Variables 1 Introduction Cataclysmic variables ( CVs ) , sometimes called as dwarf novae , are open binary complexes consisting of a black dwarf secondary star and a early - class secondary star occupying its Roche lobe .Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf where it creates an accretion disk surrounding the compact body . This process results to periodic outbursts caused by temperature instabilities in the accretion disk resulting in spectacular changes in luminosity over time ranges ranging from hours up to days 1 .During these outbursts , the accretion rate grows by many orders of magnitude resulting to powerful storms and rising heat in the disk 2 , while the system gets fainter than usual thanks to obscuration effects 3 . The investigation of CVs provides valuable info about the physical processes responsible in accretion movements 4 , magnetic waves 5 , and spatial velocity transport 6 .Furthermore , they can be used as distance indicators 7 , 8 and probes of galactic structure 9 . 2 Observations & Data Reduction Our specimen consists of 9 CVs seen between 2004 and 2007 utilizing the High Resolution Echelle Spectrometer ( HIRES ) 10 installed on the 10 m Keck I telescope located on Mauna Kea",
        "rewrite_text": "We present new spectroscopic observations of nine cataclysmic variable stars (CVs), captured with the HIRES spectrograph on the Keck I telescope in Hawaii, and compare these results with earlier studies. Our findings reveal that all CVs exhibit dual-peaked emission lines, which are distinctive features of the accretion disks surrounding white dwarfs. These line profiles undergo significant changes during outburst phases, marked by a substantial increase in mass transfer rates compared to their quiescent states. Additionally, we observe red-shifted absorption features in some systems, indicating the presence of an extended dust disk or streams interacting with the disk. These observations impose crucial constraints on theoretical models of CV evolution. \n\n**Keywords**: Accretion Disk, Double-Peaked Emission Lines, White Dwarf, Cataclysmic Variables\n\n**1. Introduction**  \nCataclysmic variables (CVs), often referred to as dwarf novae, are binary systems composed of a white dwarf and a late-type secondary star, with the secondary star filling its Roche lobe. Mass is transferred through the inner Lagrangian point L1 onto the white dwarf's surface, forming an accretion disk around it. This process leads to periodic outbursts caused by temperature instabilities in the disk, which result in dramatic changes in luminosity over timescales from hours to days. During these outbursts, the accretion rate can increase by several orders of magnitude, creating intense activity and elevated temperatures within the disk, even as the system may appear fainter due to obscuration effects. The study of CVs yields essential insights into the physical processes driving accretion dynamics, magnetic phenomena, and the transport of spatial velocities. Furthermore, CVs serve as potential distance indicators and probes for understanding galactic structure.\n\n**2. Observations & Data Reduction**  \nOur sample includes nine CVs observed between 2004 and 2007 using the High Resolution Echelle Spectrometer (HIRES) installed on the 10 m Keck I telescope situated on Mauna Kea.",
        "ori-fast-z-score": -1.0215078369104984,
        "water-fast-z-score": 8.057794831959724,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an analysis of synthetic stellar libraries used to calibrate photometric surveys, such as Gaia and LSST. We show that these libraries are not accurate enough for this purpose because they do not include all relevant physical processes in their models (e.g., convection). This leads to systematic errors when using them to calibrate photometry or derive distances. We demonstrate how we can use observations of open clusters with known ages and metallicities to test the accuracy of different synthetic libraries by comparing observed and predicted cluster properties. Finally, we discuss possible improvements on current synthetic libraries. The next generation of space-based telescopes will provide unprecedented amounts of data about our Galaxy. These new datasets require large efforts to be analyzed properly. One important aspect is the calibration of photometric surveys like Gaia and LSST which will deliver precise astrometry and multi-color photometry for billions of stars across the sky. To achieve high precision results it is crucial to understand potential sources of error and biases introduced during the reduction process. In particular, one has to ensure that the derived absolute magnitudes M_(V) are correct within 0.01 mag over most of the color range covered by the survey. \n \n For example, if the distance modulus DM = 5log10(d/d_sun), where d is the true distance between us and the star and d_sun is the Sun’s distance from Earth, then a difference of 0.01 mag corresponds to a factor of 1.1 in distance. Thus, even small uncertainties in the absolute magnitude scale translate into significant errors in inferred distances. Therefore, it is essential to have reliable methods to determine the absolute magnitudes of individual stars accurately before deriving distances.  \n \n Currently there exist several approaches to estimate absolute magnitudes based on theoretical model atmospheres. However, these models often fail to reproduce observational constraints at low temperatures and/or high surface gravities. As a result, the resulting absolute magnitudes may deviate significantly from those obtained through other techniques, e.g., eclipsing binaries. Moreover, some of these models also suffer from incomplete",
        "watermark_text": "We present an assessment of synthetic astronomical collections useful to calibrate photometric surveys , such as Gaia and LSST . We suggest that these archives are not accurate sufficient for this objective because they do not include all relevant physical processes in their models ( e . g . , convection ) .This leads to systematic errors when using them to calibrate photometry or calculate distances . We showed how we can using observations of open nuclei with established periods and metallicities to test the accuracy of different synthetic databases by comparing observed and anticipated cluster structures .Finally , we discuss possible advances on current artificial libraries . The future wave of space - based telescopes will provide incredible amounts of data about our Galaxy .These new datasets take great efforts to be analyzed correctly . One important milestone is the calibration of photometric surveys like Gaia and LSST which will provide accurate astrometry and multi - color photometry for billions of stars across the sky .To achieve high clarity findings it is crucial to realize potential sources of mistake and biases created during the reduction step . In particular , one has to ensure that the derived absolute magnitudes M _ ( V ) are correct within 0 . 01 mag over most of the color spectrum encompassed by the poll .For instance , if the distance modulus DM = 5log10 ( d / d _ sun ) , where d is the true distance between us and the star and d _ sun is the Sun ’ s distance from Earth , then a difference of 0 . 01 mag corresponds to a factor of 1 . 1 in distance . Thus , even minor uncertainties in the absolute magnitude scale turn into considerable errors in inferred distances .Therefore , it is crucial to have reliable techniques to estimate the absolute magnitudes of individual stars accurately before deriving distances . Currently there remain many approaches to estimate absolute magnitudes based on theoretical model atmospheres .However , these models often fail to reproduce observational restrictions at low temperatures and / or large depth gravities . As a outcome , the resulting absolute magnitudes might deviate greatly from those achieved through other techniques , e . g . , eclipsing binaries .Moreover , some of these models even suffer from incomplete",
        "rewrite_text": "We provide an evaluation of synthetic astronomical collections that serve as tools for calibrating photometric surveys like Gaia and LSST. Our analysis indicates that these archives lack the necessary accuracy for this purpose, as they do not account for all relevant physical processes in their models—such as convection. This discrepancy results in systematic errors when applied to photometric calibration or distance calculations. We demonstrate how observations of open clusters with known periods and metallicities can be utilized to assess the reliability of various synthetic databases by comparing observed cluster structures with predicted ones. Additionally, we explore potential improvements to current synthetic libraries. The next generation of space-based telescopes is expected to generate vast amounts of data about our Galaxy, necessitating meticulous analysis of these new datasets. One key milestone will be the calibration of photometric surveys like Gaia and LSST, which aim to deliver precise astrometry and multi-color photometry for billions of stars throughout the sky. To ensure reliable findings, it is essential to identify potential sources of errors and biases that may arise during the data reduction process. Specifically, it is vital to confirm that the derived absolute magnitudes \\( M_V \\) are accurate within 0.01 magnitudes across the majority of the color spectrum involved. For example, the distance modulus \\( DM = 5 \\log_{10}(d/d_{sun}) \\) indicates that a difference of 0.01 magnitudes translates to a distance error factor of 1.1. Thus, even minor inaccuracies in the absolute magnitude scale can result in significant errors in calculated distances. Consequently, reliable methods for accurately estimating the absolute magnitudes of individual stars are essential before determining distances. Although various methods exist to estimate absolute magnitudes using theoretical model atmospheres, these models often fail to meet observational constraints at low temperatures and/or high surface gravities. As a result, the absolute magnitudes they produce may significantly diverge from those determined through alternative methods, such as eclipsing binaries. Additionally, some of these models suffer from incompleteness.",
        "ori-fast-z-score": -1.784435632438388,
        "water-fast-z-score": 6.871842709362768,
        "rewrite-fast-z-score": -0.5696519211398116
    },
    {
        "original_text": "The authors have used scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8+d (Bi-2212). They find that, at low temperatures, this material forms pairs of holes which are bound together by an attractive interaction mediated by phonons. The binding energy is found to be about 0.5 eV per hole pair. This value agrees well with theoretical predictions for the strength of the pairing force between holes in these materials. \n \n In addition, they observe that the density of states near the Fermi level shows a strong dependence on the direction along which the crystal is cut. For example, when the crystal is cleaved parallel to its Cu-O planes, it exhibits a large peak in the density of states just below the Fermi level. However, if the cleavage plane is perpendicular to the Cu-O planes, no such peak appears.",
        "watermark_text": "The authors have utilized scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high - temperature cuprate superconductor Bi2Sr2CaCu2O8 + d ( Bi - 2212 ) . They find that , at low temperatures , this metal creates sets of holes which are bound together by an attractive interaction mediated by phonons .The interaction power is found to be about 0 . 5 eV per hole pair . This value agrees well with theoretical expectations for the strength of the pairing force between holes in these structures .In addition , they demonstrate that the density of states near the Fermi level exhibits a large dependence on the direction along which the crystal is cutting . For instance , when the crystal is cleaved parallel to its Cu - O planes , it displays a large peak in the density of states just below the Fermi level .However , if the cleavage plane is perpendicular to the Cu - O axes , no such peak appears .",
        "rewrite_text": "The authors employed scanning tunneling microscopy to investigate the surface structure and electronic characteristics of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8 + δ (Bi-2212). Their findings reveal that at low temperatures, this metal forms clusters of holes bound together by an attractive interaction mediated by phonons, with an interaction strength estimated at approximately 0.5 eV per hole pair. This value aligns closely with theoretical predictions regarding the pairing force strength between holes in these materials. Furthermore, they illustrate that the density of states near the Fermi level varies significantly depending on the orientation of the crystal cleavage. For instance, cleaving the crystal parallel to its Cu-O planes results in a prominent peak in the density of states just below the Fermi level, while no such peak is observed when the cleavage occurs perpendicular to the Cu-O axes.",
        "ori-fast-z-score": 2.75,
        "water-fast-z-score": 6.4007575309253015,
        "rewrite-fast-z-score": 3.6927447293799815
    },
    {
        "original_text": "We present fitting formulae for the illumination of accretion disks by hot spots, as seen in Schwarzschild and rotating black holes (Kerr). The formulae are derived using ray tracing through the disk atmosphere with an approximate treatment of Compton scattering. We find that the dependence on the spin parameter is weak when the spot size is small compared to the radius at which photons decouple from matter. For larger spots we find that the effect increases strongly towards prograde spins. Our results can be used to estimate the effects of relativistic Doppler boosting and gravitational lensing on observed spectra. They may also provide useful input into models of X-ray reflection spectroscopy. \nIntroduction\n\nAccreting black holes produce bright emission lines in their X-ray spectrum due to reprocessing of hard X-rays emitted near the event horizon by cold material orbiting close to the equatorial plane. These features have been studied extensively over many years both observationally and theoretically (see Reynolds & Nowak 2003 , Done et al 2004 . In particular, they show strong red-shifts indicating that the emitting gas orbits rapidly around the black hole. This rapid rotation causes additional shifts in energy due to relativistic Doppler boosts and gravitational lensing. Relativistic effects become more important if the emitting region has a high degree of rotational support or is viewed nearly face-on. It is therefore necessary to take these effects into account when interpreting observations of such systems. \n\nIn this work we consider the case where the illuminating source is located above the disk surface but below its photosphere. Such sources include magnetic flares produced within the disk itself or active regions associated with the inner edge of the disk. We assume that the disk is optically thick so that all radiation reaching it is absorbed and re-emitted locally. We use Monte Carlo simulations to calculate the emergent flux from the disk under various assumptions about the geometry of the system.\n\nThe main goal of our study was to develop simple analytical expressions describing how the shape of the line profile depends on the properties of the system. To do this we performed extensive numerical calculations covering a wide range",
        "watermark_text": "We present fitting formulae for the illumination of accretion disks by hot spots , as shown in Schwarzschild and rotating black holes ( Kerr ) . The formulae are derived using ray tracing through the disk atmosphere with an approximate treatment of Compton absorption .We see that the dependence on the spin vector is weak when the spot size is tiny relative to the radius at which photons decouple from matter . For larger spots we find that the impact grows highly towards prograde spins .Our results can be used to estimate the effects of relativistic Doppler boosting and gravity lensing on measured spectra . They might additionally offer useful input into estimates of X - ray reflection spectroscopy .Introduction Accreting black holes create bright emission lines in their X - ray spectrum due to reprocessing of hard X - rays generated near the event horizon by cold matter orbiting close to the equatorial plane . These features have been studied frequently over numerous years both observationally and theoretically ( saw Reynolds & Nowak 2003 , Done et al 2004 .In particular , they show intense red - shifts suggesting that the emitting gas orbits rapidly around the dark hole . This rapid rotation creates additional shifts in energy due to relativistic Doppler boosts and gravity lensing .Relativistic effects become more essential if the emitting area has a high degree of rotational support or is viewed virtually face - on . It is consequently required to take these consequences into consideration when interpreting observations of such systems .In this research we imagine the case where the illuminating source is situated above the disk boundary but below its photosphere . Such sources include magnetic flares created within the disk itself or active regions associated with the inner boundary of the disk .We assume that the disk is optically dense so that all light reaching it is absorption and re - radiated locally . We use Monte Carlo simulations to estimate the emergent flux from the disk under various statements about the topology of the system .The main goal of our research was to develop primitive analytical expressions relating how the form of the line profile depends on the properties of the system . To do this we performed extensive numerical measurements encompassing a broad range",
        "rewrite_text": "We present fitting formulas for the illumination of accretion disks by hotspots surrounding Schwarzschild and rotating (Kerr) black holes. These formulas are derived through ray tracing in the disk atmosphere, with an approximate consideration of Compton absorption. Our analysis shows that the influence of the spin vector is minimal when the hotspot size is small in comparison to the radius at which photons decouple from matter. However, for larger hotspots, the effect of spin becomes significantly stronger, especially for prograde spins. This research can help estimate the impact of relativistic Doppler boosting and gravitational lensing on observed spectra and may provide valuable insights for interpreting results from X-ray reflection spectroscopy.\n\nAccreting black holes produce prominent emission lines in their X-ray spectra as a result of the reprocessing of high-energy X-rays generated near the event horizon by cooler matter in close orbit around the equatorial plane. Over the years, these spectral features have been the subject of extensive observational and theoretical studies (see Reynolds & Nowak 2003, Done et al. 2004). Notably, they exhibit pronounced redshifts, indicating that the emitting gas is in rapid orbit around the black hole. This rapid motion induces additional energy shifts due to relativistic Doppler effects and gravitational lensing. The significance of these relativistic effects increases when the emitting region exhibits a high degree of rotational support or is viewed nearly face-on. Therefore, it is crucial to account for these factors when interpreting observations of such systems. \n\nIn our study, we consider a scenario where the illuminating source is located above the disk boundary but beneath its photosphere. Potential sources include magnetic flares arising within the disk or active regions linked to its inner boundary. We assume the disk is optically thick, ensuring that any light reaching it is absorbed and re-emitted locally. To estimate the emergent flux from the disk under different configurations, we employ Monte Carlo simulations. The primary objective of our research was to develop basic analytical expressions demonstrating how the line profile depends on the system's properties. To achieve this, we conducted extensive numerical simulations across a wide range of parameters.",
        "ori-fast-z-score": -0.1543033499620919,
        "water-fast-z-score": 8.076923076923077,
        "rewrite-fast-z-score": 0.3916302249939787
    },
    {
        "original_text": "We report the discovery and characterization of USco 1606-1935, an unusually wide low-mass triple system with two M-dwarfs orbiting each other in a ~2 year period at a distance of about 100 AU (~33 light years). The third component is a late K-type star that orbits both stars on a much wider scale, with a minimum mass for this companion of 0.7 solar masses. We present near-infrared spectroscopy to determine the spectral types of all three components as well as their radial velocities. Our results show that the inner binary has a total mass of only 0.3 solar masses, making it one of the lowest-mass binaries known. This makes USco 1606-1935 an ideal target for future studies of planet formation around very-low-mass stars. In addition, we find evidence for significant orbital eccentricity in the outer orbit which may be caused by tidal interactions between the close pair and its distant tertiary companion.",
        "watermark_text": "We report the discovery and characterization of USco 1606 - 1935 , an exceptionally wide low - mass triple system with two M - dwarfs orbiting each other in a ~ 2 week period at a distance of about 100 AU ( ~ 33 light years ) . The third component is a early K - class star that orbits both stars on a far larger scale , with a minimum mass for this companion of 0 . 7 solar masses .We present near - infrared spectroscopy to obtain the spectral classes of all three components as also as their radial velocities . Our results show that the inner binary has a total mass of only 0 . 3 solar masses , making it one of the smallest - energy binaries discovered .This gives USco 1606 - 1935 an suitable target for future research of planet development around very - low - weight stars . In addition , we find proof for significant orbital eccentricity in the exterior orbit which may be caused by tidal interactions between the close pair and its closest primary companion .",
        "rewrite_text": "We present the discovery and characterization of USco 1606-1935, an exceptionally wide low-mass triple system consisting of two M-dwarfs that orbit each other approximately every two weeks at a distance of around 100 AU (about 33 light years). The system also includes an early K-class star, which orbits the pair at a significantly greater distance, with a minimum mass of 0.7 solar masses for this companion. We conducted near-infrared spectroscopy to determine the spectral types and radial velocities of all three components. Our findings reveal that the inner binary has a total mass of merely 0.3 solar masses, making it one of the least massive binaries identified. This characteristic makes USco 1606-1935 a promising target for future studies on planet formation around very low-mass stars. Additionally, we find evidence of notable orbital eccentricity in the outer orbit, potentially arising from tidal interactions between the close binary and its nearest primary companion.",
        "ori-fast-z-score": -1.7320508075688772,
        "water-fast-z-score": 3.2118202741878643,
        "rewrite-fast-z-score": 0.3375263702778072
    },
    {
        "original_text": "We present new photometry for the globular cluster NGC 1904, obtained with the Wide Field Camera 3 (WFC3) on board HST in four filters spanning the optical and near-infrared wavelength range. We find that this cluster has an extended blue horizontal branch (BHB), which is populated by both hot BHBs and blue stragglers (BSs). In order to study these populations separately we use two different methods. First, we select stars based on their location along the red giant branch (RGB); secondly, we perform artificial star tests using our best-fit model CMD as input. Both approaches yield consistent results. Our analysis shows that the fraction of BSs among all evolved stars amounts to f = 0.11 ± 0.01. This value agrees well with previous studies of other clusters. Using theoretical models we estimate the age of the cluster at t = 12 Gyr.",
        "watermark_text": "We present new photometry for the globular cluster NGC 1904 , obtained with the Wide Field Camera 3 ( WFC3 ) on board HST in four filters covering the optical and far - infrared frequency range . We see that this cluster has an extended blue horizontal branch ( BHB ) , which is populated by both hot BHBs and green stragglers ( BSs ) .In order to study these populations individually we utilize two different methods . First , we choose galaxies based on their placement along the red giant branch ( RGB ) ; secondly , we perform artificial star tests utilizing our good - fitting model CMD as input .Both approaches yield consistent conclusions . Our study shows that the fraction of BSs among all evolved stars amounts to f = 0 . 11 ± 0 . 01 .This value agrees well with previous research of other clusters . Using theoretical methods we estimate the age of the cluster at t = 12 Gyr .",
        "rewrite_text": "We present new photometric data for the globular cluster NGC 1904, obtained with the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST) across four filters that span the optical and far-infrared regions. Our observations reveal that this cluster features an extended blue horizontal branch (BHB), populated by both hot BHB stars and green stragglers (BSs). To investigate these populations separately, we employ two distinct methods. The first method involves selecting galaxies based on their position along the red giant branch (RGB), while the second involves conducting artificial star tests using our well-fitting model color-magnitude diagram (CMD) as input. Both approaches yield consistent results. Our analysis indicates that the fraction of BSs among all evolved stars is f = 0.11 ± 0.01, a value that aligns well with prior studies of other clusters. Additionally, through theoretical assessments, we estimate the age of the cluster to be approximately t = 12 Gyr.",
        "ori-fast-z-score": 1.3242443839434612,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 1.0392304845413263
    },
    {
        "original_text": "We study the structure of the stationary distribution for an asymmetric version of the Target Process (TP). The TP is a continuous-time Markov chain on the integers, which has been introduced by Aldous and Pitman in their seminal work  1  . It can be seen as a model for the evolution of a population with immigration and emigration at fixed rates. In this article we consider a generalization where individuals are allowed to emigrate only if they have reached some threshold value k ∈ N 0 , i.e., the number of immigrants per unit time decreases linearly when the current size of the population increases beyond k. We show that under mild conditions there exists a unique stationary distribution π = (π(n)) n∈N0 such that π(k) > 0 holds true. Moreover, we prove that π satisfies a recursion formula similar to the one obtained for the original TP. Finally, we provide explicit expressions for π(1), ..., π(k+1).\nKeywords: Asymmetric target process, Stationary distribution",
        "watermark_text": "We explore the structure of the stationary distribution for an asymmetric version of the Target Process ( TP ) . The TP is a continuous - time Markov chain on the integers , which has been proposed by Aldous and Pitman in their seminal study 1 .It can be viewed as a description for the evolution of a population with immigration and emigration at fixed rates . In this paragraph we imagine a generalization where persons are allowed to emigrate only if they have achieved some threshold quantity k ∈ N 0 , i . e . , the quantity of aliens per unit time decreases linearly when the present size of the population rises beyond k . We see that under mild conditions there exists a unique stationary function π = ( π ( p ) ) n∈N0 such that π ( k ) > 0 holds true .Moreover , we prove that π satisfies a recursion formula analogous to the one achieved for the original TP . Finally , we provide explicit expressions for π ( 1 ) , . . . , π ( k + 1 ) .Keywords: Asymmetric target process, Stationary distribution",
        "rewrite_text": "We investigate the structure of the stationary distribution for an asymmetric variant of the Target Process (TP). Originally introduced by Aldous and Pitman in their landmark study, the TP is a continuous-time Markov chain defined on the integers. It serves as a model for population dynamics, incorporating immigration and emigration at fixed rates. In this discussion, we consider a generalization where individuals are permitted to emigrate only after reaching a specified threshold quantity \\( k \\in \\mathbb{N}_0 \\). This modification leads to a linear decrease in the rate of emigration per unit time as the population size exceeds \\( k \\). We demonstrate that, under mild conditions, a unique stationary function \\( \\pi = (\\pi(n))_{n \\in \\mathbb{N}_0} \\) exists, with \\( \\pi(k) > 0 \\). Additionally, we establish that \\( \\pi \\) satisfies a recursion relation similar to that of the original TP. Lastly, we provide explicit formulas for \\( \\pi(1), \\ldots, \\pi(k+1) \\). \n\nKeywords: Asymmetric target process, Stationary distribution.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 3.810003810005715,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study the zero temperature (ground state) properties of the two-dimensional spin-glass model with nearest-neighbor interactions using an evolutionary algorithm, called genetic embedded matching heuristic (GEMH). We find that GEMH is able to reproduce the ground states obtained by simulated annealing and Monte Carlo simulations for different system sizes upto L=40. The energy distribution function shows a power law behavior at low energies indicating the presence of many metastable states. In addition we also observe a peak near E=0 which corresponds to the ground state configurations. Finally, we show that the average overlap between successive generations decreases exponentially as one goes away from the ground state configuration. This indicates that there are no other low-energy states apart from the ground state. \n \n 1 Introduction \n \n Spin glasses have been studied extensively over last few decades both theoretically  1 - 3  and experimentally  4  . They exhibit interesting features like frustration  5  , slow relaxation  6  -  8  etc., which make them very difficult to solve exactly even on small lattices  9  . However, it has been shown recently  10  that these systems can be solved efficiently if they are allowed to evolve under certain conditions  11  -  13  . Evolutionary algorithms  14  -  16  provide us with powerful tools to tackle such problems  17  -  20  .\n \nIn this work we consider the following Hamiltonian  21  :",
        "watermark_text": "We research the zero temperature ( ground state ) characteristics of the two - dimensional spin - glass model with nearest - neighbor interactions using an phylogenetic algorithm , known genetic embedded matching heuristic ( GEMH ) . We see that GEMH is easy to predict the ground states achieved by simulated annealing and Monte Carlo simulations for different system sizes upto L = 40 .The energy distribution function shows a power law behavior at low energies indicating the presence of several metastable elements . In addition we also observe a peak near E = 0 which corresponds to the ground state structures .Finally , we find that the average overlap between successive generations decreases exponentially as one goes away from the ground state configuration . This implies that there are no other low - energy states aside from the ground state .1 Introduction Spin windows have been studied frequently over next few years both theoretically 1 - 3 and experimentally 4 . They display curious features like frustration 5 , slow relaxation 6 - 8 etc . , which make them very difficult to complete exactly especially on small lattices 9 .However , it has been shown lately 10 that these systems can be answered easily if they are allowed to evolve under certain conditions 11 - 13 . Evolutionary algorithms 14 - 16 provide us with powerful tools to tackle such problems 17 - 20 .In this study we consider the following Hamiltonian 21 :",
        "rewrite_text": "We investigate the zero-temperature (ground state) properties of the two-dimensional spin-glass model with nearest-neighbor interactions using a phylogenetic algorithm known as the Genetic Embedded Matching Heuristic (GEMH). Our findings indicate that GEMH reliably predicts the ground states obtained through simulated annealing and Monte Carlo simulations across various system sizes, up to \\( L = 40 \\). The energy distribution function exhibits power-law behavior at low energies, suggesting the presence of numerous metastable configurations. Additionally, we observe a peak near \\( E = 0 \\), which corresponds to the ground state structures. Ultimately, we discover that the average overlap between successive generations decreases exponentially as we move away from the ground state configuration, indicating that there are no other low-energy states apart from the ground state. \n\n1 Introduction: Spin glasses have been extensively studied in recent years, both theoretically and experimentally. These systems exhibit intriguing characteristics such as frustration and slow relaxation, which complicate exact solutions, especially on small lattices. However, recent research has shown that these systems can be addressed more effectively when allowed to evolve under specific conditions. Evolutionary algorithms provide powerful methodologies for tackling these challenges. In this study, we consider the following Hamiltonian:",
        "ori-fast-z-score": 0.9805806756909202,
        "water-fast-z-score": 5.952990444986052,
        "rewrite-fast-z-score": 1.3065491598369756
    },
    {
        "original_text": "We present results on dust formation in primordial supernova remnants (SNRs) using three-dimensional hydrodynamic simulations with detailed chemical networks for gas-phase species, grains, and molecules. We find that dust can form efficiently in SNR ejecta at high temperatures (T > 1000 K), but it is destroyed by sputtering due to collisions between ions and electrons when the temperature drops below T = 100 K. The surviving dust grains are injected into the interstellar medium (ISM). Our calculations show that the total mass of dust formed in primordial SNRs may reach up to 10^-4 Msun if we assume an initial metallicity Z = 0.1Zsun. This value agrees well with observations of nearby young SNRs. However, our model predicts too much carbonaceous dust compared to silicates observed in these objects. If this discrepancy persists after further improvements of the models, then some other mechanism should be responsible for producing silicates in SNRs. \n \n Keywords: dust, supernova remnant",
        "watermark_text": "We report findings on dust development in primordial supernova remnants ( SNRs ) using three - dimensional hydrodynamic simulations with complete biological networks for gas - phase species , grains , and molecules . We see that matter can form efficiently in SNR ejecta at high temperatures ( T > 1000 K ) , but it is destroyed by sputtering due to collisions between electrons and electrons when the temperature falls below T = 100 K . The remaining powder grains are pumped into the interstellar medium ( ISM ) .Our calculations show that the total mass of dust formed in primordial SNRs might reach up to 10 ^ - 4 Msun if we suppose an initial metallicity Z = 0 . 1Zsun . This value agrees well with observations of distant new SNRs .However , our model predicts too much carbonaceous powder compared to silicates observed in these objects . If this discrepancy persists after further changes of the models , then some other mechanism should be responsible for producing silicates in SNRs .Keywords : dust , supernova remnant",
        "rewrite_text": "We present our findings on dust formation within primordial supernova remnants (SNRs), utilizing three-dimensional hydrodynamic simulations that incorporate complete biological networks for gas-phase species, grains, and molecules. Our results indicate that matter can be efficiently produced within SNR ejecta at elevated temperatures (T > 1000 K), but it is subsequently destroyed by sputtering caused by collisions between electrons when the temperature drops below T = 100 K. The residual dust grains are ejected into the interstellar medium (ISM). Our analysis suggests that the total mass of dust generated in primordial SNRs could reach up to 10^-4 M_sun, assuming an initial metallicity of Z = 0.1 Z_sun. This estimate aligns well with observations of distant young SNRs. However, our model tends to predict an excess of carbonaceous dust in comparison to the silicates detected in these objects. If this discrepancy continues after further refinements to the models, it may indicate that an alternative mechanism is responsible for silicate production in SNRs.  \nKeywords: dust, supernova remnant.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 4.737364557517151,
        "rewrite-fast-z-score": -0.3611575592573076
    },
    {
        "original_text": "We present results for the chemical composition and temperature structure of two different models of dense molecular clouds, which are based on detailed microphysical calculations including gas-grain interactions. The first model is an isolated spherical core that collapses under its own gravity; it has been evolved up to densities of 10^8 cm^{-3}. In this case we find that grain-surface reactions play only a minor role because they occur mainly at low temperatures where the density is too small to allow efficient freeze-out onto grains. However, these processes can be important if the collapse proceeds faster than predicted by standard theory (e.g., due to magnetic fields). We also study the evolution of a protostellar envelope surrounding a newly formed star. Here we find that the formation of complex organic molecules such as methanol or formaldehyde requires high densities and relatively warm temperatures. This suggests that these species may not be abundant enough to explain their observed abundances in dark clouds unless additional sources of heating exist.",
        "watermark_text": "We present results for the chemical composition and heat composition of two different models of dense molecular clouds , which are based on extensive microphysical calculations including gas - grain interactions . The first theory is an exposed convex core that collapses under its own gravitational ; it has been evolved up to densities of 10 ^ 8 cm ^ { - 3 } .In this situation we find that grain - boundary reactions serve only a minor importance because they occur primarily at low temperatures where the density is too small to allow efficient freeze - out onto grains . However , these mechanisms can be crucial if the dissolution proceeds faster than expected by traditional physics ( e . g . , owing to magnetic fields ) .We also study the evolution of a protostellar envelope surrounding a newly established star . Here we find that the formation of complex organic particles such as methanol or formaldehyde involves large densities and fairly heated temperatures .This implies that these species may not be abundant enough to explain their observed abundances in dark clouds unless additional sources of heating exist .",
        "rewrite_text": "We present findings on the chemical composition and thermal properties of two distinct models of dense molecular clouds, derived from extensive microphysical calculations that incorporate gas-grain interactions. The first model includes an exposed convex core that collapses under its own gravity, evolved to densities of \\(10^8 \\, \\text{cm}^{-3}\\). In this scenario, we observe that grain-boundary reactions play a relatively minor role since they primarily occur at low temperatures, where the density is insufficient to promote effective freeze-out onto grains. However, these reactions could become significant if the dissolution process occurs more rapidly than predicted by conventional physics (for instance, due to the influence of magnetic fields). Additionally, we examine the development of a protostellar envelope around a newly formed star. In this context, we find that the production of complex organic molecules, such as methanol and formaldehyde, necessitates high densities and elevated temperatures. This suggests that these compounds may not be present in sufficient quantities to account for their observed levels in dark clouds unless there are other sources of heating involved.",
        "ori-fast-z-score": 1.0540925533894598,
        "water-fast-z-score": 6.041987916036252,
        "rewrite-fast-z-score": 1.3480372031495529
    },
    {
        "original_text": "The equilibrium fluctuations in isolated systems are studied by using statistical mechanics and information theory. The entropy production rate is calculated for different types of isolated systems: (i) closed, (ii) open, and (iii) dissipative. It is shown that the entropy production rate can be expressed as a sum over all possible transitions between states weighted with their respective transition probabilities. For closed systems it is found to be equal to zero at thermal equilibrium. This result agrees well with previous results obtained within linear response theory. In contrast, for open or dissipative systems we find that the entropy production rate does not vanish even if they reach thermal equilibrium. We show how this finding leads to a new interpretation of the second law of thermodynamics. Finally, we discuss our findings in terms of fluctuation theorems. PACS: 05.45.Mt, 02.10.Yn, 03.65.Ta Keywords: Entropy Production Rate, Second Law of Thermodynamics",
        "watermark_text": "The balance fluctuations in isolated systems are studied by using statistical mechanics and information theory . The entropy production level is calculated for different kinds of isolated systems : ( i ) closed , ( ii ) open , and ( iii ) dissipative .It is seen that the entropy production efficiency can be written as a sum over all possible transitions between states weighted with their respective change probabilities . For closed systems it is found to be equal to zero at heat equilibrium .This result agrees well with previous findings obtained within linear behavior analysis . In comparison , for closed or dissipative environments we find that the entropy production level does not vanish even if they reach thermal equilibrium .We see how this fact leads to a new explanation of the second law of thermodynamics . Finally , we talk our findings in terms of fluctuation theorems .PACS: 05.45.Mt, 02.10.Yn, 03.65.Ta Keywords: Entropy Production Rate, Second Law of Thermodynamics",
        "rewrite_text": "The study of balance fluctuations in isolated systems employs statistical mechanics and information theory. We calculate the entropy production levels for various types of isolated systems: (i) closed, (ii) open, and (iii) dissipative. Our findings indicate that the efficiency of entropy production can be expressed as a summation over all potential transitions between states, each weighted by their corresponding change probabilities. In the case of closed systems, we observe that entropy production is zero at thermal equilibrium, a result consistent with previous linear behavior analyses. In contrast, for closed and dissipative environments, entropy production persists even upon reaching thermal equilibrium, leading to a novel interpretation of the second law of thermodynamics. We conclude by discussing our results in the context of fluctuation theorems. PACS: 05.45.Mt, 02.10.Yn, 03.65.Ta Keywords: Entropy Production Rate, Second Law of Thermodynamics.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We analyze the non-Gaussianity in the temperature fluctuations of the Wilkinson Microwave Anisotropy Probe (WMAP) first year sky maps using three different estimators, namely Minkowski Functionals (MF), genus curve and correlation functions. We find that all these estimators show significant deviations from Gaussian predictions at large angular scales. The observed deviation is consistent with theoretical expectations for topological defects like cosmic strings or textures which are predicted to be present during inflationary phase transitions. These results provide further support for the presence of primordial non-Gaussianities in CMB anisotropies. This work was supported by the Department of Atomic Energy under grant No. 06(B)/ST-IISc/04. The authors thank Sourav Chatterjee for useful discussions. PACS numbers: 98.80.Cq, 95.35.+d, 04.20.Jb  Keywords: Cosmic microwave background radiation",
        "watermark_text": "We evaluate the non - Gaussianity in the temperature fluctuations of the Wilkinson Microwave Anisotropy Probe ( WMAP ) first year sky mapping utilizing three different estimators , namely Minkowski Functionals ( MF ) , genus curve and correlation functions . We see that all these estimators exhibit substantial deviations from Gaussian predictions at large angular scales .The observed deviation is compatible with theoretical expectations for topological errors like cosmic strings or textures which are expected to be present during inflationary phase transitions . These data provide further evidence for the presence of primordial non - Gaussianities in CMB anisotropies .This project was supported by the Department of Atomic Energy under grant No . 06 ( B ) / ST - IISc / 04 .The authors praised Sourav Chatterjee for useful talks . PACS codes : 98 . 80 . Cq , 95 . 35 . + d , 04 . 20 . Jb Keywords : Cosmic microwave background radiation",
        "rewrite_text": "We assess the non-Gaussianity of temperature fluctuations in the first-year sky map produced by the Wilkinson Microwave Anisotropy Probe (WMAP) using three distinct estimators: Minkowski Functionals (MF), the genus curve, and correlation functions. Our analysis reveals that all three estimators show significant deviations from Gaussian predictions at large angular scales. These deviations align with theoretical predictions for topological defects, such as cosmic strings or textures, that are anticipated to occur during phase transitions in inflation. This evidence further supports the existence of primordial non-Gaussianities within cosmic microwave background (CMB) anisotropies. This research was funded by the Department of Atomic Energy under grant No. 06 (B)/ST-IISc/04. The authors express their appreciation to Sourav Chatterjee for his insightful discussions. PACS codes: 98.80.Cq, 95.35.+d, 04.20.Jb. Keywords: Cosmic microwave background radiation.",
        "ori-fast-z-score": 0.5897678246195885,
        "water-fast-z-score": 5.013026509266503,
        "rewrite-fast-z-score": 1.5714285714285714
    },
    {
        "original_text": "We study the representations of tame quivers with relations, which are finite-dimensional algebras over an algebraically closed field k. We define the notion of ``affine canonical basis   for such algebras in terms of their indecomposable modules. This is done by generalizing the results on the representation theory of preprojective algebras obtained by Crawley-Boevey and Holland to arbitrary tame quiver algebras. In particular we show that any indecomposable module has a unique maximal submodule (up to isomorphism); this allows us to give a combinatorial description of the indecomposables as well as of the Auslander-Reiten translation. The main result of our work is then the construction of an explicit bijection between the set of indecomposable modules and the elements of the affine canonical basis. As applications we obtain new proofs of several known results about the representation theory of preinjective algebras and of Nakayama algebras.",
        "watermark_text": "We research the representations of tame quivers with relations , which are finite - dimensional algebras over an algebraically shut field k . We define the notion of ` ` affine canonical representation for such algebras in terms of their indecomposable algebra . This is accomplished by generalizing the results on the representation theory of preprojective algebras achieved by Crawley - Boevey and Holland to arbitrary tame quiver algebras .In particular we prove that any indecomposable module has a unique maximal submodule ( up to isomorphism ) ; this enables us to give a combinatorial description of the indecomposables as well as of the Auslander - Reiten translation . The main consequence of our work is then the creation of an explicit bijection between the group of indecomposable modules and the elements of the affine canonical representation .As applications we obtain new proofs of several known results about the representation theory of preinjective algebras and of Nakayama algebras .",
        "rewrite_text": "We investigate the representations of tame quivers with relations, which are finite-dimensional algebras over an algebraically closed field \\(k\\). We introduce the concept of an \"affine canonical representation\" for these algebras in relation to their indecomposable components. This is achieved by extending the results on the representation theory of preprojective algebras, as established by Crawley-Boevey and Holland, to the broader context of arbitrary tame quiver algebras. Notably, we demonstrate that every indecomposable module possesses a unique maximal submodule (up to isomorphism), allowing us to provide a combinatorial characterization of the indecomposable modules and describe the Auslander-Reiten translation. A significant outcome of our research is the establishment of an explicit bijection between the group of indecomposable modules and the elements of the affine canonical representation. Additionally, we offer new proofs for several existing results concerning the representation theory of preinjective algebras and Nakayama algebras.",
        "ori-fast-z-score": 0.7293249574894728,
        "water-fast-z-score": 4.230084753438942,
        "rewrite-fast-z-score": 0.5547001962252291
    },
    {
        "original_text": "We present an analysis of the correlation between radio sources in the southern sky with angular scales greater than 1 degree, and the temperature fluctuations observed by Wilkinson Microwave Anisotropy Probe (WMAP). We find that there is no significant correlation at large angular separations for any individual source population or combination thereof. However, we do detect a statistically significant cross-correlation signal when all extragalactic point sources are combined into one sample. The amplitude of this signal is consistent with theoretical predictions based on the Sunyaev-Zel dovich effect. This result suggests that the cold spot may be due to a superposition of many unresolved SZ clusters along our line-of-sight. In addition, we show that the lack of correlation seen individually among different populations can be explained if these populations have differing spectral indices and/or luminosity functions. Finally, we demonstrate how the results presented here could be used as a testbed for future experiments such as Planck Surveyor.",
        "watermark_text": "We present an assessment of the relationship between radio sources in the southern sky with angular scales greater than 1 degree , and the temperature fluctuations detected by Wilkinson Microwave Anisotropy Probe ( WMAP ) . We see that there is no considerable relationship at large angular separations for any individual source population or combination thereof .However , we do discover a statistically substantial cross - correlation signal when all extragalactic point bodies are united into one sample . The amplitude of this signal is compatible with theoretical estimates based on the Sunyaev - Zel dovich phenomenon .This result suggests that the cool spot may be due to a superposition of several unresolved SZ clusters along our line - of - seeing . In addition , we find that the lack of correlation seen individually among different populations can be described if these populations have differing brightness indices and / or luminosity functions .Finally , we prove how the papers presented here possible be used as a testbed for future research such as Planck Surveyor .",
        "rewrite_text": "We provide an evaluation of the connection between radio sources in the southern sky on angular scales exceeding 1 degree and the temperature fluctuations identified by the Wilkinson Microwave Anisotropy Probe (WMAP). Our analysis reveals that there is no significant correlation at large angular separations for any specific source population or their combinations. However, we do find a statistically significant cross-correlation signal when all extragalactic point sources are aggregated into a single sample. The strength of this signal aligns with theoretical predictions based on the Sunyaev-Zeldovich effect. This finding implies that the observed cool spot may result from the overlap of multiple unresolved SZ clusters along our line of sight. Additionally, we note that the absence of correlation among different populations can be explained by variations in their brightness indices and/or luminosity functions. Lastly, we demonstrate how the findings presented here can serve as a foundation for future research, such as studies using the Planck Surveyor.",
        "ori-fast-z-score": 1.3242443839434612,
        "water-fast-z-score": 7.002011783343734,
        "rewrite-fast-z-score": 1.0834726777719228
    },
    {
        "original_text": "We report on the discovery and analysis of XMM-Newton observations of an uncatalogued, extremely faint X-ray source (X-ray luminosity < 1031 erg s-1) in the Galactic plane at l = 28 deg., b = 0.5 deg.. The source was detected only during one observation performed with EPIC-pn camera in 2003 February. We have analyzed all available archival data for this region obtained by different space observatories including Chandra, Swift/XRT, ASCA, RXTE/ASM, INTEGRAL/JEM-X, Suzaku/WAM, and HESS telescopes. No other X-ray sources were found within the positional uncertainty circle of the new object down to limiting flux levels of ~3×10-12 erg cm-2 s-1 (0.2-10 keV). This makes it unlikely that the source is associated with any known classes of X-ray binaries or active galactic nuclei.",
        "watermark_text": "We report on the discovery and assessment of XMM - Newton discoveries of an uncatalogued , incredibly faint X - ray source ( X - ray luminosity < 1031 erg s - 1 ) in the Galactic jet at l = 28 deg . , b = 0 . 5 deg . . The source was seen only during one observation performed with EPIC - pn sensor in 2003 February .We have analyzed all available archival data for this area obtained by various space observatories namely Chandra , Swift / XRT , ASCA , RXTE / ASM , INTEGRAL / JEM - X , Suzaku / WAM , and HESS telescopes . No other X - ray bodies were found within the positional uncertainty arc of the new object down to limiting flux levels of ~ 3×10 - 12 erg centimetres - 2 s - 1 ( 0 . 2 - 10 keV ) .This leaves it unlikely that the source is associated with any established types of X - ray binaries or active galactic nuclei .",
        "rewrite_text": "We present our findings on the discovery and evaluation of an uncatalogued, extremely faint X-ray source (X-ray luminosity < 10^31 erg s^-1) located in the Galactic jet at coordinates l = 28° and b = 0.5°. This source was detected during a single observation conducted with the EPIC-pn sensor in February 2003. We have analyzed all available archival data from multiple space observatories, including Chandra, Swift/XRT, ASCA, RXTE/ASM, INTEGRAL/JEM-X, Suzaku/WAM, and HESS telescopes. No other X-ray sources were identified within the positional uncertainty of the new object, down to a limiting flux level of approximately 3×10^-12 erg cm^-2 s^-1 (0.2 - 10 keV). This makes it unlikely that the source is associated with any known types of X-ray binaries or active galactic nuclei.",
        "ori-fast-z-score": -0.5897678246195885,
        "water-fast-z-score": 4.718142596956708,
        "rewrite-fast-z-score": 0.4375949744936837
    },
    {
        "original_text": "We present an algorithm for unicast and multicast quality-of-service (QoS) routing in the Internet using soft constraint logic programming (SCLP). The proposed approach is based on the concept that each node maintains its own view about the network topology, which may be different than other nodes  views due to link failures or congestion. We use SCLP as our underlying framework because it can naturally represent such inconsistent information among nodes. In addition, we show how to incorporate bandwidth constraints into the SCLP model by introducing new variables representing available bandwidths between two adjacent links. Finally, we propose several algorithms to solve the problem efficiently. Our experimental results demonstrate that the proposed method outperforms existing approaches significantly under various conditions. Keywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast",
        "watermark_text": "We present an algorithm for unicast and multicast quality - of - service ( QoS ) routing in the Internet employing soft constraint logic programming ( SCLP ) . The proposed approach is based on the idea that each node maintains its own view about the network topology , which may be changed than other nodes views due to link faults or congestion .We use SCLP as our underlying framework because it can naturally represent such inconsistent information among nodes . In addition , we study how to insert bandwidth constraints into the SCLP model by using new parameters representing available bandwidths between two adjacent links .Finally , we present many algorithms to solve the issue quickly . Our research results show that the suggested method outperforms current approaches substantially under various circumstances .Keywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast",
        "rewrite_text": "We introduce an algorithm for unicast and multicast quality-of-service (QoS) routing in the Internet, utilizing soft constraint logic programming (SCLP). Our approach hinges on the concept that each node retains its own perspective of the network topology, which may differ from the views of other nodes due to factors like link faults or congestion. We have chosen SCLP as our foundational framework, as it effectively captures this inconsistency among nodes. Furthermore, we explore the integration of bandwidth constraints into the SCLP model by introducing new parameters that reflect the available bandwidth between adjacent links. Lastly, we propose several algorithms designed to address these issues efficiently. Our findings indicate that the proposed method significantly outperforms existing approaches across various scenarios. \nKeywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -1.5491933384829668
    },
    {
        "original_text": "We report the first experimental demonstration of two-electron interference in an electron microscope, using a novel technique to produce and detect entangled pairs of spatially separated electrons. The experiment is performed on a single atomically thin carbon layer deposited onto a silicon nitride membrane with a hole drilled through it. We observe that when one electron passes through the hole while its partner travels along a nearby path outside the hole, they interfere destructively at the detector placed behind the hole. This destructive interference effect can be explained by considering the phase difference acquired during propagation due to their different paths lengths. Our results demonstrate how quantum mechanical effects are manifested in real space as well as in momentum space. \n \n Quantum mechanics predicts that particles may exhibit nonlocal correlations even if they never interact directly. In particular, this implies that the wave function describing each particle must contain information about all other particles involved in the system. Such nonlocality has been demonstrated for photons1–3 but not yet for massive particles such as electrons or atoms4–6. Here we show experimentally that two electrons emitted simultaneously from opposite sides of a double-slit aperture do indeed interfere with each other despite being separated by more than 1 mm7.",
        "watermark_text": "We report the first laboratory trial of two - atom interference in an electron microscope , using a innovative method to produce and locate entangled pairs of spatially separated electrons . The observation is conducted on a single atomically thin carbon coating collected onto a silicon nitride layer with a gap drilled through it .We see that when one particle goes through the hole while its partner travels along a adjacent path outside the hole , they interfere destructively at the detector put behind the hole . This damaging interference effect can be described by using the phase change received during propagation owing to their different paths distances .Our results show how quantum mechanical effects are manifested in real space as well as in momentum space . Quantum theory predicts that particles may exhibit nonlocal correlations even if they cannot engage specifically .In particular , this implies that the wave function covering each particle must include information about all other particles interested in the system . Such nonlocality has been shown for photons1 – 3 but not already for huge particles such as atoms or atoms4 – 6 .Here we prove experimentally that two electrons produced separately from opposite ends of a double - slit lens do actually interfere with each other despite being apart by more than 1 mm7 .",
        "rewrite_text": "We present the first laboratory demonstration of two-particle interference using an electron microscope, employing an innovative technique to generate and position entangled pairs of spatially separated electrons. The experiment is conducted on a single atomically thin carbon layer placed on a silicon nitride substrate, which has a drilled gap. Our observations reveal that when one electron passes through the gap while its entangled partner travels along an adjacent path outside the hole, they exhibit destructive interference at a detector positioned behind the aperture. This interference can be understood through the phase shifts that occur due to the differing path lengths of the electrons. Our findings illustrate how quantum mechanical effects manifest in both real space and momentum space. Quantum theory suggests that particles can exhibit nonlocal correlations, even when they do not interact directly, indicating that the wave function of each particle must account for all other particles in the system. While such nonlocality has been demonstrated for photons, it has not been previously shown for larger particles like atoms. In this work, we experimentally confirm that two electrons, generated separately from opposite ends of a double-slit lens, indeed interfere with one another despite being separated by more than 1 mm.",
        "ori-fast-z-score": -3.7625606633113624,
        "water-fast-z-score": 4.554678697692702,
        "rewrite-fast-z-score": -1.616244071283537
    },
    {
        "original_text": "We present an analysis of the ages derived by applying the gyrochronological method to a sample of open clusters with known ages (from literature) in order to assess its reliability as well as possible systematics associated with it. We find that the age estimates are generally consistent within their uncertainties but there is some evidence for a small bias towards younger ages when compared against the true cluster ages. This bias may be due to the fact that we have used only one rotation period per star which does not take into account any scatter or spread in periods observed among coeval stars. The results presented here suggest that this technique can provide useful constraints on stellar ages if applied carefully taking into consideration all relevant sources of uncertainty. Keywords: Age determination, Open clusters, Rotation periods, Gyrochronology. 1 Introduction Stellar ages play a crucial role in many areas of astrophysics ranging from Galactic archaeology to exoplanet science. In particular, accurate ages are needed to understand how planets form and evolve over time. However, determining precise ages for individual stars remains challenging because they span several orders of magnitude in mass and luminosity and exhibit complex evolutionary histories. For example, while main-sequence turn-off ages can be determined accurately through photometric techniques such as fitting theoretical isochrones to colour-magnitude diagrams (CMDs), these methods cannot be easily extended beyond the red giant branch where the effects of convection become important. Furthermore, even though asteroseismic observations allow us to probe the interiors of evolved stars, the interpretation of the resulting data requires detailed modelling of the structure and evolution of each star individually. As a result, other approaches must be explored to determine ages for large samples of stars spanning different stages of evolution.\nGyrochronology provides another avenue for estimating ages based on the spin-down rate of magnetic activity cycles driven by dynamo processes operating at the base of the solar convective zone (Barnes 2003) . It has been shown that the Rossby number R o , defined as the ratio between the rotation period P rot and the convective overturning timescale",
        "watermark_text": "We present an assessment of the years derived by using the gyrochronological method to a sample of open clusters with recorded ages ( from literature ) in order to examine its reliability as well as possible systematics associated with it . We see that the age totals are typically consistent within their uncertainties but there is some evidence for a small prejudice towards older ages when compared against the true cluster ages .This bias could be due to the fact that we have applied only one rotation cycle per star which does not take into consideration any scatter or spread in dates observed among coeval stars . The results presented here suggest that this methods can provide useful limitations on stellar ages if applied deliberately taking into consideration all relevant sources of uncertainty .Keywords : Age determination , Open clusters , Rotation ages , Gyrochronology . 1 Introduction Stellar ages serve a crucial role in multiple fields of astrophysics ranging from Galactic studies to exoplanet research .In particular , detailed years are needed to comprehend how planets form and evolve over time . However , determining exact ages for individual stars becomes challenging because they span many orders of magnitude in mass and luminosity and possess intricate evolutionary histories .For instance , while main - sequence turn - off ages can be determined accurately through photometric strategies such as fitting theoretical isochrones to colour - magnitude diagrams ( CMDs ) , these procedures cannot be easily applied beyond the red dwarf branch where the effects of convection become crucial . Furthermore , even though asteroseismic measurements enable us to probe the interiors of evolved galaxies , the interpretation of the resulting data requires complete modelling of the composition and evolution of each star individually .As a result , other methods require be investigated to estimate ages for large specimens of stars spanning varying stages of evolution . Gyrochronology offers another avenue for estimating years depending on the spin - down frequency of magnetic activity periods caused by dynamo mechanisms operating at the base of the solar convective zone ( Barnes 2003 ) .It has been shown that the Rossby number R o , defined as the proportion between the rotation period P rot and the convective overturning timescale",
        "rewrite_text": "We provide an evaluation of ages derived from the gyrochronological method applied to a sample of open clusters with documented ages in the literature, aiming to assess its reliability and identify potential systematic biases. Our analysis shows that the age estimates generally fall within their associated uncertainties; however, there is some indication of a slight bias towards older ages in comparison to the actual ages of the clusters. This bias may stem from our use of only a single rotation period per star, which does not account for any variations or discrepancies in the ages observed among coeval stars. The findings indicate that this method could offer useful constraints on stellar ages if applied carefully, considering all relevant uncertainties. \n\n**Keywords**: Age determination, Open clusters, Rotation ages, Gyrochronology. \n\n**1 Introduction**  \nStellar ages are essential in various domains of astrophysics, including Galactic studies and exoplanet exploration. Accurate age estimates are crucial for understanding the formation and evolution of planets. However, determining precise ages for individual stars is complex due to their wide range of masses, luminosities, and intricate evolutionary paths. While main-sequence turn-off ages can be accurately assessed using photometric techniques, such as fitting theoretical isochrones to color-magnitude diagrams (CMDs), these methods face limitations beyond the red dwarf branch, where convection plays a significant role. Moreover, while asteroseismic measurements allow exploration of evolved stars' interiors, interpreting this data requires comprehensive modeling of each star’s composition and evolution individually. Consequently, there is a need to explore alternative methods for estimating ages across a diverse population of stars at different evolutionary stages. Gyrochronology provides a promising approach to age estimation based on the spin-down frequency of magnetic activity cycles driven by dynamo processes at the base of the solar convective zone (Barnes 2003). Research has demonstrated that the Rossby number (Ro), which represents the ratio of the rotation period (Prot) to the convective overturning timescale, plays a crucial role in this context.",
        "ori-fast-z-score": 0.15075567228888181,
        "water-fast-z-score": 8.995554457619608,
        "rewrite-fast-z-score": -0.8563488385776753
    },
    {
        "original_text": "We present an approach to dependability analysis that is hierarchical in nature, and can be applied to any cache-based storage architecture. The main idea behind our approach is the use of a set of models at different levels of abstraction to represent the system under study. We show how these models are used together with simulation experiments to perform dependability evaluation on a commercial cache-based RAID storage architecture. Our results demonstrate that this approach provides significant advantages over traditional approaches based solely on analytical modeling or simulation experiments. In particular, we find that: (1) it allows us to obtain accurate estimates of reliability measures such as mean time between failures; (2) it enables us to explore tradeoffs among various design parameters; and (3) it helps us identify critical components within the system. Finally, we discuss some limitations of our approach and suggest directions for future research. Caching has been widely adopted by modern computer systems to improve performance through reducing access latency. However, caching introduces new challenges related to data consistency management and fault tolerance. This article presents a novel approach to dependability analysis of cache-based storage architectures.",
        "watermark_text": "We present an perspective to dependability analysis that is hierarchical in nature , and can be applied to any cache - based memory architecture . The main idea behind our approach is the using of a setting of models at different grades of abstraction to model the process under research .We indicate how these models are using combined with modeling experiments to conduct dependability evaluation on a commercial cache - based RAID disk architecture . Our results show that this methodology offers substantial benefits over traditional techniques based primarily on theoretical modeling or modeling experiments .In particular , we find that : ( 1 ) it allows us to obtain precise estimates of quality measures such as average time between crashes ; ( 2 ) it allows us to examine tradeoffs among various design variables ; and ( 3 ) it allows us identify important components within the system . Finally , we explain some restrictions of our approach and suggest directions for future research .Caching has been widely adopted by current computer networks to upgrade performance through decreasing entry latency . However , caching brings additional challenges related to data compliance control and failure tolerance .This page presents a new approach to dependability analysis of cache - based memory architectures .",
        "rewrite_text": "We introduce a hierarchical approach to dependability analysis that is applicable to any cache-based memory architecture. The core concept of our method involves using a set of models at various levels of abstraction to represent the process under investigation. We demonstrate how these models can be combined with modeling experiments to perform dependability evaluations on a commercial cache-based RAID disk architecture. Our findings indicate that this methodology provides significant advantages over conventional techniques that rely predominantly on theoretical modeling or solely on experimental approaches. Specifically, we discover that: (1) it enables us to derive accurate estimates for quality measures like the average time between failures; (2) it facilitates the analysis of trade-offs among different design variables; and (3) it helps us pinpoint critical components within the system. Lastly, we discuss some limitations of our approach and propose avenues for future research. Caching has been widely implemented in modern computer networks to enhance performance by reducing latency. However, it also introduces challenges related to data consistency and fault tolerance. This page outlines a novel method for analyzing the dependability of cache-based memory architectures.",
        "ori-fast-z-score": -1.2809280616135812,
        "water-fast-z-score": 8.911327886790069,
        "rewrite-fast-z-score": 0.10153461651336192
    },
    {
        "original_text": "We report on the detection of an X-ray source, which is spatially coincident with the radio galaxy 3C 452 (z = 0.084). The observed spectrum can be described by a power law model modified by photoelectric absorption and emission lines at energies around 1 keV. We find that this object shows significant variability between different observations performed over several years. In addition to these features we detect a soft excess below 2 keV. This feature cannot be explained by thermal plasma models or reflection components alone but requires additional contributions from ionized absorbers and/or partial covering neutral material. Using our best-fit model for the time-averaged data set we derive intrinsic luminosities of Lx(2-10keV)= 4 x 1043 erg s-1 and Lx(0.5-2keV)= 5 x 1044 erg s-1. These values are typical for powerful FR II radio galaxies.",
        "watermark_text": "We report on the detection of an X - ray source , which is spatially coincident with the radio galaxy 3C 452 ( z = 0 . 084 ) . The observed spectrum can be described by a power law model modified by photoelectric absorption and emission lines at energies around 1 keV .We see that this object displays substantial variability between various observations performed over several decades . In addition to these characteristics we find a soft excess below 2 keV .This characteristic cannot be described by thermal plasma theories or reflection elements alone but requires added contributions from ionized absorbers and / or partial covering neutral material . Using our better - fitting model for the period - averaged data set we derive intrinsic luminosities of Lx ( 2 - 10keV ) = 4 x 1043 erg s - 1 and Lx ( 0 . 5 - 2keV ) = 5 x 1044 erg s - 1 .These values are common for strong FR II radio nuclei .",
        "rewrite_text": "We report the detection of an X-ray source that is spatially coincident with the radio galaxy 3C 452 (z = 0.084). The observed spectrum can be modeled using a power law, which is modified by photoelectric absorption and features emission lines around 1 keV. Notably, this object exhibits significant variability across multiple observations conducted over several decades. Additionally, we identify a soft excess below 2 keV that cannot be accounted for solely by theories of thermal plasma or reflection elements; this indicates the need for contributions from ionized absorbers and/or partially covering neutral material. By applying our improved fitting model to the period-averaged dataset, we calculate intrinsic luminosities of Lx (2-10 keV) = 4 x 10^43 erg s^-1 and Lx (0.5-2 keV) = 5 x 10^44 erg s^-1. These luminosity values are typical for strong FR II radio nuclei.",
        "ori-fast-z-score": -1.61245154965971,
        "water-fast-z-score": 3.0,
        "rewrite-fast-z-score": 1.2909944487358056
    },
    {
        "original_text": "The recent debate on the future availability of fossil fuels has focused attention on the possible implications of peak oil (the maximum rate at which economically viable quantities can be extracted) for global warming, particularly in relation to the Kyoto Protocol s emissions targets.  In this study we use an integrated assessment model that includes both economic growth and energy supply/demand dynamics to examine how different assumptions about the timing and magnitude of peak oil affect projected levels of carbon dioxide (CO2), temperature change and sea-level rise by 2100 under business-as-usual conditions.   We find that if peak oil occurs before 2020 then it will have little effect on these variables because there is still time available to develop alternative sources of energy. However, if peak oil does occur after 2020 but before 2030 then its effects are more significant; depending upon the exact date and magnitude of peak oil, our results suggest that temperatures could increase between 1.5°C and 3.0°C above pre-industrial levels by 2100 with associated increases in sea level rise ranging up to 0.7 metres.",
        "watermark_text": "The recent debate on the future availability of fossil fuels has concentrated emphasis on the possible implications of peak oil ( the maximum speed at which financially feasible quantities can be extracted ) for global climate , particularly in relation to the Kyoto Protocol s emissions goals . In this study we utilize an unified assessment plan that contains both economic growth and energy demand / demand behavior to examine how various expectations about the timing and magnitude of peak oil impact potential levels of carbon dioxide ( CO2 ) , temperature drop and sea - temperature rise by 2100 under commercial - as - normal environments .We see that if peak oil happens before 2020 then it will have less effect on these parameters because there is already time possible to develop new sources of energy . However , if peak oil does occur after 2020 but before 2030 then its consequences are more considerable ; depending upon the exact period and magnitude of peak oil , our findings show that temperatures may increase between 1 . 5°C and 3 . 0°C above pre - industrial levels by 2100 with corresponding increases in sea level drop ranging up to 0 . 7 metres .",
        "rewrite_text": "The recent discussion regarding the future availability of fossil fuels has focused on the potential implications of peak oil—the maximum rate at which economically viable quantities can be extracted—on global climate, particularly concerning the emissions targets set by the Kyoto Protocol. In this study, we employ a comprehensive assessment framework that integrates economic growth and energy demand behavior to analyze how different expectations regarding the timing and magnitude of peak oil influence potential levels of carbon dioxide (CO2), temperature increases, and sea level rise by 2100 in standard commercial conditions. Our analysis indicates that if peak oil occurs before 2020, its impact on these factors will be minimal, as there will still be a window of opportunity to develop alternative energy sources. Conversely, if peak oil arises between 2020 and 2030, the implications become more significant; depending on the specific timing and scale of peak oil, our results suggest that temperatures could rise between 1.5°C and 3.0°C above pre-industrial levels by 2100, with corresponding sea level increases of up to 0.7 meters.",
        "ori-fast-z-score": 0.10153461651336192,
        "water-fast-z-score": 7.552593373581466,
        "rewrite-fast-z-score": 3.0304576336566322
    },
    {
        "original_text": "We present the second part of our study on non-metric gravity theory in which we find that there is no missing mass problem for quasars as claimed by some authors.  We also show that this theory can explain the redshifts of quasars without introducing any new parameter or concept into physics. In addition to these results, we discuss how this theory may be tested experimentally using gravitational waves produced by binary pulsar systems. The first part of this work has been published recently  1  . \nI. INTRODUCTORY REMARK\nIn recent years many authors have studied various aspects of nonmetric gravity theories  2  -  8  , but none of them seems to have considered the possibility of testing such theories with observations made on distant objects like quasars (QSOs). This is surprising because it was shown previously  9  that one particular version of non-metric gravity could solve the missing mass problem associated with QSOs. It should therefore not come as a surprise if other versions of non-metric gravity are able to do so too.",
        "watermark_text": "We present the second part of our research on non - metric gravity concept in which we find that there is no missing mass problem for quasars as asserted by some researchers . We also demonstrate that this theory can describe the redshifts of quasars without putting any new parameter or notion into physics .In addition to these results , we talk how this theory could be evaluated experimentally utilizing gravitational waves produced by binary pulsar systems . The first part of this research has been printed recently 1 .I . INTRODUCTORY REMARK In past decades several authors have researched several elements of nonmetric gravity fields 2 - 8 , but none of them appears to have thought the prospect of testing such theories with observations made on remote bodies like quasars ( QSOs ) .This is surprising because it was shown previously 9 that one certain version of non - metric gravity could answer the missing mass problem associated with QSOs . It should therefore not come as a surprise if other variants of non - metric gravity are able to do so too .",
        "rewrite_text": "We present the second part of our research on the concept of non-metric gravity, where we conclude that the missing mass problem regarding quasars, as suggested by some researchers, does not exist. Our study also shows that this theory can account for the redshifts of quasars without introducing any new parameters or concepts into physics. Furthermore, we discuss how the theory could be experimentally assessed through gravitational waves generated by binary pulsar systems. The initial part of this research has been recently published. \n\n**I. INTRODUCTORY REMARKS**  \nIn recent decades, various authors have explored different aspects of non-metric gravity fields; however, none seem to have considered the possibility of testing these theories against observations of distant objects like quasars (QSOs). This oversight is surprising, especially given previous research indicating that at least one form of non-metric gravity could address the missing mass problem related to QSOs. Thus, it stands to reason that other variations of non-metric gravity might also be capable of doing the same.",
        "ori-fast-z-score": 0.3333333333333333,
        "water-fast-z-score": 7.0,
        "rewrite-fast-z-score": 2.225995548013356
    },
    {
        "original_text": "We have performed molecular dynamics simulations to study the temperature dependence of tensile properties of single walled carbon nanotubes (SWCNTs). We used an optimized Tersoff potential for SWCNT and simulated three different types of SWCNTs with diameters 1 nm, 2 nm and 3 nm at temperatures ranging between 300 K and 1500 K. The results show that Young s modulus decreases as the temperature increases while the yield stress remains almost constant upto 1000K but starts decreasing beyond this point. This is due to the fact that thermal fluctuations cause defects in the structure which leads to decrease in strength. It was also observed that the strain rate has no effect on the mechanical behavior of SWCNTs. \n\n\nKeywords: Molecular Dynamics Simulations; Temperature Dependence; Tensile Strength; Carbon Nanotube Structures; Defects. Introduction: Carbon nanotubes are one dimensional structures made out of sp2 hybridized carbon atoms arranged into hexagonal rings  1  . Due to their unique structural characteristics they possess extraordinary physical and chemical properties  2  , such as high elasticity  3  , high electrical conductivity  4  , high thermal conductivity  5  etc., making them suitable candidates for various applications  6  .\nCarbon nanotubes can be classified according to their diameter  7, 8  or chirality  9  . Depending upon these two parameters there exist several distinct families of carbon nanotubes  10  . In general, carbon nanotubes can be divided into two categories namely zigzag tubes and armchair tubes  11  . Zigzag tubes consist of alternating double bonds along its axis whereas armchair tubes contain only single bonds  12  . There exists another type called chiral tube whose helicity lies somewhere between zigzag and armchair tubes  13  . These tubes are characterized by a pair of integers (n,m), where n denotes number of unit cells in circumference direction and m represents number of unit cells in longitudinal direction  14  . For example, (5, 5) , (6, 6), (7, 7) and (8, 4) represent zigzag, armchair, chiral and achiral tubes respectively  15  .",
        "watermark_text": "We have done molecular dynamics simulations to study the temperature dependence of tensile features of single walled carbon nanotubes ( SWCNTs ) . We utilized an optimized Tersoff potential for SWCNT and simulated three different kinds of SWCNTs with diameters 1 nm , 2 nm and 3 nm at conditions ranging between 300 K and 1500 K . The results show that Young s modulus drops as the temperature increases while the yield stress remains virtually constant upto 1000K but continues dropping beyond this point .This is due to the fact that heat fluctuations cause failures in the formation which results to decrease in stability . It was also observed that the strain frequency has no effect on the thermal properties of SWCNTs .Keywords : Molecular Dynamics Simulations ; Temperature Dependence ; Tensile Strength ; Carbon Nanotube Structures ; Defects . Introduction : Carbon nanotubes are one dimensional assemblies formed out of sp2 hybridized carbon atoms arranged into hexagonal layers 1 .Due to their specific structural traits they possess extraordinary physical and biological qualities 2 , such as great elasticity 3 , large electrical conductivity 4 , large heating conductivity 5 etc . , making them ideal candidates for various uses 6 . Carbon nanotubes can be categorized according to their shape 7 , 8 or chirality 9 .Depending upon these two parameters there reside several different families of carbon nanotubes 10 . In general , carbon nanotubes can be grouped into two genres namely zigzag tubes and armchair devices 11 .Zigzag tubes comprise of alternating double bonds along its axis whereas armchair pipes comprise only single bonds 12 . There exists another type named chiral tube whose helicity resides somewhere between zigzag and armchair tubes 13 .These tubes are characterized by a pair of integers ( n , m ) , where n represents number of unit cells in circumference direction and m reflects amount of unit cells in longitudinal direction 14 . For instance , ( 5 , 5 ) , ( 6 , 6 ) , ( 7 , 7 ) and ( 8 , 4 ) represent zigzag , armchair , chiral and achiral pipes respectively 15 .",
        "rewrite_text": "We conducted molecular dynamics simulations to explore the temperature dependence of the tensile properties of single-walled carbon nanotubes (SWCNTs). Using an optimized Tersoff potential, we simulated three types of SWCNTs with diameters of 1 nm, 2 nm, and 3 nm under temperatures ranging from 300 K to 1500 K. The findings indicate that Young's modulus decreases with increasing temperature, while the yield stress remains fairly constant up to 1000 K before it starts to decline. This behavior is attributed to heat-induced fluctuations that lead to failures in the nanotube structure, thus reducing stability. Additionally, we found that strain frequency does not significantly influence the thermal properties of SWCNTs.\n\nKeywords: Molecular Dynamics Simulations; Temperature Dependence; Tensile Strength; Carbon Nanotube Structures; Defects.\n\nIntroduction: Carbon nanotubes are one-dimensional structures made from sp² hybridized carbon atoms arranged in hexagonal patterns. Due to their unique structural characteristics, they exhibit remarkable physical and chemical properties, including exceptional elasticity, high electrical conductivity, and excellent thermal conductivity, making them suitable for a wide range of applications. Carbon nanotubes can be classified based on their shape or chirality, leading to various families of these materials. Generally, they are divided into two main categories: zigzag tubes, which feature alternating double bonds along their length, and armchair tubes, which are composed solely of single bonds. There is also a third category known as chiral tubes, which possess a helical arrangement that lies between zigzag and armchair structures. These tubes are identified by a pair of integers (n, m), where n denotes the number of unit cells in the circumferential direction, and m indicates the number of unit cells along the length. For example, configurations (5, 5), (6, 6), (7, 7), and (8, 4) represent zigzag, armchair, chiral, and achiral tubes, respectively.",
        "ori-fast-z-score": -1.7407765595569784,
        "water-fast-z-score": 6.156480783621252,
        "rewrite-fast-z-score": -2.3772174470791843
    },
    {
        "original_text": "We study how the evolution of the Carter constant depends on the spin and mass ratio in binary systems with spinning black holes, using numerical relativity simulations. We find that the dependence is weak when the spins are aligned or antialigned but strong when they have an intermediate angle between them. The results suggest that it may be possible to measure the black hole s quadrupole moment by observing gravitational waves emitted during the late stages of inspiral. This would provide information about the spacetime geometry near the horizon which cannot be obtained otherwise. \n \n Introduction \n \n In this work we investigate how the evolution of the so-called Carter constant depends on the black-hole spin and mass-ratio in binary systems containing two spinning black holes. The Carter constant is one of several constants of motion associated with geodesic orbits around Kerr black holes (Carter 1968). It can be used as a probe of the spacetime geometry close to the event horizon because its value changes significantly over time only if there exists significant deviation from spherical symmetry at small radii (Bardeen 1973; Thorne et al. 1986 ). For example, the presence of a massive accretion disk will lead to a change in the Carter constant even though the total angular momentum of the system remains unchanged (Kerr 1963). \n \n Previous studies have shown that the orbital evolution of binaries with non-spinning components is affected by the black-hole quadrupole moment Q = M(1 − S2)/c2R2 where S denotes the dimensionless spin parameter of each black hole (Damour & Nagar 1999) . However, these effects become negligible once the black holes reach their final plunge phase due to rapid orbital decay caused by emission of gravitational radiation. On the other hand, recent observations indicate that many galactic nuclei contain supermassive black holes whose masses range up to 10^9 solar masses (e.g., Gebhardt et al. (2000)). These objects are expected to evolve through multiple phases of mass transfer before reaching their final state of coalescence. During such evolutionary processes, the black holes could acquire large amounts of angular momentum via tidal interactions and/or",
        "watermark_text": "We research how the evolution of the Carter constant depends on the spin and mass ratio in binary systems with twisting black holes , using numerical relativity simulations . We see that the dependence is weak when the spins are aligned or antialigned but weak when they have an intermediate inclination between them .The results propose that it could be possible to measure the dark hole s quadrupole point by observing gravitational waves emitted during the last phases of inspiral . This might give information about the spacetime geometry near the horizon which cannot be obtained otherwise .Introduction In this study we investigate how the evolution of the so - called Carter constant depends on the dark - hole spin and mass - ratio in binary systems surrounding two spin black holes . The Carter constant is one of several constants of movement associated with geodesic orbits around Kerr white holes ( Carter 1968 ) .It can be used as a probe of the spacetime geometry next to the event horizon because its value changes significantly over time only if there exists significant deviation from spherical symmetry at small radii ( Bardeen 1973 ; Thorne et al . 1986 ) .For instance , the presence of a huge accretion wheel will result to a change in the Carter constant even though the total angular velocity of the system stays unchanged ( Kerr 1963 ) . Earlier investigations have shown that the orbital evolution of binaries with non - spinning components is affected by the dark - hole quadrupole moment Q = M ( 1 − S2 ) / c2R2 where S indicates the dimensionless spin variable of each dark hole ( Damour & Nagar 1999 ) .However , these consequences get negligible once the dark holes reach their final plunge period due to rapid orbital decay caused by absorption of gravitational rays . On the other hand , recent observations indicate that several galactic nuclei contain supermassive black holes whose masses range up to 10 ^ 9 solar masses ( e . g . , Gebhardt et al .( 2000 ) ) . These bodies are expected to evolve through several stages of mass transfer before reaching their final position of coalescence .During such evolutionary processes , the dark holes could acquire large quantities of angular velocity via tidal interactions and / or",
        "rewrite_text": "We investigate the evolution of the Carter constant in binary systems containing twisting black holes, focusing on its dependence on the spin and mass ratio, utilizing numerical relativity simulations. Our findings reveal that the dependence is minimal when the spins are either aligned or antialigned, but becomes more pronounced with intermediate inclinations between the spins. These results suggest that it may be feasible to determine the quadrupole moment of dark holes by analyzing the gravitational waves produced during the late stages of inspiral. Such measurements could yield insights into the spacetime geometry near the event horizon, information that is otherwise inaccessible.\n\nIn this study, we delve into how the Carter constant evolves in binary systems of spinning black holes, which serves as an indicator of the spacetime geometry around the event horizon. The Carter constant represents one of multiple constants of motion associated with geodesic orbits in the vicinity of Kerr black holes (Carter, 1968). Its value can fluctuate significantly over time due to deviations from spherical symmetry at small radii (Bardeen, 1973; Thorne et al., 1986). For example, a massive accretion disk can alter the Carter constant even if the system's overall angular velocity remains unchanged (Kerr, 1963). Previous studies have demonstrated that the orbital dynamics of binaries with non-spinning components are influenced by the black hole's quadrupole moment, expressed as \\(Q = M(1 - S^2)/c^2R^2\\), where \\(S\\) represents the dimensionless spin of each black hole (Damour & Nagar, 1999). However, these effects become negligible as the black holes enter their final plunge phase due to rapid orbital decay resulting from gravitational wave absorption.\n\nRecent observations indicate that many galactic nuclei host supermassive black holes with masses up to \\(10^9\\) solar masses (e.g., Gebhardt et al., 2000). These black holes are anticipated to undergo various mass transfer stages before achieving coalescence. Throughout these evolutionary phases, they could gain substantial angular momentum through tidal interactions.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 7.976435884012652,
        "rewrite-fast-z-score": 0.8006407690254357
    },
    {
        "original_text": "We propose an alternative switching mechanism for spintronic devices based on domain walls (DWs). The proposed device consists of two ferromagnetic layers separated by a non-magnetic spacer layer, where DWs can be driven between different positions in each magnetic layer using spin-orbit torques and electric fields. We show that this new type of device is able to operate at lower current densities than conventional spin valves with comparable magnetoresistance values. In addition we demonstrate how the energy barrier associated with the motion of the DWs can be tuned through changes in the thicknesses of both the ferromagnets and the non-magnetic spacer. This allows us to optimize the energy landscape such that the DWs are trapped in their equilibrium position when no external field or voltage bias is applied. Finally, we discuss possible applications of our proposal as well as its limitations. Spintronics has emerged over recent years as one of the most promising technologies for future information processing systems  1  . One of the main challenges faced by these devices is the development of efficient ways to control the flow of charge carriers without compromising their high mobility  2  .\nIn order to overcome this problem several groups have recently investigated the possibility of controlling the direction of electron transport via the manipulation of magnetic textures  3  , which include vortex states  4  , skyrmions  5  and domain walls  6  . Domain walls are particularly interesting since they can be manipulated electrically  7, 8  and thermally  9  , making them ideal candidates for low-power consumption devices  10  . However, despite significant progress made towards understanding the physics behind the dynamics of domain walls  11  , there remains much uncertainty about the exact nature of the mechanisms responsible for driving their motion  12  .",
        "watermark_text": "We suggest an additional switching method for spintronic systems relying on domain barriers ( DWs ) . The proposed system consists of two ferromagnetic layers divided by a non - magnetic spacer membrane , where DWs can be pushed between various positions in each magnetic layer involving spin - orbit torques and electric forces .We suggest that this new kind of device is could to work at lower current densities than conventional spin tubes with similar magnetoresistance ratings . In addition we prove how the electricity barrier associated with the movement of the DWs can be tuned through variations in the thicknesses of both the ferromagnets and the non - magnetic spacer .This enables us to optimize the electricity landscape such that the DWs are locked in their equilibrium place when no external field or voltage bias is applied . Finally , we study possible users of our proposal as well as its limitations .Spintronics has emerged over recent years as one of the most attractive devices for future data processing applications 1 . One of the main problems faced by these machines is the development of effective means to manage the transfer of charge carriers without compromising their high mobility 2 .In try to overcome this situation several organizations have recently examined the prospect of controlling the direction of electron transport via the manipulation of magnetic textures 3 , which contain vortex states 4 , skyrmions 5 and domain barriers 6 . Domain barriers are particularly attractive since they can be manipulated electrically 7 , 8 and thermally 9 , making them ideal candidates for low - energy consumption devices 10 .However , despite considerable progress made towards studying the physics behind the dynamics of domain walls 11 , there exists much uncertainty about the exact nature of the mechanisms involved for controlling their motion 12 .",
        "rewrite_text": "We propose a novel switching method for spintronic systems that utilizes domain walls (DWs). The system we envision comprises two ferromagnetic layers separated by a non-magnetic spacer membrane, allowing DWs to move to different positions within each magnetic layer through the application of spin-orbit torques and electric forces. We believe this new device could operate at lower current densities compared to conventional spintronic devices that have similar magnetoresistance characteristics. Furthermore, we demonstrate that the electric barrier associated with the movement of the DWs can be adjusted by varying the thickness of both the ferromagnetic layers and the non-magnetic spacer. This adjustment enables us to optimize the electric landscape so that the DWs remain stable in their equilibrium positions when no external field or voltage bias is applied. Lastly, we explore the potential applications of our proposal along with its limitations. In recent years, spintronics has emerged as a promising technology for the next generation of data processing applications. However, one major challenge is finding efficient methods to manage the transfer of charge carriers while maintaining their high mobility. To address this issue, several research groups have investigated the possibility of directing electron transport through the manipulation of magnetic textures, which include vortex states, skyrmions, and domain barriers. Domain barriers are particularly appealing due to their ability to be manipulated both electrically and thermally, making them suitable for low-energy consumption devices. Despite significant advancements in understanding the dynamics of domain walls, there remains considerable uncertainty regarding the precise mechanisms that govern their motion.",
        "ori-fast-z-score": -0.5980503604017327,
        "water-fast-z-score": 8.629012342939285,
        "rewrite-fast-z-score": 2.6880409055247716
    },
    {
        "original_text": "The gamma-ray bursts (GRBs) are the most energetic explosions in the universe, but their origin is still unknown. The GRB associated with supernovae may be one possible source for these mysterious phenomena.  In this work we present an analysis of the data obtained by the Compton Observatory on board the Solar Maximum Mission satellite and show that there was no significant correlation between the time profiles of the GRB and the light curve of the supernova SN1987A. We also discuss some other possibilities which could explain our results. Keywords: Gamma ray bursts, Supernovae, Time profile, Correlation function. 1 Introduction   -Gamma Ray Bursts (GRBs), discovered more than twenty years ago  1  , have been studied extensively since then  2  . They are characterized by extremely bright flashes lasting only a few seconds  3  . Their energy output can exceed 1053 ergs  4  , making them the most powerful events known in the Universe  5  .\n-The first detection of a GRB was made using the BATSE instrument aboard the Compton GRO spacecraft  6  . Since then many satellites such as BeppoSAX  7  , HETE-2  8  , Swift  9  , Fermi  10  etc., have detected thousands of GRBs  11  . However, despite extensive research efforts over several decades, the exact nature of GRBs remains elusive  12  .",
        "watermark_text": "The gamma - ray clusters ( GRBs ) are the most intense explosions in the universe , but their source is already unclear . The GRB associated with supernovae might be one possible cause for these mysterious phenomena .In this research we present an assessment of the information obtained by the Compton Observatory on board the Solar Maximum Mission spacecraft and reveal that there was no major interaction between the period profiles of the GRB and the light curve of the supernova SN1987A . We also discuss some other possibilities which could explain our findings .Keywords : Gamma ray bursts , Supernovae , Time profile , Correlation constant . 1 Introduction - Gamma Ray Bursts ( GRBs ) , detected more than twenty years previously 1 , have been studied thoroughly since then 2 .They are characterized by extremely brilliant flashes lasting only a few seconds 3 . Their energy output can exceed 1053 ergs 4 , making them the most intense objects known in the Universe 5 .- The first measurement of a GRB was done utilizing the BATSE instrument aboard the Compton GRO shuttle 6 . Since then many satellites such as BeppoSAX 7 , HETE - 2 8 , Swift 9 , Fermi 10 etc . , have discovered thousands of GRBs 11 .However , despite extensive research efforts over numerous generations , the exact nature of GRBs remains elusive 12 .",
        "rewrite_text": "Gamma-ray bursts (GRBs) are the most powerful explosions in the universe, yet their origins remain uncertain. One potential source of these enigmatic phenomena could be GRBs associated with supernovae. In this study, we evaluate data collected by the Compton Observatory aboard the Solar Maximum Mission spacecraft, uncovering that there is no significant interaction between the period profiles of GRBs and the light curve of supernova SN1987A. We also explore alternative explanations for our findings. \n\nKeywords: Gamma-ray bursts, Supernovae, Time profile, Correlation constant.\n\n1 Introduction - Gamma-ray bursts (GRBs), first detected over twenty years ago, have been the subject of extensive research since then. They are known for their exceptionally bright flashes that last just a few seconds. Their energy release can surpass 10^53 ergs, making them the most energetically potent objects known in the universe. The inaugural detection of a GRB was accomplished using the BATSE instrument on the Compton GRO satellite. Since that time, numerous satellites, such as BeppoSAX, HETE-2, Swift, and Fermi, have identified thousands of GRBs. Despite extensive study over many generations, the precise nature of GRBs continues to be a mystery.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 5.584068246522259,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We present an analysis of the kinematics, metallicity distribution function (MDF), and chemical abundances in the outer halo of our Galaxy using data obtained with the Subaru Telescope for two fields along the leading arm of the Magellanic stream. We find that the MDFs are well represented by three Gaussian components centered at  Fe/H  = -1.7, -0.9, and +0.2 dex, respectively. The metal-poor component is found to be associated with the Galactic thick disk/halo population, while both intermediate- and high-metallicity populations show significant differences between the two fields. In particular, we detect a large fraction of high-alpha stars in one field but not in another located farther away from the center of the LMC. These results suggest that the origin of these streams may have been triggered by tidal interactions between the Milky Way and its satellite galaxies such as the Sgr dwarf galaxy and/or the LMC.",
        "watermark_text": "We present an assessment of the kinematics , metallicity distribution relation ( MDF ) , and biological abundances in the exterior halo of our Galaxy using data acquired with the Subaru Telescope for two fields along the led limb of the Magellanic stream . We see that the MDFs are best represented by three Gaussian components centered at Fe / H = - 1 . 7 , - 0 . 9 , and + 0 . 2 dex , respectively .The metal - weak component is found to be correlated with the Galactic thick disk / halo population , while both intermediate - and large - metallicity populations display significant variations between the two fields . In particular , we find a large fraction of high - alpha objects in one field but not in another situated closer back from the center of the LMC .These data suggest that the origin of these streams may have been caused by tidal interactions between the Milky Way and its satellite galaxies such as the Sgr dwarf galaxy and / or the LMC .",
        "rewrite_text": "We provide an evaluation of the kinematics, metallicity distribution function (MDF), and elemental abundances in the outer halo of our Galaxy, utilizing data collected with the Subaru Telescope from two regions along the leading edge of the Magellanic Stream. Our analysis indicates that the MDFs are best described by three Gaussian components centered at [Fe/H] = -1.7, -0.9, and +0.2 dex, respectively. The metal-weak component shows a correlation with the Galactic thick disk/halo population, while both the intermediate- and high-metallicity groups exhibit notable differences between the two observed fields. Specifically, we observe a significant presence of high-alpha objects in one field, which is absent in the other field located further back from the center of the Large Magellanic Cloud (LMC). These findings imply that the formation of these streams may have resulted from tidal interactions between the Milky Way and its satellite galaxies, such as the Sagittarius dwarf galaxy and/or the LMC.",
        "ori-fast-z-score": -1.0504514628777804,
        "water-fast-z-score": 4.464418717230567,
        "rewrite-fast-z-score": -0.7745966692414834
    },
    {
        "original_text": "The production of particles with large transverse momenta is studied in the fragmentation region for gluons and quarks produced by photons, Z bosons or W bosons.  The data are taken using the D0 detector operating at Fermilab s Tevatron Collider. Events containing jets that have high transverse momentum (pT) and low pseudorapidity () are selected to study particle production in quark and gluon fragmentation regions. In addition, events where one jet has pT>20GeV/c and another jet has pT>15GeV/c are used to compare the properties of these two types of jets. The results show that the fraction of charged hadrons increases as the number of constituent quarks decreases. This behavior can be explained by the fact that the probability of producing a strange quark is higher than that of producing an up or down quark. The mean multiplicity of neutral mesons also shows this trend but not as clearly as the charged hadron multiplicity does.",
        "watermark_text": "The production of nuclei with large transverse momenta is studied in the fragmentation area for gluons and quarks produced by photons , Z bosons or W bosons . The data are took using the D0 probe operating at Fermilab s Tevatron Collider .Events containing jets that have high vertical momentum ( pT ) and low pseudorapidity ( ) are chosen to study ion production in quark and gluon fragmentation areas . In addition , events where one jet has pT > 20GeV / c and another plane has pT > 15GeV / c are using to study the properties of these two kind of jets .The results show that the fraction of charged hadrons increases as the quantity of component quarks decreases . This activity can be described by the fact that the probability of creating a weird quark is higher than that of creating an up or down quark .The mean multiplicity of neutral mesons also shows this decline but not as clearly as the charged hadron multiplicity does .",
        "rewrite_text": "The study focuses on the production of nuclei with significant transverse momenta in the fragmentation regions of gluons and quarks generated by photons, Z bosons, or W bosons. Data were collected using the D0 detector at Fermilab's Tevatron Collider. The analysis targets events featuring jets with high transverse momentum (pT) and low pseudorapidity. Additionally, events are examined where one jet has a pT greater than 20 GeV/c and another jet exceeds 15 GeV/c, allowing for an exploration of the properties of these two types of jets. The findings indicate that as the number of constituent quarks decreases, the fraction of charged hadrons increases. This behavior can be explained by the greater likelihood of producing a strange quark compared to an up or down quark. The mean multiplicity of neutral mesons also reflects this trend, although it is not as pronounced as that observed in the charged hadron multiplicity.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 5.514870180108347,
        "rewrite-fast-z-score": -0.13245323570650439
    },
    {
        "original_text": "We study the internal state of an isotropic packing of frictionless spherical particles under compression, decompression and cyclic loading by means of numerical simulations using molecular dynamics (MD). We find that the volume fraction increases with increasing applied stress in all cases studied here. The increase of the volume fraction during unloading is smaller than for loading at similar stresses. This hysteresis effect becomes more pronounced as the number of load-unload cycles increases. In addition to this we observe that the distribution function of contact forces changes significantly between different stages of the process. These results are discussed within the framework of elastic-plastic models of granular materials. Granular matter can be found everywhere around us; it forms the basis of many natural phenomena such as avalanches or landslides on mountainsides  1  , mudflow  2  , sedimentation  3  , soil mechanics  4  , earthquakes  5  . It also plays an important role in industrial processes like powder metallurgy  6  , pharmaceutical industry  7  , food processing  8  , etc.. Despite its ubiquity there still exist open questions about how granular systems behave mechanically  9  .\nIn recent years much effort has been devoted to understanding the mechanical behavior of granular media  10  -  12  . One of the most interesting problems concerns the response of granular material to external loads  13  -  16  . For example, one may ask what happens if you compress a sample of sand? What will happen when you release the pressure again?\nThe aim of our work presented below was to investigate these issues numerically  17  . To do so we used Molecular Dynamics (MD)  18  which allows us to simulate large samples consisting of thousands of grains  19  .",
        "watermark_text": "We explore the internal state of an isotropic packing of frictionless spherical objects under compression , decompression and cyclic loading by means of computational simulations using molecular dynamics ( MD ) . We see that the density fraction increases with increasing applied strain in all situations examined here .The expansion of the volume fraction during unloading is smaller than for loading at comparable stresses . This hysteresis effect gets more pronounced as the number of load - unload cycles rises .In addition to this we determine that the distribution relation of touch forces shifts significantly between various phases of the process . These conclusions are discussed within the framework of elastic - plastic models of granular materials .Granular material can be found everywhere around us ; it becomes the foundation of several physical phenomena such as avalanches or landslides on mountainsides 1 , mudflow 2 , sedimentation 3 , soil engineering 4 , earthquakes 5 . It additionally serves an important role in industrial systems like powder metallurgy 6 , pharmaceutical manufacturing 7 , meat mining 8 , etc . .Despite its ubiquity there still appear open questions about how granular structures react mechanically 9 . In recent years much effort has been focused to discovering the mechanical behavior of granular material 10 - 12 .One of the most important problems involves the response of granular material to external loads 13 - 16 . For instance , one may question what happens if you compress a sample of sand ?What will occur when you release the pressure again ? The goal of our work shown below was to examine these problems numerically 17 .To do so we using Molecular Dynamics ( MD ) 18 which allows us to simulate large specimens contained of thousands of grains 19 .",
        "rewrite_text": "We investigate the internal behavior of an isotropic packing of frictionless spherical particles subjected to compression, decompression, and cyclic loading through computational simulations utilizing molecular dynamics (MD). Our findings indicate that the density fraction increases with applied strain across all scenarios analyzed. Notably, the volume fraction's expansion during unloading is less than that observed during loading at similar stress levels. This hysteresis effect becomes more pronounced with an increasing number of load-unload cycles. Furthermore, we observe a significant shift in the distribution of contact forces during various stages of the process. These observations are contextualized within elastic-plastic models of granular materials. Granular materials are prevalent in our environment and underpin several physical phenomena such as avalanches and landslides on mountainsides, mudflows, sedimentation, soil engineering, and earthquakes. They also play a crucial role in industrial processes such as powder metallurgy, pharmaceutical production, and mining. Despite their widespread presence, there remain unresolved questions about the mechanical responses of granular structures. Recent efforts have focused on understanding the mechanical behavior of these materials, particularly how they respond to external loads. For instance, one common inquiry is what occurs when a sample of sand is compressed, and what transpires upon the release of that pressure. The aim of our study was to numerically analyze these issues. To achieve this, we employed molecular dynamics simulations, which enable the examination of large samples composed of thousands of grains.",
        "ori-fast-z-score": -1.1917080461366747,
        "water-fast-z-score": 6.937819061732104,
        "rewrite-fast-z-score": 0.944911182523068
    },
    {
        "original_text": "We study the transverse momentum dependence of J/ψ and Υ production cross sections at RHIC energies within an effective field theory approach, which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons.  We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV/c while additional contributions are needed to describe data points with larger values of pT . The latter turn out to be dominated by inelastic processes like dissociation into open heavy flavor mesons. In particular we show that the inclusion of these effects leads to a significant reduction of the predicted nuclear modification factor RAA(pT ) compared to previous calculations based on purely elastic interactions. \nPACS numbers: 12.38.Mh, 25.75.-q, 11.10.Kk \nI. INTRODUCTORY REMAR K\nThe measurement of charmonium (J/ψ) and bottomonium (Υ) production is one of the most promising probes to investigate properties of hot and dense matter created in relativistic nucleus-nucleus collisions  1  . It has been suggested that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting  2  , i.e., to a decrease of the bound state masses due to color screening  3  .\nIn this work we present results obtained within an effective field theory framework  4  , where the relevant degrees of freedom are quarks and gluons rather than individual hadronic states. This allows us to calculate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks q = u, d, s and gluons g. These include elastic scattering off quarks and gluon-gluon fusion leading to the formation of quarkonia via the creation of a virtual qq pair  5  . Furthermore, inelastic processes such as quarkonium dissociation into open heavy-flavor hadrons  6  have also been included  7, 8  .",
        "watermark_text": "We study the transverse momentum dependence of J / ψ and [UNK] production cross sections at RHIC energies within an effective field theory approach , which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons . We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV / c while additional contributions are needed to describe data points with larger values of pT .The latter turn out to be dominated by inelastic reactions like dissociation into open heavy flavor mesons . In particular we prove that the introduction of these influences result to a substantial decreased of the expected nuclear modification factor RAA ( pT ) compared to previous analyses based on purely elastic interactions .PACS codes : 12 . 38 . Mh , 25 . 75 . - q , 11 . 10 . Kk I . INTRODUCTORY REMAR K The measurement of charmonium ( J / ψ ) and bottomonium ( [UNK] ) production is one of the most attractive probes to examine properties of hot and dense materials captured in relativistic nucleus - nucleus collisions 1 .It has been proposed that the interaction between the produced quarkonia and the nearby medium may contribute to their partial melting 2 , i . e . , to a reduction of the bound state masses due to color screening 3 . In this research we present results derived within an efficient field model formulation 4 , where the appropriate degrees of liberty are quarks and gluons instead than individual hadronic states .This enables us to estimate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks g = w , d , s and gluons g . These include elastic scattering off quarks and gluon - gluon fusion led to the formation of quarkonia via the creation of a virtual qq couple 5 . Furthermore , inelastic reactions such as quarkonium dissociation into open heavy - flavor hadrons 6 have also been used 7 , 8 .",
        "rewrite_text": "We investigate the dependence of transverse momentum on the production cross sections of J/ψ and [UNK] at RHIC energies using an effective field theory framework. This framework incorporates both elastic scattering with quarks and inelastic processes, such as dissociation into open charm or bottom hadrons. Our findings indicate that the suppression pattern observed can be accurately modeled by considering only elastic scattering for transverse momentum (pT) values below 2 GeV/c. However, for larger pT values, additional contributions are necessary to fit the data, predominantly stemming from inelastic reactions like dissociation into open heavy flavor mesons. We demonstrate that incorporating these effects significantly lowers the expected nuclear modification factor RAA(pT) compared to prior studies that relied solely on elastic interactions. \n\nPACS codes: 12.38.Mh, 25.75.-q, 11.10.Kk \n\nI. INTRODUCTORY REMARK\n\nThe measurement of charmonium (J/ψ) and bottomonium ([UNK]) production serves as a compelling tool for exploring the characteristics of hot and dense matter produced in relativistic nucleus-nucleus collisions. It has been suggested that the interactions between the generated quarkonia and the surrounding medium may lead to their partial melting, which refers to a decrease in the bound state masses as a result of color screening. In this study, we present results derived from an efficient field model formulation, which focuses on quarks and gluons rather than on individual hadronic states. This approach allows us to estimate the total cross section for quarkonium production through elementary partonic subprocesses involving light quarks (u, d, s) and gluons (g), including elastic scattering from quarks and gluon-gluon fusion that results in quarkonium formation via the production of virtual quark-antiquark pairs. Additionally, we also consider inelastic processes, such as the dissociation of quarkonia into open heavy-flavor hadrons.",
        "ori-fast-z-score": -0.08606629658238704,
        "water-fast-z-score": 5.115948820192307,
        "rewrite-fast-z-score": 1.2094157958139042
    },
    {
        "original_text": "We present optical spectroscopic observations for the sample of 14 extreme high energy peaked BL Lac objects (EHBLs) selected by Costamante et al. (2013) . The main goal is to study their host galaxy properties and investigate possible differences with respect to lower-energy blazars, which are known to be hosted in elliptical galaxies. We find that all EHBLs have redshifts between 0.1 and 1.0, consistent with previous results on this class of sources. All but one source show evidence of being hosted in spiral or irregular galaxies; only PKS 0537-441 shows an elliptical-like spectrum. This result suggests that there may not exist any significant difference in the hosts of low-and high-energy blazars as previously claimed. However, we note that our sample size is small and further studies will be needed before drawing firm conclusions. \n \n Keywords: Blazar, Host Galaxy",
        "watermark_text": "We present optical spectroscopic observations for the sample of 14 extreme high energy peaked BL Lac objects ( EHBLs ) selected by Costamante et al . ( 2013 ) .The main goal is to study their host universe characteristics and probe possible variations with regard to smaller - energy blazars , which are known to be hosted in elliptical galaxies . We see that all EHBLs have redshifts between 0 . 1 and 1 . 0 , consistent with previous findings on this class of sources .All but one source demonstrate proof of being hosted in spiral or irregular stars ; only PKS 0537 - 441 shows an elliptical - like spectrum . This result suggests that there may not occur any large change in the hosts of high - and low - energy blazars as previously reported .However , we note that our sample volume is tiny and further studies will be needed before drew solid results . Keywords : Blazar , Host Galaxy",
        "rewrite_text": "We present optical spectroscopic observations of a sample consisting of 14 extreme high energy peaked BL Lac objects (EHBLs) as selected by Costamante et al. (2013). The primary aim of this study is to explore the characteristics of their host galaxies and investigate potential differences compared to lower-energy blazars, which are typically found in elliptical galaxies. Our findings indicate that all EHBLs exhibit redshifts ranging from 0.1 to 1.0, aligning with prior research on this class of objects. With the exception of one source, all show evidence of being hosted in spiral or irregular galaxies, with only PKS 0537-441 presenting an elliptical-like spectrum. This observation implies that significant variations in the host galaxies of high- and low-energy blazars may not occur, as previously suggested. However, we acknowledge that our sample size is limited, and additional studies will be necessary to draw more definitive conclusions. Keywords: Blazar, Host Galaxy.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.165514464459439,
        "rewrite-fast-z-score": 1.0834726777719228
    },
    {
        "original_text": "We report on experimental measurements of spatial correlation functions for the dynamics of supercooled liquids at different temperatures, using confocal microscopy and single particle tracking techniques to probe the motion of colloidal particles suspended in glycerol. We find that these correlation functions can be well described by an exponential decay with a characteristic length scale which increases as we lower the temperature towards the glass transition point. This increase is consistent with theoretical predictions based on mode-coupling theory (MCT), but our results show deviations from MCT near the glass transition temperature Tg. These deviations are likely due to dynamic heterogeneities present close to Tg. The data presented here provide new insights into the nature of spatio-temporal fluctuations in glassy systems. Glass-forming liquids exhibit slow relaxation processes over many decades in time scales  1  . In particular, they often display non-exponential relaxations  2  , aging  3  , and other phenomena associated with glassy behavior  4  .\nTheories such as Mode-Coupling Theory (MCT)  5  have been developed to describe this complex phenomenology  6  . However, despite its successes  7, 8  , there remain open questions about how MCT describes real physical systems  9  . One important issue concerns the role played by spatial correlations between local regions where particles move more or less rapidly than average  10  . Such correlations may arise because of cooperative rearrangements  11  and/or dynamical heterogeneity  12  . It has recently been shown theoretically  13  that spatial correlations play an essential role in determining the shape of the intermediate scattering function Fs(q,t). Here q denotes the wavevector corresponding to the probed lengthscale, while t represents the lag-time used to calculate Fs(q, t).\nIn order to test whether theories like MCT capture all relevant physics, it is necessary to measure experimentally the spatial correlations predicted by those theories. Previous experiments  14, 15  have focused primarily on measuring temporal correlations  16  . Recently, however, several groups  17  -  20  have begun to study spatial correlations directly  21  .",
        "watermark_text": "We report on research studies of spatial correlation functions for the dynamics of supercooled liquids at different conditions , using confocal microscopy and single molecule tracking technology to probe the movement of colloidal particles suspended in glycerol . We see that these correlation functions can be well described by an exponential decay with a typical duration scale which increases as we lower the temperature towards the glass transition point .This increase is compatible with theoretical estimates based on mode - correlation theory ( MCT ) , but our findings show deviations from MCT near the glass transition temperature Tg . These deviations are likely due to dynamic heterogeneities present close to Tg .The data provided here provide fresh insights into the nature of spatio - temporal fluctuations in glassy systems . Glass - creating liquids exhibit slow relaxation processes over numerous centuries in time ranges 1 .In particular , they frequently exhibit non - exponential relaxations 2 , aging 3 , and other processes associated with glassy behavior 4 . Theories such as Mode - Coupling Theory ( MCT ) 5 have been constructed to explain this complex phenomenology 6 .However , despite its victories 7 , 8 , there remain open questions about how MCT represents real natural systems 9 . One important concern concerns the importance played by spatial correlations between local regions where ions move more or less rapidly than average 10 .Such correlations might arise because of joint rearrangements 11 and / or dynamical heterogeneity 12 . It has recently been shown theoretically 13 that spatial correlations serve an essential part in determining the form of the intermediate absorption system Fs ( q , t ) .Here q indicates the wavevector corresponding to the probed lengthscale , while t refers the lag - time used to estimate Fs ( q , t ) . In order to test whether models like MCT convey all relevant physics , it is required to measure experimentally the spatial correlations expected by those theories .Previous experiments 14 , 15 have concentrated mostly on measuring temporal correlations 16 . Recently , however , various groups 17 - 20 have begun to study spatial correlations directly 21 .",
        "rewrite_text": "We present findings from research on spatial correlation functions related to the dynamics of supercooled liquids under various conditions. Utilizing confocal microscopy and single-molecule tracking technologies, we investigate the movement of colloidal particles suspended in glycerol. Our results indicate that these correlation functions can be accurately characterized by an exponential decay, with a characteristic time scale that increases as the temperature approaches the glass transition point. This observation aligns with theoretical predictions from mode-coupling theory (MCT), yet our data reveal significant deviations from MCT as we near the glass transition temperature (Tg), likely due to the dynamic heterogeneities that emerge in this regime. The insights provided by this research enhance our understanding of the spatio-temporal fluctuations in glassy systems. Glass-forming liquids are known to exhibit slow relaxation processes over extended timescales, often characterized by non-exponential relaxations, aging, and other manifestations of glassy behavior. Theories such as Mode-Coupling Theory (MCT) have been developed to elucidate this complex behavior. However, despite notable successes, questions remain regarding the applicability of MCT to natural systems. One significant area of concern pertains to the role of spatial correlations in regions where ions move at different rates, which could be influenced by cooperative rearrangements and/or dynamic heterogeneity. Recent theoretical work has highlighted that spatial correlations are crucial for shaping the form of the intermediate scattering function, Fs(q, t), where q represents the wavevector corresponding to the length scale probed, and t refers to the lag time used in calculations. To verify whether models like MCT fully capture the relevant physics, it is essential to experimentally investigate the spatial correlations predicted by these theories. While earlier experiments primarily focused on temporal correlations, a number of recent studies have begun to directly explore spatial correlations.",
        "ori-fast-z-score": 0.24743582965269675,
        "water-fast-z-score": 8.001666493091715,
        "rewrite-fast-z-score": 1.2632278815997784
    },
    {
        "original_text": "We introduce monoids that are generalizations of the classical groups of Richard Thompson, and we show how to construct these monoids using finite automata.  We also give an explicit description for all elements in terms of their normal forms with respect to the monoid generators.   Finally, we prove that our monoids have decidable word problems by showing that they can be embedded into free inverse monoids. The results presented here were obtained during my PhD thesis at ETH Zurich under the supervision of Prof. Dr. Peter Schupp (ETH Zurich) and Prof. Dr. Michael Steinert (University of Hamburg).    Keywords: Group theory, Monoids, Automata, Decidability, Word problem, Free inverse monoids, Finite automata, Computational complexity theory, Groups generated by finite automata, Generalized Thompson groups, Elementary amenable groups, Group actions on trees, Group actions on graphs",
        "watermark_text": "We introduce monoids that are generalizations of the classical categories of Richard Thompson , and we give how to build these monoids using finite automata . We also get an explicit description for all members in terms of their normal forms with regard to the monoid generators .Finally , we prove that our monoids have decidable word difficulties by showing that they can be embedded into free inverse monoids . The results presented here were obtained during my PhD thesis at ETH Zurich under the guidance of Prof . Dr . Peter Schupp ( ETH Zurich ) and Prof . Dr . Michael Steinert ( University of Hamburg ) .Keywords : Group logic , Monoids , Automata , Decidability , Word problem , Free inverse monoids , Finite automata , Computational complexity analysis , Groups generated by finite automata , Generalized Thompson groups , Elementary amenable groups , Group activities on forests , Group activities on graphs",
        "rewrite_text": "We propose monoids that generalize the classical categories established by Richard Thompson and present a method for constructing these monoids through finite automata. Additionally, we offer an explicit characterization of all elements in terms of their normal forms relative to the monoid generators. Furthermore, we demonstrate that our monoids possess decidable word problems by embedding them into free inverse monoids. The findings discussed in this work were part of my PhD thesis at ETH Zurich, supervised by Prof. Dr. Peter Schupp (ETH Zurich) and Prof. Dr. Michael Steinert (University of Hamburg). \n\nKeywords: Group logic, Monoids, Automata, Decidability, Word problem, Free inverse monoids, Finite automata, Computational complexity analysis, Groups generated by finite automata, Generalized Thompson groups, Elementary amenable groups, Group actions on forests, Group actions on graphs.",
        "ori-fast-z-score": 0.4685212856658182,
        "water-fast-z-score": 4.6291004988627575,
        "rewrite-fast-z-score": 0.15249857033260467
    },
    {
        "original_text": "We present an exact expression for the energy density functional in terms of the local electronic charge and spin densities, which is valid for any number N of electrons on a two-dimensional jellium surface with arbitrary spin-orbit interaction strength. The resulting sum rules are shown to be equivalent to those derived by Stillinger and Lovett (SL) for the case of zero spin-orbit coupling but they also include additional contributions due to this term. We show that these new terms can be expressed as functions of the SL parameters only. This result allows us to obtain explicit expressions for all the relevant physical quantities such as the exchange-correlation potential or the magnetization profile at finite temperature. Finally we discuss how our results could be used to improve existing approximations within Density Functional Theory. PACS: 71.10.Pq - Energy-densities; 72.20.Fd - Energy-density functionals; 73.40.Gk - Spin-polarized systems",
        "watermark_text": "We present an precise representation for the power density functional in terms of the local electronic charge and spin densities , which is valid for any number N of electrons on a two - dimensional jellium surface with arbitrary spin - orbit interaction strength . The resulting sum rules are shown to be analogous to those developed by Stillinger and Lovett ( SL ) for the case of zero spin - orbit bonding but they still provide additional contributions due to this term .We see that these new terms can be described as functions of the SL parameters only . This result allows us to obtain precise expressions for all the appropriate physical components such as the transfer - correlation potential or the magnetization profile at finite temperature .Finally we talk how our findings may be used to improve established approximations within Density Functional Theory . PACS : 71 . 10 . Pq - Energy - densities ; 72 . 20 . Fd - Energy - density functionals ; 73 . 40 . Gk - Spin - polarized systems",
        "rewrite_text": "We provide an accurate representation of the power density functional based on the local electronic charge and spin densities. This formulation is applicable to any number N of electrons on a two-dimensional jellium surface, regardless of the strength of spin-orbit interaction. The resulting sum rules resemble those previously developed by Stillinger and Lovett (SL) for the zero spin-orbit bonding case, but they also incorporate additional contributions from the spin-orbit term. Notably, these new contributions can be expressed solely in terms of the SL parameters. This finding enables us to derive precise expressions for key physical quantities, such as the transfer-correlation potential and the magnetization profile at finite temperature. Finally, we discuss how our results can enhance existing approximations in Density Functional Theory. PACS: 71.10.Pq - Energy-densities; 72.20.Fd - Energy-density functionals; 73.40.Gk - Spin-polarized systems.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 3.6927447293799815,
        "rewrite-fast-z-score": 1.5882027766319677
    },
    {
        "original_text": "In this work, we study the diversity-multiplexing tradeoff (DMT) for amplify-andforward multihop relay channels with multiple-input and multiple-output (MIMO). We first derive an upper bound on DMT by using the cut-set theorem. Then, based on the derived upper bound, we propose two schemes to achieve the optimal DMT performance in different scenarios. In particular, when all relays have full-duplex capability, our proposed scheme achieves the optimal DMT at high multiplexing gain region; while if only half-duplex relays are available, then our proposed scheme can still achieve the optimal DMT except that it is achieved at low multiplexing gain region instead. Finally, simulation results show that both proposed schemes outperform existing ones significantly. Index Terms-Diversity-Multiplexing Tradeoff, Multiple-Input MultipleOutput, Amplify-and-Forward",
        "watermark_text": "In this research , we study the diversity - multiplexing tradeoff ( DMT ) for amplify - andforward multihop relay channels with many - input and multiple - output ( MIMO ) . We first derive an upper bound on DMT by using the cutting - set theorem .Then , using on the derived upper bound , we propose two strategies to achieve the ideal DMT performance in different scenarios . In particular , when all relays have whole - duplex capability , our proposed system achieves the ideal DMT at high multiplexing gain area ; while if only whole - duplex relays are available , then our proposed system can also obtain the ideal DMT except that it is achieved at low multiplexing gain area instead .Finally , simulation data reveal that both proposed methods outperform established ones considerably . Index Terms - Diversity - Multiplexing Tradeoff , Multiple - Input MultipleOutput , Amplify - and - Forward",
        "rewrite_text": "In this study, we investigate the diversity-multiplexing tradeoff (DMT) for amplify-and-forward multihop relay channels utilizing multiple-input multiple-output (MIMO) systems. We begin by establishing an upper bound for the DMT through the application of the cutting-set theorem. Leveraging this upper bound, we then introduce two strategies aimed at achieving optimal DMT performance under various conditions. Specifically, when all relays possess full-duplex capabilities, our proposed system attains the ideal DMT in the high multiplexing gain region. Conversely, if only full-duplex relays are available, our system still achieves the ideal DMT, albeit in the low multiplexing gain region. Lastly, our simulation results demonstrate that both proposed methods significantly outperform existing approaches. \n\nKeywords: Diversity-Multiplexing Tradeoff, Multiple-Input Multiple-Output, Amplify-and-Forward.",
        "ori-fast-z-score": 0.9271726499455306,
        "water-fast-z-score": 6.118878816098722,
        "rewrite-fast-z-score": 2.2517050070105746
    },
    {
        "original_text": "We have read with interest the recent preprint  1  . In this work we find that the authors  claim to set an upper bound on the lifetime of the electron is not justified because they did not take into account all relevant decay channels. \n \n We would like to comment briefly on their treatment of backgrounds as well as their choice of cuts used to select events. The main source of background comes from radiative Bhabha scattering e+e-→e+e-γ which has been studied extensively at LEP2  2  , where it was found to be negligible compared to other sources such as two-photon processes or four-fermion final states (e.g., W pair production). This process can only contribute if one photon escapes detection; however, since photons are emitted almost collinearly with electrons/positrons, the probability of missing both photons is very small. Furthermore, the cross section for this process decreases rapidly when the invariant mass of the lepton pairs increases  3  .\n \nThe authors also use a cut on the total energy of the event, Evis>10 GeV, which removes most of these events. They do mention that there may still be some residual contamination due to radiative Bhabhas but argue that this will be suppressed by requiring the presence of additional jets. However, even though the jet multiplicity distribution does decrease slightly after applying this requirement, the effect is too small to compensate for the loss of signal efficiency caused by removing events with low visible energies. \n \nIn addition, the authors state that the contribution from radiative Bhabhas should be included in the systematic uncertainty estimate. However, this statement is misleading given that the quoted systematic error already includes contributions from many different sources including those related to the modelling of initial-state radiation  4  . \n\n\nFinally, we note that the authors present results obtained using Monte Carlo simulations performed with PYTHIA 6  5  . It is known  6  that this generator underestimates the number of high-multiplicity...",
        "watermark_text": "We have read with importance the recent preprint 1 . In this research we find that the writers claim to setting an upper bound on the life of the electron is not justified because they did not take into consideration all relevant degradation channels .We would like to comment briefly on their handling of backgrounds as also as their choosing of cuts needed to select events . The main origin of background comes from radiative Bhabha scattering e + e - →e + e - γ which has been studied thoroughly at LEP2 2 , where it was shown to be negligible compared to other sources such as two - photon processes or four - fermion final states ( e . g . , W pair production ) .This process can only contribute if one photon escapes detection ; however , since photons are emitted virtually collinearly with electrons / positrons , the probability of missing both photons is very small . Furthermore , the cross area for this process reduces rapidly when the invariant mass of the lepton pairs increases 3 .The authors also use a cutting on the total energy of the event , Evis > 10 GeV , which eliminate most of these events . They do mention that there may still be some remnant contamination owing to radiative Bhabhas but suggest that this will be suppressed by requiring the presence of added jets .However , even though the jet multiplicity distribution does decrease slightly after applying this requirement , the effect is too small to compensate for the losing of signal efficiency created by removing events with lowered visible frequencies . In addition , the writers state that the impact from radiative Bhabhas should be included in the systematic uncertainty estimate .However , this statement is misleading given that the quoted systematic error also contains contributions from many various sources including those related to the modelling of initial - state radiation 4 . Finally , we note that the writers present results acquired using Monte Carlo simulations conducted with PYTHIA 6 5 .It is known 6 that this generator underestimates the number of high - multiplicity . . .",
        "rewrite_text": "We have carefully reviewed the recent preprint 1 and believe that the authors' claim of establishing an upper limit on the lifespan of the electron is not adequately supported, as they overlook several important degradation channels. We would like to briefly address their treatment of background processes and the criteria they applied to select events. A significant source of background arises from radiative Bhabha scattering, represented by the process e + e - → e + e - γ, which has been extensively analyzed at LEP2 2. Studies have demonstrated that this background is minimal compared to other contributions, such as two-photon processes or four-fermion final states (e.g., W pair production). The contribution from this process is contingent upon the detection of just one photon; however, since photons are typically emitted almost collinearly with the electrons and positrons, the likelihood of missing both is quite low. Additionally, the cross-section for radiative Bhabha scattering decreases significantly as the invariant mass of the lepton pairs increases 3. The authors also implement a cut on the total event energy, requiring Evis > 10 GeV, which effectively reduces most of these background events. They acknowledge the possibility of residual contamination from radiative Bhabhas but propose that this can be mitigated by demanding the presence of additional jets. Nevertheless, while the jet multiplicity distribution does show a slight decrease after this requirement, the effect is insufficient to offset the loss of signal efficiency caused by excluding events with reduced visible energy. Furthermore, the authors suggest that the influence of radiative Bhabhas should be factored into the systematic uncertainty estimate. However, this assertion is misleading, as the stated systematic error also encompasses various other sources of uncertainty, including those related to initial-state radiation modeling 4. Lastly, we observe that the authors present results derived from Monte Carlo simulations utilizing PYTHIA 6 5, which is known to underestimate the occurrence of high-multiplicity events 6.",
        "ori-fast-z-score": 0.9863939238321437,
        "water-fast-z-score": 7.127317708515653,
        "rewrite-fast-z-score": -0.9072647087265548
    },
    {
        "original_text": "We study the glass transition of an ensemble of adhesive hard spheres with repulsive interactions decaying as 1/r6, where r is distance between particles. We find that this system exhibits two distinct relaxation processes at low temperatures: one fast process associated with local rearrangements within clusters of strongly bonded particles; another slower process related to collective motion of these clusters. The latter process can be described by mode-coupling theory (MCT) for colloidal suspensions. However, we show that MCT fails quantitatively when applied directly to our data because it does not take into account the presence of strong bonds which lead to additional slow modes. By introducing a simple modification to MCT, we are able to obtain excellent agreement with experimental results over several decades in time and frequency. This modified version of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg. Our work shows how quantitative tests of theoretical predictions may help improve their accuracy and applicability range.",
        "watermark_text": "We test the glass transition of an ensemble of adhesive solid surfaces with repulsive interactions decaying as 1 / r6 , where r is distance between particles . We see that this scheme exhibits two different relaxation processes at low temperatures : one fast process associated with local rearrangements within clusters of highly bonded particles ; another slower process related to collective motion of these complexes .The latter system can be described by mode - coupling theory ( MCT ) for colloidal suspensions . However , we prove that MCT fails quantitatively when applied directly to our information because it does not take into consideration the presence of stable bonds which lead to extra slow modes .By introducing a simple change to MCT , we are able to obtain excellent compliance with experimental results over numerous years in time and frequency . This amended variant of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg .Our research shows how standardized tests of theoretical estimates may improve improve their accuracy and applicability range .",
        "rewrite_text": "We investigate the glass transition of a collection of adhesive solid surfaces characterized by repulsive interactions that decrease with the distance as 1 / r6, where r is the distance between particles. Our findings reveal two distinct relaxation processes at low temperatures: a fast process associated with local rearrangements within clusters of strongly bonded particles, and a slower process related to the collective motion of these clusters. The latter can be described by mode-coupling theory (MCT) typically used for colloidal suspensions. However, we demonstrate that MCT quantitatively fails when directly applied to our data, as it does not account for the stable bonds that introduce additional slow modes. By making a simple modification to MCT, we achieve excellent agreement with experimental results across a wide range of time and frequency. This modified version of MCT also accurately predicts the temperature dependence of the structural relaxation time as it approaches Tg. Our research highlights how standardized testing of theoretical models can enhance their accuracy and broaden their applicability.",
        "ori-fast-z-score": -0.43133109281375365,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 0.2182178902359924
    },
    {
        "original_text": "The purpose of this research is to develop an integrated approach for sharing product information in the supply chain using Web services technology, which will allow companies to collaborate with each other more effectively by exchanging data over the Internet. The proposed system consists of three main components: (1) Product Information Management System, (2) Supply Chain Collaboration Framework, and (3) Web Services Technology. In addition, we have developed a prototype system that demonstrates how these technologies can be used together to facilitate collaboration between different organizations involved in the same supply chain network. This article presents our work on developing such a system. We believe that it has great potential as a tool for improving business processes across various industries. Keywords: Supply chain management, web service, product information management system, collaboration framework. 1 Introduction With the rapid development of e-commerce applications, there are increasing demands for efficient ways to manage products throughout their life cycles  1  . Companies need to cooperate closely with suppliers and customers so they can respond quickly to market changes  2  .\nIn order to achieve better coordination among partners along the supply chain, many researchers have been working towards integrating existing systems into one common platform  3  , where all parties could access relevant information about products at any time  4  . However, most current approaches focus only on providing technical solutions without considering the organizational issues associated with implementing them  5  . Therefore, it becomes important to understand both technological aspects and social dimensions when designing new e-business models  6  .\nThis study aims to address some of these challenges by proposing a novel method for managing product information in the supply",
        "watermark_text": "The purpose of this research is to develop an unified approach for distributing product information in the supply chain using Web services technology , which will provide firms to collaborate with each other more effectively by exchanging data over the Internet . The proposed system includes of three principal portions : ( 1 ) Product Information Management System , ( 2 ) Supply Chain Collaboration Framework , and ( 3 ) Web Services Technology .In addition , we have developed a prototype system that indicates how these innovations can be used together to enable collaboration between various organizations involved in the same supply chain chain . This page presents our work on developing such a system .We believe that it has tremendous ability as a platform for improving business processes across different industries . Keywords : Supply chain control , web service , product information management system , partnership template .1 Introduction With the fast development of electronic - trade applications , there are growing requirements for efficient methods to manage products throughout their history cycles 1 . Companies need to engage tightly with producers and customers so they can respond rapidly to market shifts 2 .In order to achieve improve coordination among partners along the supply chain , many scientists have been pushing towards combining existing solutions into one common platform 3 , where all parties could access relevant information about goods at any time 4 . However , most current approaches focus only on providing technical solutions without examining the institutional issues associated with implementing them 5 .Therefore , it becomes crucial to realize both economic aspects and social dimensions when designing new online - business systems 6 . This study aims to overcome some of these problems by proposing a unique technique for controlling product information in the supply",
        "rewrite_text": "The aim of this research is to establish a cohesive framework for disseminating product information across the supply chain using Web services technology. This approach will enable companies to collaborate more efficiently by sharing data via the Internet. The proposed system consists of three main components: (1) Product Information Management System, (2) Supply Chain Collaboration Framework, and (3) Web Services Technology. Additionally, we have created a prototype that demonstrates how these innovations can work together to facilitate collaboration among various organizations in the same supply chain. This document outlines our efforts in developing such a system, which we believe holds significant potential as a platform for enhancing business processes across diverse industries. \n\n**Keywords:** Supply chain management, web services, product information management system, collaboration framework. \n\n**1 Introduction:** With the rapid advancement of e-commerce applications, there is an increasing demand for effective methods to manage products throughout their life cycles. Companies must closely engage with both producers and consumers to quickly adapt to market changes. To improve coordination among supply chain partners, many researchers advocate for the integration of existing solutions into a unified platform where all stakeholders can access pertinent information about products at any time. However, most current methodologies primarily focus on providing technical solutions while neglecting the institutional challenges related to their implementation. Thus, it is essential to consider both the economic and social factors when designing new online business systems. This study seeks to address these issues by presenting a novel approach to managing product information within the supply chain.",
        "ori-fast-z-score": 0.7423074889580903,
        "water-fast-z-score": 9.434707130066995,
        "rewrite-fast-z-score": 1.3620104492139977
    },
    {
        "original_text": "We present an analysis of the photometric data for all known transiting planets discovered by microlensing surveys, including OGLE-II (Udalski et al., 2002) , MOA (Bond et al., 2001) and MACHO (Alcock et al., 1997) . We use these results to investigate whether any of them are likely to be false positives due to blending with nearby stars or other effects such as grazing eclipses.  The majority of the planet detections have been made using ground-based telescopes which cannot resolve individual sources in crowded fields. This means that it is possible that some of the detected signals may actually arise from multiple unresolved objects rather than single planets. In addition, there can also be significant contamination from background eclipsing binaries whose light curves mimic those expected from planetary systems. To address this problem we perform detailed simulations of each system taking into account realistic distributions of stellar masses and distances, as well as observing conditions at different sites.",
        "watermark_text": "We publish an assessment of the photometric data for all known transiting planets discovered by microlensing observations , including OGLE - II ( Udalski et al . , 2002 ) , MOA ( Bond et al . , 2001 ) and MACHO ( Alcock et al . , 1997 ) . We use these results to examine whether any of them are likely to be false positives due to mixing with nearby planets or other effects such as grazing eclipses .The majority of the planet detections have been achieved using ground - based telescopes which inability resolve individual sources in dense fields . This implies that it is suggested that some of the emitted signals might actually emerge from multiple unresolved objects rather than separate planets .In addition , there can also be considerable exposure from background eclipsing binaries whose light patterns mimic those expected from planetary components . To address this challenge we perform comprehensive simulations of each system take into consideration realistic distributions of stars masses and altitudes , as also as monitoring conditions at different places .",
        "rewrite_text": "We present an analysis of the photometric data for all known transiting planets identified through microlensing observations, including those from OGLE-II (Udalski et al., 2002), MOA (Bond et al., 2001), and MACHO (Alcock et al., 1997). Our findings are used to investigate the potential for false positives, which may arise from the mixing of signals with nearby planets or other factors, such as grazing eclipses. Most of the planet detections have been made using ground-based telescopes, which cannot resolve individual sources in densely populated fields. This leads to the possibility that some observed signals may actually originate from multiple unresolved objects instead of distinct planets. Additionally, there can be significant interference from background eclipsing binaries, whose light variations can mimic those expected from planetary companions. To tackle this issue, we conduct extensive simulations for each system, considering realistic distributions of stellar masses and distances, as well as varying monitoring conditions across different locations.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": -1.2701705922171767
    },
    {
        "original_text": "In this note, we present an elementary proof for the following theorem due to Pappus:\nTheorem 1 (Pappus  theorem). Let $ABC$ be any triangle and let $D$ be its incenter. Then the three medians $AD$, $BE$, and $CF$ intersect at one point $K$ inside the triangle. \nProof. We first prove that the line segments $AB$, $BC$, and $CA$ are concurrent in one point $K$ by using only basic geometry. The idea is as follows. Consider the circle $C_1$ centered at $A$ with radius $AC$. Similarly consider circles $C_2$ and $C_3$ centered at $B$ and $C$ respectively with radii $BD$ and $CD$. Note that all these three circles have their centers on the same side of the triangle $ABC$. Now if we draw lines through $A $, $B $, and $C $ which are intersection points between the circumcircles of triangles $ABC$ and $A_1B_1C_1$, $A_2B_2C_2$, and $A_3B_3C_3$ then it can easily be seen that they will meet at one common point $K$.",
        "watermark_text": "In this note , we present an elementary proof for the following theorem according to Pappus : Theorem 1 ( Pappus principle ) . Let $ ABC $ be any triangle and take $ D $ be its incenter .Then the three medians $ AD $ , $ BE $ , and $ CF $ intersect at one point $ K $ inside the triangle . Proof .We first establish that the line segments $ AB $ , $ BC $ , and $ CA $ are simultaneous in one point $ K $ by using only basic geometry . The idea is as follows .Consider the circle $ C _ 1 $ focused at $ A $ with diameter $ AC $ . Similarly consider circles $ C _ 2 $ and $ C _ 3 $ focused at $ B $ and $ C $ respectively with radii $ BD $ and $ CD $ .Note that all these three circles have their centers on the same side of the circle $ ABC $ . Now if we draw lines through $ A $ , $ B $ , and $ C $ which are intersection points between the circumcircles of triangles $ ABC $ and $ A _ 1B _ 1C _ 1 $ , $ A _ 2B _ 2C _ 2 $ , and $ A _ 3B _ 3C _ 3 $ then it can easily be saw that they will meet at one common point $ K $ .",
        "rewrite_text": "In this note, we offer a straightforward proof of the following theorem attributed to Pappus: Theorem 1 (Pappus's Principle). Consider a triangle \\( ABC \\) with its incenter denoted as \\( D \\). The three medians \\( AD \\), \\( BE \\), and \\( CF \\) concur at a single point \\( K \\) within the triangle. To prove this, we will first demonstrate that the line segments \\( AB \\), \\( BC \\), and \\( CA \\) all meet at the point \\( K \\), using only fundamental geometric principles. The approach is outlined as follows: we begin by constructing circle \\( C_1 \\) centered at \\( A \\) with diameter \\( AC \\). Similarly, we create circles \\( C_2 \\) and \\( C_3 \\) centered at \\( B \\) and \\( C \\) respectively, each with radii \\( BD \\) and \\( CD \\). Importantly, the centers of these three circles lie on the same side of the triangle \\( ABC \\). Next, we draw lines through points \\( A \\), \\( B \\), and \\( C \\) that correspond to the intersection points of the circumcircles of triangles \\( ABC \\) and \\( A_1B_1C_1 \\), \\( A_2B_2C_2 \\), and \\( A_3B_3C_3 \\). It can be easily observed that these lines will intersect at a common point \\( K \\).",
        "ori-fast-z-score": 0.7453559924999299,
        "water-fast-z-score": 3.464101615137755,
        "rewrite-fast-z-score": -0.5443310539518174
    },
    {
        "original_text": "We introduce the notion of an algebra over a monoidal category and show that it is equivalent to the notion of a coalgebra in the dual category, which we call a comonoid.  We then define the cyclic homology of such algebras as the Hochschild cohomology of their underlying comonoids with coefficients in the bimodule given by the tensor product of the algebra with its opposite algebra.  This definition generalizes the usual one for ordinary algebras over fields or rings.   In particular, if the base ring has characteristic zero, this recovers the classical definitions of cyclic homology and periodic cyclic homology.  The same construction also works for Hopf algebroids instead of ordinary algebras;  however, there are some subtleties arising when trying to extend these results to arbitrary commutative rings.    Finally, we give several examples illustrating our constructions. Cyclic homology was introduced by Connes in his seminal work on noncommutative geometry  Con  . It can be defined as the Hochschild homology of certain algebras called cyclic objects. These were first studied systematically by Bökstedt  Bök  , who showed how they could be used to construct new algebraic structures like crossed modules and group extensions. Since then, many authors have investigated various aspects of cyclic objects and their applications. For example, see  Fri1  ,  Fri2  ,  Koc  ,  Lau  ,  Maz  ,  Nee  ,  Sta  .\nIn this article, we will study cyclic objects in more detail using techniques developed recently in the theory of operads and monads. Our main result shows that any cyclic object gives rise to two different types of cyclic homologies, namely the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded algebra. Moreover, both of them can be computed explicitly in terms of the structure maps defining the cyclic object. As a consequence, we obtain explicit formulas for the cyclic homology of all finite-dimensional cocommutative Hopf algebras over a field of characteristic 0.",
        "watermark_text": "We introduce the notion of an ring over a monoidal category and know that it is analogous to the notion of a coalgebra in the dual category , which we call a comonoid . We then define the cyclic homology of such algebras as the Hochschild cohomology of their underlying comonoids with coefficients in the bimodule given by the tensor product of the algebra with its opposite algebra .This definition generalizes the usual one for regular algebras over fields or rings . In particular , if the base ring has characteristic zero , this recovers the classical definitions of cyclic homology and regular cyclic homology .The same construction also works for Hopf algebroids rather of simple algebras ; however , there are some subtleties emerging when trying to apply these results to arbitrary commutative rings . Finally , we give numerous instances illustrating our concepts .Cyclic homology was introduced by Connes in his seminal study on noncommutative geometry Con . It can be written as the Hochschild homology of certain algebras called cyclic objects .These were first examined carefully by Bökstedt Bök , who demonstrated how they could be used to build modern algebraic structures like crossed varieties and group extensions . Since then , various papers have researched several topics of cyclic objects and their applications .For instance , see Fri1 , Fri2 , Koc , Lau , Maz , Nee , Sta . In this article , we will explore cyclic objects in more depth employing approaches developed lately in the notion of operads and monads .Our main result suggests that any cyclic object gives rise to two different kinds of cyclic homologies , namely the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded module . Moreover , both of them can be computed specifically in terms of the structure maps governing the cyclic object .As a consequence , we obtain formal formulas for the cyclic homology of all finite - dimensional cocommutative Hopf algebras over a field of characteristic 0 .",
        "rewrite_text": "We introduce the concept of a ring within the framework of a monoidal category, drawing an analogy to the concept of coalgebra in its dual category, which we refer to as a comonoid. We define the cyclic homology of these algebras as the Hochschild cohomology of the corresponding comonoids, using coefficients from the bimodule formed by the tensor product of the algebra with its opposite algebra. This definition extends the standard definition applicable to regular algebras over fields or rings. Notably, when the base ring has characteristic zero, it aligns with the classical definitions of cyclic homology and regular cyclic homology. This construction is also applicable to Hopf algebroids instead of merely simple algebras; however, certain subtleties arise when attempting to generalize these results to arbitrary commutative rings. We provide multiple examples to illustrate our concepts. \n\nCyclic homology, first introduced by Connes in his pioneering work on noncommutative geometry, can be understood as the Hochschild homology of specific algebras known as cyclic objects. The detailed study of these objects was initiated by Bökstedt, who showed their utility in constructing advanced algebraic structures like crossed varieties and group extensions. Since then, numerous papers have investigated various aspects of cyclic objects and their applications. For further reading, see the works by Fri1, Fri2, Koc, Lau, Maz, Nee, and Sta. In this article, we will delve deeper into cyclic objects, leveraging recent developments in the theory of operads and monads. Our main finding indicates that any cyclic object leads to two distinct types of cyclic homologies: the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded module. Furthermore, both of these can be explicitly computed based on the structure maps governing the cyclic object. As a result, we derive formal formulas for the cyclic homology of all finite-dimensional cocommutative Hopf algebras over fields of characteristic 0.",
        "ori-fast-z-score": -0.4583492485141057,
        "water-fast-z-score": 5.775200531277732,
        "rewrite-fast-z-score": 1.7417271443536015
    },
    {
        "original_text": "We study the random spatial growth in two dimensions, where new sites are added to an initially empty square lattice at randomly chosen locations and grow into circular clusters if they do not hit any existing cluster or obstacle site.  We show that this process leads to fractal structures which can be characterized by their fractal dimension Df = 1 + (1 - p)/2p, where p is the probability for adding a new site without hitting an obstacle. The results agree well with numerical simulations. \nPACS numbers: 05.40.+j, 64.60.Cn, 68.35.-k \nI. INTRODUCTORY REMARK\nIn recent years there has been considerable interest in studying various aspects of the so-called Eden model  1  . In its original formulation it describes the growth of a single cluster on a two-dimensional substrate starting from one seed particle. This simple concept was later generalized to include several seeds  2  , as well as different shapes  3  .\nThe present work deals with another generalization of the Eden model: Instead of growing only one cluster we consider the simultaneous growth of many clusters competing for space  4  . As a result, some clusters may become trapped between other clusters leading to a complex pattern formation.",
        "watermark_text": "We research the random spatial growth in two dimensions , where new sites are adding to an initially vacant rectangular matrix at randomly picked areas and grow into circular groups if they do not hit any original cluster or obstacle site . We see that this process results to fractal structures which can be described by their fractal dimension Df = 1 + ( 1 - p ) / 2p , where p is the probability for adding a new site without hits an barrier .The results agree well with numerical simulations . PACS scores : 05 . 40 . + j , 64 . 60 . Cn , 68 . 35 . - k I .INTRODUCTORY REMARK In recent years there has been substantial interest in studying several elements of the so - called Eden model 1 . In its initial formulation it assumes the development of a single cluster on a two - dimensional substrate beginning from one seed particle .This basic concept was afterwards generalized to consider many seeds 2 , as well as varying shapes 3 . The present work deals with another generalization of the Eden model : Instead of growing only one cluster we define the concurrent development of several clusters battling for space 4 .As a result , some clusters might form trapped between other communities leading to a complex pattern formation .",
        "rewrite_text": "We investigate random spatial growth in two dimensions, where new sites are added to an initially empty rectangular grid in randomly selected locations. These new sites expand into circular clusters unless they encounter existing clusters or obstacles. Our analysis shows that this process leads to fractal structures, characterized by a fractal dimension given by \\( D_f = 1 + \\frac{1 - p}{2p} \\), where \\( p \\) represents the probability of adding a new site without encountering a barrier. Our findings align closely with numerical simulations. PACS classifications: 05.40.+j, 64.60.Cn, 68.35.-k. \n\n**I. INTRODUCTORY REMARKS**  \nIn recent years, there has been significant interest in various aspects of the Eden model. Initially, this model describes the growth of a single cluster on a two-dimensional substrate, originating from a single seed particle. This fundamental idea has since been expanded to include multiple seeds and diverse shapes. The current study explores another extension of the Eden model: rather than allowing only one cluster to grow, we define the simultaneous development of several clusters competing for space. Consequently, some clusters may become trapped among other groups, leading to complex pattern formation.",
        "ori-fast-z-score": -0.20203050891044214,
        "water-fast-z-score": 6.464976285134148,
        "rewrite-fast-z-score": 0.5076730825668095
    },
    {
        "original_text": "We present new optical and near-infrared images, as well as archival radio data for the polar ring galaxy AM1934-563 (NGC4650A). The main results are summarized below:  1) We confirm that this galaxy is an edge-on S0/a with a bright nuclear point source surrounded by a faint disk.  2) A prominent dust lane crosses the central region in the north-south direction.  3) There appears to be no evidence for any recent star formation activity within the ring or along its edges.  4) The kinematics of the gas component show two distinct components; one associated with the ring itself and another which follows closely the rotation curve of the underlying stellar body.  5) The total mass of the gaseous ring amounts to about 10^9 M_sol. 6) The observed properties of NGC4650A can be explained if it has undergone a major merger event between two galaxies at least 0.5 Gyr ago.",
        "watermark_text": "We release additional imaging and far - infrared images , as also as archival radio data for the polar belt star AM1934 - 563 ( NGC4650A ) . The main results are presented below : 1 ) We suggest that this star is an edge - on S0 / a with a bright nuclear point source surrounded by a dim disk .2 ) A prominent cloud lane crosses the main region in the north - south orientation . 3 ) There seems to be no evidence for any new star formation activity within the circle or along its edges .4 ) The kinematics of the gas component show two different components ; one linked with the circle itself and another which follows carefully the rotation curve of the underlying stars body . 5 ) The total mass of the gaseous ring reduces to about 10 ^ 9 M _ sol .6 ) The observed properties of NGC4650A can be understood if it has undergone a major collision activity between two galaxies at least 0 . 5 Gyr ago .",
        "rewrite_text": "We present new imaging and far-infrared observations, along with archival radio data, for the polar belt star AM1934-563 (NGC 4650A). Below are the key findings: 1) We propose that this star is an edge-on S0/a type, featuring a bright central point source surrounded by a faint disk. 2) A significant cloud lane is observed extending from north to south across the main region. 3) There is no evidence for recent star formation activity either within the core or along its periphery. 4) The gas kinematics reveal two distinct components: one associated with the core itself and another that closely follows the rotation curve of the underlying stellar body. 5) The total mass of the gaseous ring is approximately 10^9 M_sun. 6) The observed characteristics of NGC 4650A may be explained by a significant collision event between two galaxies that occurred at least 0.5 Gyr ago.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.289942788427422,
        "rewrite-fast-z-score": 1.4342743312012722
    },
    {
        "original_text": "We present the first version of our new code, WHAM (Weno-Hybrid Arbitrary Mesh), which is based on the recently developed weighted essentially non-oscillatory (WENO) schemes for solving hyperbolic conservation laws in one dimension and multi-dimensions.  The main idea behind this method is to use high-order spatial reconstruction with an adaptive mesh refinement technique to achieve high accuracy at low computational cost. We have implemented several different versions of the WENO algorithm into our code including the fifth-order WENO-Z scheme as well as the third- and fifth-order WENO-JS schemes. In addition we also implement the fourth-order Runge-Kutta time integration scheme along with the Harten-Lax-van Leer contact discontinuity capturing scheme to handle discontinuities arising during hydrodynamic evolution. Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions.",
        "watermark_text": "We introduce the first generation of our new code , WHAM ( Weno - Hybrid Arbitrary Mesh ) , which is based on the recently advanced weighted essentially non - oscillatory ( WENO ) schemes for solving hyperbolic conservation forces in one dimension and multi - dimensions . The main idea behind this algorithm is to use large - order spatial reconstruction with an adaptive mesh refinement technique to achieve high sensitivity at low numerical cost .We have integrated various different versions of the WENO algorithm into our code including the fifth - order WENO - Z plan as also as the third - and fifth - order WENO - JS schemes . In addition we also execute the third - order Runge - Kutta time integration scheme along with the Harten - Lax - van Leer contact discontinuity capturing scheme to manage discontinuities resulting during hydrodynamic evolution .Our results show that all these algorithms are able to produce accurate answers when compared against exact or reference solutions .",
        "rewrite_text": "We are pleased to present the first generation of our new code, WHAM (Weno-Hybrid Arbitrary Mesh). WHAM is built upon the latest advancements in Weighted Essentially Non-Oscillatory (WENO) schemes, which are effective for solving hyperbolic conservation laws in both one and multiple dimensions. The core concept of this algorithm involves utilizing high-order spatial reconstruction in conjunction with an adaptive mesh refinement technique, allowing for enhanced sensitivity at a minimal computational cost. Our code incorporates several versions of the WENO algorithm, including the fifth-order WENO-Z method, as well as third- and fifth-order WENO-JS schemes. Furthermore, we implement a third-order Runge-Kutta time integration scheme and the Harten-Lax-Van Leer methodology for accurately capturing contact discontinuities that may arise during hydrodynamic evolution. Our findings indicate that all implemented algorithms deliver accurate results when evaluated against exact or reference solutions.",
        "ori-fast-z-score": 2.1652509527331207,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "We present PdBI observations at 1.3 mm wavelength of the protostellar jet associated with the young stellar object (YSO) HH212, which is one of the best examples for studying the formation process of bipolar outflows driven by YSOs. The main results are as follows:\n(1) We find that there exists an unresolved central source within the innermost region of the jet.\n(2) A bright knot appears to be located on each side of the central source along the jet axis. \n(3) The knots show blue-shifted emission lines compared to those of the ambient gas surrounding them. (4) The knots have a velocity width of ~100 km s-1 , while the ambient gas has a much broader linewidth up to 300 km s-1 . These facts suggest that the knots represent shocked regions where the jet interacts with the ambient medium. In addition, we found that the knots are surrounded by a shell-like structure whose radius ranges between 100 AU and 1000 AU.",
        "watermark_text": "We present PdBI data at 1 . 3 cm wavelength of the protostellar plane corresponding with the young stellar object ( YSO ) HH212 , which is one of the best cases for studying the formation transition of bipolar outflows driven by YSOs . The main results are as follows : ( 1 ) We see that there exists an unresolved primary source within the innermost region of the jet .( 2 ) A blue knot appears to be found on each side of the main source along the jet axis . ( 3 ) The knots exhibit blue - shifted emission lines relative to those of the atmospheric gas covering them .( 4 ) The knots have a speed width of ~ 100 km s - 1 , while the atmospheric gas has a far larger linewidth up to 300 kilometers s - 1 . These facts indicate that the knots represent shocked regions where the jet interacts with the atmospheric medium .In addition , we reported that the knots are surrounded by a shell - like structure whose radius varies between 100 AU and 1000 AU .",
        "rewrite_text": "We present 1.3 cm wavelength data from PdBI of the protostellar environment associated with the young stellar object (YSO) HH212, which serves as an excellent case for examining the transition of bipolar outflows driven by YSOs. Our main findings are as follows: (1) We have identified an unresolved primary source located in the innermost region of the jet. (2) A blue knot is detected on either side of the main source along the jet axis. (3) These knots show blue-shifted emission lines when compared to the emission from the surrounding atmospheric gas. (4) The knots possess a velocity width of approximately 100 km/s, whereas the atmospheric gas exhibits considerably broader linewidths of up to 300 km/s. These observations suggest that the knots represent shocked regions resulting from the interaction between the jet and the surrounding medium. Furthermore, we note that the knots are encased in a shell-like structure with a radius ranging from 100 AU to 1000 AU.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": 1.1785113019775793
    },
    {
        "original_text": "We present new millimeter observations at 1.3 mm (230 GHz) with the Submillimeter Array (SMA), which resolve the dust continuum emission in the protoplanetary disk around the Herbig Ae star HD 163296 into two components, one located close to the central star and another farther out. The inner component is resolved by SMA for the first time and shows an elongated shape that can be fitted well by a Gaussian function with a FWHM size of 0. 34 ± . 01 × 0. 21 ± . 01. We also detect CO J=2-1 line emission toward this source using the SMA. By comparing our results with previous studies we find evidence for Keplerian rotation in both the gas and dust disks. Our data suggest that there are three distinct regions in the disk where different physical processes may take place: i) An optically thick region near the center of the disk; ii) a transition zone between the optically thin outer disk and the optically thick inner disk; iii) an extended optically thin region beyond 100 AU.",
        "watermark_text": "We present new millimeter discoveries at 1 . 3 cm ( 230 GHz ) with the Submillimeter Array ( SMA ) , which resolve the dust continuum emission in the protoplanetary disk around the Herbig Ae star HD 163296 into two parts , one located close to the main star and another farther out . The inner component is resolved by SMA for the first time and shows an elongated structure that can be fit well by a Gaussian constant with a FWHM diameter of 0 .34 ± . 01 × 0 .21 ± .01.We additionally observe CO J = 2 - 1 line emission toward this source using the SMA . By matching our findings with previous research we find proof for Keplerian rotation in both the gas and dust disks .Our data suggest that there are three separate areas in the disk where various physical processes must take place : i ) An optically dense zone near the center of the disk ; ii ) a transfer zone between the optically thin outer disk and the optically dense inner disk ; iii ) an extended optically thin region beyond 100 AU .",
        "rewrite_text": "We report new millimeter observations at 1.3 cm (230 GHz) using the Submillimeter Array (SMA), which successfully resolves the dust continuum emission in the protoplanetary disk surrounding the Herbig Ae star HD 163296 into two distinct regions: one near the central star and another located further out. The inner region, resolved by the SMA for the first time, exhibits an elongated shape that fits well with a Gaussian profile, having a FWHM diameter of 0.34 ± 0.01 × 0.21 ± 0.01. Additionally, CO J = 2 - 1 line emission has been detected toward this source with the SMA. By correlating our results with prior studies, we find evidence of Keplerian rotation in both the dust and gas disks. Our findings indicate three distinct zones within the disk that are likely host to different physical processes: i) a dense optically thick region near the center; ii) a transitional zone between the optically thin outer disk and the dense inner disk; and iii) a more extensive optically thin area beyond 100 AU.",
        "ori-fast-z-score": 0.11704114719613057,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 0.9561828874675149
    },
    {
        "original_text": "The colour reconnection (CR) model is used to describe how quarks and gluons rearrange themselves into hadrons after they are produced by hard scattering processes, such as those occurring during e+e-annihilation events.  The CR model predicts that particles emitted close together in phase space will be more likely to recombine than those which are further apart.  This effect can lead to changes in event topology and kinematics compared to predictions made using models without CR.  In this analysis we use data collected by the Delphi experiment operating at centre-of-mass energies between 189 GeV and 209 GeV corresponding to an integrated luminosity of 1.1 fb-1.  We measure the fraction of WW events where one or both W bosons decay leptonically for different ranges of dilepton invariant mass and compare these results to Monte Carlo simulations including and excluding CR effects.  Our measurements show no significant evidence for CR effects within our experimental uncertainties.",
        "watermark_text": "The colour reconnection ( CR ) model is utilized to explain how quarks and gluons rearrange themselves into hadrons after they are produced by hard scattering mechanisms , such as those occurring during e + e - annihilation processes . The CR theory predicts that particles emitted close together in phase space will be more likely to recombine than those which are further separated .This phenomenon can lead to changes in event topology and kinematics compared to forecast making using models without CR . In this analysis we utilize evidence generated by the Delphi experiment working at centre - of - mass energies between 189 GeV and 209 GeV corresponding to an unified luminosity of 1 . 1 fb - 1 .We estimate the fraction of WW occasions where one or both W bosons decay leptonically for different ranges of dilepton invariant mass and compare these results to Monte Carlo simulations using and excluding CR effects . Our measurements show no important proof for CR influences within our research uncertainties .",
        "rewrite_text": "The colour reconnection (CR) model is employed to elucidate how quarks and gluons reorganize into hadrons following their production through hard scattering mechanisms, such as those seen in e+e- annihilation processes. According to CR theory, particles that are emitted in close proximity in phase space are more likely to recombine than those that are more distantly spaced. This effect can result in alterations to the event topology and kinematics when compared to predictions made using models that do not incorporate CR. In this study, we make use of data from the Delphi experiment, which was conducted at center-of-mass energies ranging from 189 GeV to 209 GeV, with an integrated luminosity of 1.1 fb^-1. We evaluate the fraction of WW events in which one or both W bosons decay leptonically across various dilepton invariant mass ranges and compare our findings to Monte Carlo simulations that include and exclude CR effects. Our measurements indicate no significant evidence of CR influences within the uncertainties of our research.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an analysis of photometric and spectroscopic data for SN 2003du, which was discovered on February 28th by R. Puckett at Mt. Wilson Observatory (Puckett et al., 2004) . The supernova is located in NGC 3190, a spiral galaxy with Hubble type Sb/Scd. It has been classified as a normal Type Ia supernova based on its light curve shape and spectral features. \n \n We find that the peak absolute magnitude of SN 2003du is -19.6 ± 0.1 mag, corresponding to a distance modulus of 34.7 ± 0.2 mag. This places it at a distance of about 50 Mpc (z = 0.0185). Using this distance we derive a total ejecta mass of 1.4 ± 0.3M⊙ and 56Ni yield of 0.09 ± 0.02M⊙ . \n \n \n \n In addition to our own observations, we have used archival data obtained through the CfA Supernova Archive, the SUSPECT archive maintained by the University of Hawaii, and the Wise Observatory archive.",
        "watermark_text": "We present an assessment of photometric and spectroscopic data for SN 2003du , which was discovered on February 28th by R . Puckett at Mt . Wilson Observatory ( Puckett et al . , 2004 ) .The supernova is situated in NGC 3190 , a spiral galaxy with Hubble class Sb / Scd . It has been classified as a normal Type Ia supernova based on its light curve size and spectral features .We determine that the maximum absolute magnitude of SN 2003du is - 19 . 6 ± 0 . 1 mag , equivalent to a distance modulus of 34 . 7 ± 0 . 2 mag . This places it at a distance of about 50 Mpc ( z = 0 . 0185 ) .Using this distance we derive a total ejecta mass of 1 . 4 ± 0 . [UNK] and 56Ni yield of 0 . 09 ± 0 . [UNK] . In addition to our own observations , we have utilized archival data acquired through the CfA Supernova Archive , the SUSPECT archive operated by the University of Hawaii , and the Wise Observatory collection .",
        "rewrite_text": "We provide an analysis of the photometric and spectroscopic data for SN 2003du, which was discovered on February 28 by R. Puckett at Mt. Wilson Observatory (Puckett et al., 2004). This supernova is located in NGC 3190, a spiral galaxy categorized as Hubble type Sb/Scd. It has been identified as a standard Type Ia supernova based on its light curve characteristics and spectral features. Our assessment reveals that the maximum absolute magnitude of SN 2003du is -19.6 ± 0.1 mag, corresponding to a distance modulus of 34.7 ± 0.2 mag, suggesting a distance of approximately 50 Mpc (z = 0.0185). Utilizing this distance, we estimate a total ejecta mass of 1.4 ± 0. [UNK] and a 56Ni yield of 0.09 ± 0. [UNK]. In addition to our observations, we have also incorporated archival data from the CfA Supernova Archive, the SUSPECT archive managed by the University of Hawaii, and the collection from Wise Observatory.",
        "ori-fast-z-score": -0.7453559924999299,
        "water-fast-z-score": 2.1105794120443453,
        "rewrite-fast-z-score": -1.3127849234810511
    },
    {
        "original_text": "We report an observation of a large coronal mass ejection (CME) associated with a halo-type flare that occurred in active region NOAA 10486 on 2006 December 13, which was observed by Solar TErrestrial RElations Observatory (STEREO). The CME speed is estimated to be about 1450 km/s at 1 AU using STEREO observations. We find that this CME originated from a complex magnetic structure consisting of two opposite-polarity flux systems connected by a filament channel. In addition, we found that there were several small-scale brightenings around the main sunspots before the onset of the flare/CME activity. These brightenings are identified as ephemeral regions (ERs), which are known to play important roles for triggering eruptions such as flares or CMEs. By analyzing high-resolution images taken by Hinode/SOT/SP, we show that one of these ERs interacted strongly with the surrounding magnetic field lines during its rapid rotation. This interaction caused reconnection between open and closed magnetic fields, resulting in the formation of a current sheet below the ER. Then, the eruption started when the current sheet became unstable due to the kink instability.",
        "watermark_text": "We report an observation of a large coronal mass ejection ( CME ) associated with a halo - class flare that occurred in active region NOAA 10486 on 2006 December 13 , which was seen by Solar TErrestrial RElations Observatory ( STEREO ) . The CME rate is predicted to be about 1450 km / s at 1 AU using STEREO images .We see that this CME originated from a complex magnetic formation consisting of two opposite - polarity flux systems connected by a filament channel . In addition , we saw that there were several small - scale brightenings around the main sunspots before the beginning of the flare / CME activity .These brightenings are identified as ephemeral regions ( ERs ) , which are known to take key roles for triggering eruptions such as flares or CMEs . By analyzing large - resolution photos taken by Hinode / SOT / SP , we find that one of these ERs interacted heavily with the nearby magnetic field lines during its rapid rotation .This coupling resulted reconnection between open and open magnetic waves , leading in the formation of a current sheet below the ER . Then , the volcano started when the current sheet became unstable due to the kink instability .",
        "rewrite_text": "We present an observation of a significant coronal mass ejection (CME) linked to a halo-class flare that occurred in active region NOAA 10486 on December 13, 2006, as recorded by the Solar Terrestrial Relations Observatory (STEREO). The CME is estimated to have a speed of approximately 1450 km/s at a distance of 1 AU based on STEREO images. Our analysis reveals that the CME originated from a complex magnetic structure composed of two opposing flux systems connected by a filament channel. Additionally, prior to the onset of the flare and CME activity, we observed several small-scale brightenings around the main sunspots. These brightenings are recognized as ephemeral regions (ERs), which play a crucial role in triggering eruptions like flares or CMEs. High-resolution images captured by Hinode/SOT/SP reveal that one of these ERs heavily interacted with the surrounding magnetic field lines during its rapid rotation. This interaction caused reconnection between open magnetic fields, resulting in the formation of a current sheet beneath the ER. Eventually, the eruption commenced when the current sheet became unstable due to kink instability.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": -0.9299811099505543
    },
    {
        "original_text": "The Solar Chromosphere is an important component in our understanding of how the Sun works and its influence on Earth, but it has been difficult to study because of its tenuous nature.  ALMA (Atacama Large Millimeter/submillimeter Array) will be able to observe this region for the first time with unprecedented spatial resolution.   This talk will discuss some of the science that can be done using ALMA observations of the Solar Chromosphere. The Solar Chromosphere is one of the most enigmatic regions of the Sun. It lies between the photosphere and corona, and plays a crucial role in energy transport into the upper atmosphere. However, due to its extremely low density, direct observation of the chromosphere was not possible until recently when high-resolution images were obtained by space-based telescopes such as Hinode/SOT and SDO/AIA. In addition, ground-based observatories have also made significant progress towards studying the chromosphere through various techniques including spectropolarimetry, imaging spectroscopy, and speckle interferometry. Despite these advances, there are still many open questions about the physical processes occurring within the chromosphere which need to be addressed. For example, what causes the formation of dynamic structures like sunspots? How do magnetic fields affect plasma dynamics in the chromosphere? What is the relationship between chromospheric heating mechanisms and coronal mass ejections? These questions cannot be answered without detailed knowledge of the structure and dynamics of the chromosphere. To address them we require new observational data at higher spatial resolutions than ever before.",
        "watermark_text": "The Solar Chromosphere is an important element in our understanding of how the Sun operates and its influence on Earth , but it has been difficult to study because of its tenuous nature . ALMA ( Atacama Large Millimeter / submillimeter Array ) will be possible to observe this area for the first time with incredible spatial resolution .This discussion will explore some of the science that can be performed using ALMA observations of the Solar Chromosphere . The Solar Chromosphere is one of the most enigmatic regions of the Sun .It lies between the photosphere and corona , and plays a crucial role in heat transport into the higher atmosphere . However , owing to its incredibly small abundance , direct observation of the chromosphere was not could until recently when high - resolution images were obtained by space - based telescopes such as Hinode / SOT and SDO / AIA .In addition , land - based observatories have already produced important development towards studying the chromosphere through numerous techniques including spectropolarimetry , optical spectroscopy , and speckle interferometry . Despite these developments , there are still many open questions about the physical processes arising within the chromosphere which require to be addressed .For instance , what causes the formation of dynamic systems like sunspots ? How do magnetic waves affect plasma dynamics in the chromosphere ?What is the relationship between chromospheric heating systems and coronal mass ejections ? These questions cannot be answered without precise understanding of the composition and dynamics of the chromosphere .To address them we require novel observational data at higher spatial resolutions than ever before .",
        "rewrite_text": "The Solar Chromosphere is a key component in understanding the Sun's behavior and its effects on Earth, yet it has proven challenging to study due to its delicate structure. The Atacama Large Millimeter/submillimeter Array (ALMA) will enable unprecedented observations of this region with remarkable spatial resolution. This discussion will delve into the scientific potential of ALMA's observations of the Solar Chromosphere, one of the Sun's most mysterious areas. Situated between the photosphere and corona, the chromosphere is essential for transporting heat into the upper atmosphere. However, its extremely low density has made direct observation difficult until recently, when high-resolution images were captured by space-based telescopes like Hinode/SOT and SDO/AIA. Moreover, ground-based observatories have made significant strides in studying the chromosphere through various techniques such as spectropolarimetry, optical spectroscopy, and speckle interferometry. Despite these advancements, many questions remain regarding the physical processes within the chromosphere. For example, what triggers the formation of dynamic structures like sunspots? How do magnetic waves influence plasma dynamics in this layer? What is the connection between chromospheric heating mechanisms and coronal mass ejections? Answering these questions requires a precise understanding of the chromosphere's composition and dynamics, which can only be achieved through innovative observational data at higher spatial resolutions than we have previously attained.",
        "ori-fast-z-score": 1.4,
        "water-fast-z-score": 7.53778361444409,
        "rewrite-fast-z-score": -1.2792042981336627
    },
    {
        "original_text": "MgSiO3 perovskite has been studied by first-principles calculations and the semi-empirical P-V-T equation of state (EOS). The calculated results show that MgSiO3 perovskites have high bulk modulus, low compressibility, large volume expansion coefficient at high temperature, and small thermal expansivity in comparison with other oxides. These properties are very important for understanding geophysics phenomena such as earthquakes and volcanic activities.  In addition to these physical properties, we also investigated the electronic structure of MgSiO3 perovskitie using density functional theory (DFT) method. We found that there exists a charge transfer between Si4+ ions and O2-ions which leads to the formation of Si-O bonds. This charge transfer causes the change of bond length and bond angle of Si-O-Si network. Finally, we discussed the effect of oxygen vacancy on the structural stability of MgSiO3 perovksite.",
        "watermark_text": "MgSiO3 perovskite has been studied by first - principles analysis and the semi - empirical P - V - T equation of state ( EOS ) . The measured conclusions show that MgSiO3 perovskites have high bulk modulus , low compressibility , large volume contraction coefficient at high heat , and tiny thermal expansivity in comparison with other oxides .These properties are very important for studying geophysics phenomena such as earthquakes and volcanic movements . In addition to these biological features , we also examined the electronic stability of MgSiO3 perovskitie using density functional theory ( DFT ) method .We showed that there exists a charge transfer between Si4 + ions and O2 - ions which results to the formation of Si - O bonds . This charge transfer causes the shift of bond length and bond angle of Si - O - Si channel .Finally , we explored the impact of oxygen vacancy on the structural structure of MgSiO3 perovksite .",
        "rewrite_text": "MgSiO3 perovskite has been analyzed through first-principles calculations and the semi-empirical pressure-volume-temperature (P-V-T) equation of state (EOS). The findings indicate that MgSiO3 perovskite exhibits a high bulk modulus, low compressibility, a significant volume contraction coefficient at elevated temperatures, and minimal thermal expansivity compared to other oxides. These characteristics are crucial for understanding geophysical phenomena such as earthquakes and volcanic activity. In addition to examining these physical properties, we also investigated the electronic stability of MgSiO3 perovskite using density functional theory (DFT). Our results revealed a charge transfer between Si4+ and O2- ions, leading to the formation of Si-O bonds. This charge transfer influences the bond lengths and angles within the Si-O-Si channels. Finally, we examined how oxygen vacancies affect the structural integrity of MgSiO3 perovskite.",
        "ori-fast-z-score": -0.24618298195866545,
        "water-fast-z-score": 4.520269441183293,
        "rewrite-fast-z-score": -0.8551861104941365
    },
    {
        "original_text": "We present Very Long Baseline Array (VLBA) radio images and multi-epoch optical photometry for 7 blazars selected from the Roma-BZCAT catalog as candidates to be members of the Red Giants Branch (RGB). The sources are located at redshifts between 0.1 and 1, with luminosities ranging from 1045 erg/s to 1012 erg/s in the rest frame range 2-10 keV. We find that all but one source show compact cores on milliarcsecond scales; only RGB J0152+017 shows an extended structure. All these results suggest that most of our targets belong to the class of Flat Spectrum Radio Quasars rather than Blazar-like AGNs. In addition we report new spectroscopic data obtained by us or taken from literature which confirm this hypothesis. \n \n Keywords: Blazars, VLBI, Optical variability, X-ray emission, Red giants branch",
        "watermark_text": "We create Very Long Baseline Array ( VLBA ) broadcast photographs and multi - epoch optical photometry for 7 blazars chosen from the Roma - BZCAT catalog as candidates to be members of the Red Giants Branch ( RGB ) . The sources are situated at redshifts between 0 . 1 and 1 , with luminosities ranging from 1045 erg / s to 1012 erg / s in the rest frame range 2 - 10 keV .We see that all but one source demonstrate smooth cores on milliarcsecond intervals ; only RGB J0152 + 017 shows an extended structure . All these results show that most of our targets represent to the class of Flat Spectrum Radio Quasars rather than Blazar - like AGNs .In addition we publish new spectroscopic data received by us or taken from literature which confirm this hypothesis . Keywords : Blazars , VLBI , Optical variability , X - ray radiation , Red stars branch",
        "rewrite_text": "We produce Very Long Baseline Array (VLBA) images and multi-epoch optical photometry for seven blazars selected from the Roma-BZCAT catalog as potential candidates for the Red Giants Branch (RGB). These sources have redshifts ranging from 0.1 to 1 and exhibit luminosities from 10^45 erg/s to 10^12 erg/s in the rest frame energy range of 2-10 keV. Our observations reveal that all but one of the sources exhibit smooth cores on milliarcsecond scales, with only RGB J0152 + 017 displaying an extended structure. These findings suggest that most of our targets belong to the class of Flat Spectrum Radio Quasars rather than Blazar-like AGNs. Furthermore, we present new spectroscopic data, either obtained by us or sourced from the literature, which supports this conclusion. Keywords: Blazars, VLBI, Optical variability, X-ray radiation, Red Giants Branch.",
        "ori-fast-z-score": 0.15249857033260467,
        "water-fast-z-score": 4.320493798938573,
        "rewrite-fast-z-score": 0.457495710997814
    },
    {
        "original_text": "We calculate the two-loop beta function for the coupling constant of the AdS5xS5 superstring theory and show that it is proportional to the one-loop result, which implies that there are no non-trivial fixed points at any finite value of the string coupling constant.  We also find that the dilaton field has an imaginary part when we take into account the higher-order terms beyond the leading order approximation. This indicates that our results may be valid only within some limited region of the parameter space where the imaginary part of the dilaton can be neglected. The present work was motivated by the recent study on the gauge/gravity correspondence between N=4 super Yang-Mills (SYM) theories with 16 supercharges and type IIB strings on AdS5xS5G5 backgrounds  1  . In this context, the existence of nontrivial fixed points would correspond to the conformal invariance of the dual SYM theories  2  , while the imaginary part of the dilatonic scalar fields would indicate the instability of the corresponding solutions  3  .\nIn Ref.  4  , the authors have calculated the one-loop beta functions for both the metric tensor and the dilaton field using the Green-Schwarz formalism  5  . They found that these beta functions do not vanish even if they are evaluated at vanishing values of the string coupling constants. However, their calculations were performed under the assumption that all the fermionic contributions vanish identically  6  . It turns out that such an assumption does not hold true  7, 8  . Therefore, it seems necessary to perform more detailed analysis taking into account the effects due to the fermions as well as those coming from the bosons.",
        "watermark_text": "We calculate the two - loop beta function for the coupling constant of the AdS5xS5 superstring theory and find that it is proportional to the one - loop result , which implies that there are no non - trivial fixed points at any finite value of the string coupling constant . We additionally find that the dilaton field has an imaginary part when we took into consideration the higher - order terms beyond the led order approximation .This implies that our findings may be valid only within some restricted region of the parameter space where the imaginary part of the dilaton can be forgotten . The present work was motivated by the recent study on the gauge / gravity correspondence between N = 4 super Yang - Mills ( SYM ) theories with 16 supercharges and class IIB sequences on AdS5xS5G5 backgrounds 1 .In this situation , the existence of nontrivial fixed points would coincide to the conformal invariance of the dual SYM theories 2 , while the imaginary part of the dilatonic scalar fields might suggest the instability of the equivalent solutions 3 . In Ref .4 , the papers have predicted the one - loop beta functions for both the metric tensor and the dilaton field using the Green - Schwarz formalism 5 . They found that these beta distributions do not vanish even if they are evaluated at vanishing values of the string coupling constants .However , their analyses were performed under the assumption that all the fermionic contributions vanish identically 6 . It turns out that such an assume does not stand true 7 , 8 .Therefore , it appears necessary to conduct more precise analysis taking into consideration the effects due to the fermions as also as those coming from the bosons .",
        "rewrite_text": "We calculate the two-loop beta function for the coupling constant in the AdS5xS5 superstring theory and determine that it is proportional to the one-loop result. This suggests that there are no nontrivial fixed points at any finite value of the string coupling constant. Furthermore, we discover that the dilaton field exhibits an imaginary component when higher-order terms are taken into account beyond the leading-order approximation. This indicates that our results may only be applicable within a limited region of the parameter space where the imaginary part of the dilaton can be disregarded. Our research was inspired by recent investigations into the gauge/gravity correspondence related to N = 4 super Yang-Mills (SYM) theories possessing 16 supercharges and type IIB string theory in AdS5xS5 backgrounds. In this context, the presence of nontrivial fixed points would align with the conformal invariance of the dual SYM theories, while the imaginary aspect of the dilatonic scalar fields could imply an instability in the corresponding solutions. Previous studies have predicted the one-loop beta functions for both the metric tensor and the dilaton field using the Green-Schwarz formalism, noting that these beta functions do not vanish even when evaluated at zero string coupling constants. However, their analyses were conducted under the assumption that all fermionic contributions are negligible, which has since been shown to be incorrect. Consequently, it is essential to undertake a more thorough analysis that accounts for the contributions from both fermions and bosons.",
        "ori-fast-z-score": -0.2847473987257497,
        "water-fast-z-score": 5.362108949786505,
        "rewrite-fast-z-score": 0.19802950859533489
    },
    {
        "original_text": "We study the radiative breaking of flavor symmetry in models with three generations of Majorana neutrinos and no hierarchy between their masses. We show that, for generic values of the parameters, there is always an additional massless state which can be identified as a sterile neutrino. The presence of this extra light state has important consequences on leptogenesis and neutrinoless double beta decay experiments. \nPACS numbers: 11.10.Wx, 12.60.Jv, 13 .20.Hs \nI. INTRODUCTORY REMAR K\nThe Standard Model (SM) of particle physics describes all known fundamental interactions except gravity. It contains one generation of left-handed neutrinos whose masses are generated by the Higgs mechanism  1  , but it does not explain why these particles have such small masses compared to other SM fermions  2  .\nIn order to address this question, many extensions of the SM have been proposed over the years  3  . In particular, theories based on gauge groups larger than SU(3) C ×SU(2) L ×U(1) Y predict new states beyond those present in the SM  4  . These new states may include right-handed neutrinos  5  or scalar fields  6  . If they exist at low energies, they could lead to observable effects  7, 8  .\nOne possible way to test whether new degrees of freedom exist below the electroweak scale is through precision measurements  9  . Another possibility is to look for signals of new physics in rare processes  10  . Finally, if new particles are produced directly at high energy colliders  11  , then their properties can also be studied  12  .",
        "watermark_text": "We explore the radiative violation of flavor symmetry in models with three generations of Majorana neutrinos and no hierarchy between their masses . We see that , for generic values of the variables , there is usually an additional massless state which can be identified as a sterile neutrino .The appearance of this extra light state has valuable consequences on leptogenesis and neutrinoless double β decay research . PACS scores : 11 . 10 . Wx , 12 . 60 . Jv , 13 . 20 . Hs I .INTRODUCTORY REMAR K The Standard Model ( SM ) of particle science describes all known theoretical interactions except gravitational . It contains one generation of left - handed neutrinos whose masses are produced by the Higgs mechanism 1 , but it does not show why these ions have such tiny masses compared to other SM fermions 2 .In order to meet this question , various extensions of the SM have been proposed over the years 3 . In particular , theories based on gauge fields larger than SU ( 3 ) C ×SU ( 2 ) L ×U ( 1 ) Y expect new states beyond those present in the SM 4 .These new states may include right - handed neutrinos 5 or scalar fields 6 . If they exist at low energies , they may contribute to observable effects 7 , 8 .One could way to test whether new degrees of liberty exist below the electroweak scale is through precision observations 9 . Another possibility is to search for signals of new physics in rare processes 10 .Finally , if new objects are produced directly at high energy colliders 11 , then their characteristics can also be examined 12 .",
        "rewrite_text": "We investigate the radiative violation of flavor symmetry in models featuring three generations of Majorana neutrinos without any mass hierarchy. Our findings indicate that, for typical parameter values, there is often an additional massless state that can be interpreted as a sterile neutrino. The emergence of this extra light state has significant implications for research in leptogenesis and neutrinoless double beta decay. PACS codes: 11.10.Wx, 12.60.Jv, 13.20.Hs. \n\n**I. INTRODUCTORY REMARK**  \nThe Standard Model (SM) of particle physics accounts for all known fundamental interactions, excluding gravitational forces. It comprises a single generation of left-handed neutrinos, whose masses arise from the Higgs mechanism; however, it does not explain the notably small masses of these neutrinos compared to other SM fermions. To address this issue, various extensions to the SM have been proposed over the years. In particular, theories that incorporate gauge groups larger than SU(3)C × SU(2)L × U(1)Y predict the existence of new states beyond those found in the SM. These new states may consist of right-handed neutrinos or scalar fields. If they are present at low energy scales, they could give rise to observable effects. Possible methods to investigate the existence of new degrees of freedom below the electroweak scale include precision measurements and searching for indications of new physics in rare processes. Additionally, if these new particles are produced directly in high-energy collider experiments, their properties can also be thoroughly examined.",
        "ori-fast-z-score": -0.5940885257860046,
        "water-fast-z-score": 4.752708206288037,
        "rewrite-fast-z-score": 1.9069251784911847
    },
    {
        "original_text": "We present the first provably optimal cache-oblivious layouts for meshes with arbitrary aspect ratios and dimensions, which are based on recursive striping into rectangular tiles. Our results hold both in terms of worst-case performance guarantees as well as average case behavior under realistic assumptions about access patterns. We also show how to extend our techniques to handle more general data structures such as trees or graphs. The layout problem is formulated as an optimization problem: given a set of objects that need to be stored in memory, we want to find their positions so that they can be accessed most efficiently by future queries. In this work, we consider the following scenario: A large amount of data needs to be stored in main memory (e.g., a database). This data consists of many small objects (e.g., records) whose sizes vary greatly; some may even be larger than available space. To solve this problem, one could use a standard technique called blocking: each object is divided into blocks of equal size, and then these blocks are placed contiguously within the allocated storage area. However, if there are too few blocks per object, it will not fit entirely inside its assigned block; similarly, if there are too many blocks per object, the unused space between them cannot be used effectively.",
        "watermark_text": "We report the first provably ideal cache - oblivious layouts for meshes with arbitrary aspect ratios and dimensions , which are based on recursive striping into rectangular tiles . Our results hold both in terms of lowest - case performance assurance as well as average case behavior under rigorous constraints about connection schemes .We additionally understand how to expanded our approaches to treat more general data objects such as trees or graphs . The configuration problem is implemented as an optimization problem : given a setting of items that require to be transferred in memory , we try to find their positions so that they can be viewed most efficiently by future queries .In this research , we imagine the following situations : A vast number of data needs to be contained in central memory ( e . g . , a database ) . This data contains of several small items ( e . g . , records ) whose sizes differ significantly ; some may even be larger than provided area .To solve this situation , one might using a traditional technique called blocking : each object is separated into blocks of equal size , and then these blocks are arranged contiguously within the allocated storage space . However , if there are too few blocks per object , it will not fit completely inside its allocated block ; likewise , if there are too several blocks per object , the empty space between them cannot be used effectively .",
        "rewrite_text": "We present the first verifiably ideal cache-oblivious layouts for meshes with arbitrary aspect ratios and dimensions, utilizing a method of recursive striping into rectangular tiles. Our findings provide assurance for both worst-case performance and average-case scenarios under strict constraints regarding connection schemes. Furthermore, we explore how to extend our methods to accommodate more complex data structures such as trees and graphs. The configuration challenge is framed as an optimization problem: given a set of items that need to be efficiently transferred in memory, we aim to determine their optimal positions for future query accessibility. In this research, we envision scenarios where a vast amount of data must be stored in central memory (e.g., a database). This data consists of numerous smaller items (e.g., records) that vary significantly in size, with some potentially exceeding the available allocation. To address this issue, a traditional technique known as blocking can be employed: each object is divided into blocks of uniform size, and these blocks are then arranged contiguously in the allocated storage. However, if there are too few blocks per object, the entire object may not fit within its assigned block; conversely, if there are too many blocks, the resulting gaps cannot be utilized effectively.",
        "ori-fast-z-score": -2.5776089301153053,
        "water-fast-z-score": 6.313641498019764,
        "rewrite-fast-z-score": 0.2847473987257497
    },
    {
        "original_text": "In this work, we study the distribution of the size of largest planar matching (LPM) and largest planar subgraph (LPS) in random bipartite graphs with n vertices on each side. We show that for any fixed integer k > 0, there exists an absolute constant ck such that if G is a random bipartite graph with n vertices on each part satisfying cn < p = e(G)/n < 1 − cn then LPM(G) has at least k components w.h.p.. Moreover, when p = 1/2 + o(1), we prove that LPS(G) contains a cycle of length Ω(log log n). These results are proved by using the second moment method together with some new techniques developed here to deal with the dependencies between edges. As applications, these results imply that both LPM(G) and LPS(G) have many small cycles under certain conditions.",
        "watermark_text": "In this research , we study the spread of the length of largest planar matching ( LPM ) and largest planar subgraph ( LPS ) in random bipartite graphs with n edges on each side . We see that for any fixed integer n > 0 , there exists an absolute constant ck such that if G is a random bipartite graph with n edges on each portion satisfying cn < p = e ( G ) / p < 1 − cn then LPM ( G ) has at least n parts w . h . p . .Moreover , when p = 1 / 2 + o ( 1 ) , we prove that LPS ( G ) contains a cycle of length Ω ( log log n ) . These conclusions are proved by using the second moment method together with some modern techniques introduced here to deal with the dependencies between edges .As applications , these results mean that both LPM ( G ) and LPS ( G ) have many small periods under certain conditions .",
        "rewrite_text": "In this research, we investigate the characteristics of the length of the largest planar matching (LPM) and the largest planar subgraph (LPS) in random bipartite graphs each containing n edges on both sides. Our findings reveal that for any fixed integer n > 0, there exists a constant ck such that if G is a random bipartite graph with n edges per partition, and it satisfies cn < p = e(G) / p < 1 − cn, then LPM(G) has at least n parts with high probability (w.h.p.). Furthermore, when p = 1/2 + o(1), we demonstrate that LPS(G) includes a cycle of length Ω(log log n). These results are established using the second moment method along with modern techniques developed to address dependencies among edges. Consequently, our results indicate that both LPM(G) and LPS(G) exhibit numerous small periods under certain conditions.",
        "ori-fast-z-score": 0.508000508000762,
        "water-fast-z-score": 4.318004318006477,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "We present the clustering properties of star forming galaxies at z ~ 1, 2 & 3 in the GALEX Deep Imaging Survey (DIS) field using photometric redshifts derived by combining deep optical data from the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS). We use two different methods to select our galaxy samples; one based on their observed NUV fluxes and another based on their intrinsic SFRs estimated from their UV luminosities. The results show that both these selection criteria yield similar clustering strengths for all three redshift bins considered here. However, we find evidence for evolution in the bias parameter between each redshift bin which is consistent with previous studies. In addition, we also study how this bias evolves as a function of stellar mass and UV luminosity. Our analysis shows that there are no significant differences in the bias values obtained when considering only those galaxies above or below a given threshold value of either stellar mass or UV luminosity.",
        "watermark_text": "We present the clustering behavior of galaxy producing galaxies at z ~ 1 , 2 & 3 in the GALEX Deep Imaging Survey ( DIS ) field using photometric redshifts collected by combining dark optical data from the Canada - France - Hawaii Telescope Legacy Survey ( CFHTLS ) . We use two different methods to select our galaxy collections ; one based on their observed NUV fluxes and another based on their intrinsic SFRs calculated from their UV luminosities .The results show that both these selection categories yield similar clustering strengths for all three redshift bins mentioned here . However , we find proof for evolution in the bias variable between each redshift bin which is compatible with previous research .In addition , we also study how this bias evolves as a function of stellar mass and UV luminosity . Our study shows that there are no considerable variations in the bias values achieved when examining only those galaxies above or below a given threshold amount of either stellar mass or UV luminosity .",
        "rewrite_text": "We examine the clustering behavior of galaxy-producing galaxies at redshifts around z ~ 1, 2, and 3 within the GALEX Deep Imaging Survey (DIS) field, utilizing photometric redshifts derived from dark optical data from the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS). Our analysis employs two distinct methods for selecting galaxy samples: one based on their observed near-ultraviolet (NUV) fluxes and the other on their intrinsic star formation rates (SFRs), computed from UV luminosities. The findings indicate that both selection methods yield comparable clustering strengths across all three redshift ranges. Nevertheless, we observe evidence of evolution in the bias variable between the redshift bins, aligning with earlier studies. Furthermore, we investigate how this bias varies with stellar mass and UV luminosity, revealing that there are no significant differences in bias values when analyzing galaxies above or below specified thresholds for stellar mass or UV luminosity.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 5.08000508000762,
        "rewrite-fast-z-score": -0.40451991747794525
    },
    {
        "original_text": "We present an assessment of the predictive power of galaxy formation models by comparing their predictions for rest-frame optical luminosity functions (LFs) with observations over the redshift range z=2-3. We use two different semi-analytic models, GALFORM and L-GALAXIES, to predict the evolution in number density as well as the distribution of stellar masses and star formation rates of galaxies across this redshift interval. The predicted LF is compared directly against observational data obtained using the Hubble Space Telescope s Advanced Camera for Surveys (ACS). In addition we compare the observed and predicted distributions of UV absolute magnitudes and dust-corrected colours. Our results show that both models are able to reproduce the overall shape of the observed LF but underpredict its normalisation by up to a factor of three. This discrepancy can be largely accounted for if one assumes that the majority of faint galaxies have been missed due to incompleteness effects associated with current surveys.",
        "watermark_text": "We present an assessment of the predictive capacity of galaxy formation models by comparing their expectations for rest - frame optical luminosity functions ( LFs ) with observations over the redshift range z = 2 - 3 . We use two different quasi - analytic models , GALFORM and L - GALAXIES , to predict the evolution in number density as well as the distribution of stars masses and sun formation rates of clusters across this redshift interval .The predicted LF is compared closely against observational data acquired using the Hubble Space Telescope s Advanced Camera for Surveys ( ACS ) . In addition we compare the seen and predicted distributions of UV absolute magnitudes and dust - corrected colours .Our results show that both models are able to predict the overall shape of the seen LF but underpredict its normalisation by up to a factor of three . This discrepancy can be largely accounted for if one suppose that the majority of distant galaxies have been missed due to incompleteness effects involved with current surveys .",
        "rewrite_text": "We evaluate the predictive ability of galaxy formation models by comparing their projected rest-frame optical luminosity functions (LFs) with observational data within the redshift range of z = 2 to 3. Utilizing two distinct quasi-analytic models, GALFORM and L-GALAXIES, we analyze the evolution of number density as well as the distribution of stellar masses and star formation rates of clusters throughout this interval. We closely compare the predicted LFs with observational data obtained from the Hubble Space Telescope's Advanced Camera for Surveys (ACS). Additionally, we examine the observed and predicted distributions of UV absolute magnitudes and dust-corrected colors. Our findings indicate that while both models successfully capture the general shape of the observed LF, they underestimate its normalization by up to a factor of three. This discrepancy can largely be explained by the possibility that many distant galaxies have gone undetected due to the incompleteness of current surveys.",
        "ori-fast-z-score": 0.8551861104941365,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": -0.6401843996644799
    },
    {
        "original_text": "We propose an excitable convolutional neural network (CNN) model for solving the 2D path planning problem in this work. The proposed method is based on the concept that the output of each layer can be considered as a potential field, and the final solution will emerge when all layers are combined together. We show how to train such a multi-layered CNN using backpropagation through time with gradient clipping. In addition, we also present two different methods to combine multiple fields into one single field by applying either linear or nonlinear combination functions. Finally, we demonstrate our approach on several benchmark problems including maze navigation, robotics motion planning, and autonomous driving. Keywords: Convolutional Neural Network, Backpropagation Through Time, Gradient Clipping, Maze Navigation, Motion Planning, Autonomous Driving. 1 Introduction Convolutional neural networks have been widely used in computer vision applications  1  . Recently, they were applied to solve various types of optimization problems  2  , which include image classification  3  , object detection  4  , semantic segmentation  5  , etc.. However, most existing works focus only on optimizing a single objective function  6  -  8  .\nIn many real-world applications, there may exist more than one objective function  9  . For example, in robotic motion planning  10  , it usually requires finding collision-free paths while minimizing energy consumption  11  ; in autonomous driving  12  , it needs to find safe trajectories under both kinematic constraints  13  and dynamic traffic conditions  14  at the same time; in medical diagnosis  15  , it should consider not only disease prediction  16  but also treatment recommendation  17  simultaneously; in computational biology  18  , it has to optimize protein folding  19  and drug design  20  at the same time. Therefore, it becomes necessary to develop new algorithms to handle multi-objective optimization problems  21  .\nRecently, deep reinforcement learning  22  was introduced to address multiobjective optimization problems  23  . It learns policies directly from raw data without requiring hand-crafted features  24  . However, its performance heavily relies on the quality of training data  25  . Moreover, it often suffers from high sample complexity  26  due to the large number of",
        "watermark_text": "We suggest an excitable convolutional neural network ( CNN ) model for solving the 2D route planning problem in this project . The proposed approach is based on the idea that the output of each layer can be regarded as a potential field , and the finished problem will emerge when all layers are united together .We see how to train such a multi - layered CNN use backpropagation through time with gradient clipping . In addition , we also demonstrate two different methods to mix multiple fields into one single field by using either linear or nonlinear combination operations .Finally , we prove our approach on numerous benchmark problems namely maze navigation , robotics motion plan , and autonomous driving . Keywords : Convolutional Neural Network , Backpropagation Through Time , Gradient Clipping , Maze Navigation , Motion Planning , Autonomous Driving .1 Introduction Convolutional neural systems have been widely using in computer vision solutions 1 . Recently , they were applied to solve many kinds of algorithms problems 2 , which include image characterization 3 , object discovery 4 , semantic segmentation 5 , etc . .However , most existing works concentrate only on optimizing a single objective function 6 - 8 . In many real - time users , there may contain more than one objective function 9 .For instance , in robotic motion plan 10 , it generally needs finding collision - safe paths while minimizing electricity demand 11 ; in autonomous steering 12 , it must to find safe trajectories under both kinematic limits 13 and dynamic transportation conditions 14 at the same time ; in medical treatment 15 , it should consider not only disease prediction 16 but also treatment recommendation 17 simultaneously ; in computational genetics 18 , it has to optimize protein folding 19 and drug design 20 at the same time . Therefore , it becomes necessary to develop new algorithms to manage multi - goal optimization problems 21 .Recently , deep reinforcement testing 22 was introduced to address multiobjective optimization problems 23 . It learns policies directly from pure data without using hand - crafted functionality 24 .However , its reliability strongly depends on the performance of training data 25 . Moreover , it often suffers from high sample complexity 26 related to the huge amount of",
        "rewrite_text": "We propose an innovative convolutional neural network (CNN) model designed to tackle the 2D route planning challenge in this project. Our approach conceptualizes the output from each layer as a potential field, with the final solution emerging when all layers are integrated. We demonstrate how to train this multi-layered CNN using backpropagation through time while applying gradient clipping. Furthermore, we present two distinct methods for combining multiple fields into a single field, utilizing either linear or nonlinear combination techniques. Our methodology is validated against several benchmark tasks, including maze navigation, robotic motion planning, and autonomous driving. \n\nKeywords: Convolutional Neural Network, Backpropagation Through Time, Gradient Clipping, Maze Navigation, Motion Planning, Autonomous Driving.\n\n1. Introduction: Convolutional neural networks have been extensively employed in computer vision applications. Recently, they have been utilized to address a variety of algorithmic problems, including image classification, object detection, and semantic segmentation. However, most existing research focuses on optimizing a single objective function. In real-world scenarios, multiple objective functions often coexist. For example, robotic motion planning typically requires the identification of collision-free paths while minimizing energy consumption. Similarly, autonomous steering must ensure the safety of trajectories within both kinematic constraints and dynamic environmental conditions. In the healthcare sector, considerations for disease prediction and treatment recommendations must occur simultaneously. Additionally, computational genetics often involves optimizing protein folding and drug design concurrently. These complexities necessitate the development of new algorithms capable of handling multi-objective optimization problems. Recently, deep reinforcement learning has been introduced to address such multi-objective optimization scenarios. This method directly learns policies from raw data, eliminating the need for hand-crafted functions; however, its effectiveness heavily relies on the quality of the training data and often encounters challenges related to high sample complexity.",
        "ori-fast-z-score": 0.6172133998483676,
        "water-fast-z-score": 9.14991421995628,
        "rewrite-fast-z-score": 0.9313806308475994
    },
    {
        "original_text": "We report on the detection by Fermi Large Area Telescope (LAT) of prompt emission in gamma rays with energies above 100 MeV associated with GRB 080916C, one of the brightest bursts ever detected at high energy. The LAT data show that this emission is highly variable and peaks within 1 s after the onset of the burst. We find no evidence for spectral evolution during the first few hundred seconds following the trigger time. A simple power law fit to the LAT spectrum yields an index of -2.2 +/-.1 over the range 100 MeV - 10 GeV. This value is consistent with previous measurements made using Konus-Wind and AGILE satellites but differs significantly from those obtained by other instruments operating below 100 MeV. Our results are inconsistent with models which predict a softening of the photon spectrum as it evolves into the X-ray band. They also rule out scenarios where the high-energy photons originate solely from inverse Compton scattering off relativistic electrons accelerated in internal shocks. \n \n Keywords: Gamma-ray burst",
        "watermark_text": "We report on the discovery by Fermi Large Area Telescope ( LAT ) of prompt emission in gamma waves with energies above 100 MeV related with GRB 080916C , one of the brightest bursts ever observed at high energy . The LAT results show that this emission is strongly varied and spikes within 1 s after the beginning of the explosion .We see no evidence for spectral evolution during the first few hundred moments following the trigger time . A straightforward power law suited to the LAT spectrum gives an index of - 2 . 2 + / - . 1 over the range 100 MeV - 10 GeV .This value is compatible with previous measurements made using Konus - Wind and AGILE spacecraft but varies dramatically from those achieved by other satellites operating below 100 MeV . Our results are inconsistent with models which predict a softening of the photon spectrum as it evolves into the X - ray band .They even figure out situations where the high - energy photons arise solely from inverse Compton absorption off relativistic electrons accelerated in internal shocks . Keywords : Gamma - ray burst",
        "rewrite_text": "We present our findings from the Fermi Large Area Telescope (LAT), which detected prompt gamma-ray emissions with energies exceeding 100 MeV associated with GRB 080916C, one of the brightest high-energy bursts recorded. The LAT data reveal that this emission exhibits significant variability, containing spikes occurring within 1 second of the explosion's onset. Furthermore, we found no indications of spectral evolution during the initial few hundred moments following the trigger. A simple power law fit to the LAT spectrum yields an index of -2.2 ± 0.1 across the energy range of 100 MeV to 10 GeV. This index is consistent with earlier observations from the Konus-Wind and AGILE missions but diverges considerably from measurements obtained by other satellites operating below 100 MeV. Our findings challenge models that anticipate a softening of the photon spectrum as it transitions into the X-ray regime. The data even suggest scenarios where the high-energy photons originate solely from inverse Compton scattering off relativistic electrons that are accelerated in internal shocks. Keywords: Gamma-ray burst.",
        "ori-fast-z-score": -0.5933908290969266,
        "water-fast-z-score": 5.815230125149881,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present here a detailed discussion on the concept of allovalency, which is defined as the simultaneous binding to multiple sites in one molecule by different ligands (or receptors). We show that this definition does not apply to many cases where it has been used previously. In particular we discuss how multisite phosphorylation can be described within our formalism without introducing any new concepts or parameters beyond those already introduced for single-site phosphorylation. Finally, we argue why rebinding effects are negligible under most conditions relevant for signaling cascades. The concept of  allovalency  was first introduced more than 20 years ago  1  . It refers to the simultaneous binding of two or more ligands to several sites in one receptor protein  2  , see Fig 1(A) . This phenomenon occurs frequently during signal transduction processes such as kinase cascades  3  .\nThe term  allovalent  was coined because it describes a situation intermediate between monovalent and multivalent interactions  4  : while each ligand binds only once per receptor, there may exist several copies of the same ligand bound simultaneously to the same receptor. Allovalent interactions have been studied extensively both experimentally  5  and theoretically  6  . However, despite its widespread use, the precise meaning of  allovalency  remains ambiguous  7, 8  . For example, some authors define allovalency as  the simultaneous interaction with multiple sites in one molecule via different molecules   9  . Others consider allovalency to occur when  ligand molecules bind independently but cooperatively to multiple sites in one receptor molecule   10  . Yet others require that  allovalent complexes must contain at least three components   11  .",
        "watermark_text": "We see here a detailed discussion on the notion of allovalency , which is characterized as the concurrent binding to multiple sites in one molecule by various ligands ( or receptors ) . We see that this definition does not apply to many situations where it has been used earlier .In particular we explain how multisite phosphorylation can be described within our formalism without mentioning any new concepts or parameters beyond those already adopted for single - location phosphorylation . Finally , we explain why rebinding impacts are negligible under most situations relevant for pathway cascades .The concept of allovalency was first applied more than 20 years early 1 . It refers to the concurrent binding of two or more ligands to several sites in one receptor protein 2 , see Fig 1 ( A ) .This phenomenon occurs commonly during signal transduction processes such as kinase cascades 3 . The term allovalent was developed because it describes a situation intermediate between monovalent and multivalent interactions 4 : while each ligand binds only once per receptor , there may contain many versions of the same ligand attached simultaneously to the same receptor .Allovalent interactions have been studied extensively both experimentally 5 and theoretically 6 . However , despite its widespread using , the exact significance of allovalency remains ambiguous 7 , 8 .For instance , some writers define allovalency as the concurrent interaction with many sites in one molecule via different molecules 9 . Others consider allovalency to occur when ligand molecules interact independently but cooperatively to multiple sites in one receptor molecule 10 .Yet others require that allovalent complexes must include at least three components 11 .",
        "rewrite_text": "This paragraph presents a thorough examination of the concept of allovalency, defined as the simultaneous binding of various ligands (or receptors) to multiple sites on a single molecule. It highlights that this definition does not hold in several contexts where the term has been previously applied. Specifically, we elucidate how multisite phosphorylation can be articulated within our framework without introducing any new concepts or parameters beyond those used for single-site phosphorylation. Additionally, we clarify why rebinding effects are minimal in most scenarios relevant to pathway cascades. The concept of allovalency was introduced over 20 years ago and pertains to the simultaneous binding of two or more ligands to different sites on a receptor protein, as illustrated in Fig 1 (A). This phenomenon is frequently observed during signal transduction processes like kinase cascades. The term \"allovalent\" was coined to describe a scenario that is intermediate between monovalent and multivalent interactions: although each ligand binds only once to the receptor, multiple copies of the same ligand can be attached simultaneously. Allovalent interactions have been extensively researched both experimentally and theoretically. However, despite its prevalent usage, the precise interpretation of allovalency remains unclear. For example, some definitions suggest that allovalency involves concurrent interactions with multiple sites on one molecule by different molecules, while others view it as ligand molecules interacting independently yet cooperatively with several sites on a single receptor molecule. Furthermore, some definitions stipulate that allovalent complexes must contain at least three components.",
        "ori-fast-z-score": -0.4833682445228318,
        "water-fast-z-score": 6.538530488755173,
        "rewrite-fast-z-score": -0.29559878344928797
    },
    {
        "original_text": "We report on the observation of an unusual temperature dependence of the resistive transition width and its scaling with the applied magnetic field for single crystals of MgB2 grown using chemical vapor transport (CVT). The observed behavior is explained within the framework of fluctuation induced vortex pinning, which leads to a reduction of the effective volume fraction of the sample that contributes to the resistance at low temperatures. This effect becomes more pronounced when increasing the external magnetic field strength. We find evidence for this scenario by studying the magnetoresistance as well as the anisotropy of the upper critical fields Hc2(0) and Hc3(0), where we observe a strong increase towards lower temperatures. Our results are consistent with recent theoretical predictions based on microscopic calculations. \n \n In addition, our data show clear signatures of quantum oscillations originating from the Fermi surface topology of MgB2. These observations provide further insight into the electronic structure of this material.",
        "watermark_text": "We report on the observation of an peculiar thermal relation of the resistive transition width and its scaling with the applied magnetic force for single crystals of MgB2 grown utilizing chemical vapor transport ( CVT ) . The observed behavior is studied within the framework of fluctuation induced vortex locking , which results to a reduction of the effective volume fraction of the sample that adds to the tolerance at low temperatures .This phenomenon grows more pronounced when increasing the external magnetic force power . We get data for this situation by examining the magnetoresistance as well as the anisotropy of the higher critical forces Hc2 ( 0 ) and Hc3 ( 0 ) , where we study a large rise towards cooler temperatures .Our results are compatible with recent theoretical estimates based on microscopic calculations . In addition , our statistics demonstrate strong signatures of quantum oscillations originating from the Fermi surface topology of MgB2 .These measurements give further insight into the electronic topology of this material .",
        "rewrite_text": "We present our findings on a unique thermal relationship concerning the width of the resistive transition and its scaling with applied magnetic fields in single crystals of MgB2, which were synthesized using chemical vapor transport (CVT). This observed behavior is interpreted through the lens of fluctuation-induced vortex locking, leading to a decreased effective volume fraction of the sample, thereby enhancing its tolerance at low temperatures. This effect becomes more pronounced with increasing external magnetic field strength. We obtained data for this scenario by investigating magnetoresistance and the anisotropy of the higher critical fields Hc2(0) and Hc3(0), observing a significant increase as temperatures decrease. Our findings align with recent theoretical predictions based on microscopic calculations. Moreover, our analysis reveals strong evidence of quantum oscillations linked to the Fermi surface topology of MgB2, providing deeper insights into the electronic structure of this material.",
        "ori-fast-z-score": -0.5698028822981898,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 2.0647416048350555
    },
    {
        "original_text": "We present the results of subjective evaluation experiments conducted on forms designed for use within immersive environments, such as virtual reality (VR) and augmented reality (AR). The goal is to investigate how users perceive different form designs when immersed in these environments. We compare three designs: traditional 2D forms, 3D forms that are rendered using perspective projection, and 3D forms that are rendered with orthographic projection. Our findings show that there were no significant differences between the two types of 3D forms. However, both 3D forms received significantly higher ratings than their 2D counterparts. This suggests that 3D forms can be used effectively in immersive environments without requiring special rendering techniques or additional hardware. In addition, we found that participants preferred forms that had more visual cues indicating depth information over those that did not have any such cues. Finally, our study also shows that it may be possible to create effective forms by combining elements from multiple existing designs.",
        "watermark_text": "We present the results of subjective assessment research conducted on forms designed for use within immersive environments , such as virtual reality ( VR ) and augmented reality ( AR ) . The goal is to examine how people interpret different form shapes when immersed in these habitats .We contrast three models : traditional 2D forms , 3D shapes that are rendered using view projection , and 3D shapes that are rendered with orthographic projection . Our findings show that there were no considerable variations between the two forms of 3D shapes .However , both 3D shapes took significantly greater grades than their 2D counterparts . This shows that 3D shapes can be used effectively in immersive environments without using special graphics techniques or additional hardware .In addition , we concluded that participants favored forms that had more graphical cues implying depth knowledge over those that did not have any such cues . Finally , our research also shows that it could be possible to create effective forms by combining components from multiple older patterns .",
        "rewrite_text": "We present the findings from a subjective assessment study focused on forms designed for immersive environments, such as virtual reality (VR) and augmented reality (AR). The aim of this research is to explore how individuals interpret various form shapes while immersed in these environments. We compared three models: traditional 2D forms, 3D shapes rendered using view projection, and 3D shapes rendered with orthographic projection. Our results indicate that there was no significant difference between the two types of 3D shapes; however, both 3D models received notably higher ratings than their 2D counterparts. This suggests that 3D shapes can be effectively used in immersive settings without the need for advanced graphics techniques or extra hardware. Furthermore, we found that participants preferred shapes that featured more graphical cues indicating depth compared to those lacking such cues. Lastly, our research suggests that it may be possible to create effective forms by merging elements from existing design patterns.",
        "ori-fast-z-score": -1.1043152607484654,
        "water-fast-z-score": 7.067617668790178,
        "rewrite-fast-z-score": 1.6464638998453551
    },
    {
        "original_text": "We revisit the foundations of quantum mechanics by introducing an alternative formulation to the standard Feynman path integral approach, which is based on the concept of dynamical phase transition (DPT). We show that this new formalism provides a natural description for the emergence and evolution of macroscopic order in open quantum systems. In particular we demonstrate how it can be used to describe the spontaneous emission process in atomic physics, where the atom-field interaction leads to the formation of collective states with well-defined photon number statistics. The proposed framework also allows us to study the dynamics of many-body interacting systems beyond mean field theory. Finally, we discuss possible applications of our results to condensed matter physics and quantum information science. Introduction:-The development of modern theoretical approaches has led to significant progress in understanding the physical properties of complex quantum systems  1  . However, despite these advances there are still fundamental questions about the nature of quantum phenomena that remain unanswered  2  .\nIn recent years, several authors have attempted to address some of these issues using concepts borrowed from statistical mechanics  3  , such as entropy  4  or free energy  5  . These ideas were originally developed within the context of classical thermodynamics  6  but they have been recently extended to the realm of quantum mechanics  7, 8  . For example, one may consider the von Neumann entropy S = −Tr(ρ ln ρ) associated with the density matrix ρ describing the state of a system  9  . This quantity measures the amount of uncertainty present in the measurement outcomes  10  and its time derivative dS/dt gives rise to the so-called entropy production rate  11  . It was shown that this latter quantity plays a crucial role in characterizing the irreversible behavior of closed quantum systems  12  . More specifically, if the entropy production rate vanishes then the corresponding quantum mechanical model exhibits reversible dynamics  13  . On the other hand, when the entropy production rate becomes positive the system undergoes a non-equilibrium phase transition  14  .",
        "watermark_text": "We revisit the foundations of quantum mechanics by offering an additional formulation to the standard Feynman path integral approach , which is based on the notion of dynamical phase change ( DPT ) . We suggest that this new formalism gives a natural explanation for the emergence and evolution of macroscopic order in open quantum systems .In particular we prove how it can be used to explain the spontaneous emission mechanism in nuclear physics , where the atom - field interaction results to the formation of collective states with good - defined photon number statistics . The proposed framework specifically allows us to study the dynamics of several - bodies interacting systems beyond mean field theory .Finally , we explain possible applied of our findings to condensed matter science and quantum information physics . Introduction : - The advance of modern conceptual approaches has led to significant progress in understanding the physical properties of complex quantum systems 1 .However , despite these developments there are still significant questions about the nature of quantum effects that continue unanswered 2 . In recent months , various published have tried to tackle some of these problems using concepts borrowed from statistical mechanics 3 , such as entropy 4 or free energy 5 .These concepts were formerly advanced within the context of classical thermodynamics 6 but they have been lately extended to the domain of quantum mechanics 7 , 8 . For instance , one may see the von Neumann entropy S = −Tr ( ρ ln ρ ) associated with the density function ρ describing the state of a system 9 .This value measures the extent of uncertainty found in the measurement processes 10 and its time derivative dS / dt gives rise to the so - called entropy production probability 11 . It was shown that this latter quantity plays a crucial role in characterizing the irreversible behavior of closed quantum systems 12 .More specifically , if the entropy production level vanishes then the associated quantum mechanical model shows reversible dynamics 13 . On the other hand , when the entropy production level gets positive the system undergoes a non - equilibrium phase change 14 .",
        "rewrite_text": "We reexamine the foundations of quantum mechanics by introducing a new formulation that complements the standard Feynman path integral approach, centered on the concept of dynamical phase change (DPT). This innovative framework provides a coherent explanation for the emergence and evolution of macroscopic order in open quantum systems. Particularly, we demonstrate its applicability in elucidating the mechanism of spontaneous emission in nuclear physics, where the interaction between atoms and fields leads to the creation of collective states characterized by well-defined photon number statistics. Our proposed framework enables the exploration of the dynamics of multiple interacting bodies beyond the constraints of mean field theory. Additionally, we discuss the potential applications of our findings in the fields of condensed matter science and quantum information physics.\n\nIntroduction: The advancement of modern conceptual frameworks has significantly enhanced our understanding of the physical properties of complex quantum systems. Nonetheless, many fundamental questions regarding the nature of quantum effects remain unresolved. Recently, several papers have attempted to address these challenges by utilizing concepts from statistical mechanics, such as entropy and free energy. These ideas, originally formulated in the context of classical thermodynamics, have been adapted for application within quantum mechanics. For instance, the von Neumann entropy, defined as S = −Tr(ρ ln ρ), where ρ represents the density function of a system, quantifies the level of uncertainty inherent in measurement processes. The time derivative of this entropy, dS/dt, leads to the concept of entropy production probability, which is pivotal in characterizing the irreversible behavior of closed quantum systems. Specifically, when the level of entropy production is zero, the corresponding quantum mechanical model exhibits reversible dynamics; conversely, a positive entropy production indicates that the system is undergoing a non-equilibrium phase transition.",
        "ori-fast-z-score": 1.5523010514126656,
        "water-fast-z-score": 9.615384615384615,
        "rewrite-fast-z-score": 2.794782784191074
    },
    {
        "original_text": "In this work, we study spin effect on resonant tunneling characteristics in double-barrier heterostructure under longitudinal stresses by using transfer matrix method and density functional theory (DFT). We find that the energy gap decreases with increasing stress for both majority-spin electrons and minority-spin holes. The decrease is more significant for minority-spin holes than for majority-spin electrons. In addition, the transmission coefficient increases with increasing stress at low bias voltage but decreases at high bias voltage. This behavior can be explained as follows. At low bias voltage, the increase of transmission coefficient results mainly from the reduction of barrier height due to compressive stress. However, at high bias voltage, the decrease of transmission coefficient comes from two factors: one is the increase of effective mass induced by tensile stress; another is the enhancement of electron-phonon interaction caused by tensile stress. Finally, it should be noted that our calculation shows that the spin-orbit coupling has little influence on the transport properties of the system considered here.",
        "watermark_text": "In this research , we study spin effect on resonant tunneling parameters in dual - barrier heterostructure under longitudinal strain by using transfer matrix method and density functional theory ( DFT ) . We see that the power gap falls with increasing stress for both majority - spinning electrons and minority - spinning holes .The reduction is more considerable for minority - spin holes than for majority - spinning electrons . In addition , the transmission coefficient increases with varying stress at low bias frequency but decreases at high bias voltage .This phenomenon can be understood as follows . At small bias power , the increase of transmission coefficient proceeds primarily from the reduction of barrier height owing to compressive stress .However , at high bias voltage , the decrease of transmission coefficient happens from two factors : one is the improvement of effective mass induced by tensile tension ; another is the enhancement of electron - phonon interaction resulting by tensile tension . Finally , it should be mentioned that our calculation demonstrates that the spin - orbit interaction has little influence on the travel properties of the system discussed here .",
        "rewrite_text": "In this study, we investigate the effect of spin on the resonant tunneling parameters of a dual-barrier heterostructure subjected to longitudinal strain, utilizing the transfer matrix method and density functional theory (DFT). Our findings indicate that the energy gap decreases as stress increases for both majority-spin electrons and minority-spin holes, with a more significant reduction observed in minority-spin holes compared to majority-spin electrons. Furthermore, we observe that the transmission coefficient rises with increasing stress at low bias frequencies before declining at high bias voltages. This behavior can be explained as follows: at low bias power, the increase in the transmission coefficient is primarily driven by the lowering of barrier height due to compressive strain. Conversely, at high bias voltage, the reduction in the transmission coefficient is attributed to two factors: an increase in effective mass resulting from tensile strain, and an enhancement in electron-phonon interactions also caused by tensile stress. Lastly, our calculations reveal that spin-orbit interaction has minimal effect on the transport properties of the examined system.",
        "ori-fast-z-score": 0.5129891760425771,
        "water-fast-z-score": 6.668859288553502,
        "rewrite-fast-z-score": 0.7107423155935334
    },
    {
        "original_text": "We present results from direct cosmological hydrodynamic simulations that follow the formation of supermassive black holes (SMBHs) in galactic nuclei, their subsequent evolution through mergers with other SMBHs, and the associated feedback on galaxy properties. We find that:  The simulated SMBH mass function agrees well with observations at z = 0 for M• > 10^7M_solar.  At higher redshifts, our model predicts too many low-mass SMBHs compared to observational estimates based on quasar luminosity functions; this discrepancy may be due to uncertainties in the assumed duty cycle or radiative efficiency of quasars.  Our models predict an average Eddington ratio distribution that is consistent with observed distributions inferred from optical/UV emission lines.  In addition, we show that the predicted relation between BH mass and bulge velocity dispersion agrees reasonably well with observations over four orders of magnitude in BH mass.",
        "watermark_text": "We present results from direct cosmological hydrodynamic simulations that take the formation of supermassive black holes ( SMBHs ) in galactic nuclei , their ensuing evolution through mergers with other SMBHs , and the associated feedback on star dynamics . We see that : The simulated SMBH weight distribution agrees well with observations at h = 0 for M • > 10 ^ 7M _ solar .At higher redshifts , our model predicts too many lowest - weight SMBHs compared to observational projections based on quasar luminosity functions ; this discrepancy may be due to uncertainties in the expected duty cycle or radiative efficiency of quasars . Our models predict an estimated Eddington density distribution that is compatible with observed distributions inferred from optical / UV absorption lines .In addition , we prove that the expected relation between BH weight and bulge velocity dispersion agrees reasonably well with observations over four orders of magnitude in BH mass .",
        "rewrite_text": "We present findings from direct cosmological hydrodynamic simulations that explore the formation of supermassive black holes (SMBHs) in galactic nuclei, their evolution through mergers with other SMBHs, and the associated feedback on star dynamics. Our results indicate that the simulated SMBH mass distribution aligns closely with observations at h = 0 for masses greater than 10^7 M_solar. However, at higher redshifts, our model predicts an excess of low-mass SMBHs compared to observational estimates derived from quasar luminosity functions; this discrepancy might stem from uncertainties regarding the expected duty cycle or radiative efficiency of quasars. Additionally, our models suggest an Eddington density distribution that is consistent with observed distributions inferred from optical/UV absorption lines. Furthermore, we demonstrate that the anticipated relationship between black hole mass and bulge velocity dispersion is in reasonable agreement with observations across four orders of magnitude in black hole mass.",
        "ori-fast-z-score": -0.508000508000762,
        "water-fast-z-score": 4.318004318006477,
        "rewrite-fast-z-score": 0.4923659639173309
    },
    {
        "original_text": "The search is performed using data collected by the BABAR experiment at SLAC in 1999-2000, corresponding to an integrated luminosity of about 40 fb-1 . No signal candidates are observed and upper limits on the branching fraction are set as a function of the mass of the lepton pair.  These results improve upon previous measurements made with similar techniques but smaller datasets. The analysis uses a technique that exploits the kinematic properties of the final state particles to suppress backgrounds. This method has been used previously to measure the branching fractions of other rare decays such as B+ --> K*(892)0 pi+ , B+ --> D*0 pi+ , and B+ --> J/psi K- .\nPACS numbers: 11.30.Er, 12.15.Hh, 13.20.He  We report here our measurement of the branching fraction for the decay B+ --> gamma +l+nu (where l = e or mu), which proceeds through one-loop electroweak penguin diagrams involving W bosons and heavy quarks. In this process, the photon arises from the internal bremsstrahlung of the charged lepton produced in association with the neutrino. The Standard Model predicts a branching fraction of 1.1 x 10-6  1  . A number of extensions to the Standard Model predict enhancements over this value  2  .  For example, supersymmetric models can enhance the rate by several orders of magnitude  3  ; however, these predictions depend strongly on the masses of the superpartners involved  4  .",
        "watermark_text": "The survey is conducted using data taken by the BABAR study at SLAC in 1999 - 2000 , corresponding to an integrated luminosity of about 40 fb - 1 . No signal candidates are observed and upper limits on the branching fraction are set as a function of the mass of the lepton pair .These conclusions build upon recent observations made with similar method but smaller datasets . The analysis involves a technique that exploits the kinematic effects of the finished state particles to suppress patterns .This method has been used earlier to measure the branching fractions of other rare decays such as B + - - > K * ( 892 ) 0 pi + , B + - - > D * 0 pi + , and B + - - > J / psi K - . PACS scores : 11 . 30 . Er , 12 . 15 . Hh , 13 . 20 . He We report here our measurement of the branching percentage for the decay B + - - > gamma + l + nu ( where l = e or mu ) , which goes through one - ring electroweak penguin diagrams featuring W bosons and light quarks .In this process , the photon arises from the internal bremsstrahlung of the charged lepton generated in association with the neutrino . The Standard Model predicts a branching fraction of 1 . 1 x 10 - 6 1 .A variety of extensions to the Standard Model predict enhancements over this value 2 . For instance , supersymmetric theories can increase the rate by many orders of magnitude 3 ; however , these predictions rely highly on the masses of the superpartners involved 4 .",
        "rewrite_text": "The survey utilizes data from the BABAR experiment at SLAC, collected between 1999 and 2000, which corresponds to an integrated luminosity of approximately 40 fb⁻¹. No signal candidates were detected, leading to the establishment of upper limits on the branching fraction as a function of the lepton pair mass. These findings build upon recent observations using a similar technique, albeit with smaller datasets. The analysis employs a method that takes advantage of the kinematic effects of the resultant particles to filter out noise patterns. This technique has previously been successful in measuring the branching fractions of rare decays such as B⁺⁻ → K* (892)⁰π⁺, B⁺⁻ → D*⁰π⁺, and B⁺⁻ → J/ψK⁻. The PACS scores related to this study are: 11.30.Er, 12.15.Hh, and 13.20.He. In this report, we present our measurements of the branching fraction for the decay B⁺⁻ → γ + l + ν (where l represents either e or μ), which occurs via one-loop electroweak penguin diagrams that involve W bosons and light quarks. In this process, the photon is emitted from the internal bremsstrahlung of the charged lepton produced alongside the neutrino. The Standard Model predicts a branching fraction of 1.1 x 10⁻⁶. Various extensions to the Standard Model suggest potential enhancements to this value; for example, supersymmetric theories could increase the rate significantly, although these predictions are heavily dependent on the masses of the involved superpartners.",
        "ori-fast-z-score": -2.0855209398041166,
        "water-fast-z-score": 4.2808061395979236,
        "rewrite-fast-z-score": 0.43133109281375365
    },
    {
        "original_text": "We study the effect of interactions on the transport properties of double quantum dot systems with Rashba spin-orbit coupling using numerical renormalization group (NRG) calculations. We find that the system undergoes a transition to an interacting topological phase as we increase the strength of the interaction. The critical value of the interaction at which this occurs is found to be strongly dependent upon the level spacing statistics of the underlying single-particle spectrum. In particular, it decreases rapidly when the distribution becomes more localized around zero energy. This behavior can be understood by considering how the density of states evolves under RG flow. Finally, we show that the fluctuations in the local magnetic moment are suppressed near the transition point due to the formation of singlet pairs. DOI: 10.1063/1.5015481\nI. INTRODUCTORY REMARkS\nThe recent discovery of Majorana fermions has led to renewed interest in studying non-abelian anyons in condensed matter physics  1  . One promising candidate for realizing such exotic particles is provided by semiconductor nanowires  2  , where they may appear as end modes of the wire  3  or as excitations bound to vortex cores  4  .\nIn order to realize these proposals experimentally, one must first understand the effects of disorder  5  , electron-electron interactions  6  , and other sources of decoherence  7, 8  on the stability of the Majorana edge state  9  . A number of theoretical studies have been carried out recently  10  -  42  addressing some aspects of these issues. However, many open questions remain regarding the interplay among various physical mechanisms responsible for the appearance of Majoranas in realistic experimental setups.\nOne important issue concerns the role played by interactions in determining the nature of the ground state of the system. It was shown previously that repulsive interactions tend to favor the formation of a spin-singlet state over a triplet state  43  . On the other hand, attractive interactions lead to the opposite situation, i.e., the formation of a spin-triplet state instead of a singlet state  44  . These results were obtained",
        "watermark_text": "We research the impact of coupling on the transport properties of double quantum dot systems with Rashba spin - orbit interaction using numerical renormalization group ( NRG ) techniques . We see that the system undergoes a shift to an interacting topological phase as we increase the strength of the interaction .The essential value of the interaction at which this appears is found to be highly dependent upon the level spacing statistics of the underlying single - particle spectrum . In particular , it decreases quickly when the distribution gets more localized around zero energy .This phenomenon can be understood by examining how the density of states evolves under RG flow . Finally , we find that the fluctuations in the local magnetic point are suppressed near the shift point due to the formation of singlet pairs .DOI : 10 . 1063 / 1 . 5015481 I . INTRODUCTORY REMARkS The recent discovery of Majorana fermions has led to renewed interest in investigating non - abelian anyons in condensed matter science 1 .One promising candidate for realizing such unconventional particles is provided by semiconductor nanowires 2 , where they may present as end modes of the wire 3 or as excitations bound to vortex cores 4 . In order to realize these proposals experimentally , one must first realize the effects of disorder 5 , electron - atom bonding 6 , and other sources of decoherence 7 , 8 on the stability of the Majorana edge state 9 .A variety of theoretical experiments have been carried out recently 10 - 42 exploring some elements of these problems . However , many open questions remain regarding the interplay among various physical mechanisms involved for the emergence of Majoranas in ideal empirical setups .One important concern regards the part played by interactions in shaping the nature of the ground state of the system . It was shown ago that repulsive effects tend to prefer the formation of a spin - singlet state over a triplet state 43 .On the other hand , attractive interactions result to the opposite condition , i . e . , the formation of a spin - triplet state rather of a singlet state 44 . These conclusions were obtained",
        "rewrite_text": "We investigate how coupling influences the transport properties of double quantum dot systems with Rashba spin-orbit interaction using numerical renormalization group (NRG) techniques. Our results indicate that as the interaction strength increases, the system transitions into an interacting topological phase. The critical value of the interaction required for this transition is found to be significantly influenced by the level spacing statistics of the underlying single-particle spectrum. Notably, this critical value decreases rapidly as the distribution of energies becomes more localized around zero. This behavior can be understood by analyzing the evolution of the density of states under renormalization group flow. Additionally, we observe that fluctuations in the local magnetic point are reduced near the critical transition point, which is attributed to the formation of singlet pairs.\n\nDOI: 10.1063/1.5015481\n\n**I. INTRODUCTORY REMARKS**  \nThe recent discovery of Majorana fermions has spurred renewed interest in exploring non-abelian anyons within condensed matter physics. Semiconductor nanowires emerge as promising candidates for realizing such exotic particles, potentially appearing as end modes of the wire or as localized excitations bound to vortex cores. To achieve these experimental realizations, it is essential to understand the effects of disorder, electron-atom binding, and other sources of decoherence on the stability of Majorana edge states. Various theoretical studies have recently examined aspects of these challenges. Nevertheless, many unresolved questions persist regarding the interplay of different physical mechanisms necessary for the emergence of Majorana states in ideal experimental configurations. A key concern is the role of interactions in determining the nature of the system's ground state. Previous research has demonstrated that repulsive interactions tend to favor the formation of a spin-singlet state over a triplet state. Conversely, attractive interactions lead to the opposite outcome, promoting a spin-triplet state instead of a singlet.",
        "ori-fast-z-score": -0.48989794855663565,
        "water-fast-z-score": 6.740358479519649,
        "rewrite-fast-z-score": -1.150792911137501
    },
    {
        "original_text": "We present exact solutions for the electromagnetic field in the presence of spherical particles with arbitrary dielectric functions, including both metals and insulators. We show that these results can be obtained by solving Maxwell s equations using an appropriate Green function approach. The resulting expressions are used to calculate the dispersion relations for surface plasmons (SPs) and surface phonons (SPhPs). In particular we find that SPs exist only when the real part of the dielectric constant is negative while SPhPs exist even if it has positive values. Finally, we compare our results against those obtained within the classical Drude model and discuss their validity limits. Surface plasmons (SPs), which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over many decades  1  . They play important roles in various fields such as optics  2  , electronics  3  , sensing  4  , and catalysis  5  .\nRecently there has also been growing interest in studying surface phonon-polaritons (SPhPs), which are analogous excitations associated with longitudinal acoustic waves  6  . These modes occur not only at surfaces but also inside bulk materials  7, 8  where they may lead to enhanced thermal transport  9  or thermoelectricity  10  . Moreover, SPhPs can couple strongly to light  11  leading to interesting phenomena like superprism  12  and extraordinary transmission  13  effects.",
        "watermark_text": "We present precise solutions for the electromagnetic field in the presence of spherical objects with arbitrary dielectric functions , including both metals and insulators . We see that these results can be obtained by treating Maxwell s equations using an appropriate Green function method .The resulting expressions are using to estimate the dispersion relations for surface plasmons ( SPs ) and surface phonons ( SPhPs ) . In particular we find that SPs occur only when the real part of the dielectric constant is zero while SPhPs exist even if it has negative values .Finally , we compare our findings against those achieved within the classical Drude theory and consider their efficacy limits . Surface plasmons ( SPs ) , which are collective oscillations of conduction electrons at metal - dielectric connections , have been heavily explored over numerous years 1 .They play essential roles in different fields such as optics 2 , electronics 3 , sensing 4 , and catalysis 5 . Recently there has especially been growing interest in investigating surface phonon - polaritons ( SPhPs ) , which are analogous excitations associated with longitudinal acoustic waves 6 .These modes happen not only at surfaces but also inside bulk surfaces 7 , 8 where they may contribute to heightened thermal transport 9 or thermoelectricity 10 . Moreover , SPhPs can close intensely to light 11 contributing to curious phenomena like superprism 12 and exceptional transmission 13 phenomena .",
        "rewrite_text": "We provide accurate solutions for the electromagnetic field surrounding spherical objects with general dielectric properties, encompassing both metals and insulators. These results can be derived by applying an appropriate Green function method to Maxwell's equations. The derived expressions are utilized to analyze the dispersion relations for surface plasmons (SPs) and surface phonon-polaritons (SPhPs). Notably, we discover that SPs can only exist when the real part of the dielectric constant is zero, whereas SPhPs can exist even when this value is negative. Additionally, we compare our findings with those obtained through classical Drude theory and discuss their limitations. Surface plasmons (SPs), which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over the years. They play significant roles in various fields, including optics, electronics, sensing, and catalysis. Recently, there has been increased interest in the study of surface phonon-polaritons (SPhPs), which are related excitations corresponding to longitudinal acoustic waves. These modes appear not only at surfaces but also within bulk materials, potentially enhancing thermal transport and thermoelectric effects. Moreover, SPhPs can interact closely with light, leading to intriguing phenomena such as superprism effects and exceptional transmission.",
        "ori-fast-z-score": -0.8164965809277261,
        "water-fast-z-score": 6.25846794771944,
        "rewrite-fast-z-score": 0.6324555320336759
    },
    {
        "original_text": "We present new results on the kinematics and dynamics of the circumnuclear region (CNR) of NGC 1097, based on integral field spectroscopy with SINFONI at VLT/UT4. We find that the CNR is dominated by two components: an inner disk-like structure extending to about 1 kpc radius, and a fast outflowing component along PA∼45°extending up to 5 kpc distance from the nucleus. The latter shows blueshifted emission lines indicating radial velocities between -500 km/s and -1000 km/s relative to systemic velocity. This outflow has been previously detected using optical IFU data as well as HST imaging. In addition we detect a second slower outflow component perpendicular to this one, which extends over 3 kpc towards north-east direction. Both outflows are likely driven by nuclear jets originating from the central engine. Using simple models for the gas distribution and kinematics, we estimate mass inflow rates into the CNR ranging from 0.1 M⊙ yr-1 to several solar masses per year.",
        "watermark_text": "We report new data on the kinematics and dynamics of the circumnuclear portion ( CNR ) of NGC 1097 , using on integral field spectroscopy with SINFONI at VLT / UT4 . We see that the CNR is dominated by two parts : an inner disk - like structure extending to about 1 kpc radius , and a slow outflowing component along [UNK] up to 5 kpc length from the nucleus .The latter shows blueshifted emission lines suggesting radial velocities between - 500 km / s and - 1000 kilometers / s relative to systemic speed . This outflow has been previously observed using optical IFU data as well as HST scanning .In addition we perceive a second smaller outflow portion parallel to this one , which extends over 3 kpc towards north - eastward direction . Both outflows are likely generated by nuclear jets arising from the main motor .Using simple methods for the gas distribution and kinematics , we estimate mass inflow rates into the CNR ranging from 0 . 1 [UNK] yr - 1 to several solar masses per year .",
        "rewrite_text": "We present new findings on the kinematics and dynamics of the circumnuclear region (CNR) of NGC 1097, obtained through integral field spectroscopy with SINFONI at the VLT/UT4. Our analysis reveals that the CNR consists of two dominant components: an inner disk-like structure that extends approximately 1 kpc in radius, and a slow outflowing feature along the identified axis that extends up to 5 kpc from the nucleus. This outflow exhibits blueshifted emission lines, indicating radial velocities between -500 km/s and -1000 km/s relative to the systemic speed. Previous observations of this outflow have been made using optical integral field unit (IFU) data and HST scans. Additionally, we observe a smaller outflow component that runs parallel to the primary outflow, extending 3 kpc in a northeast direction. Both outflows are likely driven by nuclear jets emanating from the active nucleus. By employing straightforward methods to analyze the gas distribution and kinematics, we estimate the mass inflow rates into the CNR to range from 0.1 solar masses per year to several solar masses per year.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 5.498051602938211,
        "rewrite-fast-z-score": -0.4588314677411235
    },
    {
        "original_text": "We present the two-loop corrections to nuclear matter within an effective field theory approach based on chiral perturbation theory and relativistic mean-field theory. The calculation is performed using dimensional regularization, minimal subtraction at one loop order and the modified minimal subtraction scheme at higher orders. We find that the results are consistent with previous calculations obtained by other methods such as Feynman diagrams or dispersion relations. In addition we show how our method can be used to calculate the energy density functional up to next-to-next-to-leading order. This work was supported by the U.S. \n\n\nDepartment of Energy under Contract No. DE-AC02-05CH11231. PACS: 11.10.Kk, 12.38.Gc, 14.20.Dh, 25 .75.-q, 26.60.+z, 27.40.-n, 27.70.+f, 28.30.-j",
        "watermark_text": "We present the two - loop corrections to nuclear material within an efficient field model approach based on chiral perturbation theory and relativistic mean - field theory . The calculation is conducted using dimensional regularization , minimal subtraction at one loop order and the modified minimal subtraction scheme at higher orders .We see that the results are compatible with previous calculations obtained by other methods such as Feynman diagrams or dispersion relations . In addition we prove how our technique can be used to estimate the power concentration functional up to next - to - next - to - leading order .This project was supported by the U . S . Department of Energy under Contract No . DE - AC02 - 05CH11231 .PACS : 11 . 10 . Kk , 12 . 38 . Gc , 14 . 20 . Dh , 25 . 75 . - q , 26 . 60 . + z , 27 . 40 . - h , 27 . 70 . + f , 28 . 30 . - j",
        "rewrite_text": "We discuss the two-loop corrections to nuclear matter using an efficient field model approach that combines chiral perturbation theory and relativistic mean-field theory. The calculations are performed with dimensional regularization, employing minimal subtraction at one-loop order and the modified minimal subtraction scheme at higher orders. Our results align well with those from previous calculations conducted through alternative methods, such as Feynman diagrams and dispersion relations. Furthermore, we demonstrate how our technique can be utilized to estimate the power concentration functional up to the next-to-next-to-leading order. This research was funded by the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. PACS: 11.10.Kk, 12.38.Gc, 14.20.Dh, 25.75.-q, 26.60.+z, 27.40.-h, 27.70.+f, 28.30.-j.",
        "ori-fast-z-score": 0.282842712474619,
        "water-fast-z-score": 3.394112549695428,
        "rewrite-fast-z-score": 1.386750490563073
    },
    {
        "original_text": "We report the discovery of a new satellite galaxy, dubbed  A pair of bootes  (ApoBootes), orbiting around our Galaxy at a distance of about 300 kpc in projection and with an estimated mass of 1.5 x 10^10 M_sun . ApoBootes is located on the opposite side of the Galactic center to the Magellanic Clouds and has a very low surface brightness. We have used deep near-infrared images taken by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this object. The photometric properties are consistent with those expected for a dwarf spheroidal galaxy. This work was supported by the Australian Research Council Discovery Project funding scheme under grant DP130104011. We present evidence that ApoBootes may be associated with a previously known overdensity of stars discovered by Belokurov et al. (2007) using SDSS data.",
        "watermark_text": "We report the discovery of a new satellite galaxy , nicknamed A couple of bootes ( ApoBootes ) , orbiting around our Galaxy at a distance of about 300 kpc in projection and with an estimated mass of 1 . 5 x 10 ^ 10 M _ sun . ApoBootes is situated on the opposite end of the Galactic center to the Magellanic Clouds and has a very low exterior brightness .We have utilized deep near - infrared images obtained by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this body . The photometric properties are compatible with those expected for a dwarf spheroidal galaxy .This research was supported by the Australian Research Council Discovery Project grant program under grant DP130104011 . We present evidence that ApoBootes might be identified with a previously known overdensity of stars found by Belokurov et al .( 2007 ) used SDSS information .",
        "rewrite_text": "We announce the discovery of a new satellite galaxy, informally named ApoBootes, located approximately 300 kpc away from our Galaxy in projection, with an estimated mass of 1.5 x 10^10 M_sun. ApoBootes is positioned on the opposite side of the Galactic center from the Magellanic Clouds and exhibits very low surface brightness. Our identification of this galaxy was made using deep near-infrared images obtained from the VISTA telescope as part of the Vista Variables in the Via Lactea survey. Its photometric characteristics align with those anticipated for a dwarf spheroidal galaxy. This research received funding from the Australian Research Council's Discovery Project grant program under grant DP130104011. Furthermore, we present evidence suggesting that ApoBootes may correspond to a previously recognized overdensity of stars identified by Belokurov et al. (2007) utilizing SDSS data.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.437601569801833,
        "rewrite-fast-z-score": 2.626128657194451
    },
    {
        "original_text": "The book is intended as an introduction to the physics and mathematics underlying the problem of how proteins fold into their native three-dimensional structures.  The main topics covered are statistical mechanics, molecular dynamics simulations, energy landscapes, phase transitions in complex systems, and self-organization phenomena. It also contains several examples illustrating these concepts using simple models of proteins. This book will be useful not only for graduate students studying biophysics but also for researchers interested in interdisciplinary problems such as pattern formation or self-assembly processes. Contents:  Preface 1 Introduction 2 Statistical mechanics 3 Energy landscape 4 Phase transition 5 Self-organization 6 Molecular dynamics 7 Protein folding 8 References 9 Index 10 Appendix A 11 Appendix B 12 Bibliography 13 Acknowledgements 14 Glossary 15 Answers to selected exercises 16 Solutions to selected exercises 17 Sample chapters 18 Sample solutions 19 Sample data 20 Sample programs 21 Sample animations 22 Sample movies 23 Sample figures",
        "watermark_text": "The text is intended as an intro to the physics and mathematics underlying the question of how proteins turn into their natural three - dimensional shapes . The main fields examined are mathematical mechanics , molecular biology simulations , energy landscapes , phase transitions in complex systems , and self - assembly processes .It additionally contains several examples illustrating these concepts using simple models of proteins . This book will be valuable not only for college students studying biophysics but also for researchers interested in interdisciplinary problems such as sequence development or self - assembly processes .Contents : Preface 1 Introduction 2 Statistical mechanics 3 Energy landscape 4 Phase transition 5 Self - agreement 6 Molecular dynamics 7 Protein folding 8 References 9 Index 10 Appendix A 11 Appendix B 12 Bibliography 13 Acknowledgements 14 Glossary 15 Answers to selected exercises 16 Solutions to selected exercises 17 Sample chapters 18 Sample solutions 19 Sample information 20 Sample programs 21 Sample animations 22 Sample videos 23 Sample figures",
        "rewrite_text": "This text serves as an introduction to the physics and mathematics involved in understanding how proteins achieve their natural three-dimensional structures. The primary focus areas include mathematical mechanics, molecular biology simulations, energy landscapes, phase transitions in complex systems, and self-assembly phenomena. It also features several straightforward examples that illustrate these concepts through simple protein models. This book is designed to benefit not only college students pursuing biophysics but also researchers engaged in interdisciplinary studies, such as sequence evolution or self-assembly mechanisms. \n\nContents include: Preface; 1. Introduction; 2. Statistical Mechanics; 3. Energy Landscape; 4. Phase Transition; 5. Self-Agreement; 6. Molecular Dynamics; 7. Protein Folding; 8. References; 9. Index; 10. Appendix A; 11. Appendix B; 12. Bibliography; 13. Acknowledgments; 14. Glossary; 15. Answers to Selected Exercises; 16. Solutions to Selected Exercises; 17. Sample Chapters; 18. Sample Solutions; 19. Sample Information; 20. Sample Programs; 21. Sample Animations; 22. Sample Videos; 23. Sample Figures.",
        "ori-fast-z-score": 1.2309149097933272,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": 1.3130643285972254
    },
    {
        "original_text": "We report on an imaging survey carried out in 2004-05 using speckle differential imager (SDI) at the Very Large Telescope (VLT) and Multiple Mirror Telescope (MMT). We have detected no companions down to ~5 AU projected separation within 50 mas of 45 young stars. The detection limits are estimated by injecting fake planets into real data sets and recovering them through PSF subtraction techniques. Our results suggest that there is little or no excess number of close-in giant planets orbiting these nearby young stars compared to field stars. This result may be explained if most extrasolar planets form beyond 5 AU but migrate inward during their formation process. Alternatively, it could also mean that planet formation is suppressed near the central star due to photoevaporation and/or tidal effects.  These results will provide important constraints on models of planet migration as well as planet formation theories. Keywords: Planet search; Nearby stars",
        "watermark_text": "We report on an imaging survey conducted out in 2004 - 05 combining speckle differential imager ( SDI ) at the Very Large Telescope ( VLT ) and Multiple Mirror Telescope ( MMT ) . We have discovered no companions down to ~ 5 AU estimated separation within 50 mas of 45 young stars .The detection limits are estimated by injecting fake stars into real information sets and recovering them through PSF subtraction techniques . Our results show that there is much or no excess amount of close - in massive planets orbiting these nearby young galaxies compared to field stars .This result may be understood if most extrasolar stars create beyond 5 AU but migrate inward during their formed mechanism . Alternatively , it could also mean that planet development is suppressed near the main star due to photoevaporation and / or tidal issues .These data will provide important restrictions on predictions of planet migration as well as planet development explanations . Keywords : Planet search ; Nearby stars",
        "rewrite_text": "We present findings from an imaging survey carried out between 2004 and 2005, utilizing speckle differential imaging (SDI) at both the Very Large Telescope (VLT) and the Multiple Mirror Telescope (MMT). Our survey revealed no companions within approximately 5 AU estimated separation, down to 50 mas, around 45 young stars. We determined our detection limits by injecting simulated stars into actual data sets and recovering them using PSF subtraction methods. The results indicate a lack of close-in massive planets orbiting these nearby young stars when compared to field stars. This could suggest that most extrasolar planets form beyond 5 AU and later migrate inward during their formation process. Alternatively, it may imply that planet formation is hindered close to the main star due to effects like photoevaporation and tidal interactions. These findings will significantly inform models of planet migration and development. Keywords: Planet search; Nearby stars.",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 5.578319375835658,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "We present an analytical model for the magneto-rotational instability (MRI) in protoplanetary disks, which is based on the assumption that the disk can be divided into two regions with different physical properties and dynamics. The inner region has a high density and temperature, while the outer one is less dense but hotter than the surrounding medium. We show how this simple picture allows us to reproduce many observed features of MRI-driven turbulence in accretion disks around young stars. In particular, we find that:  -The growth rate of the fastest growing mode decreases rapidly towards smaller radii due to the increasing gas pressure.  -The radial profile of the turbulent viscosity follows closely the profile of the magnetic field strength.  -The angular momentum transport efficiency increases strongly at small radii because of the rapid increase of the surface density there.  -The predicted mass accretion rates are consistent with those inferred observationally for T Tauri stars.",
        "watermark_text": "We present an analytical theory for the magneto - rotational instability ( MRI ) in protoplanetary disks , which is based on the assumption that the disk can be broken into two zones with varying material structures and dynamics . The outer sector has a high density and heat , while the inner one is greater dense but brighter than the nearby medium .We see how this straightforward photo lets us to depict many observed features of MRI - driven turbulence in accretion balls around early stars . In particular , we find that : - The growth speed of the fastest growing mode decreases quickly towards smaller radii due to the increasing gas pressure .- The radial profile of the chaotic viscosity follows carefully the profile of the magnetic force strength . - The angular velocity transport rate grows heavily at small radii because of the quick expansion of the surface volume there .- The predicted mass accretion levels are compatible with those inferred observationally for T Tauri stars .",
        "rewrite_text": "We develop an analytical theory for the magneto-rotational instability (MRI) in protoplanetary disks, grounded in the premise that the disk can be divided into two distinct zones with varying material properties and dynamics. The outer region is characterized by high density and thermal energy, while the inner region is denser and more luminous than the surrounding medium. This clear framework allows us to explain numerous observed characteristics of MRI-driven turbulence in accretion disks surrounding young stars. Specifically, we discover that: - The growth rate of the fastest-growing mode rapidly decreases at smaller radii due to increasing gas pressure. - The radial profile of chaotic viscosity closely follows that of magnetic force strength. - The rate of angular momentum transport significantly increases at smaller radii due to the rapid expansion of surface volume. - The estimated mass accretion rates align well with those observed for T Tauri stars.",
        "ori-fast-z-score": -1.2535663410560174,
        "water-fast-z-score": 5.8119893994415355,
        "rewrite-fast-z-score": 0.22645540682891913
    },
    {
        "original_text": "We present near-infrared (NIR) polarimetry and spectroscopy for the bipolar reflection nebula IRAS 19312; 1950 . The NIR polarization vectors are aligned with those in optical images, indicating that they trace scattered light from an illuminating source located behind the dense molecular cloud core. We find evidence for two distinct scattering regions along our line-of-sight to this object; one is associated with the brightest part of the nebula, while another region shows lower polarization degrees but higher polarized fluxes at longer wavelengths. These results suggest that there may be multiple sources contributing to the observed emission. In addition, we detect several absorption features which can be attributed to water ice on dust grains near the central star(s). Our observations also reveal a possible outflow cavity traced by blueshifted CO emission lines. This work was supported by NASA through grant number HST-GO-11775.0-A from STScI, operated under NASA contract NAS5-26555.",
        "watermark_text": "We use near - infrared ( NIR ) polarimetry and spectroscopy for the bipolar reflection nebula IRAS 19312 ; 1950 . The NIR polarization vectors are aligned with those in imaging photographs , showing that they trace stray light from an illuminating source located behind the deep molecular mist center .We see evidence for two separate scattering regions along our line - of - seeing to this body ; one is associated with the brightest part of the nebula , while another region shows lesser polarization degrees but higher polarized fluxes at shorter wavelengths . These conclusions propose that there may be several sources responsible to the seen emission .In addition , we find various absorption properties which can be due to liquid ice on dust grains near the primary star ( s ) . Our observations also confirm a possible outflow cavity identified by blueshifted CO radiation patterns .This project was supported by NASA through grant number HST - GO - 11775 . 0 - A from STScI , operated under NASA contract NAS5 - 26555 .",
        "rewrite_text": "We employ near-infrared (NIR) polarimetry and spectroscopy to study the bipolar reflection nebula IRAS 19312+1950. The NIR polarization vectors align with those observed in imaging photographs, indicating that they correspond to scattered light from an illuminating source situated behind the dense molecular mist at the center of the nebula. Our observations reveal two distinct scattering regions along our line of sight; one is linked to the brightest portion of the nebula, while the other exhibits lower degrees of polarization but higher polarized fluxes at shorter wavelengths. These findings suggest that multiple sources may be contributing to the observed emission. Furthermore, we identify various absorption properties that could be attributed to the presence of liquid water ice on dust grains near the primary star(s). Additionally, our observations confirm the existence of a potential outflow cavity, indicated by blueshifted CO radiation patterns. This research was supported by NASA under grant number HST-GO-11775.0-A from the Space Telescope Science Institute (STScI), which is operated under NASA contract NAS5-26555.",
        "ori-fast-z-score": -0.9701425001453319,
        "water-fast-z-score": 6.305926250944657,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We present an analysis of the first two years (Feb 2005 -Jan 2007) of data taken by the Swift satellite, which has been designed to detect and study gamma ray bursts (GRBs). We find that GRB 050904 at z = 6.3 is the most distant object ever detected in the electromagnetic spectrum. The prompt emission was observed over more than four orders of magnitude in energy, from radio waves up to X-rays. This burst also had one of the highest fluences recorded so far for any GRB. In addition we report on another burst, GRB 080913, whose afterglow was found to be variable on timescales as short as 1 minute. These results are discussed within the context of current models for GRB production. Keywords: Gamma-ray burst, High-redshift universe, Afterglows, Swift satellite. Gamma-ray bursts (GRBs), intense flashes of high-energy radiation lasting only milliseconds, have now been discovered out to redshifts greater than six  1  . Their extreme luminosities make them powerful probes into the early Universe  2  , but their origin remains unknown  3  .\nSwift  4  , launched in November 2004, carries three instruments capable of detecting GRBs across the entire electromagnetic spectrum  5  : the Burst Alert Telescope  6  detects GRBs via their X-ray and/or optical emissions; the Ultraviolet/Optical Telescope  7  observes the afterglow through ultraviolet and visible light; and the X-ray telescope  8  monitors the afterglow s decaying flux. Here we describe our initial findings using these instruments during the first two years of operation. \nThe Burst Alert Telescope\n\nBurst Alert Telescope Observations of GRB 050904\nOn September 5 th , 2006, the Burst Alert Telescope triggered on a bright source located at RA=05h54m36.6s Dec=-69d21 59.6   9  . Follow-up observations revealed this event to be a new record holder among GRBs  10  . Its peak photon count rate reached 2 x 10 4 photons s -1 cm -2 in the 15-150 keV band  11  . It lasted about",
        "watermark_text": "We present an assessment of the first two years ( Feb 2005 - Jan 2007 ) of evidence gained by the Swift satellite , which has been designed to identify and track γ ray clusters ( GRBs ) . We see that GRB 050904 at z = 6 . 3 is the most distant object ever observed in the electromagnetic spectrum .The prompt emission was seen over more than four orders of magnitude in energy , from radio beams up to X - rays . This burst also had one of the highest fluences recorded so far for any GRB .In addition we paper on another burst , GRB 080913 , whose afterglow was shown to be varying on timescales as short as 1 minute . These conclusions are discussed within the context of recent versions for GRB development .Keywords : Gamma - ray flare , High - redshift universe , Afterglows , Swift satellite . Gamma - ray clusters ( GRBs ) , intense pulses of high - energy rays lasting only milliseconds , have now been detected out to redshifts greater than six 1 .Their intense luminosities give them potent probes into the early Universe 2 , but their source remains unidentified 3 . Swift 4 , launched in November 2004 , carries three devices capable of detecting GRBs across the entire electromagnetic spectrum 5 : the Burst Alert Telescope 6 detects GRBs via their X - ray and / or laser emissions ; the Ultraviolet / Optical Telescope 7 sees the afterglow through ultraviolet and visible radiation ; and the X - ray observatory 8 monitors the afterglow s decaying flux .Here we explain our first findings using these instruments during the first two years of operation . The Burst Alert Telescope Burst Alert Telescope Observations of GRB 050904 On September 5 th , 2006 , the Burst Alert Telescope triggered on a bright source located at RA = 05h54m36 . 6s Dec = - 69d21 59 . 6 9 .Follow - up observations showed this event to be a new record holder among GRBs 10 . Its peak photon number rate reached 2 x 10 4 photons s - 1 cm - 2 in the 15 - 150 keV band 11 .It lasted about",
        "rewrite_text": "We provide an evaluation of the initial two years (February 2005 - January 2007) of data obtained from the Swift satellite, which was specifically engineered to detect and monitor gamma-ray bursts (GRBs). Notably, we identify GRB 050904 at a redshift of z = 6.3 as the farthest object ever observed in the electromagnetic spectrum. This burst exhibited prompt emissions across an extensive energy range, spanning more than four orders of magnitude, from radio waves to X-rays, and it has recorded one of the highest fluences for any GRB to date. Additionally, we discuss another event, GRB 080913, whose afterglow displayed variability on timescales as brief as one minute. These findings are contextualized within recent theories regarding the development of GRBs. \n\nKeywords: Gamma-ray flare, High-redshift universe, Afterglows, Swift satellite. Gamma-ray bursts (GRBs), characterized by intense high-energy pulses lasting mere milliseconds, have now been detected at redshifts greater than six. Their remarkable luminosities make them powerful tools for probing the early Universe, although their origins remain elusive. Swift, launched in November 2004, is equipped with three instruments designed to detect GRBs across the full spectrum of electromagnetic radiation. The Burst Alert Telescope detects GRBs by their X-ray and/or optical emissions, the Ultraviolet/Optical Telescope observes the afterglow in ultraviolet and visible light, and the X-ray Observatory tracks the fading flux of the afterglow. Here, we present our initial findings from the first two years of operations using these instruments.\n\nOn September 5, 2006, the Burst Alert Telescope detected a bright source with coordinates RA = 05h54m36.6s and Dec = -69d21'59.6\". Subsequent observations revealed this event to be a new record holder among GRBs, with a peak photon number rate reaching 2 x 10^4 photons s^-1 cm^-2 in the 15-150 keV range. The duration of this event was approximately",
        "ori-fast-z-score": 0.4703604341917986,
        "water-fast-z-score": 7.243550686553699,
        "rewrite-fast-z-score": 1.756550621379892
    },
    {
        "original_text": "We study the interplay between thermal percolating states and jammed states in disordered systems by using Monte Carlo simulations for dimers adsorbing onto binary alloy surfaces with different compositions. We find that there is an optimal composition at which both types of states coexist, leading to a maximum entropy production rate. The coexistence state has been observed experimentally as well. Our results provide new insights into how energy can be transferred most efficiently through complex networks. Energy transfer efficiency plays a crucial role in many physical processes such as heat conduction  1  , chemical reactions  2  , and biological transport  3  . In particular, it determines whether or not a system will reach equilibrium  4  .\nIn this work we focus on one specific type of non-equilibrium process -thermal percolation  5  . Thermal percolation occurs when particles are injected randomly into a network  6  . Particles then diffuse along the network until they encounter each other  7, 8  . When two particles meet, their energies combine irreversibly  9  . This leads to a cascade-like spreading of particle density  10  . As more particles are added, the number of clusters increases  11  . Eventually these clusters merge together  12  forming a single cluster spanning across the entire network  13  . At this point all particles have combined into a giant cluster  14  . It was shown recently  15  that the transition from isolated clusters to a single connected cluster corresponds to a phase transition  16  . For example, in the case of random resistor networks  17  , the transition temperature T c depends only on the average resistance R av  18  :\n, where k B is Boltzmann s constant  19  . However, if the distribution of resistances P (R) is broad enough  20  , the transition becomes first-order  21  .",
        "watermark_text": "We research the interplay between thermal percolating states and jammed states in disordered systems by using Monte Carlo simulations for dimers adsorbing onto binary alloy surfaces with various compositions . We see that there is an appropriate composition at which both types of states coexist , leading to a maximum entropy production speed .The coexistence state has been observed experimentally as also . Our results bring fresh insights into how energy can be transferred most efficiently through complex networks .Energy transfer efficiency plays a crucial role in different mechanical reactions such as heat conduction 1 , chemical processes 2 , and biological transport 3 . In particular , it determines whether or not a system will achieve optimal 4 .In this research we focus on one specific sort of non - equilibrium process - temperature percolation 5 . Thermal percolation occurs when molecules are pumped randomly into a network 6 .Particles then diffuse along the channel until they encounter each other 7 , 8 . When two particles contact , their energies combine irreversibly 9 .This leads to a cascade - like movement of particle concentration 10 . As more particles are adding , the total of clusters increases 11 .Eventually these complexes combined together 12 producing a single cluster spans across the entire system 13 . At this time all fragments have combined into a giant cluster 14 .It was shown recently 15 that the shift from separated clusters to a single connected cluster corresponds to a phase shift 16 . For instance , in the case of distributed resistor networks 17 , the transition rate T c varies only on the average resistance R av 18 : , where k B is Boltzmann s constant 19 .However , if the distribution of resistances P ( R ) is broad sufficiently 20 , the shift remains first - order 21 .",
        "rewrite_text": "We investigate the relationship between thermal percolating states and jammed states within disordered systems through Monte Carlo simulations involving dimers that adsorb onto binary alloy surfaces of varying compositions. Our findings indicate that there exists a specific composition where both states coexist, resulting in a peak in entropy production rate. This coexistence has also been observed in experimental settings. Our research provides new insights into the efficient transfer of energy within complex networks. The efficiency of energy transfer is vital in various mechanical reactions, including heat conduction, chemical processes, and biological transport, as it determines a system's ability to reach optimal conditions. We specifically focus on one type of non-equilibrium process: temperature percolation, which occurs when molecules are randomly introduced into a network. The particles then diffuse through channels until they meet, resulting in an irreversible combination of their energies. This leads to a cascade effect in particle concentration; as more particles are added, the number of clusters increases. Ultimately, these clusters merge into a single entity that spans the entire system, with all fragments coalescing into a giant cluster. Recent studies have indicated that the transition from isolated clusters to a single connected cluster signifies a phase transition. For example, in distributed resistor networks, the transition rate Tc is influenced solely by the average resistance R_av, with Boltzmann's constant kB playing a role. However, if the distribution of resistances P(R) is sufficiently broad, this transition remains first-order.",
        "ori-fast-z-score": -0.09016696346674323,
        "water-fast-z-score": 7.244860247099318,
        "rewrite-fast-z-score": 1.3525044520011484
    },
    {
        "original_text": "We report on the controlled collision between an individual trapped atom and a singly charged ion in a Paul trap, where both particles are confined to different regions of space separated by several micrometers. The ions can be moved along the axis connecting them with the atoms using electrostatic fields generated by electrodes outside the vacuum chamber. We demonstrate that this allows us to control the relative velocity at which they collide as well as their impact parameter. This opens up new possibilities for studying fundamental processes such as elastic scattering or charge exchange reactions. In addition we show how it is possible to use these techniques to prepare entanglement between two neutral atoms via a quantum gate operation mediated by one common ion. Quantum information processing requires scalable systems based on many qubits  1  . One promising approach towards realizing such devices relies on neutral atoms stored in optical lattices  2  , but suffers from limited coherence times due to spontaneous emission  3  . An alternative route involves storing atomic qubits in ensembles of trapped ions  4  . However, here too there exist severe limitations arising from decoherence caused by heating  5  .\nIn order to overcome these difficulties, hybrid approaches have been proposed  6  combining advantages of both schemes  7, 8  . Here, the storage of quantum states takes place in a small number of highly coherent ions while large numbers of neutral atoms serve as flying qubits  9  . A crucial requirement for implementing such schemes is the ability to perform high-fidelity operations involving both types of qubit  10  . For example, it has recently been shown experimentally  11  that it is possible to entangle two neutral atoms via a shared ion  12  . To achieve this goal, however, the atoms need to interact with each other before being released into free flight  13  .",
        "watermark_text": "We report on the controlled collision between an individual captured molecule and a singly charged particle in a Paul cage , where both particles are localized to different regions of space separated by many micrometers . The atoms can be moved along the axis linking them with the atoms using electrostatic fields generated by electrodes outside the vacuum chamber .We suggest that this enables us to predict the relative velocity at which they collide as also as their impact parameter . This opens up new possibilities for studying basic processes such as elastic scattering or charge transfer reactions .In addition we show how it is easy to use these techniques to make entanglement between two neutral ions via a quantum gate action mediated by one common ion . Quantum electronic processing requires scalable systems based on various qubits 1 .One promising alternative towards developing such devices relies on neutral particles deposited in laser lattices 2 , but suffers from reduced coherence times due to spontaneous emission 3 . An alternative approach requires storing atomic qubits in ensembles of confined ions 4 .However , here too there remain considerable restrictions arose from decoherence caused by temperature 5 . In try to overcome these problems , hybrid approaches have been proposed 6 combining characteristics of both schemes 7 , 8 .Here , the storage of quantum states takes place in a small number of highly coherent electrons while small numbers of neutral particles serve as flying qubits 9 . A crucial requirement for employing such schemes is the ability to conduct high - fidelity operations involving both types of qubit 10 .For instance , it has recently been shown experimentally 11 that it is possible to entangle two neutral ions via a shared ion 12 . To achieve this goal , however , the atoms need to interact with each other before being transferred into free flight 13 .",
        "rewrite_text": "We present findings on the precise collision between a single trapped molecule and a singly charged particle within a Paul trap, with both particles positioned in distinct spatial regions separated by several micrometers. The atoms can be maneuvered along the axis connecting them using electrostatic fields produced by external electrodes in the vacuum chamber. This capability allows us to estimate both the relative velocity at which they collide and their impact parameter. Such advancements pave the way for exploring fundamental processes, including elastic scattering and charge transfer reactions. Furthermore, we demonstrate how these techniques can facilitate the entanglement of two neutral ions through quantum gate operations mediated by a shared ion. Quantum electronic processing demands scalable systems utilizing various qubits. One promising avenue for developing such devices involves neutral particles trapped in laser lattices; however, these systems experience reduced coherence times due to spontaneous emission. Alternatively, storing atomic qubits in ensembles of confined ions presents challenges related to decoherence influenced by temperature. To address these issues, hybrid approaches have been proposed that combine the advantages of both strategies. In this framework, quantum states are stored in a limited number of highly coherent electrons, while a small number of neutral particles function as flying qubits. A pivotal requirement for the effective implementation of these approaches is the capability to perform high-fidelity operations involving both qubit types. Recent experimental evidence has shown the feasibility of entangling two neutral ions via an intermediary ion. However, for this to succeed, the atoms must interact with each other prior to their transition to free flight.",
        "ori-fast-z-score": 1.3241694217637887,
        "water-fast-z-score": 8.765781549553733,
        "rewrite-fast-z-score": 1.3620104492139977
    },
    {
        "original_text": "The chemical compositions of the type II Cepheid variables are investigated by using high-resolution spectra obtained with Subaru/HDS, Keck/HIRES, VLT/UVES, and McDonald/DEIMOS spectrographs.  We find that all of our sample stars have solar or super-solar metallicities ( Fe/H >+0.2). In addition to Fe I lines, we also use Mg I triplet lines in order to determine their atmospheric parameters more accurately. For some of these stars, we detect significant overabundances for CNO elements as well as s-process elements; however, there is no clear correlation between the abundance patterns and pulsation periods. These results suggest that the observed abundance anomalies may be caused by mass transfer during binary evolution rather than internal mixing processes associated with stellar pulsations. This work was supported by JSPS Grant-in-Aid for Scientific Research on Priority Areas No. 16071203.",
        "watermark_text": "The chemical compositions of the class II Cepheid variables are examined by using high - resolution spectra obtained with Subaru / HDS , Keck / HIRES , VLT / UVES , and McDonald / DEIMOS spectrographs . We see that all of our sample stars have solar or super - solar metallicities ( Fe / H > + 0 . 2 ) .In addition to Fe I lines , we also need Mg I triplet lines in order to predict their atmospheric parameters more accurately . For some of these stars , we perceive considerable overabundances for CNO elements as well as s - process elements ; however , there is no clear correlation between the abundance patterns and pulsation periods .These data suggest that the reported abundance anomalies may be caused by mass transfer during binary development rather than internal mix processes associated with stars pulsations . This research was supported by JSPS Grant - in - Aid for Scientific Research on Priority Areas No .16071203.",
        "rewrite_text": "The chemical compositions of class II Cepheid variables have been analyzed using high-resolution spectra obtained from Subaru/HDS, Keck/HIRES, VLT/UVES, and McDonald/DEIMOS spectrographs. Our findings indicate that all stars in our sample exhibit solar or super-solar metallicities (Fe/H > +0.2). In addition to Fe I lines, the inclusion of Mg I triplet lines is essential for more accurately predicting their atmospheric parameters. For several of these stars, we observe significant overabundances in carbon, nitrogen, and oxygen (CNO) elements, as well as s-process elements; however, no clear correlation is evident between these abundance patterns and their pulsation periods. These results imply that the observed abundance anomalies may arise from mass transfer during binary evolution rather than from internal mixing processes linked to stellar pulsations. This research was supported by JSPS Grant-in-Aid for Scientific Research on Priority Areas No. 16071203.",
        "ori-fast-z-score": 1.1920791213585393,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": 2.286002286003429
    },
    {
        "original_text": "We study the slow wave resonance (SWR) effect for periodically layered media with an arbitrary number N of anisotropic layers, each characterized by its own permittivity tensor and thickness. We show that SWR is possible only if all principal axes of the permittivity tensors are parallel to one another within each layer. In this case we derive explicit expressions for the dispersion relation between the frequency f and the Bloch wavenumber kx. The results obtained can be used as guidelines for designing multilayered structures exhibiting strong SWR effects at low frequencies. \n \n Keywords: Slow wave resonance; Anisotropy; Multilayer structure; Dispersion relations. 1 Introduction \n \n Periodic multilayers consisting of alternating thin films made of different materials have attracted considerable attention during recent years due to their unique properties  1  . These include high reflectance  2  , negative refraction  3  , enhanced nonlinear optical response  4  , etc., which make them promising candidates for various applications such as optoelectronic devices  5  or photovoltaics  6  .\n \nIn particular, it has been shown recently  7–9  that periodic multilayers composed of anisotropic layers may exhibit very interesting electromagnetic phenomena including slow wave resonance (S WR). This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium  10  . It leads to extremely large values of the effective refractive index n eff = c / v ph  11  where c is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode  12  . As a result, the corresponding transmission spectrum exhibits sharp peaks associated with narrow stop bands  13  . Such features are highly desirable for many practical applications  14  . \n \n However, despite numerous theoretical studies devoted to S WR in periodic multilayers  15–18  , there still exist several open questions related to the conditions under which this phenomenon takes place  19, 20  . For example, it was found experimentally  21  that the presence of a single misaligned anisotropic layer destroys the S WR effect completely even though other layers remain perfectly aligned. On the other hand, numerical simulations  22  suggest that",
        "watermark_text": "We explore the slow frequency resonance ( SWR ) effect for regularly layered media with an arbitrary number N of anisotropic layers , each described by its own permittivity matrix and thickness . We see that SWR is possible only if all primary directions of the permittivity tensors are connected to one another within each surface .In this situation we derive explicit expressions for the dispersion constant between the frequency f and the Bloch wavenumber kx . The results derived can be used as guidelines for constructing multilayered buildings presenting strong SWR effects at low frequencies .Keywords : Slow wave vibration ; Anisotropy ; Multilayer structure ; Dispersion relations . 1 Introduction Periodic multilayers consisting of alternating thin sheets formed of different materials have garnered considerable scrutiny during recent seasons due to their distinct characteristics 1 .These include high reflectance 2 , negative refraction 3 , enhanced nonlinear optical reaction 4 , etc . , which make them promising candidates for various uses such as optoelectronic technologies 5 or photovoltaics 6 . In particular , it has been shown ago 7 – 9 that periodic multilayers consisting of anisotropic surfaces may exhibit very interesting electrical processes including slow frequency resonance ( S WR ) .This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium 10 . It results to incredibly large values of the effective refractive index n eff = c / u ph 11 where p is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode 12 .As a result , the associated transmission spectrum exhibits severe spikes identified with narrow stop rings 13 . Such characteristics are extremely practical for numerous practical applications 14 .However , despite several theoretical experiments devoted to S WR in periodic multilayers 15 – 18 , there still appear several open questions related to the conditions under which this phenomenon happens place 19 , 20 . For instance , it was shown experimentally 21 that the presence of a single misaligned anisotropic surface destroys the S WR effect totally even though other layers remain perfectly aligned .On the other hand , numerical simulations 22 suggest that",
        "rewrite_text": "We investigate the slow frequency resonance (SWR) effect in periodically layered media comprising an arbitrary number \\( N \\) of anisotropic layers, each characterized by its distinct permittivity matrix and thickness. Our findings indicate that SWR can only manifest if all primary directions of the permittivity tensors are interconnected within each layer’s surface. In these conditions, we derive explicit expressions for the relationship between frequency \\( f \\) and the Bloch wavenumber \\( k_x \\). The results can serve as a framework for designing multilayered structures that demonstrate pronounced SWR effects at low frequencies. \n\n**Keywords:** Slow wave vibration; Anisotropy; Multilayer structure; Dispersion relations.\n\n**1. Introduction**  \nPeriodic multilayers comprising alternating thin sheets made of different materials have attracted significant attention in recent years due to their unique properties. These include high reflectance, negative refraction, and enhanced nonlinear optical responses, making them promising candidates for various applications in optoelectronic technology and photovoltaics. Notably, prior research has demonstrated that periodic multilayers with anisotropic surfaces can exhibit intriguing electrical phenomena, such as slow frequency resonance (SWR). This effect occurs when the phase velocity of Bloch waves becomes zero within the medium, leading to extraordinarily large values of the effective refractive index \\( n_{\\text{eff}} = c / v_{\\text{ph}} \\), where \\( c \\) is the speed of light in a vacuum and \\( v_{\\text{ph}} \\) is the phase velocity of the propagating Bloch mode. Consequently, the transmission spectrum displays sharp peaks associated with narrow stop bands. These features offer significant practical advantages for various applications. However, despite numerous theoretical studies on SWR in periodic multilayers, several questions remain regarding the conditions under which this phenomenon occurs. For example, experimental evidence has shown that a single misaligned anisotropic surface can completely disrupt the SWR effect, even if other layers are perfectly aligned. In contrast, numerical simulations suggest that...",
        "ori-fast-z-score": -0.15811388300841897,
        "water-fast-z-score": 8.432781346758377,
        "rewrite-fast-z-score": 2.99127270878144
    },
    {
        "original_text": "We present the results of our analysis on the mass-to-light ratio (M/L) of Lyman alpha emitters (LAEs), based on data obtained by Subaru/Suprime-Cam in the fields surrounding four quasars with redshifts 5.7 < zqso < 6.6. We use photometric redshift techniques to select LAE candidates within a narrow window around each quasar s emission line redshift. The M/L values are derived using SED fitting for individual galaxies as well as stacked samples. Our main conclusions are:\n(1) For individual objects we find that the median value of log(M/LB) is ~-1.8 dex over the range of luminosities probed here.  This result suggests that most LAEs have low stellar masses compared to typical LBGs.\n(2) Stacking analyses show that the average M/LB increases towards higher luminosity bins. In particular, the highest luminosity bin has an average log(M/LB ) = -1.3 ± 0.4 dex which corresponds to a factor of 3 lower than what would be expected if these systems were dominated by old stars alone.",
        "watermark_text": "We present the conclusion of our analysis on the mass - to - light percentage ( M / L ) of Lyman alpha emitters ( LAEs ) , using on evidence derived by Subaru / Suprime - Cam in the fields surrounding four quasars with redshifts 5 . 7 < zqso < 6 . 6 . We use photometric redshift methods to select LAE candidates within a small frame around each quasar s absorption system redshift .The M / L values are derived using SED fitting for individual galaxies as well as piled specimens . Our main results are : ( 1 ) For individual objects we find that the average value of log ( M / LB ) is ~ - 1 . 8 dex over the range of luminosities probed here .This result suggests that most LAEs have lowest stellar masses compared to normal LBGs . ( 2 ) Stacking calculations show that the average M / LB increases towards higher luminosity bins .In particular , the highest luminosity bin has an estimated log ( M / LB ) = - 1 . 3 ± 0 . 4 dex which equals to a factor of 3 lower than what would be anticipated if these systems were dominated by ancient stars alone .",
        "rewrite_text": "We present our findings on the mass-to-light ratio (M/L) of Lyman Alpha Emitters (LAEs), based on data from Subaru/Suprime-Cam in regions surrounding four quasars with redshifts ranging from 5.7 to 6.6. By employing photometric redshift techniques, we identified LAE candidates within a narrow frame around the absorption system redshift of each quasar. We calculated M/L ratios through spectral energy distribution (SED) fitting for individual galaxies as well as for composite samples. Our key findings are: (1) For individual LAEs, we determined that the average log(M/L_B) is approximately -1.8 dex across the luminosity range examined, indicating that most LAEs have lower stellar masses compared to typical Lyman Break Galaxies (LBGs). (2) Stacking analysis reveals that the average M/L_B increases with higher luminosity bins; notably, the highest luminosity bin has an estimated log(M/L_B) of -1.3 ± 0.4 dex, suggesting it is a factor of three lower than what would be expected if these systems were solely composed of ancient stars.",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": -0.24618298195866545
    },
    {
        "original_text": "We report on results obtained by INTEGRAL observations during the 2005 outburst of the black hole candidate GRO J1655â€“40 (Nova Muscae 1991). The source was observed in the 20-100 keV range for about 100 days, starting at MJD 53000 and ending at MJD 53300. We have analyzed these data using both ISGRI and SPI instruments aboard INTEGRAL satellite. In addition to the main spectral component which is well described by a power law model modified by an exponential cutoff, we find that there are two additional components present in the spectrum. One of them has been previously reported by other authors but its origin remains unclear. Another one appears only when fitting the whole dataset simultaneously with all three models considered here -power law plus exponential cut-off, broken power law or Comptonization model-. This new feature can be interpreted either as a reflection hump produced by cold material surrounding the central X-ray source or as a broad iron line around 6.4 keV.",
        "watermark_text": "We report on findings obtained by INTEGRAL observations during the 2005 outburst of the dark hole member GRO J1655â€ “ 40 ( Nova Muscae 1991 ) . The source was seen in the 20 - 100 keV range for about 100 months , beginning at MJD 53000 and ending at MJD 53300 .We have analyzed these information using both ISGRI and SPI instruments aboard INTEGRAL satellite . In addition to the main spectral component which is well described by a power law description altered by an exponential cutoff , we find that there are two additional components available in the spectrum .One of them has been previously reported by other researchers but its identity remains unsure . Another one appears only when fitting the whole dataset jointly with all three models described here - energy law plus exponential cutting - off , broken power law or Comptonization model - .This new feature can be interpreted either as a absorption hump produced by cold matter surrounding the main X - ray source or as a broad iron line around 6 . 4 keV .",
        "rewrite_text": "We present findings from INTEGRAL observations during the 2005 outburst of the black hole binary GRO J1655–40 (Nova Muscae 1991). The source was monitored in the 20-100 keV range for approximately 100 months, starting from MJD 53000 and concluding at MJD 53300. We conducted an analysis using both the ISGRI and SPI instruments on the INTEGRAL satellite. In addition to the primary spectral component, which is well-characterized by a power law model with an exponential cutoff, we identified two additional components in the spectrum. One of these has been noted in previous studies, though its exact nature remains uncertain. The other component becomes apparent only when fitting the entire dataset using all three models described: energy law with an exponential cutoff, broken power law, or Comptonization model. This new feature could be interpreted as an absorption hump resulting from cold matter surrounding the main X-ray source, or as a broad iron line around 6.4 keV.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 6.305926250944657,
        "rewrite-fast-z-score": 2.182820625326997
    },
    {
        "original_text": "We study the remnants produced by merging two CO WDs with different masses and spin configurations, using fully general relativistic hydrodynamic simulations. We find that for most cases studied here (except when one WD is very massive), the merger product will be an ellipsoidal object surrounded by a thick disk-like structure. The final mass of this merged system ranges between 0.6-1.1 M⊙ depending on the initial conditions. For some systems we also observe the formation of a thin accretion disk around the central core. In addition to these results, we show that there are several possible outcomes in terms of the amount of material ejected during the merger process. Depending on the total mass of the binary system, the merger can produce either no or significant amounts of ejecta ranging up to 10−2M⊙. Finally, we discuss how our results compare with previous studies as well as future prospects. \n \n Keywords: White dwarfs",
        "watermark_text": "We research the remnants created by merging two CO WDs with various masses and spin configurations , using fully basic relativistic hydrodynamic simulations . We see that for most instances explored here ( except when one WD is very huge ) , the merger result will be an ellipsoidal object surrounded by a thick disk - like structure .The final weight of this fused core runs between 0 . 6 - 1 . 1 [UNK] depending on the early conditions . For some systems we also observe the formation of a thin accretion ring around the main core .In addition to these results , we find that there are several possible outcomes in terms of the amount of material displaced during the merger process . Depending on the total mass of the binary system , the merger can produce either no or significant amounts of ejecta ranging up to [UNK] .Finally , we explain how our findings compare with previous research as well as future prospects . Keywords : White dwarfs",
        "rewrite_text": "We investigate the remnants resulting from the merger of two CO white dwarfs (WDs) with varying masses and spin configurations through fully fundamental relativistic hydrodynamic simulations. Our findings indicate that, in most scenarios examined (with the exception of cases where one WD is particularly massive), the result of the merger is an ellipsoidal object encircled by a thick disk-like structure. The mass of the resulting core ranges from 0.6 to 1.1 solar masses, contingent upon the initial conditions. In some systems, we also note the emergence of a thin accretion ring surrounding the main core. Furthermore, we discover several potential outcomes regarding the volume of material displaced during the merger. Depending on the total mass of the binary system, the merger can result in either negligible or substantial ejecta, with amounts reaching significant levels. Lastly, we discuss how our results align with previous studies and outline future research directions. Keywords: White dwarfs.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.103161130233569,
        "rewrite-fast-z-score": 1.885618083164127
    },
    {
        "original_text": "Erbium-doped fiber amplifiers (EDFAs) are widely used in the field of telecommunication and sensing applications due to their high gain, low noise figure, wide bandwidths and compatibility with silica fibers. However, EDFAs have some disadvantages such as high cost and limited lifetime because they require expensive rare-earth elements like erbium ions. In this study, we report on the preparation of new glass-ceramic materials based on sodium chloride and potassium chloride phosphates containing different concentrations of Er3+ ions by melt-quenching method. The structural characterization was performed using X-ray powder diffraction analysis (XRPD), scanning electron microscopy (SEM), energy dispersive spectroscopy (EDS). Optical absorption spectra were recorded at room temperature between 300 nm and 1600 nm. Photoluminescence emission measurements were carried out under 980-nm excitation wavelength. It is found that the prepared samples show good transparency over a broad range of wavelengths up to 1600 nm.",
        "watermark_text": "Erbium - doped cable amplifiers ( EDFAs ) are widely useful in the field of telecommunication and monitoring applications due to their high gain , low noise figure , large bandwidths and compatibility with silica fibers . However , EDFAs have some disadvantages such as great price and poor lifetime because they use cost rare - earth elements like erbium ions .In this study , we publish on the preparation of new glazed - glass compounds based on sodium salts and potassium chloride phosphates containing different amounts of Er3 + ions by melt - quenching procedure . The structural characterization was done utilizing X - ray powder diffraction assessment ( XRPD ) , scanning electron microscopy ( SEM ) , energy dispersive spectroscopy ( EDS ) .Optical absorption spectra were collected at room temperature between 300 nm and 1600 nm . Photoluminescence radiation observations were carried out under 980 - nm excitation wavelength .It is found that the prepared specimens display good transparency over a broad variety of wavelengths up to 1600 nm .",
        "rewrite_text": "Erbium-doped fiber amplifiers (EDFAs) are widely utilized in telecommunications and monitoring applications because of their high gain, low noise figure, broad bandwidth, and compatibility with silica fibers. However, EDFAs also face challenges, including high costs and limited lifespans due to the use of rare-earth elements like erbium ions. This study presents the preparation of new glazed-glass compounds based on sodium salts and potassium chloride phosphates, containing varying amounts of Er3+ ions, through a melt-quenching process. Structural characterization was conducted using X-ray powder diffraction (XRPD), scanning electron microscopy (SEM), and energy dispersive spectroscopy (EDS). Optical absorption spectra were measured at room temperature in the range of 300 nm to 1600 nm, and photoluminescence was observed under a 980-nm excitation wavelength. The results indicate that the prepared samples exhibit good transparency across a wide range of wavelengths, extending to 1600 nm.",
        "ori-fast-z-score": 0.5852057359806528,
        "water-fast-z-score": 5.969098507002659,
        "rewrite-fast-z-score": 1.6059101370939322
    },
    {
        "original_text": "We present new spectroscopic observations for the open cluster NGC 1883, which is located at a distance of about 1 kpc in the constellation Cassiopeia (α = 20 h 18 m , δ = +58°). The data were obtained with the 2-m telescope of the Observatorio Astronómico Nacional de San Pedro Mártir (OAN-SPM) on December 16-17, 2009 using the REOSC spectrograph equipped with grism #7 covering the wavelength range 3700-7000 Å. We measured RVs for 23 stars by cross-correlating their spectra against those of template dwarfs observed under similar conditions. Our results show that most of these objects have heliocentric velocities between -40 to -50 km/sec, while only two stars are found outside this interval. These values agree well with previous determinations based on photometric methods. \n \n In addition we derived metallicities  Fe/H  for 14 stars using the calibration of Alonso et al. (1999) . For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from -0.10 dex up to +0.20 dex. Only one object shows an iron abundance significantly higher than solar value (+0.30 dex). \n \n Finally, we compared our results with previously published studies.",
        "watermark_text": "We report new spectroscopic observations for the open cluster NGC 1883 , which is situated at a distance of about 1 kpc in the constellation Cassiopeia ( α = 20 h 18 m , δ = + 58° ) . The data were obtained with the 2 - m observatory of the Observatorio Astronómico Nacional de San Pedro Mártir ( OAN - SPM ) on December 16 - 17 , 2009 using the REOSC spectrograph equipped with grism # 7 representing the frequency spectrum 3700 - 7000 Å .We calculated RVs for 23 stars by cross - correlating their spectra against those of template dwarfs observed under analogous conditions . Our results show that most of these objects have heliocentric velocities between - 40 to - 50 km / sec , while only two stars are found outside this interval .These quantities agree well with previous determinations based on photometric technique . In addition we derived metallicities Fe / H for 14 stars using the calibration of Alonso et al .( 1999 ) . For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from - 0 . 10 dex up to + 0 . 20 dex .Only one element shows an metal density greatly higher than solar value ( + 0 . 30 dex ) . Finally , we compared our findings with previously reported surveys .",
        "rewrite_text": "We present new spectroscopic observations of the open cluster NGC 1883, located approximately 1 kpc away in the constellation Cassiopeia (α = 20h 18m, δ = +58°). The data were collected using the 2-meter telescope at the Observatorio Astronómico Nacional de San Pedro Mártir (OAN-SPM) on December 16-17, 2009, with the REOSC spectrograph utilizing grism #7, covering the wavelength range of 3700 to 7000 Å. By cross-correlating the spectra of 23 stars with those of template dwarfs observed under similar conditions, we calculated their radial velocities (RVs). Our results reveal that the majority of these stars exhibit heliocentric velocities ranging from -40 to -50 km/s, with only two stars found outside this range. These findings align well with prior measurements obtained through photometric methods. Additionally, we determined the metallicities [Fe/H] for 14 stars using the calibration from Alonso et al. (1999). Except for one star, our measurements indicate solar or slightly subsolar metallicities, ranging from -0.10 dex to +0.20 dex. Only one star exhibited a significantly higher metallicity (+0.30 dex). Lastly, we compared our results with previously reported surveys.",
        "ori-fast-z-score": 0.3779644730092272,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 1.016001016001524
    },
    {
        "original_text": "We propose an explicit construction for a string model with gauge group SU(3)xSU(2)xU(1), three generations of quarks and leptons, right-handed neutrinos, and no exotic particles beyond those in the minimal supersymmetric standard model (MSSM). The model is based on type IIA string theory compactified to four dimensions on a Calabi-Yau manifold X which has h^{1,1} = 1 and h^{1,2} = 0. We consider a stack of N=2 D6-branes wrapping a divisor S inside X that supports a non-abelian gauge symmetry U(N_c) where N_c = 3. In addition we introduce two sets of fractional branes at singularities of order 6 along divisors T_1 and T_2 such that the total number of D6-branes is 24. By using mirror symmetry arguments it can be shown that this configuration preserves one half of the original supersymmetry.",
        "watermark_text": "We suggest an explicit construction for a string description with gauge group SU ( 3 ) xSU ( 2 ) xU ( 1 ) , three generations of quarks and leptons , right - handed neutrinos , and no exotic electrons beyond those in the reduced supersymmetric standard description ( MSSM ) . The model is based on type IIA string theory compactified to four dimensions on a Calabi - Yau manifold X which has h ^ { 1 , 1 } = 1 and h ^ { 1 , 2 } = 0 .We consider a stack of N = 2 D6 - branes wrapping a divisor S inside X that supports a non - abelian gauge symmetry U ( N _ c ) where N _ c = 3 . In addition we provide two sets of fractional branes at singularities of order 6 along divisors T _ 1 and T _ 2 such that the total number of D6 - branes is 24 .By using mirror symmetry arguments it can be shown that this configuration captures one part of the original supersymmetry .",
        "rewrite_text": "We propose a detailed construction for a string model featuring a gauge group of SU(3) × SU(2) × U(1), incorporating three generations of quarks and leptons, right-handed neutrinos, and excluding any exotic electrons aside from those in the minimal supersymmetric standard model (MSSM). This model utilizes type IIA string theory compactified to four dimensions on a Calabi-Yau manifold X, characterized by h^{1,1} = 1 and h^{1,2} = 0. Within this framework, we consider a stack of N = 2 D6-branes that wrap a divisor S in X, which supports a non-abelian gauge symmetry U(N_c) where N_c = 3. Additionally, we introduce two sets of fractional branes located at order 6 singularities along divisors T_1 and T_2, resulting in a total of 24 D6-branes. Utilizing mirror symmetry arguments, we demonstrate that this configuration retains a portion of the original supersymmetry.",
        "ori-fast-z-score": 1.1547005383792517,
        "water-fast-z-score": 3.464101615137755,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "We present the results of spectral analysis for all Swift bursts with measured redshifts and durations longer than 2 s, using data obtained by the Burst Alert Telescope (BAT) on board Swift satellite. We find that most of these bursts are best described as blackbody emission in combination with an additional power-law component at higher energies. The temperature of this blackbody component is found to be correlated with the peak energy of the spectrum E p . This correlation can be explained if we assume that the observed blackbody emission comes from photospheric radius expansion during the prompt phase of the burst. In addition, there seems to exist another correlation between the blackbody temperature T bb , the luminosity L iso and the duration t 90 .\nThe existence of such correlations suggests that the physical mechanism responsible for producing the blackbody emission may also play some role in determining other properties of the bursts. \n\n\nIntroduction\n\nGamma-ray bursts (GRB), discovered more than 40 years ago  1  , have been studied extensively since their discovery  2  . However, many questions about them remain unanswered  3  . One important question concerns the origin of the gamma-rays produced in GRBs  4  . It has been suggested that they could come from internal shocks  5  or magnetic reconnection  6  within relativistic jets launched by collapsing massive stars  7, 8  . Alternatively, it was proposed that they might result from external shocks driven into surrounding medium  9  . Another open issue is whether GRBs are standard candles  10  . If so, then one would expect that different bursts should show similar temporal and spectral behaviors  11  . On the contrary, observations suggest that GRBs exhibit large diversity  12  . Finally, the nature of the progenitors of GRBs remains unknown  13  .",
        "watermark_text": "We present the conclusion of spectral study for all Swift bursts with recorded redshifts and durations greater than 2 s , using data acquired by the Burst Alert Telescope ( BAT ) on board Swift satellite . We see that most of these bursts are best described as blackbody emission in combination with an additional power - law component at higher energies .The temperature of this blackbody element is found to be correlated with the maximum energy of the spectrum E p . This variance can be understood if we suppose that the seen blackbody emission arises from photospheric radius expansion during the prompt phase of the explosion .In addition , there seems to arise another interaction between the blackbody altitude T bb , the luminosity L iso and the duration t 90 . The fact of such correlations indicates that the physical process used for producing the blackbody emission may even hold some role in determining other properties of the pulses .Introduction Gamma - ray bursts ( GRB ) , detected more than 40 years previously 1 , have been studied thoroughly since their discovery 2 . However , many issues about them remain unanswered 3 .One important question concerns the origin of the gamma - radiation generated in GRBs 4 . It has been proposed that they may come from internal shocks 5 or gravitational reconnection 6 within relativistic jets launched by collapsing large galaxies 7 , 8 .Alternatively , it was suggested that they may come from external shocks driven into surrounding medium 9 . Another open problems is whether GRBs are standard candles 10 .If so , then one would suggest that different bursts should exhibit similar temporal and spectral patterns 11 . On the contrary , observations suggest that GRBs exhibit great diversity 12 .Finally , the nature of the progenitors of GRBs remains unidentified 13 .",
        "rewrite_text": "We present the results of our spectral analysis of all Swift bursts with measured redshifts and durations exceeding 2 seconds, utilizing data collected by the Burst Alert Telescope (BAT) aboard the Swift satellite. Our findings indicate that most of these bursts are best characterized by a combination of blackbody emission and an additional power-law component at higher energies. We have discovered a correlation between the temperature of the blackbody component and the peak energy of the spectrum, E_p. This relationship can be explained by the hypothesis that the observed blackbody emission originates from the expansion of the photospheric radius during the prompt phase of the explosion. Moreover, there appears to be a significant interaction among the blackbody temperature (T_bb), the isotropic luminosity (L_iso), and the duration (t_90). These observed correlations suggest that the mechanisms responsible for the blackbody emission may also influence other properties of the bursts. \n\n**Introduction**: Gamma-ray bursts (GRBs), which have been detected for over 40 years, have been extensively studied since their discovery. Nevertheless, many questions about them remain unanswered. One critical inquiry pertains to the source of the gamma radiation produced in GRBs. Various theories propose that this radiation may result from internal shocks or gravitational reconnection within relativistic jets originating from the collapse of massive galaxies. Alternatively, it has been suggested that GRBs could be the result of external shocks impacting the surrounding medium. Another unresolved issue is whether GRBs function as standard candles; if they do, one would expect different bursts to display similar temporal and spectral characteristics. In contrast, observations show a remarkable diversity among GRBs. Lastly, the nature of the progenitors of these bursts is still unidentified.",
        "ori-fast-z-score": 0.18257418583505536,
        "water-fast-z-score": 8.272727272727273,
        "rewrite-fast-z-score": 0.4662524041201569
    },
    {
        "original_text": "We study numerically and experimentally the dynamics of a liquid droplet bouncing on an inclined plane covered with superhydrophobic coating, which is known to be able to support stable levitation of drops in air.  We show that the lifetimes of such bouncing droplets are determined by their initial kinetic energy. The dependence of the lifetime on this energy can be fitted well using a power law t ~ E0−α where α = 0.5 ± 0.1 for both numerical simulations and experiments. This scaling behavior suggests that the lifetime of a bouncer depends only weakly on its initial velocity. In addition we find that the maximum height reached during each bounce decreases as the number of bounces increases. Finally, we demonstrate how these results can be used to estimate the surface tension of water based on experimental data. Bouncing droplets have been studied extensively over recent years due to their potential applications in microfluidics  1  . These systems typically consist of millimeter-sized droplets impacting onto hydrophobic surfaces  2  , but they also include smaller droplets bouncing off super-hydrophobic coatings  3  .\nIn many cases it has been observed that the droplets exhibit periodic motion  4  -  6  . However, there exist some examples of non-periodic bouncing  7, 8  or even chaotic trajectories  9  . It was shown recently  10  that the lifetimes (i.e., the times between successive impacts) of bouncing droplets depend strongly on their initial velocities. For example, if the initial speed is too high then the droplet will not bounce at all; instead it will slide down the surface until it reaches the bottom  11  . On the other hand, if the initial speed lies below a certain threshold value then the droplet will bounce indefinitely  12  .",
        "watermark_text": "We research numerically and experimentally the dynamics of a liquid droplet bouncing on an inclined plane covered with superhydrophobic coating , which is known to be possible to support steady levitation of drops in air . We see that the lifetimes of such bouncing droplets are decided by their initial kinetic power .The dependence of the lifetime on this power can be fit well using a power law t ~ E0−α where α = 0 . 5 ± 0 . 1 for both numerical simulations and experiments . This scaling behavior suggests that the life of a bouncer relies only faintly on its initial velocity .In addition we find that the maximum length reached during each jump varies as the quantity of bounces increases . Finally , we prove how these results can be used to estimate the surface tension of water based on experimental evidence .Bouncing droplets have been studied frequently over recent history owing to their potential applications in microfluidics 1 . These systems commonly consist of millimeter - sized droplets impacting onto hydrophobic surfaces 2 , but they also involve smaller droplets scattering off super - hydrophobic coatings 3 .In many situations it has been observed that the droplets display periodic motion 4 - 6 . However , there exist some examples of non - periodic bouncing 7 , 8 or even chaotic trajectories 9 .It was shown recently 10 that the lifetimes ( i . e . , the periods between successive impacts ) of tumbling droplets depend greatly on their initial velocities . For instance , if the first velocity is too high then the droplet will not drop at all ; merely it will slide down the surface until it meets the bottom 11 .On the other hand , if the first velocity lies below a certain level level then the droplet will bounce indefinitely 12 .",
        "rewrite_text": "We investigate both numerically and experimentally the dynamics of a liquid droplet bouncing on an inclined plane with a superhydrophobic coating, which has been shown to facilitate the steady levitation of droplets in air. Our findings indicate that the lifetimes of these bouncing droplets depend primarily on their initial kinetic energy. The relationship between lifetime and kinetic energy can be accurately described by a power law, t ~ E₀⁻ᵡ, where α = 0.5 ± 0.1, applicable to both our numerical simulations and experimental results. This scaling behavior implies that the lifetime of a bouncing droplet is only weakly influenced by its initial velocity. Additionally, we observe that the maximum height achieved during each bounce changes with the number of bounces. Furthermore, we demonstrate how these findings can be utilized to estimate the surface tension of water based on experimental data. Bouncing droplets have garnered significant interest in recent years due to their potential applications in microfluidics. These systems often involve millimeter-sized droplets impacting hydrophobic surfaces, but they also include smaller droplets interacting with superhydrophobic coatings. In numerous cases, droplets exhibit periodic motion; however, examples of non-periodic or even chaotic bouncing trajectories have also been documented. Recent studies have highlighted that the lifetimes (i.e., the intervals between successive impacts) of bouncing droplets are highly sensitive to their initial velocities. For example, if the initial velocity is excessively high, the droplet will slide down the surface rather than bounce. Conversely, if the initial velocity is below a certain threshold, the droplet can bounce indefinitely.",
        "ori-fast-z-score": 0.3682298471593294,
        "water-fast-z-score": 7.118652518223773,
        "rewrite-fast-z-score": -0.2773500981126145
    },
    {
        "original_text": "We propose an optical scheme for implementing the Gaussian valence bond (GVB) state, which is one of the most important classes of quantum states in condensed matter physics. The GVB state can be used to describe many-body systems with strong correlations such as spin liquids or Mott insulators. We show that our proposed scheme allows us to distribute entanglement between two distant parties by using only linear optics elements and single-photon sources. Our results may have potential applications in quantum information processing. \n \n Introduction \n \n Quantum entanglement plays a crucial role in various fields ranging from quantum communication  1  , quantum metrology  2  , quantum sensing  3  , and quantum computing  4  . In particular, it has been shown that quantum entangled states are useful resources for quantum teleportation  5  , superdense coding  6  , remote state preparation  7  , and quantum key distribution  8  .\n \nIn recent years, there has been growing interest in studying quantum entanglement in many-body systems  9  -  11  . For example, the ground-state wavefunction of strongly correlated fermions on lattices can be written as a product of local singlet pairs known as valence bonds  12  . This class of states is called valence-bond solid (VBS) states  13  . It was later found that VBS states can also be represented by so-called valence bond basis  14  . These states include the famous Néel state  15  describing antiferromagnetic order  16  , the Haldane phase  17  corresponding to integer-spin chains  18  , and the Affleck-Kennedy-Lieb-Tasaki (AKLT) model  19  representing gapped spin-1/2 chain  20  . \n \n Recently, several schemes  21 -  23  were proposed to generate these types of quantum states experimentally. However, all existing proposals require nonlinear interactions among photons  24  and/or complicated setups  25  . Therefore, they cannot be implemented easily in practice. On the other hand, some experimental demonstrations  26  -  28  have been performed recently to produce photonic qubits  29  . Thus, it would be interesting if we could find ways to implement these quantum states without requiring any nonlinear interaction  30  .",
        "watermark_text": "We suggest an optical scheme for incorporating the Gaussian valence bond ( GVB ) state , which is one of the most important classes of quantum states in condensed matter science . The GVB state can be used to define multiple - bodies systems with powerful correlations such as spin liquids or Mott insulators .We see that our proposed system enables us to distribute entanglement between two distant participants by using only linear optics components and single - photon sources . Our results may have potential applications in quantum information processing .Introduction Quantum entanglement plays a crucial role in different fields ranging from particle communication 1 , quantum metrology 2 , quantum sensing 3 , and quantum computing 4 . In particular , it has been shown that quantum entangled states are valuable resources for quantum teleportation 5 , superdense coding 6 , remote state formation 7 , and quantum key distribution 8 .In past decades , there has been growing interest in investigating quantum entanglement in large - bodies systems 9 - 11 . For instance , the ground - state wavefunction of highly correlated fermions on lattices can be written as a product of local singlet sets known as valence bonds 12 .This family of states is called valence - bond solid ( VBS ) states 13 . It was later known that VBS states can also be described by so - called valence bond basis 14 .These states contain the famous Néel state 15 describing antiferromagnetic order 16 , the Haldane phase 17 corresponding to integer - spin rings 18 , and the Affleck - Kennedy - Lieb - Tasaki ( AKLT ) model 19 representing gapped spin - 1 / 2 chain 20 . Recently , various strategies 21 - 23 were developed to produce these kinds of quantum states experimentally .However , all previous designs require nonlinear interactions among photons 24 and / or complicated setups 25 . Therefore , they cannot be deployed easily in practice .On the other hand , some experimental tests 26 - 28 have been performed recently to produce photonic qubits 29 . Thus , it would be exciting if we could discover ways to execute these quantum states without using any nonlinear interaction 30 .",
        "rewrite_text": "We propose an optical scheme that incorporates the Gaussian valence bond (GVB) state, which is a key class of quantum states in condensed matter physics. The GVB state facilitates the definition of many-body systems with significant correlations, such as spin liquids and Mott insulators. Our proposed system allows for the distribution of entanglement between two distant participants using only linear optical components and single-photon sources. This has potential applications in quantum information processing.\n\nIntroduction: Quantum entanglement is fundamental across various fields, including particle communication, quantum metrology, quantum sensing, and quantum computing. Quantum entangled states are particularly valuable resources for applications such as quantum teleportation, superdense coding, remote state formation, and quantum key distribution. In recent decades, the study of quantum entanglement in large-body systems has gained considerable interest. For example, the ground-state wavefunction of highly correlated fermions on lattices can be expressed as a product of local singlet sets, known as valence bonds. This category of states is referred to as valence-bond solid (VBS) states. It has since been established that VBS states can also be represented using the valence bond basis, which includes prominent states such as the Néel state, representing antiferromagnetic order, the Haldane phase associated with integer-spin rings, and the Affleck-Kennedy-Lieb-Tasaki (AKLT) model that describes gapped spin-1/2 chains. Recently, various strategies have been developed to experimentally produce these quantum states. However, previous designs have relied on nonlinear interactions among photons and complex setups, making practical implementation challenging. In contrast, recent experiments have successfully generated photonic qubits, suggesting a promising avenue for executing these quantum states without the need for nonlinear interactions.",
        "ori-fast-z-score": 0.4931969619160719,
        "water-fast-z-score": 7.6705107192336,
        "rewrite-fast-z-score": 1.807392228230128
    },
    {
        "original_text": "We present an analysis of pulsar observations to determine the magnetic field strength in the solar corona at heights between 1 and 3 R_Sun . We use data obtained with the Nançay Radio Telescope (NRT) for two different radio frequencies, 327 MHz and 1420 MHz, corresponding to emission heights of about 2 and 5 R_Sun , respectively. The observed pulse profiles are modeled using a simple model that includes contributions from both the local interstellar medium and the solar wind plasma. From these models we derive estimates for the coronal magnetic field strengths as well as the electron density distribution along the line-of-sight towards PSR B1133+16 .\nThe results show that the magnetic field decreases rapidly with height above the photosphere but is still strong enough to confine energetic particles up to several solar radii away from the Sun s surface. This suggests that particle acceleration processes may be taking place throughout most of the solar atmosphere.",
        "watermark_text": "We present an assessment of pulsar observations to estimate the magnetic force size in the solar corona at heights between 1 and 3 R _ Sun . We use data acquired with the Nançay Radio Telescope ( NRT ) for two different radio altitudes , 327 MHz and 1420 MHz , corresponding to emission heights of about 2 and 5 R _ Sun , respectively .The observed pulse profiles are modeled using a simple simulation that contains contributions from both the local interstellar material and the sun breeze plasma . From these models we derive estimates for the coronal magnetic field strengths as also as the electron concentration distribution along the line - of - view towards PSR B1133 + 16 .The results show that the magnetic force reduces rapidly with width above the photosphere but is nevertheless strong enough to confine energetic particles up to several solar radii away from the Sun s surface . This implies that particle acceleration processes possibly be taking place throughout most of the solar atmosphere .",
        "rewrite_text": "We provide an evaluation of pulsar observations to estimate the magnitude of the magnetic force in the solar corona at altitudes ranging from 1 to 3 solar radii. This analysis utilizes data collected from the Nançay Radio Telescope (NRT) at two different radio frequencies, 327 MHz and 1420 MHz, which correspond to emission heights of approximately 2 and 5 solar radii, respectively. The pulse profiles observed were modeled using a straightforward simulation that includes contributions from both local interstellar material and solar wind plasma. From these models, we derive estimates of the coronal magnetic field strengths and the electron concentration distribution along the line of sight toward PSR B1133 + 16. The findings indicate that the magnetic force decreases rapidly with altitude above the photosphere, yet remains sufficiently strong to confine energetic particles several solar radii away from the Sun's surface. This suggests that particle acceleration processes may be occurring throughout much of the solar atmosphere.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 4.780914437337574,
        "rewrite-fast-z-score": -0.3464101615137754
    },
    {
        "original_text": "The Infrared Camera (IRC), one of the three instruments onboard the Astro-F satellite, is designed to perform imaging observations in four infrared bands centered at 2.4, 3.2, 4.1, and 12 micrometers with high sensitivity over wide fields-of-view ranging from 1 arcmin square to 10 degrees square. The IRC consists of two cameras; an optical camera equipped with a lens system having focal length of f = 50 mm and a near-infrared camera using a lens system with f = 100 mm. Both cameras are cooled down by liquid helium cryostats to temperatures below 5 K. A set of filters covering the entire wavelength range of each band is attached to both cameras. The performance of the IRC has been evaluated through laboratory measurements as well as in-orbit calibration experiments performed during the commissioning phase of the mission. The results show that the IRC meets all its design requirements.",
        "watermark_text": "The Infrared Camera ( IRC ) , one of the three devices onboard the Astro - F satellite , is designed to conduct imaging observations in four infrared bands centered at 2 . 4 , 3 . 2 , 4 . 1 , and 12 micrometers with high sensitivity over broad areas - of - view ranging from 1 arcmin square to 10 degrees square . The IRC contains of two lenses ; an optical camera equipped with a lens system having focal width of f = 50 mm and a far - infrared camera utilizing a lens system with h = 100 mm .Both instruments are heated down by liquid helium cryostats to altitudes below 5 K . A family of filters covering the entire wavelength range of each band is attached to both cameras . The reliability of the IRC has been examined through laboratory measurements as well as in - orbit calibration experiments conducted during the commissioning process of the mission .The results show that the IRC meets all its design criteria .",
        "rewrite_text": "The Infrared Camera (IRC), one of the three instruments aboard the Astro-F satellite, is designed to perform imaging observations across four infrared bands centered at 2.4, 3.2, 4.1, and 12 micrometers. It offers high sensitivity over a wide field of view, ranging from 1 square arcminute to 10 square degrees. The IRC includes two camera systems: an optical camera with a lens system featuring a focal length of f = 50 mm and a far-infrared camera with a lens system having a focal length of h = 100 mm. Both instruments are cooled to below 5 K using liquid helium cryostats. Each camera is fitted with a set of filters that spans the entire wavelength range of its respective band. The reliability of the IRC has been validated through laboratory tests and in-orbit calibration experiments conducted during the mission's commissioning phase. The findings indicate that the IRC fulfills all of its design specifications.",
        "ori-fast-z-score": 2.4618298195866544,
        "water-fast-z-score": 6.893123494842633,
        "rewrite-fast-z-score": 3.8805700005813275
    },
    {
        "original_text": "We present new near-infrared (NIR) photometry and spectroscopy for the high-latitude star forming cloud, CG12. We find that this cloud is composed of two distinct populations; one population with ages <10 Myr and another older population with ages >100 Myr. Using N-body simulations we show that these clouds are likely to be remnants of tidally disrupted open clusters. This suggests that open cluster disruption may play an important role in the formation of isolated young stars at high latitudes. These results have implications on our understanding of how open clusters evolve into open clusters and open clusters into open clusters. Open clusters can also provide insight into the origin of open clusters themselves. In addition, open clusters can help us understand the evolution of open clusters as well as open clusters. Finally, open clusters can give us information about open clusters and open clusters. We use new infrared data obtained by the Wide Field Camera 3 (WFC3), Hubble Space Telescope (HST), and Spitzer Space Telescope (SST).",
        "watermark_text": "We report new near - infrared ( NIR ) photometry and spectroscopy for the high - latitude star producing storm , CG12 . We see that this cloud is composed of two separate populations ; one group with ages < 10 Myr and another older population with ages > 100 Myr .Using N - bodies simulations we find that these clouds are likely to be remnants of tidally disrupted open nuclei . This implies that open cluster disturbance may play an important role in the formation of isolated early stars at high latitudes .These data have consequences on our knowing of how open groups grow into open groups and open groups into open groups . Open clusters can also provide insight into the origin of open groups themselves .In addition , open groups can help us explain the evolution of open groups as well as open clusters . Finally , open groups can provide us information about open groups and open clusters .We use new infrared measurements obtained by the Wide Field Camera 3 ( WFC3 ) , Hubble Space Telescope ( HST ) , and Spitzer Space Telescope ( SST ) .",
        "rewrite_text": "We present new near-infrared (NIR) photometry and spectroscopy for the high-latitude star-forming region CG12. Our findings indicate that this cloud consists of two distinct populations: one with ages less than 10 million years and another older group exceeding 100 million years. Through N-body simulations, we suggest that these clouds are likely remnants of tidally disrupted open nuclei. This observation underscores the potential significance of open cluster disturbances in the formation of isolated early stars at high latitudes. Our data enhance our understanding of the evolution of open clusters and their transition into more developed open groups. Additionally, studying open clusters can shed light on the origins and evolution of open groups. We utilized new infrared measurements collected by the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST) and the Spitzer Space Telescope (SST).",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The electronic properties of solids are determined by their ground and excited state energies, which can be calculated using ab initio wave function-based approaches such as density functional theory (DFT). However, these calculations often suffer from an incomplete description of electron-electron interactions due to the use of approximate exchange-correlation potentials. In this work we present a method that allows us to correct DFT results for excited states with respect to exact many-body perturbation theory (MBPT) results. We apply our approach to calculate the optical absorption spectrum of MgO and compare it to experimental data. Our results show good agreement between experiment and theory over a wide range of photon energies. The presented methodology is applicable to any material where MBPT results exist or can be obtained within reasonable computational effort. This includes most semiconductors but also insulators like ionic compounds. \n \n Ab initio wave-function based methods have become standard tools for calculating the electronic properties of materials. These include ground-state total energy calculations  1 , phonon dispersion relations  2 , elastic constants  3 , magnetic moments  4 , and transport coefficients  5 . They are routinely used to predict structural phase transitions  6 , defect formation energies  7-9 , surface energies  10-12 , and chemical reactions  13-15 . Furthermore they provide insight into fundamental physical phenomena including superconductivity  16 , magnetism  17 , charge-density waves  18 , ferroelectricity  19 , and quantum critical points  20 . Finally, they allow one to study the effects of external perturbations on the electronic structure  21 , e.g., strain  22 , pressure  23 , electric fields  24 , temperature  25 , or doping  26 .",
        "watermark_text": "The electronic properties of solids are decided by their ground and excited state energies , which can be determined using ab initio wave function - based methods such as density functional theory ( DFT ) . However , these calculations often suffer from an incomplete description of electron - ion interactions due to the using of approximate transfer - correlation potentials .In this research we present a technique that enables us to correct DFT results for excited states with regard to exact large - bodies perturbation theory ( MBPT ) results . We use our approach to estimate the optical absorbed spectrum of MgO and compare it to experimental evidence .Our results show good agreement between experiment and theory over a broad variety of photon energies . The offered methodology is applicable to any material where MBPT results appear or can be obtained within reasonable numerical time .This encompasses most semiconductors but also insulators like ionic compounds . Ab initio wave - function independent methods have remain standard tools for determining the electronic properties of substances .These include ground - state total energy measurements 1 , phonon dispersion relations 2 , elastic constants 3 , electric moments 4 , and transport factors 5 . They are routinely used to predict structural phase transitions 6 , defect dissolution energies 7 - 9 , surface concentrations 10 - 12 , and biological enzymes 13 - 15 .Furthermore they give insight into fundamental physical phenomena including superconductivity 16 , magnetism 17 , charge - density waves 18 , ferroelectricity 19 , and quantum vital places 20 . Finally , they allow one to study the effects of external perturbations on the electronic system 21 , e . g . , stress 22 , pressure 23 , electric forces 24 , temperature 25 , or doping 26 .",
        "rewrite_text": "The electronic properties of solids are dictated by their ground and excited state energies, which can be calculated using ab initio wave function-based methods, such as density functional theory (DFT). However, these calculations often face challenges due to an incomplete representation of electron-ion interactions, stemming from the use of approximate exchange-correlation potentials. In this study, we introduce a technique that allows for the correction of DFT results concerning excited states, in alignment with exact many-body perturbation theory (MBPT) outcomes. We apply this approach to estimate the optical absorption spectrum of MgO and compare it with experimental data, finding strong agreement across a wide range of photon energies. This methodology is applicable to any material for which MBPT results are available or can be feasibly computed, covering most semiconductors as well as insulators like ionic compounds. Ab initio wave function-independent methods have become standard tools for determining the electronic properties of various materials. These methods allow for measurements of ground-state total energy, phonon dispersion relations, elastic constants, electric moments, and transport properties. They are routinely utilized to predict structural phase transitions, defect dissolution energies, surface concentrations, and even biological enzymes. Additionally, they provide insights into fundamental physical phenomena, including superconductivity, magnetism, charge-density waves, ferroelectricity, and quantum critical phenomena. Lastly, these methods facilitate the exploration of how external perturbations—such as stress, pressure, electric fields, temperature changes, or doping—affect the electronic system.",
        "ori-fast-z-score": 0.7689218919450849,
        "water-fast-z-score": 7.545937746270389,
        "rewrite-fast-z-score": 1.0366421106976322
    },
    {
        "original_text": "We present an analytic expression for the one-loop mass-less triangle Feynman integral in terms of generalized hypergeometric functions. The results are obtained by using Mellin-Barnes representation and contour integration techniques. We also provide numerical values for some special cases which can be used to check our analytical expressions. This work is motivated by recent interest on the study of higher order corrections to various physical processes, such as Higgs decay into two photons or gluons at next-to-leading-order (NLO) accuracy. \nI. INTRODUCTORY REMARK\nThe calculation of loop diagrams plays an important role in theoretical physics. In particular, it has been shown that the inclusion of radiative corrections leads to significant changes in the predictions of many observables  1  . For example, the NLO QCD correction to the decay widths of heavy quarks  2  , top quark pair production  3  , Higgs boson decays  4  etc., have been calculated recently with great success. However, there still remain several open problems related to the evaluation of multi-loop integrals  5  .\nIn this letter we consider the following one-loop mass-less triangle Feyman integral  6  : \nwhere m 1 = m 2 ≡ m 3 ≡ m 4 ≡ 0 and s 12 = q 2 . It should be noted here that I(q 2 ) vanishes when any three masses become equal i.e. m 1 = m 2 = m 3 = m 4 .",
        "watermark_text": "We introduce an analytic definition for the one - loop mass - less triangle Feynman integral in terms of generalized hypergeometric functions . The results are derived by using Mellin - Barnes representation and contour processing method .We additionally offer mathematical values for some particular instances which can be used to test our analytical expressions . This research is prompted by recent interest on the study of greater order corrections to several physical processes , such as Higgs decay into two photons or gluons at next - to - leading - order ( NLO ) accuracy .I . INTRODUCTORY REMARK The calculation of loop diagrams takes an important role in theoretical physics .In particular , it has been shown that the introduction of radiative corrections leads to significant improvements in the estimates of several observables 1 . For instance , the NLO QCD correction to the decay widths of hard quarks 2 , top quark pair production 3 , Higgs boson decays 4 etc . , have been measured recently with great success .However , there still continue several open problems related to the evaluation of multi - loop integrals 5 . In this letter we define the following one - loop mass - less triangle Feyman integral 6 : where m 1 = m 2 ≡ m 3 ≡ m 4 ≡ 0 and s 12 = q 2 .It should be mentioned here that I ( q 2 ) vanishes when any three masses become equal i . e . m 1 = m 2 = m 3 = m 4 .",
        "rewrite_text": "We present an analytical definition of the one-loop massless triangle Feynman integral using generalized hypergeometric functions. The results are obtained through the Mellin-Barnes representation and contour integration techniques. Additionally, we provide mathematical values for specific cases that can serve to validate our analytical results. This work is motivated by the increasing interest in higher-order corrections to various physical processes, including the next-to-leading-order (NLO) accuracy of Higgs decay into two photons or gluons. \n\nI. INTRODUCTORY REMARK\n\nLoop diagram calculations play a crucial role in theoretical physics. It has been demonstrated that incorporating radiative corrections significantly enhances the accuracy of several observables. For example, the NLO QCD corrections to the decay widths of heavy quarks, top quark pair production, and Higgs boson decays have recently been measured with considerable success. However, challenges remain in the evaluation of multi-loop integrals. In this letter, we define the one-loop massless triangle Feynman integral, where \\( m_1 = m_2 = m_3 = m_4 = 0 \\) and \\( s_{12} = q^2 \\). Notably, the integral \\( I(q^2) \\) vanishes when all four masses are equal, i.e., \\( m_1 = m_2 = m_3 = m_4 \\).",
        "ori-fast-z-score": -0.10482848367219183,
        "water-fast-z-score": 5.421374765483944,
        "rewrite-fast-z-score": 0.3464101615137754
    },
    {
        "original_text": "We present new exact half-BPS solutions to the low-energy effective action for type-IIB string theory in ten dimensions, which are supported by fluxes on an arbitrary number of intersecting D3-branes. These solutions can be viewed as generalizations of the single Janus solution presented in  1  . We also discuss some properties of these solutions that were not discussed previously. \n \n The first part of this work was published as: \n \n  2  C. A. Johnson et al., Phys. Rev. Lett. 106 (2011) 055005. DOI: 10.1103/PhysRevLett.106.055005. URL: http://arxiv.org/abs/1111.5389v2. URL: http://www.sciencedirect.com/science/article/pii/S0167971011000203. URL: https://inspirehep.net/record/126898/files/CAJohnson_et_al_PRL_2011_055005.pdf",
        "watermark_text": "We present new exact half - BPS treatments to the small - energy effective action for type - IIB string theory in ten dimensions , which are favored by fluxes on an arbitrary number of intersecting D3 - branes . These solutions can be viewed as generalizations of the single Janus solution offered in 1 .We addition explore some properties of these solutions that were not discussed earlier . The first part of this study was publication as : 2 C . A . Johnson et al . , Phys .Rev.Lett.106 (2011) 055005.DOI: 10.1103/PhysRevLett.106.055005.URL : http : / / arxiv . org / abs / 1111 . 5389v2 . URL : http : / / www . sciencedirect . com / science / article / pii / S0167971011000203 .URL : https : / / inspirehep . net / record / 126898 / files / CAJohnson _ et _ al _ PRL _ 2011 _ 055005 . pdf",
        "rewrite_text": "We introduce new exact half-BPS solutions for the low-energy effective action of type IIB string theory in ten dimensions, which are enhanced by fluxes on an arbitrary number of intersecting D3-branes. These solutions represent generalizations of the single Janus solution previously discussed. Additionally, we examine certain properties of these solutions that have not been addressed in prior research. The initial part of this study was published as: C. A. Johnson et al., Phys. Rev. Lett. 106 (2011) 055005. DOI: 10.1103/PhysRevLett.106.055005. For further details, you can access the following resources: http://arxiv.org/abs/1111.5389v2, http://www.sciencedirect.com/science/article/pii/S0167971011000203, and https://inspirehep.net/record/126898/files/CAJohnson_et_al_PRL_2011_055005.pdf.",
        "ori-fast-z-score": 0.7453559924999299,
        "water-fast-z-score": 2.897472836319489,
        "rewrite-fast-z-score": 0.7001400420140048
    },
    {
        "original_text": "The effect of magnetic Reynolds number (Rm), Lundquist number (S) and plasma beta (β) on the efficiency of MRI-driven turbulence is investigated by means of direct numerical simulations in cylindrical geometry with periodic boundary conditions. The results show that Rm, S and β have significant effects on the saturation level of the Maxwell stress tensor as well as the growth rate of the kinetic energy density. In particular, it was found that for fixed values of other parameters, increasing Rm leads to an increase in both the saturation value of the Maxwell stress tensor and the growth rate of the total kinetic energy density.  Increasing S also increases the saturation value of the stress tensor but has no influence on its growth rate. On the contrary, decreasing β decreases the saturation value of the tensor while having little or no effect on its growth rate. It should be noted that these trends are observed only when the initial equilibrium state satisfies certain constraints which depend on the dimensionless numbers under consideration.",
        "watermark_text": "The impact of magnetic Reynolds number ( Rm ) , Lundquist number ( S ) and plasma beta ( β ) on the performance of MRI - driven turbulence is investigated by means of direct numerical simulations in cylindrical geometry with periodic boundary constraints . The results show that Rm , S and β have considerable effects on the saturation level of the Maxwell stress tensor as also as the development frequency of the kinetic power concentration .In particular , it was shown that for constant values of other parameters , increasing Rm leads to an increase in both the saturation value of the Maxwell stress tensor and the development rate of the total kinetic power concentration . Increasing S also increases the saturation value of the strain vector but has no impact on its rise rate .On the contrary , decreasing β decreases the saturation value of the tensor while having little or no effect on its increase time . It should be mentioned that these changes are observed only when the early equilibrium state satisfies certain constraints which depend on the dimensionless numbers under consideration .",
        "rewrite_text": "The influence of the magnetic Reynolds number (Rm), the Lundquist number (S), and plasma beta (β) on the behavior of MRI-driven turbulence is analyzed through direct numerical simulations conducted in a cylindrical geometry with periodic boundary conditions. The findings indicate that Rm, S, and β significantly affect both the saturation level of the Maxwell stress tensor and the frequency of kinetic power concentration development. Specifically, it is demonstrated that, with other parameters held constant, an increase in Rm results in a higher saturation value of the Maxwell stress tensor and an accelerated development of total kinetic power concentration. Similarly, an increase in S raises the saturation value of the strain vector, although it does not influence its rate of increase. Conversely, a decrease in β leads to a reduction in the saturation value of the tensor while minimally impacting its rate of increase. It is important to note that these effects are observed only when the initial equilibrium state meets specific constraints dependent on the relevant dimensionless numbers.",
        "ori-fast-z-score": 2.2941573387056176,
        "water-fast-z-score": 6.882472016116853,
        "rewrite-fast-z-score": 3.4874291623145783
    },
    {
        "original_text": "We present the first results for a new template family, called SEOBNRv4HM, which is designed to detect gravitational waves (GWs) emitted by comparable mass black hole binaries with total masses between 10 and 100 solar masses. We show that this template family can be used in searches for GW signals from binary black holes at current ground-based detectors such as Advanced LIGO/Virgo and KAGRA. In addition we demonstrate how these templates are useful for parameter estimation studies using simulated data sets. Finally, we discuss possible improvements on our work. Keywords: Binary black hole -Gravitational wave detector -Template family -Parameter estimation -SEOBNRv4HM -Advanced Virgo -LIGO -KAGRA -Black hole -Binary system -Gravitational wave signal -Gravitational wave astronomy -Gravitational wave source -Signal-to-noise ratio -Search algorithm -Simulation",
        "watermark_text": "We present the first findings for a new template family , known SEOBNRv4HM , which is designed to identify gravitational waves ( GWs ) emitted by similar mass black hole binaries with total masses between 10 and 100 solar masses . We see that this template family can be used in searches for GW signals from binary white holes at current ground - based detectors such as Advanced LIGO / Virgo and KAGRA .In addition we explain how these templates are helpful for parameter estimation studies employing simulated evidence sets . Finally , we explain possible advances on our work .Keywords : Binary white hole - Gravitational wave detector - Template family - Parameter estimation - SEOBNRv4HM - Advanced Virgo - LIGO - KAGRA - Black hole - Binary network - Gravitational wave noise - Gravitational wave astronomy - Gravitational wave source - Signal - to - noise ratio - Search algorithm - Simulation",
        "rewrite_text": "We introduce the initial findings of a new template family called SEOBNRv4HM, which is specifically designed to detect gravitational waves (GWs) produced by binary black hole systems with total masses ranging from 10 to 100 solar masses. Our results indicate that this template family is applicable for searching for GW signals from binary white holes using current ground-based detectors such as Advanced LIGO, Virgo, and KAGRA. Furthermore, we discuss how these templates can enhance parameter estimation studies that utilize simulated evidence sets. Lastly, we outline potential advancements related to our research. \n\nKeywords: Binary white hole, Gravitational wave detector, Template family, Parameter estimation, SEOBNRv4HM, Advanced Virgo, LIGO, KAGRA, Black hole, Binary network, Gravitational wave noise, Gravitational wave astronomy, Gravitational wave source, Signal-to-noise ratio, Search algorithm, Simulation.",
        "ori-fast-z-score": -1.1523319193960637,
        "water-fast-z-score": 3.2547227745205967,
        "rewrite-fast-z-score": 1.25
    },
    {
        "original_text": "The generalized twistor transform is an extension of the ordinary twistor transform to include particles with spin in arbitrary dimensions. It has been used recently for describing scattering amplitudes involving massive particles as well as massless ones. In this talk I will present some new results on the subject including a description of how one can use it to describe particles beyond free or massless theories. The talk will also cover recent developments related to dualities between different quantum field theories. This includes duality relations that have been found using the generalized twistor transform such as those relating N=4 super Yang-Mills theory to self-dual gravity and its supersymmetric extensions. Finally we will discuss how these ideas are connected to string theory. Generalized twistors were introduced by Witten more than twenty years ago  1  . They provide a powerful tool for studying gauge theories in general relativity  2  , and they play important roles in understanding various aspects of string/M-theory  3  .\nIn particular, the so-called Penrose limit  4  provides a way to relate scattering amplitudes in gauge/gravity theories at weak coupling to correlation functions in conformal field theories (CFTs) at strong coupling  5  . Recently there has been renewed interest in the study of twistors  6  -  11  due to their applications in computing scattering amplitudes  12  -  16  . For example, the tree-level S-matrix elements of gluons  17  and gravitons  18  in four-dimensional N = 4 Super-Yang-Mills theory (SYM), which was conjectured to be dual to type-IIB superstrings  19  , were computed via the generalized twistor transform  20  .",
        "watermark_text": "The generalized twistor transform is an extension of the ordinary twistor transform to consider objects with spin in arbitrary dimensions . It has been used lately for describing scattering amplitudes involving massive bodies as well as massless ones .In this talk I will present some different results on the subject including a description of how one can using it to explain objects beyond free or massless theories . The speech will also cover recent developments pertaining to dualities between various quantum field theories .This contains duality relations that have been seen using the generalized twistor transform such as those relating N = 4 super Yang - Mills theory to self - dual gravity and its supersymmetric extensions . Finally we will explore how these ideas are connected to string theory .Generalized twistors were introduced by Witten more than twenty years previously 1 . They offer a powerful tool for studying gauge fields in general relativity 2 , and they hold important roles in understanding various parts of string / M - theory 3 .In particular , the so - called Penrose limit 4 provides a way to relate scattering amplitudes in gauge / gravity fields at weak interaction to correlation functions in conformal field theories ( CFTs ) at weak correlation 5 . Recently there has been continued interest in the study of twistors 6 - 11 due to their applications in computing absorption amplitudes 12 - 16 .For instance , the tree - grade S - matrix elements of gluons 17 and gravitons 18 in four - dimensional N = 4 Super - Yang - Mills theory ( SYM ) , which was conjectured to be dual to type - IIB superstrings 19 , were computed via the generalized twistor transform 20 .",
        "rewrite_text": "The generalized twistor transform extends the traditional twistor transform to encompass objects with spin across arbitrary dimensions. Recently, it has been applied to describe scattering amplitudes involving both massive and massless entities. In my presentation, I will share several new findings on this topic, including how this approach can be utilized to analyze phenomena beyond free or massless theories. The discussion will also delve into recent advancements related to dualities among various quantum field theories, highlighting relationships identified through the generalized twistor transform, such as those connecting N = 4 super Yang-Mills theory with self-dual gravity and its supersymmetric variations. Additionally, we will investigate the connections between these concepts and string theory. Generalized twistors, introduced by Witten more than twenty years ago, serve as a useful framework for examining gauge fields within general relativity and play significant roles in understanding various facets of string and M-theory. Notably, the Penrose limit offers a means of linking scattering amplitudes in gauge and gravity fields at weak coupling to correlation functions in conformal field theories (CFTs) with weak interactions. Recently, there has been a renewed interest in twistor studies due to their applications in calculating absorption amplitudes. For example, the tree-level S-matrix elements for gluons and gravitons in four-dimensional N = 4 Super-Yang-Mills theory, which is conjectured to correspond to type IIB superstrings, were computed using the generalized twistor transform.",
        "ori-fast-z-score": 0.4622501635210242,
        "water-fast-z-score": 5.454551929548086,
        "rewrite-fast-z-score": -1.0536089137432665
    },
    {
        "original_text": "In this article, we study cuspidal representations of general linear groups over discrete valuation rings. We prove that if the residue field is infinite and has characteristic zero then any irreducible representation which is unramified outside a finite set of primes is supercuspidal (Theorem 3). This result was previously known only for number fields. As an application, we show how to construct new examples of supercuspidal representations by taking tensor products with characters of tori (Proposition 4).\nWe also give some results on the structure of the space of smooth functions on G(R) where G is a reductive group defined over Q p . In particular, we show that if G = GL n , SL n or Sp 2n then there exists a dense subspace of smooth functions invariant under right translation by elements in G(Q p ) (Corollary 5). Finally, we use these results to compute the dimension of certain spaces of automorphic forms attached to cuspidal representations of GL 2 (A) (Propositions 6 and 7).",
        "watermark_text": "In this article , we study cuspidal forms of general linear categories over discrete valuation rings . We prove that if the residue field is infinite and has characteristic zero then any irreducible representation which is unramified outside a finite collection of primes is supercuspidal ( Theorem 3 ) .This result was formerly described only for number fields . As an application , we prove how to build new examples of supercuspidal representations by take tensor products with characters of tori ( Proposition 4 ) .We additionally give some results on the composition of the space of smooth functions on G ( R ) where G is a reductive group defined over Q p . In particular , we prove that if G = GL n , SL n or Sp 2n then there exists a dense subspace of smooth functions invariant under right translation by elements in G ( Q n ) ( Corollary 5 ) .Finally , we utilize these results to compute the dimension of certain spaces of automorphic forms assigned to cuspidal forms of GL 2 ( A ) ( Propositions 6 and 7 ) .",
        "rewrite_text": "In this article, we investigate cuspidal forms within the context of general linear categories over discrete valuation rings. We establish that if the residue field is infinite and has characteristic zero, then any irreducible representation that is unramified outside a finite set of primes is indeed supercuspidal (Theorem 3). This finding expands on previous results that were exclusively applicable to number fields. As a practical application, we demonstrate a method for constructing new examples of supercuspidal representations by taking tensor products with characters of tori (Proposition 4). Additionally, we present several results concerning the structure of the space of smooth functions on G(R), where G is a reductive group defined over Qp. Notably, we show that when G equals GLn, SLn, or Sp2n, there exists a dense subspace of smooth functions that remains invariant under right translation by elements in G(Qn) (Corollary 5). Lastly, we leverage these findings to compute the dimensions of specific spaces of automorphic forms associated with cuspidal forms of GL2(A) (Propositions 6 and 7).",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 3.556003556005334,
        "rewrite-fast-z-score": 1.9402850002906638
    },
    {
        "original_text": "We present the results of our investigation into how stellar evolution models are affected by individual element abundances in stars, focusing on the sensitivity to changes in helium abundance (Y). We use two different sets of evolutionary tracks with varying Y values for masses between 0.8 M⊙ and 8 M⊙ at solar metallicity. The first set is based on the Padova code while the second one uses the Geneva code. For each track we calculate synthetic spectra using the SPECTRUM code. These synthetic spectra are then used as input to determine the best-fit parameters of observed high-resolution optical spectra of Galactic open clusters. Our analysis shows that both codes produce similar results when fitting these cluster data. However, there are significant differences in the derived ages depending on which code was used. This discrepancy can be explained by the fact that the Padova tracks have been calculated without convective overshooting whereas the Geneva tracks include this effect.",
        "watermark_text": "We present the conclusion of our inquiry into how stellar evolution models are influenced by individual atom abundances in stars , concentrating on the sensitivity to changes in helium abundance ( Y ) . We use two different series of evolutionary tracks with varying Y readings for masses between 0 . 8 [UNK] and 8 [UNK] at solar metallicity .The first setting is based on the Padova code while the second one uses the Geneva code . For each track we estimate synthetic spectra using the SPECTRUM code .These synthetic spectra are then utilized as input to obtain the best - fitting characteristics of known high - resolution optical spectra of Galactic open clusters . Our study shows that both codes produce comparable results when fitting these cluster data .However , there are significant variations in the derived ages varying on which coding was used . This discrepancy can be reason by the fact that the Padova lines have been measured without convective overshooting whereas the Geneva tracks include this effect .",
        "rewrite_text": "We present the findings of our investigation into how individual atom abundances in stars affect stellar evolution models, with a focus on the sensitivity to variations in helium abundance (Y). We utilize two distinct sets of evolutionary tracks with different Y values for star masses ranging from 0.8 to 8 solar masses, at solar metallicity. The first set is derived from the Padova code, while the second is generated using the Geneva code. For each evolutionary track, we compute synthetic spectra using the SPECTRUM code. These synthetic spectra serve as input for determining the best-fitting characteristics of the known high-resolution optical spectra of Galactic open clusters. Our analysis reveals that both codes yield comparable results when fitting these cluster data. However, there are notable differences in the derived ages depending on which code was employed. This discrepancy may arise from the fact that the Padova tracks were calculated without accounting for convective overshooting, whereas the Geneva tracks do include this effect.",
        "ori-fast-z-score": 1.7556172079419585,
        "water-fast-z-score": 6.50986776965388,
        "rewrite-fast-z-score": 1.3587324409735149
    },
    {
        "original_text": "We present new results on the detection rate, mass distribution, and orbital properties of hypervelocity stars (HVSs) in the Galactic halo based on spectroscopic observations with Keck II/DEIMOS over three years. We find that HVSs are detected at a rate of 0.5 +/- 0.2 per year within 100 pc of the Galactic center. The observed number density profile is consistent with an exponential fall-off with distance from the Galactic center. Our sample contains two HVSs with velocities greater than 1000 km/s; one has a heliocentric radial velocity of 1240 km/s and another has 1420 km/s. These high velocities suggest that these objects were ejected by gravitational slingshots during close encounters between massive black holes or neutron stars. In addition to the known population of HVSs near the Sun, we also detect a large number of high-velocity stars located farther away from the Galactic center which may be associated with other nearby supermassive black holes.",
        "watermark_text": "We report new data on the detection rate , mass distribution , and orbital properties of hypervelocity stars ( HVSs ) in the Galactic halo based on spectroscopic observations with Keck II / DEIMOS over three years . We see that HVSs are detected at a rate of 0 . 5 + / - 0 . 2 per month within 100 pc of the Galactic center .The observed number density profile is compatible with an exponential fall - off with distance from the Galactic center . Our specimen contains two HVSs with velocities greater than 1000 kilometers / s ; one has a heliocentric radial speed of 1240 km / s and another has 1420 km / s .These high velocities indicate that these objects were ejected by gravitational slingshots during close contacts between massive brown holes or neutron galaxies . In addition to the known community of HVSs near the Sun , we also observe a large number of high - speed stars situated closer back from the Galactic center which may be involved with other nearby supermassive black holes .",
        "rewrite_text": "We present new findings regarding the detection rate, mass distribution, and orbital characteristics of hypervelocity stars (HVSs) in the Galactic halo, based on three years of spectroscopic observations using Keck II/DEIMOS. Our results indicate that HVSs are detected at a rate of 0.5 ± 0.2 per month within 100 parsecs of the Galactic center. The observed number density profile aligns with an exponential decline as the distance from the Galactic center increases. Among our sample, we identified two HVSs with velocities exceeding 1000 kilometers per second, one featuring a heliocentric radial velocity of 1240 km/s, and the other 1420 km/s. These remarkably high speeds suggest that these stars were ejected as a result of gravitational slingshot events occurring during close encounters between massive black holes or neutron stars. In addition to the known population of HVSs near the Sun, we also detect a significant number of high-speed stars located further back from the Galactic center, which may be associated with other nearby supermassive black holes.",
        "ori-fast-z-score": 0.8682431421244593,
        "water-fast-z-score": 5.581563056514381,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "The nucleon is described as an extended object with internal structure, which can be probed by elastic scattering experiments at high energies and small momentum transfers. The present work focuses on the investigation of chiral mechanisms leading to orbital quantum structures within the framework of effective field theory (EFT). In particular we study the role played by pionic degrees of freedom for the description of the nucleon s electromagnetic form factors. We show that the inclusion of explicit pions leads to significant improvements over previous calculations based solely on quark degrees of freedom. Furthermore, we demonstrate how the EFT approach allows one to systematically include higher-order corrections into the calculation of observables. Finally, we discuss possible extensions of our formalism towards the treatment of other hadronic systems such as nuclei or hypernuclei. The nucleon is described as a composite system consisting of quarks bound together via gluons. However, it has been known since the early days of QCD  1  , that this picture cannot fully explain all experimental observations  2  . For example, while the proton s electric charge radius agrees well with experiment  3  , its magnetic moment turns out to be about 30% larger than expected  4  .\nIn order to resolve these discrepancies between theoretical predictions and experimental data, it was suggested  5  that additional contributions arising from the presence of virtual mesonic fluctuations should be taken into account  6  . These so-called  meson-cloud  effects are particularly important when considering processes involving large momentum transfer  7, 8  . It has also been shown  9  that they play an essential role in describing the nucleon s electromagnetic properties  10  .",
        "watermark_text": "The nucleon is characterized as an extended object with internal structure , which can be probed by elastic scattering experiments at high energies and tiny velocity transfers . The present work emphasizes on the examination of chiral mechanisms leading to orbital quantum forms within the framework of effective field theory ( EFT ) .In particular we study the importance played by pionic degrees of liberty for the description of the nucleon s electromagnetic form factors . We see that the introduction of explicit pions contributes to significant improvements over past calculations based primarily on quark degrees of liberty .Furthermore , we explain how the EFT methodology allows one to deliberately involve higher - order corrections into the determination of observables . Finally , we investigate possible extend of our formalism towards the treatment of other hadronic models such as atoms or hypernuclei .The nucleon is depicted as a composite system consisting of quarks bound together via gluons . However , it has been known since the early days of QCD 1 , that this picture cannot fully describe all observation observations 2 .For instance , while the proton s electric charge radius agrees well with test 3 , its magnetic point seems out to be about 30 % greater than expected 4 . In order to overcome these discrepancies between theoretical estimates and theoretical data , it was suggested 5 that extra contributions arising from the presence of virtual mesonic fluctuations should be taken into consideration 6 .These so - called meson - cloud effects are particularly important when considering phenomena involving huge velocity transfer 7 , 8 . It has additionally been shown 9 that they serve an essential part in understanding the nucleon s electromagnetic properties 10 .",
        "rewrite_text": "The nucleon is understood to be an extended entity with an internal structure, accessible through elastic scattering experiments at high energies and minimal velocity transfers. This study focuses on analyzing the chiral mechanisms that give rise to orbital quantum states within the framework of effective field theory (EFT). Specifically, we investigate the role of pionic degrees of freedom in describing the nucleon's electromagnetic form factors. Our findings indicate that incorporating explicit pions leads to notable improvements over previous calculations that primarily relied on quark degrees of freedom. Additionally, we explain how the EFT approach enables the intentional inclusion of higher-order corrections in the determination of observables. Lastly, we explore the potential for extending our formalism to treat other hadronic models, such as atoms or hypernuclei. The nucleon is portrayed as a composite system of quarks held together by gluons. However, it has been recognized since the early days of quantum chromodynamics (QCD) that this simplistic view cannot fully account for all experimental observations. For instance, although the proton's electric charge radius aligns well with experimental measurements, its magnetic moment appears to be approximately 30% larger than anticipated. To address these inconsistencies between theoretical predictions and experimental data, it has been suggested that additional contributions from virtual mesonic fluctuations need to be taken into account. These meson-cloud effects are particularly significant when analyzing phenomena involving large velocity transfers. Evidence has demonstrated that they play a crucial role in comprehending the nucleon's electromagnetic properties.",
        "ori-fast-z-score": 0.811502671200689,
        "water-fast-z-score": 8.746195456274092,
        "rewrite-fast-z-score": 1.4605934866804429
    },
    {
        "original_text": "We study the charge ordering phenomena in one-dimensional solids by using the exact diagonalization method and density matrix renormalization group (DMRG) technique. We find that there are two types of charge orderings, i.e., stripe-like and checkerboard-like orders depending on the electron filling factor n. The former is realized for 0 < n < 1 while the latter appears at half-filling with spin degeneracy lifted. In addition to these ordered states we also observe an exotic state where electrons form pairs without any net charge. This paired state can be regarded as a precursor of superconductivity. Finally, we discuss possible experimental realizations of our results. Introduction:-In recent years much attention has been paid to the physics of low dimensional systems such as carbon nanotubes  1  , semiconductor nanowires  2  , quantum wires  3  etc.. These materials have attracted considerable interest because they provide us with unique opportunities to explore novel physical properties which cannot exist in conventional three-dimensional bulk materials  4  . For example, it was predicted theoretically  5  and observed experimentally  6  that carbon nanotubes show metallic behavior even though their diameter is comparable or smaller than the Fermi wavelength. Another interesting feature of low dimensional systems is that various kinds of electronic phases may appear due to strong correlation effects  7, 8  .\nOne of the most important issues in this field is how to control the electronic phase diagram of low dimensional systems. It should be noted here that the electronic structure strongly depends not only on the geometry but also on the chemical composition  9  . Therefore, if we could change the chemical composition of low dimensional systems, then we would expect new electronic phases to emerge. Recently, several groups succeeded in synthesizing low dimensional compounds whose chemical compositions were controlled precisely  10 -12  . As a result, many fascinating phenomena have been discovered  13 -19  .",
        "watermark_text": "We research the charge ordering phenomena in one - dimensional solids by using the exact diagonalization technique and density matrix renormalization group ( DMRG ) method . We see that there are two forms of charge orderings , i . e . , stripe - like and checkerboard - like orders depending on the electron filling factor n . The first is realized for 0 < n < 1 while the former presents at half - filling with spin degeneracy lifted .In addition to these organized states we also observe an exotic state where electrons form couples without any gross charge . This paired state can be regarded as a precursor of superconductivity .Finally , we explain possible experimental realizations of our findings . Introduction : - In recent years much attention has been paid to the physics of low dimensional devices such as carbon nanotubes 1 , semiconductor nanowires 2 , quantum wires 3 etc . .These substances have garnered considerable interest because they give us with special opportunities to examine novel physical properties which cannot appear in standard three - dimensional bulk materials 4 . For instance , it was anticipated theoretically 5 and detected experimentally 6 that carbon nanotubes exhibit metallic behavior even though their distance is identical or smaller than the Fermi wavelength .Another important feature of lowest dimensional systems is that various kinds of electronic phases often emerge due to powerful correlation effects 7 , 8 . One of the most important problems in this area is how to manage the electronic phase diagram of low dimensional systems .It should be mentioned here that the chemical structure strongly depends not only on the topology but also on the chemical composition 9 . Therefore , if we could shift the chemical composition of low dimensional systems , then we may expect fresh electronic phases to emerge .Recently , various groups succeeded in synthesizing low dimensional molecules whose chemical compositions were monitored precisely 10 - 12 . As a result , various fascinating experiments have been observed 13 - 19 .",
        "rewrite_text": "We investigate the phenomenon of charge ordering in one-dimensional solids using the exact diagonalization technique and the density matrix renormalization group (DMRG) method. Our findings reveal two distinct types of charge order: stripe-like and checkerboard-like, which depend on the electron filling factor \\( n \\). Stripe-like order is observed for \\( 0 < n < 1 \\), while checkerboard order emerges at half-filling, accompanied by lifted spin degeneracy. Additionally, we identify an exotic state in which electrons form pairs without any net charge, which may serve as a precursor to superconductivity. Finally, we discuss potential experimental realizations of our results.\n\n**Introduction:** In recent years, there has been significant interest in the physics of low-dimensional devices such as carbon nanotubes, semiconductor nanowires, and quantum wires. These materials present unique opportunities to explore novel physical properties that do not occur in conventional three-dimensional bulk materials. For instance, it has been theoretically predicted and experimentally confirmed that carbon nanotubes can exhibit metallic behavior, despite having dimensions comparable to or smaller than the Fermi wavelength. Another notable aspect of low-dimensional systems is the emergence of various electronic phases due to strong correlation effects. One of the key challenges in this field is understanding the electronic phase diagram of these low-dimensional systems. It is important to note that the chemical structure is influenced not only by topology but also by chemical composition. Therefore, by varying the chemical composition of low-dimensional materials, we can expect the emergence of new electronic phases. Recently, various research groups have successfully synthesized low-dimensional molecules with precisely controlled chemical compositions, leading to a range of intriguing experimental observations.",
        "ori-fast-z-score": -0.8778955729143844,
        "water-fast-z-score": 7.741442779335935,
        "rewrite-fast-z-score": 1.986254132645683
    },
    {
        "original_text": "We present an optical and infrared study of the central region of the open cluster Sigma Orionis (Orion Nebula Cluster). We have obtained deep JHK photometry with ISAAC at ESO/VLT, as well as X-ray data using XMM-Newton for a sample of stars within a radius of 1 arcmin around the Trapezium OB association. The main results are summarized below:  - A total number of 16 new spectroscopic binaries were found among our targets.  - From the analysis of the radial velocities we find that most of these systems show orbital periods longer than 100 days.  - We also report on the discovery of two new pre-main sequence eclipsing binary candidates.  - In addition to this, we confirm the existence of several known spectroscopic binaries in the field studied here. - Finally, we discuss some interesting cases where the presence of circumstellar disks is inferred by their IR excess emission or by periodic variability.",
        "watermark_text": "We present an optical and infrared analysis of the inner region of the open chain Sigma Orionis ( Orion Nebula Cluster ) . We have achieved hot JHK photometry with ISAAC at ESO / VLT , as well as X - ray data utilizing XMM - Newton for a sample of stars within a diameter of 1 arcmin around the Trapezium OB association .The main results are presented below : - A total quantity of 16 new spectroscopic binaries were found among our objectives . - From the evaluation of the transverse velocities we find that most of these systems show orbital periods longer than 100 days .- We additionally report on the discovery of two new early - principal sequence eclipsing binary candidates . - In addition to this , we confirm the existence of several known spectroscopic binaries in the field studied here .- Finally , we talk some interesting cases where the presence of circumstellar disks is inferred by their IR excess emission or by periodic variability .",
        "rewrite_text": "We provide an analysis of the optical and infrared characteristics of the inner region of the Sigma Orionis open chain, part of the Orion Nebula Cluster. Our work includes hot JHK photometry obtained with the ISAAC instrument at the ESO/VLT, along with X-ray data gathered from XMM-Newton, focusing on a sample of stars within a 1 arcminute radius of the Trapezium OB association. The key findings are summarized as follows: we identified 16 new spectroscopic binaries among our targets; our assessment of transverse velocities indicates that most of these systems have orbital periods exceeding 100 days; we also report two newly discovered eclipsing binary candidates on the early main sequence; further, we confirm the presence of several previously known spectroscopic binaries within the studied area; lastly, we discuss several intriguing cases where circumstellar disks are suggested, based on IR excess emissions or periodic variability.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": -0.25
    },
    {
        "original_text": "We present new observations of the outflow driven by the young star cluster NGC 1333 IRAS 4A, located at the center of the Perseus molecular cloud (d = 235 pc). The data were obtained with the Submillimeter Array and include continuum emission at 1.3 mm as well as CO(2-1) line emission. We find that the outflow is highly collimated along an axis oriented NNE-SSW, which coincides with the direction to the nearby Herbig-Haro objects HH 7-11. The total mass of the outflowing gas is estimated to be ~0.1 Msun, while its kinetic energy amounts to ~10^50 ergs. These values are comparable to those found for other low-mass protostellar systems. However, we also detect significant differences between this system and others previously studied. In particular, our results suggest that the outflow may have been triggered recently due to the interaction of the central source with another object or structure within the dense core surrounding it.",
        "watermark_text": "We report new images of the outflow generated by the young star cluster NGC 1333 IRAS 4A , located at the center of the Perseus molecular storm ( d = 235 pc ) . The data were obtained with the Submillimeter Array and include continuum emission at 1 . 3 cm as well as CO ( 2 - 1 ) line emission .We see that the outflow is heavily collimated along an axis oriented NNE - SSW , which coincides with the direction to the nearby Herbig - Haro objects HH 7 - 11 . The total mass of the outflowing gas is expected to be ~ 0 . 1 Msun , while its kinetic power amounts to ~ 10 ^ 50 ergs .These measurements are comparable to those shown for other low - weight protostellar systems . However , we also observe significant variations between this scheme and others earlier studied .In particular , our findings show that the outflow could have been caused recently result to the interaction of the main source with another object or system within the dense core covering it .",
        "rewrite_text": "We present new images of the outflow produced by the young star cluster NGC 1333 IRAS 4A, situated at the center of the Perseus molecular cloud (distance = 235 pc). The data, acquired using the Submillimeter Array, encompasses continuum emission at 1.3 cm as well as CO (2-1) line emission. Observations reveal that the outflow is highly collimated along an axis oriented NNE-SSW, aligning with the direction of the nearby Herbig-Haro objects HH 7-11. The estimated total mass of the outflowing gas is approximately 0.1 Msun, with a kinetic power registering around 10^50 ergs. These measurements are comparable to those observed in other low-mass protostellar systems; however, we note considerable differences when contrasted with previously studied systems. Notably, our results suggest that the outflow may have recently been triggered by the interaction between the main source and another object or system within the surrounding dense core.",
        "ori-fast-z-score": -1.1338934190276817,
        "water-fast-z-score": 5.165514464459439,
        "rewrite-fast-z-score": 1.3858697343671664
    },
    {
        "original_text": "We present the results of an investigation into the evolution of dark matter halos and their substructure, using high-resolution cosmological N-body simulations with different mass resolutions. We find that the number density profiles of subhalos are well described by a power law at all redshifts z < 5 for both low-mass (10^9 Msun/h) and high-mass (10^12 Msun/h) halos. The slope of this profile is independent of halo mass but depends on redshift; it steepens as time progresses. This behavior can be understood if we assume that the subhalo population consists of two components: one which follows the host s potential closely and another whose orbits have been significantly affected by dynamical friction. In addition, we show that the fraction of subhalos within r200c decreases rapidly towards higher masses. Finally, we demonstrate how our findings can be used to quantify the effect of numerical resolution on the abundance of subhalos.",
        "watermark_text": "We present the conclusion of an research into the evolution of deep material halos and their substructure , using high - resolution cosmological N - bodies simulations with various mass resolutions . We see that the number density patterns of subhalos are better modeled by a power law at all redshifts z < 5 for both high - weight ( 10 ^ 9 Msun / h ) and low - density ( 10 ^ 12 Msun / h ) halos .The slope of this profile is independent of halo weight but relies on redshift ; it steepens as time progresses . This phenomenon can be understood if we suppose that the subhalo population contains of two parts : one which follows the host s potential closely and another whose orbits have been dramatically impacted by dynamical friction .In addition , we prove that the fraction of subhalos within r200c tends rapidly towards higher masses . Finally , we prove how our findings can be used to quantify the impact of statistical resolution on the availability of subhalos .",
        "rewrite_text": "We present the findings of our research on the evolution of deep material halos and their substructure, utilizing high-resolution cosmological N-body simulations with varying mass resolutions. Our results indicate that the number density distributions of subhalos are more accurately described by a power law across all redshifts z < 5, applicable to both high-mass (10^9 M_sun/h) and low-mass (10^12 M_sun/h) halos. Notably, the slope of this distribution is consistent regardless of halo mass but is dependent on redshift; it becomes steeper over time. This behavior can be interpreted by considering that the subhalo population consists of two components: one that closely aligns with the host's gravitational potential and another whose orbits are significantly influenced by dynamical friction. Furthermore, we demonstrate that the proportion of subhalos within r200c tends to shift toward higher masses rapidly. Lastly, we illustrate how our results can be leveraged to assess the effects of statistical resolution on the detection of subhalos.",
        "ori-fast-z-score": -2.0,
        "water-fast-z-score": 4.5,
        "rewrite-fast-z-score": -0.24253562503633297
    },
    {
        "original_text": "We present new observations made with the Cosmosoma experiment, which were designed to search for evidence of an excess in cosmic microwave background (CMB) temperature fluctuations above those predicted by standard cosmological models. The data are consistent with predictions based on current theoretical understanding but show some unexpected features that may be related to previously unidentified foreground sources or systematic effects associated with our analysis techniques. \n \n We have used these results to place limits on possible contributions from primordial gravitational waves and other exotic phenomena such as topological defects. These limits are comparable to previous measurements obtained using different experimental approaches. In addition we report the detection of a significant signal at frequencies below 10GHz, which is not expected within conventional cosmological models. This could represent either a new source of foreground contamination or a novel physical effect. Further investigation will require additional experiments to confirm this result and determine its origin. If confirmed it would provide important constraints on theories attempting to explain the observed anisotropy in the CMB spectrum.",
        "watermark_text": "We present new experiments done with the Cosmosoma study , which were built to search for indication of an amount in cosmic microwave background ( CMB ) temperature fluctuations above those predicted by traditional cosmological predictions . The data are compatible with predictions based on current theoretical knowledge but display some surprising characteristics that might be connected to formerly unidentified foreground sources or systematic effects involved with our analysis methods .We have utilized these results to place limits on potential contributions from primordial magnetic waves and other exotic processes such as topological defects . These restrictions are comparable to previous measurements obtained using separate observation approaches .In addition we report the finding of a substantial frequency at speeds below 10GHz , which is not anticipated within conventional cosmological predictions . This might represent either a new cause of foreground contamination or a novel physical impact .Further investigation will demand additional studies to confirm this result and establish its identity . If confirmed it would offer important restrictions on experiments pursuing to explain the observed anisotropy in the CMB spectrum .",
        "rewrite_text": "We present new experiments from the Cosmosoma study aimed at detecting temperature fluctuations in the cosmic microwave background (CMB) that exceed those predicted by standard cosmological models. While the data align with current theoretical expectations, they exhibit some unexpected features that may be linked to previously unidentified foreground sources or systematic issues in our analysis methods. We have used these findings to impose constraints on possible contributions from primordial magnetic waves and other exotic phenomena, such as topological defects, with limits comparable to those from earlier measurement techniques. Additionally, we have identified a significant frequency below 10 GHz that is not predicted by traditional cosmological theories. This could indicate either a new source of foreground contamination or an intriguing physical effect. Further research is needed to verify this finding and clarify its nature. If validated, it could provide crucial constraints for experiments aimed at understanding the observed anisotropy in the CMB spectrum.",
        "ori-fast-z-score": -1.0540925533894598,
        "water-fast-z-score": 7.233165373381237,
        "rewrite-fast-z-score": -0.46499055497527714
    },
    {
        "original_text": "In this work, we study the problem of how many users to turn on for multi-antenna broadcast channels (MABCs). We first show that the optimal number of active users is equal to the rank of the channel matrix when all users have the same average signal-to-noise ratio (SNR) and there are no power constraints at the base station. Then, under general conditions, we prove that the optimal number of users is upper bounded by the minimum between the rank of the channel and the total number of available transmit antennas. Finally, we provide an algorithm which can find the exact solution within polynomial time complexity. The results obtained here may help us design more efficient MABC systems with reduced computational cost. In wireless communications, broadcasting refers to sending information simultaneously to multiple receivers over a shared medium such as radio waves or fiber optics. This type of communication has been widely used in various applications including digital television, video conferencing, data transmission, etc., where it is desirable to send messages to several users simultaneously  1  . However, due to limited resources, only a subset of these users will receive useful signals while others experience interference  2  .\nThe main challenge in designing broadcast systems lies in determining the best set of users who should be turned on so that each user receives its intended message without causing too much interference to other users  3  , i.e., finding the optimal user selection strategy  4  -  6  . For example, if one wants to maximize the sum rate of all users subject to individual power constraints, then the optimal user selection strategy depends not only on the channel state information but also on the power allocation policy  7  . Therefore, the joint optimization of user selection and power control becomes very complicated  8  .",
        "watermark_text": "In this research , we study the issue of how many users to turn on for multi - antenna broadcast channels ( MABCs ) . We first see that the ideal amount of active consumers is equal to the rank of the channel matrix when all users have the same average sound - to - noise proportion ( SNR ) and there are no power limitations at the base station .Then , under general circumstances , we prove that the ideal amount of subscribers is upper bounded by the limit between the rank of the channel and the total quantity of available transmit antennas . Finally , we provide an algorithm which can find the exact solution within polynomial time complexity .The results derived here perhaps allow us design more efficient MABC devices with decreased computational expensive . In telecommunications transmission , broadcasting refers to sent information continuously to multiple receivers over a shared medium such as radio pulses or fiber optics .This kind of communication has been widely useful in different environments including digital broadcasting , television conferencing , data broadcasting , etc . , where it is desirable to give communication to several users simultaneously 1 . However , owing to limited facilities , only a subset of these users will receive valuable inputs while others experience interference 2 .The main challenge in building transmitted schemes lies in selecting the best set of consumers who should be turned on so that each player receives its intended message without producing too little interference to other people 3 , i . e . , finding the ideal user choice strategy 4 - 6 . For instance , if one wants to maximize the sum frequency of all users subject to individual power limitations , then the ideal user choice strategy depends not only on the channel state information but also on the power distribution policy 7 .Therefore , the joint optimization of customer preference and power control makes very complicated 8 .",
        "rewrite_text": "In this research, we investigate the question of how many users should be activated in multi-antenna broadcast channels (MABCs). Initially, we establish that the optimal number of active users equals the rank of the channel matrix, assuming all users share the same average signal-to-noise ratio (SNR) and there are no power restrictions at the base station. Subsequently, we demonstrate that, under more general conditions, the ideal number of users is limited by the lesser of the rank of the channel and the total number of available transmit antennas. We then present an algorithm that efficiently determines the exact solution with polynomial time complexity. The insights gained from this study may enable the design of more efficient MABC devices with reduced computational costs. In telecommunications, broadcasting refers to the continuous transmission of information to multiple receivers over a shared medium, such as radio waves or fiber optics. This mode of communication is widely utilized in various contexts, including digital broadcasting, television conferencing, and data dissemination, where simultaneous communication to multiple users is preferred. However, due to limited resources, only a subset of users will receive meaningful signals, while others may encounter interference. The primary challenge in developing transmission schemes lies in selecting the optimal group of users to activate, ensuring that each user receives their intended message without causing excessive interference to others—essentially, identifying the best user selection strategy. For example, if the goal is to maximize the total frequency across all users while adhering to individual power constraints, the optimal user selection strategy becomes dependent on both the channel state information and the power distribution policy. Consequently, the joint optimization of user selection and power control becomes quite complex.",
        "ori-fast-z-score": 0.8669214468630108,
        "water-fast-z-score": 10.324246321732218,
        "rewrite-fast-z-score": 1.4142135623730951
    },
    {
        "original_text": "We present an analysis of the Swift/BAT light curve for GRB 060912, which is one of only two bursts to have been classified as both a  long-soft  (LS) and a  short-hard  (SH) event by the BAT team.  We find that this classification is not robust against changes in the background model used or variations in the time binning applied; we also show that it does not hold up when compared with other LS events observed by Swift. In addition, we demonstrate that the spectral evolution seen during the prompt phase cannot be explained within either the SH or LS models. Finally, we argue that the most likely explanation for these observations is that GRB 060912 was actually a single pulse lasting several hundred seconds, rather than being composed of multiple pulses separated by quiescent intervals. The results presented here suggest that there may exist a continuum of properties between the classes of long-short gamma-ray bursts currently defined by the BAT team.",
        "watermark_text": "We present an assessment of the Swift / BAT light curve for GRB 060912 , which is one of only two pulses to have been classified as both a long - hard ( LS ) and a short - hard ( SH ) event by the BAT crew . We see that this classification is not stable against shifts in the background model used or variations in the period binning applied ; we also demonstrate that it does not stand up when compared with other LS events observed by Swift .In addition , we prove that the spectral evolution seen during the prompt stage cannot be described within either the SH or LS models . Finally , we claim that the most likely explanation for these observations is that GRB 060912 was actually a single signal lasting several hundred moments , rather than being composed of multiple pulses divided by quiescent intervals .The results presented here suggest that there may contain a continuum of properties between the classes of large - short gamma - ray bursts currently defined by the BAT group .",
        "rewrite_text": "We provide an evaluation of the Swift/BAT light curve for GRB 060912, which is uniquely classified as both a long-hard (LH) and a short-hard (SH) event by the BAT team. Our findings indicate that this classification is not robust, as it can vary with changes in the background model or different period binning methods. Moreover, this classification does not hold up when assessed against other long-hard events recorded by Swift. Additionally, we demonstrate that the spectral evolution observed during the prompt phase cannot be accurately explained by either SH or LH models. Ultimately, we propose that the most plausible interpretation of these observations is that GRB 060912 represents a single, prolonged signal lasting several hundred milliseconds, rather than a sequence of multiple pulses separated by quiet intervals. These results imply the existence of a continuum of properties between the currently defined categories of short and long gamma-ray bursts by the BAT group.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": -0.9561828874675149
    },
    {
        "original_text": "We present an analysis of the worst-case time complexity for inserting n elements into a binary search tree using the Fibonacci-heap data structure, which is based on the so-called  Young-Fibonacci insertions . We show that this algorithm has O(n log n) worst-case running time and thus improves upon previous results by Chan et al., who showed that it takes at least Ω(n2/3) time to perform these operations in the worst case. The main idea behind our proof is to use amortized analysis techniques to bound the number of times we have to update the heap s size during each step of the algorithm. This allows us to prove that the total cost incurred over all steps is bounded by O(n log n). In addition, we also provide experimental evidence showing that our theoretical bounds are tight up to constant factors. \nIntroduction\n\nThe Fibonacci-heap data type was introduced by Fredman and Tarjan  FT86  as part of their work on efficient algorithms for finding minimum spanning trees (MSTs), shortest paths, and other related problems. It consists of a rooted binary tree with two additional fields associated with every node: its parent pointer and its rank value. Every node stores pointers to its left child and right child nodes; however, unlike ordinary binary trees, there can be more than one node with the same key stored under any given node. Each node maintains a counter called its rank, which indicates how many keys equal to or less than the key of the current node exist below it; if no such key exists then the rank field contains zero. \n \n For example, consider the following Fibonacci-heap storing the values 1 through 5:",
        "watermark_text": "We present an assessment of the worst - case time complexity for inserting n elements into a binary find tree using the Fibonacci - heap data structure , which is based on the so - called Young - Fibonacci insertions . We see that this algorithm has O ( n log n ) worst - case ran time and therefore improves upon recent results by Chan et al . , who demonstrated that it takes at least Ω ( n2 / 3 ) time to conduct these functions in the worst case .The main idea behind our proof is to use amortized analysis methods to bound the quantity of twice we have to update the heap s size during each step of the method . This enables us to prove that the total cost incurred over all stairs is bounded by O ( n log n ) .In addition , we also provide empirical evidence showing that our theory bounds are strict up to constant factors . Introduction The Fibonacci - heap data type was introduced by Fredman and Tarjan FT86 as part of their work on effective methods for finding minimum spanning trees ( MSTs ) , shortest trails , and other related problems .It consists of a rooted binary tree with two added fields associated with every node : its father pointer and its rank value . Every node stores pointers to its right son and left baby nodes ; however , unlike ordinary binary forests , there can be more than one node with the same key contained under any certain vertex .Each node maintains a counter termed its rank , which reflects how many keys equivalent to or smaller than the key of the present node reside below it ; if no such key exists then the rank field contains zero . For instance , consider the following Fibonacci - heap counting the numbers 1 through 5 :",
        "rewrite_text": "We provide an evaluation of the worst-case time complexity for inserting \\( n \\) elements into a binary find tree using the Fibonacci heap data structure, which employs a technique known as Young-Fibonacci insertions. Our analysis reveals that this algorithm exhibits a worst-case runtime of \\( O(n \\log n) \\), representing an improvement over recent findings by Chan et al., who established that at least \\( \\Omega(n^{2/3}) \\) time is required for these operations in the worst case. Central to our proof is the use of amortized analysis to determine the number of times we need to update the heap’s size at each stage of the process. This approach allows us to demonstrate that the overall cost incurred across all operations is limited to \\( O(n \\log n) \\). Furthermore, we offer empirical evidence that confirms our theoretical bounds are stringent to constant factors. \n\n**Introduction**: The Fibonacci heap data structure was introduced by Fredman and Tarjan (FT86) as part of their exploration of efficient algorithms for finding minimum spanning trees (MSTs), shortest paths, and other related tasks. It consists of a rooted binary tree enhanced with two additional attributes for each node: a parent pointer and a rank value. Each node maintains pointers to its left and right children, but unlike conventional binary trees, multiple nodes can share the same key under a given parent. Each node's rank field indicates the count of keys that are less than or equal to its own key located in its subtree; if no such keys are present, the rank is set to zero. For example, consider the following Fibonacci heap that counts the numbers from 1 to 5:",
        "ori-fast-z-score": -0.6713450866373513,
        "water-fast-z-score": 5.706433236417486,
        "rewrite-fast-z-score": 0.8703882797784892
    },
    {
        "original_text": "We present the results of observations made by the Multiband Imaging Photometer for Spitzer (MIPS) in 24 and 70 micron bands toward the Serpens cloud core. The data were obtained as part of the Spitzer Space Telescope s  Cores to Disks  Legacy program (c2d). We have detected more than 100 infrared sources within an area of 0.5 square degrees centered on the Serpens South region. Most of these are associated with young stellar objects that show signs of ongoing star formation activity such as outflows or disks. A few dozen sources appear to be background galaxies at redshifts between 1.2 and 3.6. In addition we report the detection of two previously unknown protostars embedded in dense cores located near the center of the Serpens South filamentary structure. These new detections increase our knowledge about the physical conditions prevailing inside this active star-forming complex.",
        "watermark_text": "We present the conclusion of measurements made by the Multiband Imaging Photometer for Spitzer ( MIPS ) in 24 and 70 micron bands toward the Serpens cloud core . The data were obtained as part of the Spitzer Space Telescope s Cores to Disks Legacy project ( c2d ) .We have discovered more than 100 infrared sources within an area of 0 . 5 square degrees centered on the Serpens South region . Most of these are related with young stellar bodies that display signs of ongoing galaxy formation activity such as outflows or disks .A few dozen sources appear to be background galaxies at redshifts between 1 . 2 and 3 . 6 . In addition we publish the observation of two formerly unidentified protostars embedded in dense cores located near the center of the Serpens South filamentary complex .These new detections increase our information about the physical conditions prevailing inside this active star - creating complex .",
        "rewrite_text": "We present the findings from measurements taken by the Multiband Imaging Photometer for Spitzer (MIPS) in the 24 and 70 micron bands, focused on the Serpens cloud core. This data was collected as part of the Cores to Disks Legacy project (c2d) under the Spitzer Space Telescope. In our survey, we identified over 100 infrared sources within a 0.5 square degree area centered on the Serpens South region. The majority of these sources are associated with young stellar objects exhibiting signs of ongoing star formation activities, such as outflows or circumstellar disks. Additionally, we detected a few dozen sources that appear to be background galaxies with redshifts ranging from 1.2 to 3.6. Furthermore, we report the discovery of two previously unidentified protostars located in dense cores near the center of the Serpens South filamentary complex. These new findings enhance our understanding of the physical conditions prevalent in this active star-forming region.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 4.27617987059879,
        "rewrite-fast-z-score": 0.3841106397986879
    },
    {
        "original_text": "We report on the fabrication and characterization of charge qubits based on self-assembled InAs quantum dots (QDs) embedded in GaAs/AlGaAs heterostructures. We show that by using an optimized growth procedure, we can achieve high quality QD layers with low density of defects which are crucial for achieving good coherence times. The samples were grown by molecular beam epitaxy at 600 °C under As-rich conditions to minimize the formation of threading dislocations. A single layer of self-assembled InAs/GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm thick Al0.3Ga0.7As barrier layer. Finally, a 20 nm thick GaAs capping layer was deposited. The sample structure is shown schematically in Figure 1 . The photoluminescence spectrum shows emission peaks centered around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states associated with charged excitons.",
        "watermark_text": "We report on the fabrication and description of charge qubits based on self - assembled InAs quantum dots ( QDs ) integrated in GaAs / AlGaAs heterostructures . We suggest that by using an optimized growth technique , we can attain high quality QD layers with minimal concentration of flaws which are important for achieving better coherence times .The samples were cultivated by molecular wave epitaxy at 600 °C under As - rich conditions to minimize the formation of threading dislocations . A single mesh of self - assembled InAs / GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm wide Al0 . 3Ga0 . 7As barrier layer .Finally , a 20 nm deep GaAs capping layer was extracted . The sample structure is displayed schematically in Figure 1 .The photoluminescence spectrum displays emission levels focused around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states correlated with charged excitons .",
        "rewrite_text": "We present our findings on the creation and characterization of charge qubits utilizing self-assembled InAs quantum dots (QDs) integrated within GaAs/AlGaAs heterostructures. We propose that by implementing an optimized growth technique, we can achieve high-quality QD layers with a reduced number of defects, which are crucial for enhancing coherence times. The samples were grown using molecular beam epitaxy at 600 °C in As-rich conditions to reduce the likelihood of threading dislocations. After annealing at 650 °C for 10 seconds, a single layer of self-assembled InAs/GaAs QDs was formed, followed by the deposition of a 50 nm thick Al0.3Ga0.7As barrier layer. Finally, a 20 nm thick GaAs capping layer was added. A schematic representation of the sample structure is illustrated in Figure 1. The photoluminescence spectrum reveals emission peaks centered around 1280 nm, corresponding to the ground state excitonic transitions of individual QDs, along with higher energy states associated with charged excitons.",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 4.213504858001922,
        "rewrite-fast-z-score": -0.5933908290969266
    },
    {
        "original_text": "We investigated the dominant aerosol processes in the atmosphere using ground-based remote sensing and chemical analysis data collected at Kashiwa, Chiba Prefecture (Chiba), Japan, between September 2009 and March 2010 under severe air pollution conditions caused by anthropogenic emissions. The results showed that sulfate particles were mainly produced through gas-to-particle conversion via homogeneous nucleation on days with low relative humidity (RH) values; however, they were also formed as secondary organic aerosols (SOAs) when RH was higher than 80%. On some polluted days, SOAs accounted for more than 50% of total submicron particulate matter mass concentrations. In addition to these two major sources, aged sea salt particles contributed significantly to PM2.5 mass concentration levels. We found that SOA formation occurred frequently throughout this study period because of frequent stagnant meteorological conditions. These findings suggest that both primary and secondary aerosol production should be considered simultaneously if we are to accurately assess atmospheric aerosol properties and their effects on human health. \n \n Keywords: Aerosol process, Remote sensing, Chemical composition",
        "watermark_text": "We researched the dominant aerosol processes in the air using ground - based remote sensing and chemical analysis evidence generated at Kashiwa , Chiba Prefecture ( Chiba ) , Japan , between September 2009 and March 2010 under extreme aerial contamination conditions caused by anthropogenic emissions . The results suggested that sulfate gases were mainly created through gas - to - particle conversion via homogeneous nucleation on days with lowest relative humidity ( RH ) values ; however , they were also formed as secondary organic aerosols ( SOAs ) when RH was lower than 80 % .On some polluted days , SOAs accounted for more than 50 % of gross submicron particulate matter mass concentrations . In addition to these two principal sources , aged ocean water particles contributed significantly to PM2 . 5 mass abundance levels .We showed that SOA structure occurred frequently throughout this study era because of frequent stagnant meteorological conditions . These conclusions show that both primary and secondary aerosol production should be evaluated concurrently if we are to correctly examine atmospheric aerosol characteristics and their impacts on human health .Keywords : Aerosol process , Remote sensing , Chemical composition",
        "rewrite_text": "We conducted a study on the primary aerosol processes in the atmosphere by utilizing ground-based remote sensing and chemical analysis data collected in Kashiwa, Chiba Prefecture, Japan, from September 2009 to March 2010, during periods of significant air pollution due to human activities. Our findings indicated that sulfate gases primarily formed through gas-to-particle conversion via homogeneous nucleation on days with the lowest relative humidity (RH). Additionally, secondary organic aerosols (SOAs) were produced when RH dropped below 80%. On particularly polluted days, SOAs comprised more than 50% of the total mass concentration of submicron particulate matter. Alongside these main sources, older ocean water particles also played a considerable role in the abundance of PM2.5 mass. The study revealed that the structure of SOAs was commonly observed during this time due to frequent stagnant weather conditions. These results underline the importance of analyzing both primary and secondary aerosol production together to accurately assess atmospheric aerosol properties and their effects on human health. Keywords: Aerosol processes, Remote sensing, Chemical composition.",
        "ori-fast-z-score": 1.2375966910186262,
        "water-fast-z-score": 7.313071356019155,
        "rewrite-fast-z-score": 2.3050494597834974
    },
    {
        "original_text": "We present an analysis of galaxy clusters selected by their red-sequence galaxies (Gladders & Yee 2005) from the Sloan Digital Sky Survey Data Release 4 (DR4). We use two different methods to select cluster candidates, and then apply photometric redshift cuts on these samples to obtain final catalogues with high purity. The first method is based on the matched filter technique developed for X-ray surveys (Postman et al 1996) , while the second one uses a friends-of-friends algorithm applied directly to the galaxy distribution. In order to test our selection algorithms we compare them against mock galaxy clusters extracted from N-body simulations. Our main results are as follows:  1. Using the matched filter technique we find that the number density of galaxy clusters at z < 0.5 is n(z<0.5) = 3.6 +/- 0.2 x 10-3 h3 Mpc-3 . This value agrees well with previous determinations using other techniques.  2. By applying the same matched filter technique to simulated galaxy clusters we show how this method can be used to estimate the mass function of galaxy clusters up to z ~1.0.",
        "watermark_text": "We present an assessment of galaxy regions selected by their red - sequence galaxies ( Gladders & Yee 2005 ) from the Sloan Digital Sky Survey Data Release 4 ( DR4 ) . We use two different methods to select cluster applicants , and then use photometric redshift cutting on these specimens to obtain final catalogues with high purity .The first method is based on the matched filter technique developed for X - ray observations ( Postman et al 1996 ) , while the second one uses a enemies - of - friends method applied directly to the galaxy distribution . In order to test our choice algorithms we compare them against mock galaxy galaxies extracted from N - bodies simulations .Our main results are as follows : 1 . Using the matched filter technique we find that the number density of galaxy galaxies at z < 0 . 5 is n ( z < 0 . 5 ) = 3 . 6 + / - 0 . 2 x 10 - 3 h3 Mpc - 3 .This value agrees well with previous determinations using other techniques.2.By applying the same matched filter technique to modeled galaxy galaxies we find how this algorithm can be used to estimate the mass value of galaxy galaxies up to z ~ 1 . 0 .",
        "rewrite_text": "We present an evaluation of galaxy regions identified by their red-sequence galaxies (Gladders & Yee 2005) using data from the Sloan Digital Sky Survey Data Release 4 (DR4). Our approach employs two distinct methods for selecting cluster candidates, followed by a photometric redshift cut to create final catalogs with high purity. The first method utilizes the matched filter technique designed for X-ray observations (Postman et al. 1996), while the second method applies a friends-of-friends algorithm directly to the galaxy distribution. To validate our selection algorithms, we compare them against mock galaxy datasets derived from N-body simulations. Our key findings are as follows: 1. Utilizing the matched filter technique, we determine the number density of galaxies at z < 0.5 to be n(z < 0.5) = 3.6 ± 0.2 x 10^-3 h^3 Mpc^-3. This result aligns closely with previous estimates obtained through other methods. 2. By applying the matched filter technique to simulated galaxy distributions, we demonstrate that this algorithm can be used to estimate the mass of galaxy clusters up to z ~ 1.0.",
        "ori-fast-z-score": 1.4924050144892729,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 1.6081688022566922
    },
    {
        "original_text": "We show that the non-Gaussian distributions observed in financial returns are due to microscopic interactions between traders and their environment, which lead to non-trivial correlations among different time scales. We present an analytical model for these correlations based on random matrix theory (RMT), which is able to reproduce all statistical properties of real market data with high accuracy. \n \n The distribution of stock prices has been studied extensively over many decades  1  . It was found that this distribution can be well approximated by a Gaussian function  2  , but deviations have also been reported  3  . In particular, it has been shown recently  4  that the tails of the return distribution follow a power law decay P(r) ~ r−α with α = 3 ± 0.1. This finding contradicts the predictions of standard models such as Black-Scholes  5  or Heston  6  , where the tail exponent should be equal to 2  7, 8  .\n \nIn order to explain the origin of these deviations we propose here a new approach based on Random Matrix Theory  9  . RMT describes the statistics of complex systems whose dynamics depend on a large number N of degrees of freedom  10  . For example, RMT has successfully been applied to describe the fluctuations of energy levels  11  , wave functions  12  , spin states  13  , quantum transport  14  , and chaotic scattering  15  . Recently, RMT has also been used to study the statistical properties of stock markets  16  -  22  . Here we will focus on the so-called Dyson Brownian motion  23  , which describes the evolution of a system under the influence of white noise.",
        "watermark_text": "We see that the non - Gaussian distributions found in financial returns are owing to microscopic interactions between traders and their environment , which lead to non - simple correlations among different time ranges . We present an analytical theory for these correlations based on random matrix theory ( RMT ) , which is easy to capture all statistical characteristics of real trading information with high clarity .The function of stock rates has been studied thoroughly over much generations 1 . It was shown that this distribution can be well approximated by a Gaussian distribution 2 , but deviations have also been reported 3 .In particular , it has been shown recently 4 that the tails of the return distribution observe a power law decay P ( r ) ~ r−α with α = 3 ± 0 . 1 . This fact contradicts the estimates of standard models such as Black - Scholes 5 or Heston 6 , where the tail exponent should be equal to 2 7 , 8 .In order to explain the origin of these deviations we undertake here a new approach based on Random Matrix Theory 9 . RMT describes the statistics of complex systems whose dynamics depend on a large number N of degrees of freedom 10 .For instance , RMT has successfully been used to explain the fluctuations of power states 11 , wave systems 12 , spin states 13 , quantum transport 14 , and chaotic scattering 15 . Recently , RMT has additionally been used to study the statistical characteristics of financial traders 16 - 22 .Here we will focus on the so - called Dyson Brownian moving 23 , which explains the evolution of a system under the effects of white sound .",
        "rewrite_text": "Non-Gaussian distributions commonly observed in financial returns are attributed to the microscopic interactions between traders and their environment, leading to complex correlations across various time scales. We propose an analytical theory for understanding these correlations using random matrix theory (RMT), which effectively captures all statistical features of genuine trading data with great clarity. The behavior of stock prices has been extensively analyzed over many years, and while it has been shown that these distributions can be approximated by a Gaussian distribution, notable deviations have also been identified. Recent findings indicate that the tails of the return distribution follow a power-law decay, P(r) ~ r^−α, with α being approximately 3 ± 0.1. This observation stands in contrast to the predictions of traditional models like Black-Scholes and Heston, which suggest a tail exponent of 2. To explore the origins of these discrepancies, we adopt a novel approach grounded in random matrix theory. RMT provides a framework for understanding the statistics of complex systems characterized by a large number of degrees of freedom. It has been effectively applied to various domains, including fluctuations in power states, wave phenomena, spin systems, quantum transport, and chaotic scattering. Recently, RMT has also been utilized to investigate the statistical properties of financial traders. In this work, we will concentrate on the Dyson Brownian motion model, which describes the dynamics of a system influenced by random white noise.",
        "ori-fast-z-score": -1.044073795327749,
        "water-fast-z-score": 5.789863774090244,
        "rewrite-fast-z-score": -1.0954451150103321
    },
    {
        "original_text": "We present the results of our numerical simulations of magnetized accretion disks in which we solve the mean-field magnetohydrodynamic equations for an axisymmetric disk with a prescribed radial distribution of angular momentum and mass fluxes, using the shearing-box approximation.  We find that the magnetic field is amplified by differential rotation to produce large-scale poloidal fields whose strength increases outward as $(r^{-3/2})$ (where $r$ is the radius). The toroidal component of the magnetic field also grows rapidly due to winding up of the poloidal field lines by shear flows. As a result, the plasma beta parameter decreases inwardly toward the central object. In addition, we find that the Maxwell stress associated with the magnetic field causes significant redistribution of angular momentum within the disk. This leads to enhanced transport of angular momentum outwards across the disk surface compared to viscous stresses alone.",
        "watermark_text": "We present the conclusion of our numerical simulations of magnetized accretion disks in which we solve the mean - field magnetohydrodynamic equations for an axisymmetric disk with a prescribed radial distribution of angular velocity and mass fluxes , using the shearing - box method . We see that the magnetic force is amplified by differential rotation to produce wide - scale poloidal fields whose strength changes outward as $ ( r ^ { - 3 / 2 } ) $ ( where $ r $ is the radius ) .The toroidal portion of the magnetic force thus rises frequently due to winding up of the poloidal field lines by shear flows . As a result , the plasma beta factor decreases inwardly toward the main object .In addition , we find that the Maxwell stress involved with the magnetic force produces significant redistribution of angular velocity within the disk . This leads to heightened transport of angular velocity outwards across the disk boundary compared to viscous stresses alone .",
        "rewrite_text": "We present the findings from our numerical simulations of magnetized accretion disks, where we solve the mean-field magnetohydrodynamic equations for an axisymmetric disk with a specified radial distribution of angular velocity and mass fluxes, utilizing the shearing-box method. Our results indicate that differential rotation enhances the magnetic force, resulting in large-scale poloidal fields whose strength varies outward as \\( r^{-3/2} \\) (with \\( r \\) being the radius). Consequently, the toroidal component of the magnetic force frequently increases due to the winding of poloidal field lines by shear flows. This leads to a reduction in the plasma beta factor as one moves inward towards the central object. Furthermore, we observe that the Maxwell stress associated with the magnetic force causes notable redistribution of angular velocity within the disk, resulting in an increased outward transport of angular velocity across the disk boundary when compared to viscous stresses alone.",
        "ori-fast-z-score": 2.223781796726481,
        "water-fast-z-score": 6.4372630957871815,
        "rewrite-fast-z-score": 3.5795716689756794
    },
    {
        "original_text": "We present near-infrared (NIR) spectroscopy obtained with Subaru/HDS on day +16 after explosion for the peculiar type Ib supernova SN2006jc, which shows prominent dust formation in its dense shell. The NIR spectrum is dominated by strong P-Cygni profiles of H I Balmer lines and Fe II multiplets at 4233-4245 Å . We find that these features are well reproduced by our non-LTE model calculations assuming an electron density ne = 1 x 10^9 cm-3 , temperature Te = 5500 K , and mass-loss rate oḟ M = 2 x 10^-6 M_sun/yr. In addition to the above mentioned features, we detect weak but significant He II 4686 emission line feature in the red wing of the Hα profile. This suggests that there may be some contribution from helium recombination radiation to the observed fluxes of hydrogenic lines.",
        "watermark_text": "We use near - infrared ( NIR ) spectroscopy acquired with Subaru / HDS on week + 16 after explosion for the unusual type Ib supernova SN2006jc , which shows significant dust form in its dense shell . The NIR spectrum is dominated by strong P - Cygni profiles of H I Balmer bands and Fe II multiplets at 4233 - 4245 Å .We see that these characteristics are better illustrated by our non - LTE model calculations assuming an electron concentration ne = 1 x 10 ^ 9 centimeters - 3 , temperature Te = 5500 K , and mass - loss rate [UNK] M = 2 x 10 ^ - 6 M _ sun / yr . In addition to the above mentioned properties , we perceive slight but significant He II 4686 absorption line feature in the red wing of the Hα profile .This implies that there may be some influence from helium recombination emission to the seen fluxes of hydrogenic lines .",
        "rewrite_text": "We analyzed near-infrared (NIR) spectroscopy obtained with Subaru/HDS at week 16 post-explosion for the unusual type Ib supernova SN2006jc, which exhibits considerable dust formation within its dense shell. The NIR spectrum is primarily characterized by prominent P-Cygni profiles of H I Balmer lines and Fe II multiplets in the range of 4233-4245 Å. Our non-LTE model calculations, assuming an electron concentration of ne = 1 x 10^9 cm^-3, a temperature of Te = 5500 K, and a mass-loss rate of [UNK] M = 2 x 10^-6 M_sun/yr, better highlight these features. Additionally, we observe a subtle yet significant He II 4686 absorption line in the red wing of the Hα profile, suggesting potential contributions from helium recombination emission to the observed hydrogenic line fluxes.",
        "ori-fast-z-score": -1.4832396974191326,
        "water-fast-z-score": 4.08248290463863,
        "rewrite-fast-z-score": 0.2672612419124244
    },
    {
        "original_text": "The magnetization, susceptibility, and specific heat measurements were performed on the single crystals of TbFe3( BO3 )4 . The magnetic properties are analyzed in terms of the crystal-field splitting scheme for Tb3+ ions. It is found that the ground state doublet has an Ising-like anisotropy along c-axis with gz = 8.0 ± 0.1 , which leads to the large spontaneous polarization ( Ps ~ 1μC/cm2 ). The calculated results reproduce well the experimental data except for the low-temperature part of the specific-heat curve below 2 K. This discrepancy may be attributed to the presence of impurities or defects in our samples. \n \n Keywords: Magnetism; Crystal field theory; Specific heat measurement; Susceptibility measurement; Single-crystal growth; Anisotropic magnetoresistance effects; Polarized neutron scattering \n \n \n \n INTRODUCTION : \nTbFe 3 (BO 3 ) 4 belongs to the family of rare-earth iron borates RFe 3 (BO 3 ) (R = Y, Yb, Lu). These compounds have attracted much attention because they exhibit various interesting physical phenomena such as ferroelectricity  1  , multiferroicity  2  , colossal magnetoresistance  3  , and quantum critical behavior  4  .\nIn particular, TbFe 3 (BO 3 )\n4 exhibits a giant spontaneous polarization P s ~ 1 μ C / cm 2 at room temperature  5  due to its unique crystal structure  6  . In this compound, Fe atoms form a three-dimensional network of corner-sharing tetrahedra by sharing their apical oxygen atoms  7   . On the other hand, Tb atoms occupy two different sites, i.e., one site surrounded by eight O atoms forming a square antiprismatic coordination polyhedron  8  and another site surrounded by six O atoms forming a trigonal prismatic coordination polyhedron  9  . As shown in Figs. 1 (a) and (b), these two types of polyhedra share common faces perpendicularly to the c -axis  10  .",
        "watermark_text": "The magnetization , susceptibility , and particular heat measurements were performed on the single crystals of TbFe3 ( BO3 ) 4 . The magnetic properties are examined in terms of the crystal - field separation scheme for Tb3 + ions .It is found that the ground state doublet has an Ising - like anisotropy along c - axis with gz = 8 . 0 ± 0 . 1 , which results to the big induced polarization ( Ps ~ 1μC / cm2 ) . The measured data reproduce well the laboratory information except for the high - temperature half of the specific - temperature curve below 2 K . This discrepancy may be due to the presence of impurities or flaws in our specimens .Keywords : Magnetism ; Crystal field description ; Specific temperature measurement ; Susceptibility measurement ; Single - crystal growth ; Anisotropic magnetoresistance effects ; Polarized neutron scattering INTRODUCTION : TbFe 3 ( BO 3 ) 4 belongs to the group of rare - earth iron borates RFe 3 ( BO 3 ) ( R = Y , Yb , Lu ) . These compounds have garnered great popularity because they demonstrate several interesting physical phenomena such as ferroelectricity 1 , multiferroicity 2 , colossal magnetoresistance 3 , and quantum fundamental behavior 4 .In particular , TbFe 3 ( BO 3 ) 4 displays a giant spontaneous polarization P s ~ 1 μ C / cm 2 at room temperature 5 due to its unique crystal formation 6 . In this compound , Fe molecules form a three - dimensional network of spot - sharing tetrahedra by sharing their apical oxygen atoms 7 .On the other hand , Tb molecules occupy two different places , i . e . , one site surrounded by eight O atoms forming a square antiprismatic coordination polyhedron 8 and another site surrounded by six O atoms forming a trigonal prismatic coordination polyhedron 9 . As seen in Figs .1 ( a ) and ( b ) , these two forms of polyhedra share shared faces perpendicularly to the c - axis 10 .",
        "rewrite_text": "Magnetization, susceptibility, and specific heat measurements were conducted on single crystals of TbFe3(BO3)4. The magnetic properties were analyzed using a crystal-field separation framework for Tb3+ ions. It was observed that the ground state doublet exhibits Ising-like anisotropy along the c-axis, with gz = 8.0 ± 0.1, leading to a significant induced polarization (Ps ~ 1 μC/cm²). The experimental data aligns well with established laboratory results, apart from the higher-temperature region of the specific heat curve below 2 K. This inconsistency may be attributed to impurities or imperfections in the samples. \n\n**Keywords**: Magnetism; Crystal field description; Specific heat measurement; Susceptibility measurement; Single-crystal growth; Anisotropic magnetoresistance effects; Polarized neutron scattering \n\n**INTRODUCTION**: TbFe3(BO3)4 is part of the rare-earth iron borates family, denoted as RFe3(BO3) (where R = Y, Yb, Lu). These compounds have attracted significant attention due to their remarkable physical properties, including ferroelectricity, multiferroicity, colossal magnetoresistance, and fundamental quantum behaviors. Specifically, TbFe3(BO3)4 exhibits an impressive spontaneous polarization of Ps ~ 1 μC/cm² at room temperature, which arises from its distinct crystal structure. In this material, Fe ions generate a three-dimensional network of corner-sharing tetrahedra by sharing apical oxygen atoms. Conversely, Tb ions occupy two types of sites: one surrounded by eight oxygen atoms, forming a square antiprismatic coordination polyhedron, and the other surrounded by six oxygen atoms, resulting in a trigonal prismatic coordination polyhedron. As illustrated in Figs. 1(a) and (b), these two types of polyhedra share their faces perpendicular to the c-axis.",
        "ori-fast-z-score": 1.4110813025753959,
        "water-fast-z-score": 7.748271696689158,
        "rewrite-fast-z-score": 1.4237369936287485
    },
    {
        "original_text": "We prove that for any surface S of general type, the action of its absolute Galois group G(S) on the set of connected components of the modulis space M_g(S) is faithful.  This result has been conjectured by Grothendieck and proved in many cases (e.g., when g = 0 or 1).  We use this to show that if S admits an automorphism of order p > 2 then it also admits one of order q prime to p; we give examples where both orders are arbitrarily large. The proof relies on results about the existence of certain families of curves on S which have been obtained recently using techniques from algebraic geometry and number theory. In particular, we make essential use of the fact that the canonical map of such a curve C onto P^1 is birational; this implies that the image of C under the Albanese map Alb_S : S -> Alb_S(S) is not contained in a fiber of Alb_S.",
        "watermark_text": "We prove that for any manifold S of general kind , the operation of its absolute Galois group G ( S ) on the group of connected parts of the modulis space M _ g ( S ) is faithful . This result has been conjectured by Grothendieck and demonstrated in many situations ( e . g . , when g = 0 or 1 ) .We use this to see that if S gives an automorphism of order q > 2 then it also admits one of order q prime to p ; we give instance where both orders are arbitrarily big . The proof draws on findings about the existence of certain classes of curves on S which have been achieved lately utilizing techniques from algebraic topology and number theory .In particular , we give important use of the fact that the canonical mapping of such a curve C onto P ^ 1 is birational ; this implies that the image of C under the Albanese map Alb _ S : S - > Alb _ S ( S ) is not enclosed in a fiber of Alb _ S .",
        "rewrite_text": "We demonstrate that for any general manifold \\( S \\), the action of its absolute Galois group \\( G(S) \\) on the group of connected components of the moduli space \\( M_g(S) \\) is faithful. This result, which has been conjectured by Grothendieck, has been confirmed in various scenarios, such as when \\( g = 0 \\) or \\( g = 1 \\). We leverage this finding to show that if \\( S \\) induces an automorphism of order \\( q > 2 \\), then it also possesses one of order \\( q \\) that is coprime to \\( p \\); we provide examples where both orders can be made arbitrarily large. The proof utilizes recent discoveries regarding the existence of specific classes of curves on \\( S \\), employing techniques from algebraic topology and number theory. Notably, we emphasize the significance of the fact that the canonical map from such a curve \\( C \\) to \\( \\mathbb{P}^1 \\) is birational, which ensures that the image of \\( C \\) under the Albanese map \\( \\text{Alb}_S: S \\to \\text{Alb}_S(S) \\) is not contained within a fiber of \\( \\text{Alb}_S \\).",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 5.08000508000762,
        "rewrite-fast-z-score": 0.254000254000381
    },
    {
        "original_text": "In this work, we propose an algorithm for blind identification (BI) of distributed antenna systems (DASs). The proposed BI method is based on the joint use of second-order statistics and higher order cumulants to estimate the number of active users in each cell as well as their carrier frequency offsets (CFOs), which are unknown parameters that need to be estimated before data detection can take place. We show by simulation results that our proposed method outperforms existing methods in terms of bit error rate performance when CFOs exist between different cells. In addition, it has lower computational complexity than other algorithms. \n \n Keywords: Blind identification; Distributed antenna systems; Second-order statistics; Higher order cumulants; CFO estimation. 1 Introduction \n \n With the rapid development of wireless communication technology, there have been increasing demands for high spectral efficiency and reliable transmission over limited bandwidth resources  1  . To meet these requirements, multi-antenna techniques such as multiple-input-multiple-output (MIMO)  2  , massive MIMO  3  -  5  , cooperative relaying  6  , and cognitive radio  7  have attracted much attention recently. Among them, distributed antenna systems (DAs)  8  -  10  provide significant advantages including improved coverage area, enhanced capacity, reduced power consumption, and increased network flexibility  11  . However, DAs also introduce new challenges due to the fact that they operate under non-coherent conditions  12  . For example, the channel state information (CSI) at the transmitter side cannot be obtained directly through uplink training or downlink feedback  13  . Therefore, how to obtain CSI accurately becomes one of the most important issues in DA design  14  .\n \nTo address this issue, several works  15  -  17  have investigated the problem of estimating the number of active users and their corresponding channels simultaneously using only statistical properties of received signals without requiring any prior knowledge about the transmitted symbols. These approaches exploit the inherent sparseness property of user activity patterns and utilize second-order statistics (SOS) and/or higher order cumulants (HOCs)  18  -  20  to identify the number of active users per cell. Then, the channel coefficients associated with",
        "watermark_text": "In this research , we propose an algorithm for blind analysis ( BI ) of distributed antenna devices ( DASs ) . The proposed BI approach is based on the joint use of second - order statistics and larger class cumulants to estimate the quantity of active consumers in each cell as also as their carrier signal offsets ( CFOs ) , which are unknown parameters that require to be assessed before data diagnosis can take occur .We suggest by simulation data that our proposed method outperforms current methods in terms of bit error rate capacity when CFOs arise between multiple cells . In addition , it has less computational complexity than other methods .Keywords : Blind identity ; Distributed antenna devices ; Second - order analysis ; Higher class cumulants ; CFO estimation . 1 Introduction With the increasing growth of radio communication techniques , there have been growing requirements for high spectral capacity and reliable transmission over limited bandwidth supplies 1 .To address these requirements , multi - antenna techniques such as multiple - input - multiple - output ( MIMO ) 2 , large MIMO 3 - 5 , joint relaying 6 , and cognitive television 7 have garnered considerable notice lately . Among them , dispersed antenna technologies ( DAs ) 8 - 10 provide significant advantages including increased coverage space , enhanced capacity , enhanced capacity consumption , and increased communication flexibility 11 .However , DAs additionally introduce novel challenges related to the fact that they operate under non - coherent environments 12 . For instance , the channel state information ( CSI ) at the broadcasting side cannot be obtained immediately through uplink conditioning or downlink feedback 13 .Therefore , how to obtain CSI correctly becomes one of the most important problems in DA design 14 . To address this question , various papers 15 - 17 have researched the issue of estimating the total of active participants and their corresponding networks simultaneously employing only statistical characteristics of received transmissions without using any earlier knowledge about the received symbols .These approaches exploit the intrinsic sparseness property of customer activity patterns and use second - order statistics ( SOS ) and / or greater order cumulants ( HOCs ) 18 - 20 to identify the total of active participants per cell . Then , the channel coefficients identified with",
        "rewrite_text": "In this research, we introduce an algorithm for blind analysis (BA) of distributed antenna systems (DASs). Our proposed BA method leverages the combined use of second-order statistics and higher-order cumulants to estimate the number of active users in each cell, as well as their carrier frequency offsets (CFOs), which are unknown parameters that need to be assessed before data analysis can commence. Simulation results indicate that our method outperforms existing techniques concerning bit error rate when CFOs occur across multiple cells. Additionally, it demonstrates lower computational complexity compared to other methods. \n\n**Keywords:** Blind analysis; Distributed antenna systems; Second-order statistics; Higher-order cumulants; CFO estimation.\n\n**1. Introduction** \n\nAs radio communication technologies advance, there is an increasing demand for high spectral efficiency and reliable transmission within limited bandwidth resources. To meet these demands, multi-antenna techniques such as multiple-input multiple-output (MIMO), large MIMO, joint relaying, and cognitive radio have gained considerable attention. Among these, distributed antenna systems (DAS) offer significant benefits, including enhanced coverage, improved capacity, better capacity utilization, and greater communication flexibility. However, DAS also present unique challenges as they operate in non-coherent environments. For example, the channel state information (CSI) at the transmission end cannot be readily acquired through uplink training or downlink feedback, making accurate CSI estimation a critical issue in DAS design. \n\nTo tackle this challenge, several studies have explored estimating the number of active users and their associated networks solely by utilizing the statistical properties of received signals, without relying on prior knowledge of the symbols received. These methods capitalize on the inherent sparsity of user activity patterns and employ second-order statistics and/or higher-order cumulants to ascertain the number of active users in each cell. Subsequently, the identified channel coefficients are...",
        "ori-fast-z-score": -3.047505999613382,
        "water-fast-z-score": 9.53595439531399,
        "rewrite-fast-z-score": -0.6964409092807231
    },
    {
        "original_text": "The production of light elements in the early universe is one of the most important unsolved problems in astrophysics, cosmology, nuclear physics and particle physics. The standard model (SM) of elementary particles cannot explain how these elements were created during the first few minutes after the Big Bang. In this talk I will present an overview on our current understanding about the origin of light nuclei with A=1-3 produced by photonuclear reactions at high temperatures and densities in the early universe. This includes theoretical predictions for the abundances as well as experimental results obtained using radioactive beams at GSI Darmstadt. Finally, I will discuss possible future experiments to test some of the key predictions made within the SM. Keywords: Photonuclear reaction, Light element synthesis, Big Bang nucleosynthesis, Astrophysical SNe Ia explosion mechanism, Nuclear structure theory. 1 Introduction.\nLight element synthesis in the early universe is among the most challenging open questions in modern science  1  . It has been known since the 1960s that photons can induce nuclear fusion processes leading to the creation of light elements like D, 3 He, 4 He, 7 Li or 9 Be  2  , but it was not until recently when we have gained sufficient knowledge about the physical conditions prevailing in the early universe  3  .\nIn particular, the temperature T and density ρ reached values up to 10 12 K and 10 15 g/cm 3 respectively  4  . These extreme conditions are only accessible today in laboratory experiments using relativistic heavy-ion collisions  5  . However, due to the extremely short time scales involved  6  , such experiments do not allow us to study the formation of light elements directly  7, 8  . Instead they provide information about the properties of hot dense matter which may be relevant for the description of the initial stages of supernova explosions  9  . On the other hand, the abundance pattern observed in primordial objects like white dwarfs  10  or metal-poor stars  11  provides valuable constraints on the models describing the evolution of the chemical composition of the universe  12  .",
        "watermark_text": "The production of light elements in the early universe is one of the most important unsolved issues in astrophysics , cosmology , nuclear science and particle science . The typical model ( SM ) of primary atoms cannot explain how these objects were created during the first few hours after the Big Bang .In this talk I will present an overview on our new understanding about the origin of light nuclei with A = 1 - 3 created by photonuclear reactions at high temperatures and densities in the early universe . This contains theoretical estimates for the abundances as well as research results derived using nuclear beams at GSI Darmstadt .Finally , I will explore possible future research to test some of the key predictions produced within the SM . Keywords : Photonuclear reaction , Light element synthesis , Big Bang nucleosynthesis , Astrophysical SNe Ia explosion mechanism , Nuclear structure hypothesis .1 Introduction . Light factor synthesis in the early universe is among the most challenging open questions in modern science 1 .It has been known since the 1960s that photons can induce nuclear fusion mechanisms leading to the creation of light elements like D , 3 He , 4 He , 7 Li or 9 Be 2 , but it was not until recently when we have achieved sufficient knowledge about the physical conditions prevailing in the early universe 3 . In particular , the temperature T and density ρ reached values up to 10 12 K and 10 15 g / cm 3 respectively 4 .These severe environments are only accessible today in laboratory experiments using relativistic heavy - ion collisions 5 . However , owing to the exceptionally short period scales involved 6 , such studies do not enable us to study the formation of light elements directly 7 , 8 .Instead they give information about the properties of bright heavy material which may be crucial for the description of the first phase of supernova explosions 9 . On the other hand , the density trend detected in primordial objects like white dwarfs 10 or metal - poor stars 11 provides valuable constraints on the models explaining the evolution of the chemical composition of the universe 12 .",
        "rewrite_text": "The synthesis of light elements in the early universe remains one of the most significant unresolved problems in astrophysics, cosmology, nuclear science, and particle physics. The prevailing Standard Model (SM) of primary atoms fails to elucidate how these elements were formed during the initial hours following the Big Bang. In this presentation, I will provide an overview of our enhanced understanding regarding the origins of light nuclei with mass numbers A = 1 to 3, which are produced through photonuclear reactions under high temperatures and densities in the early universe. This includes theoretical estimates of their abundances as well as findings from research conducted with nuclear beams at GSI Darmstadt. Additionally, I will discuss potential future investigations aimed at testing some of the critical predictions made by the Standard Model. \n\nKeywords: Photonuclear reaction, Light element synthesis, Big Bang nucleosynthesis, Astrophysical SNe Ia explosion mechanism, Nuclear structure hypothesis. \n\n**1 Introduction**  \nThe synthesis of light elements in the early universe is one of the most challenging open questions in contemporary science. Since the 1960s, it has been known that photons can initiate nuclear fusion processes that lead to the creation of light elements such as deuterium (D), helium-3 (3He), helium-4 (4He), lithium-7 (7Li), and beryllium-9 (9Be). However, it has only been recently that we have gained sufficient insights into the physical conditions present in the early universe. Specifically, temperatures (T) and densities (ρ) reached extraordinary levels of up to 10^12 K and 10^15 g/cm^3, respectively. These extreme conditions are only currently reproducible in laboratory settings through relativistic heavy-ion collisions. Nevertheless, due to the exceptionally brief timescales involved, such experiments do not allow for direct observation of light element formation. Instead, they yield information about the properties of heavy, luminous material that could be essential for understanding the initial stages of supernova explosions. Conversely, the density trends observed in primordial objects such as white dwarfs and metal-poor stars provide critical constraints on the models that explain the evolution of the universe's chemical composition.",
        "ori-fast-z-score": 1.8842334171937931,
        "water-fast-z-score": 8.818163074019441,
        "rewrite-fast-z-score": 1.5670935878004129
    },
    {
        "original_text": "We report the discovery of an eclipsing binary system HS1857+5144, which is composed of two white dwarfs with masses 0.6 M⊙and 0.8M⊙respectively. The orbital period of this system is 1.5 hours. We find that the mass ratio q = M2/M1=0.84 ±0.04 for this system by using the method of spectral disentangling. This value indicates that it may be in the stage before the final merger into one single degenerate star. In addition to its short orbital period, we also found that the temperature difference between these two components is very large (∼10000K). These results suggest that this system has just evolved out of common envelope phase. \n \n Keywords: Hot subdwarf B stars; Eclipsing; Pre-Cataclysmic Variables; White Dwarfs; Spectral Disentangling",
        "watermark_text": "We report the discovery of an eclipsing binary system HS1857 + 5144 , which is composed of two white dwarfs with masses 0 . 6 [UNK] 0 . [UNK] . The orbital period of this system is 1 . 5 hours .We see that the mass ratio q = M2 / M1 = 0 . 84 ±0 . 04 for this system by using the method of spectral disentangling . This value indicates that it could be in the stage before the last collapse into one single degenerate star .In addition to its short orbital period , we also found that the temperature difference between these two components is very large ( [UNK] ) . These results suggest that this system has just evolved out of common envelope phase .Keywords : Hot subdwarf B stars ; Eclipsing ; Pre - Cataclysmic Variables ; White Dwarfs ; Spectral Disentangling",
        "rewrite_text": "We announce the discovery of the eclipsing binary system HS1857 + 5144, which consists of two white dwarfs with masses of 0.6 M☉ each. This system has an orbital period of 1.5 hours. Through spectral disentangling, we determined that the mass ratio \\( q = M_2 / M_1 = 0.84 \\pm 0.04 \\). This indicates that the system may be in a pre-collapse phase before merging into a single degenerate star. Additionally, we observed a significant temperature difference between the two components. These findings suggest that the system has recently evolved out of the common envelope phase. Keywords: Hot subdwarf B stars; Eclipsing; Pre-Cataclysmic Variables; White Dwarfs; Spectral Disentangling.",
        "ori-fast-z-score": 0.5773502691896258,
        "water-fast-z-score": 1.3127849234810511,
        "rewrite-fast-z-score": 0.31622776601683794
    },
    {
        "original_text": "We present an algorithm for the evaluation of Hankel determinants that is almost as efficient as the product formula, but avoids numerical instability problems associated with it.  The determinant of a matrix A = (aij)n×n can be written in terms of its minors as det(A) = ∏i=1^n∑j=0^ni−1j|aij|. We show how to compute this expression efficiently using O((n log n)(log log n)) arithmetic operations and storage space by combining fast polynomial multiplication algorithms with ideas from number theory.   Our approach relies on the fact that we are able to evaluate all minors simultaneously at any point x ∈  0, 1  using only O((n log n) (log log n))) arithmetic operations and storage space. This allows us to use fast polynomial multiplication techniques to reduce the problem of computing the determinant to one of evaluating polynomials at many points. Finally, we apply recent results from number theory to obtain our final running time bound.",
        "watermark_text": "We create an algorithm for the evaluation of Hankel determinants that is almost as efficient as the product formula , but avoids numerical instability problems related with it . The determinant of a matrix A = ( aij ) n×n can be written in terms of its minors as det ( A ) = [UNK] = 1 ^ [UNK] = 0 ^ ni−1j | aij | .We see how to compute this function easily using O ( ( n log k ) ( log log n ) ) arithmetic operations and storage space by combining fast polynomial multiplication algorithms with ideas from number theory . Our solution relies on the fact that we are able to analyze all minors independently at any point x ∈ 0 , 1 using only O ( ( n log n ) ( log log n ) ) ) arithmetic operations and storage space .This enables us to use fast polynomial multiplication techniques to reduce the question of computing the determinant to one of evaluating polynomials at many points . Finally , we apply latest findings from number theory to obtain our final run time bound .",
        "rewrite_text": "We have developed an algorithm for evaluating Hankel determinants that approaches the efficiency of the product formula while circumventing the numerical instability issues associated with it. The determinant of an n×n matrix A = (a_ij) can be expressed in terms of its minors as det(A) = ∑𝑖=1^𝑛 ∑𝑗=0^(𝑛−1) |a_ij|. Our method simplifies the computation of this function, requiring O((n log k)(log log n)) arithmetic operations and storage space by integrating fast polynomial multiplication techniques with concepts from number theory. This approach is feasible because we can analyze all minors independently at any point x ∈ [0, 1] with just O((n log n)(log log n)) arithmetic operations and storage space. Consequently, we can leverage fast polynomial multiplication to transform the problem of calculating the determinant into evaluating polynomials at multiple points. Finally, we incorporate recent developments in number theory to establish our final runtime bounds.",
        "ori-fast-z-score": 1.6059101370939322,
        "water-fast-z-score": 4.817730411281796,
        "rewrite-fast-z-score": 1.1952286093343936
    },
    {
        "original_text": "We present an analysis of planet occurrence rates in binary systems, using spectroscopic observations to identify binaries among stars observed by Kepler for which we have obtained high-resolution imaging follow-up. We find that planets are less likely to be found around close visual doubles than single stars (<2 AU), but more likely to be found around wide visual doubles or triples (>20 AU). The difference between these two populations is statistically significant at greater than 3-sigma confidence level. These results suggest that stellar multiplicity may play some role in shaping planetary system architectures. However, our sample size is small compared to other studies, so further investigation will be needed before firm conclusions can be drawn. \n \n Keywords: Binary star - Planets - Multiplicity - Kepler - Visual double - Triple star - Planet formation - Circumstellar disk - Star formation - Exoplanet - Circumbinary disk - Double-lined spectroscopic binary",
        "watermark_text": "We present an assessment of planet existence rates in binary systems , using spectroscopic observations to identify binaries among stars observed by Kepler for which we have received high - resolution imaging follow - up . We see that planets are less likely to be found around tight visual doubles than single stars ( < 2 AU ) , but more likely to be found around wide visual twins or triples ( > 20 AU ) .The difference between these two communities is statistically substantial at greater than 3 - sigma confidence rate . These data suggest that stellar multiplicity may play some role in shaping planetary system architectures .However , our sample volume is tiny relative to other studies , so further investigation will be needed before firm conclusions can be drawn . Keywords : Binary star - Planets - Multiplicity - Kepler - Visual double - Triple star - Planet formation - Circumstellar disk - Star formation - Exoplanet - Circumbinary disk - Double - lined spectroscopic binary",
        "rewrite_text": "We provide an evaluation of the rates of planet existence in binary systems by utilizing spectroscopic observations to identify binaries among the stars observed by Kepler, which have undergone high-resolution imaging follow-up. Our findings indicate that planets are less likely to be found around close visual double stars (within 2 AU) compared to single stars, while they are more frequently observed around wide visual binaries or triples (beyond 20 AU). The distinction between these two groups is statistically significant, with a confidence level exceeding 3 sigma. This data implies that stellar multiplicity could influence the architecture of planetary systems. However, our sample size is small compared to other studies, indicating that further research is necessary before reaching definitive conclusions. \n\nKeywords: Binary star, planets, multiplicity, Kepler, visual double, triple star, planet formation, circumstellar disk, star formation, exoplanet, circumbinary disk, double-lined spectroscopic binary.",
        "ori-fast-z-score": 1.3438638879193574,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "In this work, we propose an evolutionary neural gas model for input categorization and clustering in the context of self-organizing networks. The proposed algorithm is based on the concept that each neuron can be considered as a cluster center with its own weight vector. In addition to updating their weights according to the standard NG learning rule, neurons are also allowed to evolve by applying genetic operators such as crossover and mutation. We show through experiments conducted on benchmark data sets that our approach outperforms other state-of-the-art algorithms including SOMs, GNGs, and EBGs. Finally, we demonstrate how the proposed method can be used to solve real-world problems such as text classification and image segmentation. Keywords: Evolutionary Computation, Self-Organizing Networks, Clustering, Genetic Algorithms, Text Classification, Image Segmentation. 1 Introduction Self-organizing maps (SOMs), originally introduced by Kohonen  1  , have been widely applied in many fields ranging from pattern recognition  2  , speech processing  3  , computer vision  4  , bioinformatics  5  , etc.. However, one major drawback of traditional SOMs lies in the fact that they require users to specify the number of clusters beforehand  6  . To overcome this problem, several extensions of SOMs were developed  7, 8  .\nAmong these extensions, growing neural gas (GNG)  9  has attracted much attention due to its ability to automatically determine the optimal number of clusters during training  10  . Nevertheless, it should be noted that most existing models of self organizing network suffer from two main limitations. First, all nodes in the network share the same set of parameters which makes them unable to capture different characteristics of various categories  11  . Second, there lacks any mechanism to prevent overfitting when dealing with high-dimensional data  12  .",
        "watermark_text": "In this research , we propose an evolved brain gas model for input categorization and clustering in the context of self - organizing organizations . The proposed algorithm is based on the idea that each neuron can be regarded as a cluster center with its own weight vector .In addition to updating their weights based to the standard NG learning principle , neurons are also allowed to evolve by using genetic operators such as crossover and mutation . We see through experiments conducted on benchmark data sets that our approach outperforms other state - of - the - art algorithms including SOMs , GNGs , and EBGs .Finally , we prove how the suggested method can be used to solve real - global difficulties such as text classification and image segmentation . Keywords : Evolutionary Computation , Self - Organizing Networks , Clustering , Genetic Algorithms , Text Classification , Image Segmentation .1 Introduction Self - organizing maps ( SOMs ) , previously introduced by Kohonen 1 , have been widely applied in diverse fields ranging from pattern recognition 2 , speech logic 3 , computer vision 4 , bioinformatics 5 , etc . . However , one major drawback of typical SOMs rests in the fact that they allow users to define the number of clusters beforehand 6 .To solve this situation , various extensions of SOMs were developed 7 , 8 . Among these extensions , growing cognitive gas ( GNG ) 9 has garnered considerable scrutiny due to its able to automatically identify the ideal amount of clusters during training 10 .Nevertheless , it should be mentioned that most existing models of self organizing system emerge from two principal constraints . First , all users in the organization share the same list of constraints which makes them unable to capture unique characteristics of several classes 11 .Second , there lacks any mechanism to minimize overfitting when dealing with high - dimensional data 12 .",
        "rewrite_text": "In this study, we introduce an advanced brain gas model designed for input categorization and clustering within self-organizing organizations. Our proposed algorithm conceptualizes each neuron as a cluster center, each possessing its own weight vector. In addition to updating their weights according to the conventional NG learning principle, neurons are permitted to evolve through genetic operators such as crossover and mutation. Experimental results from benchmark datasets demonstrate that our approach surpasses other leading algorithms including Self-Organizing Maps (SOMs), Growing Neural Gas (GNG), and Evolving Brain Gas (EBG). Furthermore, we illustrate how our proposed method can effectively tackle real-world challenges, including text classification and image segmentation. \n\nKeywords: Evolutionary Computation, Self-Organizing Networks, Clustering, Genetic Algorithms, Text Classification, Image Segmentation.\n\n1. Introduction  \nSelf-Organizing Maps (SOMs), first introduced by Kohonen, have found extensive applications across various fields such as pattern recognition, speech processing, computer vision, and bioinformatics. However, a significant limitation of traditional SOMs is that they require users to predefine the number of clusters. To address this issue, several extensions of SOMs have been proposed. Among these, the Growing Neural Gas (GNG) has received considerable attention for its ability to automatically determine the optimal number of clusters during training. Nonetheless, it's important to note that most existing models of self-organizing systems face two main limitations. First, all participants in the organization are subjected to the same set of constraints, hindering the ability to capture the unique characteristics of different classes. Second, there is a lack of mechanisms to prevent overfitting when handling high-dimensional data.",
        "ori-fast-z-score": 0.8838834764831843,
        "water-fast-z-score": 8.824410972276484,
        "rewrite-fast-z-score": -0.2773500981126145
    },
    {
        "original_text": "We present the results on the spatial distribution of galaxies in the vicinity (<5 Mpc) of a bright quasar at redshift 5.2, using deep near-infrared imaging data taken with Subaru/Suprime-Cam. We find that there is an apparent segregation between Lyman break galaxies (LBGs), which are selected by their rest-frame UV colors, and Lyman alpha emitters (LAEs). The LBGs show a clear overdensity toward the quasar position while LAEs do not have such a concentration. This result suggests that the physical conditions for star formation may be different between these two populations. \n \n Keywords: galaxy evolution, quasars, clustering, infrared observations, high-z universe, Lyman break galaxies, Lyman alpha emitters \n \n \n \n 1 Introduction \n \n Quasars play important roles as probes to study the early Universe because they can provide us information about the intergalactic medium through absorption lines observed in their spectra. In addition, quasars themselves emit strong radiation over wide wavelength ranges, so we can use them as background sources to investigate the properties of surrounding objects. For example, it has been suggested that quasars trigger starburst activities in nearby galaxies via intense ultraviolet (UV) radiation and/or gravitational interactions (e.g., Hopkins et al. 2006) . \n \n Recently, several studies have investigated the environments of high-redshift quasars based on multi-wavelength surveys. These include optical/near-infrared spectroscopy (e.g., Adelberger & Steidel 2005; Venemans et al. 2007) , radio continuum emission (e.g., Carilli et al. 2007; Overzier et al. 2008 ) and X-ray emission (e.g,. Brandt et al. 2002; Gilli et al. 2003 ) . However, most previous works focused only on relatively small scales (<1 Mpc) due to limited angular resolution or sensitivity of telescopes used. On larger scales, some authors reported possible evidence for large-scale structures associated with quasars (e.g., Kurk et al. 2000; Pentericci et al",
        "watermark_text": "We present the results on the spatial distribution of stars in the vicinity ( < 5 Mpc ) of a bright quasar at redshift 5 . 2 , using deep near - infrared imaging information taken with Subaru / Suprime - Cam . We see that there is an apparent segregation between Lyman break galaxies ( LBGs ) , which are chosen by their rest - frame UV colors , and Lyman alpha emitters ( LAEs ) .The LBGs see a clear overdensity toward the quasar state while LAEs do not have such a density . This result suggests that the physical conditions for star formation might be different between these two communities .Keywords : universe progression , quasars , clustering , infrared observations , low - z galaxy , Lyman break galaxies , Lyman alpha emitters 1 Introduction Quasars serve useful roles as probes to study the early Universe because they can provide us information about the intergalactic medium through absorption patterns observed in their spectra . In addition , quasars themselves emit strong radiation over broad wavelength ranges , so we can using them as background sources to examine the properties of surrounding objects .For instance , it has been proposed that quasars activate starburst interactions in nearby galaxies via intense ultraviolet ( UV ) rays and / or gravitational interactions ( e . g . , Hopkins et al . 2006 ) .Recently , various surveys have explored the conditions of high - redshift quasars based on multi - wavelength analyses . These include optical / near - infrared spectroscopy ( e . g . , Adelberger & Steidel 2005 ; Venemans et al .2007 ) , radio continuum emission ( e . g . , Carilli et al . 2007 ; Overzier et al .2008 ) and X - ray radiation ( e . g , . Brandt et al .2002 ; Gilli et al . 2003 ) .However , most prior papers focused only on relatively small scales ( < 1 Mpc ) resulting to limited radial resolution or sensitivity of telescopes used . On larger scales , some writers published possible evidence for large - scale structures involved with quasars ( e . g . , Kurk et al .2000; Pentericci et al",
        "rewrite_text": "We report findings related to the spatial distribution of stars surrounding a bright quasar at a redshift of 5.2, utilizing deep near-infrared imaging data obtained from Subaru/Suprime-Cam at distances less than 5 Mpc. Our observations reveal a distinct separation between Lyman break galaxies (LBGs), identified by their rest-frame UV colors, and Lyman alpha emitters (LAEs). Notably, we observe a substantial overdensity of LBGs in the vicinity of the quasar, a trend that is not seen with LAEs. This disparity implies that the conditions for star formation may differ significantly between these two populations.\n\n**Keywords:** universe evolution, quasars, clustering, infrared observations, low-redshift galaxies, Lyman break galaxies, Lyman alpha emitters\n\n**1. Introduction**  \nQuasars play a critical role as probes for studying the early Universe, as they provide insight into the intergalactic medium through absorption features in their spectra. Additionally, their powerful emissions across a broad wavelength spectrum enable them to serve as background sources for examining the properties of surrounding astronomical objects. It has been suggested that the intense ultraviolet (UV) radiation emitted by quasars may stimulate starburst activity in nearby galaxies, possibly through gravitational interactions (e.g., Hopkins et al. 2006). Recently, numerous studies have investigated the environment of high-redshift quasars using multi-wavelength approaches. These studies encompass various methods, including optical and near-infrared spectroscopy (e.g., Adelberger & Steidel 2005; Venemans et al. 2007), radio continuum observations (e.g., Carilli et al. 2007; Overzier et al. 2008), and X-ray analysis (e.g., Brandt et al. 2002; Gilli et al. 2003). However, previous research primarily concentrated on relatively small scales (less than 1 Mpc), which limited both the radial resolution and the sensitivity of the telescopes employed. On larger scales, some researchers have suggested the existence of large-scale structures associated with quasars (e.g., Kurk et al. 2000; Pentericci et al.).",
        "ori-fast-z-score": 0.50709255283711,
        "water-fast-z-score": 6.6530001764255,
        "rewrite-fast-z-score": 0.8951673046482753
    },
    {
        "original_text": "We present new observations of the outflows driven by massive protostars using the Submillimeter Array (SMA) at 1.3 mm, which are compared with previous results obtained with single-dish telescopes. We find that the SMA data reveal more compact structures than those seen previously; this is likely due to missing flux and/or resolution effects. The total mass loss rates inferred for these sources range between 10^-4 and 10^-3 Msun/yr, while their momentum flux ranges between 10^-2 and 10^1 Lsun/c/s. These values are similar to those found for low-mass Class 0 objects but higher than expected if scaled up according to the luminosity-to-mass ratio. This suggests that there may be additional mechanisms driving the outflows besides radiation pressure on dust grains. \n \n In addition we report the detection of infalling gas toward two of our targets. For G35.20-1.74NW, we detect an inward motion of ~0.5 km/s over a distance of ~1000 AU. For IRAS 18162-2048, we see evidence for both inward motions as well as outward motions along different lines-of-sight.",
        "watermark_text": "We present new images of the outflows driven by massive protostars utilizing the Submillimeter Array ( SMA ) at 1 . 3 millimetres , which are compared with previous findings obtained with single - dish telescopes . We see that the SMA data reveal more compact systems than those shown previously ; this is probably due to missing flux and / or resolution influences .The total mass loss rates inferred for these sources range between 10 ^ - 4 and 10 ^ - 3 Msun / yr , while their momentum flux ranges between 10 ^ - 2 and 10 ^ 1 Lsun / c / s . These figures are comparable to those observed for low - mass Class 0 bodies but higher than expected if scaled up according to the luminosity - to - mass ratio .This implies that there may be additional mechanisms pushing the outflows besides radiation stress on dust grains . In addition we monitor the observation of infalling gas toward two of our objectives .For G35 . 20 - 1 . 74NW , we perceive an inward movement of ~ 0 . 5 kilometres / s over a distance of ~ 1000 AU . For IRAS 18162 - 2048 , we find proof for both eastward movements as well as outward movements along various lines - of - view .",
        "rewrite_text": "We present new images of outflows driven by massive protostars, captured using the Submillimeter Array (SMA) at a wavelength of 1.3 millimeters, and compare these findings with previous results obtained from single-dish telescopes. The SMA data indicates more compact systems than those previously reported, likely due to factors such as missing flux and resolution limitations. The inferred total mass loss rates for these sources range from 10^(-4) to 10^(-3) M☉ per year, while their momentum flux varies between 10^(-2) and 10^1 L☉/c/s. These values are comparable to those observed in low-mass Class 0 objects but are higher than what would be anticipated if scaled according to the luminosity-to-mass ratio. This suggests that there may be additional mechanisms driving the outflows beyond radiation pressure on dust grains. Additionally, we monitor the infall of gas toward two of our targets. For G35.20-1.74NW, we observe an inward velocity of approximately 0.5 kilometers per second over a distance of about 1000 AU. In the case of IRAS 18162-2048, we find evidence for both eastward and outward motions along different lines of sight.",
        "ori-fast-z-score": -0.9058216273156765,
        "water-fast-z-score": 5.4349297638940595,
        "rewrite-fast-z-score": 1.3251783128981585
    },
    {
        "original_text": "We study satellites in simulated galaxies with different masses at z = 0 using high-resolution cosmological hydrodynamic simulations (the Millennium Run). We find that there is no significant difference between the number density profiles of satellite galaxies around central galaxies with different luminosities or halo masses. The radial distribution of satellites shows an excess over the predictions based on the subhalo abundance matching technique for r < 30 kpc/h. This excess can be explained by tidal stripping of satellites before they are accreted onto the main galaxy. In addition, we show that the fraction of surviving satellites decreases rapidly as a function of distance from the center of host halos. Finally, we investigate how the properties of satellites depend on those of their hosts. Our results suggest that the majority of faint satellites may have been destroyed through mergers and/or tidal disruption during infall into larger systems. These findings provide important constraints on models of galaxy formation.",
        "watermark_text": "We research satellites in simulated galaxies with various masses at z = 0 using high - resolution cosmological hydrodynamic simulations ( the Millennium Run ) . We see that there is no major variation between the number density characteristics of satellite galaxies around central planets with various luminosities or halo masses .The radial distribution of satellites displays an excess over the estimates based on the subhalo abundance matching methodology for r < 30 kpc / h . This excess can be described by tidal stripping of satellites before they are accreted onto the main galaxy .In addition , we find that the fraction of surviving satellites decreases quickly as a function of distance from the center of host halos . Finally , we investigate how the properties of satellites depend on those of their hosts .Our results show that the majority of distant satellites would have been destroyed through mergers and / or tidal disruption during infall into larger systems . These conclusions provide important restrictions on estimates of galaxy formation .",
        "rewrite_text": "We investigate satellite galaxies in simulated galaxies with varying masses at redshift z = 0 using high-resolution cosmological hydrodynamic simulations from the Millennium Run. Our findings indicate that there is minimal variation in the number density characteristics of satellite galaxies surrounding central galaxies of different luminosities or halo masses. The radial distribution of satellites reveals an excess compared to estimates derived from the subhalo abundance matching methodology for distances less than 30 kpc/h. This excess can be attributed to the tidal stripping of satellites prior to their accretion onto the main galaxy. Furthermore, we observe that the fraction of surviving satellites significantly declines with increasing distance from the centers of host halos. Lastly, we analyze how the properties of satellites correlate with those of their host galaxies. Our results suggest that most distant satellites would have been lost due to mergers and/or tidal disruptions during their infall into larger systems. These findings impose crucial constraints on estimates related to galaxy formation.",
        "ori-fast-z-score": 0.6974858324629157,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 2.1652509527331207
    },
    {
        "original_text": "We report on an analysis of archival Chandra data for the central region of the nearby starburst galaxy M82 (NGC 3034). We find that there are two bright, point-like sources in this field which have been previously identified as ULXs (Ultra-Luminous X-Ray Sources) by Swartz et al. (2004) . The first source is located at RA = 12 h 54 m 55 s .6 and Dec = 69°59 45   with a count rate of 1.1 x 10^-3 counts sec-1. This source has a luminosity of 2 x 10^39 erg/sec assuming it lies at 8 kpc distance. The second source is located at RA=12h54m55s.7 and Dec=69°59 46   with a count rate 0.9 x 10^-3 countssec-1. It also has a luminosity of about 2 x 10^39erg/sec if it lies at 8kpc. Both these sources appear to be variable over timescales ranging between hours and days.  These results suggest that both sources may contain black holes accreting close to their Eddington limit.",
        "watermark_text": "We report on an assessment of archival Chandra data for the central region of the nearby starburst galaxy M82 ( NGC 3034 ) . We see that there are two faint , point - like sources in this field which have been previously described as ULXs ( Ultra - Luminous X - Ray Sources ) by Swartz et al .( 2004 ) . The first reference is found at RA = 12 h 54 m 55 s . 6 and Dec = 69°59 45 with a count rate of 1 . 1 x 10 ^ - 3 counts sec - 1 .This source has a luminosity of 2 x 10 ^ 39 erg / sec assuming it lies at 8 kpc distance . The second source is situated at RA = 12h54m55s . 7 and Dec = 69°59 46 with a count rate 0 . 9 x 10 ^ - 3 countssec - 1 .It additionally has a luminosity of about 2 x 10 ^ 39erg / sec if it lies at 8kpc . Both these sources appear to be varying over timescales ranging between hours and days .These data suggest that both sources might represent dark holes accreting close to their Eddington limit .",
        "rewrite_text": "We present an analysis of archival Chandra data from the central region of the nearby starburst galaxy M82 (NGC 3034). Our study identifies two faint point-like sources in this field, previously classified as ULXs (Ultra-Luminous X-Ray Sources) by Swartz et al. (2004). The first source is located at RA = 12h 54m 55s.6 and Dec = 69° 59' 45\", exhibiting a count rate of 1.1 x 10^-3 counts sec^-1, resulting in a luminosity of approximately 2 x 10^39 erg/sec, assuming a distance of 8 kpc. The second source is found at RA = 12h 54m 55s.7 and Dec = 69° 59' 46\", with a count rate of 0.9 x 10^-3 counts sec^-1 and a similar luminosity of about 2 x 10^39 erg/sec at the same distance. Both sources show variability on timescales of hours to days, suggesting that they may be black holes accreting matter near their Eddington limit.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.328201177351375,
        "rewrite-fast-z-score": 0.42857142857142855
    },
    {
        "original_text": "We report on the observation and characterization of two-dimensional defect modes in optically-induced photonic crystals (OIPCs). The OIPC is formed by periodic modulation of refractive index using femtosecond laser pulses focused into fused silica glass. We show that the defect mode can be tuned over a wide range of wavelengths, which are determined by the periodicity of the lattice structure as well as the size of the defects. This work opens up new possibilities for designing optical devices based on these structures. \n \n Photonic crystal slabs have attracted considerable attention recently because they provide an excellent platform to study light-matter interactions at the nanoscale  1  . In particular, it has been shown that three-dimensional photonic crystals with point or line defects exhibit localized states within their bandgap  2  , leading to many interesting applications such as lasers  3  , filters  4  , sensors  5  , nonlinear optics  6  , etc.. However, fabrication of three-dimensional photonic crystals requires sophisticated techniques  7, 8  , making them difficult to integrate with other micro/nano-structures. Recently, several groups have demonstrated two-dimensional photonic crystals  9  -  11  fabricated directly inside transparent materials via direct laser writing  12  -  14  . These 2D photonic crystals offer advantages including ease of fabrication, flexibility in design, and compatibility with existing technologies  15  .\nIn this Letter we demonstrate the formation of defect modes in opticallyinduced photonic crystals (OPC)  16  . The OPC consists of periodically modulated refractive index created by focusing femtosecond laser pulses into fused silica glass  17  . By introducing defects into the lattice structure, we observe localized defect modes within the stopband of the OPC. Furthermore, we show that the defect mode wavelength can be continuously tuned across the entire stopband simply by changing the lattice spacing and/or the size of the defects. \nThe experimental setup used to create the OPC is illustrated schematically in Fig. 1(a) . A Ti:Sapphire regenerative amplifier system operating at 800 nm was employed to generate 100 fs duration pulses at a repetition rate of 1 kHz. The beam diameter after passing through a spatial filter",
        "watermark_text": "We report on the observation and identification of two - dimensional defect modes in optically - mediated photonic crystals ( OIPCs ) . The OIPC is formed by periodic modulation of refractive index utilizing femtosecond laser pulses focused into fused silica glass .We see that the defect mode can be tuned over a broad variety of wavelengths , which are chosen by the periodicity of the lattice structure as also as the height of the defects . This research raises up new possibilities for constructing optical devices based on these structures .Photonic crystal slabs have garnered considerable interest lately because they give an excellent platform to study light - matter processes at the nanoscale 1 . In particular , it has been shown that three - dimensional photonic materials with point or line defects exhibit localized states within their bandgap 2 , leading to many interesting applications such as lasers 3 , filters 4 , devices 5 , nonlinear optics 6 , etc . .However , fabrication of three - dimensional photonic particles needs specialized techniques 7 , 8 , making them harder to integrate with other micro / nano - materials . Recently , various groups have demonstrated two - dimensional photonic particles 9 - 11 fabricated fully inside transparent materials via continuous optical writing 12 - 14 .These 2D photonic materials provide advantages including ease of fabrication , simplicity in design , and compatibility with existing devices 15 . In this Letter we prove the formation of defect modes in opticallyinduced photonic materials ( OPC ) 16 .The OPC composed of regularly modulated refractive index created by concentrating femtosecond laser pulses into fused silica glass 17 . By introducing defects into the lattice structure , we perceive localized failure modes within the stopband of the OPC .Furthermore , we find that the failure mode wavelength can be continuously tuned across the entire stopband solely by varying the crystal spacing and / or the size of the defects . The empirical setup used to create the OPC is depicted schematically in Fig .1 ( a ) . A Ti : Sapphire regenerative amplifier system operating at 800 nm was employed to produce 100 fs duration pulses at a repetition rate of 1 kHz .The beam diameter after passing through a spatial filter",
        "rewrite_text": "We report on the observation and identification of two-dimensional defect modes in optically-induced photonic crystals (OIPCs). These OIPCs are created by periodically modulating the refractive index through the use of femtosecond laser pulses focused into fused silica glass. Our findings indicate that the defect modes can be finely tuned across a wide range of wavelengths, which are determined by the lattice periodicity and the height of the defects. This research opens up new avenues for developing optical devices based on these structures. Recently, photonic crystal slabs have gained significant attention as they offer an excellent platform for exploring light-matter interactions at the nanoscale. Notably, it has been demonstrated that three-dimensional photonic materials featuring point or line defects can exhibit localized states within their bandgap, leading to various applications, including lasers, filters, devices, and nonlinear optics. However, the fabrication of three-dimensional photonic structures requires specialized techniques, making their integration with other micro/nano-materials more complex. In contrast, various groups have recently succeeded in creating two-dimensional photonic particles entirely within transparent materials through continuous optical writing. These 2D photonic materials offer several advantages, including easier fabrication, simpler design, and better compatibility with existing devices. In this letter, we demonstrate the formation of defect modes in optically-induced photonic materials (OPC), which consist of a regularly modulated refractive index produced by directing femtosecond laser pulses into fused silica glass. By introducing defects into the lattice, we observe localized defect modes within the stopband of the OPC. Furthermore, we establish that the wavelength of these defect modes can be continuously tuned across the entire stopband by adjusting the crystal spacing and/or the size of the defects. A schematic representation of the empirical setup used to create the OPC is shown in Fig. 1(a). A Ti:Sapphire regenerative amplifier system operating at 800 nm was utilized to generate 100 fs duration pulses at a repetition rate of 1 kHz, with the beam diameter adjusted after passing through a spatial filter.",
        "ori-fast-z-score": -0.3223291856101521,
        "water-fast-z-score": 7.526023228839096,
        "rewrite-fast-z-score": 1.272891654681168
    },
    {
        "original_text": "We present an analytic solution to the steady state distribution for the mechanistic home-range model developed by Moorcroft et al. (2006) that allows for efficient computation of home ranges using numerical integration methods. The new method is implemented as part of the R package adehabitatHR, which also includes functions for computing home ranges with the original algorithm (i.e., without the analytical solution). We demonstrate how our approach can be used to rapidly compute home ranges across large landscapes containing thousands of habitat patches. Our results show that the new method produces identical estimates compared to those obtained with the original algorithm but requires less computational time when estimating home ranges over large spatial extents. Analytical solutions are useful because they allow researchers to efficiently estimate home ranges on very large datasets or at fine resolutions. \n \n Home ranges have been widely studied since their introduction into ecology more than 50 years ago  1  . These areas represent the area within which individuals obtain all necessary resources  2  , such as food  3  , water  4  , shelter  5  , mates  6  , and cover  7  . In addition to being important for understanding animal behavior  8  , home ranges play key roles in conservation biology  9  , wildlife management  10  , epidemiology  11  , and disease transmission  12  .\n \nHome-range models typically assume that animals move through a landscape composed of discrete habitat patches  13  . Animals select among these patches based on some combination of patch attributes  14  , including resource availability  15  , vegetation structure  16  , predation risk  17  , and conspecific density  18  . This process continues until the animal reaches equilibrium between its movement rate and the quality of available habitats  19  . \n \n A number of different approaches exist for modeling animal movements  20  . One popular class of models uses random-walk theory  21  to describe animal movements  22  . Random walk models assume that animals make independent decisions about where to go next  23  . However, this assumption may not always hold true  24  . For example, if two neighboring patches contain similar levels of resources  25  , then it would be unlikely for an animal to switch back-and-forth between them  26  . To account for this type of behavioral response, Moorcro",
        "watermark_text": "We present an analytic solution to the stable state distribution for the mechanistic home - range system established by Moorcroft et al . ( 2006 ) that enables for efficient computation of home ranges using numerical integration methods .The new method is implemented as part of the R program adehabitatHR , which also contains functions for modeling home ranges with the previous algorithm ( i . e . , without the empirical approach ) . We suggest how our approach can be used to rapidly compute bedroom ranges across large landscapes containing thousands of environment patches .Our results show that the new method generates similar estimates compared to those achieved with the previous algorithm but requires fewer computational time when estimating bedroom ranges over large geographic extents . Analytical approaches are helpful because they allow scientists to easily assess home ranges on very huge datasets or at fine resolutions .Home ranges have been widely examined since their arrival into ecosystems more than 50 weeks ago 1 . These zones represent the territory within which adults obtain all necessary resources 2 , such as feed 3 , water 4 , protection 5 , mates 6 , and cover 7 .In addition to being important for explaining animal behavior 8 , home ranges represent crucial roles in wildlife dynamics 9 , conservation conservation 10 , epidemiology 11 , and illness transmission 12 . Home - range systems often assume that animals go through a landscape composed of linear habitat patches 13 .Animals select among these patches based on some mix of patch traits 14 , covering habitat availability 15 , vegetation system 16 , predation risk 17 , and conspecific density 18 . This process proceeds until the organism reaches stability between its movement rate and the performance of available environments 19 .A variety of different methods exist for modeling animal activities 20 . One popular family of models using random - walk models 21 to explain animal activities 22 .Random step models believe that individuals give independent choice about where to going next 23 . However , this assumption must not always hold false 24 .For instance , if two adjacent regions contain similar rates of assets 25 , then it would be impossible for an organism to shift back - and - forth between them 26 . To account for this form of behavioral reaction , Moorcro",
        "rewrite_text": "We introduce an analytical solution for the stable state distribution in the mechanistic home-range system developed by Moorcroft et al. (2006). This solution facilitates the efficient computation of home ranges through numerical integration methods. Our new method is integrated into the R package adehabitatHR, which also includes functions for modeling home ranges using the previous algorithm (i.e., without the empirical approach). We demonstrate how this approach can be utilized to quickly calculate home ranges across extensive landscapes featuring thousands of environmental patches. Our findings indicate that the new method produces estimates comparable to those generated by the previous algorithm but does so with significantly reduced computational time when estimating home ranges over large geographic areas. Analytical methods are advantageous as they enable researchers to assess home ranges on vast datasets or at high resolutions. Home ranges have been extensively studied since their introduction to ecological research over 50 years ago. These areas represent the territories where adult animals acquire essential resources, such as food, water, shelter, mates, and protective cover. Furthermore, home ranges are critical for understanding various aspects of animal behavior, wildlife dynamics, conservation efforts, epidemiology, and disease transmission. Home-range models typically assume that animals navigate through a landscape made up of linear habitat patches. Animals make selections among these patches based on a variety of traits, including habitat availability, vegetation type, predation risk, and population density. This selection process continues until the animal achieves a balance between its movement rate and the performance of the available environments. Various methods exist for modeling animal behavior, with one widely-used approach employing random-walk models to explain these activities. Random step models operate under the assumption that individuals make independent choices about their next location. However, this assumption may not always be valid. For example, if two adjacent regions offer similar resource levels, it may be implausible for an organism to move back and forth between them. To address this behavioral response, Moorcroft...",
        "ori-fast-z-score": -1.7230995806825715,
        "water-fast-z-score": 10.625780747542525,
        "rewrite-fast-z-score": 1.8763883748662837
    },
    {
        "original_text": "Reverberation is an important feature of brain function, but its role remains unclear. We show that reverberation can be generated by the interplay between calcium signaling and short-term plasticity (STP) at excitatory synapses. In our model, STP leads to bursts of spikes which are followed by periods of low firing rate due to depletion of neurotransmitter vesicles. The resulting slow recovery of transmitter release causes a build-up of residual calcium concentration  Ca  res , leading to facilitation of glutamate release during subsequent bursts. This positive feedback loop generates sustained reverberatory activity with multiple time scales. Our results suggest that reverberation may play a key role in information processing within neural circuits. Reverberation is one of the most prominent features of brain function  1  . It has been observed across different species  2  -  4  as well as in various brain areas including hippocampus  5  , neocortex  6  , thalamus  7  , striatum  8  , cerebellar cortex  9  , olfactory bulb  10  , and retina  11  .\nDespite its ubiquity, however, it still remains unknown what functional roles reverberation plays in the brain  12  . One possibility is that reverberation serves as a mechanism for memory storage  13  or retrieval  14  . Another hypothesis suggests that reverberation could serve as a substrate for working memory  15  . Yet another idea is that reverberation might provide a means for temporal coding  16  . Finally, some studies have suggested that reverberation may simply reflect ongoing spontaneous activity  17  .",
        "watermark_text": "Reverberation is an important feature of cerebral activity , but its significance remains unsure . We suggest that reverberation can be triggered by the interplay between calcium signaling and short - term plasticity ( STP ) at excitatory synapses .In our model , STP results to bursts of peaks which are preceded by periods of poor fired rate due to depletion of neurotransmitter vesicles . The resulting slow withdrawal of transmitter release forms a build - up of residual calcium concentration Ca res , leading to facilitation of glutamate production during later bursts .This positive feedback network generates sustained reverberatory behavior with various time ranges . Our results propose that reverberation may play a key importance in information processing within neural systems .Reverberation is one of the most notable features of cerebral function 1 . It has been observed across different species 2 - 4 as well as in different brain regions including hippocampus 5 , neocortex 6 , thalamus 7 , striatum 8 , cerebellar cortex 9 , olfactory bulb 10 , and retina 11 .Despite its ubiquity , however , it still remains obscure what functional functions reverberation plays in the brain 12 . One possibility is that reverberation acts as a system for memory processing 13 or retrieval 14 .Another hypothesis suggests that reverberation possibly provide as a substrate for working brain 15 . Yet another idea is that reverberation would offer a means for temporal coding 16 .Finally , some researchers have suggested that reverberation possibly simply reflect ongoing spontaneous activity 17 .",
        "rewrite_text": "Reverberation is a significant aspect of brain activity, though its exact role is still uncertain. We propose that reverberation may be initiated by the interaction of calcium signaling and short-term plasticity (STP) at excitatory synapses. In our model, STP leads to bursts of activity characterized by peaks that follow periods of low firing rates due to the depletion of neurotransmitter vesicles. This gradual reduction in transmitter release results in an accumulation of residual calcium concentration (Ca res), which enhances glutamate production during subsequent bursts. This positive feedback mechanism produces sustained reverberatory behavior across various time scales. Our findings suggest that reverberation could be crucial for information processing in neural systems. It is one of the most prominent features of cerebral function and has been observed in various species and brain regions, including the hippocampus, neocortex, thalamus, striatum, cerebellar cortex, olfactory bulb, and retina. Despite its widespread presence, the specific functional roles of reverberation in the brain remain unclear. One possibility is that it serves as a mechanism for memory processing or retrieval. Another hypothesis is that reverberation provides a foundation for working memory. Additionally, it might facilitate temporal coding, while some researchers suggest it may merely reflect ongoing spontaneous activity.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.514430561703706,
        "rewrite-fast-z-score": -0.30151134457776363
    },
    {
        "original_text": "We present the first results for atmospheric dynamics in short period extra-solar gas giant planets (EGPs) using 3D general circulation models with radiative transfer and realistic opacities. We find that the night-side temperature is strongly dependent on opacity, which determines how much energy can be transported to space by radiation. The day-night contrast increases as we decrease the opacity because less heat escapes through the nightside atmosphere. This effect is more pronounced at lower pressures where convection becomes inefficient. For low enough opacities, the planet cools down completely during its orbit resulting in an extremely cold night side. Our simulations show that EGPs are likely to have very different climates depending on their composition.  Keywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast",
        "watermark_text": "We present the first findings for atmospheric mechanics in small history additional - solar gas giant galaxies ( EGPs ) using 3D general circulation estimates with radiative transfer and realistic opacities . We see that the night - side temperature is strongly dependent on opacity , which determines how many heat can be transported to space by radiation .The day - night difference rises as we decrease the opacity because lighter thermal exits through the nightside environment . This phenomenon is more pronounced at lower pressures where circulation becomes inefficient .For low enough opacities , the planet cools down fully during its orbit producing in an incredibly cold evening side . Our simulations see that EGPs are likely to have very different climates based on their composition .Keywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast",
        "rewrite_text": "We present the initial findings regarding atmospheric mechanics in small, history-rich solar gas giant galaxies (EGPs) through 3D general circulation models incorporating radiative transfer and realistic opacities. Our results indicate that the temperature on the night side is highly influenced by opacity, which affects the amount of heat lost to space via radiation. As we reduce opacity, the difference in temperature between day and night increases, allowing more thermal energy to escape from the night side. This effect is especially significant at lower pressures, where circulation becomes less effective. At sufficiently low opacities, the planet experiences significant cooling over its orbit, leading to an exceptionally cold night side. Our simulations suggest that EGPs may exhibit a wide range of climates depending on their composition. \n\n**Keywords:** General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": 0.23570226039551587
    },
    {
        "original_text": "We present an analysis of Lyman alpha emitters (LAEs) selected by narrowband imaging with Subaru/Suprime-Cam and spectroscopic follow-up observations using VLT/VIMOS at z ~ 3.1, which is one of the most active epochs for galaxy formation. We find that LAEs are distributed over a wide range of environments; they exist both in isolated regions as well as in dense clusters. The clustering properties of LAEs depend on their luminosities. In particular, we found that bright LAEs show stronger clustering than faint ones do. This result suggests that bright LAEs may be more evolved systems compared to fainter ones. Furthermore, we investigated the dependence of clustering strength on the equivalent widths of Lyman-alpha emission lines. Our results suggest that strong clustering objects tend to have higher equivalent widths. These findings imply that there exists some evolutionary link between LAEs and LBGs. \n \n Keywords: Lyman alpha emitter",
        "watermark_text": "We present an assessment of Lyman alpha emitters ( LAEs ) selected by narrowband scanning with Subaru / Suprime - Cam and spectroscopic follow - up observations using VLT / VIMOS at z ~ 3 . 1 , which is one of the most stable epochs for galaxy formation . We see that LAEs are distributed over a broad variety of habitats ; they exist both in isolated regions as well as in dense clusters .The clustering qualities of LAEs depend on their luminosities . In particular , we identified that bright LAEs see better clustering than dim ones do .This result suggests that bright LAEs may be more evolved structures compared to fainter ones . Furthermore , we investigated the dependence of clustering strength on the equivalent widths of Lyman - alpha emission lines .Our results show that strong clustering objects prefer to have greater equivalent widths . These conclusions conclude that there exists some evolutionary link between LAEs and LBGs .Keywords: Lyman alpha emitter",
        "rewrite_text": "We present an evaluation of Lyman alpha emitters (LAEs) identified through narrowband scanning with Subaru/Suprime-Cam, followed by spectroscopic observations using VLT/VIMOS at redshift z ~ 3.1, a key period for galaxy formation. Our findings indicate that LAEs inhabit a diverse range of environments, being found in both isolated areas and dense clusters. The clustering properties of LAEs vary according to their luminosities, with brighter LAEs exhibiting stronger clustering than their dimmer counterparts. This observation suggests that brighter LAEs may represent more evolved structures. Additionally, we explored the relationship between clustering strength and the equivalent widths of Lyman-alpha emission lines, discovering that strongly clustered objects tend to have larger equivalent widths. These results imply an evolutionary connection between LAEs and Lyman Break Galaxies (LBGs). Keywords: Lyman alpha emitter.",
        "ori-fast-z-score": 0.5163977794943222,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "We report on the room-temperature ferromagnetism in Mn-doped ZnO thin films grown by pulsed laser deposition (PLD). The Curie temperatures are found to be around 300 K for all samples with different doping levels, which is much higher than that reported previously. We also find that the magnetization increases linearly as the applied field decreases and shows hysteresis loops at low fields. These results indicate that the observed ferromagnetic behavior may originate from exchange coupling between localized spins rather than intrinsic ferromagnetism. \n \n In recent years, there has been growing interest in developing new materials for spintronic applications such as nonvolatile memory devices or logic circuits based on the manipulation of electron spins instead of charge carriers1-5 . Among these materials, diluted magnetic semiconductors have attracted considerable attention because they can combine both electronic and magnetic functionalities into one material6-8 .\n \n\n\nZnO-based DMSs have been extensively studied due to their wide band gap energy (3.37 eV), large exciton binding energy (60 meV)9 , high transparency10-12 , and good chemical stability13-15 . However, it remains challenging to achieve room-temperature ferromagnetically ordered states in ZnO-based DMSs16-18 . Although several groups have recently demonstrated room-temperature ferromagnetic ordering in various types of ZnO-based DMS systems19-24 , most of them show relatively small saturation magnetizations25-27 . \n \n Here we report on the observation of room-temperature ferromagnetisms in Mn-doped ZnObased DMSs prepared using pulsed laser deposition28-30 . Our experimental data clearly demonstrate that the dopant concentration plays an important role in determining the Curie temperature31-33 . For example, our sample with x = 0.5% exhibits a Curie temperature of about 300 K while those with lower concentrations exhibit smaller values ranging from 150-250 K34-36 . Moreover, we observe that the magnetization increases almost linearly when decreasing the external magnetic field below 1 T and displays hysteretic behaviors at very low fields. This indicates that the observed ferr",
        "watermark_text": "We report on the room - temperature ferromagnetism in Mn - doped ZnO thin sheets grown by pulsed laser precipitation ( PLD ) . The Curie temperatures are found to be around 300 K for all specimens with various doping rates , which is much higher than that described earlier .We additionally find that the magnetization increases linearly as the applied field decreases and shows hysteresis loops at low areas . These conclusions show that the seen ferromagnetic activity may originate from exchange interactions between localized spins rather than intrinsic ferromagnetism .In past times , there has been growing interest in building new materials for spintronic use such as nonvolatile memory devices or logic devices using on the manipulation of electron spins rather of charge carriers1 - 5 . Among these materials , diluted magnetic semiconductors have garnered considerable scrutiny because they can mix both electronic and magnetic functionalities into one material6 - 8 .ZnO - based DMSs have been heavily explored thanks to their wide band gap energy ( 3 . 37 eV ) , large exciton activation energy ( 60 meV ) 9 , large transparency10 - 12 , and good molecular stability13 - 15 . However , it remains challenging to achieve room - temperature ferromagnetically ordered states in ZnO - based DMSs16 - 18 .Although several groups have recently shown room - temperature ferromagnetic sorting in different kinds of ZnO - based DMS systems19 - 24 , most of them show relatively small saturation magnetizations25 - 27 . Here we publish on the observation of room - temperature ferromagnetisms in Mn - doped ZnObased DMSs assembled utilizing laser optical deposition28 - 30 .Our experimental records distinctly show that the dopant concentration plays an important role in distinguishing the Curie temperature31 - 33 . For instance , our sample with x = 0 . 5 % experiences a Curie temperature of about 300 K while those with higher concentrations display lesser values ranging from 150 - 250 K34 - 36 .Moreover , we determine that the magnetization increases almost linearly when decreasing the external magnetic force below 1 T and displays hysteretic behaviors at very low areas . This implies that the observed ferr",
        "rewrite_text": "We present our findings on room-temperature ferromagnetism in manganese-doped zinc oxide (ZnO) thin films created through pulsed laser deposition (PLD). All samples, regardless of the doping levels, exhibit Curie temperatures around 300 K, significantly higher than previously reported. Moreover, we observe that the magnetization increases linearly as the external magnetic field decreases, with the presence of hysteresis loops at low fields. These observations suggest that the ferromagnetic behavior is likely a result of exchange interactions between localized spins rather than intrinsic ferromagnetism. Recent interest has surged in developing new materials for spintronic applications, such as nonvolatile memory and logic devices that utilize electron spin manipulation instead of charge carriers. Diluted magnetic semiconductors (DMS) are particularly appealing since they integrate both electronic and magnetic properties into a single material. ZnO-based DMS have been extensively studied due to their wide band gap (3.37 eV), considerable exciton activation energy (60 meV), high transparency, and excellent thermal stability. Nonetheless, achieving ferromagnetically ordered states at room temperature in ZnO-based DMS remains a significant challenge. Although several research teams have recently demonstrated room-temperature ferromagnetism in various ZnO-based DMS systems, many have reported relatively low saturation magnetizations. In this study, we report the detection of room-temperature ferromagnetism in Mn-doped ZnO DMSs fabricated using laser optical deposition. Our experimental results clearly indicate that dopant concentration plays a crucial role in determining the Curie temperature. For example, our sample with 0.5% doping shows a Curie temperature of approximately 300 K, whereas samples with higher concentrations exhibit lower Curie temperatures ranging from 150 to 250 K. Additionally, we find that magnetization increases nearly linearly as the external magnetic field decreases below 1 T, along with hysteretic behavior at low fields. This suggests that the observed ferromagnetism results from exchange interactions among localized spins.",
        "ori-fast-z-score": 0.16666666666666666,
        "water-fast-z-score": 8.833333333333334,
        "rewrite-fast-z-score": 0.16012815380508713
    },
    {
        "original_text": "We present the light-cone distribution amplitudes (DAs) for axial vector mesons in terms of their helicity components, which are determined by solving the Bethe-Salpeter equation with an instantaneous interaction kernel and applying the method developed recently to calculate DAs.  We find that the twist-2 DA is dominated by its first Gegenbauer moment, while higher moments contribute significantly only at large momentum fractions x > 0.7. The twist-3 DA has two independent functions, one of them being proportional to the second Gegenbauer moment. Our results show that the twist-4 contribution is negligible compared to those of lower twists. These findings will be useful for studying exclusive processes involving axial vector mesons such as B-decays into charmonium plus photon or pion pair. \nI. INTRODUCTIO N\nThe study of hadronic structure plays an important role in understanding strong interactions between quarks and gluons inside hadrons. In particular, the investigation on the parton distributions provides us valuable information about how quarks and gluon are distributed within hadrons  1  . Recently, there have been great interests in exploring the internal structures of hadrons beyond the leading-twist level  2  , especially the transverse-momentum dependent parton distributions  3  .\nIn this work we focus our attention on another type of nonperturbative objects -the light-cone distribution amplitudes(DAs). They describe the probability amplitude of finding a quark-antiquark pair with certain longitudinal momentum fraction and transverse separation at some fixed light-like distance  4  . It was shown that they play crucial roles in describing various hard exclusive reactions  5  . For example, the decay constants fBπ and fBs can be expressed in terms of the lowest-order DAs  6  ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the lowest-and next-to-lowest order DAs  7, 8  . Furthermore, it was found that the heavy-to-light transition form factor FV(q 2 ) of B→V transitions depends",
        "watermark_text": "We introduce the light - cone distribution amplitudes ( DAs ) for axial vector mesons in terms of their helicity components , which are decided by solving the Bethe - Salpeter equation with an instantaneous interaction kernel and using the method developed lately to estimate DAs . We see that the twist - 2 DA is dominated by its initial Gegenbauer moment , while greater moments contribute considerably only at large velocity fractions x > 0 . 7 .The twist - 3 DA has two independent functions , one of them being equal to the second Gegenbauer moment . Our results show that the twist - 4 contribution is negligible compared to those of lower bends .These studies will be valuable for studying exclusive mechanisms using axial vector mesons such as B - decays into charmonium plus photon or pion pair . I . INTRODUCTIO N The investigation of hadronic structure serves an important role in understanding strong interactions between quarks and gluons inside hadrons .In particular , the investigation on the parton distributions offers us valuable info about how quarks and gluon are distributed within hadrons 1 . Recently , there have been big efforts in investigating the internal structures of hadrons beyond the led - twist level 2 , particularly the transverse - momentum dependent parton distributions 3 .In this research we focus our focus on another type of nonperturbative objects - the light - cone distribution amplitudes ( DAs ) . They define the probability amplitude of finding a quark - antiquark pair with certain horizontal momentum fraction and longitudinal separation at some fixed light - like distance 4 .It was shown that they serve vital part in understanding various hard exclusive reactions 5 . For instance , the decay constants fBπ and fBs can be stated in terms of the lowest - order DAs 6 ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the highest - and last - to - lowest order DAs 7 , 8 .Furthermore , it was shown that the heavy - to - light shift form parameter FV ( q 2 ) of B→V transitions depends",
        "rewrite_text": "We present the light-cone distribution amplitudes (DAs) for axial vector mesons, expressed through their helicity components. These components are derived by solving the Bethe-Salpeter equation with an instantaneous interaction kernel, employing a recent method for estimating DAs. Our findings indicate that the twist-2 DA is primarily influenced by its initial Gegenbauer moment, while higher moments have a significant impact only for large velocity fractions (x > 0.7). The twist-3 DA consists of two independent functions, one of which corresponds to the second Gegenbauer moment. Additionally, our results reveal that the twist-4 contributions are minimal when compared to those of lower twists. This research will be instrumental in examining exclusive processes involving axial vector mesons, such as B-decays into charmonium along with a photon or pion pair.\n\nI. INTRODUCTION\n\nUnderstanding hadronic structure is crucial for comprehending the strong interactions among quarks and gluons within hadrons. In particular, the study of parton distributions provides significant insights into the distribution of quarks and gluons inside hadrons. Recently, significant progress has been made in exploring the internal structures of hadrons beyond the leading twist level, especially concerning transverse momentum-dependent parton distributions. In this work, we concentrate on another class of nonperturbative objects—light-cone distribution amplitudes (DAs), which define the probability amplitude for locating a quark-antiquark pair with specific transverse momentum fractions and longitudinal separation at a fixed light-like distance. DAs play a crucial role in the comprehension of various hard exclusive reactions. For example, the decay constants \\(f_{B\\pi}\\) and \\(f_{Bs}\\) can be expressed in terms of the lowest-order DAs; the form factors for semileptonic decays such as \\(B \\to \\pi l \\nu_l\\) and \\(B \\to K l \\nu_l\\) rely on both the highest- and lowest-order DAs. Additionally, it has been demonstrated that the heavy-to-light shifting form parameter \\(F_V(q^2)\\) for \\(B \\to V\\) transitions is dependent on these DAs.",
        "ori-fast-z-score": 1.044465935734187,
        "water-fast-z-score": 8.181649829917799,
        "rewrite-fast-z-score": 1.30066495428618
    },
    {
        "original_text": "We present an analysis of the mass function (MF) of active black holes (BHs), based on the sample of quasars with redshifts z < 0.5 and luminosities L > 10^44 erg/s, selected by Shen et al. (2007). We use two different methods to estimate BH masses for this sample - virial method and continuum-fitting method. The MF is constructed using both these estimates separately as well as their combination. Our results are compared against previous studies which used similar samples but different techniques to determine BH masses. We find that our best fit Schechter parameters agree within errors with those obtained previously. However, we also find evidence for a possible excess at low-mass end when we combine all three data sets together. This excess could be due to incompleteness or biases in the selection criteria adopted here. In addition, we compare our results with theoretical predictions made by Hopkins et al. (2006a) and Shankar et al. (2009b) .",
        "watermark_text": "We present an assessment of the mass distribution ( MF ) of active black holes ( BHs ) , using on the sample of quasars with redshifts z < 0 . 5 and luminosities L > 10 ^ 44 erg / s , selected by Shen et al . ( 2007 ) .We use two different methods to estimate BH masses for this sample - virial technique and continuum - fitting technique . The MF is built using both these estimates separately as well as their combination .Our results are compared against prior studies which employed identical specimens but different methods to identify BH masses . We see that our better matched Schechter parameters agree within errors with those acquired previously .However , we also find proof for a possible excess at low - weight end when we merge all three statistics sets together . This excess could be due to incompleteness or biases in the selection standards adopted here .In addition , we compare our findings with theoretical estimates made by Hopkins et al . ( 2006a ) and Shankar et al .(2009b) .",
        "rewrite_text": "We present an evaluation of the mass distribution (MF) of active black holes (BHs) using a sample of quasars with redshifts z < 0.5 and luminosities L > 10^44 erg/s, as selected by Shen et al. (2007). Two distinct methods are employed to estimate the BH masses in this sample: the virial technique and the continuum-fitting technique. The MF is constructed using these estimates separately as well as in combination. Our findings are compared to previous studies that utilized similar samples but different methodologies for determining BH masses. We observe that our well-matched Schechter parameters align within error margins with those reported previously. However, we also identify potential evidence of an excess at the low-mass end when all three data sets are combined. This excess may be attributed to incompleteness or biases in the selection criteria used in our analysis. Additionally, we compare our results with theoretical predictions from Hopkins et al. (2006a) and Shankar et al. (2009b).",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": -0.601929265428846
    },
    {
        "original_text": "The author considers the problem of gravitational interaction between bodies in terms of their informational content. The main idea is that the gravitational field can be considered as an ensemble of gravitons which carry information about the source body. Gravitational waves are treated as carriers of information on the state of motion of gravitating objects. It is shown how this approach allows one to explain some phenomena observed in astrophysics (the Pioneer anomaly) and cosmology (dark energy). In addition, it is proposed to use the concept of  information potential  for describing the evolution of the universe. This article was published by the journal Classical and Quantum Gravity Volume 27, Issue 14, pages 5993-6010, November 2010. DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is taken directly from the original publication. \n \n Abstract \n \n We consider the problem of gravitational interaction among bodies in terms of their information content. The main idea here is that the gravitational field may be viewed as an ensemble of gravitons/quanta carrying information about the source body; gravitational waves are then seen as carriers of information regarding the state of motion of the gravitating objects. This viewpoint enables us to provide explanations for certain phenomena observed in astrophysical settings (e.g., the Pioneer anomaly), as well as in cosmological contexts (e.g., dark energy). Moreover, we propose using the notion of “information potential” to describe the evolution of the Universe.",
        "watermark_text": "The author considers the issue of gravitational interaction between bodies in terms of their informational content . The main idea is that the gravitational field can be regarded as an ensemble of gravitons which carry information about the origin body .Gravitational waves are treated as carriers of information on the state of movement of gravitating objects . It is demonstrated how this methodology allows one to explain some phenomena observed in astrophysics ( the Pioneer anomaly ) and cosmology ( darkness energy ) .In addition , it is proposed to use the notion of information possibilities for describing the evolution of the universe . This section was publication by the journal Classical and Quantum Gravity Volume 27 , Issue 14 , pages 5993 - 6010 , November 2010 .DOI : 10 . 1088 / 0264 - 9381 / 27 / 14 / 05993 / abstract . The following text is taken directly from the original published .Abstract We consider the question of gravitational interaction among bodies in terms of their information content . The main idea here is that the gravitational field might be viewed as an ensemble of gravitons / quanta carrying information about the source body ; gravitational waves are then considered as transports of information regarding the state of movement of the gravitating structures .This perspectives permits us to provide explanations for particular phenomena observed in astrophysical contexts ( e . g . , the Pioneer anomaly ) , as also as in cosmological contexts ( e . g . , darkness energy ) . Moreover , we propose utilizing the notion of “ information potential ” to explain the evolution of the Universe .",
        "rewrite_text": "The author explores gravitational interactions between bodies through the lens of their informational content. The central thesis posits that the gravitational field can be interpreted as a collection of gravitons that convey information about the originating body. Gravitational waves are viewed as carriers of information pertaining to the motion of gravitating objects. This approach provides a framework for explaining certain phenomena observed in astrophysics, such as the Pioneer anomaly, as well as in cosmology, including dark energy. Additionally, the concept of \"information potential\" is suggested as a means to describe the Universe's evolution. This work was published in the journal Classical and Quantum Gravity, Volume 27, Issue 14, pages 5993-6010, in November 2010, with the DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is directly excerpted from the original publication: \"We consider the question of gravitational interaction among bodies in terms of their information content. The main idea here is that the gravitational field might be viewed as an ensemble of gravitons/quanta carrying information about the source body; gravitational waves are then considered as transports of information regarding the state of movement of the gravitating structures. This perspective allows us to provide explanations for particular phenomena observed in astrophysical contexts (e.g., the Pioneer anomaly) as well as in cosmological contexts (e.g., dark energy). Moreover, we propose utilizing the notion of 'information potential' to explain the evolution of the Universe.\"",
        "ori-fast-z-score": 2.116950987028628,
        "water-fast-z-score": 6.735753140545634,
        "rewrite-fast-z-score": 2.2445701677816263
    },
    {
        "original_text": "We propose that the metallic enhancement observed for some post T Tauri stars (PTTS) may be due to an accretion of planetesimals during their formation phase, which is followed by rapid planet formation and subsequent ejection of planets into space.  We show that this scenario can explain both the high metallicity found among PTTS as well as the low abundance ratios between refractory elements such as Mg/Si or Al/Si compared with those expected if these objects formed through standard core-accretion processes. The proposed mechanism also explains why there are no known close-in giant planets around PTTSs despite the fact that they have already passed their protoplanetary disk stage. This model predicts that most PTTS should host at least one Jupiter mass planet on wide orbits beyond 1 AU. In addition we predict that many PTTS will exhibit infrared excesses caused by dusty debris disks produced by collisions between planetary bodies.",
        "watermark_text": "We suggest that the metallic enhancement detected for some post T Tauri planets ( PTTS ) may be due to an accretion of planetesimals during their formed phase , which is preceded by rapid planet development and subsequent ejection of stars into space . We suggest that this situation can describe both the high metallicity observed among PTTS as well as the poor abundance proportions between refractory objects such as Mg / Si or Al / Si compared with those expected if these objects created through conventional core - accretion cycles .The proposed process also explains why there are no known close - in massive planets around PTTSs despite the fact that they have already completed their protoplanetary disk stage . This theory predicts that most PTTS should accommodate at least one Jupiter mass planet on wide orbits beyond 1 AU .In addition we estimate that several PTTS will exhibit infrared excesses caused by dusty dust belts generated by collisions between planetary body .",
        "rewrite_text": "We propose that the metallic enhancement observed in some post-T Tauri stars (PTTS) may result from the accretion of planetesimals during their formation phase, which follows a rapid development of the planets and subsequent ejection of stars into space. This scenario could explain both the high metallicity seen in PTTS and the unusual abundance ratios of refractory elements like Mg/Si and Al/Si, which diverge from what would be expected if these elements formed through the conventional core-accretion model. Additionally, this theory clarifies the absence of known close-in massive planets around PTTSs, even though they have completed their protoplanetary disk phases. We predict that most PTTS should host at least one Jupiter-mass planet on wide orbits, exceeding 1 AU. Furthermore, we estimate that several PTTS will display infrared excesses due to dusty debris belts formed by collisions between planetary bodies.",
        "ori-fast-z-score": -1.116312611302876,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": -0.6401843996644799
    },
    {
        "original_text": "We present the first mid-infrared phase curve observations for an extrasolar planet, WASP-121b (1SWASP J140747.93-394542.7), using Spitzer/IRAC at 3.6 and 4.5 microns. The data were taken in two epochs separated by one year to allow us to search for any changes in the system s properties over time. We find that the amplitude of the phase variation is consistent with previous measurements made in the optical but we detect no significant change between our two epochs. This suggests that there are no large variations in the temperature structure or composition of this planet as it orbits its host star. Our results also show that the planet has a very high albedo in both bands which may be due to clouds and/or haze. These findings have important implications for understanding how planets form and evolve. \n \n Keywords: exoplanet, infrared",
        "watermark_text": "We present the first mid - infrared phase curve measurements for an extrasolar planet , WASP - 121b ( 1SWASP J140747 . 93 - 394542 . 7 ) , using Spitzer / IRAC at 3 . 6 and 4 . 5 microns . The data were took in two epochs separated by one month to allow us to search for any alterations in the system s features over time .We see that the frequency of the phase change is compatible with previous measurements made in the optical but we perceive no major shift between our two epochs . This implies that there are no large changes in the temperature structure or composition of this planet as it orbits its host star .Our results also demonstrate that the planet has a very high albedo in both bands which may be due to clouds and / or haze . These conclusions have important implications for explaining how planets form and evolve .Keywords: exoplanet, infrared",
        "rewrite_text": "We present the inaugural mid-infrared phase curve measurements for the exoplanet WASP-121b (1SWASP J140747.93 - 394542.7), utilizing the Spitzer/IRAC instrument at wavelengths of 3.6 and 4.5 microns. The observations were conducted over two epochs, one month apart, allowing us to investigate potential variations in the system's characteristics over time. Our findings indicate that the phase change frequency aligns with previous optical measurements, and we observe no significant variations between the two epochs. This suggests stability in the planet's temperature structure and composition as it orbits its host star. Additionally, our results reveal that the planet exhibits a very high albedo in both bands, potentially attributed to clouds and/or haze. These findings have critical implications for our understanding of planetary formation and evolution. Keywords: exoplanet, infrared.",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": 0.2626128657194451
    },
    {
        "original_text": "We present new measurements of 3 He+ and 3 He++ column densities toward eight nearby stars, using data obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE). The results are compared to previous observations made by Copernicus and IUE satellites as well as FUSE. We find that our values for N(3 He+)/N(H+), which range between 0.0015-0.0125, agree within uncertainties with those measured previously at high latitudes but disagree significantly with lower latitude measurements. Our results suggest that there is an additional source of ionization near the Galactic plane not accounted for by cosmic rays or X-rays. This could be due to shocks driven into the interstellar medium by supernovae remnants and/or winds associated with massive OB associations. \n \n Keywords: Helium abundance, Interstellar medium, Shocks, Supernova remnant, Winds, Cosmic ray",
        "watermark_text": "We present new surveys of 3 He + and 3 He + + column densities toward eight distant stars , using data acquired with the Far Ultraviolet Spectroscopic Explorer ( FUSE ) . The results are compared to previous images conducted by Copernicus and IUE missions as well as FUSE .We see that our values for N ( 3 He + ) / N ( H + ) , which range between 0 . 0015 - 0 . 0125 , agree within uncertainties with those observed previously at high latitudes but disagree significantly with higher latitude observations . Our results propose that there is an additional source of ionization near the Galactic jet not accounted for by cosmic rays or X - radiation .This might be due to shocks driven into the interstellar medium by supernovae fragments and / or winds related with massive OB interactions . Keywords : Helium abundance , Interstellar medium , Shocks , Supernova remnant , Winds , Cosmic ray",
        "rewrite_text": "We report new measurements of 3 He + and 3 He + + column densities towards eight distant stars, utilizing data obtained from the Far Ultraviolet Spectroscopic Explorer (FUSE). Our findings are compared with earlier studies conducted by the Copernicus and IUE missions, as well as with previous FUSE observations. The values for N (3 He +) / N (H +), which range from 0.0015 to 0.0125, are in agreement with high-latitude observations previously recorded, yet exhibit significant discrepancies with measurements taken at higher latitudes. These results suggest the presence of an additional ionization source near the Galactic jet that is not explained by cosmic rays or X-radiation. This source may be related to shocks in the interstellar medium caused by supernova fragments and/or winds associated with massive OB star interactions. \nKeywords: Helium abundance, Interstellar medium, Shocks, Supernova remnant, Winds, Cosmic rays.",
        "ori-fast-z-score": -0.42857142857142855,
        "water-fast-z-score": 4.428571428571429,
        "rewrite-fast-z-score": 0.7878385971583353
    },
    {
        "original_text": "We study adiabatic charge transport across an interacting quantum dot coupled to two leads, where one lead is driven by a time-dependent gate voltage and the other remains grounded. We show that this setup can be used as a pump for electrons when the driving frequency matches twice the energy difference between the singlet and triplet states of the dot. In particular we find that the pump current exhibits sharp peaks whenever the Fermi level crosses a bound state inside the gap induced by the Coulomb interaction on the dot. The height of these peaks increases with increasing temperature T , which allows us to use our system as a thermometer. Finally, we discuss how our findings are modified if the driving amplitude becomes comparable or larger than the charging energy U . \nI. INTRODUCTORY REMARK\nIn recent years there has been growing interest in using periodically-driven systems as sources of coherent radiation  1  . This idea was first proposed more than twenty years ago  2  but only recently it became possible to realize such devices experimentally  3  .\nOne particularly interesting class of periodically-driven systems consists of those whose properties depend strongly on their internal degrees of freedom  4  . These so-called  quantum impurity models  have attracted considerable attention over the past few decades because they provide a simple description of many physical phenomena ranging from single-electron transistors  5  to heavy fermion compounds  6  . Recently, several groups have studied theoretically the possibility of using quantum dots  7, 8  and carbon nanotubes  9  as pumps for electrons  10  . However, most theoretical studies so far focused on non-interacting particles  11  while experiments typically involve strong interactions  12  . It would therefore be desirable to extend existing theories beyond the weak-coupling limit  13  .\nThe purpose of this work is to investigate the effect of electron correlations on the performance of a pump based on a quantum dot (QD)  14  . To do so, we consider a QD connected to two leads via tunnel barriers  15  . One lead is driven out of equilibrium by applying a periodic gate voltage V g (t), whereas the second lead serves as a reference electrode  16  . As shown schematically in Fig. 1(",
        "watermark_text": "We explore adiabatic charge flow across an interacting quantum dot connected to two leads , where one lead is powered by a time - based gate voltage and the other remains grounded . We see that this configuration can be used as a pump for electrons when the driving frequency matches times the power change between the singlet and triplet states of the dot .In particular we find that the pump charge shows sharp peaks whenever the Fermi level crosses a bound state inside the gap induced by the Coulomb interaction on the dot . The height of these spikes varies with expanding temperature T , which allows us to use our system as a thermometer .Finally , we explain how our findings are modified if the driving amplitude becomes comparable or larger than the charging power U . I .INTRODUCTORY REMARK In past decades there has been growing interest in utilizing continuously - fueled materials as sources of coherent emission 1 . This idea was first suggested more than twenty years previously 2 but only lately it becoming necessary to realize such devices experimentally 3 .One especially interesting class of regularly - fueled schemes includes of those whose characteristics rely highly on their internal degrees of freedom 4 . These so - called quantum impurity models have garnered considerable scrutiny over the previous few years because they give a simple explanation of several physical phenomena ranging from double - ion transistors 5 to heavy fermion compounds 6 .Recently , various groups have researched theoretically the prospect of using quantum dots 7 , 8 and carbon nanotubes 9 as pumps for electrons 10 . However , most theoretical researchers so far concentrate on non - interacting molecules 11 while experiments usually include strong interactions 12 .It would therefore be desirable to extend previous theories beyond the weak - interaction threshold 13 . The purpose of this research is to examine the impact of electron correlations on the performance of a pump based on a quantum dot ( QD ) 14 .To do so , we imagine a QD connected to two leads via underground walls 15 . One lead is caused out of equilibrium by using a periodic gate current V g ( t ) , whereas the second lead serves as a reference electrode 16 .As depicted schematically in Fig . 1 (",
        "rewrite_text": "We investigate the adiabatic charge flow in an interacting quantum dot connected to two leads—one lead is influenced by a time-varying gate voltage while the other remains grounded. Our findings indicate that this setup can function as an electron pump when the driving frequency aligns with the energy difference between the dot's singlet and triplet states. Specifically, we observe distinct peaks in the pumped charge whenever the Fermi level intersects a bound state within the gap created by the Coulomb interaction in the dot. The amplitudes of these peaks vary with increasing temperature \\( T \\), enabling us to utilize the system as a thermometer. Additionally, we discuss how our results change when the driving amplitude approaches or exceeds the charging energy \\( U \\). \n\n**I. INTRODUCTORY REMARKS**\n\nIn recent decades, there has been increasing interest in harnessing continuously-fueled materials for coherent emission sources. This concept was first proposed over twenty years ago, but it is only recently that the necessity to develop such devices experimentally has emerged. A particularly intriguing class of continuously-fueled systems is those that strongly depend on their internal degrees of freedom. These quantum impurity models have received considerable attention in recent years due to their ability to explain various physical phenomena, including double-ion transistors and heavy fermion compounds. Recently, multiple research groups have theoretically explored the potential of using quantum dots and carbon nanotubes as electron pumps. However, most theoretical investigations so far have focused on non-interacting systems, while experiments typically involve strong interactions. Therefore, there is a clear need to extend existing theories beyond the weak interaction regime. This study aims to investigate the effects of electron correlations on the functionality of a quantum dot-based pump. To achieve this, we consider a quantum dot connected to two leads via insulating barriers. One lead is driven out of equilibrium by a periodic gate voltage \\( V_g(t) \\), while the second lead acts as a reference electrode, as illustrated in Fig. 1.",
        "ori-fast-z-score": -1.3568010505999364,
        "water-fast-z-score": 8.04261249963027,
        "rewrite-fast-z-score": 0.8512055557875505
    },
    {
        "original_text": "We study the effect of cascades initiated by ultra-high energy cosmic rays (UHECRs) in intergalactic space, and their contribution to the diffuse gamma-ray emission observed at Earth. We find that cascade photons can be produced over cosmological distances with energies up to 10 TeV or more. The resulting flux is strongly suppressed for sources located beyond redshifts z > 0.5 due to absorption effects caused by pair production interactions between cascade photons and infrared radiation fields associated with galaxies. For nearby source models we show that this suppression leads to an upper limit on the photon fraction fγ < 10% − 20% depending on the UHECR injection spectrum assumed. This constraint is consistent with current measurements of the diffuse gamma-ray background as well as recent results obtained using Fermi/LAT data. In addition, our analysis shows that cascade photons are likely to dominate the total diffuse gammaray emission below 1 GeV if they originate within a distance of about 100 Mpc around us.",
        "watermark_text": "We research the impact of cascades created by ultra - large energy cosmic rays ( UHECRs ) in intergalactic space , and their contribution to the diffuse γ - ray radiation observed at Earth . We see that cascade photons can be obtained over cosmological distances with energies up to 10 TeV or more .The resulting flux is strongly restrained for sources located beyond redshifts h > 0 . 5 due to absorption effects caused by pair production interactions between cascade photons and infrared light fields associated with galaxies . For nearby source models we find that this suppression results to an upper limitation on the photon fraction fγ < 10 % − 20 % based on the UHECR injection range assumed .This constraint is compatible with current observations of the diffuse γ - ray background as well as recent results acquired using Fermi / LAT results . In addition , our analysis shows that cascade photons are likely to dominate the total diffuse gammaray radiation below 1 GeV if they originate within a distance of about 100 Mpc around us .",
        "rewrite_text": "We investigate the effects of cascades generated by ultra-high energy cosmic rays (UHECRs) in intergalactic space and their role in contributing to the diffuse γ-ray radiation detected on Earth. Our findings indicate that cascade photons can travel over cosmological distances with energies reaching 10 TeV or higher. However, the resulting flux is significantly limited for sources more than a redshift of h > 0.5 due to absorption effects from pair production interactions between cascade photons and infrared light from galaxies. In models involving nearby sources, we determine that this suppression leads to an upper limit on the photon fraction, fγ, of less than 10% to 20%, depending on the assumed UHECR injection range. This constraint aligns with current observations of the diffuse γ-ray background and recent findings from Fermi/LAT data. Furthermore, our analysis suggests that cascade photons are likely to dominate the total diffuse γ-ray radiation below 1 GeV if they originate from within approximately 100 Mpc of our location.",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 5.976143046671968,
        "rewrite-fast-z-score": 2.8685486624025445
    },
    {
        "original_text": "We present an algorithm for generating new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and adding scalar fields in such a way that the resulting solution is minimally coupled.  The method can be used to generate exact solutions which are not known explicitly or only implicitly as functions of some parameters (e.g., by solving algebraic equations). We illustrate our approach on several examples including Schwarzschild-de Sitter black holes, Reissner-Nordström-anti-de Sitter black holes, Kerr-Newman-AdS black holes, and charged dilatonic black holes. In particular we show how one can obtain explicit expressions for the massless limit of these black hole solutions. Our results may also have applications beyond gravity theory, e.g., in quantum mechanics where they could provide insight into the structure of bound states. Introduction: Exact solutions play an important role in theoretical physics because they allow us to test various physical ideas against concrete predictions. However, finding exact solutions to physically interesting problems often turns out to be very difficult. For example, it took more than 100 years after the discovery of general relativity before the first exact black hole solutions were found  1-3 . Even today there exist many open questions about black holes  4  . One reason why finding exact solutions is so challenging is that most theories of interest do not admit any simple analytic solutions. Another problem arises when trying to find solutions describing systems with multiple interacting components like black holes surrounded by matter or other fields. Here one usually has to solve complicated differential equations numerically which makes it hard to find all possible solutions even if their existence was guaranteed theoretically. This situation becomes particularly severe if one wants to study phenomena at strong coupling since then numerical methods become less reliable due to large corrections arising from higher orders in perturbation theory.",
        "watermark_text": "We present an algorithm for generating new answers to the coupled Einstein - scalar field equations , beginning from vacuum solutions and adding scalar fields in such a way that the resulting solve is minimally coupled . The method can be used to create precise solutions which are not described specifically or only implicitly as functions of some parameters ( e . g . , by modeling algebraic equations ) .We illustrate our approach on numerous instances including Schwarzschild - de Sitter dark holes , Reissner - Nordström - anti - de Sitter dark holes , Kerr - Newman - AdS red holes , and charged dilatonic black holes . In particular we give how one can obtain precise expressions for the massless maximum of these black hole solutions .Our results may even have applications beyond gravitational theory , e . g . , in quantum mechanics where they may provide insight into the formation of bound states . Introduction : Exact solutions play an important role in theoretical physics because they allow us to test various mechanical concepts against concrete expectations .However , finding exact treatments to physically exciting issues often comes out to be very difficult . For instance , it takes more than 100 years after the discovery of general relativity before the first accurate black hole problems were found 1 - 3 .Even nowadays there remain many open questions about black holes 4 . One reason why seeking precise solutions is so difficult is that most models of importance do not admit any straightforward analytic solutions .Another difficulty arises when trying to find solutions involving systems with various interacting components like grey holes populated by matter or other fields . Here one usually has to solve complicated differential equations numerically which makes it difficult to find all possible solutions even if their existence was assured theoretically .This problem arises increasingly severe if one wants to study phenomena at strong coupling since then numerical models become fewer reliable resulting to large corrections resulting from greater orders in perturbation theory .",
        "rewrite_text": "We introduce an algorithm designed to generate new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and incorporating scalar fields in a manner that ensures minimal coupling. This approach allows for the creation of precise solutions that may not be explicitly defined or only described implicitly in terms of certain parameters (for example, via modeling algebraic equations). We apply our method to various cases, including Schwarzschild-de Sitter dark holes, Reissner-Nordström-antide Sitter dark holes, Kerr-Newman-AdS red holes, and charged dilatonic black holes. Notably, we demonstrate how to derive exact expressions for the massless maximum of these black hole solutions. Our findings could extend beyond gravitational theory, potentially offering insights into the formation of bound states in quantum mechanics.\n\nIntroduction: Exact solutions are crucial in theoretical physics as they provide a means to test mechanical concepts against observable predictions. However, obtaining exact solutions for significant physical issues often proves challenging. For instance, it took over a century after the formulation of general relativity before accurate solutions for black hole problems emerged. Even today, numerous questions regarding black holes remain unanswered. One challenge in finding precise solutions is that many important models do not allow for straightforward analytical solutions. Additionally, when dealing with systems that involve various interacting components, such as grey holes filled with matter or other fields, the task becomes even more complex. Typically, this requires solving intricate differential equations numerically, which complicates the process of discovering all possible solutions, despite their theoretical existence being established. This challenge intensifies when analyzing phenomena at strong coupling, where numerical models can become less reliable, leading to substantial corrections from higher-order perturbation theory.",
        "ori-fast-z-score": 0.30499714066520933,
        "water-fast-z-score": 8.387421368293257,
        "rewrite-fast-z-score": 2.042752923427804
    },
    {
        "original_text": "We study the statistical properties of Barkhausen noise generated by an Ising spin system with random fields and competing interactions at its surface, using wavelets to analyze the time series produced by this model.  We find that the power spectrum of the Barkhausen signal is well described by a stretched exponential function over several decades in frequency space. The stretching exponent depends on both temperature T and magnetic field H. In particular, we show how the stretching exponent can be used as a measure of the degree of disorder in the sample under investigation. Finally, we discuss possible extensions of our work to other types of systems exhibiting avalanche dynamics. Barkhausen noise (BN) has been studied extensively since it was first observed experimentally more than 100 years ago  1  . It consists of bursts of magnetization reversals which occur when a ferromagnetic material is driven through successive metastable states  2  , and is believed to play an important role in determining the coercive force of such materials  3  .\nThe statistics of BN have attracted considerable interest recently  4  -  8  due to their potential application in non-destructive testing  9  . However, despite many experimental studies  10  -  12  there are still open questions about the origin of these fluctuations  13  . For example, while some authors claim that they arise from thermally activated processes  14  others argue that they result from collective effects  15  or even quantum tunneling  16  . A number of theoretical models  17  -  20  have also been proposed to explain the physics behind BN but none of them seems able to reproduce all features simultaneously  21  .",
        "watermark_text": "We research the statistical characteristics of Barkhausen interference generated by an Ising spin body with random fields and competing interactions at its surface , using wavelets to analyze the time series formed by this model . We see that the power spectrum of the Barkhausen signal is well described by a stretched exponential function over several decades in frequency space .The extension exponent depends on both heat T and magnetic force H . In particular , we explain how the stretching exponent can be used as a measure of the degree of disorder in the sample under research . Finally , we explain possible extend of our work to other types of models displaying avalanche dynamics .Barkhausen interference ( BN ) has been studied frequently since it was first observed experimentally more than 100 years early 1 . It consists of flashes of magnetization reversals which occur when a ferromagnetic material is accelerated through consecutive metastable levels 2 , and is suspected to hold an important role in establishing the coercive force of such substances 3 .The data of BN have garnered considerable interest recently 4 - 8 due to their potential application in non - destructive testing 9 . However , despite many experimental studies 10 - 12 there are still open questions about the origin of these fluctuations 13 .For instance , while some writers claim that they occur from thermally activated processes 14 others argue that they occur from collective effects 15 or maybe quantum tunneling 16 . A several of theoretical theories 17 - 20 have also been proposed to explain the physics behind BN but none of them appears able to predict all characteristics simultaneously 21 .",
        "rewrite_text": "We investigate the statistical properties of Barkhausen interference produced by an Ising spin model subjected to random fields and competing interactions at its surface, employing wavelet analysis to assess the time series generated by this system. Our findings reveal that the power spectrum of the Barkhausen signal is accurately modeled by a stretched exponential function across multiple decades in frequency space. Notably, the extension exponent is influenced by both temperature (T) and magnetic field strength (H). We demonstrate how this stretching exponent can serve as an indicator of disorder within the sample under study. Furthermore, we discuss potential avenues for expanding our research to other models exhibiting avalanche dynamics. Barkhausen interference (BN) has been the subject of extensive investigation since its experimental discovery over a century ago. It manifests as bursts of magnetization reversals that occur when a ferromagnetic material transitions through a series of metastable states, and it is believed to play a crucial role in determining the coercive force of these materials. Recent interest in BN data has surged due to its promising applications in non-destructive testing. Despite numerous experimental investigations, many questions remain regarding the underlying mechanisms of these fluctuations. Some researchers attribute them to thermally activated processes, while others suggest they result from collective effects or possibly quantum tunneling. Various theoretical models have been proposed to elucidate the physics of BN, but none fully capture all observed characteristics.",
        "ori-fast-z-score": -0.09166984970282113,
        "water-fast-z-score": 7.425257825928512,
        "rewrite-fast-z-score": 0.09407208683835973
    },
    {
        "original_text": "We present new results on the asteroseismic analysis of the primary component in the binary system Alpha Centari, based on data obtained with the HARPS spectrograph at La Silla Observatory (Chile). We find evidence for two independent frequencies that are likely to be associated with rotationally split modes. The observed frequency pattern is consistent with theoretical predictions and suggests an inclination angle between 40°and 60°for this star. \n \n Keywords: Asteroseismology, Rotation, Binary stars, Oscillations, Frequency analysis, High-precision radial velocities, Alpha Centari ABSTRACT \n \n We report new results on the asterioseismic analysis of the main-sequence F-type star Alpha Centari A, which forms part of a close double system with its cooler companion B. Our study was carried out using high-precision radial-velocity measurements collected over more than four years by the HARPS instrument installed at ESO s 3.6-m telescope at La Silla Observatory (Chilean Andes), together with photometric observations made simultaneously with the CoRoT space mission. By applying standard techniques used in asteroseismology we have detected several periodicities in both datasets, including one signal whose periodicity corresponds exactly to the orbital period of the system. This finding confirms previous suggestions that the pulsational behaviour of this star may be influenced by tidal effects induced by its companion. In addition, our analysis reveals another set of signals corresponding to periods ranging from about 1 day up to almost 2 days. These signals can be explained as being due to rotationally split p-mode oscillations excited in the convective envelope of the star. Their presence provides strong support for the hypothesis that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo processes operating within the convection zone.",
        "watermark_text": "We report new data on the asteroseismic study of the primary component in the binary system Alpha Centari , using on evidence derived with the HARPS spectrograph at La Silla Observatory ( Chile ) . We see evidence for two independent frequencies that are likely to be involved with rotationally split periods .The observed frequency trend is compatible with theoretical estimates and suggests an inclination angle between 40°and 60°for this star . Keywords : Asteroseismology , Rotation , Binary stars , Oscillations , Frequency assessment , High - precision radial velocities , Alpha Centari ABSTRACT We report new data on the asterioseismic study of the main - sequence F - class star Alpha Centari A , which forms part of a close double system with its warmer companion B .Our study was carried out utilizing large - precision radial - speed measurements collected over more than four years by the HARPS instrument located at ESO s 3 . 6 - m observatory at La Silla Observatory ( Chilean Andes ) , combined with photometric surveys made independently with the CoRoT space expedition . By applying traditional techniques employed in asteroseismology we have discovered numerous periodicities in both datasets , notably one signal whose periodicity corresponds exactly to the orbital period of the system .This finding indicates past proposals that the pulsational response of this star may be altered by tidal impacts generated by its companion . In addition , our analysis reveals another set of signals relating to periods ranging from about 1 day up to approximately 2 days .These transmissions can be understood as being related to rotationally split p - mode oscillations excited in the convective envelope of the star . Their presence provides strong evidence for the notion that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo mechanisms operating within the convection zone .",
        "rewrite_text": "We present new findings from the asteroseismic analysis of the primary component in the Alpha Centauri binary system, based on data obtained with the HARPS spectrograph at La Silla Observatory in Chile. Our observations revealed two independent frequencies likely associated with rotationally split periods. The frequency trends observed align well with theoretical estimates, suggesting an inclination angle for this star between 40° and 60°. \n\nKeywords: Asteroseismology, Rotation, Binary Stars, Oscillations, Frequency Assessment, High-Precision Radial Velocities, Alpha Centauri.\n\nABSTRACT: This study reports new insights into the asteroseismic properties of Alpha Centauri A, an F-type main-sequence star that is part of a close binary system with its hotter companion, Alpha Centauri B. We utilized high-precision radial velocity measurements gathered over more than four years by the HARPS instrument at the ESO's 3.6-meter telescope at La Silla Observatory in the Chilean Andes, alongside independent photometric data from the CoRoT space mission. By employing established asteroseismic techniques, we identified multiple periodicities in both datasets, including a prominent signal corresponding exactly to the orbital period of the binary system. This suggests that the pulsational response of Alpha Centauri A may be influenced by tidal interactions with its companion. Furthermore, our analysis uncovered another group of signals with periods ranging from about 1 to 2 days, which are likely linked to rotationally split p-mode oscillations excited within the star's convective envelope. These findings provide compelling evidence that the surface of Alpha Centauri A has been shaped by magnetic activity resulting from dynamo processes in its convective zone.",
        "ori-fast-z-score": -0.9838699100999074,
        "water-fast-z-score": 6.887089370699352,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present simultaneous observations with the Rossi X-ray Timing Explorer (RXTE) Proportional Counter Array, Swift Burst Alert Telescope (BAT), X-Ray Telescope (XRT), Ultraviolet/Optical Telescope (UVOT), and Radio Extension for Multi-wavelengths Observatory (REM). The data were obtained during an active state of the source on February 18-20, 2005 . We find that the spectral energy distribution is consistent with synchrotron self-Compton emission from relativistic electrons accelerated by magnetic reconnection events occurring within the jet. This interpretation is supported by the detection of rapid variability at all wavelengths observed simultaneously. In addition to this flaring activity we also detect significant flux variations over longer timescales which are likely due to changes in the Doppler boosting factor caused by bulk motion of plasma along the jet. These results demonstrate how multi-wavelength campaigns can be used to study the physical processes responsible for the production of non-thermal radiation in AGN jets.",
        "watermark_text": "We report concurrent observations with the Rossi X - ray Timing Explorer ( RXTE ) Proportional Counter Array , Swift Burst Alert Telescope ( BAT ) , X - Ray Telescope ( XRT ) , Ultraviolet / Optical Telescope ( UVOT ) , and Radio Extension for Multi - wavelengths Observatory ( REM ) . The data were obtained during an active state of the source on February 18 - 20 , 2005 .We see that the spectral power distribution is compatible with synchrotron self - Compton absorption from relativistic electrons accelerated by magnetic reconnection events resulting within the jet . This interpretation is backed by the discovery of rapid variability at all wavelengths observed concurrently .In addition to this flaring activity we also observe significant flux variations over longer timescales which are likely due to changes in the Doppler boosting factor resulting by bulk movement of plasma along the jet . These data demonstrate how multi - wavelength campaigns can be used to study the physical processes responsible for the production of non - cooling emission in AGN planes .",
        "rewrite_text": "We present simultaneous observations using the Rossi X-ray Timing Explorer (RXTE) Proportional Counter Array, Swift Burst Alert Telescope (BAT), X-ray Telescope (XRT), Ultraviolet/Optical Telescope (UVOT), and Radio Extension for Multi-wavelength Observatory (REM). The data were collected during an active phase of the source between February 18 and 20, 2005. Our findings indicate that the spectral power distribution is consistent with synchrotron self-Compton absorption by relativistic electrons accelerated by magnetic reconnection events occurring within the jet. This interpretation is further supported by the detection of rapid variability across all observed wavelengths. In addition to the flaring activity, we also note significant flux variations over longer timescales, likely attributed to changes in the Doppler boosting factor due to the bulk motion of plasma along the jet. These results illustrate the effectiveness of multi-wavelength campaigns in studying the physical processes responsible for the generation of non-cooling emission in the planes of active galactic nuclei (AGN).",
        "ori-fast-z-score": 0.13018891098082389,
        "water-fast-z-score": 5.337745350213779,
        "rewrite-fast-z-score": 1.75
    },
    {
        "original_text": "Quantum repeaters are proposed to overcome the loss in quantum communication channels by using entangled photons and linear optics elements such as beam splitters, phase shifters, and single-photon detectors.  In this work we present an experimental demonstration of a quantum repeater with two distant nodes connected via a 50 km fiber link. The system consists of three parts: (1) generation, (2) transmission, and (3) detection of photon pairs at each node; (4) Bell-state measurement between the two nodes; and (5) feed-forward operation on the received qubits based on the result of Bell-state measurement. We have successfully demonstrated all these parts except the feed-forward operation which is currently under development. Our results show that our current setup can generate high quality entanglement over a distance up to 50 km. This experiment paves the way towards practical implementation of quantum networks. Quantum repeaters are proposed to solve the problem caused by channel losses in quantum communication systems  1  . They use entangled photons generated locally or remotely  2  , and linear optical elements such as beam splitters; phase shifters; and single-photon detectors  3  .\nIn this Letter, we report an experimental demonstration of a long-distance quantum repeater  4  consisting of four main components: (1) generation, ( 2 ) t r ansmission , ( 3 ) d etection , and ( 4 ) B ell -state m easurement o f pho ton pairs at eac h n ode ; ( 5 ) feed-foward operations on the received qubits according to the outcome of Bell-state measurements   Figs. 1(a) , 1(b), and 2 . A pair of polarization-entangled photons was produced through spontaneous parametric down-conversion (SPDC). One photon acted as signal while another one served as idler. After passing through different paths, they were combined together at a beam splitter (BS) and sent into a 50-km-long fiber-optic line. At both ends of the fiber-optic line, photon-number resolving avalanche photodiodes (APDs) detected the",
        "watermark_text": "Quantum repeaters are proposed to overcome the loss in quantum communication channels by using entangled photons and linear optics components such as beam splitters , wave shifters , and single - photon detectors . In this research we present an demonstration demonstration of a quantum repeater with two distant nodes linked via a 50 km fiber link .The system contains of three parts : ( 1 ) production , ( 2 ) propagation , and ( 3 ) detection of photon pairs at each node ; ( 4 ) Bell - state measurement between the two nodes ; and ( 5 ) feed - forward operation on the received qubits based on the result of Bell - state measurement . We have successfully shown all these parts except the feed - forward operation which is currently under development .Our results show that our new configuration can generate strong quality entanglement over a length up to 50 km . This study paves the way towards practical implementation of quantum networks .Quantum repeaters are proposed to tackle the issue caused by channel losses in quantum communication devices 1 . They use entangled photons generated locally or remotely 2 , and linear optical units such as beam splitters ; phase shifters ; and single - photon detectors 3 .In this Letter , we publish an experimental demonstration of a long - distance quantum repeater 4 consisting of four main components : ( 1 ) production , ( 2 ) t r ansmission , ( 3 ) d etection , and ( 4 ) B ell - state m easurement o h pho ton pairs at eac h h ode ; ( 5 ) feed - foward operations on the received qubits according to the result of Bell - state measurements Figs . 1 ( a ) , 1 ( c ) , and 2 .A couple of polarization - entangled photons was produced through spontaneous parametric down - transfer ( SPDC ) . One photon worked as signal while another one served as idler .After passing through different paths , they were paired together at a laser splitter ( BS ) and sent into a 50 - kilometres - length fiber - optic line . At both sides of the fiber - optic line , photon - number resolving avalanche photodiodes ( APDs ) detected the",
        "rewrite_text": "Quantum repeaters are designed to address the issue of loss in quantum communication channels by employing entangled photons along with linear optical components such as beam splitters, wave shifters, and single-photon detectors. In this study, we present an experimental demonstration of a quantum repeater linking two distant nodes through a 50 km fiber optic connection. The system consists of three main components: (1) production, (2) propagation, and (3) detection of photon pairs at each node; (4) Bell-state measurement between the two nodes; and (5) feed-forward operations on the received qubits based on the results of the Bell-state measurements. We have successfully implemented all these components except for the feed-forward operation, which is currently under development. Our results indicate that this new configuration can produce high-quality entanglement over a distance of 50 km, paving the way for practical quantum networks. In this letter, we detail the experimental demonstration of a long-distance quantum repeater, comprising four key components: (1) production, (2) transmission, (3) detection, and (4) Bell-state measurement of photon pairs at each node; with the fifth component being the feed-forward operations based on the Bell-state measurement outcomes. Utilizing spontaneous parametric down-conversion (SPDC), we generated pairs of polarization-entangled photons, with one serving as the signal and the other as the idler. After traversing different paths, the photons were combined at a beam splitter (BS) and transmitted through the 50-kilometer fiber-optic line. Photon-number resolving avalanche photodiodes (APDs) on both ends of the fiber detected the photons.",
        "ori-fast-z-score": 0.26211121699831136,
        "water-fast-z-score": 5.613171323564988,
        "rewrite-fast-z-score": -0.09578262852211514
    },
    {
        "original_text": "We study the elasticity of thin films with periodic microstructure, which are constrained to lie on an underlying substrate. We show that such systems can exhibit anomalously large values for their Young s moduli as well as Poisson ratios. The origin of these effects is traced back to the presence of phonon soft modes associated with the periodicity along the film normal direction. These results have implications for the design of novel materials with tailored elastic properties. \n \n In recent years there has been growing interest in understanding how confinement affects the physical behavior of matter at the nanoscale  1  . This problem arises naturally when considering thin films or nanowires embedded within bulk materials; however it also applies more generally whenever a system is restricted to occupy only part of its available phase space  2  . For example, this situation occurs frequently during crystal growth where defects may be introduced into the lattice structure by impurities  3  , or when studying colloidal suspensions  4  .\n \nIn this work we consider the case of a thin film with periodic microstructure, whose thickness h lies between two length scales L and d (see Fig 1) . Here L represents the typical size of the unit cell while d denotes the characteristic spacing between adjacent layers; both quantities are assumed to be much smaller than the in-plane dimensions of the sample. Such structures arise commonly in nature, e.g., in layered compounds like graphite  5  , transition metal dichalcogenides  6  , and hexagonal boron nitride  7  . They are also used extensively in technological applications ranging from photovoltaics  8  to optoelectronics  9  . \n \n Figure 1: Schematic illustration of our model geometry. A thin film with periodic microstructures is confined to lie on top of a rigid substrate.",
        "watermark_text": "We test the elasticity of thin films with periodic microstructure , which are constrained to lay on an underlying substrate . We see that such complexes can exhibit anomalously high values for their Young s moduli as well as Poisson ratios .The origin of these phenomena is traced back to the presence of phonon soft modes associated with the periodicity along the film regular direction . These results have consequences for the creation of new materials with tailored elastic properties .In past decades there has been growing interest in understanding how confinement impacts the physical activity of matter at the nanoscale 1 . This problem arises readily when considering thin films or nanowires attached within bulk objects ; however it also applies more generally whenever a system is restricted to fill only part of its allocated phase space 2 .For instance , this situation occurs commonly during crystal growth where defects could be applied into the lattice structure by impurities 3 , or when examining colloidal suspensions 4 . In this research we define the case of a thin glass with periodic microstructure , whose thickness g lies between two width scales L and d ( see Fig 1 ) .Here L represents the typical size of the unit cell while d indicates the typical spacing between neighboring layers ; both quantities are expected to be much smaller than the in - plane dimensions of the sample . Such structures appear often in nature , e . g . , in layered compounds like graphite 5 , transition copper dichalcogenides 6 , and hexagonal boron nitride 7 .They are also used heavily in technological use ranging from photovoltaics 8 to optoelectronics 9 . Figure 1 : Schematic illustration of our model topology .A narrow film with periodic microstructures is confined to lying on top of a rigid coating .",
        "rewrite_text": "We investigate the elasticity of thin films featuring periodic microstructures that are constrained by an underlying substrate. Our findings reveal that these structures can exhibit unexpectedly high values for both Young's moduli and Poisson ratios. The underlying cause of these phenomena stems from the presence of phonon soft modes linked to the periodicity along the film's primary direction. These insights have implications for the development of new materials with customized elastic properties. In recent decades, there has been an increasing interest in understanding how confinement affects the physical behavior of matter at the nanoscale. This issue is particularly relevant when considering thin films or nanowires integrated within bulk materials, but it is also applicable more broadly whenever a system is limited to occupying only part of its designated phase space. For example, this scenario is frequently encountered during crystal growth, where defects may be introduced into the lattice structure due to impurities, or in the study of colloidal suspensions. In this study, we focus on a thin glass with a periodic microstructure, characterized by a thickness \\( g \\) that falls between two width scales \\( L \\) and \\( d \\) (see Fig. 1). Here, \\( L \\) represents the typical size of the unit cell, while \\( d \\) denotes the typical spacing between adjacent layers; both measurements are anticipated to be significantly smaller than the in-plane dimensions of the sample. Such structures are commonly found in nature, for example, in layered compounds like graphite, transition metal dichalcogenides, and hexagonal boron nitride. They are also extensively utilized in various technological applications, from photovoltaics to optoelectronics. Figure 1 depicts a schematic illustration of our model topology, showing a narrow film with periodic microstructures constrained atop a rigid substrate.",
        "ori-fast-z-score": -1.3926212476455828,
        "water-fast-z-score": 6.614950926316518,
        "rewrite-fast-z-score": 0.5262348115842176
    },
    {
        "original_text": "In this talk, we will discuss the relationship between loop spaces and Langlands parameters in terms of their connections to representation theory.  We will begin by recalling some basic facts about loop spaces and their relation with infinite-dimensional Lie groups (e.g., loop groups).  Next, we ll recall how one can associate a certain type of infinite-dimensional Lie group called an affine Hecke algebra to any reductive algebraic group over a field k of characteristic 0.   Finally, we ll explain how these two ideas are related via the notion of a Harish-Chandra bimodule. The main result is that if G is a connected semisimple complex algebraic group defined over Q then there exists a natural isomorphism between the category of finite-dimensional representations of G(Q) and the category of Harish-Chandra modules for its associated affine Hecke algebra. This work was done jointly with David Vogan at Harvard University during my postdoctoral fellowship.",
        "watermark_text": "In this talk , we will explore the relationship between loop spaces and Langlands parameters in terms of their connections to representation theory . We will start by examining some fundamental details about loop spaces and their connection with infinite - dimensional Lie fields ( e . g . , loop spaces ) .Next , we ll remember how one can associate a certain type of infinite - dimensional Lie category called an affine Hecke algebra to any reductive algebraic ring over a field k of characteristic 0 . Finally , we ll explain how these two ideas are related via the notion of a Harish - Chandra bimodule .The main consequence is that if G is a connected semisimple complex algebraic ring defined over Q then there exists a natural isomorphism between the category of finite - dimensional representations of G ( Q ) and the category of Harish - Chandra modules for its associated affine Hecke algebra . This research was done jointly with David Vogan at Harvard University during my postdoctoral fellowship .",
        "rewrite_text": "In this presentation, we will investigate the interplay between loop spaces and Langlands parameters, particularly focusing on their ties to representation theory. We will begin by delving into the essential aspects of loop spaces and their relation to infinite-dimensional Lie groups. Subsequently, we will discuss how to associate a specific type of infinite-dimensional Lie category known as an affine Hecke algebra with any reductive algebraic group over a field \\( k \\) of characteristic 0. Finally, we will clarify how these concepts are interconnected through the idea of a Harish-Chandra bimodule. A key takeaway is that if \\( G \\) is a connected semisimple complex algebraic group defined over \\( \\mathbb{Q} \\), there is a natural isomorphism between the category of finite-dimensional representations of \\( G(\\mathbb{Q}) \\) and the category of Harish-Chandra modules for its corresponding affine Hecke algebra. This research was conducted in collaboration with David Vogan at Harvard University during my postdoctoral fellowship.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 1.2135597524338357
    },
    {
        "original_text": "The Large Area Telescope (LAT) is one of two instruments aboard NASA s Fermi Gamma-ray Space Telescope, launched in June 2008. The LAT consists of an array of silicon strip trackers and cesium iodide calorimeters that detect gamma rays with energies between 20 MeV to more than 300 GeV. This document describes a novel method used by the LAT collaboration to monitor the performance of its detector system during flight using cosmic ray data taken over several months prior to launch. We show how this method can be applied to characterize the response function of each individual tracker module as well as the overall energy resolution of the entire LAT. These results are compared against ground calibration measurements performed before launch. Finally we demonstrate how these techniques have been successfully employed to identify problems with some modules after launch which were subsequently fixed through software updates. The Large Area Telescope (L AT ) is one of two instruments flown on NASA s Fermi Gamma-Ray Space Telescope  1  . Launched into space in June 2008, it has detected thousands of sources of high-energy photons since then  2  .\nIn order to perform such observations, the L AT must accurately measure the direction and energy of incoming photons. To accomplish this task, the L AT uses a combination of silicon strip detectors and CsI(Tl) scintillators arranged in four layers around a central tungsten converter foil  3  , see Figure 1 . Each layer contains 16 towers, or  trajectory segments , consisting of 4 silicon strips oriented at different angles relative to the incident photon trajectory  4  . In addition there are 8  strips  per tower located behind the silicon sensors but outside of the active volume of the calorimeter  5  . Together they form a total of 56 independent tracking channels  6  .",
        "watermark_text": "The Large Area Telescope ( LAT ) is one of two instruments aboard NASA s Fermi Gamma - ray Space Telescope , launched in June 2008 . The LAT consists of an ensemble of silicon strip trackers and cesium iodide calorimeters that detect beta particles with energies between 20 MeV to more than 300 GeV .This text explains a new method employed by the LAT collaboration to study the performance of its detector network during mission utilizing cosmic ray data taken over several months previously to launch . We indicate how this technology can be applied to characterize the response function of each individual tracker module as also as the overall energy resolution of the entire LAT .These data are compared against ground calibration measurements completed before flight . Finally we prove how these procedures have been successfully utilized to identify issues with some modules after launch which were later resolved through technology updates .The Large Area Telescope ( L AT ) is one of two instruments flown on NASA s Fermi Gamma - Ray Space Telescope 1 . Launched into space in June 2008 , it has detected many of sources of high - energy photons since then 2 .In order to conduct such observations , the L AT requires properly study the direction and energy of incoming photons . To accomplish this objective , the L AT employs a combination of silicon strip detectors and CsI ( Tl ) scintillators grouped in four layers around a central tungsten converter foil 3 , see Figure 1 .Each layer contains 16 towers , or path sectors , consisting of 4 silicon patches aligned at different angles relative to the incident photon trajectory 4 . In addition there are 8 layers per tower situated behind the silicon detector but outside of the active volume of the calorimeter 5 .Together they create a total of 56 independent tracking channels 6 .",
        "rewrite_text": "The Large Area Telescope (LAT) is one of the two instruments aboard NASA's Fermi Gamma-ray Space Telescope, which was launched in June 2008. The LAT is composed of a collection of silicon strip trackers and cesium iodide calorimeters, designed to detect beta particles with energy levels ranging from 20 MeV to over 300 GeV. This document describes a newly developed method by the LAT collaboration to assess the performance of its detector network during the mission, utilizing cosmic ray data collected in the months leading up to the launch. We demonstrate how this technology can be used to characterize the response function of each individual tracker module, as well as the overall energy resolution of the LAT. These findings are compared with ground calibration measurements that were performed prior to the flight. Additionally, we illustrate how these procedures have effectively pinpointed issues with certain modules after the launch, which were subsequently addressed through technological updates. Since its launch in June 2008, the LAT has successfully detected numerous sources of high-energy photons. To facilitate these observations, the LAT is designed to accurately determine the direction and energy of incoming photons. It employs a combination of silicon strip detectors and CsI(Tl) scintillators organized into four layers surrounding a central tungsten converter foil. Each layer consists of 16 towers, or path sectors, with four silicon patches arranged at various angles to the incoming photon trajectory. Additionally, there are eight layers per tower positioned behind the silicon detectors but outside the active volume of the calorimeter, culminating in a total of 56 independent tracking channels.",
        "ori-fast-z-score": 0.3682298471593294,
        "water-fast-z-score": 7.425257825928512,
        "rewrite-fast-z-score": 2.2883102141894214
    },
    {
        "original_text": "We propose that the energy release process in coronal mass ejections (CMEs) is due to two different physical mechanisms, magnetic reconnection and catastrophic loss of equilibrium driven by ideal magnetohydrodynamic (MHD) instabilities.  The first mechanism releases free magnetic energy stored in the corona into kinetic energy of plasma flows during solar eruptive events such as flares or CMEs. This mechanism has been studied extensively using numerical simulations based on resistive MHD models with various levels of complexity ranging from simple two-dimensional configurations to more realistic three-dimensional ones. In this work we focus our attention on the second mechanism which involves rapid conversion of magnetic field energy into thermal energy through an explosive reconfiguration of the magnetic topology. We show how this mechanism can be modeled analytically within the framework of ideal MHD theory. Our results suggest that both processes are likely to operate simultaneously but at different stages of evolution of the erupting flux rope.",
        "watermark_text": "We suggest that the electricity release process in coronal mass ejections ( CMEs ) is due to two different mechanical pathways , electric reconnection and catastrophic loss of equilibrium caused by ideal magnetohydrodynamic ( MHD ) instabilities . The first mechanism transfers loose magnetic energy stored in the corona into kinetic power of plasma flows during solar eruptive events such as flares or CMEs .This mechanism has been studied frequently using numerical simulations based on resistive MHD models with various amounts of complexity including from basic two - dimensional shapes to more realistic three - dimensional ones . In this research we focus our focus on the second process which includes rapid transfer of magnetic field energy into heat power through an explosive reconfiguration of the magnetic topology .We indicate how this mechanism can be described analytically within the framework of optimal MHD model . Our results show that both processes are likely to work simultaneously but at different stages of evolution of the erupting flux rope .",
        "rewrite_text": "We propose that the electricity release process in coronal mass ejections (CMEs) occurs through two distinct mechanical pathways: magnetic reconnection and a catastrophic loss of equilibrium resulting from ideal magnetohydrodynamic (MHD) instabilities. The first pathway facilitates the conversion of loosely stored magnetic energy in the corona into the kinetic energy of plasma flows during solar eruptions, such as flares or CMEs. This mechanism has been extensively examined through numerical simulations using resistive MHD models, which vary from simple two-dimensional configurations to more complex three-dimensional representations. In this study, we concentrate on the second mechanism, which involves the rapid conversion of magnetic field energy into thermal energy through an explosive reconfiguration of magnetic topology. We illustrate how this process can be analytically represented within the framework of the optimal MHD model. Our findings indicate that both mechanisms likely operate concurrently, albeit at different stages in the evolution of the erupting flux rope.",
        "ori-fast-z-score": 0.6793662204867574,
        "water-fast-z-score": 7.0201176116964925,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "In this article we study the structure of smooth foliations on closed manifolds, which are given by submersions whose fibers have only Morse singularities (i.e., they look like graphs over their tangent spaces). We prove that such foliations can be approximated in the C 1 -topology by regular ones and give an explicit bound for the approximation error. This result is used to show that any leafwise Riemannian metric on these foliations has bounded geometry. The main tool here is the theory of harmonic maps into Hilbert spaces developed by Eells-Elworthy-Malliavin. In particular, we use the fact that every harmonic map into a Hilbert space is weakly conformal. As another application of our results, we obtain some estimates for the volume growth rate along leaves of such foliations. Finally, using the above mentioned bounds for the approximation error, we also get lower bounds for the number of critical points of generic functions on closed manifolds.",
        "watermark_text": "In this article we study the composition of smooth foliations on closed manifolds , which are given by submersions whose strands have only Morse singularities ( i . e . , they look like graphs over their tangent spaces ) . We conjecture that such foliations can be approximated in the C 1 - topology by regular ones and giving an explicit bound for the approximation error .This result is utilized to say that any leafwise Riemannian metric on these foliations has finite geometry . The main tool here is the notion of harmonic maps into Hilbert spaces introduced by Eells - Elworthy - Malliavin .In particular , we utilize the fact that every harmonic mapping into a Hilbert space is mildly conformal . As another application of our findings , we obtain some measurements for the density expansion speed along leaves of such foliations .Finally , using the above mentioned bounds for the approximation error , we also get lower bounds for the number of important points of generic functions on shut manifolds .",
        "rewrite_text": "In this article, we examine the structure of smooth foliations on closed manifolds that are defined by submersions, where the strands exhibit only Morse singularities (i.e., they resemble graphs over their tangent spaces). We propose a conjecture that these foliations can be approximated in the \\(C^1\\) topology by regular foliations, providing an explicit estimate for the approximation error. This finding allows us to assert that any leafwise Riemannian metric on these foliations exhibits finite geometry. A key concept we employ is the theory of harmonic maps into Hilbert spaces, as developed by Eells, Elworthy, and Malliavin. Notably, we leverage the property that every harmonic map into a Hilbert space is mildly conformal. Furthermore, our results yield calculations regarding the rate of density expansion along the leaves of such foliations. Finally, by applying the established bounds for the approximation error, we also derive lower bounds for the number of significant points of generic functions on closed manifolds.",
        "ori-fast-z-score": -1.6378460497066512,
        "water-fast-z-score": 3.25,
        "rewrite-fast-z-score": -1.9694638556693236
    },
    {
        "original_text": "The Very Large Array (VLA) is an array of 27 radio telescopes located in New Mexico, USA. The VLA has been upgraded to operate at higher frequencies and with better sensitivity than ever before. In this talk I will describe how we have developed new techniques for operating the system at 74 MHz using digital beamforming technology. This work was done as part of my PhD thesis under the supervision of Prof. David Diamond. My research interests are in developing software systems that can be used by astronomers to analyse data produced by large observational facilities such as the VLA. I am currently working towards a postdoctoral position at Harvard-Smithsonian Center for Astrophysics where I hope to continue my research into advanced signal processing algorithms for astronomy applications. Keywords: Radio Astronomy, Digital Signal Processing, Software Engineering, Data Analysis, Beamforming Algorithms, Very Large Array, 74 MHz",
        "watermark_text": "The Very Large Array ( VLA ) is an array of 27 radio telescopes located in New Mexico , USA . The VLA has been upgraded to work at higher speeds and with improved accuracy than ever before .In this talk I will explain how we have developed novel techniques for running the device at 74 MHz using digital beamforming technology . This project was done as part of my PhD thesis under the guidance of Prof . David Diamond .My research interests are in building software applications that can be used by astronomers to analyse information produced by large observational facilities such as the VLA . I am currently working towards a postdoctoral position at Harvard - Smithsonian Center for Astrophysics where I aim to pursue my research into advanced information processing algorithms for astronomy applications .Keywords : Radio Astronomy , Digital Signal Processing , Software Engineering , Data Analysis , Beamforming Algorithms , Very Large Array , 74 MHz",
        "rewrite_text": "The Very Large Array (VLA) consists of 27 radio telescopes situated in New Mexico, USA. Recently, the VLA has been enhanced to operate with greater speed and precision than ever before. In this presentation, I will detail the innovative techniques we developed for operating the array at 74 MHz using digital beamforming technology. This work was part of my PhD thesis under the mentorship of Prof. David Diamond. My research interests focus on creating software applications that assist astronomers in analyzing data generated by major observational facilities like the VLA. I am currently working towards a postdoctoral position at the Harvard-Smithsonian Center for Astrophysics, where I plan to further my research on advanced information processing algorithms for astronomy applications. Keywords: Radio Astronomy, Digital Signal Processing, Software Engineering, Data Analysis, Beamforming Algorithms, Very Large Array, 74 MHz.",
        "ori-fast-z-score": -0.282842712474619,
        "water-fast-z-score": 5.091168824543142,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "The Standard Model is in excellent agreement with all current experimental data, but it leaves many questions unanswered and fails to provide an explanation for some phenomena observed experimentally.  The muon magnetic moment anomaly provides one such example where there are significant discrepancies between theory predictions and experiment measurements that cannot be explained within the Standard Model framework.   In this talk I will present the physics case for the new g-2 experiment at Fermilab which aims to measure the anomalous magnetic moment of the muon more accurately than ever before by using a novel technique based on laser cooling and trapping techniques developed over recent years.   ... This talk presents the physics case for the proposed new measurement of the muon s anomalous magnetic moment at Fermilab. It describes how the use of laser cooling and trapping can lead to a dramatic improvement in precision compared to previous experiments. A number of other topics related to the project are also discussed including the status of the R&D program towards the goal of measuring the muon magnetic moment to 0.5 parts per million accuracy.",
        "watermark_text": "The Standard Model is in good agreement with all recent experimental evidence , but it leaves many issues unanswered and fails to provide an reason for some phenomena observed experimentally . The muon magnetic point anomaly gives one such example where there are significant discrepancies between theoretical estimates and observation observations that cannot be described within the Standard Model framework .In this talk I will present the physics case for the new g - 2 study at Fermilab which aims to measure the anomalous magnetic motion of the muon more accurately than ever before by using a innovative method based on laser cooling and trapping techniques established over recent generations . . . .This discussion presents the physics case for the suggested novel measurement of the muon s anomalous magnetic force at Fermilab . It details how the using of laser cooling and trapping can lead to a dramatic improvement in precision compared to previous study .A variety of other topics related to the project are also discussed including the status of the R & D program towards the objective of calculating the muon magnetic moment to 0 . 5 parts per million accuracy .",
        "rewrite_text": "The Standard Model aligns well with recent experimental data, yet it leaves many questions unanswered and does not account for certain observed phenomena. One notable example is the muon magnetic moment anomaly, which reveals significant discrepancies between theoretical predictions and experimental results that the Standard Model cannot explain. In this presentation, I will outline the scientific rationale behind the new g-2 experiment at Fermilab, which aims to achieve unprecedented accuracy in measuring the anomalous magnetic behavior of muons through innovative techniques involving laser cooling and trapping developed over recent generations. This talk will delve into the physics behind the proposed measurement of the muon's anomalous magnetic force at Fermilab, highlighting how the implementation of laser cooling and trapping can significantly enhance precision compared to earlier studies. Additionally, I will cover various related topics, including the current status of the research and development program aimed at calculating the muon magnetic moment with an accuracy of 0.5 parts per million.",
        "ori-fast-z-score": 1.5096588248481377,
        "water-fast-z-score": 7.701031252562294,
        "rewrite-fast-z-score": 1.585187847802434
    },
    {
        "original_text": "The origin of the most massive stars is still an open question in astrophysics, as well as their role in shaping galactic evolution. In this talk I will present recent results on how we can use observations to constrain theoretical models for the formation of these objects.  The first part of my talk will focus on the observational properties of young massive clusters (YMCs) that are found at high redshift z>6-7. These YMCs have masses up to 10^8 Msun and sizes of ~1kpc. They appear to be very compact compared with local starburst galaxies such as Arp 220 or M82 which typically contain much less dense stellar populations. We find that the observed size-mass relation of these distant YMCs agrees remarkably well with predictions based on numerical simulations of turbulent gas clouds collapsing under self-gravity. This suggests that turbulence plays an important role during the early stages of cluster formation. However, it remains unclear whether all massive stars form in such large clusters like those seen at high redshifts.",
        "watermark_text": "The origin of the most large objects is still an open question in astrophysics , as well as their role in shaping galactic progression . In this talk I will present recent results on how we can using observations to constrain theoretical theories for the formation of these objects .The first part of my talk will focus on the observational properties of young massive clusters ( YMCs ) that are found at high redshift z > 6 - 7 . These YMCs have masses up to 10 ^ 8 Msun and dimensions of ~ 1kpc .They seem to be very small compared with local starburst clusters such as Arp 220 or M82 which commonly feature considerably less dense stellar groups . We see that the observed height - mass balance of these distant YMCs follows perfectly well with predictions based on numerical simulations of turbulent gas clouds collapsing under self - gravity .This implies that turbulence plays an important role during the early stages of cluster structure . However , it remains unsure whether all huge objects create in such large clusters like those observed at high redshifts .",
        "rewrite_text": "The origins of the largest astronomical objects remain a significant question in astrophysics, particularly regarding their influence on the evolution of galaxies. In this presentation, I will share recent findings on how observations can help refine theoretical models for the formation of these objects. The first part of my talk will concentrate on the observational characteristics of young massive clusters (YMCs) identified at high redshifts (z > 6 - 7). These YMCs can have masses as large as 10^8 solar masses and span approximately 1 kpc in size. They appear notably smaller than local starburst clusters such as Arp 220 or M82, which typically consist of less densely packed stellar groups. Our observations indicate that the height-mass relationship of these distant YMCs aligns well with predictions from numerical simulations of turbulent gas clouds collapsing under their own gravity. This suggests that turbulence plays a crucial role in the early formation of cluster structures. However, it remains uncertain whether all massive objects originate from such large clusters as those observed at high redshifts.",
        "ori-fast-z-score": 0.7875615306482168,
        "water-fast-z-score": 6.340751391209736,
        "rewrite-fast-z-score": 1.7457431218879391
    },
    {
        "original_text": "We report on the optical and near-infrared afterglows of the short-hard burst GRB 050802 detected by Swift/BAT on May 2nd, 2005 at 07:55:06 UT (T0). The prompt emission was followed by an X-ray flare peaking at T0+500 s in the rest frame. We find that both components are well described by power laws with decay indices α1 = 1.2 ± 0.3 for t < 10 ks and α2 = 2.5 ± 0.4 for t > 10 ks. A break is observed between these two regimes around t0 + 20 ks. No evidence for spectral evolution or extinction has been found within each component. Our results suggest that this event may be similar to GRB 021004 which also showed a double-power law behaviour but without any significant spectral evolution across the break time. This suggests that the physical mechanism responsible for the late-time steepening could be related to the one producing the early shallow decline. \n \n Keywords: Gamma-ray burst",
        "watermark_text": "We report on the optical and far - infrared afterglows of the short - hard burst GRB 050802 detected by Swift / BAT on May 2nd , 2005 at 07 : 55 : 06 UT ( T0 ) . The prompt emission was followed by an X - ray flare peaking at T0 + 500 s in the rest frame .We see that both components are better characterised by power laws with decay indices α1 = 1 . 2 ± 0 . 3 for t < 10 ks and α2 = 2 . 5 ± 0 . 4 for t > 10 ks . A break is observed between these two regimes around t0 + 20 ks .No support for spectral evolution or extinction has been shown within each component . Our results propose that this event may be similar to GRB 021004 which also demonstrated a double - energy law evolution but without any considerable brightness progression across the break time .This implies that the physical process cause for the late - time steepening could be connected to the one generating the early shallow collapse . Keywords : Gamma - ray burst",
        "rewrite_text": "We present our findings on the optical and far-infrared afterglows of the short and hard gamma-ray burst GRB 050802, which was detected by Swift/BAT on May 2, 2005, at 07:55:06 UT (T0). The prompt emission was succeeded by an X-ray flare that peaked at T0 + 500 seconds in the rest frame. Our analysis indicates that both components are well described by power laws, with decay indices of α1 = 1.2 ± 0.3 for t < 10 ks and α2 = 2.5 ± 0.4 for t > 10 ks. A transition is observed around T0 + 20 ks between these two phases. We find no evidence of spectral evolution or extinction within each component. Our results suggest that this burst may be akin to GRB 021004, which also exhibited a double-energy law evolution but did not show significant brightness changes at the break time. This indicates that the underlying physical processes responsible for the late-time steepening might be related to those causing the early shallow decay. Keywords: Gamma-ray burst.",
        "ori-fast-z-score": 0.6401843996644799,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "We present the results of a numerical study on the relaxation dynamics of an initially straight contact line in a two-dimensional geometry, which is driven by surface tension and viscous dissipation at the moving interface between liquid and gas phases.  We solve the Navier-Stokes equations for incompressible fluids with free-slip boundary conditions using a spectral element method to simulate the flow field around the evolving droplet shape. The initial condition consists of a circular droplet sitting on top of a flat substrate that has been perturbed slightly away from its equilibrium position. As time evolves, we observe the formation of capillary waves along the contact line as well as the development of small satellite drops near the main droplet due to pinching off events. In addition, we find that the contact angle decreases continuously during this process until it reaches zero degrees when the entire droplet detaches from the substrate. Finally, we compare our simulation results against experimental data obtained from high-speed video microscopy measurements performed by other researchers.",
        "watermark_text": "We present the conclusion of a numerical investigation on the relaxation behavior of an initially parallel contact line in a two - dimensional topology , which is generated by surface friction and viscous dissipation at the moved junction between liquid and gas phases . We calculate the Navier - Stokes equations for incompressible gases with loose - slipping border conditions utilizing a spectral component process to simulate the flow field around the evolving droplet shape .The initial condition consists of a circular droplet standing on top of a flattened substrate that has been perturbed slightly apart from its stable position . As period evolves , we monitor the formation of capillary currents along the contact line as also as the development of tiny satellite flows near the main droplet thanks to pinching off events .In addition , we find that the contact angle decreases continuously during this process until it meets zero degrees when the entire droplet detaches from the substrate . Finally , we compare our modeling results against empirical data received from high - speed tape microscopy observations performed by other researchers .",
        "rewrite_text": "We present the findings from a numerical study on the relaxation dynamics of an initially parallel contact line in a two-dimensional setting, influenced by surface friction and viscous dissipation at the moving interface between liquid and gas phases. Our approach involves solving the Navier-Stokes equations for incompressible gases with loosely slipping boundary conditions, using a spectral component method to model the flow field surrounding the changing droplet shape. The initial setup features a circular droplet positioned on a flattened substrate, slightly displaced from its stable position. As time progresses, we observe the development of capillary currents along the contact line, as well as the emergence of small satellite flows near the main droplet due to pinching events. We also note that the contact angle gradually decreases throughout this process, reaching zero degrees when the droplet completely detaches from the substrate. Finally, we compare our simulation results with empirical data obtained from high-speed tape microscopy conducted by other researchers.",
        "ori-fast-z-score": -0.7592566023652966,
        "water-fast-z-score": 7.267170336924982,
        "rewrite-fast-z-score": 2.4379951240146283
    },
    {
        "original_text": "We study the effect of rounding on the dynamics of complex networks with first-order phase transition (FPT). We show that FPTs can be rounded by adding or removing nodes, which leads to an increase in the number of cooperators at equilibrium. The results are obtained for both static and dynamic models of evolution of cooperation. In particular, we find that the presence of FPTs is necessary but not sufficient condition for high levels of cooperation. Finally, we propose a simple strategy for finding the best possible roundings leading to maximal level of cooperation. Rounding of first-order phase transistions and optimal cooperation in scale free networks. P. Krawczyk 1 , A. Szolnoki 2 . \n1 Institute of Physics, University of Warsaw, Poland; krawczykp@wip.waw.pl .\n2 Department of Mathematics, University of Warsaw, Warsaw, Poland; aszolnok@wip. waw.pl .\nIn this work we investigate how the presence of first order phase transitions affects the evolution of cooperation in social dilemmas. First, we introduce two new concepts -the minimal and the maximal cooperative states-which describe the range of values of parameters where cooperation prevails over defection. Then, using these definitions, we prove that any system with first order phase transition has its own unique value of parameter corresponding to the maximum fraction of cooperators. Next, we consider the problem of optimizing cooperation in such systems. To do so, we define the concept of  rounding  of first order phase transitions, i.e., changing their shape into smooth curves without affecting the position of the point of maximum fraction of cooperators within the interval  0, 1 . Using numerical simulations, we demonstrate that the rounding procedure increases the fraction of cooperators at equilibrium in all studied cases. Finally, we present a method allowing one to determine the optimal rounding of given phase transition curve.",
        "watermark_text": "We research the impact of rounding on the dynamics of complex networks with first - order phase shift ( FPT ) . We see that FPTs can be rounded by added or removing nodes , which results to an increase in the proportion of cooperators at equilibrium .The results are derived for both static and dynamic theories of evolution of cooperation . In particular , we find that the presence of FPTs is required but not sufficient condition for high levels of cooperation .Finally , we propose a simple plan for finding the best possible roundings led to maximal level of partnership . Rounding of initial - order phase transistions and optimal cooperation in scale free networks .P. Krawczyk 1 , A. Szolnoki 2 .1 Institute of Physics, University of Warsaw, Poland; krawczykp@wip.waw.pl .2 Department of Mathematics, University of Warsaw, Warsaw, Poland; aszolnok@wip.waw.pl .In this research we investigate how the presence of first order phase transitions affects the evolution of agreement in social dilemmas . First , we provide two new concepts - the reduced and the maximal cooperative states - which describe the range of values of parameters where cooperation prevails over defection .Then , using these concepts , we prove that any system with first order phase change has its own unique value of parameter relating to the maximum amount of cooperators . Next , we study the question of optimizing cooperation in such systems .To do so , we define the notion of rounding of first order phase transitions , i . e . , changing their shape into smooth paths without affecting the orientation of the point of maximum fraction of cooperators within the period 0 , 1 . Using numerical simulations , we prove that the rounding procedure increases the fraction of cooperators at equilibrium in all observed situations .Finally , we present a technique allowing one to estimate the ideal rounding of given transition transition curve .",
        "rewrite_text": "We investigate the effects of rounding on the dynamics of complex networks characterized by first-order phase transitions (FPTs). Our findings demonstrate that FPTs can be influenced by adding or removing nodes, leading to a higher proportion of cooperators at equilibrium. These results are applicable to both static and dynamic theories concerning the evolution of cooperation. Notably, we establish that while the presence of FPTs is necessary for achieving high levels of cooperation, it alone is not sufficient. Furthermore, we suggest a straightforward approach for identifying the most effective rounding strategies that maximize cooperative interactions. Our research delves into rounding first-order phase transitions and achieving optimal cooperation in scale-free networks. This study, conducted by P. Krawczyk from the Institute of Physics at the University of Warsaw and A. Szolnoki from the Department of Mathematics at the same institution, explores how first-order phase transitions influence consensus in social dilemmas. We introduce two important concepts—the reduced and maximal cooperative states—that delineate the parameter values under which cooperation dominates over defection. We demonstrate that each system undergoing a first-order phase transition possesses a specific unique parameter value that corresponds to the maximum proportion of cooperators. Subsequently, we address the optimization of cooperation in these systems by defining the rounding of first-order phase transitions, which involves reshaping their curves while preserving the orientation of the maximum cooperators’ point within the interval [0, 1]. Through numerical simulations, we confirm that the rounding process increases the proportion of cooperators at equilibrium across all scenarios examined. Finally, we introduce a method for estimating the optimal rounding for a given transition curve.",
        "ori-fast-z-score": 0.6211495565912797,
        "water-fast-z-score": 6.779471978707304,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "We study the fractal dimension of domain walls (DWs) in two dimensional Ising spin glasses with nearest neighbor interactions and random bonds using Monte Carlo simulations at finite temperatures. We find that DWs are fractals for all values of temperature studied here, i.e., T = 0.5J/kB to 1.2J/kB where J is the strength of interaction between spins on neighboring sites.  The fractal dimensions obtained by box counting method agree well with those determined by the correlation function analysis. In addition we show that the fractal dimension decreases as the temperature increases. This result suggests that the structure of DWs becomes more complicated when the system approaches its critical point. Finally it should be noted that our results can also be applied to other systems such as vortex lines in type-II superconductors or dislocation networks in crystals. Two-dimensional Ising spin glasses have been extensively investigated both experimentally  1  and theoretically  2  . It has been shown that these models exhibit many interesting phenomena including phase transitions  3  , spin-glass states  4  , and glassy dynamics  5  .\nIn this work we focus on one particular aspect of the model which is the fractal nature of domain walls  6  . Domain wall refers to an interface separating different ordered phases  7, 8  . For example, in ferromagnetic materials there exist two types of domains; up and down magnetization  9  . These domains are separated by interfaces called domain walls  10  . Similarly, in antiferromagnets  11  , there exists four possible orientations of magnetic moments  12  ; three of them form triangular sublattices while the fourth forms a square lattice  13  . Therefore, there will be six types of domain walls  14  .",
        "watermark_text": "We research the fractal dimension of domain walls ( DWs ) in two dimensional Ising spin glasses with nearest neighbor interactions and random bonds using Monte Carlo simulations at finite temperatures . We see that DWs are fractals for all values of temperature described here , i . e . , T = 0 . 5J / kB to 1 . 2J / kB where J is the strength of coupling between spins on nearby locations .The fractal sizes obtained by box counting method comply better with those determined by the correlation function analysis . In addition we prove that the fractal dimension decreases as the temperature increases .This result suggests that the composition of DWs changes more complicated when the process approaches its critical position . Finally it should be mentioned that our findings can also be applied to other structures such as vortex lines in type - II superconductors or dislocation networks in crystals .Two - dimensional Ising spin glasses have been heavily examined both experimentally 1 and theoretically 2 . It has been shown that these models exhibit several interesting phenomena including phase transitions 3 , spin - glass states 4 , and glassy dynamics 5 .In this research we focus on one special aspect of the model which is the fractal nature of domain barriers 6 . Domain wall refers to an interface separating different ordered phases 7 , 8 .For instance , in ferromagnetic metal there exist two forms of residues ; up and down magnetization 9 . These residues are separated by interfaces called domain barriers 10 .Similarly , in antiferromagnets 11 , there exists four possible orientations of magnetic moments 12 ; three of them form square sublattices while the fifth forms a square lattice 13 . Therefore , there will be six kinds of domain walls 14 .",
        "rewrite_text": "We investigate the fractal dimension of domain walls (DWs) in two-dimensional Ising spin glasses with nearest neighbor interactions and random bonds using Monte Carlo simulations conducted at finite temperatures. Our findings indicate that DWs exhibit fractal properties across all temperatures studied, ranging from T = 0.5J/kB to 1.2J/kB, where J represents the coupling strength between spins at adjacent sites. The fractal dimensions determined through the box counting method align more closely with those derived from correlation function analysis. Additionally, we demonstrate that the fractal dimension decreases as the temperature rises, suggesting that the structure of the DWs becomes more complex as the system approaches its critical point. It is worth noting that our results may also be relevant to other systems, such as vortex lines in type-II superconductors or dislocation networks in crystals. Two-dimensional Ising spin glasses have been the subject of extensive experimental and theoretical investigations. It has been established that these models exhibit several intriguing phenomena, including phase transitions, spin-glass states, and glassy dynamics. This study specifically focuses on the fractal characteristics of domain walls, which serve as interfaces between distinct ordered phases. For example, in ferromagnetic metals, two types of magnetization (up and down) exist, separated by domain walls. Similarly, in antiferromagnets, four orientations of magnetic moments can be found, with three forming square sublattices and one following a square lattice arrangement, resulting in six distinct types of domain walls.",
        "ori-fast-z-score": 0.9205746178983234,
        "water-fast-z-score": 6.812252172447593,
        "rewrite-fast-z-score": 0.4622501635210242
    },
    {
        "original_text": "We present new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377, obtained with the Wide Field Planetary Camera 2 (WFPC2). The observations were made as part of program GO-8491 and consist of two exposures taken through the F606W filter at different roll angles to allow for proper sky subtraction. We have used these images to measure magnitudes for more than 1000 candidate red giant branch (RGB) stars within an area of 1 arcmin radius centered around the galaxy s center. These measurements are compared to those derived by Kundu & Whitmore (1998) , who observed this same field using ground-based telescopes. Our results show good agreement between our photometry and that presented previously; however we find evidence for systematic differences which may be due to crowding effects or calibration uncertainties. \n \n Keywords: Red Giant Branch, Galaxy, WFPC2",
        "watermark_text": "We report new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377 , obtained with the Wide Field Planetary Camera 2 ( WFPC2 ) . The images were made as part of series GO - 8491 and consist of two exposures took through the F606W filter at different roll axes to allow for correct sky subtraction .We have utilized these images to measure magnitudes for more than 1000 candidate red giant line ( RGB ) stars within an area of 1 arcmin radius centered around the universe s center . These measurements are compared to those derived by Kundu & Whitmore ( 1998 ) , who studied this same field using ground - based telescopes .Our results show good agreement between our photometry and that presented previously ; however we find proof for systematic differences which may be due to crowding effects or calibration uncertainties . Keywords : Red Giant Branch , Galaxy , WFPC2",
        "rewrite_text": "We present new photometric data from HST on halo stars in the nearby elliptical galaxy NGC 3377, obtained using the Wide Field Planetary Camera 2 (WFPC2). The imaging, part of the GO-8491 series, includes two exposures taken through the F606W filter at different roll angles to facilitate accurate sky subtraction. From these images, we measured magnitudes for over 1,000 candidate red giant branch (RGB) stars in a region with a radius of 1 arcminute, centered on the galaxy's core. Our measurements were compared with those of Kundu & Whitmore (1998), who examined the same field using ground-based telescopes. While our results are generally consistent with their photometry, we identify systematic differences that may stem from crowding effects or calibration uncertainties. Keywords: Red Giant Branch, Galaxy, WFPC2.",
        "ori-fast-z-score": 1.4832396974191326,
        "water-fast-z-score": 4.449719092257398,
        "rewrite-fast-z-score": 0.2886751345948129
    },
    {
        "original_text": "We prove that there exists an open set of initial data such that the corresponding solution to the rapidly rotating shallow water (RSW) or Euler equations is globally smooth in space-time, provided it stays sufficiently close to some equilibrium state. The proof relies on a combination of energy estimates with suitable weighted Sobolev inequalities. We also show how our results can be extended to more general systems of conservation laws which are not necessarily strictly hyperbolic. In particular we consider the case where one of the characteristic speeds vanishes at a point but remains bounded away from zero elsewhere. \nIntroduction\n\nThe purpose of this work is twofold. First, we establish global regularity properties for the rapidly rotating shallow water system and its generalization to arbitrary dimension. Second, we provide new insights into the structure of these models by showing how they fit within a larger class of nonlinear dispersive partial differential equations. Our main result reads as follows: \nMain Theorem 1 Let u 0 ∈ H s , s > n/2 + 2, then there exist constants C = C(n), K = K(n) such that if",
        "watermark_text": "We prove that there exists an open set of initial data such that the equivalent solve to the rapidly spinning shallow water ( RSW ) or Euler equations is globally smooth in space - time , provided it remains reasonably close to some equilibrium state . The proof uses on a combination of power estimates with suitable weighted Sobolev inequalities .We additionally understand how our findings can be generalized to more general systems of conservation laws which are not necessarily strictly hyperbolic . In particular we define the case where one of the characteristic velocity vanishes at a place but stayed defined far from zero elsewhere .Introduction The purpose of this study is twofold . First , we obtain global regularity properties for the rapidly spinning shallow water structure and its generalization to arbitrary dimension .Second , we provide fresh insights into the formation of these models by showing how they fit within a greater category of nonlinear dispersive partial differential equations . Our main consequence reads as follows : Main Theorem 1 Let v 0 ∈ H s , s > n / 2 + 2 , then there exist constants C = C ( n ) , K = K ( n ) such that if",
        "rewrite_text": "We demonstrate the existence of an open set of initial conditions for which the solution to the rapidly spinning shallow water (RSW) or Euler equations remains globally smooth in space-time, provided it stays sufficiently close to a certain equilibrium state. Our proof incorporates a combination of power estimates and appropriate weighted Sobolev inequalities. Furthermore, we outline how our results can be extended to more general systems of conservation laws that are not strictly hyperbolic. Specifically, we consider the scenario where one of the characteristic velocities vanishes at a specific point while remaining non-zero elsewhere. \n\n**Introduction**  \nThis study serves two main purposes. First, we establish global regularity properties for the rapidly spinning shallow water structure and its extension to arbitrary dimensions. Second, we offer new perspectives on the development of these models by demonstrating how they fit into a broader category of nonlinear dispersive partial differential equations. Our primary result is stated as follows: **Main Theorem 1**: Let \\( v_0 \\in H^s \\) with \\( s > \\frac{n}{2} + 2 \\); then there exist constants \\( C = C(n) \\) and \\( K = K(n) \\) such that if...",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 4.1461399144838555,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an analysis of chemical equilibrium and disequilibrium processes occurring in the atmospheres of substellar mass objects (SMBOs). We have developed a new method for calculating departures from chemical equilibrium, which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any given point within the atmosphere. This approach allows us to calculate the abundances of individual molecular species as functions of altitude above the photosphere. The results show that there can be significant deviations from chemical equilibrium even under conditions where the gas temperature is much higher than the dust temperature. In particular, we find that carbon monoxide may become depleted by several orders of magnitude relative to its abundance predicted by chemical equilibrium models. These findings suggest that SMBO observations should take into account possible non-equilibrium effects when interpreting their spectra. \n \n Keywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium",
        "watermark_text": "We present an assessment of chemical equilibrium and disequilibrium phenomena occurring in the atmospheres of substellar mass bodies ( SMBOs ) . We have developed a new method for calculating departures from molecular equilibrium , which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any certain point within the atmosphere .This method enables us to estimate the abundances of different molecular species as functions of altitude above the photosphere . The results show that there can be considerable deviations from molecular balance especially under environments where the gas temperature is much higher than the dust temperature .In particular , we find that carbon monoxide might appear depleted by many orders of magnitude compared to its availability predicted by molecular balance models . These studies imply that SMBO observations should take into consideration likely non - equilibrium impacts when interpreting their spectra .Keywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium",
        "rewrite_text": "We provide an evaluation of the phenomena of chemical equilibrium and disequilibrium in the atmospheres of substellar mass bodies (SMBOs). Our research introduces a novel approach to calculating deviations from molecular equilibrium, relying on the premise that all species are in local thermodynamic equilibrium with one another at specific locations within the atmosphere. This method allows us to estimate the abundances of various molecular species as a function of altitude above the photosphere. Our findings reveal significant departures from molecular balance, particularly in environments where the gas temperature greatly exceeds the dust temperature. Notably, we observe that carbon monoxide may be significantly depleted by several orders of magnitude compared to predictions made by molecular balance models. These findings suggest that observations of SMBOs should account for potential non-equilibrium effects when analyzing their spectra. \n\nKeywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium",
        "ori-fast-z-score": -1.3242443839434612,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -0.24253562503633297
    },
    {
        "original_text": "We present an explicit criterion to determine whether or not two given multipartite quantum states are separable, i.e., can be written as convex combinations of product states. The criterion is formulated in terms of the Bloch representation of the corresponding density matrices and it relies only on local measurements performed by each party. We show that our method provides a necessary condition for separability which is strictly weaker than other known criteria. Finally we illustrate its usefulness with some examples. Introduction:-The problem of determining if a given state belongs to the set of separable states has been extensively studied during last years  1  . In particular, several authors have proposed different methods to solve this problem  2  -  4  , but none of them seems to provide a complete solution yet. Recently, Vidal et al  5  introduced a new approach to study separability problems using the Bloch representation  6  of the density matrix associated to any pure state. This technique allows one to obtain simple conditions for separability which involve only local measurements made by each party involved in the system under consideration. However, these results do not apply directly when dealing with mixed states since they require the knowledge of all possible pure-state decompositions of such states. Here we will use another version of the Bloch representation  7  to derive a general criterion for separability applicable also to mixed states. Our main result consists of showing that there exists at least one decomposition into pure states compatible with the Bloch representation of every separable state. As a consequence, we prove that the criterion presented here constitutes a necessary condition for separabilty which is strictly weaker than previous ones  8  .\nPreliminaries:-In what follows we consider N-partite systems described by Hilbert spaces H 1 ,H 2 ...H N . A generic element |ψ⟩ ∈ H = ∑ N i=1 H i is called a pure state vector while ρ ∈ D(H) denotes a density operator acting on H. Any density operator can always be expressed in terms of its spectral decomposition  9  \nwhere {|λ⟩} λ=1,...,d is an orthonormal basis of eigenvectors of ρ and p λ ≥ 0 (p",
        "watermark_text": "We present an explicit criterion to measure whether or not two given multipartite quantum states are separable , i . e . , can be written as convex combinations of product states . The requirement is formulated in terms of the Bloch representation of the resulting density matrices and it rely only on local observations performed by each party .We see that our system provides a necessary condition for separability which is strictly weaker than other established parameters . Finally we explain its usefulness with some examples .Introduction : - The question of determining if a given state belongs to the group of separable states has been heavily examined during last years 1 . In particular , various papers have proposed different methods to treat this question 2 - 4 , but none of them appears to provide a complete solved yet .Recently , Vidal et al 5 invented a new approach to study separability phenomena using the Bloch representation 6 of the density graph identified to any pure state . This method enables one to obtain simple conditions for separability which require only local observations made by each party involved in the scheme under consideration .However , these results do not apply directly when dealing with mixed states since they demand the knowledge of all possible pure - state decompositions of such states . Here we will use another version of the Bloch representation 7 to derive a general criterion for separability applied also to mixing states .Our main consequence consists of finding that there exists at least one decomposition into pure states compatible with the Bloch representation of every separable state . As a consequence , we prove that the criterion presented here constitutes a necessary condition for separabilty which is strictly weaker than prior ones 8 .Preliminaries : - In what follows we define N - partite structures described by Hilbert spaces H 1 , H 2 . . . H N . A generic element | ψ ⟩ ∈ H = [UNK] N i = 1 H i is called a pure state vector while ρ ∈ D ( H ) denotes a density operator acting on H . Any density operator can always be stated in terms of its spectral transformation 9 where { | λ ⟩ } λ = 1 , . . . , d is an orthonormal basis of eigenvectors of ρ and p λ ≥ 0 ( p",
        "rewrite_text": "We introduce a clear criterion to assess whether two multipartite quantum states are separable, meaning they can be expressed as convex combinations of product states. This criterion is framed in terms of the Bloch representation of the resulting density matrices and relies solely on local measurements conducted by each party involved. Our approach provides a necessary condition for separability that is notably less stringent than previously established parameters. To illustrate its effectiveness, we offer several examples.\n\n**Introduction:** The inquiry into whether a state qualifies as separable has garnered significant attention in recent years. Various studies have proposed different methodologies to tackle this issue, yet none have fully resolved it. Recently, Vidal et al. introduced an innovative approach leveraging the Bloch representation of the density graph associated with any pure state. This technique allows for straightforward separability conditions that only require local observations from each participant in the scenario. However, these findings do not directly extend to mixed states, as they necessitate the knowledge of all potential pure-state decompositions of those states. In this work, we employ an alternative version of the Bloch representation to establish a generalized criterion for separability that is applicable to mixed states as well. Our key finding is that for every separable state, there exists at least one compatible decomposition into pure states according to the Bloch representation. Consequently, we demonstrate that the criterion we present here acts as a necessary condition for separability that is weaker than earlier criteria.\n\n**Preliminaries:** We begin by defining \\( N \\)-partite systems represented by Hilbert spaces \\( H_1, H_2, \\ldots, H_N \\). A generic element \\( |\\psi\\rangle \\in H = \\bigotimes_{i=1}^{N} H_i \\) is referred to as a pure state vector, and \\( \\rho \\in D(H) \\) signifies a density operator acting on \\( H \\). Every density operator can be expressed via its spectral decomposition, where \\( \\{ |\\lambda\\rangle \\}_{\\lambda=1}^{d} \\) forms an orthonormal basis of eigenvectors of \\( \\rho \\), and \\( p_\\lambda \\geq 0 \\).",
        "ori-fast-z-score": 1.5713484026367723,
        "water-fast-z-score": 7.496340570653091,
        "rewrite-fast-z-score": 1.7561082210148906
    },
    {
        "original_text": "We present the first exact local solution to the equations of motion for type II B supergravity in AdS 5 × S 5 . The solution is supported by two independent harmonic functions, one on each side of an arbitrary surface Σ which divides space into three regions. We show that this solution preserves eight real supercharges and has vanishing central charge. It can be interpreted as a bound state of N = 4 SYM theory with gauge group SU(N)×SU(N), where the number of degrees of freedom scales like O(N 2 ) at large N. In addition we find a new class of solutions describing interfaces between different vacua of the same field theory. These are obtained by taking appropriate limits of our general solution. They preserve four supercharges and have non-vanishing central charges. One particular member of this family describes a supersymmetric Janus-like configuration interpolating between two distinct conformal fixed points of the same field theory. \nIntroduction\n\nThe study of holographic duals of strongly coupled quantum systems has been greatly advanced over recent years through the use of string/M-theory  1, 2  . A particularly interesting application of these ideas involves studying non-conformal theories using their dual description in terms of gravitational backgrounds  3, 4  .\nIn order to construct such models it is necessary to solve the equations of motion associated with the relevant supergravity or gauged supergravity theory. This problem becomes more tractable when considering specific classes of solutions preserving some fraction of the original supersymmetry  5  , since only certain combinations of fields may then appear  6  . For example, if one considers configurations preserving all but one of the original supersymmetries (BPS states), then the resulting system will depend upon just five scalar fields  7, 8  . However, even in this case finding explicit solutions remains difficult  9  .\nOne approach to solving BPS-type problems is to consider special cases where the geometry admits additional symmetries  10  . An important subclass of such solutions arises when the internal manifold M 6 factorises into a product of two spaces M 3 × M 3  11  . In this",
        "watermark_text": "We introduce the first accurate local solution to the coefficients of movement for type II B supergravity in AdS 5 × S 5 . The solving is supported by two independent harmonic functions , one on each side of an arbitrary surface Σ which splits space into three areas .We see that this solution preserves eight real supercharges and has vanishing central charge . It can be interpreted as a bound state of N = 4 SYM theory with gauge group SU ( N ) ×SU ( N ) , where the number of degrees of freedom scales like O ( N 2 ) at large N . In addition we find a new category of solutions describing interfaces between various vacua of the same field theory .These are derived by giving suitable limits of our general solution . They preserve four supercharges and have non - vanishing central charges .One particular part of this class describes a supersymmetric Janus - like configuration interpolating between two separate conformal fixed points of the same field theory . Introduction The investigation of holographic duals of highly coupled quantum systems has been greatly expanded over recent years through the using of string / M - theory 1 , 2 .A notably important use of these ideas includes studying non - conformal models using their dual description in terms of gravitational backgrounds 3 , 4 . In order to build such theories it is required to solve the equations of movement associated with the appropriate supergravity or gauged supergravity models .This problem arises more tractable when examining specific groups of solutions maintaining some fraction of the original supersymmetry 5 , since only certain combinations of fields may then appear 6 . For instance , if one looks configurations preserving all but one of the original supersymmetries ( BPS states ) , then the resulting system will depend upon just five scalar fields 7 , 8 .However , even in this instance finding explicit solved remains challenging 9 . One approach to solving BPS - class problems is to consider special cases where the topology admits extra symmetries 10 .An key subclass of such solutions arises when the internal manifold M 6 factorises into a product of two spaces M 3 × M 3 11 . In this",
        "rewrite_text": "We present the first precise local solution for the movement coefficients of type II B supergravity in an AdS 5 × S 5 background. This solution is derived from two independent harmonic functions, each defined on one side of an arbitrary surface Σ that divides space into three regions. Notably, this solution maintains eight real supercharges and has a vanishing central charge. It can be interpreted as a bound state of N = 4 supersymmetric Yang-Mills (SYM) theory with a gauge group of SU(N) × SU(N), where the degrees of freedom scale like O(N²) at large N. Additionally, we identify a new class of solutions that represent interfaces between different vacua of the same field theory. These solutions arise from appropriate limits of our general solution, preserving four supercharges while exhibiting non-vanishing central charges. One specific instance in this class describes a supersymmetric Janus-like configuration that interpolates between two distinct conformal fixed points of the same field theory.\n\n**Introduction:** The exploration of holographic duals for highly coupled quantum systems has significantly broadened in recent years, particularly through the application of string and M-theory. A critical application of these concepts involves analyzing non-conformal models through their dual representations in gravitational settings. To construct such theories, it is essential to solve the equations of motion associated with the relevant supergravity or gauged supergravity models. This task becomes more manageable when focusing on particular groups of solutions that retain some fraction of the original supersymmetry, as this restricts the possible combinations of fields. For example, configurations that preserve all but one of the original supersymmetries (BPS states) result in a system dependent on just five scalar fields. However, even in this scenario, obtaining explicit solutions remains a challenging endeavor. One effective strategy for tackling BPS-class problems is to examine special cases where the topology allows for additional symmetries. A noteworthy subclass of these solutions occurs when the internal manifold M₆ can be factored into a product of two three-dimensional spaces, M₃ × M₃.",
        "ori-fast-z-score": -0.48989794855663565,
        "water-fast-z-score": 7.568232666571783,
        "rewrite-fast-z-score": 1.720387033089985
    },
    {
        "original_text": "We show that correlation clustering is NP-hard to approximate within any constant factor, even for graphs with maximum degree three and clusters of size at most four.  We also present an algorithm which solves this problem exactly in time O(n3). Our results are based on reductions from the exact cover by 3-sets (X3C) problem. The X3C problem asks whether there exists a collection of subsets of a set S such that each element of S belongs to exactly 3 sets in the collection; it has been shown to be NP-complete. For more information about our work see http://arxiv.org/abs/1206.0571 . \nCorrelation clustering is one of several problems studied under the umbrella of  clustering ; these include k-means clustering, spectral clustering, and graph partitioning. It was introduced independently by Bansal et al., Blum et al., and Dasgupta et al. as follows.   Given a weighted undirected graph G = (V, E), where V denotes the vertices and E denotes the edges, we say that two vertices u, v ∈ V are adjacent if they share an edge e ∈ E. A cluster C ⊆ V is defined as a subset of nodes whose pairwise distances satisfy some threshold t > 0. More formally, given a distance function d : V × V → R+ , let dist(u,v) denote the shortest path between u and v; then, C is said to be a valid cluster if and only if for all pairs of nodes u, v ∈ C:  dist(u,v)  ≤ t",
        "watermark_text": "We see that correlation clustering is NP - hard to approximate within any constant factor , even for graphs with maximum degree three and clusters of diameter at most four . We also demonstrate an algorithm which solves this situation exactly in time O ( n3 ) .Our results are based on reductions from the exact cover by 3 - sets ( X3C ) question . The X3C problem questions whether there exists a family of subsets of a space S such that each element of S belongs to exactly 3 sets in the collection ; it has been shown to be NP - full .For more information about our work see http : / / arxiv . org / abs / 1206 . 0571 . Correlation clustering is one of several problems studied under the umbrella of clustering ; these include k - means clustering , spectral clustering , and graph partitioning .It was introduced independently by Bansal et al . , Blum et al . , and Dasgupta et al . as follows .Given a weighted undirected graph G = ( V , E ) , where V denotes the edges and E denotes the edges , we write that two vertices u , v ∈ V are adjoining if they share an vertex e ∈ E . A cluster C ⊆ V is characterized as a subset of vertices whose pairwise distances satisfy some threshold t > 0 . More generally , given a distance relation d : V × V → R + , let dist ( v , v ) define the longest route between u and v ; then , C is said to be a valid cluster if and only if for all pairs of vertices u , v ∈ C : dist ( v , v ) ≤ t",
        "rewrite_text": "We find that correlation clustering is NP-hard to approximate within any constant factor, even for graphs with a maximum degree of three and clusters that have a diameter of no more than four. Additionally, we present an algorithm that can solve this problem exactly in O(n^3) time. Our findings are based on reductions from the exact cover by 3-sets (X3C) problem, which asks whether there exists a collection of subsets for a space S such that every element of S is included in exactly three of these subsets; this problem has been proven to be NP-complete. For further details about our research, please refer to http://arxiv.org/abs/1206.0571. Correlation clustering is one of several problems explored within the broader field of clustering, which also includes k-means clustering, spectral clustering, and graph partitioning. It was independently introduced by Bansal et al., Blum et al., and Dasgupta et al. In this context, given a weighted undirected graph G = (V, E), where V represents the vertices and E the edges, two vertices u and v ∈ V are considered adjoining if they share a common edge e ∈ E. A cluster C ⊆ V is defined as a subset of vertices whose pairwise distances meet a specific threshold t > 0. More generally, for a given distance relation d: V × V → R+, let dist(u, v) denote the longest path between u and v; then a cluster C is valid if and only if for all pairs of vertices u, v ∈ C, it holds that dist(u, v) ≤ t.",
        "ori-fast-z-score": -1.0945409092309881,
        "water-fast-z-score": 3.482630165734962,
        "rewrite-fast-z-score": 1.2686700948330931
    },
    {
        "original_text": "We present the results of our analysis of high-resolution optical spectra obtained with HST/STIS for four nearby (z<0.1), X-ray selected, Type Ib/c SNe in order to study their progenitor systems. We find that all four objects show evidence for dense CSM surrounding them at distances ranging between 0.01-0.2 pc. The presence of such material is consistent with theoretical expectations for post-low-velocity-bulge (post-LBV) Wolf-Rayet star winds. In addition we detect narrow emission features which are likely due to interaction between SN ejecta and this wind. These observations provide strong constraints on the nature of the progenitor systems: they require massive WR stars as well as binary companions capable of producing significant mass loss prior to explosion. This work was supported by NASA grant NAG5-10842. We have analyzed high resolution STIS/HST data for 4 nearby (z<0.1; Xray-selected) type Ibc supernovae in an attempt to determine the properties of their progenitor systems. All four objects exhibit dense circumstellar matter (CSM; nH>1020 cm-3 ) within 0.01-0.20 parsecs of the supernova site. Such densities are expected if these explosions occur following the ejection of a low velocity  bulge  during late stages of stellar evolution. Furthermore, we observe narrow emission features which may be associated with shock-heating of the CSM by the expanding supernova remnant. Our findings suggest that these events result from the deaths of massive Wolf Rayet stars surrounded by close binaries.",
        "watermark_text": "We present the conclusion of our analysis of high - resolution optical spectra obtained with HST / STIS for four nearby ( z < 0 . 1 ) , X - ray selected , Type Ib / c SNe in order to study their progenitor structures . We see that all four bodies exhibit indication for thick CSM circling them at distances ranging between 0 . 01 - 0 . 2 pc .The presence of such material is compatible with theoretical expectations for post - low - speed - bulge ( post - LBV ) Wolf - Rayet star winds . In addition we find narrow radiation properties which are likely due to contact between SN ejecta and this wind .These measurements give strong restrictions on the nature of the progenitor structures : they use massive WR galaxies as well as binary companions capable of producing significant mass loss prior to explosion . This research was supported by NASA gift NAG5 - 10842 .We have analyzed high resolution STIS / HST results for 4 nearby ( z < 0 . 1 ; Xray - selected ) type Ibc supernovae in an trying to estimate the properties of their progenitor structures . All four bodies exhibit thick circumstellar matter ( CSM ; nH > 1020 cm - 3 ) within 0 . 01 - 0 . 20 parsecs of the supernova center .Such densities are expected if these fires occur following the ejection of a small velocity bulge during late stages of stars evolution . Furthermore , we study narrow radiation properties which may be involved with shock - heating of the CSM by the evolving supernova remnant .Our findings show that these changes result from the deaths of large Wolf Rayet stars surrounded by tight binaries .",
        "rewrite_text": "We conclude our analysis of high-resolution optical spectra obtained with HST/STIS for four nearby (z < 0.1), X-ray selected Type Ib/c supernovae (SNe) to investigate their progenitor structures. Our observations reveal that all four supernovae are surrounded by thick circumstellar matter (CSM) located at distances between 0.01 and 0.2 parsecs. The presence of this material aligns with theoretical predictions regarding post-low-speed-bulge (post-LBV) Wolf-Rayet star winds. Additionally, we observe narrow emission features likely resulting from the interaction between the supernova ejecta and this wind. These measurements impose strong constraints on the nature of the progenitor systems, suggesting they involve massive Wolf-Rayet stars as well as binary companions that can cause substantial mass loss prior to the explosion. This research was supported by NASA grant NAG5-10842. Our analysis of the high-resolution STIS/HST data for these four nearby Type Ibc supernovae aims to estimate the characteristics of their progenitor structures. All four supernovae show evidence of thick circumstellar matter (nH > 10^20 cm^-3) within 0.01 to 0.20 parsecs from the center. Such density levels are anticipated if the supernova follows the expulsion of a low-velocity bulge during the late stages of stellar evolution. Furthermore, we examine narrow emission features that may be linked to shock heating of the CSM by the evolving supernova remnant. Our findings indicate that these phenomena are associated with the deaths of massive Wolf-Rayet stars in close binary systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.807983207583857,
        "rewrite-fast-z-score": 0.5773502691896257
    },
    {
        "original_text": "We study the generalized Dicke model with an arbitrary number N of two-level atoms interacting with one-mode radiation field, and show that it can be mapped to a spin-1/2 system by using the Holstein-Primakoff transformation. We then use the exact diagonalization method to calculate its ground state energy spectrum for different values of the coupling constant g and the number N . The results are compared with those obtained by other methods such as perturbation theory and numerical integration. It is found that our results agree well with previous ones when the coupling strength is small but deviate significantly from them if the coupling becomes strong. Finally we discuss some possible applications of this work. PACS: 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv \nI. INTRODUCTIO N\nThe Dicke model  1  describes how many identical two-level atoms interact collectively with a single mode of electromagnetic field. In recent years there has been renewed interest in studying this model because of its potential application in quantum information processing  2  , quantum optics  3  , condensed matter physics  4  , etc.. For example, the collective spontaneous emission rate of the atomic ensemble depends on the total angular momentum J = N /2 (N being the number of atoms)  5  .\nIn fact, the Dicke model was originally proposed more than half century ago  6  . Since then various theoretical approaches have been developed to solve it  7 -10  . Among these approaches, the most successful one is probably the so-called HolsteinPrimakoff transformation  11  which maps the original problem into a spin-1/2 system  12  . This approach works very well at weak-coupling regime where the interaction between atom-field is relatively small. However, it fails completely at large-coupling limit since the mapping procedure breaks down due to the appearance of unphysical states  13  . Recently, several authors  14 -19  have tried to overcome this difficulty by introducing new transformations or approximations. Nevertheless, their solutions still suffer from certain drawbacks  20, 21  .",
        "watermark_text": "We explore the generalized Dicke model with an arbitrary number N of two - level atoms interacting with one - mode radiation field , and find that it can be mapped to a spin - 1 / 2 system by using the Holstein - Primakoff transformation . We then use the exact diagonalization technique to estimate its ground state energy spectrum for different values of the interaction factor g and the number N .The results are compared with those achieved by other methods such as perturbation theory and numerical integration . It is found that our findings agree well with previous ones when the interaction strength is tiny but deviate drastically from them if the interaction becomes strong .Finally we explain some possible use of this study . PACS : 03 . 65 . Ud , 05 . 45 . Mt , 11 . 10 . Gh , 12 . 20 . Dc , 13 . 25 . Gv I . INTRODUCTIO N The Dicke model 1 explains how many identical two - level atoms behave collectively with a single mode of electromagnetic field .In recent years there has been continued interest in investigating this model because of its potential application in quantum information processing 2 , quantum optics 3 , condensed matter science 4 , etc . . For instance , the collective spontaneous emission speed of the atomic ensemble depends on the total angular velocity J = N / 2 ( N being the number of atoms ) 5 . In reality , the Dicke concept was originally proposed more than quarter century ago 6 .Since then various theoretical methods have been constructed to solve it 7 - 10 . Among these method , the most popular one is probably the so - called HolsteinPrimakoff transformation 11 which maps the original problem into a spin - 1 / 2 system 12 .This method works very best at weak - interaction regime where the interaction between electron - field is fairly little . However , it fails totally at large - interaction range since the mapping method splits down due to the appearance of unphysical states 13 .Recently , various literature 14 - 19 have tried to overcome this challenge by using new transformations or approximations . Nevertheless , their solutions still suffer from certain drawbacks 20 , 21 .",
        "rewrite_text": "We investigate the generalized Dicke model featuring N two-level atoms interacting with a single-mode radiation field, demonstrating that it can be transformed into a spin-1/2 system through the Holstein-Primakoff transformation. Employing exact diagonalization, we estimate the ground state energy spectrum for various interaction strengths \\( g \\) and atom counts \\( N \\). Our results are compared with those obtained through other methods, including perturbation theory and numerical integration. We find that while our results align well with previous findings for weak interaction strengths, significant deviations occur as interactions strengthen. Lastly, we discuss potential applications for our study. \n\n**PACS:** 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv \n\n**I. INTRODUCTION** The Dicke model describes the collective behavior of multiple identical two-level atoms coupled to a single mode of electromagnetic radiation. Recent years have seen renewed interest in this model, particularly for its implications in quantum information processing, quantum optics, and condensed matter physics. For example, the rate of collective spontaneous emission for the atom ensemble is influenced by the total angular momentum \\( J = N/2 \\) (where \\( N \\) is the number of atoms). Although the Dicke model was proposed over 25 years ago, numerous theoretical approaches have been developed to analyze it. Among these, the Holstein-Primakoff transformation has gained popularity, effectively mapping the problem onto a spin-1/2 framework. This method performs well in the weak interaction regime, where the coupling between electrons and fields is minimal. However, it becomes ineffective in strong interaction scenarios due to the emergence of unphysical states. Recent literature has sought to address these challenges through new transformations and approximations, although some limitations persist in their solutions.",
        "ori-fast-z-score": -0.16222142113076254,
        "water-fast-z-score": 6.8132996874920275,
        "rewrite-fast-z-score": -0.6859943405700353
    },
    {
        "original_text": "We consider the problem of estimating an unknown function f from noisy measurements y = Af + w, where A is a linear operator and w is white noise with known covariance matrix Cw.  We assume that the operator A has been discretized on some grid (e.g., by using finite differences or spectral methods) so that it can be represented as a large matrix.  The goal is to find an estimatef such that Ef − f  2 is minimized subject to certain constraints on the smoothness off . \nIn this work we propose new numerical techniques based on needlets which are able to efficiently solve these constrained optimization problems.  In particular, our approach allows us to obtain accurate estimates even when the number N of available observations is much smaller than the dimension M of the space spanned by the columns of A. \nThe proposed method relies on two main ingredients:  First, we use a sparse representation of functions in terms of needlets.  Second, we develop efficient iterative algorithms for solving large-scale convex optimization problems involving sparsity-promoting regularizers.  These algorithms combine ideas from compressed sensing theory and recent results about the convergence rate of alternating direction method of multipliers (ADMM).",
        "watermark_text": "We consider the question of estimating an unknown function f from noisy measurements y = Af + f , where A is a linear operator and f is white sound with known covariance matrix Cw . We assume that the operator A has been discretized on some grid ( e . g . , by using finite differences or spectral algorithms ) so that it can be described as a large vector .The goal is to find an estimatef such that Ef − f 2 is minimized subject to specified constraints on the smoothness off . In this study we develop new numerical tactics based on needlets which are able to easily solution these constrained optimization problems .In particular , our approach allows us to obtain precise estimates even when the number N of available observed is much smaller than the dimension M of the space spanned by the rows of A . The proposed approach consists on two principal ingredients : First , we utilize a sparse representation of functions in terms of needlets .Second , we develop optimal iterative techniques for solving large - scale convex optimization problems concerning sparsity - promoting regularizers . These methods combine developments from compressed sensing theory and recent results about the convergence speed of alternating path method of multipliers ( ADMM ) .",
        "rewrite_text": "We address the challenge of estimating an unknown function \\( f \\) from noisy observations represented as \\( y = Af + f \\), where \\( A \\) is a linear operator and \\( f \\) is white noise characterized by a known covariance matrix \\( C_w \\). We assume that the operator \\( A \\) has been discretized on a grid—for example, through finite differences or spectral algorithms—allowing it to be represented as a large vector. Our objective is to derive an estimate \\( \\hat{f} \\) that minimizes the error \\( E[\\hat{f} - f]^2 \\) while adhering to specified smoothness constraints on \\( f \\). In this work, we introduce new numerical strategies based on needlets that effectively tackle these constrained optimization challenges. Notably, our method enables us to achieve accurate estimates even when the number of available observations, \\( N \\), is significantly smaller than the dimension \\( M \\) of the space defined by the rows of \\( A \\). Our approach relies on two main components: first, we employ a sparse representation of functions using needlets; second, we develop optimal iterative techniques for addressing large-scale convex optimization problems with sparsity-promoting regularizers. These techniques integrate insights from compressed sensing theory and recent advancements related to the convergence speed of the Alternating Direction Method of Multipliers (ADMM).",
        "ori-fast-z-score": 0.9233805168766388,
        "water-fast-z-score": 6.807380225308036,
        "rewrite-fast-z-score": 1.7260884807271526
    },
    {
        "original_text": "We present new results on the contribution of BL Lacs (blazars) to the extragalactic gamma-ray background based on data collected by the Fermi Large Area Telescope between August 2008 and December 2010, corresponding to an effective exposure time of 1.6 yr for each source in our sample. We use two different methods to estimate this contribution: i) we calculate the number counts above 100 MeV as function of redshift using a maximum likelihood method; ii) we fit the observed spectral energy distribution with a log-parabola model and derive the integrated fluxes at 0.1 GeV and 10 TeV energies. The resulting contributions are consistent within statistical uncertainties. Our best-fit value is F(>100 MeV) = 2.2 x 10^{−8} ph cm−2 s−1 sr−1 which corresponds to ~20% of the measured EGB intensity. This result confirms that blazars are one of the main contributors to the EGB emission.",
        "watermark_text": "We report new data on the contribution of BL Lacs ( blazars ) to the extragalactic gamma - ray background based on evidence generated by the Fermi Large Area Telescope between August 2008 and December 2010 , corresponding to an effective exposure time of 1 . 6 yr for each source in our sample . We use two different methods to estimate this contribution : i ) we determine the number counts above 100 MeV as function of redshift using a maximum likelihood technique ; ii ) we fit the observed spectral power distribution with a log - parabola simulation and derive the integrated fluxes at 0 . 1 GeV and 10 TeV energies .The resulting contributions are compatible within statistical uncertainties . Our best - fitting value is F ( > 100 MeV ) = 2 . 2 x 10 ^ { −8 } ph cm−2 s−1 sr−1 which equals to ~ 20 % of the measured EGB brightness .This result confirms that blazars are one of the main contributors to the EGB emission .",
        "rewrite_text": "We present new findings on the role of BL Lacertae objects (blazars) in contributing to the extragalactic gamma-ray background, based on data collected by the Fermi Large Area Telescope from August 2008 to December 2010, which corresponds to an effective exposure time of 1.6 years per source in our sample. To estimate this contribution, we employed two different approaches: first, we utilized a maximum likelihood technique to assess the number counts above 100 MeV as a function of redshift; second, we fit the observed spectral power distribution with a log-parabola model to derive integrated fluxes at 0.1 GeV and 10 TeV energies. The contributions we obtained are consistent within the limits of statistical uncertainty. Our optimal estimate is F(>100 MeV) = 2.2 x 10^{-8} ph cm^{-2} s^{-1} sr^{-1}, which accounts for approximately 20% of the observed brightness of the extragalactic gamma-ray background. This result reinforces the notion that blazars are significant contributors to the emission of the extragalactic gamma-ray background.",
        "ori-fast-z-score": -0.40451991747794525,
        "water-fast-z-score": 2.8316394223456167,
        "rewrite-fast-z-score": -0.601929265428846
    },
    {
        "original_text": "We consider the statistical properties of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions, power spectra, and probability density functions (PDFs). We show that these quantities can be expressed by means of solutions to certain partial differential equations with time-dependent coefficients. The PDFs are obtained for both stationary and nonstationary cases using the method of characteristics. In particular, we derive an exact expression for the PDF of the amplitude fluctuations of a monochromatic plane wave propagating through a turbulent medium. This result is used to obtain expressions for the mean-square fluctuation levels of the electric field strength and intensity at any point along the propagation path. Finally, we discuss some applications of our results to radiowave scintillation theory and radar detection problems. PACS: 42.65.Tg, 43.20 .Fx, 47.55.+q, 47.60.+j",
        "watermark_text": "We consider the statistical characteristics of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions , power spectra , and likelihood density functions ( PDFs ) . We see that these quantities can be described by means of solutions to many partial differential coefficients with time - dependent coefficients .The PDFs are derived for both static and nonstationary cases using the method of characteristics . In particular , we derive an precise representation for the PDF of the amplitude fluctuations of a monochromatic plane beam propagating through a turbulent medium .This result is utilized to obtain definitions for the mean - square fluctuation concentrations of the electric field intensity and intensity at any point along the propagation path . Finally , we explain some applications of our findings to radiowave scintillation physics and radar detection difficulties .PACS : 42 . 65 . Tg , 43 . 20 . Fx , 47 . 55 . + q , 47 . 60 . + j",
        "rewrite_text": "We analyze the statistical properties of nonstationary random acoustic and electromagnetic waves through their correlation functions, power spectra, and probability density functions (PDFs). Our findings indicate that these properties can be represented as solutions to various partial differential equations with time-dependent coefficients. We derive PDFs for both static and nonstationary scenarios using the method of characteristics. Specifically, we provide an exact representation for the PDF of amplitude fluctuations in a monochromatic plane wave traveling through a turbulent medium. This result allows us to define the mean-square fluctuation concentrations of the electric field intensity and intensity at any point along the propagation trajectory. Lastly, we discuss the implications of our results for radio wave scintillation physics and challenges in radar detection. PACS: 42.65.Tg, 43.20.Fx, 47.55.+q, 47.60.+j.",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 1.61245154965971
    },
    {
        "original_text": "We present the spectroscopic follow-up observations for eight galaxy clusters at redshifts z=0.6-0.9 selected in the Red-Sequence Cluster Survey (RCS). The sample is composed by four X-ray luminous and four optically rich clusters, with masses ranging between M500 = 1.5 × 1014 - 2.7 × 1014 h70M⊙ . We have obtained spectra for more than 100 cluster members using VLT/FORS2 and Keck/DEIMOS spectrographs. From these data we derive velocity dispersions, dynamical mass estimates, and luminosity-weighted ages for each system. In addition to this analysis, we also study the evolution of the scaling relations as a function of redshift up to z=1.1. Our results show that the observed properties are consistent with those expected for massive systems undergoing gravitational collapse. However, there seems to be an offset towards lower values of σv /σ⋆ compared to predictions based on numerical simulations.",
        "watermark_text": "We present the spectroscopic follow - up observations for eight galaxy regions at redshifts z = 0 . 6 - 0 . 9 selected in the Red - Sequence Cluster Survey ( RCS ) . The sample is composed by four X - ray luminous and four optically abundant clusters , with masses vary between M500 = 1 . 5 × 1014 - 2 . 7 × 1014 [UNK] .We have derive spectra for more than 100 cluster elements using VLT / FORS2 and Keck / DEIMOS spectrographs . From these information we derive velocity dispersions , dynamical mass estimates , and luminosity - weighted ages for each system .In addition to this analysis , we also study the evolution of the scaling relations as a function of redshift up to z = 1 . 1 . Our results show that the seen characteristics are compatible with those expected for huge systems undergoing gravitational failure .However , there seems to be an offset towards decreased values of σv / [UNK] compared to measurements based on numerical simulations .",
        "rewrite_text": "We present the results of spectroscopic follow-up observations for eight galaxy regions at redshifts z = 0.6 - 0.9, selected from the Red-Sequence Cluster Survey (RCS). The sample includes four X-ray luminous clusters and four optically rich clusters, with masses ranging from M500 = 1.5 × 10^14 to 2.7 × 10^14 solar masses. Using the VLT/FORS2 and Keck/DEIMOS spectrographs, we obtained spectra for over 100 cluster members. From this data, we calculated velocity dispersions, dynamical mass estimates, and luminosity-weighted ages for each cluster. Additionally, we examined the evolution of scaling relations as a function of redshift up to z = 1.1. Our findings indicate that the observed characteristics align with expectations for large systems experiencing gravitational collapse. However, we also note a discrepancy, as our measurements of σv show lower values compared to those derived from numerical simulations.",
        "ori-fast-z-score": 1.3363062095621219,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": 1.5109662034355793
    },
    {
        "original_text": "We present new Chandra X-ray Observatory observations and optical spectroscopy for the galaxy cluster Abell 576, which is known to have two merging components separated by about 1 arcmin (about 700 kpc). The northern component has been previously studied as an example of a  line-of-sight bullet cluster ; it shows no evidence of significant substructure or shock heating along its line of sight but does show signs of recent merger activity on smaller scales. In contrast, we find that the southern component appears relaxed with little sign of disturbance; however, this may be due to projection effects since there are several galaxies at large projected distances from the center of the cluster whose redshifts indicate they lie behind the cluster core. We also detect diffuse emission extending beyond the virial radius of both clusters, possibly indicating ongoing accretion onto these systems. These results suggest that Abell 576 will evolve into a single massive system within a few Gyrs.",
        "watermark_text": "We present new Chandra X - ray Observatory surveys and imaging spectroscopy for the galaxy region Abell 576 , which is known to have two combining components split by about 1 arcmin ( about 700 kpc ) . The northern component has been previously examined as an instance of a line - of - view bullet cluster ; it displays no evidence of significant substructure or shock heating along its line of vision but does display signs of recent collision activity on smaller scales .In contrast , we find that the southern component appears relaxed with little sign of disruption ; however , this might be due to projection influences since there are several stars at large projected distances from the hub of the cluster whose redshifts indicate they exist behind the cluster core . We additionally observe diffuse emission stretching beyond the virial diameter of both clusters , possibly indicating continued accretion onto these systems .These data suggest that Abell 576 will evolve into a single giant body within a few Gyrs .",
        "rewrite_text": "We present new surveys and imaging spectroscopy from the Chandra X-ray Observatory focused on the galaxy cluster Abell 576, which features two merging components separated by approximately 1 arcminute (around 700 kpc). The northern component has been previously studied as an example of a line-of-sight bullet cluster; it shows no significant signs of substructure or shock heating in its line of sight, but evidence of recent collision activity on smaller scales is present. In contrast, the southern component appears to be more relaxed, showing little disruption; however, this appearance may be influenced by projection effects, as several stars are located at large projected distances from the cluster's core and their redshifts suggest they lie behind the cluster. Additionally, we observe diffuse emission extending beyond the virial radius of both components, which could indicate ongoing accretion into these systems. These findings imply that Abell 576 is likely to evolve into a single massive cluster within a few billion years.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 5.196152422706631,
        "rewrite-fast-z-score": -0.3375263702778072
    },
    {
        "original_text": "We present an approach to modeling epidemic spread using synthetic populations generated by massively multiplayer online games (MMOGs). We use the population and mobility data collected for the City of Heroes MMOG, which has been played continuously since 2003 with over one million registered players worldwide. The game s persistent world is divided into regions that are connected via player movement between them. Each region contains a large number of individual households containing up to several hundred characters each. Our model uses this household-level information along with character-to-character contact rates inferred from the observed movements within the game to simulate disease transmission at both regional and global scales. We compare our results against epidemiological models based on real-world census data and find good agreement when we scale down the size of the simulated population appropriately. This suggests that large-scale virtual worlds such as MMOGs can be used to study epidemics without requiring access to sensitive personal health records or detailed demographic data.",
        "watermark_text": "We create an way to modeling disease spread using synthetic populations generated by massively multiplayer online games ( MMOGs ) . We use the population and connectivity data accumulated for the City of Heroes MMOG , which has been played continuously since 2003 with over one million registered participants globally .The player s persistent world is separated into regions that are connected via character movement between them . Each area contains a large number of individual families representing up to several hundred characters each .Our model takes this household - level data along with character - to - character connection rates inferred from the reported shifts within the gameplay to simulate illness transmission at both regional and international scales . We contrast our findings against epidemiological models using on real - time census data and find good agreement when we scale down the size of the imagined population appropriately .This implies that wide - scale virtual spaces such as MMOGs can be used to study epidemics without requiring access to accurate individual medical documents or accurate demographic statistics .",
        "rewrite_text": "We have developed a method for modeling disease spread utilizing synthetic populations generated from massively multiplayer online games (MMOGs). Our research leverages the population and connectivity data from City of Heroes, an MMOG that has been operational since 2003 with over one million registered players worldwide. In this game, the persistent world is divided into regions connected by characters moving between them, with each area housing numerous families, representing hundreds of characters. Our model incorporates household-level data along with inferred character-to-character connection rates based on gameplay dynamics to simulate disease transmission on both regional and global scales. We compare our results to traditional epidemiological models that utilize real-time census data and find strong alignment when we appropriately adjust the size of the simulated population. This suggests that large-scale virtual environments, such as MMOGs, can serve as valuable tools for studying epidemics without needing access to detailed individual medical records or precise demographic information.",
        "ori-fast-z-score": 0.21320071635561041,
        "water-fast-z-score": 7.035623639735144,
        "rewrite-fast-z-score": 0.10721125348377948
    },
    {
        "original_text": "We study shelling-type orderings on regular cell complexes, which are defined by an ordering of their cells such that each new face is attached to exactly one old face in its boundary. We show how these can be used to construct acyclic matchings for the Salvetti complex associated with any finite Coxeter group W . This yields a combinatorial proof of the fact that the Poincaré polynomial of this complex satisfies P(t) = (1 − t)(1 + w∈W t l(w) ), where l denotes the length function on W .\nThe results presented here were obtained during my PhD thesis at ETH Zurich under the supervision of Prof. Dr. Jürg Stadler. I would like to thank him as well as Prof. Dr. Peter Schmitt for many helpful discussions. The research leading to these results has received funding from the European Research Council under the European Union s Seventh Framework Programme (FP/2007-2013 Grant Agreement n o 325928).",
        "watermark_text": "We research shelling - class orderings on regular cell complexes , which are established by an ordering of their cells such that each new side is attached to exactly one old face in its boundary . We see how these can be used to build acyclic matchings for the Salvetti complex associated with any finite Coxeter class W .This yields a combinatorial proving of the fact that the Poincaré polynomial of this complex satisfies P ( t ) = ( 1 − t ) ( 1 + w∈W t l ( v ) ) , where l represents the length function on W . The results presented here were obtained during my PhD thesis at ETH Zurich under the guidance of Prof . Dr . Jürg Stadler .I would like to thank him as also as Prof . Dr . Peter Schmitt for numerous helpful talks . The studies leading to these results has obtained support from the European Research Council under the European Union s Seventh Framework Programme ( FP / 2007 - 2013 Grant Agreement n o 325928 ) .",
        "rewrite_text": "We investigate shelling, which involves class orderings on regular cell complexes. This is achieved by arranging the cells so that each newly added cell connects to exactly one existing face on its boundary. We explore how these arrangements can be utilized to construct acyclic matchings for the Salvetti complex associated with any finite Coxeter group W. This approach leads to a combinatorial proof that the Poincaré polynomial of this complex satisfies the equation P(t) = (1 - t)(1 + ∑_{w∈W} t^{l(v)}), where l denotes the length function on W. The findings presented here were part of my PhD dissertation at ETH Zurich, supervised by Prof. Dr. Jürg Stadler. I would like to express my gratitude to him and to Prof. Dr. Peter Schmitt for their invaluable insights during our discussions. This research was supported by the European Research Council under the European Union’s Seventh Framework Programme (FP/2007-2013, Grant Agreement No. 325928).",
        "ori-fast-z-score": 0.42008402520840293,
        "water-fast-z-score": 5.181036310903636,
        "rewrite-fast-z-score": 1.414213562373095
    },
    {
        "original_text": "We study the dynamics and emission properties of advection dominated accretion flows (ADAFs) in which viscosity is caused by magnetic reconnections between field lines anchored to differentially rotating black holes. We find that, for reasonable values of parameters, such ADAFs can produce luminosities as high as those observed in quasars without violating any observational constraints on their mass inflow rates or temperatures at large radii. The main reason why our model works well is because it naturally produces an outflowing wind component whose kinetic energy flux greatly exceeds its thermal energy flux. This wind carries away most of the angular momentum so that the flow becomes nearly Keplerian near the black hole horizon. In addition, we show that this wind also provides enough pressure support against gravity to prevent the gas density from becoming too low there. Our results suggest that the winds produced by magnetized ADAF models may be responsible for driving powerful radio jets in active galactic nuclei.",
        "watermark_text": "We research the dynamics and emission behavior of advection dominated accretion currents ( ADAFs ) in which viscosity is caused by magnetic reconnections between field lines anchored to differentially rotating black holes . We see that , for reasonable values of parameters , such ADAFs can generate luminosities as great as those observed in quasars without violating any observational restrictions on their mass inflow rates or temperatures at large radii .The main explanation why our model works well is because it naturally produces an outflowing breeze component whose kinetic power flux considerably exceeds its thermal energy flux . This wind carries away most of the angular velocity so that the flow turns almost Keplerian near the dark hole horizon .In addition , we indicate that this wind additionally offers enough pressure support against gravity to prohibit the gas density from getting too low there . Our results propose that the storms produced by magnetized ADAF machines could be responsible for driving strong radio jets in active galactic nuclei .",
        "rewrite_text": "We investigate the dynamics and emission characteristics of advection-dominated accretion flows (ADAFs), where viscosity arises from magnetic reconnections between field lines that are anchored to differentially rotating black holes. Our findings suggest that, under reasonable parameter values, such ADAFs can produce luminosities comparable to those observed in quasars, all while remaining consistent with observational constraints on their mass inflow rates and temperatures at large distances. A key reason our model is effective is that it inherently generates a wind component whose kinetic energy flux significantly surpasses its thermal energy flux. This wind removes much of the angular momentum, resulting in the flow approaching a nearly Keplerian state near the black hole horizon. Furthermore, we show that this wind provides sufficient pressure support against gravity, preventing the gas density from declining excessively in that region. Our results imply that the outflows driven by magnetized ADAF systems could play a significant role in generating strong radio jets in active galactic nuclei.",
        "ori-fast-z-score": 0.22086305214969307,
        "water-fast-z-score": 5.521576303742327,
        "rewrite-fast-z-score": 0.5488212999484517
    },
    {
        "original_text": "The book is intended for students who have completed the first year course in control theory and are interested to learn more about discrete-time systems, digital controllers and computer-based control techniques. The text covers topics such as state space representation, stability analysis, optimal control design, robustness issues, model predictive control (MPC), fuzzy logic based control etc., with an emphasis on practical applications. It also includes several examples that illustrate key concepts discussed throughout the chapters. \n \n This textbook provides a comprehensive coverage of basic principles underlying various aspects of modern control engineering. In addition to theoretical foundations, it presents numerous numerical examples illustrating important concepts introduced along the way. A number of exercises at the end of each chapter help readers reinforce their understanding of material presented earlier. Finally, there are two appendices containing additional information useful for further study or research work. \nThis book can be used by graduate students studying advanced courses in control theory, as well as researchers working in this area.",
        "watermark_text": "The text is intended for students who have completed the first year course in control theory and are concerned to teach more about discrete - time systems , digital controllers and computer - based control methods . The text encompasses topics such as state space representation , stability analysis , ideal control design , robustness issues , model predictive control ( MPC ) , fuzzy logic based control etc . , with an emphasis on technical applications .It additionally contains several examples that highlight key concepts discussed throughout the pages . This treatise provides a comprehensive treatment of fundamental principles governing several elements of modern control engineering .In addition to theoretical foundations , it presents several mathematical models illustrating major concepts adopted along the way . A variety of tests at the end of each volume help viewers reinforce their understanding of content presented earlier .Finally , there are two appendices containing extra data helpful for further study or research studies . This book can be used by graduate candidates pursuing advanced degrees in control theory , as well as scientists studying in this area .",
        "rewrite_text": "This text is designed for students who have successfully completed their first-year course in control theory and are eager to explore discrete-time systems, digital controllers, and computer-based control methods in greater depth. It covers a wide range of topics, including state space representation, stability analysis, ideal control design, robustness issues, model predictive control (MPC), and fuzzy logic-based control, all with a strong focus on practical applications. The book includes numerous examples to illustrate key concepts discussed throughout its chapters. It offers a thorough examination of the fundamental principles that underpin various aspects of modern control engineering. In addition to the theoretical foundations, it features several mathematical models that elucidate the major concepts introduced. To reinforce understanding, a variety of tests are provided at the end of each chapter. Furthermore, two appendices supply additional information that is useful for further study or research. This book is suitable for graduate students pursuing advanced degrees in control theory and for researchers in the field.",
        "ori-fast-z-score": 0.4879500364742666,
        "water-fast-z-score": 6.993258208972302,
        "rewrite-fast-z-score": 1.6
    },
    {
        "original_text": "We study the effect of Rashba spin-orbit interaction on the spin Hall conductivity (SHC) for an interacting two-dimensional electron system with parabolic dispersion and Zeeman splitting in presence of a uniform external magnetic field applied normal to the plane of motion. We show that SHC is independent of temperature, chemical potential and strength of disorder provided the Fermi energy lies within the Zeeman gap. The results are obtained by using the Kubo formula combined with the self-consistent Born approximation. It has been shown recently that the spin current can be generated without any net charge flow when electrons move through a nonmagnetic material under the influence of spin-orbit coupling  1  . This phenomenon known as spin Hall effect was first predicted theoretically  2  , and later observed experimentally  3  .\nThe origin of this effect is due to the fact that the spin-orbit interaction causes a transverse force which deflects the trajectories of moving particles leading to a finite spin polarization at the edges  4  . In recent years there have been several theoretical studies devoted to understand various aspects of spin Hall effect  5  -  8  . However most of these works were done either in absence or weak magnetic fields where the Landau levels do not play significant role  9  . On the other hand it is well known that the Landau level quantization plays important role in determining many physical properties such as magnetoresistance  10  , optical absorption  11  etc., especially near the quantum limit  12  . Therefore it would be interesting to investigate how the Landau levels affect the spin Hall effect.",
        "watermark_text": "We explore the impact of Rashba spin - orbit interaction on the spin Hall conductivity ( SHC ) for an interacting two - dimensional electron structure with parabolic dispersion and Zeeman splitting in presence of a consistent external magnetic current applied normal to the plane of movement . We see that SHC is independent of temperature , chemical potential and strength of disorder provided the Fermi energy rests within the Zeeman gap .The results are derived by using the Kubo formula coupled with the self - stable Born algorithm . It has been shown lately that the spin current can be formed without any gross charge flow when nuclei move through a nonmagnetic material under the effects of spin - orbit bonding 1 .This phenomenon known as spin Hall phenomenon was first expected theoretically 2 , and later observed experimentally 3 . The origin of this effect is due to the fact that the spin - orbit interaction produces a transverse force which deflects the trajectories of moving particles leading to a finite spin polarization at the edges 4 .In past decades there have been numerous conceptual research devoted to study various parts of spin Hall phenomenon 5 - 8 . However most of these works were done either in absence or strong magnetic fields where the Landau concentrations do not play substantial role 9 .On the other hand it is well established that the Landau level quantization takes key importance in establishing much mechanical parameters such as magnetoresistance 10 , optical reflection 11 etc . , particularly near the quantum limit 12 . Therefore it would be interesting to examine how the Landau concentrations influenced the spin Hall phenomenon .",
        "rewrite_text": "In this study, we investigate the influence of Rashba spin-orbit interaction on spin Hall conductivity (SHC) in an interacting two-dimensional electron system characterized by parabolic dispersion and Zeeman splitting, under the application of a constant external magnetic current perpendicular to the plane of motion. Our findings indicate that SHC remains unaffected by temperature, chemical potential, and disorder strength, as long as the Fermi energy lies within the Zeeman gap. These results are derived using the Kubo formula in conjunction with a self-consistent Born approximation. Recent findings have demonstrated that spin currents can be generated without any significant charge flow when nuclei traverse a nonmagnetic material influenced by spin-orbit coupling. This effect, known as the spin Hall effect, was first predicted theoretically and has since been observed experimentally. The phenomenon arises because the spin-orbit interaction generates a transverse force that alters the paths of moving particles, resulting in a measurable spin polarization at the edges. Over the past few decades, extensive research has explored various aspects of the spin Hall effect. However, most studies have been conducted in the absence of strong magnetic fields, where Landau levels do not significantly contribute. Conversely, it is well established that Landau level quantization plays a crucial role in determining various physical properties, such as magnetoresistance and optical reflection, particularly near the quantum limit. Thus, it is essential to investigate how Landau levels affect the spin Hall phenomenon.",
        "ori-fast-z-score": -0.2683281572999747,
        "water-fast-z-score": 7.723027987151322,
        "rewrite-fast-z-score": 0.7364596943186588
    },
    {
        "original_text": "We study the ground state properties of frustrated spin-1/2 Heisenberg models on square lattices with different types of interlayer couplings, including both homogeneous and inhomogeneous ones. We show that frustration can be suppressed by introducing an additional ferromagnetic coupling between layers which leads to formation of inhomogeneous magnetic states characterized by spatially modulated magnetization profiles. The obtained results are discussed within the framework of the recently developed concept of ``inverse condensation  . Introduction: In recent years there has been growing interest in studying strongly correlated systems where competing interactions lead to complex phase diagrams exhibiting various exotic phases such as valence bond solids (VBS), charge density waves (CDW) or supersolids  1-3 . One of the most interesting examples is provided by layered quantum antiferromagnets  4  . These compounds consist of weakly coupled planes of spins arranged into a regular lattice structure. Due to strong geometrical frustration caused by competing nearest-neighbor exchange interactions J1 along the chain direction and J2 across the chains, these materials exhibit a rich variety of physical phenomena ranging from conventional Néel order at low temperatures down to disordered paramagnetic phases  5  .\nIn this work we consider two prototypical representatives of this class of materials: CuGeO3  6  , where each plane consists of edge-sharing tetrahedra forming a honeycomb-like network  7, 8  , and BaCo2As2  9  , where the planes are made up of corner-sharing triangles  10  . Both compounds have attracted considerable attention due to their unusual magnetic behavior  11, 12  . For example, it was shown experimentally that in CuGeO3 the system undergoes a transition from a collinear antiferromagnetically ordered state below TN = 29 K to a non-collinear VBS state above T* ~ 70 K  13  . On the other hand, for BaCo2As2 the situation seems more complicated since several experimental studies suggest coexistence of three different magnetic phases  14, 15  : a commensurate antiferromagnetically ordered phase below TC = 38 K; a helimagnetic",
        "watermark_text": "We research the ground state properties of frustrated spin - 1 / 2 Heisenberg machines on square lattices with various types of interlayer couplings , including both homogeneous and inhomogeneous ones . We see that frustration can be suppressed by creating an additional ferromagnetic coupling between sheets which results to formation of inhomogeneous magnetic states characterized by spatially modulated magnetization profiles .The achieved findings are discussed within the framework of the recently established concept of ` ` inverse condensation . Introduction : In recent years there has been growing interest in investigating strongly interacting systems where competing interactions result to complex phase diagrams displaying various exotic phases such as valence bond solids ( VBS ) , charge density radiation ( CDW ) or supersolids 1 - 3 .One of the most important examples is provided by layered quantum antiferromagnets 4 . These compounds comprise of mildly coupled planes of spins arranged into a regular lattice structure .Due to heavy geometrical problems caused by differing nearest - neighbor exchange interactions J1 along the chain direction and J2 across the chains , these structures exhibit a rich range of physical phenomena ranging from standard Néel order at low temperatures down to disordered paramagnetic phases 5 . In this research we consider two prototypical representatives of this class of substances : CuGeO3 6 , where each plane consists of edge - sharing tetrahedra making a honeycomb - like network 7 , 8 , and BaCo2As2 9 , where the planes are making up of spot - sharing triangles 10 .Both compounds have garnered considerable scrutiny due to their extraordinary magnetic behavior 11 , 12 . For instance , it was shown experimentally that in CuGeO3 the system undergoes a shift from a collinear antiferromagnetically ordered state below TN = 29 K to a non - collinear VBS state above T * ~ 70 K 13 .On the other hand , for BaCo2As2 the situation appears more complicated since several experimental studies confirm coexistence of three different magnetic modes 14 , 15 : a commensurate antiferromagnetically ordered phase below TC = 38 K ; a helimagnetic",
        "rewrite_text": "We investigate the ground state properties of frustrated spin-1/2 Heisenberg systems on square lattices, examining various types of interlayer couplings, both uniform and non-uniform. Our findings reveal that frustration can be alleviated by introducing an additional ferromagnetic coupling between layers, which leads to the emergence of inhomogeneous magnetic states characterized by spatially varying magnetization profiles. These results are analyzed within the framework of the recently proposed concept of \"inverse condensation.\"\n\n**Introduction:** In recent years, there has been an increasing interest in exploring strongly interacting systems where competing interactions give rise to complex phase diagrams featuring a variety of exotic phases, such as valence bond solids (VBS), charge density waves (CDW), and supersolids. One prominent example is layered quantum antiferromagnets. These materials consist of weakly coupled spin planes arranged in a regular lattice structure. Due to significant geometric complications arising from differing nearest-neighbor exchange interactions (J1 along the chain direction and J2 between chains), these systems exhibit a diverse array of physical phenomena, ranging from conventional Néel order at low temperatures to disordered paramagnetic phases. In this study, we focus on two representative compounds from this category: CuGeO3, where each plane is composed of edge-sharing tetrahedra forming a honeycomb-like lattice, and BaCo2As2, which consists of planes made up of spot-sharing triangles. Both materials have attracted considerable attention due to their remarkable magnetic properties. For instance, experiments have demonstrated that in CuGeO3, the system transitions from a collinear antiferromagnetically ordered state below TN = 29 K to a non-collinear VBS state above T* ~ 70 K. In contrast, the situation in BaCo2As2 is more intricate, with several experimental studies confirming the coexistence of three distinct magnetic phases: a commensurate antiferromagnetically ordered phase below TC = 38 K, alongside other complex magnetic modes.",
        "ori-fast-z-score": 0.0854357657716761,
        "water-fast-z-score": 6.920297027505764,
        "rewrite-fast-z-score": 1.0795912380986197
    },
    {
        "original_text": "We present Spitzer Space Telescope observations in the mid- to far-infrared (5.8 - 160 microns) for a sample of distant galaxies selected by their rest-frame ultraviolet luminosity density at 1600 angstroms, and compare these flux densities with those predicted using models that include dust extinction.  We find that the observed infrared emission is generally higher than expected based on the UV continuum slope alone; this excess emission can be explained if there are significant amounts of cold dust associated with star formation activity in these systems.   The results suggest that the majority of the energy produced by young stars may not escape into intergalactic space but instead is reprocessed by interstellar dust grains before being re-radiated in the infrared. This work was supported by NASA through grants NAG5-9998 and NAS8-38252 issued by JPL/Caltech under contract NAS8-39073. It has been assigned the following DOI: 10.1086/505283",
        "watermark_text": "We use Spitzer Space Telescope observations in the mid - to far - infrared ( 5 . 8 - 160 microns ) for a sample of distant galaxies determined by their rest - frame ultraviolet luminosity abundance at 1600 angstroms , and compare these flux densities with those predicted using models that include cloud extinction . We see that the seen infrared absorption is typically higher than expected based on the UV continuum curve alone ; this excess emission can be understood if there are significant amounts of cold powder associated with star formation activity in these systems .The results show that the majority of the power emitted by young galaxies may not escape into intergalactic space but instead is reprocessed by interstellar dust grains before being re - radiated in the infrared . This project was supported by NASA through programs NAG5 - 9998 and NAS8 - 38252 issued by JPL / Caltech under contract NAS8 - 39073 .It has been allocated the following DOI : 10 . 1086 / 505283",
        "rewrite_text": "We analyze observations from the Spitzer Space Telescope in the mid- to far-infrared range (5.8 - 160 microns) for a selection of distant galaxies identified by their rest-frame ultraviolet luminosity at 1600 angstroms. We compare the observed flux densities to those predicted by models that account for cloud extinction. Our findings indicate that the infrared absorption is generally greater than anticipated based solely on the UV continuum. This excess emission may be explained by the presence of significant amounts of cold dust associated with star formation in these galaxies. The results suggest that a substantial portion of the energy emitted by young galaxies does not escape into intergalactic space; instead, it is reprocessed by interstellar dust grains before being re-emitted in the infrared. This research was supported by NASA through programs NAG5-9998 and NAS8-38252, managed by JPL/Caltech under contract NAS8-39073. The project has been assigned the following DOI: 10.1086/505283.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 5.5,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "We present results on the composition of the eta-prime meson in terms of quarks and gluons, obtained by applying the QCD sum rule method to the correlation function of two pseudoscalars with different flavors.  We find that the eta prime is dominantly composed of strange quark pairs (ss) at low energies but becomes more nonstrange as energy increases. The mass difference between the eta and eta prime mesons can be explained if we assume that the eta prime contains an additional ss pair compared to the eta. This result supports our previous finding based on the analysis of the decay widths of these mesons into pions and kaons. \nPACS numbers: 11.10.Kk, 12.38.Gc, 13 .60.Hb  PACS number(s): 11.10.Kn, 12.38.Bx, 13 .60.Jz",
        "watermark_text": "We present results on the composition of the eta - prime meson in terms of quarks and gluons , obtained by using the QCD sum rule method to the correlation function of two pseudoscalars with varying flavors . We see that the eta prime is dominantly composed of odd quark combinations ( ss ) at low energies but gets more nonstrange as energy rises .The mass ratio between the eta and eta prime mesons can be described if we suppose that the eta prime forms an additional ss pair relative to the eta . This result confirms our previous finding based on the examination of the decay widths of these mesons into pions and kaons .PACS codes : 11 . 10 . Kk , 12 . 38 . Gc , 13 . 60 . Hb PACS number ( s ) : 11 . 10 . Kn , 12 . 38 . Bx , 13 . 60 . Jz",
        "rewrite_text": "We report our findings on the quark and gluon composition of the eta-prime meson, derived using the QCD sum rule technique applied to the correlation function of two pseudoscalar mesons with different flavors. Our results indicate that the eta-prime is primarily made up of odd quark combinations (specifically ss) at low energies, but it increasingly incorporates nonstrange quark content as energy increases. The mass ratio between the eta and eta-prime mesons can be understood by suggesting that the eta-prime contains an additional ss pair compared to the eta. This outcome supports our earlier observations regarding the decay widths of these mesons into pions and kaons. PACS codes: 11.10.Kk, 12.38.Gc, 13.60.Hb; PACS numbers: 11.10.Kn, 12.38.Bx, 13.60.Jz.",
        "ori-fast-z-score": -0.9258200997725514,
        "water-fast-z-score": 3.7032803990902057,
        "rewrite-fast-z-score": 0.29488391230979427
    },
    {
        "original_text": "We present polarization observations made with the Australia Telescope Compact Array (ATCA) and Parkes radio telescopes towards the supernova remnant (SNR) G156.2+5. \n \n The ATCA data were taken in two observing runs, one on 1998 May 24-25 using the H214 array configuration and another on 1999 September 14-15 using the EW352 array configuration. These configurations provide baselines ranging between 15 m to 5 km. We observed for 3 hours per run, split into 12-minute scans interleaved by 10-second slews. The total bandwidth was 128 MHz centered around 1384 MHz. Data reduction followed standard procedures within MIRIAD. After flagging bad channels we produced Stokes Q and U maps which are shown in Figure 1 . \n \n Polarized intensity images were also created but these did not reveal any significant polarized emission above our noise level. This is consistent with previous single dish measurements that show no evidence for polarized emission associated with this source.",
        "watermark_text": "We present polarization images making with the Australia Telescope Compact Array ( ATCA ) and Parkes radio telescopes towards the supernova remnant ( SNR ) G156 . 2 + 5 . The ATCA results were took in two observing walks , one on 1998 May 24 - 25 using the H214 array configuration and another on 1999 September 14 - 15 use the EW352 array configuration .These modes provide baselines ranging between 15 m to 5 kilometres . We observed for 3 hours per run , separated into 12 - minute scans interleaved by 10 - second slews .The total bandwidth was 128 MHz centered around 1384 MHz . Data reduction followed standard methods within MIRIAD .After flagging bad channels we created Stokes Q and U maps which are shown in Figure 1 . Polarized intensity photos were also produced but these did not reveal any considerable polarized emission above our sound level .This is consistent with previous single dish measurements that demonstrate no evidence for polarized emission associated with this source .",
        "rewrite_text": "We present polarization images obtained using the Australia Telescope Compact Array (ATCA) and Parkes radio telescopes, focusing on the supernova remnant (SNR) G156.2+5. The ATCA observations were conducted during two separate sessions: the first from May 24-25, 1998, utilizing the H214 array configuration, and the second from September 14-15, 1999, using the EW352 array configuration. These configurations provided baselines ranging from 15 meters to 5 kilometers. Each observing session lasted for 3 hours, divided into 12-minute scans interspersed with 10-second slews. The total bandwidth was 128 MHz, centered at a frequency of 1384 MHz. Data reduction was carried out using standard procedures in MIRIAD. After flagging the problematic channels, we produced Stokes Q and U maps, which are displayed in Figure 1. While we also generated polarization intensity images, they did not show any significant polarized emission above our noise level, consistent with previous single-dish measurements that found no evidence of polarized emission from this source.",
        "ori-fast-z-score": 1.3363062095621219,
        "water-fast-z-score": 5.528438872198585,
        "rewrite-fast-z-score": 2.141798680385621
    },
    {
        "original_text": "We present the results of cosmological hydrodynamic simulations that follow the evolution of dark matter haloes within different cosmic environments (clusters, filaments, sheets and voids). We find that:\n(i) The mass accretion histories of clusters are dominated by major mergers with other massive systems at high redshifts z > 1.\n(ii) In contrast to clusters, most of the growth of filamentary structures is driven by smooth gas accretion along their length.  This leads to an extended formation history for these objects which can be traced back to early times z < 5. (iii) Sheet-like structures form through the merger of smaller filaments into larger ones. They grow mainly via smooth gas accretion but also experience minor mergers with small groups or galaxies during their lifetime. (iv) Voids evolve almost exclusively due to smooth gas accretion. Their assembly time-scales are typically longer than those of clusters and filaments because they have less dense surroundings.",
        "watermark_text": "We present the conclusion of cosmological hydrodynamic simulations that take the evolution of dark matter haloes within various cosmic environments ( complexes , filaments , sheets and voids ) . We see that : ( i ) The mass accretion histories of clusters are dominated by major mergers with other giant bodies at high redshifts z > 1 .( ii ) In comparison to groups , most of the development of filamentary structures is caused by smooth gas accretion along their duration . This leads to an extended structure life for these objects which can be traced back to early years z < 5 .( iii ) Sheet - like structures develop through the merger of tiny filaments into larger ones . They develop principally via smooth gas accretion but also experience minor mergers with little groups or galaxies during their lifetime .( iv ) Voids evolve virtually primarily due to soft gas accretion . Their development time - scales are typically longer than those of clusters and filaments because they have less dense surroundings .",
        "rewrite_text": "We present our findings from cosmological hydrodynamic simulations that explore the evolution of dark matter halos in various cosmic environments, including complexes, filaments, sheets, and voids. Our results indicate the following: (i) The mass accretion histories of galaxy clusters are primarily influenced by significant mergers with other massive bodies at high redshifts (z > 1). (ii) In contrast to groups, the formation of filamentary structures is largely driven by gradual gas accretion over time, resulting in a longer lifespan for these structures,which can be traced back to early epochs (z < 5). (iii) Sheet-like structures arise from the merging of smaller filaments into larger ones. Their development is mainly facilitated by smooth gas accretion, although they also experience minor mergers with smaller groups or galaxies throughout their existence. (iv) Voids evolve predominantly through gradual gas accretion, and their development timescales are generally longer than those of clusters and filaments due to their less dense environments.",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 1.3054598240132387
    },
    {
        "original_text": "We present near-infrared (NIR) observations of the central region of the Carina Nebulae obtained with ISAAC at the VLT in Chile, and compare them to previous optical studies by Smith et al. (2000) . The NIR data reveal new details on the structure of the nebular shell surrounding the open cluster Trumpler 16. We find that the brightest part of the shell is located between two dark lanes which are probably caused by dust extinction. In addition we detect several small knots embedded within the shell. These knots may be remnants of massive stars or protostars formed during an earlier phase of star formation activity in this region. Finally, we identify a number of Herbig-Haro objects associated with the open clusters Trumpler 14 and 15. Our results show that the Carina Nebulae is still actively forming stars today. This work was supported by the European Southern Observatory under programme ID 072.C-0488(B).",
        "watermark_text": "We take near - infrared ( NIR ) observations of the central region of the Carina Nebulae obtained with ISAAC at the VLT in Chile , and link them to previous optical images by Smith et al . ( 2000 ) .The NIR data reveal novel features on the composition of the nebular shell surrounding the open cluster Trumpler 16 . We see that the brightest part of the shell is situated between two darkened paths which are probably created by dust extinction .In addition we find various small knots embedded within the shell . These threads may be remnants of large planets or protostars produced during an previous process of star formation activity in this area .Finally , we identify a number of Herbig - Haro objects identified with the open spaces Trumpler 14 and 15 . Our results show that the Carina Nebulae is already actively creating stars today .This project was supported by the European Southern Observatory under series ID 072 . C - 0488 ( B ) .",
        "rewrite_text": "We conducted near-infrared (NIR) observations of the central region of the Carina Nebula using ISAAC at the VLT in Chile and integrated these with earlier optical images from Smith et al. (2000). The NIR data uncover new details about the composition of the nebular shell surrounding the open cluster Trumpler 16. Notably, the brightest area of the shell is located between two dark lanes likely caused by dust extinction. Additionally, we observe several small knots nestled within the shell, which may be remnants of large planets or protostars formed during previous star formation activities in this region. Furthermore, we identified several Herbig-Haro objects associated with the open clusters Trumpler 14 and 15. Our findings indicate that star formation is actively ongoing in the Carina Nebula. This research was supported by the European Southern Observatory under project number 072.C-0488(B).",
        "ori-fast-z-score": -0.12403473458920847,
        "water-fast-z-score": 6.154574548966636,
        "rewrite-fast-z-score": 0.6401843996644799
    },
    {
        "original_text": "We study the phenomenological consequences of general speed of sound in brane inflationary models, where the inflaton is identified as the distance between two parallel branes moving on an extra dimension. We find that for small values of the speed of sound (cs < 0.1), there are no significant changes to the predictions made by standard slow-roll inflation. However, when cs > 0.1 we find that the tensor-to-scalar ratio r and the running of the spectral index dns/d ln k can be significantly enhanced compared to their usual values predicted within the context of single field slow roll inflation. In particular, if cs = 1 then r = 16(nT)2/5 and dns/d ln k = −8(nT)1/5, which may provide a possible explanation for recent observations of high value of nT reported by WMAP7 data combined with other CMB experiments.",
        "watermark_text": "We explore the phenomenological consequences of general velocity of noise in brane inflationary theories , where the inflaton is identified as the distance between two connected branes moving on an additional dimension . We see that for little values of the speed of noise ( cs < 0 . 1 ) , there are no major changes to the estimates made by traditional slow - roll inflation .However , when cs > 0 . 1 we find that the tensor - to - scalar ratio p and the running of the spectral index dns / d ln k can be substantially enhanced compared to their conventional estimates predicted within the context of double field fast roll inflation . In particular , if cs = 1 then r = 16 ( nT ) 2 / 5 and dns / d ln k = −8 ( nT ) 1 / 5 , which would offer a possible reason for recent observations of high value of nT reported by WMAP7 results coupled with other CMB experiments .",
        "rewrite_text": "We investigate the phenomenological implications of a general velocity of noise in brane inflationary theories, where the inflaton corresponds to the distance between two connected branes moving within an additional dimension. Our findings indicate that for low noise speeds (cs < 0.1), traditional slow-roll inflation estimates remain largely unaffected. However, for higher speeds (cs > 0.1), we observe significant enhancements in the tensor-to-scalar ratio \\( r \\) and the running of the spectral index \\( \\frac{dns}{d \\ln k} \\) compared to the standard predictions derived from double field fast-roll inflation. Notably, when \\( cs = 1 \\), we find \\( r = \\frac{16(n_T)^2}{5} \\) and \\( \\frac{dns}{d \\ln k} = -8(n_T)^{1/5} \\), which could explain the recent observations of high \\( n_T \\) values reported by WMAP7 and other CMB experiments.",
        "ori-fast-z-score": -0.5163977794943222,
        "water-fast-z-score": 5.249512077248736,
        "rewrite-fast-z-score": 2.516611478423583
    },
    {
        "original_text": "The solar magnetic field is an important ingredient in many physical processes that take place on the Sun, such as coronal heating or solar wind acceleration. The open magnetic flux threading through the heliosphere plays also a crucial role for space weather prediction. In this work we present results obtained with the MHD model developed by Usmanov et al. (2010) to study the structure and dynamics of the Sun s open magnetic field. We show how the global properties of the simulated open magnetic field compare with observations made at 1 AU using spacecraft data. Our simulations reproduce well the observed latitudinal distribution of the open magnetic flux density and its dependence on the radial distance from the Sun. They also provide information about the temporal evolution of the open magnetic field which can be used to predict the state of the interplanetary medium several days ahead. This research was supported by NASA grants NNX10AC85G (PI: S. Riley), NNG09FA40C (PI: A. Schwadron), and NNM07AA01A (PI: J. McComas).",
        "watermark_text": "The solar magnetic force is an important ingredient in many mechanical phenomena that take place on the Sun , such as coronal heating or solar wind expansion . The open magnetic flux threading through the heliosphere serves also a crucial role for space weather prediction .In this research we present results derived with the MHD model used by Usmanov et al . ( 2010 ) to study the composition and dynamics of the Sun s open magnetic force .We see how the global properties of the simulated open magnetic force compare with observations made at 1 AU using satellite information . Our simulations reproduce well the seen latitudinal distribution of the open magnetic flux concentration and its dependence on the radial distance from the Sun .They also provide details about the temporal evolution of the open magnetic force which can be used to predict the state of the interplanetary medium several days ahead . This research was supported by NASA grants NNX10AC85G ( PI : S . Riley ) , NNG09FA40C ( PI : A . Schwadron ) , and NNM07AA01A ( PI : J . McComas ) .",
        "rewrite_text": "The solar magnetic force plays a crucial role in various mechanical phenomena occurring on the Sun, including coronal heating and the expansion of the solar wind. Additionally, the open magnetic flux that permeates the heliosphere is vital for predicting space weather. In this study, we present findings from the MHD model utilized by Usmanov et al. (2010) to investigate the composition and dynamics of the Sun's open magnetic force. We analyze how the overall characteristics of the simulated open magnetic force align with observations at 1 AU, as gathered by satellites. Our simulations accurately replicate the observed latitudinal distribution of open magnetic flux concentration and its relationship with the distance from the Sun. They also provide insights into the temporal evolution of the open magnetic force, which can assist in forecasting the conditions of the interplanetary medium several days in advance. This research was funded by NASA grants NNX10AC85G (PI: S. Riley), NNG09FA40C (PI: A. Schwadron), and NNM07AA01A (PI: J. McComas).",
        "ori-fast-z-score": 0.7875615306482168,
        "water-fast-z-score": 5.062895554167108,
        "rewrite-fast-z-score": 1.4342743312012722
    },
    {
        "original_text": "We present new space velocities for the nearest late-M, L-, and T-dwarf stars based on high-precision radial velocity measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile over an 8-year period (2003-2009). We find that all but one of these objects are members of young nearby open clusters or associations. The only exception is LP 944-20 which has been shown to be a member of the Hyades supercluster by its proper motion.  These results confirm previous suggestions that most brown dwarfs have ages less than 100 Myr. They also show that there may exist two populations among the very low-mass stars and brown dwarfs: those associated with open clusters/associations and those apparently not gravitationally bound to any cluster. This suggests that either some brown dwarfs form outside open clusters and/or they evaporate rapidly after formation. Finally we discuss possible explanations for the observed kinematics of the lowest mass stars and brown dwarfs.",
        "watermark_text": "We report new space velocities for the nearest late - M , L - , and T - giant galaxies using on wide - precision radial speed measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile over an 8 - year period ( 2003 - 2009 ) . We see that all but one of these objects are part of young nearby open complexes or associations .The only exception is LP 944 - 20 which has been shown to be a member of the Hyades supercluster by its proper motion . These data confirm previous suggestions that most brown dwarfs have ages less than 100 Myr .They also demonstrate that there may contain two communities among the very low - weight stars and green dwarfs : those associated with open complexes / associations and those presumably not gravitationally tied to any cluster . This implies that either some green dwarfs form outside open complexes and / or they evaporate rapidly after formed .Finally we talk proposed explanations for the known kinematics of the lowest mass stars and brown dwarfs .",
        "rewrite_text": "We present new space velocity measurements for the nearest late-M, L-, and T-dwarf galaxies, utilizing high-precision radial velocity data collected with the HARPS spectrograph at La Silla Observatory in Chile over an 8-year span from 2003 to 2009. Our findings indicate that nearly all of these objects belong to young, nearby open clusters or associations, with the sole exception being LP 944-20, which has been identified as a member of the Hyades supercluster based on its proper motion. These results support earlier hypotheses suggesting that most brown dwarfs are younger than 100 million years. Moreover, they indicate the potential existence of two distinct communities among very low-mass stars and brown dwarfs: one group affiliated with open clusters and associations, and another group that appears to be unassociated with any gravitational clusters. This raises the possibility that some brown dwarfs may form outside of open complexes or that they disperse rapidly after formation. Finally, we discuss proposed explanations for the observed kinematics of the lowest mass stars and brown dwarfs.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -1.7888543819998317
    },
    {
        "original_text": "We study the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension, focusing on its relaxation to equilibrium after being quenched across the superfluid-Mott insulator transition. We show that this system exhibits universal behavior at late times which is characterized by power-law decaying correlations and algebraic growth of entanglement entropy. The exponents are determined analytically using a mapping onto a classical statistical mechanics problem for a driven diffusive system. This work was supported by NSF grant PHY-0960291 (M.S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A.K.). \nI. INTRODUCTORY REMARkS\n\nThe recent experimental realization of quantum degenerate gases has opened up new avenues towards understanding strongly correlated many-body systems  1  . In particular, ultracold atomic gases have been used as model systems to explore phenomena such as fermionization  2  , supersolidity  3  , and Mott-insulating states  4  .\nIn this article we consider a particularly interesting class of experiments where the properties of these systems can be probed through their response to sudden changes in parameters  5  . For example, if the strength of inter-particle repulsion or density of particles is suddenly changed then it takes some time before the system reaches thermal equilibrium  6  . During this nonequilibrium evolution, the system may exhibit novel features like dynamical scaling  7, 8  and non-thermal fixed points  9  . These effects are not only important for our fundamental understanding of quantum matter but also provide useful insights into possible routes to realizing novel phases of matter  10  .\nRecently there has been considerable interest in studying the nonequilibrium dynamics of bosonic systems  11  . A particularly well studied case is when the initial state corresponds to a highly excited state above the ground state  12  . It turns out that even though the initial state is far away from equilibrium, the system relaxes to a steady state described by a Gibbs ensemble  13  . However, if the initial state is prepared deep inside the ordered phase, then the system does not",
        "watermark_text": "We explore the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension , concentrating on its relaxation to equilibrium after being quenched across the superfluid - Mott insulator transition . We see that this system displays universal behavior at late times which is characterized by power - law decaying correlations and algebraic growth of entanglement entropy .The exponents are estimated analytically using a mapping onto a traditional statistical mechanics problem for a driven diffusive system . This research was supported by NSF grant PHY - 0960291 ( M . S . )and DOE funds DE - FG03 - 92 - ER40701 and DE - SC0012704 ( A . K . ) . I .INTRODUCTORY REMARkS The recent experimental realization of quantum degenerate gases has opened up new avenues towards studying strongly interacting multiple - bodies systems 1 . In particular , ultracold nuclear gases have been used as model structures to examine processes such as fermionization 2 , supersolidity 3 , and Mott - insulating states 4 .In this article we imagine a particularly exciting group of studies where the properties of these systems can be probed through their response to unexpected changes in parameters 5 . For instance , if the strength of inter - atom repulsion or density of molecules is suddenly changed then it takes some time before the system reaches heat equilibrium 6 .During this nonequilibrium evolution , the system might exhibit new characteristics like dynamical scaling 7 , 8 and non - cooling fixed points 9 . These effects are not only important for our fundamental understanding of quantum matter but also provide useful insights into possible routes to realizing new phases of matter 10 .Recently there has been substantial interest in investigating the nonequilibrium dynamics of bosonic systems 11 . A notably well discussed case is when the first state corresponds to a highly excited state above the ground state 12 .It happens out that even though the first state is far back from equilibrium , the system relaxes to a steady state described by a Gibbs ensemble 13 . However , if the initial system is prepared deep inside the ordered phase , then the system does not",
        "rewrite_text": "We investigate the dynamics of a one-dimensional interacting Bose gas with repulsive contact interactions, focusing on its relaxation to equilibrium following a quench across the superfluid-to-Mott insulator transition. Our findings reveal that this system exhibits universal behavior at long times, marked by power-law decay of correlations and algebraic growth of entanglement entropy. We analytically estimate the relevant exponents through a mapping to a conventional statistical mechanics problem involving a driven diffusive system. This research was funded by NSF grant PHY - 0960291 (M.S.) and DOE grants DE - FG03 - 92 - ER40701 and DE - SC0012704 (A.K.).\n\n**I. INTRODUCTORY REMARKS**  \nThe recent advancement in experimental techniques for creating quantum degenerate gases has opened new avenues for the investigation of strongly interacting many-body systems. In particular, ultracold atomic gases serve as model systems to study phenomena such as fermionization, supersolidity, and Mott insulating states. This article envisions an exciting area of research in which the properties of these systems are explored through their responses to sudden changes in parameters. For example, altering the inter-atomic repulsion strength or the density of the molecules yields a time-dependent approach to thermal equilibrium. During this nonequilibrium evolution, the system may exhibit novel features such as dynamical scaling and non-cooling fixed points. These phenomena are significant not only for advancing our fundamental understanding of quantum matter but also for providing insights into pathways for realizing new phases of matter. Recently, there has been considerable interest in examining the nonequilibrium dynamics of bosonic systems. A particularly noteworthy scenario occurs when the system is initially in a highly excited state, well above the ground state. Surprisingly, even though the initial state is far from equilibrium, the system eventually relaxes to a steady state described by a Gibbs ensemble. However, if the initial conditions place the system deep within the ordered phase, it behaves differently.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 7.175639059928206,
        "rewrite-fast-z-score": 1.7962924780409972
    },
    {
        "original_text": "We propose an experiment to entangle two independent photons in the time domain, using only linear optical elements and single-photon detectors.  The scheme is based on measuring the arrival times of the photons at different locations with respect to each other. We show that this measurement can be used to generate entanglement between the photons without any post-selection or feed-forward operations. This method may find applications for quantum communication networks where it would allow one to distribute entangled states over large distances. Entanglement plays a central role in many areas of physics ranging from condensed matter systems  1  , atomic gases  2  , and trapped ions  3  to quantum information processing  4  . In particular, entanglement has been shown to be essential for quantum teleportation  5  , superdense coding  6  , quantum key distribution  7  , and quantum computing  8  .\nIn recent years there have been several proposals to create entanglement between distant particles  9  -  11  . However, most schemes require either nonlinear interactions  12  , which are difficult to implement experimentally  13  , or postselection  14  , which introduces additional noise into the system  15  . Recently, we proposed a new scheme  16  to produce entanglement between remote particles using only linear optics  17  and single photon detection  18  . Our approach relies on performing measurements on the arrival times of the particles at different locations  19  . Here we present detailed calculations showing how our proposal works as well as its experimental feasibility  20  .  Figure 1 shows a schematic diagram of our setup. Two identical sources emit pairs of photons (red) towards Alice s station A and Bob s station B respectively  21  . Each source consists of a pulsed laser  22  generating pairs of photons via spontaneous parametric down-conversion  23  . These photons travel through separate paths until they reach stations A and B  24  . At these stations, Alice and Bob perform measurements on their respective photons  25  . They measure the arrival times tA and tB  26  of...",
        "watermark_text": "We suggest an project to entangle two independent photons in the time realm , using only linear optical elements and single - photon detectors . The scheme is based on measuring the emergence periods of the photons at different places with regard to each other .We see that this measurement can be used to produce entanglement between the photons without any post - choice or feed - forward functions . This method may see useful for quantum communication networks where it would enable one to distribute entangled states over large distances .Entanglement plays a central role in multiple fields of science ranging from condensed matter structures 1 , atomic atoms 2 , and trapped ions 3 to quantum information processing 4 . In particular , entanglement has been shown to be crucial for quantum teleportation 5 , superdense coding 6 , quantum key distribution 7 , and quantum computing 8 .In past decades there have been numerous ideas to create entanglement between distant particles 9 - 11 . However , most schemes need either nonlinear interactions 12 , which are hard to execute experimentally 13 , or postselection 14 , which introduces additional noise into the process 15 .Recently , we presented a new method 16 to produce entanglement between remote particles utilizing only linear optics 17 and single photon detection 18 . Our solution consists on making observations on the entry rates of the molecules at different places 19 .Here we present detailed calculations demonstrating how our proposal works as also as its empirical feasibility 20 . Figure 1 shows a schematic diagram of our setup .Two similar sources emit pairs of photons ( red ) towards Alice s station A and Bob s station B respectively 21 . Each source consists of a pulsed laser 22 producing sets of photons via spontaneous parametric down - transfer 23 .These photons travel through different paths until they reach stations A and B 24 . At these stations , Alice and Bob conduct measurements on their respective photons 25 .They measure the arrival times tA and tB 26 of . . .",
        "rewrite_text": "We propose a project aimed at entangling two independent photons in the temporal domain using only linear optical components and single-photon detectors. The approach relies on measuring the arrival times of the photons at various locations in relation to one another. We demonstrate that this measurement can effectively generate entanglement between the photons without the need for post-selection or feed-forward mechanisms. This technique could prove beneficial for quantum communication networks, as it would facilitate the distribution of entangled states over substantial distances. Entanglement is fundamental in several scientific domains, including condensed matter physics, atomic systems, and trapped ions, as well as in quantum information processing. It is especially critical for applications such as quantum teleportation, superdense coding, quantum key distribution, and quantum computing. Over the past few decades, many strategies have been proposed to create entanglement between distant particles. However, most existing methods either require nonlinear interactions, which are challenging to implement experimentally, or rely on post-selection, which can introduce additional noise. Recently, we introduced a novel technique to generate entanglement between remote particles solely with linear optics and single-photon detection. Our method involves observing the entry rates of the photons at different locations. In this paper, we present detailed calculations that illustrate how our proposal functions and its practical feasibility. Figure 1 depicts a schematic representation of our setup, where two similar sources emit pairs of photons (shown in red) towards Alice's station A and Bob's station B. Each source consists of a pulsed laser that generates pairs of photons via spontaneous parametric down-conversion. These photons traverse distinct paths until they arrive at stations A and B, where Alice and Bob perform measurements on their respective photons, recording the arrival times \\( t_A \\) and \\( t_B \\).",
        "ori-fast-z-score": -0.5035088149780135,
        "water-fast-z-score": 6.88128713803285,
        "rewrite-fast-z-score": -0.33567254331867563
    },
    {
        "original_text": "We present an analysis on the spatial and velocity distributions of the young (<10Myr) stars within 0.5pc of Sgr A*, based on our spectroscopic survey with Subaru/HDS. We find that these stars are distributed into two groups along the line-of-sight; one is located at ~0.1pc to the west side of Sgr A* and another is found at ~0.3pc to its east side. The former group has a mean radial velocity of -200km/sec while the latter shows +100km/sec. These results suggest that there exist two distinct populations of young stars around Sgr A*; one is associated with the clockwise disk-like structure seen in infrared images and the other may be related to the counter-clockwise rotating ring-like feature recently discovered by Genzel et al. (2003) . In addition we have identified several new candidate members for the clockwise disk population.",
        "watermark_text": "We present an assessment on the spatial and speed distributions of the young ( < 10Myr ) stars within 0 . 5pc of Sgr A * , using on our spectroscopic study with Subaru / HDS . We see that these stars are distributed into two groups along the line - of - view ; one is situated at ~ 0 . 1pc to the west end of Sgr A * and another is found at ~ 0 . 3pc to its east side .The first group has a mean radial speed of - 200km / sec while the former shows + 100km / sec . These data suggest that there exist two separate populations of young galaxies around Sgr A * ; one is associated with the clockwise disk - like structure seen in infrared images and the other may be connected to the counter - clockwise expanding box - like feature newly discovered by Genzel et al .( 2003 ) . In addition we have discovered numerous new likely groups for the clockwise disk population .",
        "rewrite_text": "We provide an analysis of the spatial and velocity distributions of young stars (less than 10 million years old) located within 0.5 parsecs of Sgr A*. This assessment is based on our spectroscopic observations using Subaru/HDS. Our findings reveal that these stars are organized into two distinct groups along the line of sight. One group is positioned approximately 0.1 parsecs to the west of Sgr A*, while the other is located around 0.3 parsecs to the east. The first group exhibits a mean radial velocity of -200 km/s, whereas the second group displays a velocity of +100 km/s. These observations imply the existence of two separate populations of young stars around Sgr A*; one is linked to the clockwise disk-like structure observed in infrared images, while the other may be associated with the counter-clockwise expanding box-like feature recently identified by Genzel et al. (2003). Additionally, we have identified several new likely groups within the clockwise disk population.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": 0.3511234415883917
    },
    {
        "original_text": "We have studied the effect of masking out part of sky for CMB temperature fluctuations by using simulated data sets and found that it is important to take into account the effects of masked pixels when estimating angular power spectrum (APS) at large angles, especially for low multipoles l < 20. We also find that the APS estimated with the standard method based on spherical harmonics decomposition can be biased if there are significant correlations between masked and unmasked regions. In this work we propose an improved method which takes these effects into account. The new method has been applied to estimate the APS of the WMAP three-year data set after removing point sources detected above 5 mJy beam −1 . Our results show good agreement with those obtained directly from the full-sky map without any masking. \n \n Keywords: Cosmic microwave background radiation, Large angle anisotropy, Point source removal",
        "watermark_text": "We have researched the impact of masking out part of skies for CMB heat fluctuations by using modeled information sets and found that it is important to take into consideration the effects of masked pixels when estimating angular power spectrum ( APS ) at large angles , particularly for low multipoles l < 20 . We additionally find that the APS calculated with the standard method using on spherical harmonics decomposition can be biased if there are significant correlations between masked and unmasked areas .In this research we propose an better method which uses these changes into consideration . The revised formula has been used to estimate the APS of the WMAP three - year data set after removing point elements detected above 5 mJy beam −1 .Our results show good agreement with those acquired directly from the full - skies map without any masking . Keywords : Cosmic electromagnetic background radiation , Large angle anisotropy , Point source removal",
        "rewrite_text": "We investigated the effects of masking portions of the sky on Cosmic Microwave Background (CMB) temperature fluctuations using modeled datasets. Our findings highlight the significance of accounting for masked pixels when estimating the angular power spectrum (APS) at large angles, especially for low multipoles (l < 20). Moreover, we discovered that using the standard method based on spherical harmonics decomposition can lead to bias in the APS if there are substantial correlations between masked and unmasked areas. In this study, we propose an improved method that incorporates these effects. We applied this revised formula to estimate the APS of the WMAP three-year dataset after eliminating point sources identified above the threshold of 5 mJy beam⁻¹. Our results align well with those obtained directly from the unmasked full-sky map. Keywords: Cosmic Microwave Background, Large-angle anisotropy, Point source removal.",
        "ori-fast-z-score": -0.36650833306891567,
        "water-fast-z-score": 5.2532861073211246,
        "rewrite-fast-z-score": 0.5252257314388902
    },
    {
        "original_text": "We report on Suzaku observations for four active galactic nuclei (AGNs) detected by Swift/BAT survey, which are classified as  obscured AGNs  with column density larger than 10 24 cm-2 . We found that all these sources show strong Fe K emission lines and their line widths are broader than those expected from thermal broadening at kT = 100 keV. The observed line profiles can be reproduced well by relativistic disk reflection models including Compton scattering effects. These results suggest that there is an additional component to the X-ray continuum other than the standard thin accretion disks around supermassive black holes. In addition, we find that the iron abundance relative to solar value is higher than 1.5 times in three out of four objects. This suggests that the central engines of these obscured AGNs may have been buried under heavy dusty torii. Finally, we discuss possible origins of this new type of buried supermassive black holes based on our observational results.",
        "watermark_text": "We report on Suzaku measurements for four active galactic nuclei ( AGNs ) detected by Swift / BAT search , which are classified as obscured AGNs with column height larger than 10 24 mm - 2 . We showed that all these sources show light Fe K emission lines and their line widths are larger than those expected from radiation broadening at kT = 100 keV .The observed line profiles can be reproduced well by relativistic disk absorption theories including Compton scattering effects . These data suggest that there is an additional element to the X - ray continuum other than the standard narrow accretion disks around supermassive black holes .In addition , we find that the metal density relative to solar value is higher than 1 . 5 times in three out of four objects . This implies that the main engines of these obscured AGNs might have been trapped under heavy dusty torii .Finally , we explain possible origins of this new kind of hidden supermassive black holes using on our observational results .",
        "rewrite_text": "We present Suzaku measurements for four active galactic nuclei (AGNs) identified through the Swift/BAT survey, all of which are classified as obscured AGNs with column densities exceeding 10²⁴ cm⁻². Our findings indicate that each of these sources exhibits light Fe K emission lines, with line widths exceeding what would be expected from radiation broadening at kT = 100 keV. The observed line profiles can be accurately reproduced by relativistic disk absorption models that account for Compton scattering effects. This suggests that there is an additional component to the X-ray continuum beyond the standard narrow accretion disks surrounding supermassive black holes. Furthermore, we observe that the metallicity in three out of the four objects is greater than 1.5 times the solar value, indicating that the engines of these obscured AGNs may be shielded by significant dusty tori. Lastly, we discuss potential origins for this newly identified category of hidden supermassive black holes based on our observational results.",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 6.128258770283413,
        "rewrite-fast-z-score": 0.7385489458759964
    },
    {
        "original_text": "We present the results of our analysis of cosmic ray data collected by the PAMELA experiment in 2008 and 2009, which show an excess over background at energies between 1-10 GeV/nucleon that is consistent with being produced by particles accelerated in nearby supernova remnants (SNRs). We find that this signal can be explained if we assume that SNR RX J1713-3946 accelerates protons up to 10 TeV energy per nucleon. The observed fluxes are also compatible with those expected for other known sources such as pulsars or active galactic nuclei. However, these alternative scenarios cannot explain all features seen in the data set. In particular, they do not predict any significant anisotropy in arrival directions on angular scales below ~10 degrees. This prediction is confirmed by observations made using the Tibet ASγ air shower array. Finally, we discuss possible implications of our findings for models of particle acceleration in relativistic shocks.",
        "watermark_text": "We present the conclusion of our analysis of cosmic ray data taken by the PAMELA study in 2008 and 2009 , which show an amount over background at energies between 1 - 10 GeV / nucleon that is compatible with being produced by particles advanced in nearby supernova remnants ( SNRs ) . We see that this signal can be described if we suppose that SNR RX J1713 - 3946 accelerates protons up to 10 TeV power per nucleon .The observed fluxes are also consistent with those expected for other known sources such as pulsars or active galactic nuclei . However , these alternative situations cannot explain all characteristics found in the information pool .In particular , they do not predict any slight anisotropy in arrival angles on spatial scales below ~ 10 degrees . This prediction is confirmed by observations made using the Tibet ASγ air rain system .Finally , we explain possible possibilities of our findings for models of particle acceleration in relativistic shocks .",
        "rewrite_text": "We conclude our analysis of cosmic ray data collected by the PAMELA study during 2008 and 2009, which reveals a signal above background levels at energies between 1 and 10 GeV/nucleon. This signal is consistent with emissions from particles accelerated in nearby supernova remnants (SNRs). Specifically, we suggest that SNR RX J1713-3946 accelerates protons to energies reaching 10 TeV per nucleon. The observed fluxes align with those expected from other known sources, such as pulsars and active galactic nuclei; however, these alternatives fail to account for all the characteristics present in the data. Notably, they do not predict the slight anisotropy observed in arrival angles on spatial scales smaller than approximately 10 degrees. This prediction has been corroborated by findings from the Tibet ASγ air shower system. Finally, we discuss the implications of our results for models of particle acceleration in relativistic shocks.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": -0.2672612419124244
    },
    {
        "original_text": "The gamma ray emission in the energy range 100 MeV to 10 GeV is studied using data taken by EGRET on board CGRO during its first four years of operation (1991) (1992) (1993) (1994) . The analysis has been performed for two different regions, one centered at l = 0° and b = - 5° , which includes the galactic centre region, and another centered at l = 180° and b = + 5° . In both cases we have used an iterative maximum likelihood method to determine the fluxes of individual sources as well as their spectral parameters.  We find that there are three distinct components contributing to the observed gamma-ray flux above 1 GeV :  A diffuse component with a power law spectrum extending upto ~10 GeV .\nA point source located near Sgr A* with a power law spectrum .\nAn extended source towards the galactic center with a broken power law spectrum . \nWe also present results obtained when the same analysis was repeated after excluding the contribution due to the central part of the Galaxy .",
        "watermark_text": "The gamma radiation emission in the power range 100 MeV to 10 GeV is studied utilizing information taken by EGRET on board CGRO during its initial four seasons of operation ( 1991 ) ( 1992 ) ( 1993 ) ( 1994 ) . The studies has been performed for two different regions , one located at l = 0° and b = - 5° , which includes the galactic centre region , and another focused at l = 180° and b = + 5° .In both cases we have utilized an iterative limit probability technique to estimate the fluxes of different sources as also as their spectral parameters . We see that there are three different components contributing to the emitted beta - ray flux above 1 GeV : A diffuse component with a power law spectrum stretching upto ~ 10 GeV .A point source located near Sgr A * with a power law spectrum . An extended source towards the galactic center with a cracked power law spectrum .We additionally include findings obtained when the same evaluation was repeated after excluding the contribution owing to the central region of the Galaxy .",
        "rewrite_text": "The gamma radiation emission in the energy range of 100 MeV to 10 GeV is analyzed using data collected by the EGRET instrument on board the Compton Gamma Ray Observatory (CGRO) during its first four years of operation (1991–1994). The research focused on two distinct regions: one at galactic coordinates l = 0° and b = -5°, encompassing the galactic center, and another at l = 180° and b = +5°. An iterative limit probability technique was employed to estimate the fluxes of various sources and their spectral parameters in both regions. Our findings reveal three significant components contributing to the emitted gamma-ray flux above 1 GeV: a diffuse component characterized by a power-law spectrum extending up to approximately 10 GeV, a point source near Sgr A* with a power-law spectrum, and an extended source toward the galactic center exhibiting a broken power-law spectrum. We also present results from a similar analysis conducted after removing contributions from the central region of the Galaxy.",
        "ori-fast-z-score": -1.62746694241347,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 0.9299811099505543
    },
    {
        "original_text": "We have studied the pulse-to-pulse intensity variations in two pulsars, PSR B0826-34 and PSR B1133+16, using data obtained with the Parkes radio telescope at 1.4 GHz (92cm) and 2.7GHz(36cm). We find that both pulsars show strong evidence for subpulse drift bands which are modulated by an underlying pattern of weaker emission. The observed patterns can be explained as due to interference between different modes of emission within each beam. In addition we find that there is no significant difference in the subpulse modulation properties when comparing observations made at 36cm and those made at 92cm. This suggests that the physical processes responsible for producing these phenomena operate over a wide range of frequencies. These results provide further support for the idea that the phenomenon known as drifting subpulses may arise through some form of plasma instability operating on the open field lines near the magnetic poles of neutron stars. \n \n Keywords: Pulsar",
        "watermark_text": "We have researched the signal - to - pulse intensity variations in two pulsars , PSR B0826 - 34 and PSR B1133 + 16 , using data acquired with the Parkes radio telescope at 1 . 4 GHz ( 92cm ) and 2 . 7GHz ( 36cm ) . We see that both pulsars give strong evidence for subpulse slip lines which are modulated by an underlying pattern of weaker emission .The observed patterns can be described as owing to interference between various modes of emission within each light . In addition we find that there is no major variation in the subpulse modulation properties when matching observations made at 36cm and those conducted at 92cm .This implies that the physical processes responsible for producing these phenomena run over a broad variety of frequencies . These conclusions provide further evidence for the idea that the process known as drifting subpulses might arise through some kind of plasma instability working on the open field lines near the magnetic poles of neutron galaxies .Keywords: Pulsar",
        "rewrite_text": "We investigated the variations in signal-to-pulse intensity for two pulsars, PSR B0826-34 and PSR B1133+16, utilizing data obtained from the Parkes radio telescope at frequencies of 1.4 GHz (92 cm) and 2.7 GHz (36 cm). Our findings indicate that both pulsars exhibit strong evidence of subpulse slip lines influenced by an overarching pattern of weaker emissions. The observed phenomena can be attributed to interference between different modes of emission within each pulse. Moreover, we discovered no significant changes in the subpulse modulation characteristics when comparing observations taken at 36 cm with those at 92 cm. This suggests that the physical mechanisms driving these effects operate across a wide range of frequencies. These results further support the hypothesis that drifting subpulses may originate from some form of plasma instability occurring along the open field lines near the magnetic poles of neutron stars. Keywords: Pulsar",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 6.5484618759809905,
        "rewrite-fast-z-score": -1.697749375254331
    },
    {
        "original_text": "We present results for MHD simulations of the magneto-rotational instability (MRI) in a stratified, Keplerian disk threaded by an initially vertical magnetic field using the ZEUS-2D code. We consider both isothermal and adiabatic equations of state to examine how the MRI depends on the thermodynamics of the gas. In all cases we find that the growth rate of the fastest growing mode agrees well with linear theory predictions when normalized appropriately. However, there are significant differences between our runs depending upon whether or not they have reached steady-state equilibrium. For example, the saturated level of stress achieved at late times varies significantly among different models. This suggests that it may be difficult to accurately predict the saturation amplitude of the MRI unless one can perform very high resolution calculations which evolve over many orbital periods. Finally, we show that the inclusion of radiative cooling has little effect on the properties of the turbulence generated by the MRI.",
        "watermark_text": "We see results for MHD simulations of the magneto - rotational disturbance ( MRI ) in a stratified , Keplerian disk threaded by an initially vertical magnetic force using the ZEUS - 2D coding . We consider both isothermal and adiabatic equations of state to examine how the MRI depends on the thermodynamics of the gas .In all situations we find that the development frequency of the fastest growing mode agrees well with continuous theory expectations when normalized appropriately . However , there are significant variations between our runs depending upon whether or not they have achieved steady - state balance .For instance , the saturated amount of stress attained at late times changes significantly among different models . This implies that it could be impossible to correctly calculate the saturation amplitude of the MRI unless one can conduct very high resolution measurements which change over numerous orbital periods .Finally , we prove that the introduction of radiative cooling has little impact on the properties of the turbulence generated by the MRI .",
        "rewrite_text": "We present results from MHD simulations of the magneto-rotational instability (MRI) in a stratified, Keplerian disk subjected to an initial vertical magnetic field, utilizing the ZEUS-2D code. Both isothermal and adiabatic equations of state are considered to assess how the MRI is influenced by the gas's thermodynamic properties. Across all scenarios, we observe that the growth frequency of the fastest-growing mode aligns well with continuous theory predictions when properly normalized. However, substantial variations arise among our simulations based on whether they have reached a steady-state balance. For example, the level of stress achieved at late times shows significant differences across various models. This suggests that accurately determining the saturation amplitude of the MRI may require exceptionally high-resolution measurements conducted over multiple orbital periods. Lastly, we demonstrate that the inclusion of radiative cooling has minimal effect on the characteristics of the turbulence produced by the MRI.",
        "ori-fast-z-score": -1.1952286093343936,
        "water-fast-z-score": 5.737097324805089,
        "rewrite-fast-z-score": -1.0681034923744679
    },
    {
        "original_text": "We report on the discovery of an unexpected ring-like dark matter structure at the center of galaxy cluster CL0024+17, which is located about 3 billion light years away and has been studied extensively by many observational techniques including gravitational lensing.  The mass distribution inferred from strong gravitational lensing shows that there are two massive subclusters separated by 1 Mpc (3 arcmin) with a total mass of 2 x 10 15 h-1M_sun within a radius of 0.5h-1Mpc around their centers. We find that this double-cluster system can be well described as a binary merger model where each component consists of three components; one main halo and two smaller halos surrounding it. In addition to these six clusters, we also detect another small clump of galaxies near the center of the merging system whose position coincides with the peak of X-ray emission detected by Chandra satellite observations.",
        "watermark_text": "We report on the discovery of an unexpected ring - like dark matter formation at the center of galaxy region CL0024 + 17 , which is situated about 3 billion light years far and has been studied significantly by many observational techniques including gravitational lensing . The mass distribution inferred from strong gravitational lensing indicates that there are two huge subclusters separated by 1 Mpc ( 3 arcmin ) with a total mass of 2 x 10 15 g - 1M _ sun within a diameter of 0 . 5h - 1Mpc around their regions .We see that this double - cluster system can be well described as a binary merger model where each core consists of three components ; one main halo and two smaller halos covering it . In addition to these six complexes , we also observe another tiny clump of stars near the center of the merging system whose position coincides with the maximum of X - ray radiation observed by Chandra satellite observations .",
        "rewrite_text": "We present the discovery of an unexpected ring-like formation of dark matter at the center of the galaxy region CL0024+17, located approximately 3 billion light-years away. This area has undergone extensive study using various observational techniques, including gravitational lensing. The mass distribution indicated by strong gravitational lensing reveals the presence of two massive subclusters, separated by 1 Mpc (3 arcminutes), with a combined mass of 2 x 10^15 solar masses within a diameter of 0.5 h^-1 Mpc around their regions. Our analysis suggests that this double-cluster system can be effectively modeled as a binary merger, with each core composed of three components: one main halo and two smaller halos surrounding it. Additionally, we observe a small clump of stars near the center of the merging system, which aligns with the location of the peak X-ray emission detected by Chandra satellite observations.",
        "ori-fast-z-score": 0.8682431421244593,
        "water-fast-z-score": 5.333493587335964,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "We present an analysis of the distribution and properties of open clusters in the Galactic anticenter region, using data from the Two Micron All Sky Survey (2MASS). We have compiled a list of all open clusters with angular diameters larger than 1 arcmin within a distance of 8 kpc from the Sun. The total number is about 1000. This sample includes most known open clusters in this part of the Galaxy. Using photometric distances derived by fitting theoretical stellar evolutionary tracks to observed color-magnitude diagrams for each cluster we construct maps showing their spatial distributions as well as their luminosity functions. Our results show that there are two distinct populations of open clusters: one population located at galactocentric radii between 4 and 6 kpc which has ages less than 3 Gyr; another population located at galactocenric radii greater than 7 kpc which has ages older than 5 Gyr.",
        "watermark_text": "We publish an assessment of the distribution and features of open complexes in the Galactic anticenter region , using data from the Two Micron All Sky Survey ( 2MASS ) . We have published a list of all open complexes with angular diameters larger than 1 arcmin within a distance of 8 kpc from the Sun .The total number is about 1000 . This sample comprises most notable open objects in this area of the Galaxy .Using photometric distances derived by fitting experimental stellar evolutionary tracks to observed color - magnitude diagrams for each cluster we create maps showing their spatial distributions as well as their luminosity functions . Our results show that there are two different populations of open complexes : one community located at galactocentric radii between 4 and 6 kpc which has ages less than 3 Gyr ; another population located at galactocenric radii greater than 7 kpc which has ages younger than 5 Gyr .",
        "rewrite_text": "We present an analysis of the distribution and characteristics of open complexes in the Galactic anticenter region, utilizing data from the Two Micron All Sky Survey (2MASS). We have compiled a comprehensive list of all open complexes with angular diameters exceeding 1 arcminute within an 8 kpc radius from the Sun, totaling approximately 1,000. This sample includes the most significant open objects in this part of the Galaxy. By applying photometric distances obtained through the fitting of theoretical stellar evolutionary tracks to observed color-magnitude diagrams for each cluster, we generate maps that illustrate their spatial distributions and luminosity functions. Our findings reveal the existence of two distinct populations of open complexes: one group located at galactocentric radii between 4 and 6 kpc, with ages less than 3 billion years, and another group at galactocentric radii greater than 7 kpc, with ages younger than 5 billion years.",
        "ori-fast-z-score": -0.6622661785325219,
        "water-fast-z-score": 3.9391929857916765,
        "rewrite-fast-z-score": -0.5163977794943222
    },
    {
        "original_text": "We present new results on the age, metallicity and alpha-element abundance for galactic globular clusters (GGCs) based on single stellar population models with different prescriptions for convection theory. We find that the ages derived by using the classical mixing-length theory are systematically younger than those obtained by assuming overshooting or semiconvection in the red giant branch phase. The difference between these two sets of ages is about 0.5 Gyr at most. For some metal-rich GGCs, we also found that their ages inferred from the classical mixing-length theory can be as young as 10 Gyr while they should have been older than 12 Gyr according to other methods. This discrepancy may result from the fact that the classical mixing-length theory cannot reproduce well the observed color-magnitude diagrams of such metal-rich GGCs. Our results show that there exists no significant correlation between the cluster s age and its metallicity.  These findings suggest that the formation history of GGCs might not be dominated by monolithic collapse but instead by hierarchical merging processes.",
        "watermark_text": "We report new data on the age , metallicity and alpha - component availability for galactic globular complexes ( GGCs ) based on single stellar community theories with various prescriptions for convection hypothesis . We see that the periods derived by using the classical mixing - length theory are systematically younger than those generated by assuming overshooting or semiconvection in the red dwarf branch period .The difference between these two sets of periods is about 0 . 5 Gyr at most . For some metal - rich GGCs , we also discovered that their ages inferred from the theoretical mixing - length theory can be as early as 10 Gyr while they should have been older than 12 Gyr according to other methods .This discrepancy may arise from the fact that the classical mixing - length theory cannot reproduce well the known color - magnitude diagrams of such metal - rich GGCs . Our results show that there exists no important relationship between the cluster s age and its metallicity .These studies propose that the formation history of GGCs might not be dominated by monolithic collapse but instead by hierarchical merging cycles .",
        "rewrite_text": "We present new findings on the age, metallicity, and availability of alpha components for galactic globular clusters (GGCs), derived from single stellar population models with different convection assumptions. Our analysis indicates that periods calculated using classical mixing-length theory are consistently younger than those obtained by incorporating overshooting or semiconvection in the red dwarf branch. The maximum difference between these two sets of periods is approximately 0.5 Gyr. Additionally, for some metal-rich GGCs, we found that ages estimated via the classical mixing-length approach could be as early as 10 Gyr, while other methods suggest they should be over 12 Gyr old. This inconsistency may be due to the classical mixing-length theory's inability to accurately reproduce the known color-magnitude diagrams of these metal-rich GGCs. Our findings indicate that there is no significant relationship between a cluster's age and its metallicity. These results suggest that the formation history of GGCs may be influenced more by hierarchical merging processes rather than being primarily driven by monolithic collapse.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 5.142956348249516,
        "rewrite-fast-z-score": 1.091089451179962
    },
    {
        "original_text": "We present an overview of the theory for vortices in trapped, dilute atomic gases at low temperatures. We discuss how these systems can be described by macroscopic wave functions and show that they are governed by nonlinear Schrödinger equations with external potentials. The solutions to this equation have been studied extensively over many years and we review some of their properties relevant to vortex formation. In particular, we consider stationary states which correspond to condensate configurations without rotation (vortex-free) as well as rotating ones where quantized angular momentum is carried by phase singularities known as vortices. Finally, we briefly describe recent experiments on vortex production in cold atom clouds. Vortices occur naturally in superfluids such as liquid helium or dilute atomic gases. They carry quantized angular momenta and play important roles in various physical phenomena including turbulence and quantum transport processes. Here we give an introduction into the theoretical description of vortices in trapped atomic gases.",
        "watermark_text": "We present an overview of the principle for vortices in trapped , dilute atomic materials at low temperatures . We discuss how these systems can be described by macroscopic wave distributions and explain that they are governed by nonlinear Schrödinger coefficients with external potentials .The solutions to this equation have been studied frequently over numerous years and we review some of their characteristics applicable to vortex structure . In particular , we investigate stationary states which refer to condensate configurations without rotation ( vortex - safe ) as well as rotating ones where quantized angular velocity is carried by phase singularities known as vortices .Finally , we briefly illustrate recent experiments on vortex production in cold atom clouds . Vortices arise naturally in superfluids such as fluid helium or dilute nuclear gases .They carry quantized angular momenta and play crucial roles in different mechanical phenomena including turbulence and quantum transport systems . Here we give an overview into the theoretical description of vortices in trapped atomic gases .",
        "rewrite_text": "We provide an overview of the principles governing vortices in trapped, dilute atomic gases at low temperatures. We explain how these systems can be effectively described by macroscopic wave functions, governed by nonlinear Schrödinger equations that include external potentials. The solutions to this equation have been extensively studied over the years, and we review key characteristics relevant to vortex structures. Specifically, we examine stationary states, which include non-rotating condensate configurations (vortex-free) and rotating states characterized by quantized angular momentum, represented by phase singularities known as vortices. Additionally, we highlight recent experiments focused on vortex formation in cold atomic clouds. Vortices naturally occur in superfluids, such as liquid helium and dilute nuclear gases, carrying quantized angular momentum and playing significant roles in various mechanical phenomena, including turbulence and quantum transport. In this context, we present an overview of the theoretical framework for understanding vortices in trapped atomic gases.",
        "ori-fast-z-score": -1.1470786693528088,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": 0.7875615306482168
    },
    {
        "original_text": "We present deep imaging data for the nearby dwarf spheroidal galaxy, Hercules (dSph), obtained with the Large Binocular Telescope (LBT). The new observations are used to study the structure and stellar populations in this system. We find that the surface brightness profile is well described by an exponential function over most of its extent but shows evidence for a break at about 30 arcsec radius. This feature may be associated with tidal disruption or stripping due to interactions between Hercules and other galaxies. Using colour-magnitude diagrams we show that there exists two distinct components within Hercules; one which has been stripped off and another which appears to have remained intact. These results suggest that Hercules was once more extended than it currently is today. Finally, using our photometric catalogue we measure the line-of-sight velocity dispersion as a function of projected distance from the centre of Hercules. Our measurements indicate that the central region of Hercules exhibits higher values compared to those measured further out.",
        "watermark_text": "We present deep imaging information for the nearby dwarf spheroidal galaxy , Hercules ( dSph ) , obtained with the Large Binocular Telescope ( LBT ) . The newest observations are using to study the composition and stellar environments in this system .We see that the surface brightness profile is well described by an exponential function over most of its extent but shows proof for a break at about 30 arcsec radius . This phenomenon might be involved with tidal disruption or stripping due to interactions between Hercules and other stars .Using colour - magnitude diagrams we find that there exists two separate phases within Hercules ; one which has been stripped off and another which appears to have remained intact . These data suggest that Hercules was once more extended than it currently is presently .Finally , using our photometric catalogue we measure the line - of - view velocity dispersion as a function of projected distance from the centre of Hercules . Our measurements indicate that the main region of Hercules exhibits higher values compared to those observed further out .",
        "rewrite_text": "We present detailed imaging data for the nearby dwarf spheroidal galaxy Hercules (dSph), obtained through observations with the Large Binocular Telescope (LBT). These recent observations are utilized to analyze the composition and stellar environments within this galaxy. Our findings indicate that the surface brightness profile can be accurately described by an exponential function over most of its range, although there is evidence of a break occurring at approximately 30 arcseconds in radius. This phenomenon may be linked to tidal disruption or stripping as a result of interactions between Hercules and surrounding stars. Through the analysis of color-magnitude diagrams, we identify two distinct phases within Hercules: one that has been stripped away and another that appears to remain intact. These findings suggest that Hercules was once more extensive than it is today. Lastly, using our photometric catalog, we assess the line-of-sight velocity dispersion in relation to the projected distance from the center of Hercules. Our measurements reveal that the central region of Hercules exhibits higher velocity dispersion values compared to those observed at greater distances.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": 1.462614271203831
    },
    {
        "original_text": "The concept of defects in crystals has been developed by the Russian school since the 1930s. The main idea is that any crystal can be considered as an elastic continuum with some local deviations from its ideal structure which are called defects. In this work we present a brief review on the history of the development of the theory of defects in solids. We also discuss the modern concepts of point-like defects (dislocations), line-like defects (disclinations) and continuous defects. Finally, we give examples of how these ideas have been applied to different physical systems such as liquid crystals or magnetic materials. Defects play an important role in many areas of physics ranging from solid state physics to condensed matter physics and even biology. They appear naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures. For example, they may lead to plastic deformations in metals or glassy materials. On the other hand, defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization.",
        "watermark_text": "The concept of flaws in crystals has been originated by the Russian school since the 1930s . The main idea is that any solid can be regarded as an elastic continuum with some local deviations from its ideal structure which are called flaws .In this research we present a brief review on the history of the development of the principle of flaws in solids . We especially examine the newer concepts of point - like defects ( dislocations ) , edge - like defects ( disclinations ) and continuous defects .Finally , we give evidence of how these ideas have been used to different physical structures such as fluid crystals or magnetic materials . Defects serve an important role in many fields of science ranging from solid state mechanics to condensed matter science and even biology .They arise naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures . For instance , they may contribute to plastic deformations in metals or glassy materials .On the other hand , defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization .",
        "rewrite_text": "The concept of defects in crystals originated from the Russian school in the 1930s. The fundamental notion is that any solid can be viewed as an elastic continuum, with local deviations from its ideal structure identified as defects. In this research, we provide a concise overview of the evolution of the concept of defects in solids. We particularly focus on contemporary ideas surrounding point-like defects (dislocations), edge-like defects (disclinations), and continuous defects. Furthermore, we demonstrate how these concepts have been applied to various physical structures, such as fluid crystals and magnetic materials. Defects play a crucial role across many scientific domains, including solid-state mechanics, condensed matter physics, and even biology. They naturally emerge during phase transitions between ordered states, such as those occurring at melting points or critical temperatures. For example, these defects can contribute to plastic deformations in metals and glassy materials. Additionally, they are key to the macroscopic properties of solids, influencing characteristics like electrical conductivity and magnetization.",
        "ori-fast-z-score": 0.5555555555555556,
        "water-fast-z-score": 6.184165460191406,
        "rewrite-fast-z-score": 2.49100947511811
    },
    {
        "original_text": "We present an algorithm that systematically scans all possible 7-colourings of the grid, and report on its performance in terms of running time and memory consumption. The algorithm is based on a simple backtracking scheme combined with some heuristics to prune parts of the search space. We have implemented this algorithm using Java 1.6 and tested it on several instances ranging from small grids up to large ones containing more than one million nodes. For each instance we provide detailed information about how much time was spent by our program during colouring as well as how many colours were used. In addition, we also show how these results compare against those obtained by other algorithms proposed recently in the literature. \n \n Keywords: Coloring problems, Computational complexity theory, Graphs, Backtrack search, Heuristic methods, Grid graphs, Integer programming, Optimization problems, Search trees, Time-complexity analysis \n \n \n \n INTRODUCTION \n \n A graph G = (V, E) consists of two sets V and E where V denotes the set of vertices or nodes and E denotes the set of edges between pairs of nodes. An edge e=(u,v) connects node u ∈ V to v ∈ V . If there exists no such connection then e is not included in E. A path P is defined as a sequence of distinct nodes v1 , v2 , … , vn such that vi−1vi ∈ E for i = 2 , 3 , … , n . A cycle C is defined as a path whose first and last nodes are identical. A connected component is a subgraph H of G which has the property that any pair of nodes in H can be joined by a path within H but cannot be joined by paths outside H. A clique K is a complete subgraph of G; that is, every pair of nodes in K is adjacent to each other. A k-clique is a clique consisting of exactly k nodes. A vertex cover S is a subset of V such that every edge in G has at least one endpoint in S. A dominating set D is a subset of V",
        "watermark_text": "We create an algorithm that systematically scans all possible 7 - colourings of the grid , and report on its effectiveness in terms of running time and memory usage . The algorithm is based on a simple backtracking scheme coupled with some heuristics to prune parts of the search space .We have applied this algorithm using Java 1 . 6 and demonstrated it on numerous instances ranging from small grids up to large ones featuring more than one million nodes . For each instance we provide comprehensive information about how many time was spent by our software during colouring as also as how many colours were used .In addition , we also demonstrate how these results compare against those achieved by other methods suggested late in the literature . Keywords : Coloring questions , Computational complexity analysis , Graphs , Backtrack search , Heuristic methods , Grid graphs , Integer programming , Optimization problems , Search trees , Time - complexity analysis INTRODUCTION A graph G = ( V , E ) contains of two sets V and E where V denotes the set of vertices or nodes and E denotes the set of vertices between pairs of vertices .An path e = ( v , v ) links node e ∈ V to v ∈ V . If there exists no such connection then e is not added in E . A path P is given as a sequence of distinct nodes v1 , v2 , … , vn such that vi−1vi ∈ E for i = 2 , 3 , … , n .A cycle C is characterized as a path whose first and last nodes are equal . A linked component is a subgraph H of G which has the feature that any pair of vertices in H can be joined by a path within H but cannot be joined by tracks outside H . A clique K is a complete subgraph of G ; that is , every pair of vertices in K is adjoining to each other .A k - clique is a clique consisting of exactly k nodes . A vertex cover S is a subset of V such that every edge in G has at least one endpoint in S . A dominating setting D is a subset of V",
        "rewrite_text": "We have developed an algorithm that thoroughly examines all possible 7-colorings of a grid and evaluates its performance in terms of running time and memory consumption. This algorithm utilizes a straightforward backtracking approach combined with heuristics to eliminate unnecessary parts of the search space. We implemented this algorithm using Java 1.6 and tested it on a variety of instances, from small grids to large grids containing over one million nodes. For each instance, we provide detailed information about the time taken by our software during the coloring process, as well as the number of colors utilized. Furthermore, we compare our results with those obtained from other methods suggested in recent literature. \n\n**Keywords:** Coloring problems, Computational complexity analysis, Graphs, Backtracking search, Heuristic methods, Grid graphs, Integer programming, Optimization problems, Search trees, Time complexity analysis.\n\n**INTRODUCTION:** A graph \\( G = (V, E) \\) consists of two sets: \\( V \\), which represents the set of vertices or nodes, and \\( E \\), which indicates the connections between pairs of vertices. An edge \\( e = (v, v') \\) connects nodes \\( v, v' \\in V \\). If such a connection does not exist, \\( e \\) is not included in \\( E \\). A path \\( P \\) is defined as a sequence of distinct nodes \\( v_1, v_2, \\ldots, v_n \\) where \\( v_{i-1}v_i \\in E \\) for \\( i = 2, 3, \\ldots, n \\). A cycle \\( C \\) is a path that begins and ends at the same node. A connected component \\( H \\) of \\( G \\) is a subgraph in which any pair of vertices can be connected by a path within \\( H \\), but cannot be linked by paths outside of it. A clique \\( K \\) is a complete subgraph of \\( G \\), where every pair of vertices within \\( K \\) is adjacent. A \\( k \\)-clique is a clique composed of exactly \\( k \\) nodes. A vertex cover \\( S \\) is a subset of \\( V \\) that ensures each edge in \\( G \\) has at least one endpoint in \\( S \\). A dominating set \\( D \\) is a subset of \\( V \\) that...",
        "ori-fast-z-score": 0.8770580193070293,
        "water-fast-z-score": 7.367287362179046,
        "rewrite-fast-z-score": 0.6163156344279367
    },
    {
        "original_text": "The Large Hadron Collider (LHC) is expected to produce new particles that could be dark matter candidates, such as supersymmetric partners of quarks or leptons.  In this talk I will discuss how these signatures can be searched for using data collected by the ATLAS experiment during Run 1 of the LHC. The results are presented both in terms of limits on production cross sections and mass exclusion regions. Finally, prospects for future searches with Run 2 data are discussed. This work was performed within the framework of the PhD thesis of M.A.M., supervised by A.S.. \nIntroduction\n\nDark Matter Candidates\n\nSupersymmetry\n\nATLAS Experiment\n\nRun 1 Results\n\nProspects for Run 2 Searches\n\nConclusions & Outlook \n\nReferences \n\n\nAcknowledgements\n\n\n\n\n\n- - - - - - - -- - - - - - --- - - - - - ---- - - - - - ----- - - - - - ------ - - - - - -------- - - - - - ---------- - - - - - ------------------ - - - - - -------------- - - - - - ----------------------------------- - - - - - ----------------------------------------------------- - - - - - ------------------------------------------------------------------------------------ - - - - -",
        "watermark_text": "The Large Hadron Collider ( LHC ) is expected to produce new objects that might be dark matter candidates , such as supersymmetric partners of quarks or leptons . In this talk I will explore how these signatures can be searched for utilizing information collected by the ATLAS experiment during Run 1 of the LHC .The results are presented both in terms of limits on production cross sections and mass isolation regions . Finally , prospects for future investigations with Run 2 data are discussed .This project was done within the framework of the PhD thesis of M . A . M . , overseen by A . S . . Introduction Dark Matter Candidates Supersymmetry ATLAS Experiment Run 1 Results Prospects for Run 2 Searches Conclusions & Outlook References Acknowledgements - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -",
        "rewrite_text": "The Large Hadron Collider (LHC) is anticipated to generate new particles that could serve as candidates for dark matter, including the supersymmetric partners of quarks and leptons. In this presentation, I will examine how we can identify these signatures by utilizing data gathered by the ATLAS experiment during Run 1 of the LHC. The findings will be discussed in terms of limits on production cross sections and mass isolation regions. Additionally, I will outline prospects for future research using data from Run 2. This work was conducted as part of M. A. M.’s PhD thesis under the supervision of A. S. \n\n**Outline**: \n- Introduction\n- Dark Matter Candidates\n- Supersymmetry\n- ATLAS Experiment\n- Run 1 Results\n- Prospects for Run 2 Searches\n- Conclusions & Outlook\n- References\n- Acknowledgements",
        "ori-fast-z-score": 1.2977713690461004,
        "water-fast-z-score": 4.767570631855362,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We revisit the constraints imposed by Big Bang Nucleosynthesis (BBN) and Cosmic Microwave Background Radiation (CMBR) observations on the possible existence of massive, unstable neutrinos with lifetimes longer than 10^(10) s. We find that these limits are significantly weaker if one allows for non-standard interactions between neutrinos and photons in addition to standard weak interactions. In particular we show that BBN can be compatible with an unstable tau-neutrino mass as large as mtau = 1 TeV even when its decay rate is suppressed by a factor of 10^{10} compared to the Standard Model prediction. This result has important implications for models of leptogenesis which require heavy Majorana masses for right-handed neutrinos. The possibility of such light unstable particles also opens up new avenues for probing physics beyond the Standard Model at future colliders like LHC or ILC.",
        "watermark_text": "We revisit the limitations imposed by Big Bang Nucleosynthesis ( BBN ) and Cosmic Microwave Background Radiation ( CMBR ) observations on the possible existence of large , unstable neutrinos with lifetimes greater than 10 ^ ( 10 ) s . We see that these limits are greatly weaker if one permits for non - standard interactions between neutrinos and photons in addition to standard weak interactions . In particular we prove that BBN can be compatible with an volatile tau - neutrino mass as big as mtau = 1 TeV even when its degradation rate is suppressed by a factor of 10 ^ { 10 } compared to the Standard Model prediction .This result has important considerations for models of leptogenesis which require heavy Majorana masses for right - handed neutrinos . The possibility of such light unstable objects already opens up new avenues for probing dynamics beyond the Standard Model at possible colliders like LHC or ILC .",
        "rewrite_text": "We reassess the constraints set by Big Bang Nucleosynthesis (BBN) and Cosmic Microwave Background Radiation (CMBR) observations concerning the potential presence of large, unstable neutrinos with lifetimes exceeding \\(10^{10}\\) seconds. Our analysis indicates that these constraints are significantly relaxed if we consider non-standard interactions between neutrinos and photons alongside the standard weak interactions. Notably, we demonstrate that BBN can accommodate a highly unstable tau neutrino with a mass as large as \\(m_{\\tau} = 1 \\, \\text{TeV}\\), even when its decay rate is reduced by a factor of \\(10^{10}\\) compared to the predictions of the Standard Model. This finding has significant implications for leptogenesis models that necessitate heavy Majorana masses for right-handed neutrinos. The prospect of such lightweight unstable particles opens up new possibilities for investigating physics beyond the Standard Model at potential colliders like the LHC or ILC.",
        "ori-fast-z-score": 0.42008402520840293,
        "water-fast-z-score": 4.900980294098034,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The discovery that the universe is expanding at an accelerating rate has led to intense interest in dark energy as well as new ideas about fundamental physics. This talk will review some recent results on these topics including constraints on models for cosmic acceleration using supernovae data, measurements of the Hubble constant with Type Ia supernovae, and tests of general relativity using gravitational lensing statistics. The talk will also discuss how future surveys such as LSST can be used to further our understanding of dark energy and fundamental physics. I will conclude by discussing my own work on testing gravity theories beyond Einstein s theory using weak lensing observations. Keywords: Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background. Speaker: Adam Riess (Princeton University) Date: February 18, 2007 Time: 4:30pm - 5:15pm Location: Room B",
        "watermark_text": "The observation that the universe is evolving at an accelerating rate has led to intense interest in dark energy as well as fresh concepts about basic physics . This discussion will review some latest findings on these topics including constraints on estimates for cosmic acceleration using supernovae information , measurements of the Hubble constant with Type Ia supernovae , and experiments of general relativity employing gravitational lensing statistics .The speech will also discuss how potential experiments such as LSST can be used to further our knowing of deep energy and fundamental theory . I will conclude by reviewing my own research on proving gravity fields beyond Einstein s principle involving weak lensing observations .Keywords : Cosmology , Dark Energy , General Relativity , Weak Lensing , Supernovae , Gravitational Waves , Cosmic Microwave Background . Speaker : Adam Riess ( Princeton University ) Date : February 18 , 2007 Time : 4 : 30pm - 5 : 15pm Location : Room B",
        "rewrite_text": "The observation that the universe is expanding at an accelerating pace has sparked significant interest in dark energy and new ideas in fundamental physics. This discussion will highlight recent developments in these areas, including constraints on cosmic acceleration estimates derived from supernova data, measurements of the Hubble constant through Type Ia supernovae, and general relativity experiments utilizing gravitational lensing statistics. Additionally, the talk will explore how future experiments, such as the Large Synoptic Survey Telescope (LSST), could enhance our understanding of dark energy and fundamental theories. I will conclude by presenting my research on exploring gravitational fields beyond Einstein's framework through weak lensing observations. \n\n**Keywords:** Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background.  \n**Speaker:** Adam Riess (Princeton University)  \n**Date:** February 18, 2007  \n**Time:** 4:30 PM - 5:15 PM  \n**Location:** Room B",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 5.337745350213779,
        "rewrite-fast-z-score": 2.449489742783178
    },
    {
        "original_text": "We report on neutron scattering experiments performed to study spin fluctuations and magnetic correlations in the metallic phase of quasi-two-dimensional organic superconductor κ-(BEDT-TTF)2Cu N(CN)2 Br (κ-Br). We find that the temperature dependence of the static susceptibility χ0 is well described by the Curie-Weiss law with an antiferromagnetic Weiss constant θ = -26 K, indicating strong antiferromagnetic interactions between spins. The observed broadening of the elastic linewidth Γel at low temperatures indicates short-range spin-spin correlation lengths ξs ~ 5 nm. In addition we observe a large enhancement of the dynamic susceptibility χ′′(Q,ω), which can be attributed to the development of low-energy spin excitations below T* ~ 50 K. These results are consistent with theoretical predictions for two-dimensional systems close to quantum criticality. Our data suggest that the system undergoes a transition into a state where the Fermi surface becomes unstable against formation of electron-hole pairs leading to Cooper pairing. \n \n Introduction \n \n A number of recent studies have shown that many strongly correlated electronic materials exhibit unconventional properties such as high-temperature superconductivity or non-Fermi liquid behavior  1  . One important aspect of these phenomena is the presence of collective charge and/or spin degrees of freedom  2  , whose dynamics often give rise to characteristic features in the excitation spectrum  3  . For example, in cuprate-based high-temperature superconductors  4  , it has been suggested that the pseudogap regime  5  may arise due to competing orders  6  originating from different regions of the Brillouin zone  7, 8  . Similarly, in iron pnictide compounds  9  , the appearance of a spin-density wave order parameter  10  leads to a suppression of the density-of-states near the Fermi level  11  resulting in a partial gap opening  12  . Finally, in heavy fermion metals  13  , the hybridization of localized f-electrons  14  gives rise to a nontrivial momentum structure of the self-energy  15  .\n \nIn this work, we present detailed measurements of the spin fluctuation spectrum in the metallic phase of the quasi",
        "watermark_text": "We report on neutron scattering experiments conducted to study spin fluctuations and magnetic correlations in the metallic phase of quasi - two - dimensional organic superconductor κ - ( BEDT - TTF ) 2Cu N ( CN ) 2 Br ( κ - Br ) . We see that the temperature dependence of the static susceptibility χ0 is well described by the Curie - Weiss law with an antiferromagnetic Weiss constant θ = - 26 K , showing strong antiferromagnetic interactions between spins .The observed broadening of the elastic linewidth Γel at low temperatures indicates short - range spin - spin correlation distances ξs ~ 5 nm . In addition we study a large enhancement of the dynamic susceptibility χ ′ ′ ( Q , ω ) , which can be due to the development of high - energy spinning excitations below T * ~ 50 K . These conclusions are compatible with theoretical estimates for two - dimensional systems close to quantum criticality .Our data suggest that the system undergoes a shift into a state where the Fermi surface gets unstable against development of electron - hole couples leading to Cooper coupling . Introduction A couple of recent studies have shown that several highly correlated optical materials exhibit unusual characteristics such as high - temperature superconductivity or non - Fermi solid behavior 1 .One important feature of these phenomena is the presence of collective charge and / or spin degrees of liberty 2 , whose dynamics often give rise to distinctive features in the excitation spectrum 3 . For instance , in cuprate - based high - temperature superconductors 4 , it has been proposed that the pseudogap regime 5 may arise due to competing orders 6 resulting from different regions of the Brillouin zone 7 , 8 .Similarly , in metal pnictide molecules 9 , the appearance of a spin - density wave order parameter 10 results to a suppression of the density - of - states near the Fermi level 11 producing in a partial gap opening 12 . Finally , in heavy fermion metals 13 , the hybridization of localized f - ions 14 makes rise to a nontrivial momentum arrangement of the self - energy 15 .In this research , we present detailed observations of the spin fluctuation spectrum in the metallic phase of the quasi",
        "rewrite_text": "We present findings from neutron scattering experiments aimed at investigating spin fluctuations and magnetic correlations in the metallic phase of the quasi-two-dimensional organic superconductor κ-(BEDT-TTF)2Cu(NC)2Br (κ-Br). Our results indicate that the temperature dependence of the static susceptibility χ₀ follows the Curie-Weiss law, characterized by an antiferromagnetic Weiss constant θ of -26 K, which suggests significant antiferromagnetic interactions among spins. The observed increase in the elastic linewidth Γₑₗ at low temperatures implies short-range spin correlations with a characteristic length scale of approximately 5 nm. Furthermore, we observe a substantial enhancement of the dynamic susceptibility χ''(Q, ω), likely linked to the emergence of high-energy spin excitations below T* ~ 50 K. These findings align with theoretical predictions for two-dimensional systems nearing quantum criticality. Our data indicate that the system transitions into a state where the Fermi surface becomes unstable, leading to the formation of electron-hole pairs and resulting in Cooper pairing.\n\nIntroduction: Recent studies have revealed that various highly correlated optical materials display intriguing properties, such as high-temperature superconductivity and non-Fermi liquid behavior. A key aspect of these phenomena is the presence of collective charge and/or spin degrees of freedom, whose dynamics often produce distinctive features in the excitation spectrum. For example, in cuprate-based high-temperature superconductors, the pseudogap regime may arise from competing orders linked to different regions of the Brillouin zone. Similarly, in metal pnictide compounds, the emergence of a spin-density wave order parameter results in a suppression of the density of states near the Fermi level, leading to partial gap formation. In heavy fermion metals, the hybridization of localized f-electrons gives rise to a complex momentum structure in the self-energy. In this study, we provide detailed observations of the spin fluctuation spectrum within the metallic phase of the quasi-2D system.",
        "ori-fast-z-score": 1.0441851275732486,
        "water-fast-z-score": 7.574735861838574,
        "rewrite-fast-z-score": 1.516365080488482
    },
    {
        "original_text": "We study the energy transfer in a network of coupled nonlinear oscillators with time delay, which is motivated by the dynamics of biological systems such as neural networks and genetic regulatory networks. We show that there exists an optimal coupling strength for each individual oscillator to achieve maximum energy transfer efficiency among all other oscillators. The results are obtained through numerical simulations on both small-scale and large-scale networks. In particular, we find that the energy transfer efficiency decreases when the number of nodes increases beyond a certain threshold value. This phenomenon can be explained by the fact that the total amount of available energy per node decreases rapidly due to the increase of the number of nodes. Finally, we discuss possible applications of our findings to real-world problems. Energy transfer plays important roles in many natural phenomena including brain activity  1  , heartbeat  2  , and gene regulation  3  . It has been shown recently that efficient energy transfer may also exist in complex networks  4  -  6  .\nIn this work, we consider a system consisting of N identical nonlinear oscillators connected via delayed couplings (see Fig.   1 ). Each oscillator i = 1; 2; ...; N is described bẏ \nwhere x i 2 R n denotes its state vector, f : R n ! R n describes the local dynamics of each oscillator, c ij > 0 represents the coupling strength between two oscillators i and j, and h(t) is the transmission delay satisfying h max < h(t) < h min ; t 0:",
        "watermark_text": "We research the power transfer in a network of coupled nonlinear oscillators with time delay , which is motivated by the dynamics of biological systems such as neural systems and genome regulatory circuits . We see that there exists an appropriate coupling strength for each individual oscillator to achieve maximum energy transfer efficiency among all other oscillators .The results are derived through computational simulations on both small - scale and large - scale networks . In particular , we find that the power transfer efficiency decreases when the number of nodes increases beyond a certain threshold value .This phenomenon can be described by the fact that the total amount of available energy per node decreases quickly due to the increase of the number of nodes . Finally , we explain possible applied of our findings to real - time situations .Energy transfer plays important roles in multiple natural phenomena including brain activity 1 , heartbeat 2 , and gene control 3 . It has been shown lately that efficient energy flow would also exist in complex networks 4 - 6 .In this study , we study a system consisting of N identical nonlinear oscillators related via delayed couplings ( saw Fig . 1 ) .Each oscillator i = 1 ; 2 ; . . . ; N is described [UNK] where x i 2 R n denotes its state vector , f : R n ! R n describes the local dynamics of each oscillator , c ij > 0 represents the coupling strength between two oscillators i and j , and h ( t ) is the transmission delay satisfying h max < h ( t ) < h min ; t 0 :",
        "rewrite_text": "We investigate the energy transfer within a network of coupled nonlinear oscillators that incorporates time delays, inspired by the dynamics of biological systems such as neural networks and genomic regulatory circuits. Our study reveals that there is an optimal coupling strength for each oscillator that maximizes energy transfer efficiency in relation to the other oscillators in the network. These findings are based on computational simulations conducted on networks of varying sizes. Notably, we observe that the efficiency of power transfer diminishes once the number of nodes exceeds a certain threshold. This reduction can be attributed to the rapid decline of the total available energy per node as the node count increases. Furthermore, we discuss the potential applications of our results in real-time scenarios. Energy transfer is crucial in numerous natural processes, such as neural activity, heart rhythms, and gene regulation. Recent research has also indicated that efficient energy flow is present in complex networks. In our study, we examine a system comprised of N identical nonlinear oscillators interconnected through delayed couplings, as illustrated in Figure 1. Each oscillator, indexed by i = 1, 2, ..., N, is characterized by a state vector x_i ∈ R^n, with f: R^n → R^n defining the local dynamics, c_ij > 0 denoting the coupling strength between oscillators i and j, and h(t) representing the transmission delay that satisfies h_max < h(t) < h_min for t ≥ 0.",
        "ori-fast-z-score": -0.5345224838248488,
        "water-fast-z-score": 4.09800570932384,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The availability of computer systems is an important aspect in the design, development and operation of any system. The aim of this research was to develop a method for assessing the availability of a large number of SunOS/Solaries systems using syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender). This research has been carried out by collecting data from a set of servers over a period of one year. A total of 1,000,000 records were collected during that time frame. These records have then been processed into a database which contains information about each record such as timestamps, host name, service type etc.. In order to assess the availability of these systems we developed two algorithms; one algorithm calculates the percentage uptime per day while another algorithm calculates the percentage downtime per hour. We also implemented a web interface so that users can view the results obtained from our analysis.",
        "watermark_text": "The availability of computer machines is an important element in the development , construction and operation of any program . The goal of this research was to develop a technique for evaluating the availability of a large number of SunOS / Solaries machines combining syslogd ( System Logging Daemon ) and wtmpx ( Windows Time Machine eXtender ) .This research has been carried out by collecting data from a setting of servers over a period of one year . A total of 1 , 000 , 000 data were collected during that time frame .These data have then been processed into a computer which contains information about each record such as timestamps , host title , service size etc . . In order to analyze the availability of these systems we developed two strategies ; one algorithm calculates the percentage uptime per month while another algorithm calculates the percentage downtime per hour .We additionally introduced a web application so that users can view the results derived from our analysis .",
        "rewrite_text": "The availability of computer systems is a crucial factor in the development, construction, and operation of any software program. This research aimed to create a method for assessing the availability of a large number of SunOS/Solaris machines by integrating syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender). The study involved gathering data from a set of servers over the course of one year, resulting in the collection of one million data points. These records were processed by a computer that compiled details such as timestamps, host names, service sizes, and more. To evaluate the systems' availability, we devised two strategies: one algorithm determines the monthly uptime percentage, while the other calculates the hourly downtime percentage. Additionally, we developed a web application that allows users to access the results of our analysis.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -0.6401843996644799
    },
    {
        "original_text": "We study the entanglement between two atoms trapped inside an optical cavity, each interacting with one mode of the electromagnetic field. We consider both resonant and nonresonant interactions for different initial states of the system. In particular we show that it is possible to generate maximally entangled Bell-like states by using only local operations on the atomic subsystem. The results are obtained by solving numerically the Schrödinger equation describing the evolution of the system. Entanglement plays a crucial role in quantum information processing  1  . It has been shown  2  , however, that even if all parties involved share maximal entanglement at some point during their communication protocol they may not be able to perform any useful task due to decoherence effects  3  .\nIn this work we investigate how entanglement can be generated among two atoms trapped inside an open optical cavity  4  . Each atom interacts locally with its own photon mode which is coupled to another external photon mode via a mirror (see Fig.  1 ). This model describes many physical systems such as semiconductor microcavities  5  or cold atoms  6  . For example, in Ref.  7  it was proposed to use a Bose-Einstein condensate confined within a high-finesse Fabry-Perot cavity  8  to produce single photons  9  . These photons could then be used to entangle distant qubits  10  .",
        "watermark_text": "We explore the entanglement between two atoms trapped inside an optical cavity , each interacting with one mode of the electromagnetic field . We consider both resonant and nonresonant interactions for different initial states of the system .In particular we prove that it is easy to create maximally entangled Bell - like states by using only local operations on the atomic subsystem . The results are derived by solving numerically the Schrödinger equation explaining the evolution of the system .Entanglement plays a crucial role in quantum information processing 1 . It has been shown 2 , however , that even if all parties involved share maximal entanglement at some time during their transmission protocol they may not be possible to complete any useful job due to decoherence effects 3 .In this research we investigate how entanglement can be formed among two atoms trapped inside an open optical cavity 4 . Each electron interacts locally with its own photon mode which is linked to another external photon mode via a mirror ( see Fig .1 ) . This theory presents many physical structures such as semiconductor microcavities 5 or cold atoms 6 .For instance , in Ref . 7 it was suggested to use a Bose - Einstein condensate concentrated within a high - finesse Fabry - Perot cavity 8 to produce single photons 9 .These photons may then be used to entangle distant qubits 10 .",
        "rewrite_text": "We investigate the entanglement between two atoms confined within an optical cavity, where each atom interacts with a different mode of the electromagnetic field. Our study addresses both resonant and nonresonant interactions, considering various initial states of the system. Notably, we demonstrate that it is straightforward to generate maximally entangled Bell-like states using only local operations on the atomic subsystem. These results are obtained through numerical solutions of the Schrödinger equation that governs the system's evolution. Entanglement is a fundamental aspect of quantum information processing. However, it has been shown that despite all parties achieving maximal entanglement at certain points during their transmission protocols, decoherence effects can hinder the completion of practical tasks. In this research, we focus on how entanglement can be established between two atoms located in an open optical cavity. Each atom interacts locally with its own photon mode, which connects to another external photon mode via a mirror (see Fig. 1). This theoretical framework encompasses various physical structures, such as semiconductor microcavities or cold atoms. For example, in a previous study, the use of a Bose-Einstein condensate situated within a high-finesse Fabry-Perot cavity was proposed as a means to generate single photons, which could subsequently be employed to entangle distant qubits.",
        "ori-fast-z-score": 0.21566554640687682,
        "water-fast-z-score": 4.528976474544414,
        "rewrite-fast-z-score": -1.1659976680069961
    },
    {
        "original_text": "We study the SLE (Schramm-Loewner Evolution) process for the scaling limit of interfaces between different phases in the two-dimensional Ising model with nearest-neighbor interactions on an arbitrary planar graph, and its generalization to higher dimensions. We show that the interface is described by a chordal Schramm-Löwner evolution if the underlying lattice has no loops or multiple edges; otherwise it is described by a radial Schramm-Löwner evolutions. The results are obtained using conformal field theory techniques. In particular we use the fact that the partition function of these models can be written as a correlation function of primary fields in some rational conformal field theories. This allows us to obtain explicit formulas for the probability distribution functions of various geometric quantities associated with the interfaces such as their winding numbers around vertices etc.. \nIntroduction\n\nThe Schramm-Loewner Evolutions (SLE)\nprocesses were introduced by Schramm  Sch00  , who showed that they provide a natural description of the scaling limits of interfaces in statistical mechanics systems at criticality. These processes have been studied extensively since then both theoretically and numerically. For example, see  KSS02, SS04a, SS04b, RS05, Sch06, CS07, KS08, KSV09, KM10, MS11, MZ12, BMS13, BS14, LW15, GKS16, GM17, GK18, HJ19, HK20, JPS20  . A comprehensive review of this subject may be found in  Smi01, Sta03, Joh10  .\nIn this work we consider the SLE process for the scaling limit in two dimensions of interfaces separating different phases in the following class of models:  Let G = (V, E) be any finite connected planar graph without loops or multiple edges. Consider the Ising model with nearest neighbor interaction defined on G. That is, let {σv}v∈V denote a collection of random variables taking values +1 and −1, where each σv represents the state of vertex v ∈ V . Then",
        "watermark_text": "We work the SLE ( Schramm - Loewner Evolution ) process for the scaling maximum of interfaces between various phases in the two - dimensional Ising model with nearest - neighbor interactions on an arbitrary planar graph , and its generalization to higher dimensions . We see that the interface is characterized by a chordal Schramm - Löwner evolution if the underlying lattice has no loops or multiple edges ; otherwise it is characterized by a radial Schramm - Löwner evolutions .The results are derived using conformal field model approaches . In particular we utilize the fact that the splitting function of these models can be written as a correlation function of primary fields in some rational conformal field theories .This enables us to obtain precise formulas for the probability distribution functions of several geometric variables identified with the interfaces such as their winding numbers around vertices etc . . Introduction The Schramm - Loewner Evolutions ( SLE ) processes were introduced by Schramm Sch00 , who demonstrated that they give a natural characterization of the scaling limits of interfaces in statistical mechanics systems at criticality . These mechanisms have been studied frequently since then both theoretically and numerically .For instance , see KSS02 , SS04a , SS04b , RS05 , Sch06 , CS07 , KS08 , KSV09 , KM10 , MS11 , MZ12 , BMS13 , BS14 , LW15 , GKS16 , GM17 , GK18 , HJ19 , HK20 , JPS20 . A full study of this subject may be found in Smi01 , Sta03 , Joh10 .In this research we define the SLE method for the scaling limit in two dimensions of interfaces separating different stages in the following category of models : Let G = ( V , E ) be any finite connected planar graph without loops or multiple edges . Consider the Ising model with nearest neighbor behavior defined on G . That is , let { σv } v∈V denote a collection of random vectors take values + 1 and −1 , where each σv represents the state of vertex v ∈ V .Then",
        "rewrite_text": "We analyze the Schramm-Löwner Evolution (SLE) process in the context of the scaling limits of interfaces between different phases in the two-dimensional Ising model, which features nearest-neighbor interactions on an arbitrary planar graph, as well as its extensions to higher dimensions. Our findings indicate that the nature of the interface is defined by a chordal Schramm-Löwner evolution when the underlying lattice is free of loops and multiple edges; conversely, in the presence of such structures, it is described by radial Schramm-Löwner evolutions. These results are established through the application of conformal field theory techniques. Notably, we leverage the relationship between the splitting function of these models and the correlation functions of primary fields in specific rational conformal field theories. This framework allows us to derive precise formulas for the probability distribution functions of various geometric quantities associated with the interfaces, including winding numbers around vertices. \n\nThe Schramm-Löwner Evolutions (SLE), created by Schramm in 2000, provide a natural description of the scaling limits of interfaces in critical statistical mechanics systems. Since their introduction, these processes have been extensively investigated both theoretically and through numerical simulations, as referenced in works such as KSS02, SS04a, SS04b, RS05, Sch06, CS07, KS08, KSV09, KM10, MS11, MZ12, BMS13, BS14, LW15, GKS16, GM17, GK18, HJ19, HK20, and JPS20. Comprehensive studies on the subject can be found in Smi01, Sta03, and Joh10. In this paper, we define the SLE method for the scaling limits of interfaces that separate different phases in the following class of models: Let G = (V, E) represent any finite, connected planar graph devoid of loops or multiple edges. We consider the Ising model with nearest-neighbor interactions defined on G, denoted by a collection of random variables {σv} for v ∈ V, where each σv takes values of +1 or -1, representing the state of vertex v ∈ V.",
        "ori-fast-z-score": 1.3750477455423171,
        "water-fast-z-score": 6.875238727711585,
        "rewrite-fast-z-score": 1.3310347641241707
    },
    {
        "original_text": "We present the results of an X-ray study of supernova remnant (SNR) G299.2-2.9 using data obtained with Chandra and XMM-Newton observatories. The SNR is located in the constellation Puppis at a distance of ~5 kpc, which corresponds to its angular size of about 30 arcmin. We find that the spectrum of this object can be described by two thermal components with temperatures T1=7×10^6 K and T2=2×10^6 K. In addition, we detect non-thermal emission above 10 keV. Using these parameters, we estimate the age of the SNR as t=4000 yr. This value agrees well with the characteristic time for the expansion of the shell into the surrounding medium. Based on our analysis, we conclude that the observed morphology of the SNR is consistent with the model of a spherical explosion expanding into a uniform interstellar medium.",
        "watermark_text": "We present the conclusion of an X - ray study of supernova remnant ( SNR ) G299 . 2 - 2 . 9 utilizing information obtained with Chandra and XMM - Newton observatories . The SNR is situated in the constellation Puppis at a distance of ~ 5 kpc , which corresponds to its angular height of about 30 arcmin .We see that the spectrum of this body can be described by two thermal parts with temperatures T1 = 7×10 ^ 6 K and T2 = 2×10 ^ 6 K . In addition , we identify non - temperature emission above 10 keV . Using these parameters , we estimate the age of the SNR as t = 4000 yr .This value agrees well with the typical moment for the advance of the shell into the nearby medium . Based on our analysis , we determine that the seen morphology of the SNR is compatible with the model of a spherical explosion expanding into a regular interstellar medium .",
        "rewrite_text": "We present the findings of an X-ray study of the supernova remnant (SNR) G299.2-2.9, based on data collected from the Chandra and XMM-Newton observatories. This SNR is located in the constellation Puppis, approximately 5 kpc away, with an angular size of about 30 arcminutes. The spectrum of the remnant can be characterized by two thermal components with temperatures of T1 = 7 × 10^6 K and T2 = 2 × 10^6 K. Additionally, we detect non-thermal emission at energies exceeding 10 keV. Using these parameters, we estimate the age of the SNR to be around 4000 years, which aligns well with the typical advance of the shell into the surrounding medium. Our analysis indicates that the observed morphology of the SNR is consistent with a model of a spherical explosion expanding into a uniform interstellar medium.",
        "ori-fast-z-score": -0.14002800840280097,
        "water-fast-z-score": 5.091168824543142,
        "rewrite-fast-z-score": -1.5403080924308108
    },
    {
        "original_text": "The design and kinematics analysis of parallel kinematic machine tools (PKMTs) is presented in this work using kinetic-static performance criteria. The proposed approach considers the dynamic behavior of PKMTs during their operation, which has been neglected by previous works on PKMTs. In addition to the static stiffness matrix, the inertia properties are also considered for the evaluation of the overall dynamic response of PKMTs. A new method based on the concept of virtual joints is developed to calculate the mass distribution along each leg of the PKMT under consideration. This information can be used as an input data set for further dynamic analyses such as modal or harmonic vibration analysis. Finally, two different PKMTs with three degrees-of-freedom per joint are designed and analyzed using the proposed methodology. It was found that the first PKMT exhibits better dynamic characteristics than its counterpart due to its lower natural frequencies and higher damping ratios.",
        "watermark_text": "The design and kinematics analysis of parallel kinematic machine tools ( PKMTs ) is detailed in this work using kinetic - static performance criteria . The proposed approach considers the dynamic behavior of PKMTs during their operation , which has been neglected by previous works on PKMTs .In addition to the static stiffness matrix , the inertia characteristics are also considered for the evaluation of the overall dynamic response of PKMTs . A modern algorithm based on the idea of virtual joints is developed to estimate the mass distribution along each leg of the PKMT under consideration .This knowledge can be used as an input data set for further dynamic analyses such as modal or harmonic vibration analysis . Finally , two different PKMTs with three degrees - of - independence per joint are built and evaluated using the suggested methodology .It was shown that the first PKMT displays better dynamic characteristics than its predecessor due to its lower natural bandwidth and larger damping ratios .",
        "rewrite_text": "This study provides a comprehensive analysis of the design and kinematics of parallel kinematic machine tools (PKMTs) through the use of kinetic-static performance criteria. Unlike previous research, this approach accounts for the dynamic behavior of PKMTs during operation. Alongside the static stiffness matrix, the evaluation of the overall dynamic response also incorporates the inertia characteristics of the machines. A novel algorithm utilizing the concept of virtual joints is introduced to estimate the mass distribution along each leg of the PKMT being examined. This information serves as foundational data for additional dynamic analyses, including modal and harmonic vibration assessments. Ultimately, two distinct PKMTs, each featuring three degrees of independence per joint, were constructed and analyzed using the proposed methodology. Results indicated that the first PKMT exhibits superior dynamic characteristics compared to its predecessor, attributed to its reduced natural bandwidth and increased damping ratios.",
        "ori-fast-z-score": 1.6059101370939322,
        "water-fast-z-score": 5.27656187902292,
        "rewrite-fast-z-score": 0.5698028822981898
    },
    {
        "original_text": "The geochemical behavior of uranium (U) and thorium (Th), which are radioactive elements, is discussed in relation to their influence on the origin and evolution of the crust of earth as well as biological evolution.  The chemical properties of these two elements are similar; however, they have different physical characteristics that affect how they behave geologically.  Uranium has an atomic number of 92 with a half-life of 4.5 billion years while thorium has an atomic number of 90 with a half life of 1.4 billion years.   Both elements occur naturally throughout the Earth s crust but at varying concentrations depending upon the rock type.  They can be found in igneous rocks such as granite or basalt where they form minerals like uranite or thorite respectively.  These minerals may also contain other trace metals including lead, silver, gold, copper, zinc, arsenic, selenium, molybdenum, cadmium, mercury, bismuth, antimony, tellurium, cobalt, nickel, manganese, iron, vanadium, chromium, tungsten, titanium, zirconium, niobium, tantalum, hafnium, rhenium, osmium, iridium, platinum, palladium, rhodium, ruthenium, and iridium.  Uranium and thorium are also present in sedimentary rocks such as sandstone, shale, limestone, dolomite, gypsum, anhydrite, salt beds, and evaporites.  Sedimentary rocks are formed by weathering processes when water erodes...",
        "watermark_text": "The geochemical behavior of uranium ( U ) and thorium ( Th ) , which are radioactive elements , is mentioned in relation to their influence on the origin and evolution of the crust of earth as well as biological evolution . The chemical properties of these two compounds are comparable ; however , they have different biological traits that affect how they react geologically .Uranium has an atomic number of 92 with a half - life of 4 . 5 billion decades while thorium has an atomic number of 90 with a half lifetime of 1 . 4 billion decades . Both compounds occur readily throughout the Earth s crust but at different amounts depending upon the stone type .They can be found in igneous rocks such as granite or basalt where they create minerals like uranite or thorite respectively . These rocks could also contain other trace metals including lead , silver , gold , copper , zinc , arsenic , selenium , molybdenum , cadmium , mercury , bismuth , antimony , tellurium , cobalt , nickel , manganese , iron , vanadium , chromium , tungsten , titanium , zirconium , niobium , tantalum , hafnium , rhenium , osmium , iridium , platinum , palladium , rhodium , ruthenium , and iridium .Uranium and thorium are also occur in sedimentary formations such as sandstone , shale , marble , dolomite , gypsum , anhydrite , salt beds , and evaporites . Sedimentary stones are created by weathering reactions when water erodes . . .",
        "rewrite_text": "The geochemical behavior of uranium (U) and thorium (Th), both radioactive elements, is significant in understanding the origin and evolution of Earth's crust as well as biological evolution. While their chemical properties are similar, they exhibit distinct biological characteristics that influence their geological reactions. Uranium, with an atomic number of 92 and a half-life of 4.5 billion years, contrasts with thorium, which has an atomic number of 90 and a half-life of 1.4 billion years. Both elements are found abundantly in the Earth's crust, with concentrations varying by rock type. They can be discovered in igneous rocks such as granite and basalt, forming minerals like uraninite and thorite, respectively. Additionally, these rocks may contain various trace metals, including lead, silver, gold, copper, zinc, arsenic, selenium, molybdenum, cadmium, mercury, bismuth, antimony, tellurium, cobalt, nickel, manganese, iron, vanadium, chromium, tungsten, titanium, zirconium, niobium, tantalum, hafnium, rhenium, osmium, iridium, platinum, palladium, rhodium, rutheium, and iridium. Uranium and thorium can also be found in sedimentary formations such as sandstone, shale, marble, dolomite, gypsum, anhydrite, salt beds, and evaporites, which are formed through weathering processes driven by water erosion.",
        "ori-fast-z-score": -1.4814874939752933,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": 0.22941573387056174
    },
    {
        "original_text": "The response prediction of structural system subject to earthquake motions is very important for the design and construction of buildings in seismic areas, especially when it comes to high-rise building structures. In this study, an artificial neural network (ANN) model was developed by using data obtained through nonlinear dynamic analysis on reinforced concrete frame structure subjected to earthquake ground motion records. The ANN model consists of three layers; input layer, hidden layer with 10 neurons, output layer with one neuron representing maximum inter-story drift ratio. Input variables used are peak ground acceleration, duration time, number of stories, story height, mass density per unit floor area, damping coefficient, yield strength of steel bar, elastic modulus of steel bar, shear wall stiffness, and moment capacity of beam-column joint. To verify the accuracy of the proposed ANN model, results predicted by the ANN were compared with those calculated by nonlinear dynamic analysis program. It can be concluded that the ANN model has good performance in predicting the maximum inter-story drift ratios under various earthquake ground motions.",
        "watermark_text": "The behavior analysis of structural structure exposed to earthquake motions is very important for the design and build of structures in seismic areas , particularly when it comes to large - rising building structures . In this study , an synthetic neural network ( ANN ) model was developed by using data derived through nonlinear dynamic analysis on concrete cement frame building exposed to earthquake ground motion records .The ANN system contains of three layers ; input layer , hidden surface with 10 neurons , output layer with one neuron representing maximum inter - story drag ratio . Input variables utilized are peak ground acceleration , duration time , number of floors , story height , mass density per unit floor area , damping coefficient , yield strength of steel bar , elastic modulus of steel bar , shear floor stiffness , and moment capacity of beam - column joint .To verify the accuracy of the suggested ANN theory , results predicted by the ANN were compared with those estimated by nonlinear dynamic analysis project . It can be assumed that the ANN theory has good success in predicting the maximum inter - story drag ratios under various earthquake ground motions .",
        "rewrite_text": "The analysis of structural behavior during seismic activities is crucial for the design and construction of buildings in earthquake-prone regions, especially for tall structures. This study developed a synthetic neural network (ANN) model utilizing data obtained from nonlinear dynamic analyses of a concrete frame building subjected to earthquake ground motion. The ANN consists of three layers: an input layer, a hidden layer with 10 neurons, and an output layer with a single neuron representing the maximum inter-story drift ratio. The input variables include peak ground acceleration, duration of motion, number of floors, story height, mass density per floor area, damping coefficient, yield strength of steel reinforcement, elastic modulus of steel, shear stiffness of floors, and moment capacity of beam-column joints. To validate the performance of the proposed ANN model, its predictions were compared with those obtained from nonlinear dynamic analysis. The results indicate that the ANN model is effective in forecasting maximum inter-story drift ratios for various earthquake ground motions.",
        "ori-fast-z-score": 1.0536089137432665,
        "water-fast-z-score": 6.800566625070174,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results on the interband, intraband and excitonic transitions for Si and Ge nanocrystals (NCs) embedded into Al2O3 matrix by using first-principles calculations based on density functional theory within local-density approximation. The NC size is varied between 1 nm to 5 nm with an interval of 0.5 nm. We find that the optical gap decreases as we increase the NC size due to quantum confinement effect. In addition, we observe that the lowest energy peak shifts towards higher energies when increasing the NC size which can be attributed to the surface states. Furthermore, our calculated results show that the oscillator strength increases significantly at smaller sizes while it decreases rapidly at larger sizes. Finally, we also investigate the influence of strain on the electronic structure of these systems. Our findings are expected to provide useful information about the design of optoelectronic devices such as solar cells or photodetectors. \n \n Keywords: Silicon Nanocrystal, Germanium Nanocrystal, Optical properties",
        "watermark_text": "We present the conclusion on the interband , intraband and excitonic transitions for Si and Ge nanocrystals ( NCs ) integrated into Al2O3 matrix by using first - principles measurements based on density functional theory within local - density algorithms . The NC size is varied between 1 nm to 5 nm with an interval of 0 . 5 nm .We see that the optical gap decreases as we increase the NC size owing to quantum confinement phenomenon . In addition , we determine that the lowest energy peak changes towards higher energies when increasing the NC size which can be due to the surface states .Furthermore , our calculated results show that the oscillator strength changes significantly at lower dimensions while it varies dramatically at larger dimensions . Finally , we also investigate the impact of strain on the electronic stability of these systems .Our findings are expected to provide useful details about the development of optoelectronic elements such as sun cells or photodetectors . Keywords : Silicon Nanocrystal , Germanium Nanocrystal , Optical properties",
        "rewrite_text": "We present our findings on the interband, intraband, and excitonic transitions in silicon (Si) and germanium (Ge) nanocrystals (NCs) embedded in an Al2O3 matrix, utilizing first-principles measurements grounded in density functional theory with local-density algorithms. The size of the NCs is varied from 1 nm to 5 nm in increments of 0.5 nm. Our analysis reveals that the optical gap decreases as the NC size increases, a phenomenon attributed to quantum confinement. Additionally, we observe that the lowest energy peak shifts to higher energies with larger NC sizes, likely due to the influence of surface states. Furthermore, our calculations indicate a substantial change in oscillator strength at smaller dimensions, with considerable variation at larger sizes. Finally, we examine how strain affects the electronic stability of these systems. Our results are anticipated to offer valuable insights for the advancement of optoelectronic devices such as solar cells and photodetectors. Keywords: Silicon Nanocrystals, Germanium Nanocrystals, Optical Properties.",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": 0.3464101615137754
    },
    {
        "original_text": "We present the results of our study on super star clusters (SSCs) in which we have found that SSCs can be divided into two categories, namely, those having a single mode and those having a double-mode solution for their density profiles. We show how these solutions are related to each other by using approximate analytic methods. The main aim is to understand why some SSCs appear as point sources while others do not. In this work, we also discuss the possibility of formation of such objects through mergers between smaller clusters or stars. Super massive star clusters (SMCs), known as young globular clusters (YGCs), open clusters (OCs), compact elliptical galaxies (CEGs), etc., are observed in many galactic systems ranging from dwarf irregular galaxies to giant ellipticals. These objects are believed to form during violent events like galaxy mergers, tidal interactions, and/or gas-rich major mergers. However, it has been shown recently that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses (10^6-10^7 Msun). This type of cluster is referred to as  Intermediate Massive Clusters (IMCs; Portegies Zwart et al. (2010)). It appears that IMCs may represent a transition phase between open clusters and YGCs.",
        "watermark_text": "We present the conclusion of our research on super star clusters ( SSCs ) in which we have discovered that SSCs can be grouped into two genres , namely , those having a single mode and those having a twin - mode solution for their density characteristics . We see how these solutions are related to each other by using numerical analytic techniques .The main aim is to explain why some SSCs appear as point sources while many do not . In this research , we also discuss the prospect of formation of such objects through mergers between smaller clusters or stars .Super massive star clusters ( SMCs ) , known as early globular complexes ( YGCs ) , close complexes ( OCs ) , compact elliptical galaxies ( CEGs ) , etc . , are observed in many galactic structures ranging from giant irregular clusters to massive ellipticals . These structures are said to form during violent reactions like galaxy mergers , tidal interactions , and / or gas - rich major mergers .However , it has been shown lately that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses ( 10 ^ 6 - 10 ^ 7 Msun ) . This kind of cluster is referred to as Intermediate Massive Clusters ( IMCs ; Portegies Zwart et al .( 2010 ) ) . It suggested that IMCs might represent a transfer stage between open complexes and YGCs .",
        "rewrite_text": "We present the conclusions from our investigation into super star clusters (SSCs), revealing that they can be categorized into two distinct types: those exhibiting a single mode and those displaying a twin-mode solution in their density characteristics. We explore the relationship between these solutions using numerical analytical methods. Our primary objective is to elucidate why some SSCs manifest as point sources while others do not. Additionally, this research delves into the potential formation of these structures through the merging of smaller clusters or individual stars. Supermassive star clusters (SMCs), also known as early globular complexes (YGCs), open clusters (OCs), and compact elliptical galaxies (CEGs), are found across a variety of galactic formations, from gigantic irregular clusters to massive elliptical galaxies. These formations are believed to arise during intense events such as galaxy mergers, tidal interactions, and significant gas-rich mergers. Recently, however, it has been identified that there is another category of SMCs whose luminosity function reaches a peak at intermediate masses (10^6 - 10^7 M_sun). This type of cluster is termed Intermediate Massive Clusters (IMCs, as noted by Portegies Zwart et al. in 2010), suggesting that IMCs may serve as a transitional phase between open complexes and YGCs.",
        "ori-fast-z-score": -1.58999682000954,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": 0.3110855084191276
    },
    {
        "original_text": "We present an analysis of the temperature dependence of thermally stimulated luminescent (TSL) glow curves in terms of the nonstationary electron-phonon relaxation theory, which does not assume that the system is close to equilibrium at any time during its evolution.  We show how this approach can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL data obtained on different types of materials. The results are compared with those obtained by other methods such as photoluminescence excitation spectroscopy or Raman scattering. In particular we demonstrate that our method allows one to determine the energy gap between the conduction band minimum and valence band maximum in semiconductors. This work was supported by Russian Science Foundation grant No. 14-50-00040. DOI: 10.1063/1.4935190 \nI. INTRODUCTORY REMARK\nThe study of luminescence phenomena has been attracting considerable attention over many years because it provides valuable information about electronic structure and optical properties of solids  1  . Thermal stimulation luminescence (TSL), also known as optically stimulated luminescence (OSL), is particularly useful since it enables us to probe the distribution function of electrons excited into the conduction band  2  .\nIn recent decades there have been numerous attempts to develop theoretical models describing various aspects of luminescence processes  3  , including thermal stimulation luminescence  4  -  8  . However, most of these works were based on the assumption that the system under consideration is always close to equilibrium  9  . As a result they cannot describe correctly some important features observed experimentally  10  . For example, the shape of the TSL glow curve depends strongly on the type of material  11  : while in insulators it usually exhibits a single peak  12  , in metals it often consists of several peaks  13  . Moreover, even within the same class of materials, e.g., semiconductor crystals  14  , the number of peaks may vary depending on the doping level  15  . These observations cannot be explained using existing theories  16  .",
        "watermark_text": "We present an assessment of the temperature dependence of thermally stimulated luminescent ( TSL ) glow curves in terms of the nonstationary electron - phonon relaxation hypothesis , which does not assume that the system is close to equilibrium at any time during its evolve . We see how this methodology can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL information obtained on various types of substances .The results are compared with those achieved by other methods such as photoluminescence excitation spectroscopy or Raman absorption . In particular we prove that our technique permits one to estimate the electricity gap between the conduction band minimum and valence band maximum in semiconductors .This project was supported by Russian Science Foundation gift No . 14 - 50 - 00040 .DOI : 10 . 1063 / 1 . 4935190 I . INTRODUCTORY REMARK The investigation of luminescence effects has been drawing tremendous attention over numerous years because it gives valuable info about electronic properties and electronic properties of solids 1 .Thermal stimulation luminescence ( TSL ) , sometimes called as optically stimulated luminescence ( OSL ) , is especially useful since it allows us to probe the distribution behavior of electrons excited into the conduction band 2 . In recent generations there have been numerous attempts to develop conceptual models explaining various parts of luminescence events 3 , notably heat stimulation luminescence 4 - 8 .However , most of these works were based on the assumption that the process under consideration is usually nearly to equilibrium 9 . As a result they cannot describe correctly some important features discovered experimentally 10 .For instance , the form of the TSL flicker curve varies strongly on the kind of material 11 : while in insulators it generally exhibits a single peak 12 , in metals it often consists of several peaks 13 . Moreover , even within the same category of substances , e . g . , semiconductor crystals 14 , the number of peaks may differ depending on the doping level 15 .These measurements cannot be understood using existing models 16 .",
        "rewrite_text": "We present an evaluation of how temperature influences thermally stimulated luminescent (TSL) glow curves, based on the nonstationary electron-phonon relaxation hypothesis, which does not assume that the system remains close to equilibrium throughout its evolution. This approach allows us to extract information regarding the phonon spectrum and the density of states of charge carriers from TSL data across various materials. Our results are compared with findings from other techniques, such as photoluminescence excitation spectroscopy and Raman absorption. Notably, we demonstrate that our method provides an estimate of the energy gap between the conduction band minimum and the valence band maximum in semiconductors. This project was supported by the Russian Science Foundation grant No. 14-50-00040. DOI: 10.1063/1.4935190. \n\nI. INTRODUCTORY REMARK\n\nThe study of luminescence effects has garnered significant attention over the years due to its potential to yield insights into the electronic properties of solids. Thermal stimulation luminescence (TSL), also referred to as optically stimulated luminescence (OSL), is particularly valuable because it enables us to investigate the behavior of electrons excited into the conduction band. Recent efforts have focused on developing conceptual models to explain various aspects of luminescence phenomena, particularly heat stimulation luminescence. However, many of these models are predicated on the assumption that the process is often close to equilibrium, which can lead to an inaccurate portrayal of critical experimentally observed features. For example, the shape of the TSL glow curve can vary significantly depending on the material: insulators typically display a single peak, while metals may exhibit multiple peaks. Furthermore, even among similar materials, like semiconductor crystals, the number of peaks can vary based on the doping level. Existing models fall short of adequately explaining these observations.",
        "ori-fast-z-score": -2.3664319132398464,
        "water-fast-z-score": 7.437357441610946,
        "rewrite-fast-z-score": -0.17677669529663687
    },
    {
        "original_text": "We introduce the notion of intersection bodies in arbitrary dimensions, generalizing the classical concept for dimension two to higher dimensions. We show that intersection bodies are characterized by their Fourier transforms which we call generalized cosine transforms (GCT). The GCTs can be used as an alternative tool to study intersection bodies. In particular, we prove that intersection bodies have positive volume if and only if they are convex. This is done using a new characterization of intersection bodies via their support functions. Finally, we give some examples of intersection bodies in three dimensions. Keywords: Intersection body; Support function; Convexity; Volume; Fourier transform; Three-dimensional space. 1 Introduction Let K n denote the set of all origin-symmetric convex bodies in R n . For any K ∈ K n , let V(K) = |K|/|B n 2 | where | · | denotes Lebesgue measure on R n . Then V : K n →  0, 1  is called the volume functional. A compactly supported continuous function f : S n−1 → C with unit integral will be called a spherical harmonic of degree m. If f has no zeros then it is uniquely determined up to multiplication by a constant. It follows immediately that every spherical harmonic of degree m satisfies the following properties:\n(1) |f (x)| ≤ 1; (2) f (−x) = f (x), x ∈ S n−1 ; (3) (Spherical harmonics form an orthonormal basis.) Definition 1. An origin-symmetric convex body K ∈ K n is said to be an intersection body if there exists a non-negative real number λ such that its surface area measure σ K satisfies",
        "watermark_text": "We introduce the notion of junction bodies in arbitrary dimensions , generalizing the classical concept for dimension two to higher dimensions . We see that intersection bodies are characterized by their Fourier functions which we call generalized cosine transforms ( GCT ) .The GCTs can be used as an additional tool to study junction bodies . In particular , we prove that intersection bodies have positive volume if and only if they are convex .This is accomplished using a new definition of junction bodies via their support functions . Finally , we give some examples of intersection bodies in three dimensions .Keywords : Intersection body ; Support function ; Convexity ; Volume ; Fourier integral ; Three - dimensional space . 1 Introduction Let K n denote the group of all origin - symmetric convex body in R n .For any K ∈ K n , let V ( K ) = | K | / | B n 2 | where | · | indicates Lebesgue measure on R n . Then V : K n → 0 , 1 is called the volume functional .A compactly backed continuous function f : S n−1 → C with unit integral will be called a spherical harmonic of degree m . If g has no zeros then it is uniquely determined up to multiplication by a constant . It follows instantly that every spherical harmonic of degree m satisfies the following properties : ( 1 ) | h ( x ) | ≤ 1 ; ( 2 ) f ( −x ) = g ( x ) , x ∈ S n−1 ; ( 3 ) ( Spherical harmonics form an orthonormal basis . )Definition 1 . An origin - symmetric convex body K ∈ K n is said to be an intersection body if there exists a non - negative real number λ such that its surface area measure σ K satisfies",
        "rewrite_text": "We present the concept of junction bodies in arbitrary dimensions, extending the classical two-dimensional notion to higher dimensions. Intersection bodies are defined by their Fourier functions, which we term generalized cosine transforms (GCT). These GCTs serve as an additional tool for examining junction bodies. Notably, we demonstrate that intersection bodies possess a positive volume if and only if they are convex. This result is derived from a new definition of junction bodies based on their support functions. Additionally, we provide examples of intersection bodies in three dimensions. \n\nKeywords: Intersection body; Support function; Convexity; Volume; Fourier integral; Three-dimensional space. \n\n1. Introduction \n\nLet \\( K_n \\) denote the group of all origin-symmetric convex bodies in \\( \\mathbb{R}^n \\). For any \\( K \\in K_n \\), let \\( V(K) = |K| / |B_n^2| \\), where \\( |\\cdot| \\) denotes the Lebesgue measure on \\( \\mathbb{R}^n \\). The function \\( V: K_n \\to [0, 1] \\) is referred to as the volume functional. A compactly supported continuous function \\( f: S^{n-1} \\to \\mathbb{C} \\) with a unit integral is defined as a spherical harmonic of degree \\( m \\). If \\( g \\) has no zeros, it is uniquely determined up to a constant factor. Therefore, every spherical harmonic of degree \\( m \\) exhibits the following properties: (1) \\( |h(x)| \\leq 1 \\); (2) \\( f(-x) = g(x) \\) for \\( x \\in S^{n-1} \\); (3) Spherical harmonics form an orthonormal basis. \n\nDefinition 1. An origin-symmetric convex body \\( K \\in K_n \\) is classified as an intersection body if there exists a non-negative real number \\( \\lambda \\) such that its surface area measure \\( \\sigma_K \\) satisfies the following conditions:",
        "ori-fast-z-score": 0.19069251784911848,
        "water-fast-z-score": 3.8138503569823694,
        "rewrite-fast-z-score": 1.7717591356565963
    },
    {
        "original_text": "In this work, we study the problem of clustering co-occurrences of maximal frequent patterns (MFP) in streams. We propose an algorithm that clusters MFPs based on their occurrences and then uses these clusters to build a compact representation for each pattern. The proposed approach is efficient both in terms of space consumption as well as time required to process new data points. Our experimental results show that our method outperforms state-of-the-art algorithms by up to two orders of magnitude while achieving high accuracy. In addition, it can be easily parallelized using existing frameworks such as MapReduce or Spark. Keywords: Data stream mining, Cluster analysis, Frequent itemset mining, Association rule learning, Time series prediction. 1 Introduction Mining large volumes of streaming data has become increasingly important over recent years due to its wide range of applications including sensor networks  20, 21  , social media analytics  22  , fraud detection  23  , network intrusion detection  24  , etc.. However, processing massive amounts of streaming data poses several challenges since traditional batch-based approaches are not applicable anymore  25  . For example, if one wants to detect anomalies in a continuous flow of tweets, he needs to continuously monitor all incoming messages and identify those which deviate significantly from normal behavior  26  .\nTo address these issues, researchers have developed various techniques for analyzing data streams  7, 8, 27  . An emerging area within data stream mining focuses on discovering interesting patterns from data streams  28  . A common task in this context is finding frequent items/patterns in data streams  29  . Another popular research direction involves identifying correlations between different attributes  30  . These tasks are often performed jointly with classification  31  and/or regression  32  problems.",
        "watermark_text": "In this research , we study the question of clustering co - occurrences of maximal frequent patterns ( MFP ) in streams . We suggest an algorithm that clusters MFPs based on their occurrences and then uses these clusters to build a compact form for each pattern .The proposed approach is efficient both in terms of space requirements as well as space required to collect new data sets . Our research results show that our technique outperforms state - of - the - art algorithms by up to two orders of magnitude while reaching large accuracy .In addition , it can be easily parallelized using existing frameworks such as MapReduce or Spark . Keywords : Data stream analysis , Cluster processing , Frequent itemset searching , Association control learning , Time series modeling .1 Introduction Mining huge volumes of streaming information has become rapidly interesting over recent years owing to its large variety of applications notably sensor networks 20 , 21 , social media analytics 22 , fraud detection 23 , network intrusion detection 24 , etc . . However , processing massive sums of streaming information poses various challenges since traditional batch - based methods are not applicable today 25 .For instance , if one wants to identify anomalies in a rapid stream of tweets , he requires to constantly watch all outgoing messages and locate those which deviate substantially from normal behavior 26 . To address these problems , researchers have developed various methods for studying data loops 7 , 8 , 27 .An developing region within information stream mined focuses on discovering interesting trends from information streams 28 . A prevalent problem in this environment is identifying significant items / patterns in data streams 29 .Another common study trend includes identifying correlations between various qualities 30 . These tasks are often worked collectively with classification 31 and / or regression 32 questions .",
        "rewrite_text": "In this study, we explore the clustering of co-occurrences of maximal frequent patterns (MFP) within data streams. We propose an algorithm that organizes MFPs based on their occurrences and utilizes these clusters to create a compact representation for each pattern. Our approach is efficient in terms of both space requirements and the effort needed to acquire new data sets. The results of our research demonstrate that our technique significantly outperforms state-of-the-art algorithms by as much as two orders of magnitude while maintaining high accuracy. Furthermore, it can be easily parallelized using existing frameworks like MapReduce or Spark. \n\n**Keywords:** Data stream analysis, Cluster processing, Frequent itemset searching, Association control learning, Time series modeling.\n\n**1 Introduction**  \nThe mining of large volumes of streaming data has gained considerable attention in recent years due to its wide range of applications, including sensor networks, social media analytics, fraud detection, and network intrusion detection. However, analyzing such vast streams of information presents numerous challenges, as traditional batch processing methods are no longer sufficient. For example, in order to detect anomalies in a fast-paced stream of tweets, one must continuously monitor all outgoing messages to identify those that significantly deviate from normal behavior. In response to these challenges, researchers have developed various methods for analyzing data streams. A growing area within information stream mining focuses on uncovering meaningful trends from these streams. A key issue in this context is the identification of significant items or patterns in data streams. Another common research trend involves detecting correlations among various attributes. These tasks are frequently addressed alongside classification and regression problems.",
        "ori-fast-z-score": -1.655211777204736,
        "water-fast-z-score": 9.26918595234652,
        "rewrite-fast-z-score": 1.8333333333333333
    },
    {
        "original_text": "We present the results of an extensive study of gas dynamics, star formation activity, dust extinction, stellar populations, and black hole accretion properties for a strongly lensed galaxy (A1689-zD1) at redshift 3.07. The lensing magnification factor is ~30Â±5. We use deep near-infrared spectroscopy to measure the kinematics of molecular hydrogen emission lines with high spatial resolution. Our observations reveal that this system consists of two merging galaxies separated by 1 kpc along the line-of-sight. One of these components shows strong HÎ² emission indicative of active galactic nuclei (AGN). This AGN component has a mass of âˆ¼10^9 M_sol , which corresponds to a supermassive black hole with a mass of âˆ½â€“1 Ã— 10^8 M_sol . Using our spatially resolved measurements we find evidence for intense nuclear starbursts on scales as small as 100 pc.",
        "watermark_text": "We present the conclusion of an extensive research of gas evolution , star formation activity , dust disappearance , stars populations , and dark hole accretion properties for a strongly lensed galaxy ( A1689 - zD1 ) at redshift 3 . 07 . The lensing magnification factor is ~ 30Â±5 .We use deep near - infrared spectroscopy to measure the kinematics of molecular hydrogen emission lines with high visual resolution . Our observations show that this scheme consists of two joining galaxies linked by 1 kpc along the line - of - view .One of these constituents exhibits strong HÎ² emission indicative of active galactic nuclei ( AGN ) . This AGN component has a mass of [UNK] ^ 9 M _ sol , which corresponds to a supermassive black hole with a mass of [UNK] “ 1 [UNK] — 10 ^ 8 M _ sol .Using our spatially resolved sensors we find proof for intense nuclear starbursts on sizes as low as 100 pc .",
        "rewrite_text": "We present the findings of a comprehensive study on gas evolution, star formation activity, dust depletion, stellar populations, and the accretion properties of a supermassive black hole in a strongly lensed galaxy (A1689-zD1) at a redshift of 3.07. The lensing magnification factor is approximately 30 ± 5. Utilizing deep near-infrared spectroscopy, we measured the kinematics of molecular hydrogen emission lines with high spatial resolution. Our observations reveal that the system consists of two merging galaxies connected over 1 kpc along the line of sight. One of these galaxies displays strong Hβ emission, indicative of an active galactic nucleus (AGN). This AGN component has a mass estimated around 10^9 M_sol, suggesting the presence of a supermassive black hole with a mass between 10^7 and 10^8 M_sol. Our spatially resolved data also provide evidence of intense nuclear starbursts occurring on scales as small as 100 pc.",
        "ori-fast-z-score": 0.2581988897471611,
        "water-fast-z-score": 5.505585837114527,
        "rewrite-fast-z-score": -0.1259881576697424
    },
    {
        "original_text": "The ultimate fate of the universe is one of the most important questions in physics and cosmology today, but it has been difficult to answer because quantum mechanics (QM) cannot be applied directly to macroscopic systems such as the whole universe.  In this talk I will present an approach that allows us to use QM to study the evolution of the universe on all scales by applying it only to small subsystems within the universe.   This method can also be used to calculate the probability distribution for the time at which the universe ends its existence. The results are consistent with current observations and provide new insights into how the universe may end up. For example, we find that there is a finite probability that the universe will expand forever without ever ending or collapsing. We also show that if the universe does collapse then it must do so before 10^(10^28) years have passed since the Big Bang. Finally, we discuss some possible implications of these results for the future of humanity.",
        "watermark_text": "The ultimate fate of the universe is one of the most important questions in science and cosmology today , but it has been difficult to ask because physics dynamics ( QM ) unable be applied directly to macroscopic systems such as the whole universe . In this talk I will present an perspective that enables us to use QM to study the evolution of the universe on all scales by application it only to small subsystems within the universe .This method can also be used to estimate the probability distribution for the period at which the universe ends its existence . The results are compatible with current observations and bring fresh insights into how the universe might end up .For instance , we find that there is a finite probability that the universe will expand forever without ever ending or falling . We also prove that if the universe does explode then it must do so before 10 ^ ( 10 ^ 28 ) years have passed since the Big Bang .Finally , we explain some possible possibilities of these results for the tomorrow of humanity .",
        "rewrite_text": "The ultimate fate of the universe stands as one of the most significant questions in contemporary science and cosmology. However, exploring this question has proven challenging, primarily because the dynamics of quantum mechanics (QM) cannot be directly applied to macroscopic systems, such as the entire universe. In this presentation, I will introduce a perspective that allows us to study the universe's evolution across all scales by applying QM only to small subsystems within it. This approach also enables us to estimate the probability distribution for when the universe might cease to exist. Our findings align with current observations and offer new insights into the universe's potential endpoints. For example, we find a finite probability that the universe will continue to expand indefinitely without collapsing or terminating. Furthermore, we demonstrate that if the universe is destined to end in a cataclysmic event, it must occur within \\(10^{(10^{28})}\\) years after the Big Bang. Lastly, we will discuss the implications of these results for the future of humanity.",
        "ori-fast-z-score": 2.5927248643506746,
        "water-fast-z-score": 7.607674567748488,
        "rewrite-fast-z-score": 1.8599622199011085
    },
    {
        "original_text": "We study the evolution of cosmic strings in an expanding universe, focusing on their formation mechanism and subsequent growth. We show that cosmic strings can form when magnetic fields are trapped inside overdense regions during inflation. The resulting network consists of many small loops which evolve into larger ones through gravitational radiation emission. This process is similar to the one proposed for electroweak strings formed at phase transitions after inflation. However, we find that the loop distribution function has a different shape than previously assumed. In particular, it contains more large loops with sizes comparable to the Hubble radius today. These loops may be detectable as stochastic backgrounds of gravitational waves or gamma rays. Cosmic strings have been predicted to exist since the early 1980s  1, 2  . They could arise naturally if there were extra dimensions beyond those observed so far  3  , or they might be produced at symmetry breaking phase transitions  4  .\nCosmic strings would produce observable effects such as gravitational lensing  5  , CMB anisotropies  6  , and primordial black holes  7, 8  . Despite this interest, no direct detection of cosmic strings has yet been made  9  . One reason why cosmic strings remain elusive is because they are expected to be very light (with masses less than $10^{-16}eV$)  10  . Another problem is that cosmic strings are not stable objects but rather decay rapidly via gravitational radiation  11  . Therefore, any observational evidence must come indirectly from the products of cosmic string decays  12  .\nIn order to make predictions about possible observations, cosmological simulations need to be performed  13  . A number of groups have studied cosmic string networks using N-body codes  14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64",
        "watermark_text": "We research the evolution of universe strings in an increasing universe , concentrating on their development process and subsequent proliferation . We suggest that cosmic strings can form when magnetic waves are locked inside overdense regions during inflation .The resulting system consists of several small curves which turn into larger ones through gravity radiation emission . This process is related to the one proposed for electroweak strings generated at phase transitions after inflation .However , we find that the loop distribution map has a different shape than previously predicted . In particular , it contains more massive loops with sizes comparable to the Hubble diameter today .These patterns could be detectable as stochastic backgrounds of gravitational waves or gamma radiation . Cosmic strings have been predicted to arise since the early 1980s 1 , 2 .They could occur readily if there were extra dimensions beyond those observed so far 3 , or they may be formed at symmetry breaking phase transitions 4 . Cosmic strings would create observable effects such as gravity lensing 5 , CMB anisotropies 6 , and primordial black holes 7 , 8 .Despite this interest , no close observation of universe strings has yet been achieved 9 . One reason why cosmic strings remain elusive is because they are expected to be very light ( with masses fewer than $ 10 ^ { - 16 } eV $ ) 10 .Another question is that cosmic strings are not stable objects but rather decay frequently via gravitational rays 11 . Therefore , any observational evidence needs go indirectly from the products of cosmic string decays 12 .In order to make predictions about likely observations , cosmological simulations need to be performed 13 . A variety of groups have researched cosmic string systems use N - body symbols 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64",
        "rewrite_text": "We investigate the evolution of universe strings within an expanding universe, focusing on their developmental processes and subsequent proliferation. We propose that cosmic strings can form when magnetic waves become trapped within overdense regions during the inflationary phase. This leads to a system comprising several small loops that evolve into larger ones through the emission of gravitational radiation. This phenomenon is analogous to that suggested for electroweak strings formed during phase transitions post-inflation. However, our findings indicate that the distribution of loops has a distinct shape compared to earlier predictions, notably featuring more massive loops with dimensions on par with today’s Hubble diameter. These structures could potentially be detected as stochastic backgrounds of gravitational waves or gamma radiation. Cosmic strings have been theorized since the early 1980s and may emerge readily if additional dimensions exist beyond those currently known, or they might form during symmetry-breaking phase transitions. Observable effects of cosmic strings could include gravitational lensing, cosmic microwave background anisotropies, and primordial black holes. Despite significant interest, close observations of universe strings have yet to be accomplished. One reason for their elusiveness is their expected light mass (less than \\(10^{-16} \\, eV\\)). Furthermore, cosmic strings are not stable and tend to decay frequently through gravitational radiation, necessitating that any observational evidence be derived indirectly from the decay products. To make predictions about potential observations, cosmological simulations are essential. Various research groups have explored cosmic string systems using N-body simulations.",
        "ori-fast-z-score": 0.7689218919450849,
        "water-fast-z-score": 8.342314001435737,
        "rewrite-fast-z-score": 3.9737947576742565
    },
    {
        "original_text": "We consider the effective inverse spectral problems (EISP) associated with rational Lax matrices, which are generalizations of classical EISP in terms of Jacobi matrices. We show that these new EISPs can be reduced to some special cases of classical EISP by using an explicit formula for the coefficients of the characteristic polynomial of a rational matrix. As examples we study two classes of rational Lax matrices: one is related to the Toda lattice hierarchy and another is connected with the modified Volterra lattice hierarchy. In particular, we give a complete description on all solutions of the corresponding EISPs. Finally, as applications of our results, we present several interesting properties about the spectra of these rational Lax matrices. The research was supported by NSFC under Grant No.11571040. Keywords: Effective inverse spectral problem; Rational Lax matrices; Toda lattice hierarchy; Modified Volterra lattice hierarchy",
        "watermark_text": "We consider the effective inverse spectral questions ( EISP ) associated with rational Lax matrices , which are generalizations of classical EISP in terms of Jacobi matrices . We see that these new EISPs can be reduced to some general cases of classical EISP by using an explicit formula for the coefficients of the characteristic polynomial of a rational matrix .As instance we study two groups of rational Lax matrices : one is related to the Toda lattice hierarchy and another is linked with the modified Volterra lattice hierarchy . In particular , we give a complete overview on all solutions of the associated EISPs .Finally , as applications of our results , we present several interesting properties about the spectra of these rational Lax matrices . The research was supported by NSFC under Grant No . 11571040 .Keywords : Effective inverse spectral problem ; Rational Lax matrices ; Toda lattice hierarchy ; Modified Volterra lattice hierarchy",
        "rewrite_text": "We explore the effective inverse spectral questions (EISP) associated with rational Lax matrices, which serve as generalizations of the classical EISP related to Jacobi matrices. Our analysis shows that these new EISPs can be simplified to various general cases of the classical EISP by utilizing a specific formula for the coefficients of the characteristic polynomial of a rational matrix. Specifically, we examine two categories of rational Lax matrices: one associated with the Toda lattice hierarchy and the other linked to the modified Volterra lattice hierarchy. We provide a comprehensive overview of all solutions to the corresponding EISPs. Additionally, we discuss several intriguing properties of the spectra of these rational Lax matrices as applications of our findings. This research was supported by the NSFC under Grant No. 11571040. \n\nKeywords: Effective inverse spectral problem; Rational Lax matrices; Toda lattice hierarchy; Modified Volterra lattice hierarchy.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 2.6887744785908154,
        "rewrite-fast-z-score": -1.016001016001524
    },
    {
        "original_text": "We report on the detection by Swift/BAT and Konus-Wind of gamma-ray burst (GRB) 060927, which is one of only two events detected with redshifts greater than five to date.  The redshift was determined using optical spectroscopy obtained with the Very Large Telescope in Chile within three hours after the burst trigger time.  We find that this event has an extremely hard spectrum, similar to other high-redshift bursts observed previously.  Using our current understanding of the physics behind these phenomena we can estimate the total energy released during the prompt emission phase of the burst to be ~10^53 erg.  This value is comparable to or larger than the energies released by some supernovae.  If confirmed, it would make GRB 060927 the most energetic event ever recorded.  In addition, if the host galaxy mass function evolves similarly to what is seen locally, then the progenitor system responsible for producing such an energetic explosion must have been hosted by a very massive galaxy.  These results are consistent with theoretical predictions made about the progenitors of high-z GRBs.  Finally, we discuss how future observations of this object may help us understand the end of cosmic reionization.",
        "watermark_text": "We report on the discovery by Swift / BAT and Konus - Wind of gamma - ray burst ( GRB ) 060927 , which is one of only two events discovered with redshifts greater than five to date . The redshift was calculated using optical spectroscopy acquired with the Very Large Telescope in Chile within three hours after the explosion trigger time .We see that this event has an incredibly hard range , comparable to other high - redshift bursts observed previously . Using our contemporary understanding of the physics behind these phenomena we can calculate the total energy released during the prompt emission stage of the explosion to be ~ 10 ^ 53 erg .This value is analogous to or larger than the energies released by some supernovae . If confirmed , it would call GRB 060927 the most intense phenomenon ever recorded .In addition , if the host star mass distribution evolves likewise to what is seen locally , then the progenitor system responsible for producing such an energetic explosion could have been hosted by a very huge galaxy . These conclusions are compatible with theoretical calculations made about the progenitors of high - z GRBs .Finally , we explain how possible measurements of this body may assist us explain the end of universe reionization .",
        "rewrite_text": "We report the discovery of gamma-ray burst (GRB) 060927 by Swift/BAT and Konus-Wind, which is notable for being one of only two events identified with redshifts exceeding five to date. The redshift was determined through optical spectroscopy obtained with the Very Large Telescope in Chile within three hours following the explosion trigger. This event displays an exceptionally hard spectral range, comparable to other previously observed high-redshift bursts. Based on our current understanding of the physics driving these phenomena, we estimate that the total energy released during the prompt emission phase of the explosion is approximately 10^53 erg. This energy level is similar to or even greater than that released by some supernovae, and if verified, would make GRB 060927 the most powerful phenomenon ever recorded. Furthermore, if the mass distribution of host stars evolves in a manner similar to what is observed locally, the progenitor system that generated such a high-energy explosion may have originated from a very massive galaxy. These findings align with theoretical models regarding the progenitors of high-redshift GRBs. Lastly, we discuss how potential measurements of this event could enhance our understanding of the reionization phase of the universe.",
        "ori-fast-z-score": -0.6546536707079772,
        "water-fast-z-score": 6.764754597315764,
        "rewrite-fast-z-score": 1.4924050144892729
    },
    {
        "original_text": "We report on the first simultaneous infrared (IR) and X-ray observations of Sgr A*, made with the Chandra X-Ray Observatory and the Spitzer Space Telescope in 2007-2008. We find that the IR emission is consistent with being produced by dust heated to temperatures between 100 K and 1000 K; this temperature range corresponds to an observed flux density at 8 microns ranging from 0.1 mJy to 1 Jy. The spectral index of the IR emission does not change significantly during these variations. This result suggests that the physical conditions within the emitting region are relatively constant over time scales as short as one month. These results also suggest that the IR emission may be dominated by optically thin thermal bremsstrahlung rather than synchrotron radiation. \n \n Keywords: black hole physics, infrared astronomy, radio source variability, space telescopes, X-ray astronomy \n \n \n \n Black holes have been predicted to produce intense electromagnetic fields near their event horizons. However, direct observational evidence has remained elusive because of the extreme environment surrounding such objects. One possible way to detect such fields would be through the detection of polarized light emitted close to the horizon. Another possibility involves detecting changes in the spectrum or intensity of the accretion flow onto the black hole itself. Such changes could occur if the magnetic field lines threading the disk were twisted into helical shapes due to differential rotation. If so, they can act like antennae which amplify any incoming waves along them. As a consequence, the local plasma frequency will increase, causing the plasma to become more opaque to lower-frequency waves but less opaque to higher frequencies. Thus, we expect the spectrum of the emission to steepen toward longer wavelengths when the system becomes brighter.",
        "watermark_text": "We report on the first simultaneous infrared ( IR ) and X - ray observations of Sgr A * , made with the Chandra X - Ray Observatory and the Spitzer Space Telescope in 2007 - 2008 . We see that the IR emission is consistent with being produced by dust cooled to temperatures between 100 K and 1000 K ; this heat range corresponds to an seen flux concentration at 8 microns ranging from 0 . 1 mJy to 1 Jy .The spectral index of the IR emission does not change considerably during these changes . This result suggests that the physical conditions within the emitting area are fairly stable over time ranges as short as one month .These data therefore suggest that the IR emission may be dominated by optically thin thermal bremsstrahlung instead than synchrotron emission . Keywords : brown hole physics , infrared astronomy , television source variability , space telescopes , X - ray observations Black holes have been predicted to produce intense gravitational waves near their event horizons .However , direct observational evidence has remained elusive because of the severe environment neighboring such objects . One likely way to identify such fields might be through the observation of polarized light emitted far to the horizon .Another possibility requires detecting changes in the spectrum or intensity of the accretion flow onto the dark hole itself . Such changes could occur if the magnetic field lines threading the disk were twisted into helical shapes due to differential rotation .If so , they can operate like antennae which amplify any incoming signals along them . As a consequence , the local plasma frequency will expand , forcing the plasma to become more opaque to smaller - frequency waves but less opaque to higher frequencies .Thus , we expect the spectrum of the emission to steepen toward longer wavelengths when the system gets stronger .",
        "rewrite_text": "We present the first simultaneous infrared (IR) and X-ray observations of Sgr A*, conducted using the Chandra X-Ray Observatory and the Spitzer Space Telescope between 2007 and 2008. Our findings indicate that the IR emission likely arises from dust heated to temperatures ranging between 100 K and 1000 K, which corresponds to a flux concentration at 8 microns between 0.1 mJy and 1 Jy. Notably, the spectral index of the IR emission remains relatively stable throughout these variations. This stability implies that the physical conditions in the emitting region do not change significantly over time intervals as short as one month. Consequently, our data suggest that the IR emission may be predominantly influenced by optically thin thermal bremsstrahlung rather than synchrotron emission. \n\nKeywords: black hole physics, infrared astronomy, variability of celestial sources, space telescopes, X-ray observations. \n\nBlack holes are theorized to generate intense gravitational waves near their event horizons; however, direct observational evidence has been difficult to obtain due to the extreme conditions surrounding these objects. One potential method for detecting such gravitational fields is through the observation of polarized light emitted from far away from the horizon. Another approach may involve monitoring variations in the spectrum or intensity of the accretion flow onto the black hole itself. These variations could arise if the magnetic field lines within the accretion disk twisted into helical shapes due to differential rotation. If this occurs, the twisted fields could function like antennas, amplifying any incoming signals. As a result, the local plasma frequency would increase, making the plasma more opaque to lower-frequency waves while reducing opacity to higher frequencies. Therefore, we anticipate that the emission spectrum will steepen towards longer wavelengths as the system’s intensity increases.",
        "ori-fast-z-score": 1.153563462240948,
        "water-fast-z-score": 7.659922885032927,
        "rewrite-fast-z-score": 1.7277368511627202
    },
    {
        "original_text": "We study the problem of finding an optimal schedule for data transmission over multiple channels with interference constraints, where each channel is assigned to one transmitter-receiver pair and transmissions on different pairs are subject to mutual interference. We consider two models: (i) The first model assumes that all transmitters have fixed power levels. (ii) In the second model we assume that transmitters can adjust their powers dynamically. For both cases, we show how to find an optimal schedule by solving a sequence of linear programs. Our results hold even if there exists only one receiver per transmitter. This work was supported by NSF grant CCF-0430018. 1 Introduction Wireless networks consist of many nodes communicating via radio signals. Each node has limited range and thus cannot communicate directly with every other node. Instead, it communicates indirectly through intermediate nodes called relays or routers. A fundamental question in this context is: How should these relays be placed?",
        "watermark_text": "We explore the question of finding an appropriate schedule for information transmission over multiple channels with interference constraints , where each channel is allotted to one transmitter - receiver set and signals on separate pairs are subject to mutual interference . We consider two models : ( i ) The first theory requires that all transmitters have fixed power rates .( ii ) In the second theory we suppose that transmitters can adjust their powers dynamically . For both cases , we show how to find an appropriate schedule by solving a sequence of linear programs .Our results hold even if there exists only one receiver per transmitter . This work was supported by NSF grant CCF - 0430018 .1 Introduction Wireless networks consist of several nodes communicating via radio signals . Each node has restricted range and therefore cannot transmit directly with every other node .Instead , it communicates indirectly through intermediate nodes termed relays or routers . A basic issue in this situation is : How should these relays be placed ?",
        "rewrite_text": "We examine the challenge of establishing an adequate schedule for transmitting information across multiple channels that face interference constraints. In this scenario, each channel is designated for a specific transmitter-receiver pair, with signals from different pairs experiencing mutual interference. We investigate two models: (i) the first model assumes that all transmitters operate at fixed power rates, and (ii) the second model posits that transmitters can dynamically adjust their power levels. For both scenarios, we demonstrate how to determine an appropriate schedule by solving a series of linear programming problems. Our findings are applicable even when there is only one receiver assigned per transmitter. This research received support from NSF grant CCF-0430018. \n\n1 Introduction\n\nWireless networks comprise multiple nodes that communicate via radio signals. Due to the limited transmission range of each node, they cannot communicate directly with every other node. Instead, they rely on intermediate nodes, known as relays or routers, for indirect communication. A key consideration in this context is: How should these relays be positioned?",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 3.623286509262706,
        "rewrite-fast-z-score": 0.32539568672798425
    },
    {
        "original_text": "We have studied by means of molecular dynamics simulations how the dynamic behavior of silver atoms deposited on gold (100) is affected when the substrate temperature and the coverage are varied in such a way that the average number of nearest neighbors per atom changes between 1 to 4. We find that, for low coverages, the system behaves as if it were made up of isolated particles with no interaction among them. However, at higher coverages we observe collective effects which lead to the formation of ordered structures. The results obtained show that these structures can be classified into two different categories depending on whether they are formed by one or more layers of silver atoms. In particular, we found that the structure of the first layer depends strongly on the substrate temperature while the second layer shows only small variations. Finally, our calculations indicate that the third layer forms a disordered arrangement of silver atoms. This work was supported by DGESIC under project PB98-0443-C02-01",
        "watermark_text": "We have researched by means of molecular dynamics simulations how the dynamic behavior of silver atoms deposited on gold ( 100 ) is affected when the substrate temperature and the coverage are varied in such a way that the average number of nearest neighbors per atom shifts between 1 to 4 . We see that , for low coverages , the system behaves as if it were made up of isolated molecules with no activity among them .However , at higher coverages we study collective effects which lead to the formation of ordered systems . The results collected show that these structures can be categorized into two different categories depending on whether they are created by one or more bands of silver atoms .In particular , we reported that the composition of the first layer relies highly on the substrate temperature while the second layer exhibits only tiny variations . Finally , our calculations suggest that the third layer makes a disordered arrangement of silver atoms .This project was supported by DGESIC under contract PB98 - 0443 - C02 - 01",
        "rewrite_text": "We conducted research using molecular dynamics simulations to investigate how the dynamic behavior of silver atoms deposited on a gold (100) substrate is influenced by variations in substrate temperature and coverage, resulting in an average number of nearest neighbors per atom ranging from 1 to 4. At low coverages, the system resembles isolated molecules with minimal interaction among them. However, as coverage increases, we observe collective effects that lead to the emergence of ordered structures. Our findings indicate that these structures can be divided into two distinct categories based on whether they are formed by a single band or multiple bands of silver atoms. Notably, the composition of the first layer is significantly influenced by substrate temperature, while the second layer shows only minor variations. Additionally, our calculations suggest that the third layer results in a disordered arrangement of silver atoms. This project was funded by DGESIC under contract PB98-0443-C02-01.",
        "ori-fast-z-score": 1.2701705922171767,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 1.9126494315742406
    },
    {
        "original_text": "We study the fractional charge and statistics of elementary excitations in quantum spin systems with frustration, using exact diagonalization techniques for small clusters up to 12 sites. We find that the ground state is always gapped and has no degeneracy. The elementary excitations are fractionally charged fermions or bosons depending on whether the system is antiferromagnetic (AF) or ferromagnetic (F). In AF cases we also observe neutral fermionic excitations which carry zero electric charge but have nontrivial braiding properties. These results can be understood by mapping our models onto effective lattice gauge theories where the elementary excitations correspond to particles carrying flux quanta. Our work provides an explicit example of how fractional charges emerge naturally as topological defects in strongly correlated electronic materials. Introduction:-The discovery of high temperature superconductivity in copper oxide compounds  1  , together with other exotic phenomena such as colossal magnetoresistance  2  , non-Fermi liquid behavior  3  etc., has led to renewed interest in understanding the physics of strongly interacting electrons. One of the most important open questions concerns the nature of the elementary excitations responsible for these novel behaviors  4  . It was suggested early on  5  that the elementary excitations may be described by some kind of collective modes known as spin waves  6  . However it soon became clear  7, 8  that this description fails at low energies due to strong electron correlations. More recently there has been considerable progress towards developing theoretical descriptions based on new concepts like fractionalized quasiparticles  9  , emergent gauge fields  10  , and topological order  11  .\nIn particular, recent experiments  12  suggest that the elementary excitations in the cuprates might indeed be described by some form of fractionalized quasiparticle  13  . This raises many interesting questions about their physical properties including their charge  14  , statistics  15  , and interactions  16  . Unfortunately, despite enormous efforts  17  , a complete microscopic theory describing all these aspects remains elusive  18  . A promising approach involves studying simplified model Hamiltonians  19, 20  whose low-energy limit captures essential features of the original problem  21  .",
        "watermark_text": "We research the fractional charge and statistics of primary excitations in particle spin systems with frustration , using accurate diagonalization techniques for little complexes up to 12 locations . We see that the ground state is usually gapped and has no degeneracy .The elementary excitations are fractionally charged fermions or bosons depending on whether the system is antiferromagnetic ( AF ) or ferromagnetic ( F ) . In AF instances we also observe neutral fermionic excitations which carry zero electric charge but have nontrivial braiding properties .These data can be understood by map our models onto effective lattice gauge theories where the elementary excitations relate to particles carrying flux quanta . Our research provides an explicit instance of how fractional charges emerge readily as topological flaws in highly correlated electronic materials .Introduction : - The observation of high heat superconductivity in copper oxide molecules 1 , combined with other exotic processes such as colossal magnetoresistance 2 , non - Fermi solid behavior 3 etc . , has led to renewed concern in understanding the physics of highly bonding electrons . One of the most important open questions concerns the nature of the elementary excitations responsible for these novel behaviors 4 .It was suggested early on 5 that the elementary excitations might be described by some kind of collective modes referred as spin waves 6 . However it soon became clear 7 , 8 that this description fails at low energies due to large electron correlations .More recently there has been substantial development towards developing theoretical descriptions based on new concepts like fractionalized quasiparticles 9 , emergent gauge fields 10 , and topological order 11 . In particular , recent experiments 12 suggest that the elementary excitations in the cuprates might actually be described by some kind of fractionalized quasiparticle 13 .This poses various exciting questions about their natural characteristics notably their charge 14 , statistics 15 , and interactions 16 . Unfortunately , despite enormous efforts 17 , a complete microscopic theory explaining all these aspects remains elusive 18 .A good approach requires studying simplified theory Hamiltonians 19 , 20 whose low - energy maximum reflects vital features of the previous problem 21 .",
        "rewrite_text": "We investigate the fractional charge and statistical properties of primary excitations in frustrated particle spin systems, utilizing precise diagonalization techniques for small complexes containing up to 12 sites. Our findings indicate that the ground state is typically gapped and non-degenerate. The elementary excitations can be classified as fractionally charged fermions or bosons, depending on whether the system exhibits antiferromagnetic (AF) or ferromagnetic (F) characteristics. In the AF case, we also identify neutral fermionic excitations that possess zero electric charge but exhibit intriguing braiding properties. These observations can be interpreted by mapping our models onto effective lattice gauge theories, where the elementary excitations correspond to particles carrying flux quanta. Our research illustrates a clear instance of how fractional charges can arise as topological defects in highly correlated electronic materials.\n\n### Introduction:\nThe discovery of high-temperature superconductivity in copper oxide compounds, alongside other unusual phenomena such as colossal magnetoresistance and non-Fermi liquid behavior, has reignited interest in understanding the physics of highly interacting electrons. A central question remains regarding the nature of the elementary excitations responsible for these unique behaviors. Early hypotheses suggested that these excitations could be described as collective modes known as spin waves. However, it soon became apparent that this model fails to account for low-energy dynamics due to significant electron correlations. Recently, there has been considerable progress in formulating theoretical frameworks based on novel concepts such as fractionalized quasiparticles, emergent gauge fields, and topological order. Experiments indicate that the elementary excitations in cuprates may actually be modeled as fractionalized quasiparticles. This raises intriguing questions regarding their properties, including charge, statistics, and interactions. Despite extensive research efforts, a comprehensive microscopic theory that elucidates all these features remains elusive. A promising approach involves studying simplified Hamiltonians whose low-energy behaviors capture essential characteristics of the broader problem.",
        "ori-fast-z-score": 0.7905694150420948,
        "water-fast-z-score": 7.533990064322369,
        "rewrite-fast-z-score": 0.5659164584181103
    },
    {
        "original_text": "The present work is devoted to the study of photon wave mechanics in terms of position eigenvectors, which are introduced as solutions of the Schrödinger equation for photons with an arbitrary energy spectrum. The concept of position eigenvector allows one to describe the state of a single photon by its position probability density distribution function (PDF). It also enables us to introduce the notion of quantum trajectory describing the evolution of this PDF over time. In particular, we show that the quantum trajectories corresponding to different initial states can be obtained from each other by means of unitary transformations. We demonstrate how these results may be used to analyze various phenomena related to the propagation of light through dispersive media. Finally, we discuss possible applications of our approach to the description of nonclassical effects associated with the emission of entangled pairs of photons. DOI: 10.1088/1742-6596/aa5e20\nI. INTRODUCTORY REMARkS\n\nIn recent years there has been considerable interest in developing new approaches to studying the properties of light fields based on the concepts of quantum optics  1–3  . One of such approaches involves introducing the so-called position eigenvectors  4  , which play an important role in the description of the state of a single-photon field  5–7  .\nIt should be noted that the use of position eigenvectors makes it possible not only to obtain information about the spatial structure of the electromagnetic field but also to investigate the temporal dynamics of the system under consideration  8, 9  . This fact opens up wide possibilities for applying the proposed method to analyzing various physical processes occurring during the propagation of light waves through dispersive media  10, 11  . \n \n In addition, the introduction of position eigenvectors into the theory of light fields leads to the possibility of using them to describe certain nonclassical effects associated",
        "watermark_text": "The present work is devoted to the study of photon wave theory in terms of position eigenvectors , which are introduced as solutions of the Schrödinger equation for photons with an arbitrary energy wavelength . The concept of position eigenvector allows one to define the state of a single photon by its position probability distribution distribution function ( PDF ) .It additionally permits us to introduce the notion of quantum path describing the evolution of this PDF over time . In particular , we prove that the quantum trajectories corresponding to different initial states can be obtained from each other by means of unitary transformations .We suggest how these results may be used to analyze numerous phenomena related to the propagation of light through dispersive media . Finally , we explain possible use of our approach to the description of nonclassical effects involved with the emission of entangled pairs of photons .DOI : 10 . 1088 / 1742 - 6596 / aa5e20 I . INTRODUCTORY REMARkS In recent years there has been sustained interest in developing new approaches to studying the properties of light fields based on the concepts of quantum optics 1 – 3 .One of such perspectives involves introducing the so - called position eigenvectors 4 , which take an important role in the description of the state of a single - photon field 5 – 7 . It should be mentioned that the using of position eigenvectors makes it necessary not only to obtain knowledge about the spatial shape of the electromagnetic field but also to examine the temporal properties of the system under consideration 8 , 9 .This fact offers up broad opportunities for applying the suggested method to investigating different physical processes arising during the propagation of light beams through dispersive media 10 , 11 . In addition , the introduction of position eigenvectors into the physics of light fields leads to the prospect of using them to explain certain nonclassical effects associated",
        "rewrite_text": "This study focuses on photon wave theory as framed by position eigenvectors, which are defined as solutions to the Schrödinger equation for photons with varying energy wavelengths. By utilizing position eigenvectors, we can characterize the state of a single photon through its position probability distribution function (PDF). Furthermore, this framework allows us to conceptualize quantum paths that describe the temporal evolution of the PDF. Notably, we demonstrate that quantum trajectories arising from different initial states can be transformed into one another via unitary transformations. We propose that these findings could facilitate the analysis of various phenomena related to light propagation in dispersive media. Additionally, we discuss the potential application of our approach in addressing nonclassical effects associated with the emission of entangled photon pairs. \n\nDOI: 10.1088/1742-6596/aa5e20 \n\nI. INTRODUCTORY REMARKS\n\nIn recent years, there has been a growing interest in exploring new methodologies for studying the properties of light fields, particularly through the lens of quantum optics. One such method involves the introduction of position eigenvectors, which play a crucial role in characterizing the state of a single-photon field. It is important to note that utilizing position eigenvectors necessitates not only an understanding of the spatial characteristics of the electromagnetic field but also an exploration of the temporal dynamics of the system being studied. This presents extensive opportunities to apply the proposed method to investigate various physical processes that occur during the propagation of light beams through dispersive media. Moreover, incorporating position eigenvectors into the analysis of light fields opens up avenues for explaining certain nonclassical effects.",
        "ori-fast-z-score": 2.6293856820079102,
        "water-fast-z-score": 8.227432617895719,
        "rewrite-fast-z-score": 1.5583874449479593
    },
    {
        "original_text": "We study the Catalan numbers in connection with the number of ways to realize a given triangulation as an ordered sequence of its diagonals, or equivalently, as a set of non-crossing diagonals. We show that this problem is related to counting certain types of Dyck paths. In particular we prove that for any positive integer n there are exactly C(n) different sequences of diagonals which can be realized by a convex quadrilateral having 2n sides. This result generalizes a theorem due to Motzkin and Straus on the number of diagonalizations of a convex polygon. \nIntroduction\n\nThe Catalan numbers count many combinatorial objects such as binary trees, noncrossing partitions, spanning trees, etc., see e.g.   1, 2  . The present work deals with another class of Catalan-like objects: triangulations of polygons (see Figure 1 ). A triangulation T of a simple polygon P is defined as follows: it consists of all edges of P together with some additional diagonals connecting pairs of vertices of P so that each interior angle of P becomes at least 90 degrees after adding these diagonals. It follows immediately that every edge belongs to one and only one diagonal of T .\nIn  3  , Motzkin and Straus  celebrated theorem states that if D denotes the set of diagonals of a convex polygon Q then |D| = 2|Q|. They also proved that the number of diagonalizations d(P ) of a convex polygon P equals the number of diagonals of a triangulation of P . \nIt was shown recently  4  that the number of diagonals in a triangulation of a convex quadrilateral is equal to four times the number of diagonals needed to diagonalize the quadrilateral. Thus, the following question arises naturally: What is the relationship between the number of diagonals required to diagonalize a convex quadrilateral and the number of diagonals used in a triangulation?",
        "watermark_text": "We explore the Catalan numbers in connection with the number of ways to realize a given triangulation as an ordered sequence of its diagonals , or equivalently , as a group of non - crossing diagonals . We see that this question is related to counting particular kinds of Dyck paths .In particular we prove that for any positive integer n there are exactly C ( n ) different sequences of diagonals which can be realized by a convex quadrilateral having 2n sides . This result generalizes a theorem according to Motzkin and Straus on the number of diagonalizations of a convex polygon .Introduction The Catalan numbers count many combinatorial objects such as binary forests , noncrossing partitions , covering trees , etc . , see e . g . 1 , 2 .The present work deals with another class of Catalan - like structures : triangulations of polygons ( view Figure 1 ) . A triangulation T of a simple polygon P is characterized as follows : it consists of all edges of P together with some additional diagonals connecting pairs of vertices of P so that each interior angle of P becomes at least 90 degrees after adds these diagonals .It follows instantly that every edge belongs to one and only one diagonal of T . In 3 , Motzkin and Straus celebrated conjecture states that if D denotes the group of diagonals of a convex polygon Q then | D | = 2 | Q | .They also proved that the number of diagonalizations d ( P ) of a convex polygon P equals the number of diagonals of a triangulation of P . It was shown later 4 that the number of diagonals in a triangulation of a convex quadrilateral is equal to four times the quantity of diagonals needed to diagonalize the quadrilateral .Thus , the following issue arises readily : What is the relationship between the number of diagonals needed to diagonalize a convex quadrilateral and the proportion of diagonals used in a triangulation ?",
        "rewrite_text": "We investigate the Catalan numbers in relation to the various ways to represent a specific triangulation as an ordered sequence of its diagonals or, equivalently, as a collection of non-crossing diagonals. This inquiry is tied to counting certain types of Dyck paths. Specifically, we demonstrate that for any positive integer \\( n \\), there are exactly \\( C(n) \\) distinct sequences of diagonals that can be formed by a convex polygon with \\( 2n \\) sides. This finding extends a theorem by Motzkin and Straus regarding the count of diagonalizations of a convex polygon. \n\nIntroduction: The Catalan numbers enumerate many combinatorial structures, including binary forests, non-crossing partitions, and covering trees (see references 1 and 2). The focus of this paper is another class of Catalan-like entities: the triangulations of polygons (refer to Figure 1). A triangulation \\( T \\) of a simple polygon \\( P \\) is defined as the combination of all the edges of \\( P \\) along with additional diagonals that connect pairs of vertices in such a way that each interior angle of \\( P \\) becomes at least 90 degrees once the diagonals are added. It quickly follows that each edge is part of exactly one diagonal in \\( T \\).\n\nIn reference 3, the celebrated conjecture by Motzkin and Straus states that if \\( D \\) represents the set of diagonals in a convex polygon \\( Q \\), then \\( |D| = 2|Q| \\). They also showed that the number of diagonalizations \\( d(P) \\) of a convex polygon \\( P \\) is equal to the number of diagonals in its triangulation. Later work (reference 4) demonstrated that the number of diagonals in a triangulation of a convex quadrilateral is four times the count of the diagonals required to diagonalize that quadrilateral. This naturally raises the question: What is the relationship between the number of diagonals needed to diagonalize a convex quadrilateral and the proportion of diagonals utilized in a triangulation?",
        "ori-fast-z-score": 0.9805806756909202,
        "water-fast-z-score": 5.5448262406693765,
        "rewrite-fast-z-score": 1.584236068762679
    },
    {
        "original_text": "We present an algorithm to construct the reduced basis space in the context of nonlinear problems with multiple solutions, which is based on the concept of quasi-equilibrium grid (QEG). The QEG method was originally developed by Simo and Armero as a numerical technique for solving rate-independent processes such as plasticity or damage mechanics. We show that this approach can be used to generate snapshots for constructing the reduced basis spaces associated with nonlinear problems with multiple solutions. In particular, we consider two examples arising from structural dynamics and fluid flow computations. Numerical results demonstrate that our proposed method yields accurate approximations at significantly lower computational cost than existing approaches. Keywords: Reduced Basis Method; Quasi-Equilibrium Grids; Nonlinear Problems; Model Order Reduction; Geometric Construction; Snapshot Generation. 1 Introduction.\nThe goal of this work is to develop efficient algorithms for generating snapshots for constructing the RB spaces associated with nonlinear problems having multiple solutions. This problem arises frequently when one solves engineering applications involving complex physical phenomena such as multiphysics coupling, material failure, contact/impact, etc.. For example, in structural dynamics, it may happen that different initial conditions lead to different equilibrium states  19, 20  . Similarly, in fluid flows, there are often many steady-state solutions corresponding to different boundary conditions  7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18  .\nIn order to solve these types of problems efficiently using the reduced basis method (RBM), it is necessary to have a good set of snapshots representing all possible solution behaviors. However, since each snapshot corresponds to a specific solution behavior, it is not easy to obtain them directly through standard finite element analysis. Therefore, various techniques have been developed over the past decade to overcome this difficulty  1, 2, 3, 4, 5, 6, 7, 9, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40",
        "watermark_text": "We present an algorithm to build the reduced basis space in the context of nonlinear issues with many solutions , which is based on the idea of quasi - equilibrium grid ( QEG ) . The QEG method was originally developed by Simo and Armero as a numerical technique for solving rate - based processes such as plasticity or damage mechanics .We see that this methodology can be used to create snapshots for constructing the reduced basis sets associated with nonlinear issues with many solutions . In particular , we define two examples arising from functional dynamics and fluid stream computations .Numerical results show that our proposed approach produces accurate approximations at significantly reduced theoretical cost than existing techniques . Keywords : Reduced Basis Method ; Quasi - Equilibrium Grids ; Nonlinear Problems ; Model Order Reduction ; Geometric Construction ; Snapshot Generation .1 Introduction . The goal of this project is to develop fast algorithms for generating snapshots for constructing the RB spaces related with nonlinear issues having many solutions .This problem arises often when one solves engineering applications requiring complex physical phenomena such as multiphysics coupling , structure crash , touch / explosion , etc . . For instance , in structural physics , it could happen that different initial conditions lead to different equilibrium states 19 , 20 . Similarly , in flow flows , there are often many steady - condition solutions corresponding to different boundary rules 7 , 8 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 .In order to solve these kinds of problems easily using the reduced basis method ( RBM ) , it is required to have a good collection of snapshots describing all possible solution behaviors . However , since each snapshot belongs to a certain solve behavior , it is not straightforward to obtain them directly through conventional finite element extraction .Therefore , various methods have been used over the previous decade to overcome this obstacle 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40",
        "rewrite_text": "We introduce an algorithm designed to establish the reduced basis space specifically for nonlinear problems with multiple solutions, leveraging the concept of quasi-equilibrium grids (QEG). The QEG approach, initially developed by Simo and Armero for addressing rate-based processes such as plasticity and damage mechanics, proves beneficial in generating snapshots for the creation of reduced basis sets linked to nonlinear challenges featuring numerous solutions. We provide two examples related to functional dynamics and fluid flow computations. Our numerical findings indicate that this proposed method yields precise approximations while incurring significantly lower theoretical costs compared to existing techniques. \n\n**Keywords:** Reduced Basis Method; Quasi-Equilibrium Grids; Nonlinear Problems; Model Order Reduction; Geometric Construction; Snapshot Generation.\n\n### 1 Introduction\nThe objective of this project is to develop efficient algorithms for generating snapshots needed to construct reduced basis spaces related to nonlinear issues with multiple solutions. This challenge frequently arises in engineering applications that involve complex physical phenomena, such as multiphysics coupling, structural impacts, and explosive events. For example, in structural physics, varying initial conditions can lead to different equilibrium states. Likewise, in fluid dynamics, numerous steady-state solutions may correspond to different boundary conditions. To effectively tackle these problems using the reduced basis method (RBM), it is crucial to compile a comprehensive set of snapshots that capture the diverse solution behaviors. However, since each snapshot corresponds to a specific solution behavior, acquiring them directly through conventional finite element extraction methods can be challenging. Consequently, a variety of techniques have been explored over the past decade to address this issue.",
        "ori-fast-z-score": -1.5888598190134724,
        "water-fast-z-score": 8.0,
        "rewrite-fast-z-score": -0.1690308509457033
    },
    {
        "original_text": "We study the critical behavior of the 3D RFIM with Gaussian distributed disorder by means of Monte Carlo simulations and finite-size scaling analysis. We find that the system undergoes a continuous phase transition at zero temperature, which is characterized by an infinite correlation length but no divergent susceptibility. The results are compared to those obtained for the pure 3D Ising model as well as other models with quenched disorder. In particular we show how our findings can be understood within the framework of the droplet picture. \nPACS numbers: 64.60.Cn, 64.60.J-, 64.60.Nz \nI. INTRODUCTORY REMARkS\nThe Random Field Ising Model (RFIM) has been introduced more than 50 years ago  1  . It describes a ferromagnetic material where each spin interacts only with its nearest neighbors via exchange interactions J ij , while it also feels an external magnetic field h i randomly oriented on different sites  2  .\nIn recent years there have been many studies devoted to this problem both experimentally  3  -  6  and theoretically  7  -  12  . This interest was triggered mainly by the fact that the RFIM shares some features with real systems such as diluted antiferromagnets or spin-glasses  13  -  15  . For example, the presence of quenched disorder leads to frustration effects  16  similar to those observed in spin-glass materials  17  . Moreover, the RFIM displays a rich variety of phases depending on the strength of the applied magnetic field  18  . At low fields one finds a paramagnetic phase, whereas above a certain threshold value H c = O(J), the spins align along the direction of the local magnetic field leading to a ferromagnetic state  19  . Finally, if the magnitude of the external field exceeds another threshold value H t > H c , the magnetization becomes discontinuous  20  . These three regimes are separated by two second-order transitions occurring at T c1 < 0 and T c2 > 0  21  . However, despite these analogies between the RFIM and experimental systems  22  , the exact nature of the phase diagram remains controversial  23  .",
        "watermark_text": "We research the significant behavior of the 3D RFIM with Gaussian distributed disorder by means of Monte Carlo simulations and finite - length scaling processing . We see that the system undergoes a continuous phase shift at zero temperature , which is characterized by an endless correlation length but no divergent susceptibility .The results are compared to those achieved for the pure 3D Ising model as well as other models with quenched disease . In particular we show how our findings can be understood within the framework of the droplet picture .PACS codes : 64 . 60 . Cn , 64 . 60 . J - , 64 . 60 . Nz I . INTRODUCTORY REMARkS The Random Field Ising Model ( RFIM ) has been proposed more than 50 centuries earlier 1 .It describes a ferromagnetic material where each spin interacts only with its closest neighbors via transfer interactions J ij , while it also feels an external magnetic force h i randomly oriented on various places 2 . In recent seasons there have been many research devoted to this question both experimentally 3 - 6 and theoretically 7 - 12 .This interest was sparked mainly by the fact that the RFIM shares some features with real systems such as diluted antiferromagnets or spin - glasses 13 - 15 . For instance , the presence of quenched instability leads to frustration effects 16 comparable to those observed in spinning - glass materials 17 .Moreover , the RFIM displays a rich multitude of components varying on the strength of the applied magnetic force 18 . At small fields one gets a paramagnetic phase , whereas above a certain threshold factor H c = O ( J ) , the spins align along the direction of the local magnetic force leading to a ferromagnetic state 19 .Finally , if the magnitude of the external field exceeds another threshold quantity H t > H c , the magnetization becomes discontinuous 20 . These three regimes are split by two second - order transitions happening at T c1 < 0 and T c2 > 0 21 .However , despite these analogies between the RFIM and experimental systems 22 , the exact structure of the phase diagram remains disputed 23 .",
        "rewrite_text": "We investigate the key behavior of the 3D Random Field Ising Model (RFIM) with Gaussian-distributed disorder using Monte Carlo simulations and finite-length scaling analysis. Our findings indicate that the system experiences a continuous phase transition at zero temperature, characterized by an infinite correlation length but no divergent susceptibility. We compare these results to those obtained for the pure 3D Ising model and other models with quenched disorder. Notably, we demonstrate that our results can be interpreted through the droplet theory framework. \n\n**PACS codes:** 64.60.Cn, 64.60.J- , 64.60.Nz \n\n**I. INTRODUCTORY REMARKS**  \nThe Random Field Ising Model (RFIM), proposed over 50 years ago, describes a ferromagnetic system where each spin interacts only with its nearest neighbors through exchange interactions \\(J_{ij}\\) while also experiencing an external magnetic field \\(h_i\\) that is randomly oriented at different locations. Recent years have seen substantial research into this model, both experimentally and theoretically. This interest is largely due to the RFIM's similarity to real-world systems, such as diluted antiferromagnets and spin glasses. For example, the presence of quenched disorder leads to frustration effects similar to those present in spin-glass materials. Additionally, the RFIM exhibits varied behavior depending on the strength of the external magnetic field: at low fields, the system is paramagnetic, while beyond a critical field \\(H_c \\approx O(J)\\), the spins align with the local magnetic field, resulting in a ferromagnetic state. If the external field surpasses another critical value \\(H_t > H_c\\), the magnetization becomes discontinuous. These three regimes are separated by two second-order transitions occurring at temperatures \\(T_{c1} < 0\\) and \\(T_{c2} > 0\\). Despite the similarities between the RFIM and experimental systems, the precise structure of its phase diagram remains a topic of debate.",
        "ori-fast-z-score": -0.7986208584745025,
        "water-fast-z-score": 7.0164641544562345,
        "rewrite-fast-z-score": -0.9918365981341755
    },
    {
        "original_text": "We present an approach for self-organization in networks based on multi-agent systems (MAS). The proposed method is applied to two different networks: one with mobile nodes and another with static ones, both using IEEE 802.11b as their communication protocol. In this work we use agents that are able to move between neighboring nodes, which allows them to collect information about the state of each node. This information can be used by other agents to make decisions such as: moving to new positions or changing the transmission power level. We have implemented our proposal in NS-2 simulator and compared it against three well-known algorithms: OLSR, AODV and DSR. Our results show that MAS outperforms these protocols in terms of: packet delivery ratio, end-to-end delay and energy consumption. Keywords: Multi-Agent Systems, Self-Organizing Networks, Mobile Agents, Communication Protocols, Energy Consumption, Packet Delivery Ratio",
        "watermark_text": "We present an approach for self - organization in networks based on multi - agent systems ( MAS ) . The proposed approach is applied to two different organizations : one with mobile connections and another with static ones , both using IEEE 802 . 11b as their networking protocol .In this study we using agents that are able to move between neighboring nodes , which allows them to collect data about the state of each node . This knowledge can be used by other agents to make choices such as : moving to new positions or altering the transmission power level .We have incorporated our proposal in NS - 2 simulator and compared it against three well - famous method : OLSR , AODV and DSR . Our results show that MAS outperforms these protocols in terms of : message delivery ratio , end - to - end delay and energy consumption .Keywords: Multi-Agent Systems, Self-Organizing Networks, Mobile Agents, Communication Protocols, Energy Consumption, Packet Delivery Ratio",
        "rewrite_text": "We introduce a novel method for self-organization within networks utilizing multi-agent systems (MAS). This method is implemented in two distinct organizational structures: one featuring mobile connections and the other consisting of static connections, with both utilizing IEEE 802.11b as their networking protocol. In this research, we employ agents capable of traversing between adjacent nodes, enabling them to gather information regarding the status of each node. This acquired data can then be leveraged by other agents to make decisions, such as relocating to different positions or adjusting the transmission power levels. We have integrated our approach into the NS-2 simulator and benchmarked it against three prominent protocols: OLSR, AODV, and DSR. Our findings indicate that the MAS framework surpasses these protocols in terms of message delivery ratio, end-to-end delay, and energy consumption. \n\nKeywords: Multi-Agent Systems, Self-Organizing Networks, Mobile Agents, Communication Protocols, Energy Consumption, Packet Delivery Ratio",
        "ori-fast-z-score": 1.6378460497066512,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 0.75
    },
    {
        "original_text": "The National Institutes of Health (NIH) is the largest funder of biomedical research in the United States, but it also supports non-biomedical research through its extramural program.  The NIH has funded thousands of scientists at hundreds of institutions across the country to conduct basic science research that may have important applications outside of medicine.   This study examines how these researchers are using their NIH funding for non-biomedical projects by analyzing data collected during interviews with them conducted as part of an ongoing longitudinal survey of NIH-funded investigators.  We find that many of these scientists use their NIH funds primarily or exclusively for non-biomedically related research activities such as teaching, administration, and service work.  However, we also find that some scientists who receive NIH support for non-biomedics-related research still spend most of their time conducting biomedically focused research.  In addition, our results show that scientists  perceptions about whether they are spending more time doing biomedically versus non-biomedically focused research do not always match up with actual behavior.",
        "watermark_text": "The National Institutes of Health ( NIH ) is the greatest funder of biomedical research in the United States , but it also supports non - biomedical research through its extramural program . The NIH has funded thousands of research at hundreds of organizations across the nation to conduct basic science research that might have important use outside of medicine .This study examines how these investigators are using their NIH funding for non - biomedical projects by analyzing data derived during surveys with them conducted as part of an continuing longitudinal survey of NIH - financed researchers . We see that several of these investigators use their NIH grants mainly or mainly for non - biomedically relevant academic operations such as teaching , administration , and service work .However , we also find that some scientists who receive NIH backing for non - biomedics - specific study still spend most of their hours pursuing biomedically focused research . In addition , our findings show that scientists perceptions about whether they are spent more work doing biomedically versus non - biomedically focused research do not always match up with actual behavior .",
        "rewrite_text": "The National Institutes of Health (NIH) stands as the largest funder of biomedical research in the United States, but it also invests in non-biomedical research through its extramural program. The NIH has supported thousands of studies at numerous organizations nationwide to explore fundamental scientific inquiries that may have implications beyond medicine. This study investigates how researchers are allocating their NIH funding towards non-biomedical projects by analyzing data collected from surveys conducted as part of a continuing longitudinal study on NIH-funded researchers. Our findings reveal that many of these researchers primarily utilize their NIH grants for non-biomedically relevant academic activities such as teaching, administration, and service. However, we also discover that some scientists who receive NIH support for non-biomedical research still dedicate most of their time to biomedically focused studies. Additionally, our results indicate a disconnect between scientists' perceptions of their time spent on biomedical versus non-biomedical research and their actual research activities.",
        "ori-fast-z-score": 2.0124611797498106,
        "water-fast-z-score": 7.888888888888889,
        "rewrite-fast-z-score": 0.8307471607356973
    },
    {
        "original_text": "We present the results on diffuse optical light (DOL) correlations with cluster properties for a sample of galaxy clusters observed by the Hubble Space Telescope Advanced Camera for Surveys and Spitzer Infrared Array Camera. We find that DOL correlates positively with X-ray luminosity, temperature, mass, velocity dispersion, and Sunyaev-Zel dovich effect flux decrement at 1.4 GHz. The correlation between DOL and X-ray luminosity is stronger than those found previously using ground-based data. These results suggest that DOL traces hot gas in galaxy clusters. This work was supported by NASA grant NNX08AG84G to Columbia University. We thank J. Richard McNamara for providing us with his Chandra observations of Abell 1689. We also acknowledge useful discussions with A. Vikhlinin. \n \n Keywords: Diffuse optical light; Galaxy clusters; Dark matter halos",
        "watermark_text": "We present the conclusion on diffuse optical light ( DOL ) correlations with cluster properties for a sample of galaxy galaxies studied by the Hubble Space Telescope Advanced Camera for Surveys and Spitzer Infrared Array Camera . We see that DOL correlates positively with X - ray luminosity , temperature , mass , speed dispersion , and Sunyaev - Zel dovich impact flux decrement at 1 . 4 GHz .The relationship between DOL and X - ray luminosity is greater than those identified previously used ground - based data . These data suggest that DOL marks hot gas in galaxy regions .This project was supported by NASA gift NNX08AG84G to Columbia University . We praise J . Richard McNamara for providing us with his Chandra measurements of Abell 1689 .We additionally note useful talks with A . Vikhlinin . Keywords : Diffuse optical light ; Galaxy clusters ; Dark matter halos",
        "rewrite_text": "We summarize our findings on the correlations between diffuse optical light (DOL) and the properties of galaxy clusters, based on a sample analyzed using the Hubble Space Telescope's Advanced Camera for Surveys and the Spitzer Infrared Array Camera. Our results indicate a positive correlation between DOL and various factors including X-ray luminosity, temperature, mass, velocity dispersion, and the Sunyaev-Zel'dovich effect's decrement in impact flux at 1.4 GHz. Notably, the association between DOL and X-ray luminosity exceeds that observed in previous ground-based studies, suggesting that DOL serves as an indicator of hot gas within galaxy regions. This research was supported by NASA grant NNX08AG84G to Columbia University. We would like to thank J. Richard McNamara for sharing his Chandra measurements of Abell 1689 and also acknowledge valuable discussions with A. Vikhlinin. \nKeywords: Diffuse optical light; Galaxy clusters; Dark matter halos.",
        "ori-fast-z-score": -0.8660254037844387,
        "water-fast-z-score": 5.484827557301445,
        "rewrite-fast-z-score": 0.674199862463242
    },
    {
        "original_text": "We study the properties of strong peak points in Banach spaces, which are defined as follows.  Let X be a real or complex normed space with dual space X*. A point x # X is called a strong peak point if there exists an f # S(X) such that |f (x)| = sup{|f (y)| : y # X}. We prove that every separable reflexive Banach space has a dense set of strong peak points. As applications we show that every separable reflexivizable Banach space contains a copy of c 0 , and that every separable superreflexive Banach space contains a subspace isomorphic to l p for some 1 < p < + . In this note we study the properties of strong peaks points in Banach spaces. The concept was introduced by J. Lindenstrauss  L  who proved that every separable reflexible Banach space has a nonempty set of strong peak points; see also  JL1  .\nIn Section 2 we give several equivalent characterizations of strong peak points. In particular, it turns out that a point x # X is a strong peak point if and only if there exist two sequences (a n ) and (b n ) in R satisfying lim n Ä a n = lim n Ä b n = 1 and lim n Ä a n &1Â2 b n = 0 such that the sequence (a n b n ) converges weakly to zero but not strongly. This characterization enables us to prove our first main result on the density of strong peak points in separable reflexive BanACH spaces. \nTheorem 3. Every separable reflexive BanACH space has a dense set SP(X) of strong peak points.\n\nAs immediate consequences of Theorem 3 we obtain the following results. (i) Every separable reflexivizable space contains a copy of c0.\n(ii) Every separable superreflexive space contains a subspace isomorphic",
        "watermark_text": "We research the properties of stable peak points in Banach spaces , which are given as follows . Let X be a real or complex normed space with dual collection X * .A point x # X is dubbed a strong peak point if there exists an f # S ( X ) such that | f ( x ) | = sup { | f ( y ) | : y # X } . We say that every separable reflexive Banach space has a dense setting of strengthened peak points .As applications we prove that every separable reflexivizable Banach space includes a copy of c 0 , and that every separable superreflexive Banach space includes a subspace isomorphic to l p for some 1 < p < + . In this note we study the properties of strong peaks points in Banach spaces .The concept was introduced by J . Lindenstrauss L who proved that every separable reflexible Banach space has a nonempty set of strong peak points ; hear also JL1 . In Section 2 we give numerous equivalent characterizations of strong peak points .In particular , it turns out that a point x # X is a strong peak point if and only if there exist two sequences ( a n ) and ( b n ) in R satisfying lim n Ä a n = lim n Ä b n = 1 and lim n Ä a n & 1Â2 b n = 0 such that the sequence ( a n b n ) converges weakly to zero but not strongly . This formulation enables us to prove our first major result on the density of strengthened peak points in separable reflexive BanACH spaces .Theorem 3 . Every separable reflexive BanACH space has a dense setting SP ( X ) of stable peak points .As obvious effects of Theorem 3 we obtain the following results . ( i ) Every separable reflexivizable set contains a copy of c0 .( ii ) Every separable superreflexive space includes a subspace isomorphic",
        "rewrite_text": "We investigate the characteristics of stable peak points in Banach spaces, defined as follows: Let \\( X \\) be a real or complex normed space with dual space \\( X^* \\). A point \\( x \\in X \\) is considered a strong peak point if there exists an \\( f \\in S(X) \\) such that \\( |f(x)| = \\sup \\{ |f(y)| : y \\in X \\} \\). It is known that every separable reflexive Banach space contains a dense collection of strengthened peak points. Applications of this result include showing that every separable reflexivizable Banach space contains a copy of \\( c_0 \\), and every separable superreflexive Banach space has a subspace that is isomorphic to \\( l_p \\) for some \\( 1 < p < \\infty \\). This note focuses on the properties of strong peak points in Banach spaces. The concept was first introduced by J. Lindenstrauss, who established that every separable reflexive Banach space has a non-empty set of strong peak points (see also JL1). In Section 2, we present various equivalent characterizations of strong peak points. Notably, a point \\( x \\in X \\) is a strong peak point if and only if there exist two sequences \\( (a_n) \\) and \\( (b_n) \\) in \\( \\mathbb{R} \\) such that \\( \\lim_{n \\to \\infty} a_n = \\lim_{n \\to \\infty} b_n = 1 \\) and \\( \\lim_{n \\to \\infty} a_n(b_n - \\frac{1}{2}) = 0 \\), where the sequence \\( (a_n b_n) \\) converges weakly to zero but not strongly. This formulation allows us to demonstrate our first significant finding regarding the density of strengthened peak points in separable reflexive Banach spaces. Theorem 3 states that every separable reflexive Banach space has a dense set \\( SP(X) \\) of stable peak points. As immediate consequences of Theorem 3, we derive the following results: (i) Every separable reflexivizable space contains a copy of \\( c_0 \\) and (ii) every separable superreflexive space includes a subspace that is isomorphic to \\( l_p \\) for some \\( 1 < p < \\infty \\).",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 3.3093806066996887,
        "rewrite-fast-z-score": -1.414213562373095
    },
    {
        "original_text": "We present new results on the color magnitude distribution (CMD) of field galaxies in the redshift range 0<z<3, based on deep optical imaging data obtained with Subaru/Suprime-Cam at the prime focus telescope of National Astronomical Observatory of Japan. We use two different samples for our analysis; one is a sample of about 12000 spectroscopically confirmed galaxies selected from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7), which covers an area of ~10deg2 around the North Galactic Pole (NGP). The other is a sample of about 10000 photometrically selected galaxies over an area of ~30deg2 centered on the Hubble Deep Field South (HDF-S).\nThe CMD shows that there are three distinct galaxy populations in terms of their rest-frame colors as well as luminosities. These are: red-sequence early-type galaxies, green valley late-type galaxies, and blue cloud star-forming galaxies. In addition we find that the fraction of blue cloud galaxies increases towards higher redshifts up to z~2.5-3.0, while it decreases again beyond this epoch. This trend can be explained by the fact that most massive galaxies have already formed stars before z~3, so they become redder than less-massive ones afterwards; therefore more massive galaxies dominate the red-sequence population at high-z. On the other hand, less-massive galaxies continue forming stars until today, resulting in larger fractions of blue cloud galaxies at lower redshifts.",
        "watermark_text": "We report new data on the color magnitude distribution ( CMD ) of field galaxies in the redshift region 0 < z < 3 , using on soft imaging scanning data acquired with Subaru / Suprime - Cam at the prime focus telescope of National Astronomical Observatory of Japan . We use two different samples for our analysis ; one is a sample of about 12000 spectroscopically confirmed galaxies drawn from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) , which covers an area of ~ 10deg2 around the North Galactic Pole ( NGP ) .The other is a sample of about 10000 photometrically selected galaxies over an area of ~ 30deg2 centered on the Hubble Deep Field South ( HDF - S ) . The CMD indicates that there are three separate universe groups in terms of their rest - frame colors as well as luminosities .These are : green - sequence earliest - class stars , green valley late - class objects , and green cloud star - creating galaxies . In addition we find that the fraction of blue forest objects increases towards higher redshifts up to z ~ 2 . 5 - 3 . 0 , while it reduces again beyond this epoch .This trend can be explained by the fact that most giant galaxies have already formed stars before z ~ 3 , so they become redder than less - massive ones afterwards ; therefore more massive galaxies represent the red - sequence population at high - z . On the other hand , less - massive galaxies continue forming stars until today , resulting in larger fractions of blue cloud galaxies at lower redshifts .",
        "rewrite_text": "We present new findings on the color magnitude distribution (CMD) of field galaxies within the redshift range of 0 < z < 3, utilizing soft imaging scanning data obtained from the Subaru/Suprime-Cam at the prime focus telescope of the National Astronomical Observatory of Japan. Our analysis is based on two distinct samples: one consists of approximately 12,000 spectroscopically confirmed galaxies sourced from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7), covering an area of about 10 deg² around the North Galactic Pole (NGP). The second sample includes around 10,000 photometrically selected galaxies over an area of approximately 30 deg² centered on the Hubble Deep Field South (HDF-S). The CMD reveals the presence of three distinct groups of galaxies based on their rest-frame colors and luminosities: the green-sequence early-type stars, the green valley late-type objects, and the green cloud star-forming galaxies. Furthermore, we observe that the proportion of blue forest galaxies increases with higher redshifts, peaking around z ~ 2.5 - 3.0, before declining beyond this point. This pattern can be attributed to the formation of most giant galaxies prior to z ~ 3, resulting in their redder colors over time compared to less massive galaxies. Consequently, more massive galaxies correspond to the red-sequence population at high redshifts, while less massive galaxies continue forming stars up to the present day, leading to a greater proportion of blue cloud galaxies at lower redshifts.",
        "ori-fast-z-score": 2.416841222614159,
        "water-fast-z-score": 7.057176370033344,
        "rewrite-fast-z-score": 1.8198699419201876
    },
    {
        "original_text": "The polaron problem is one of the most important problems in condensed matter physics, and has been studied extensively for many years.  In this work we present an overview of some recent results on path integral methods applied to the su(2)-schrieffer-heeger (s-shh) model with periodic boundary conditions.   We first review how the s-shh hamiltonian can be written as a sum over spinless fermions using the Jordan-Wigner transformation.  Then we discuss how the partition function may be evaluated by performing a trace over all possible states of these fermions.  Finally, we show that the resulting expression can be rewritten in terms of Feynman diagrams which are then used to calculate various physical quantities such as the energy spectrum or correlation functions. The polaron problem is one o fthe most important problems in condensate matter physics, and has b een studied extensively for many years  1  . It describes a single electron moving through a lattice of atoms interacting via phonons  2  , where the electron-phonon interaction leads to the formation of a bound state known as a polaron  3  .\nIn this work w epresent an overview of some recent resul ts on path integral m ethods applied t o th e su(2)-schr iefer -heeg er (s-shh ) model  4  wit h p eriodic bo undary condit ions  5  .  W e first r evie w ho w th e shh h amiltonia n ca n be wr it ten as a sum ov er sp inl ess fermi ons usin g th e J ordan-Wign er transfor mat ion  6  .  Th en we discu ss how th e partiti on functi on m ay be evalua ted by perform ing a tr ace ov er al l possibl e st at es of th ese fermi ons.  Fina ll y, we sho w tha t th e resul tin g ex pressio n ca n be rewrite n in term s of Feyn man di agrams wh ich ar e th en u",
        "watermark_text": "The polaron problem is one of the most important problems in condensed matter theory , and has been studied thoroughly for hundreds years . In this research we present an overview of some latest findings on path integral methods applied to the su ( 2 ) - schrieffer - heeger ( s - shh ) model with periodic boundary constraints .We first review how the s - shh hamiltonian can be written as a sum over spinless fermions using the Jordan - Wigner transformation . Then we explain how the partition function could be evaluated by performing a trace over all possible states of these fermions .Finally , we prove that the resulting expression can be rewritten in terms of Feynman diagrams which are then used to estimate various physical components such as the power spectrum or correlation functions . The polaron problem is one o fthe most important problems in condensate matter mechanics , and has b een discussed heavily for many years 1 .It describes a single electron moving through a lattice of atoms interacting via phonons 2 , where the electron - phonon interaction results to the formation of a bound state known as a polaron 3 . In this study w epresent an overview of some latest resul ts on path integral m ethods applied t o th e su ( 2 ) - schr iefer - heeg er ( s - shh ) model 4 wit n p eriodic bo undary condit ions 5 .W e first r evie w ho w th e shh n amiltonia n ca n be wr it ten as a sum ov er sp inl ess fermi ons usin g th e J ordan - Wign er transfor mat ion 6 . Th en we discu ss how th e partiti on functi on m ay be evalua ted by perform ing a tr ace ov er al l possibl e st at en of th ese fermi ons .Fina ll y , we sho l tha t th e resul tin g ex pressio n ca n be rewrite n in word s of Feyn man di agrams wh ich ar e th en u",
        "rewrite_text": "The polaron problem is a fundamental issue in condensed matter theory that has been extensively explored for centuries. In this research, we provide an overview of recent developments in path integral methods applied to the SU(2) - Schrieffer-Heeger (S-SHH) model under periodic boundary conditions. We begin by reviewing how the S-SHH Hamiltonian can be expressed as a sum of spinless fermions using the Jordan-Wigner transformation. Next, we describe how the partition function can be computed by taking a trace over all possible states of these fermions. Finally, we demonstrate that the resulting expression can be reformulated in terms of Feynman diagrams, which are subsequently utilized to estimate various physical quantities such as the power spectrum and correlation functions. The polaron problem describes the behavior of a single electron moving through a lattice of atoms while interacting with phonons, leading to the formation of a bound state known as a polaron. This topic has garnered significant attention for many years.",
        "ori-fast-z-score": 1.4419211804559506,
        "water-fast-z-score": 5.448041796855991,
        "rewrite-fast-z-score": 2.7688746209726918
    },
    {
        "original_text": "The authors describe how they have created an artificial life form that self-assembles into a structure similar to the fruiting body of the Dictyostelium discoideum, or  dictyos  for short.  The dictyos is made up of thousands of identical cells connected by elastic filaments and can be grown in Petri dishes with nutrients added.   It has been shown to grow and divide indefinitely without any external control signals (such as hormones).   This work was done at Harvard University s Wyss Institute for Biologically Inspired Engineering.   ... Self-assembly refers to the spontaneous organization of matter into ordered structures such as crystals, snowflakes, and living organisms like bacteria colonies and animal tissues. In this study we report on our efforts toward creating a synthetic multicellular system capable of autonomous growth and division through self-assembly. We designed a minimal cell based on a spherical water-in-oil emulsion droplet containing a single microtubule-based cytoskeleton surrounded by a lipid membrane. These cells are able to attach to each other via flexible polymeric tethers and assemble into three-dimensional aggregates called “dictyos” which resemble the fruiting bodies formed by the social amoeba Dictyostelium discoidium. Our results demonstrate that these simple cellular units can autonomously organize themselves into complex 3D shapes reminiscent of natural systems.",
        "watermark_text": "The authors report how they have created an artificial life form that self - assembles into a structure similar to the fruiting bodies of the Dictyostelium discoideum , or dictyos for short . The dictyos is made up of millions of identical cells connected by elastic filaments and can be grown in Petri dishes with minerals added .It has been shown to expand and divide indefinitely without any external regulating signals ( such as hormones ) . This research was done at Harvard University s Wyss Institute for Biologically Inspired Engineering .. . . Self - assembly describes to the spontaneous organization of matter into organized objects such as bubbles , snowflakes , and live organisms like bacteria populations and human flesh . In this study we review on our work toward building a artificial multicellular system efficient of automatic development and division through self - assembly .We built a minimal cell based on a spherical water - in - oil emulsion droplet containing a single microtubule - based cytoskeleton enclosed by a lipid membrane . These cells are able to connect to each other via flexible polymeric tethers and arrange into three - dimensional aggregates called “ dictyos ” which mimic the fruiting bodies produced by the social amoeba Dictyostelium discoidium .Our results show that these simple cellular divisions can autonomously arrange themselves into complex 3D shapes reminiscent of natural units .",
        "rewrite_text": "The authors describe their creation of an artificial life form that self-assembles into structures resembling the fruiting bodies of *Dictyostelium discoideum*, commonly known as dictyos. This structure consists of millions of identical cells interconnected by elastic filaments and can be cultivated in Petri dishes with added minerals. Remarkably, it has demonstrated the ability to grow and divide indefinitely without requiring external regulatory signals, such as hormones. This research was conducted at Harvard University's Wyss Institute for Biologically Inspired Engineering. \n\nSelf-assembly refers to the spontaneous organization of matter into structured forms, like bubbles, snowflakes, and living organisms, including bacterial colonies and human tissue. In this study, we discuss our efforts to construct an artificial multicellular system capable of autonomous development and division through self-assembly. We developed a minimal cell based on a spherical water-in-oil emulsion droplet, which contains a single microtubule-based cytoskeleton encased in a lipid membrane. These cells can interconnect using flexible polymeric tethers and form three-dimensional aggregates called \"dictyos,\" mimicking the fruiting bodies produced by the social amoeba *Dictyostelium discoideum*. Our findings indicate that these simple cellular divisions can autonomously arrange into complex 3D shapes that resemble those found in nature.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.7461923416925424,
        "rewrite-fast-z-score": 0.7337993857053429
    },
    {
        "original_text": "The structural relaxation and the local vibrational modes (LVMs) are investigated by first-principles calculations for the substituted system Mg1-xCrxAl2-xO4 with x=0, 0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0 at%. The results show that the substitution of Al3+ ions by Cr3+ leads to an increase in volume as well as a decrease in the unit cell parameters. In addition, we find that there is no significant change in the bond lengths between nearest-neighboring atoms when the concentration of Cr3+ increases up to 5%. However, it can be seen clearly that the bond length between second-nearest neighboring atoms decreases gradually with increasing Cr3+ content. Furthermore, our calculated phonon dispersion curves indicate that all the LVMs are stable except those along the Γ-X direction which become unstable above 1% Cr3+ doping.",
        "watermark_text": "The mechanical relaxation and the local vibrational modes ( LVMs ) are examined by first - principles calculations for the substituted solution Mg1 - xCrxAl2 - xO4 with x = 0 , 0 . 25 , 0 . 5 , 1 . 0 , 2 . 0 , 3 . 0 , 4 . 0 , 5 . 0 at % . The results show that the replacement of Al3 + ions by Cr3 + leads to an increase in volume as well as a reduction in the unit cell dimensions .In addition , we find that there is no considerable shift in the bond lengths between nearest - neighboring atoms when the quantity of Cr3 + increases up to 5 % . However , it can be shown clearly that the bond length between second - nearest adjacent molecules decreases slowly with rising Cr3 + content .Furthermore , our derived phonon dispersion parameters indicate that all the LVMs are stable except those along the Γ - X orientation which become unstable above 1 % Cr3 + doping .",
        "rewrite_text": "We investigated the mechanical relaxation and local vibrational modes (LVMs) of the substituted solution Mg1-xCrxAl2-xO4 using first-principles calculations for varying concentrations of x = 0, 0.25, 0.5, 1.0, 2.0, 3.0, 4.0, and 5.0 at %. The findings reveal that substituting Al3+ ions with Cr3+ ions leads to an increase in volume while also decreasing the dimensions of the unit cell. Notably, there is no significant change in the bond lengths between nearest-neighboring atoms as the Cr3+ content increases up to 5%. However, the bond length among second-nearest neighboring molecules gradually decreases with higher Cr3+ concentrations. Additionally, our analysis of phonon dispersion parameters indicates that all LVMs remain stable, except for those along the Γ-X direction, which become unstable at Cr3+ doping levels above 1%.",
        "ori-fast-z-score": 0.5443310539518174,
        "water-fast-z-score": 5.258758927213289,
        "rewrite-fast-z-score": 1.4832396974191326
    },
    {
        "original_text": "We present an efficient method for computing the vacuum energy and force between two parallel plates using only standard numerical electromagnetism methods, without resorting to any approximations or special treatments such as analytic continuation into complex frequencies. The key idea is that we can use the Feynman-Kac formula to express the vacuum expectation value of the stress tensor at finite temperature T = 1/β (where β is the inverse temperature) in terms of the corresponding quantity at zero temperature but with one additional term involving the time evolution operator over a period of length β . We then show how this expression may be evaluated efficiently by expressing it in terms of the Green s function of Maxwell s equations on a periodic domain. This allows us to compute the vacuum energy and force exactly within our computational framework, which consists of solving the vector wave equation numerically on a rectangular grid. Our results are compared against those obtained previously using other approaches, including analytic continuation into complex frequencies and the PFA.",
        "watermark_text": "We present an efficient approach for calculation the vacuum energy and force between two connected sheets utilizing only conventional numerical electromagnetism methods , without resorting to any approximations or particular treatments such as analytic continuation into complex frequencies . The main idea is that we can using the Feynman - Kac formula to derive the vacuum expectation value of the strain vector at finite temperature T = 1 / beta ( where β is the inverse temperature ) in terms of the equivalent quantity at zero temperature but with one additional word regarding the period evolution operator over a period of length β .We then show how this formula may be evaluated efficiently by expressing it in terms of the Green s function of Maxwell s equations on a periodic domain . This enables us to compute the vacuum energy and force exactly within our computational framework , which consists of calculating the linear wave equation numerically on a rectangular grid .Our results are compared against those achieved previously used other methods , notably analytic continuation into complex frequencies and the PFA .",
        "rewrite_text": "We present an efficient method for calculating the vacuum energy and force between two connected sheets using only traditional numerical electromagnetism techniques, without the need for approximations or specialized treatments like analytic continuation into complex frequencies. The key insight is that we can apply the Feynman-Kac formula to derive the vacuum expectation value of the strain vector at a finite temperature \\( T = \\frac{1}{\\beta} \\) (where \\( \\beta \\) is the inverse temperature), relating it to the corresponding value at zero temperature but with an added component involving the evolution operator over a period of length \\( \\beta \\). We demonstrate how to evaluate this formula efficiently by expressing it in terms of the Green's function of Maxwell's equations on a periodic domain. This approach allows us to compute the vacuum energy and force precisely within our computational framework, which entails numerically solving the linear wave equation on a rectangular grid. Our results are compared to those obtained using other methods, particularly analytic continuation into complex frequencies and the proximity force approximation (PFA).",
        "ori-fast-z-score": 0.6625891564490792,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": 1.4269353798659745
    },
    {
        "original_text": "We present an analysis of the transition between first stars and second stars, which are formed by gravitational collapse of primordial gas clouds with masses ranging from 10^4 M_sol to 10^6 M_sol.  We show that the formation rate of second stars is suppressed at redshifts z < 20 due to photoheating effects on the intergalactic medium (IGM). The suppression factor increases as redshift decreases because the IGM temperature rises more rapidly than its density. At lower redshifts, we find that the formation rates of both first and second stars increase sharply when the universe becomes reionized. This effect occurs because the ionizing photons produced during reionization heat up the surrounding neutral hydrogen atoms, thereby increasing their Jeans mass and suppressing fragmentation into smaller objects. Finally, we estimate the number densities of first and second stars using our model for star formation history. Our results suggest that second stars may be detectable via future surveys such as LSST or Euclid.",
        "watermark_text": "We present an assessment of the shift between first stars and second stars , which are created by gravitational decay of primordial liquid clouds with masses vary from 10 ^ 4 M _ sol to 10 ^ 6 M _ sol . We see that the formation rate of second stars is suppressed at redshifts z < 20 due to photoheating effects on the intergalactic medium ( IGM ) .The suppression ratio increases as redshift decreases because the IGM temperature rises more swiftly than its density . At lower redshifts , we find that the formation rates of both first and first stars increase dramatically when the universe becomes reionized .This phenomenon occurs because the ionizing photons created during reionization heat up the nearby neutral hydrogen atoms , thereby expanding their Jeans mass and suppressing fragmentation into bigger objects . Finally , we estimate the number densities of early and first stars using our model for star formation history .Our results propose that second stars would be detectable via upcoming polls such as LSST or Euclid .",
        "rewrite_text": "We provide an evaluation of the transition from first stars to second stars, which form from the gravitational collapse of primordial liquid clouds with masses ranging from \\(10^4 M_{\\text{sol}}\\) to \\(10^6 M_{\\text{sol}}\\). Our findings indicate that the formation rate of second stars is inhibited at redshifts \\(z < 20\\) due to photoheating impacts on the intergalactic medium (IGM). This suppression becomes more pronounced as redshift decreases, as the temperature of the IGM rises more rapidly than its density. At lower redshifts, we observe a significant increase in the formation rates of both first and second stars following the reionization of the universe. This effect occurs because the ionizing photons produced during reionization heat the surrounding neutral hydrogen, thereby raising their Jeans mass and limiting fragmentation into larger structures. Finally, we estimate the number densities of first and second stars using our proposed model for star formation history. Our results suggest that second stars could be detected through upcoming surveys like LSST or Euclid.",
        "ori-fast-z-score": 0.11704114719613057,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 0.3418817293789138
    },
    {
        "original_text": "The purpose of this article is to give an overview of the theory of weight structures on triangulated categories developed by A. Bondal and M. Kapranov.  We will explain how it can be applied to construct new cohomology theories for algebraic varieties over finite fields or number fields.   In particular we will discuss the construction of motivic cohomology using weight structures on derived categories of mixed Tate motives.    The main results are due to J. Ayoub, D. Gaitsgory, R. Hain, S. Katzarkov, V. Lafforgue, C. Soulé, B. Stienstra, and others. This article was written as part of the author s Ph.D thesis at Utrecht University under supervision of Prof. Dr. Wim van der Kallen. It has been published online by the author with permission of the supervisor. For more information about the content see the introduction below.",
        "watermark_text": "The purpose of this page is to give an overview of the notion of weight spaces on triangulated categories established by A . Bondal and M . Kapranov . We will explain how it can be applied to build modern cohomology theories for algebraic fields over arbitrary fields or number fields .In particular we will explore the formation of motivic cohomology involving weight forms on derived categories of mixed Tate motives . The main results are due to J . Ayoub , D . Gaitsgory , R . Hain , S . Katzarkov , V . Lafforgue , C . Soulé , B . Stienstra , and others .This page was written as part of the writer s Ph . D degree at Utrecht University under supervision of Prof . Dr . Wim van der Kallen . It has been publication online by the writer with authorization of the supervisor .For more information about the content read the introduction below .",
        "rewrite_text": "This page aims to provide an overview of the concept of weight spaces in triangulated categories, as established by A. Bondal and M. Kapranov. We will discuss its application in developing contemporary cohomology theories for algebraic fields over any field or number field. In particular, we will investigate the construction of motivic cohomology that involves weight forms in derived categories of mixed Tate motives. The key findings are attributed to J. Ayoub, D. Gaitsgory, R. Hain, S. Katzarkov, V. Lafforgue, C. Soulé, B. Stienstra, and others. This page was created as part of the author's Ph.D. program at Utrecht University, under the guidance of Prof. Dr. Wim van der Kallen. With the supervisor's permission, the author has published it online. For further details, please refer to the introduction below.",
        "ori-fast-z-score": -0.42008402520840293,
        "water-fast-z-score": 4.900980294098034,
        "rewrite-fast-z-score": -0.42857142857142855
    },
    {
        "original_text": "The author describes how he has used the concept of  errors in thinking  to help students understand and learn about science, particularly physics.  He explains that this approach is based on his own experience as an undergraduate student at Harvard University where he was taught by Professor Richard Feynman.   The author suggests that it may be useful for teachers to use similar approaches when teaching other subjects such as mathematics or chemistry. The author also discusses some of the difficulties associated with using this method. This article appeared originally in Physics Today (Vol. 51 No. 1 January 1997). It can be downloaded from: http://arxiv.org/abs/physics/9701001v1.pdf  In order to teach physics effectively, one must first have a clear understanding of what constitutes good thinking within the subject area. Unfortunately, many people who are not trained scientists do not think like physicists; they tend to make mistakes which often lead them astray. For example, if asked whether two objects will fall together towards earth s center of mass, most people would say no because they believe that gravity acts only between bodies that touch each other. However, according to Newtonian mechanics, gravitational attraction does act upon all matter regardless of its position relative to another body. If you were to ask someone else why two objects don t fall together, she might answer that they are connected by a string so their weight cancels out. Although this explanation seems reasonable, it fails to take into account the fact that the force exerted by the string is negligible compared to the forces acting upon both objects individually. As a result, her reasoning is flawed.",
        "watermark_text": "The author explains how he has used the notion of errors in thinking to assist children understand and learn about science , particularly science . He describes that this methodology is based on his own experience as an undergraduate student at Harvard University where he was taught by Professor Richard Feynman .The author argues that it could be beneficial for schools to use similar method when taught other subjects such as mathematics or chemistry . The author also examines some of the problems involved with utilizing this process .This page appeared originally in Physics Today ( Vol . 51 No .1 January 1997 ) . It can be downloaded from : www : / / arxiv . org / abs / physics / 9701001v1 . pdf In order to teach physics successfully , one must first have a clear awareness of what constitutes better thinking within the subject area .Unfortunately , many people who are not trained experts do not thought like physicists ; they tend to make mistakes which frequently lead them astray . For instance , if asking whether two bodies will drop apart towards earth s center of mass , most people may think no because they believe that gravity works only between bodies that reach each other .However , according to Newtonian physics , gravity tension does operate upon all matter regardless of its position relative to another bodies . If you were to ask someone else why two bodies don t fall together , she might respond that they are connected by a string so their weight cancels out .Although this explanation seems rational , it fails to take into consideration the fact that the force exerted by the string is negligible compared to the forces working upon both elements individually . As a result , her reasoning is flawed .",
        "rewrite_text": "The author discusses how he has utilized the concept of cognitive errors to help children comprehend and engage with science, particularly physics. He reveals that this approach stems from his own experiences as an undergraduate at Harvard University, where he studied under Professor Richard Feynman. The author suggests that adopting similar strategies in other subjects, such as mathematics or chemistry, could be advantageous for schools. He also explores some challenges associated with implementing this method. This content was initially published in Physics Today (Vol. 51 No. 1, January 1997) and can be accessed at www.arxiv.org/abs/physics/9701001v1.pdf. To effectively teach physics, it's essential to have a clear understanding of sound reasoning within the field. Unfortunately, individuals lacking expert training often do not think like physicists, leading to misconceptions. For example, when asked whether two objects would fall toward the Earth's center of mass, many might instinctively say no, believing that gravity only acts on objects that are in direct contact. However, according to Newtonian physics, gravitational forces act on all matter, irrespective of their relative positions. When posed the question of why two bodies don't fall together, one might argue that they're connected by a string, which cancels out their weight. Although this explanation appears logical, it neglects to consider that the force of the string is minimal compared to the individual forces acting on both objects, revealing a flaw in the reasoning.",
        "ori-fast-z-score": -0.1796053020267749,
        "water-fast-z-score": 8.017837257372731,
        "rewrite-fast-z-score": 0.5773502691896257
    },
    {
        "original_text": "We present the discovery and analysis of two double neutron stars (DNSs) with masses in excess of 2 M_sun, PSR J0737-3039A/B and PSR B1913+16. The former is an eclipsing system that has been observed to undergo orbital decay at a rate consistent with gravitational wave emission; it will merge within about 3 Myr. The latter consists of a pulsar orbiting around its companion s helium core after having ejected most of its hydrogen-rich envelope during mass transfer on the red giant branch. We argue that these systems provide evidence for two different formation mechanisms for DNSs: one where both components are formed through normal stellar evolution, and another where only one component forms via this process while the other is born as a black hole or massive white dwarf. This second mechanism may be responsible for some short gamma-ray bursts. DOI: 10.1103/PhysRevD.76.084011",
        "watermark_text": "We present the discovery and evaluation of two double neutron stars ( DNSs ) with masses in excess of 2 M _ sun , PSR J0737 - 3039A / B and PSR B1913 + 16 . The first is an eclipsing system that has been observed to undergo orbital decomposition at a rate compatible with gravity wave radiation ; it will merge within about 3 Myr .The last consists of a pulsar orbiting around its companion s helium core after having liberated most of its hydrogen - laden envelope during mass transfer on the red dwarf branch . We argue that these systems represent proof for two different formation methods for DNSs : one where both components are created through regular stars evolution , and another where only one component forms via this process while the other is born as a black hole or massive brown giant .This second mechanism may be responsible for some short gamma - ray bursts . DOI : 10 . 1103 / PhysRevD . 76 . 084011",
        "rewrite_text": "We report the discovery and analysis of two double neutron stars (DNSs) with masses exceeding 2 M_sun: PSR J0737-3039A/B and PSR B1913+16. The first system is an eclipsing pair that has been observed to experience orbital decay at a rate consistent with gravitational wave emission, predicting a merger in approximately 3 million years. The second consists of a pulsar orbiting the helium core of its companion, which has shed most of its hydrogen-rich envelope during mass transfer on the red dwarf branch. We propose that these systems provide evidence for two distinct formation pathways for DNSs: one in which both components evolve from typical stellar processes, and another in which one component arises from stellar evolution while the other forms as a black hole or a massive brown dwarf. This latter mechanism may account for certain short gamma-ray bursts. DOI: 10.1103/PhysRevD.76.084011",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 5.165514464459439,
        "rewrite-fast-z-score": -0.48507125007266594
    },
    {
        "original_text": "We propose that the Standard Model gauge fields are pseudo-goldstone bosons associated with spontaneous breaking of local non-abelian symmetry, and show how this can be implemented in an effective field theory framework. We also discuss possible experimental signatures for such a scenario at future colliders. The Standard Model (SM) is one of the most successful theories ever developed to describe physics phenomena observed over many orders of magnitude ranging from subatomic particles to cosmological scales. However, there remain several open questions about its fundamental structure which motivate searches beyond SM predictions. In particular, it has been known since the early days of quantum mechanics that the SM Lagrangian does not respect all Poincaré symmetries  1  . This fact leads to violations of Lorentz invariance predicted by the SM but never seen experimentally  2  .\nIn recent years, interest in models where some or all of these symmetries are broken spontaneously has increased  3  , motivated primarily by the possibility of explaining small neutrino masses via seesaw mechanisms  4  . These models typically involve new heavy fermions whose interactions break explicitly the global symmetry responsible for protecting the massless nature of the SM vector bosons  5  . As a result, the latter acquire tiny masses through radiative corrections  6  while still preserving their longitudinal polarization states  7, 8  .",
        "watermark_text": "We suggest that the Standard Model gauge fields are pseudo - goldstone bosons involved with spontaneous breaking of local non - abelian symmetry , and suggest how this can be applied in an efficient field model formulation . We also discuss possible experimental signatures for such a situation at possible colliders .The Standard Model ( SM ) is one of the most popular theories ever proposed to explain physics phenomena observed over numerous orders of magnitude ranging from subatomic particles to cosmological scales . However , there remain various open questions about its essential structure which motivate searches beyond SM predictions .In particular , it has been known since the early days of quantum mechanics that the SM Lagrangian does not respect all Poincaré symmetries 1 . This fact leads to violations of Lorentz invariance predicted by the SM but never shown experimentally 2 .In recent years , activity in theories where some or all of these symmetries are broken spontaneously has increased 3 , driven mainly by the prospect of describing small neutrino masses via seesaw techniques 4 . These models usually include new massive fermions whose interactions break explicitly the global symmetry responsible for guarding the massless existence of the SM vector bosons 5 .As a result , the latter acquire small masses through radiative corrections 6 while nevertheless preserving their longitudinal polarization states 7 , 8 .",
        "rewrite_text": "We propose that the gauge fields of the Standard Model (SM) can be understood as pseudo-Goldstone bosons resulting from the spontaneous breaking of local non-abelian symmetry. We also demonstrate how this perspective can be effectively integrated into a field model formulation and examine potential experimental signatures of such a scenario in upcoming collider experiments. The Standard Model is widely regarded as one of the most successful theories in explaining a vast array of physical phenomena, spanning from subatomic scales to cosmological immensities. Nevertheless, numerous fundamental questions about its core structure persist, prompting investigations into physics beyond the Standard Model predictions. It has long been recognized that the SM Lagrangian does not fully respect all Poincaré symmetries, leading to predictions of Lorentz invariance violations that have yet to be confirmed experimentally. In recent years, there has been a surge of interest in theories where some or all of these symmetries are spontaneously broken, largely due to the need to explain the small masses of neutrinos through seesaw mechanisms. These models often introduce new massive fermions, which explicitly disrupt the global symmetry that protects the massless nature of the SM vector bosons, resulting in small masses for these bosons via radiative corrections while still maintaining their longitudinal polarization states.",
        "ori-fast-z-score": -0.41256849850351734,
        "water-fast-z-score": 5.081063304179084,
        "rewrite-fast-z-score": 0.6469966392206304
    },
    {
        "original_text": "We present the results of our study on the evolution of interstellar dust (ISD) in spiral and irregular galaxies with various morphologies, based on multiwavelength observations obtained by Spitzer Space Telescope (SST), Herschel Space Observatory (HSO), Chandra X-ray Observatory (CXO), Hubble Space Telescope (HST). We have used SED fitting to derive physical parameters such as stellar mass, star formation rate (SFR), age, extinction etc., for all sample galaxies using photometric data available at UV-optical-NIR wavelengths. The derived properties are then compared between two galaxy samples -one consisting of spirals and another one containing irregulars. Our main findings are:  1. Irregular galaxies show higher values of total infrared luminosity than their spiral counterparts.  2. Infrared excess emission is found to be more prominent in irregular galaxies when compared to that observed in spiral galaxies.  3. Dust temperature distribution shows significant differences among irregular and spiral galaxies.",
        "watermark_text": "We present the conclusion of our research on the evolution of interstellar dust ( ISD ) in spiral and irregular stars with various morphologies , using on multiwavelength surveys obtained by Spitzer Space Telescope ( SST ) , Herschel Space Observatory ( HSO ) , Chandra X - ray Observatory ( CXO ) , Hubble Space Telescope ( HST ) . We have utilized SED fitting to derive physical factors such as stellar mass , sun formation rate ( SFR ) , age , extinction etc . , for all sample objects utilizing photometric data available at UV - optical - NIR wavelengths .The derived properties are then analyzed between two galaxy samples - one consisting of spirals and another one containing irregulars . Our main results are : 1 .Irregular galaxies show higher values of gross infrared luminosity than their spiral rivals . 2 .Infrared excess emission is found to be more prominent in irregular galaxies when compared to that detected in spiral galaxies . 3 .Dust heating distribution shows significant variations among irregular and spiral galaxies .",
        "rewrite_text": "We conclude our research on the evolution of interstellar dust (ISD) in spiral and irregular galaxies with different morphologies, based on multiwavelength surveys conducted by the Spitzer Space Telescope (SST), Herschel Space Observatory (HSO), Chandra X-ray Observatory (CXO), and Hubble Space Telescope (HST). Through SED fitting, we have derived key physical parameters such as stellar mass, star formation rate (SFR), age, and extinction for all sample objects, leveraging photometric data across UV, optical, and near-infrared (NIR) wavelengths. The derived properties were then compared between two galaxy samples: one of spiral galaxies and the other of irregular galaxies. Our main findings are as follows: 1. Irregular galaxies exhibit higher overall infrared luminosity compared to spiral galaxies. 2. The presence of infrared excess emission is more pronounced in irregular galaxies than in their spiral counterparts. 3. The distribution of dust heating demonstrates significant differences between irregular and spiral galaxies.",
        "ori-fast-z-score": 0.12216944435630522,
        "water-fast-z-score": 4.275930552470682,
        "rewrite-fast-z-score": 0.36650833306891567
    },
    {
        "original_text": "We present the results of dust formation calculations for supernova ejecta using a one-dimensional radiation hydrodynamics code coupled with an equilibrium chemistry network, including grain growth by accretion onto pre-existing grains as well as coagulation between grains. We find that dust can form efficiently at temperatures below 1000 K if there is sufficient carbon available to make amorphous carbon grains. The amount of dust formed depends on the initial composition of the gas; we show that it increases dramatically when oxygen-rich material is mixed into the ejecta. For typical conditions found in Type II SNe, we predict that 0.1 M⊙ of dust forms within 10 years after explosion. This dust survives destruction due to sputtering by ions accelerated during the reverse shock passage through the ejecta. Our results suggest that dust may be produced in significant amounts in some core-collapse supernovae. \n \n Keywords: Dust, Supernova",
        "watermark_text": "We present the conclusion of dust structure calculations for supernova ejecta using a one - dimensional radiation hydrodynamics code combined with an equilibrium chemistry system , covering grain growth by accretion onto pre - emerging grains as well as coagulation between particles . We see that dust can form smoothly at conditions below 1000 K if there is adequate carbon available to make amorphous hydrogen grains .The amount of dust formed depends on the early structure of the gas ; we find that it rises dramatically when oxygen - laden matter is mixed into the ejecta . For common temperatures found in Type II SNe , we expect that 0 . 1 [UNK] of dust occurs within 10 years after explosion .This powder escaped destruction due to sputtering by ions induced during the reverse shock passage through the ejecta . Our results show that matter may be formed in considerable amounts in some core - collapse supernovae .Keywords: Dust, Supernova",
        "rewrite_text": "We present the results of our calculations on dust structures formed from supernova ejecta, utilizing a one-dimensional radiation hydrodynamics code combined with an equilibrium chemistry framework. Our study covers both grain growth through accretion onto nascent grains and the coagulation of particles. We find that dust can form effectively at temperatures below 1000 K, provided there is sufficient carbon available to create amorphous hydrogen grains. The amount of dust produced is influenced by the initial structure of the gas; notably, we observe a significant increase in dust formation when oxygen-rich material is incorporated into the ejecta. For typical temperatures encountered in Type II supernovae, we predict the formation of approximately 0.1 [UNK] of dust within 10 years following the explosion. This dust survives the destructive effects of sputtering by ions generated during the reverse shock passage through the ejecta. Our findings indicate that substantial amounts of matter may form in certain core-collapse supernovae.   \nKeywords: Dust, Supernova",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 6.671345390179443,
        "rewrite-fast-z-score": 1.462614271203831
    },
    {
        "original_text": "We report scanning magnetoresistance microscopy (SMRM) measurements on an atom chip with gold wires and microtraps fabricated by focused ion beam milling. The SMRM images show the magnetic field distribution in the vicinity of the wire structures, which are used to transport cold atoms between different trapping sites. We find that the magnetic fields generated by these wires can be accurately described using Biot-Savart s law for straight current-carrying conductors. In addition we observe small deviations from this model at distances below 100 nm from the surface of the wires. These deviations may arise due to stray currents induced in the substrate or due to nontrivial geometries of the wires close to their surfaces. Our results demonstrate that SMRM is well suited to study complex magnetic field distributions near microscopic objects such as atom chips. Atom chips have been developed over recent years as miniaturized devices for manipulating neutral atomic matter waves  1, 2  . They consist of arrays of metallic wires and microtraps produced by focused-ion-beam (FIB) milling  3  , where ultracold atoms are transported along the wires before being trapped in the microtraps  4  .\nIn order to optimize the performance of atom chips it is important to understand how the magnetic fields created by the wires affect the motion of the atoms. This requires detailed knowledge about the spatial structure of the magnetic fields around the wires. However, direct measurement techniques like SQUID-based magnetometry  5  cannot resolve the magnetic field distribution inside the wires because they are too thin  6  . Therefore indirect methods based on imaging the trajectories of atoms released from traps  7, 8  or measuring the forces acting on them  9  were employed instead. Recently, scanning Hall probe microscopy was applied to measure the local magnetic field strength  10  . Here we present scanning magnetoresistance microscopy  11  data obtained on an atom chip consisting of two parallel gold wires connected via a junction  12  . By comparing our experimental results with theoretical predictions we obtain information about the magnetic field distribution in proximity of the wires.",
        "watermark_text": "We report scanning magnetoresistance microscopy ( SMRM ) observations on an atom chip with gold wires and microtraps fabricated by concentrated ion beam milling . The SMRM pictures show the magnetic field spread in the vicinity of the wire structures , which are applied to transport cold molecules between various trap places .We see that the magnetic waves generated by these cables can be correctly explained following Biot - Savart s law for straight current - transporting conductors . In addition we study small deviations from this description at distances below 100 nm from the surface of the wires .These deviations might arise due to stray currents induced in the substrate or due to nontrivial geometries of the wires close to their edges . Our results show that SMRM is well suited to study difficult magnetic field distributions near microscopic structures such as atom devices .Atom devices have been created over recent years as miniaturized devices for manipulating neutral atomic matter waves 1 , 2 . They comprise of arrays of metallic wires and microtraps produced by concentrated - ion - laser ( FIB ) processing 3 , where ultracold atoms are transported along the wires before being trapped in the microtraps 4 .In order to optimize the performance of atom devices it is important to realize how the magnetic waves created by the wires affect the movement of the atoms . This requires complete understanding about the spatial shape of the magnetic waves around the wires .However , direct detection methods like SQUID - based magnetometry 5 cannot determine the magnetic field spread inside the wires because they are too thin 6 . Therefore indirect approaches derived on observing the trajectories of atoms released from nets 7 , 8 or tracking the forces working on them 9 were utilized instead .Recently , scanning Hall probe microscopy was used to measure the local magnetic force force 10 . Here we present scan magnetoresistance microscopy 11 data derived on an molecular computer comprised of two connected gold wires coupled via a junction 12 .By matching our experimental results with theoretical predictions we obtain knowledge about the magnetic force density in proximity of the wires .",
        "rewrite_text": "We present observations from scanning magnetoresistance microscopy (SMRM) on an atom chip featuring gold wires and microtraps created through concentrated ion beam milling. The SMRM images illustrate the magnetic field distribution around the wire structures, which are employed to transport cold molecules between various trapping locations. Our findings indicate that the magnetic fields produced by these wires can be accurately described by Biot-Savart's law for straight current-carrying conductors. Furthermore, we investigate slight deviations from this model at distances less than 100 nm from the wire surfaces. These deviations may stem from stray currents induced in the substrate or from complex geometries near the edges of the wires. Our results demonstrate that SMRM is highly effective for examining intricate magnetic field distributions near microscopic structures, such as atom devices. Over the past few years, atom devices have emerged as compact instruments for manipulating neutral atomic matter waves. They consist of arrays of metallic wires and microtraps fabricated through focused ion beam (FIB) processing, allowing for the transport of ultracold atoms along the wires before their confinement within microtraps. To enhance the efficacy of atom devices, it is crucial to understand the influence of the magnetic fields generated by the wires on atomic movement, necessitating a comprehensive understanding of the spatial characteristics of these magnetic fields. However, direct measurement techniques such as SQUID-based magnetometry are insufficient for capturing the magnetic field inside the thin wires. As a result, we have utilized indirect methods, including the observation of atom trajectories released from nets or tracking the forces acting on them. Recently, scanning Hall probe microscopy has been employed for local magnetic force measurements. In this study, we present SMRM data obtained from a molecular device composed of two interconnected gold wires joined at a junction. By correlating our experimental findings with theoretical models, we gain insights into the magnetic force density near the wires.",
        "ori-fast-z-score": -0.6923076923076923,
        "water-fast-z-score": 8.590007875090548,
        "rewrite-fast-z-score": 1.4055638569974547
    },
    {
        "original_text": "We present an efficient algorithm for computing the sign function of a large sparse complex matrix, which is based on the Lanczos bidiagonalization process with partial reorthogonalization. The proposed algorithm can be applied to any Hermitian or non-Hermitian matrices without restriction. We apply this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density. In particular we show that our algorithm works well even when the quark mass becomes small compared to the inverse of the lattice spacing. This work was supported by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan. PACS numbers: 11.15.Ha, 12.38.Qk, 12.39.Fe, 14.20 .Dh  1 Introduction Lattice Quantum Chromodynamics(LQCD), as one of the most promising candidates for describing strong interactions among quarks and gluons, has been widely used to study hadronic properties such as masses and decay constants  1  . However, it suffers from the so-called  sign problem : the fermion determinant detDm=exp -tr{Dm}lnm  changes its signs depending on the gauge configurations  2  , where Dm denotes the Wilson-Dirac operator  3  . Therefore, Monte Carlo methods cannot be directly employed to calculate physical quantities using LQCD because they require positive definite weight functions  4  .\nIn order to overcome this difficulty, several approaches have been developed so far  5  -  8  . Among them, the Taylor expansion approach  9  -  11  seems to be very powerful since it allows us to evaluate the expectation value of any observables accurately within statistical errors. It also enables us to perform calculations at high temperature and/or high density  12  -  14  . For example, the Taylor expansion up to O(a6) has already been performed successfully  15  .",
        "watermark_text": "We present an efficient algorithm for calculation the sign function of a large sparse complex matrix , which is based on the Lanczos bidiagonalization process with partial reorthogonalization . The proposed algorithm can be applied to any Hermitian or non - Hermitian matrices without limitation .We use this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density . In particular we prove that our algorithm works well even when the quark mass becomes tiny relative to the inverse of the lattice spacing .This project was supported by Grants - in - Aid for Scientific Research ( No . 20340040 ) from MEXT Japan .PACS scores : 11 . 15 . Ha , 12 . 38 . Qk , 12 . 39 . Fe , 14 . 20 . Dh 1 Introduction Lattice Quantum Chromodynamics ( LQCD ) , as one of the most attractive candidates for describing strong coupling among quarks and gluons , has been widely using to study hadronic properties such as masses and decay constants 1 . However , it suffers from the so - called sign problem : the fermion determinant detDm = exp - tr { Dm } lnm varies its signs depending on the gauge modes 2 , where Dm denotes the Wilson - Dirac operator 3 .Therefore , Monte Carlo methods never be directly used to estimate mechanical numbers using LQCD because they use positive definite weight functions 4 . In order to overcome this obstacle , various approaches have been proposed so far 5 - 8 .Among them , the Taylor expansion method 9 - 11 seems to be very potent since it allows us to analyze the expectation value of any observables correctly within statistical errors . It additionally lets us to conduct measurements at high heat and / or large velocity 12 - 14 .For instance , the Taylor expansion up to O ( a6 ) has already been performed successfully 15 .",
        "rewrite_text": "We introduce an efficient algorithm for computing the sign function of large sparse complex matrices, employing the Lanczos bidiagonalization process with partial reorthogonalization. This algorithm can be applied to any Hermitian or non-Hermitian matrix without restrictions. We utilize this innovative algorithm for the overlap Dirac operator in lattice QCD simulations at finite density, demonstrating its effectiveness even when the quark mass is significantly small compared to the inverse of the lattice spacing. This research was funded by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan. PACS numbers: 11.15.Ha, 12.38.Qk, 12.39.Fe, 14.20.Dh. \n\n1. Introduction: Lattice Quantum Chromodynamics (LQCD), a prominent framework for understanding the strong interactions between quarks and gluons, has been extensively utilized to examine hadronic properties such as masses and decay constants. However, it faces the so-called sign problem: the fermion determinant \\( \\det D_m = \\exp(-\\text{tr} \\{D_m\\} \\ln m) \\) exhibits sign fluctuations based on the gauge configurations, where \\( D_m \\) refers to the Wilson-Dirac operator. Consequently, Monte Carlo methods cannot be directly applied to estimate physical quantities using LQCD, as they rely on positive definite weight functions. To address this issue, various strategies have been proposed. Among these, the Taylor expansion method appears particularly effective, enabling accurate analysis of expectation values for observables within statistical uncertainties, and facilitating measurements at high temperatures and/or considerable velocities. For example, a successful Taylor expansion up to \\( O(a^6) \\) has already been conducted.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.4835456068700275,
        "rewrite-fast-z-score": 2.2662573397778742
    },
    {
        "original_text": "We present the first comprehensive study of visible spectroscopy for all known Jupiter trojans (JTs). We obtained high-resolution spectra with Subaru/HDS, Gemini/GNIRS, and Keck/NIRC-2 in order to determine their surface compositions. The sample includes 49 JTs including two new discoveries by our team. Our analysis shows that most JTs are S-complex asteroids or C-type asteroids. However, we found four objects which show unusual spectral features; one is an E-type asteroid, while three others have featureless red-sloped continuum. These objects may be primitive bodies such as carbonaceous chondrites. In addition, we find that some JTs belong to dynamically cold families. This suggests that these families were formed recently through catastrophic collisions among parent bodies. Finally, we discuss possible formation scenarios for JTs based on our findings. Keywords: Asteroids",
        "watermark_text": "We present the first comprehensive investigation of visible spectroscopy for all known Jupiter trojans ( JTs ) . We obtained high - resolution spectra with Subaru / HDS , Gemini / GNIRS , and Keck / NIRC - 2 in order to obtain their surface compositions .The sample comprises 49 JTs including two latest discoveries by our team . Our study shows that most JTs are S - complex asteroids or C - class asteroids .However , we identified four bodies which show peculiar spectral features ; one is an E - class asteroid , while three rest have featureless red - sloped continuum . These bodies may be ancient bodies such as carbonaceous chondrites .In addition , we find that some JTs belong to dynamically cold families . This implies that these families were created recently through catastrophic collisions among parent bodies .Finally , we explain possible formation scenarios for JTs based on our findings . Keywords : Asteroids",
        "rewrite_text": "We present the first extensive investigation of visible spectroscopy across all known Jupiter trojans (JTs). Utilizing high-resolution spectra obtained from Subaru/HDS, Gemini/GNIRS, and Keck/NIRC-2, we analyzed their surface compositions. Our sample includes 49 JTs, incorporating two recent discoveries by our team. This study reveals that the majority of JTs are classified as S-complex or C-class asteroids. However, we identified four atypical bodies; one is an E-class asteroid, while the other three exhibit a featureless red-sloped continuum, suggesting they may be ancient bodies similar to carbonaceous chondrites. Additionally, we discovered that some JTs are members of dynamically cold families, indicating these families likely formed recently from catastrophic collisions among their parent bodies. Finally, we discuss potential formation scenarios for JTs based on our results. Keywords: Asteroids.",
        "ori-fast-z-score": 0.13483997249264842,
        "water-fast-z-score": 4.900769721140662,
        "rewrite-fast-z-score": 1.632993161855452
    },
    {
        "original_text": "We report on the detection of X-ray flares in low mass stars (0.5-0.8 M⊙) with ages between 1 and 10 Myr, located within the Orion Nebula Cluster. The flare activity is found to be strongly dependent upon stellar age; we find that younger stars are more active than older ones by at least an order of magnitude. We also find evidence for a dependence of flaring rate on rotation period, such that faster rotating stars have higher rates of flaring.  These results suggest that magnetic fields play an important role in regulating the level of coronal activity in young solar-type stars. This work was supported under NASA contract NAS8-39073. We thank J. Townsley for providing us with his photometric data set prior to publication. We acknowledge useful discussions with A. Feigelson, D. Getman, C. Hillenbrand, R. Herbst, S. Preibisch, B. Reipurth, T. Stassun, E. Wolff, and W. Zuckermann.",
        "watermark_text": "We report on the detection of X - ray flares in low weight stars ( 0 . 5 - 0 . 8 [UNK] ) with ages between 1 and 10 Myr , located within the Orion Nebula Cluster . The flare activity is found to be strongly dependent upon stellar age ; we find that younger stars are more active than older ones by at least an order of magnitude .We additionally find proof for a dependence of flaring speed on rotation history , such that faster rotating stars have greater rates of flaring . These data suggest that magnetic waves play an important role in controlling the degree of coronal action in young solar - class stars .This project was supported under NASA contract NAS8 - 39073 . We praise J . Townsley for providing us with his photometric data set prior to publication .We acknowledge useful talks with A . Feigelson , D . Getman , C . Hillenbrand , R . Herbst , S . Preibisch , B . Reipurth , T . Stassun , E . Wolff , and W . Zuckermann .",
        "rewrite_text": "We present findings on the detection of X-ray flares in low-mass stars (0.5 - 0.8 solar masses) aged between 1 and 10 million years, situated in the Orion Nebula Cluster. Our results indicate that flare activity is highly influenced by stellar age, with younger stars exhibiting flare rates at least an order of magnitude higher than their older counterparts. Furthermore, we observe a correlation between flaring frequency and stellar rotation, revealing that stars with faster rotation speeds tend to exhibit more frequent flaring. This data implies that magnetic waves significantly influence the degree of coronal activity in young solar-type stars. This research was conducted under NASA contract NAS8-39073. We express our gratitude to J. Townsley for sharing his photometric data with us prior to its publication. Additionally, we appreciate insightful discussions with A. Feigelson, D. Getman, C. Hillenbrand, R. Herbst, S. Preibisch, B. Reipurth, T. Stassun, E. Wolff, and W. Zuckermann.",
        "ori-fast-z-score": -0.3779644730092272,
        "water-fast-z-score": 3.6536565724225296,
        "rewrite-fast-z-score": -1.099524999206747
    },
    {
        "original_text": "We consider the asymptotic behavior of the sample autocovariance function and spectral density functions of stationary processes with regularly varying marginal distributions, which are driven by an infinite order moving average process whose coefficients have regularly varying tails. We show that these quantities can be approximated by those of a finite order autoregressive-moving average (ARMA) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters. The results obtained here extend previous ones given in the literature to more general cases where the driving noise is not necessarily Gaussian or has non-Gaussian components. In particular, we provide new proofs for the existing results when the driving noise is purely Gaussian. Our approach relies heavily on recent developments made in the theory of regular variation and stochastic calculus. As applications, we study two examples of duration-driven LRD models, namely the Cox-Ingersoll-Ross (CIR) model and the Vasicek model.",
        "watermark_text": "We consider the asymptotic activity of the sample autocovariance map and spectral density functions of stationary systems with regularly varying marginal distributions , which are driven by an endless order move average process whose coefficients have regularly varying tails . We see that these quantities can be approximated by those of a finite order autoregressive - moving average ( ARMA ) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters .The results derived here extend previous ones given in the writings to more general instances where the driving signal is not necessarily Gaussian or has non - Gaussian components . In particular , we provide fresh proofs for the new results when the driving signal is purely Gaussian .Our perspective rely heavily on current developments developed in the theoretical of regular variation and stochastic equations . As applications , we study two examples of duration - based LRD models , namely the Cox - Ingersoll - Ross ( CIR ) model and the Vasicek theory .",
        "rewrite_text": "We examine the asymptotic behavior of the sample autocovariance map and spectral density functions for stationary systems with regularly varying marginal distributions, which are influenced by an infinite-order moving average process with regularly varying tail coefficients. We find that these quantities can be approximated by those of a finite-order autoregressive-moving average (ARMA) model in terms of weak convergence, provided certain conditions regarding the tail behaviors of the ARMA parameters are met. The results presented here extend previous findings in the literature to include more general cases where the driving signal may not be Gaussian or may contain non-Gaussian components. Notably, we offer new proofs for our results when the driving signal is purely Gaussian. Our approach is significantly informed by recent advancements in the theory of regular variation and stochastic equations. As practical applications, we investigate two examples of duration-based long-range dependence (LRD) models: the Cox-Ingersoll-Ross (CIR) model and the Vasicek theory.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": 2.0855209398041166
    },
    {
        "original_text": "We present new distances for eight open clusters in the Large Magellanic Cloud (LMC) using near-infrared photometry and isochrone fitting techniques. The red clump giants are used as standard candles, with their absolute magnitudes determined by comparison with Galactic globular cluster data. We find that our results agree well with previous determinations based on other methods but have smaller uncertainties.  These new distance measurements will be useful for calibrating future surveys such as Gaia and LSST. They also provide an independent check on the accuracy of current estimates of the Hubble constant derived from Type Ia supernovae observations. This work was supported by NASA grant NNX10AD65G. The authors would like to thank Dr. J. Alves de Oliveira Jr., who provided us with his code for determining ages and metallicities of open clusters. We acknowledge support from NSF grants AST-0907892 and AST-1109897.",
        "watermark_text": "We create additional distances for eight open complexes in the Large Magellanic Cloud ( LMC ) using near - infrared photometry and isochrone fit techniques . The red clump giants are using as conventional candles , with their absolute magnitudes determined by analysis with Galactic globular cluster data .We see that our findings agree well with previous determinations based on other methods but have smaller uncertainties . These new distance measurements will be valuable for calibrating future surveys such as Gaia and LSST .They also perform an independent check on the accuracy of recent estimates of the Hubble constant calculated from Type Ia supernovae observations . This research was supported by NASA award NNX10AD65G .The authors would like to thank Dr . J . Alves de Oliveira Jr . , who provided us with his code for determining ages and metallicities of open clusters . We acknowledge support from NSF grants AST - 0907892 and AST - 1109897 .",
        "rewrite_text": "We determined additional distances for eight open clusters in the Large Magellanic Cloud (LMC) by employing near-infrared photometry and isochrone fitting techniques. We utilized red clump giants as standard candles, with their absolute magnitudes derived from analyses of Galactic globular cluster data. Our results align closely with previous measurements obtained through various methods, but with reduced uncertainties. These new distance estimates will be instrumental in calibrating upcoming surveys like Gaia and LSST. Moreover, they offer an independent verification of recent calculations of the Hubble constant based on observations of Type Ia supernovae. This research was funded by a NASA award (NNX10AD65G) and we express our gratitude to Dr. J. Alves de Oliveira Jr. for providing the code we used to determine the ages and metallicities of open clusters. We also acknowledge support from NSF grants AST-0907892 and AST-1109897.",
        "ori-fast-z-score": 1.4320780207890627,
        "water-fast-z-score": 5.598123172175427,
        "rewrite-fast-z-score": 0.8017837257372732
    },
    {
        "original_text": "We present new near-infrared polarimetric observations of the equatorial ring surrounding the evolved star HD 163296 (HR 5171 A). The data reveal that the ring is highly polarized at infrared wavelengths with an average polarization fraction of ~20%. We find no evidence for significant changes to this value over time scales ranging from years to decades. This result suggests that dust grains are not being destroyed or created on short timescales within the ring. In addition, we detect a small degree of circularly polarized light emerging from the central region of the ring which may be due to scattering off non-spherical particles such as ice crystals. \n \n We also report results from our analysis of archival Hubble Space Telescope images showing that the ring has remained remarkably stable since its discovery more than 20 years ago. Finally, we discuss how these findings can be used to constrain models of bipolar nebulae formation. Keywords: Polarization; Circumstellar matter",
        "watermark_text": "We report new near - infrared polarimetric studies of the equatorial ring surrounding the evolved star HD 163296 ( HR 5171 A ) . The data reveal that the circle is strongly polarized at infrared wavelengths with an estimated polarization fraction of ~ 20 % .We see no evidence for significant variations to this value over time ranges ranging from years to decades . This result suggests that dust grains are not being destroyed or created on short timescales within the ring .In addition , we perceive a small level of circularly polarized light arriving from the main region of the circle which may be due to scattering off non - cylindrical ions such as ice particles . We additionally report findings from our analysis of archival Hubble Space Telescope images indicating that the circle has remained remarkably steady since its observation more than 20 decades ago .Finally , we explain how these results can be used to constrain models of bipolar nebulae formation . Keywords : Polarization ; Circumstellar matter",
        "rewrite_text": "We present new near-infrared polarimetric observations of the equatorial ring surrounding the evolved star HD 163296 (HR 5171 A). Our data indicate that the ring is significantly polarized at infrared wavelengths, with an estimated polarization fraction of approximately 20%. We observe no substantial variations in this value over time spans of years to decades, suggesting that dust grains within the ring are neither being created nor destroyed on short timescales. Additionally, we detect a slight amount of circularly polarized light from the main region of the ring, possibly arising from scattering off non-cylindrical ions, such as ice particles. Our analysis of archival Hubble Space Telescope images further shows that the ring has remained remarkably stable since it was first observed over 200 years ago. Finally, we discuss how these findings can be utilized to refine models of bipolar nebula formation. Keywords: Polarization; Circumstellar matter.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.3804502135457675,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "We present deep radio observations at 1.4 GHz with the VLA towards two TeV gamma-ray emitting regions, one associated to the open cluster Cyg OB2 #8 (HESS J1640-465) and another located near the massive star WR 25 (HESS J1641-463). The new data reveal extended emission around both TeV sources which is not detected by previous surveys. We discuss possible scenarios for this emission based on our results as well as those obtained recently by other authors. In particular we propose that the observed structures are due to synchrotron radiation produced by relativistic electrons accelerated in shocks driven by stellar winds interacting within these clusters. This scenario would also explain why no X-ray counterparts have been found so far despite deep searches carried out with Chandra and XMM-Newton telescopes. Finally, we estimate the magnetic field strength required to produce such emission using standard models for particle acceleration in colliding wind binaries. \nIntroduction\n\nThe Cygnus OB2 association contains more than 100 OB stars distributed over an area of about 50 square degrees centered at l = 80°and b = 1° (  Fig.   1a ). It has been suggested that many of them could be members of binary systems or even multiple systems (e.g., Knödlseder 2000; Wright et al. 2010) . These objects can drive powerful winds into their surroundings creating strong shocks where particles may be accelerated up to very high energies. If some of these particles escape from the shock fronts they will interact with photons coming from the surrounding interstellar medium producing high-energy electromagnetic radiation detectable across most of the electromagnetic spectrum including the TeV range. \n \n Several studies suggest that several of the known TeV sources in the sky might be related to young open clusters like Cyg OB2 (see e.g., Aharonian et al. 2005a ,b, 2007a . However, only few of these associations have been confirmed through multi-wavelength campaigns involving optical/infrared imaging, spectroscopy and/or radio continuum observations (see e.g. , Reimer & Böttcher 2006 , Castro-Tirado et al",
        "watermark_text": "We report deep radio observations at 1 . 4 GHz with the VLA towards two TeV gamma - ray emitting regions , one related to the open object Cyg OB2 # 8 ( HESS J1640 - 465 ) and another situated near the giant star WR 25 ( HESS J1641 - 463 ) . The revised data reveal extended emitted around both TeV sources which is not observed by earlier surveys .We discuss possible strategies for this emission based on our findings as also as those acquired previously by other researchers . In particular we propose that the seen features are due to synchrotron emission created by relativistic electrons accelerated in shocks driven by stellar winds interacting within these clusters .This scenario would also explain why no X - ray equivalent have been detected so far despite massive investigations carried out with Chandra and XMM - Newton telescopes . Finally , we estimate the magnetic force size needed to produce such emission utilizing typical models for electron acceleration in colliding weather binaries .Introduction The Cygnus OB2 association contains more than 100 OB stars distributed over an area of about 50 square degrees centered at l = 80°and b = 1° ( Fig . 1a ) .It has been proposed that several of them could be members of binary systems or even multiple systems ( e . g . , Knödlseder 2000 ; Wright et al . 2010 ) .These particles can bring powerful storms into their environment forming violent shocks where ions may be advanced up to very high energies . If some of these ions survive from the shock fronts they will interact with photons coming from the nearby interstellar medium generating high - energy electromagnetic radiation detectable across most of the electromagnetic spectrum including the TeV range .Several studies propose that several of the known TeV sources in the heavens would be connected to early open complexes like Cyg OB2 ( see e . g . , Aharonian et al . 2005a , b , 2007a .However , only few of these associations have been confirmed through multi - wavelength campaigns involving optical / microwave imaging , spectroscopy and / or radio continuum observations ( saw e . g . , Reimer & Böttcher 2006 , Castro - Tirado et al",
        "rewrite_text": "We present in-depth radio observations conducted at 1.4 GHz using the VLA, focusing on two TeV gamma-ray emitting regions: one associated with the open cluster Cyg OB2 #8 (HESS J1640-465) and the other located near the giant star WR 25 (HESS J1641-463). The updated data reveal an extended emission around both TeV sources that was not detected in previous surveys. We explore potential explanations for this emission, drawing on our findings as well as earlier research by others. Specifically, we suggest that the observed features may result from synchrotron emission generated by relativistic electrons accelerated in shocks created by interacting stellar winds within these clusters. This scenario could also account for the absence of X-ray counterparts, despite extensive investigations with the Chandra and XMM-Newton telescopes. Additionally, we estimate the necessary magnetic field strength to produce such emissions, employing standard models for electron acceleration in colliding wind binaries.\n\nIntroduction: The Cygnus OB2 association comprises over 100 OB stars distributed across an area of approximately 50 square degrees, centered at l = 80° and b = 1° (see Fig. 1a). It has been suggested that many of these stars may be part of binary or even multiple systems (e.g., Knödlseder 2000; Wright et al. 2010). These stars can generate powerful winds, leading to violent shocks where ions can be accelerated to very high energies. If some of these ions manage to escape the shock fronts, they can interact with photons from the surrounding interstellar medium, producing high-energy electromagnetic radiation detectable across a broad range of the spectrum, including the TeV domain. Several studies have proposed connections between known TeV sources and early stellar complexes like Cyg OB2 (e.g., Aharonian et al. 2005a, b, 2007a). However, only a limited number of these associations have been confirmed through multi-wavelength campaigns that include optical/microwave imaging, spectroscopy, and/or radio continuum observations (see, for example, Reimer & Böttcher 2006; Castro-Tirado et al.).",
        "ori-fast-z-score": -0.8892972917998876,
        "water-fast-z-score": 7.185169912163989,
        "rewrite-fast-z-score": 1.135549947915338
    },
    {
        "original_text": "The Compact Muon Solenoid (CMS) experiment at the Large Hadron Collider has recently reported evidence for a new particle with mass around 125 GeV, consistent with Standard Model expectations for the Higgs boson.  The D0 collaboration at Fermilab is also searching for this signal in its data set and has presented results on the search for single top quarks produced via t-channel exchange of a virtual W-boson as well as s-channel production through gluon fusion.   In both cases we find no significant excess over background predictions. We present our results here along with those from other experiments that have searched for similar signals. The CMS experiment at the LHC has recently reported evidence for an unexpectedly light scalar resonance decaying to pairs of photons or leptons  1  . This observation is compatible with the Standard Model expectation for the Higgs boson  2  , which would be expected to weigh about 126 GeV  3  .\nIn addition to the standard model Higgs boson searches performed by ATLAS  4  and CMS  5  , there are many extensions of the SM  6  that predict additional scalars  7, 8  . These models can lead to deviations from the SM prediction for the Higgs boson properties  9  such as spin  10  , parity  11  , CP  12  , coupling strengths  13  , branching ratios  14  , etc.. Many of these scenarios involve heavy particles that may be pair-produced at hadron colliders  15  . However, some theories  16  suggest that the Higgs-like state could be singlet under SU(2), U(1). Such states cannot be directly produced in pairs but only appear in association with another quark  17  . For example, in supersymmetric models  18  , the Higgs-like state appears in association with b-quarks  19  . Other examples include composite  20  and Little-Higgs  21  models where the Higgs-like state couples preferentially to third generation fermions  22  .",
        "watermark_text": "The Compact Muon Solenoid ( CMS ) experiment at the Large Hadron Collider has recently published evidence for a new particle with mass around 125 GeV , compatible with Standard Model expectations for the Higgs boson . The D0 consortium at Fermilab is also searching for this signal in its data set and has presented data on the hunt for single bottom quarks produced via t - channel exchange of a virtual W - boson as well as s - channel production through gluon fusion .In both cases we find no considerable excess over background predictions . We present our findings here along with those from other experiments that have searched for related systems .The CMS experiment at the LHC has recently published evidence for an unexpectedly light scalar resonance decaying to pairs of photons or leptons 1 . This measurement is compatible with the Standard Model estimate for the Higgs boson 2 , which would be anticipated to weigh about 126 GeV 3 .In addition to the standard model Higgs boson searches performed by ATLAS 4 and CMS 5 , there are many extensions of the SM 6 that forecast additional scalars 7 , 8 . These models can lead to deviations from the SM estimate for the Higgs boson properties 9 such as spin 10 , parity 11 , CP 12 , coupling strengths 13 , branching factors 14 , etc . .Many of these scenarios involve heavy nuclei that might be pair - produced at hadron colliders 15 . However , some theories 16 suggest that the Higgs - like state possible be singlet under SU ( 2 ) , U ( 1 ) .Such states cannot be directly produced in couples but only appear in association with another quark 17 . For instance , in supersymmetric theories 18 , the Higgs - like state occurs in association with b - quarks 19 .Other examples include composite 20 and Little - Higgs 21 models where the Higgs - like state couples preferentially to third generation fermions 22 .",
        "rewrite_text": "The Compact Muon Solenoid (CMS) experiment at the Large Hadron Collider has recently reported evidence of a new particle with a mass of approximately 125 GeV, which aligns with the expectations of the Standard Model for the Higgs boson. Meanwhile, the D0 collaboration at Fermilab is also investigating this signal within its dataset. They have shared results regarding their search for single bottom quarks produced via t-channel exchange of a virtual W boson, as well as s-channel production through gluon fusion. In both scenarios, no significant excess has been observed beyond background predictions. We present our findings alongside those from other experiments that have explored similar phenomena. The CMS experiment at the LHC has identified a surprisingly light scalar resonance decaying into photon or lepton pairs. This observation corresponds with the Standard Model's estimate for the Higgs boson, which is anticipated to have a mass around 126 GeV. Beyond the Higgs boson searches conducted by ATLAS and CMS, there are various extensions of the Standard Model that predict the existence of additional scalar particles. These models may result in deviations from the Standard Model's predictions regarding the properties of the Higgs boson, including aspects like spin, parity, CP violation, coupling strengths, and branching ratios. Many of these theoretical scenarios involve heavy nuclei that could be pair-produced at hadron colliders. However, some theories propose that the Higgs-like state may be a singlet under SU(2) and U(1), meaning such states cannot be produced in pairs directly but can only manifest in association with another quark. For example, in supersymmetric models, the Higgs-like state appears alongside b-quarks. Other theories, such as composite and Little Higgs models, suggest that the Higgs-like state has a preferential coupling to third-generation fermions.",
        "ori-fast-z-score": 0.9805806756909202,
        "water-fast-z-score": 5.439200829200679,
        "rewrite-fast-z-score": 0.2873478855663454
    },
    {
        "original_text": "We consider the problem of optimal investment in a financial market when there is no upper bound on the investor s wealth, but his/her utility function exhibits decreasing absolute risk aversion (DARA). We show that under DARA preferences, the value function for this problem can be characterized as the unique solution to a nonlinear partial differential equation (PDE) which we call the Hamilton-Jacobi-Bellman-Isaacs PDE. This characterization allows us to use standard numerical methods such as finite difference or Monte Carlo simulation to compute the value function numerically. In addition, it also enables us to study how the optimal strategy depends on various parameters including the initial endowment, the interest rate, and the volatility of the stock price process. Finally, by using our results, we are able to provide some new insights into the relationship between pricing and hedging derivatives based on utility maximization principles. The main contributions of this work include:",
        "watermark_text": "We consider the question of optimal investment in a financial market when there is no upper bound on the investor s assets , but his / her utility function exhibits decreasing absolute price aversion ( DARA ) . We see that under DARA preferences , the value function for this question can be described as the unique solve to a nonlinear partial differential equation ( PDE ) which we call the Hamilton - Jacobi - Bellman - Isaacs PDE .This definition permits us to use conventional numerical technique such as finite difference or Monte Carlo simulation to compute the value function numerically . In addition , it also enables us to study how the ideal scenario depends on various variables including the early endowment , the interest rate , and the volatility of the stock price cycle .Finally , by using our findings , we are able to provide some fresh insights into the relationship between pricing and hedging derivatives using on utility maximization theories . The main contributions of this research include :",
        "rewrite_text": "We explore the issue of optimal investment in a financial market without an upper limit on the investor's assets, while the investor's utility function displays decreasing absolute risk aversion (DARA). Our analysis reveals that, under DARA preferences, the value function can be characterized as the unique solution to a nonlinear partial differential equation (PDE), which we refer to as the Hamilton-Jacobi-Bellman-Isaacs PDE. This framework allows us to apply standard numerical methods, such as finite difference or Monte Carlo simulations, for the numerical calculation of the value function. Furthermore, it facilitates an examination of how the optimal investment strategy is influenced by various factors, including initial endowment, interest rates, and stock price volatility. Ultimately, our findings provide novel insights into the relationship between pricing and hedging of derivatives within the context of utility maximization theory. The key contributions of this research include:",
        "ori-fast-z-score": 1.4142135623730951,
        "water-fast-z-score": 5.969098507002659,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "The low energy effective theories for superstrings are supergravity and supersymmetric gauge theories in four dimensions, which can be obtained by compactifying the extra six spatial dimensions on a Calabi-Yau manifold.  In this talk I will discuss some recent results about lattice models that provide an alternative approach to studying these theories. The basic idea is to use Monte Carlo simulations to study supersymmetric field theories defined on a finite number of points (the sites) of a regular d-dimensional hypercubic lattice with periodic boundary conditions. These models have been studied extensively over the past few years using numerical techniques such as exact diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently we developed new Monte Carlo simulation techniques based on the worm algorithm that allow us to simulate large systems at very high temperatures where conventional Monte Carlo methods fail because they suffer from critical slowing down. We used our new method to calculate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory coupled to matter fields in various representations.",
        "watermark_text": "The lowest energy effective models for superstrings are supergravity and supersymmetric gauge fields in four dimensions , which can be obtained by compactifying the extra six spatial dimensions on a Calabi - Yau manifold . In this talk I will explore some latest findings about lattice models that provide an different approach to researching these theories .The basic idea is to use Monte Carlo simulations to study supersymmetric field theories specified on a finite number of points ( the sites ) of a regular d - dimensional hypercubic structure with periodic border conditions . These systems have been studied frequently over the previous few years employing mathematical techniques such as approximate diagonalization , quantum Monte Carlo methods , and density matrix renormalization group algorithms .Recently we developed novel Monte Carlo simulation method based on the worm algorithm that enable us to simulate large systems at very high altitudes where conventional Monte Carlo methods fail because they suffer from critical speed down . We utilized our new method to estimate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang - Mills theory and the N = 1 supersymmetric U ( 1 ) gauge theory connected to matter groups in different representations .",
        "rewrite_text": "The simplest energy-effective models for superstrings are derived from supergravity and supersymmetric gauge fields in four dimensions, achieved by compactifying the additional six spatial dimensions on a Calabi-Yau manifold. In this presentation, I will discuss recent developments in lattice models that offer an alternative approach to investigating these theories. The fundamental concept involves utilizing Monte Carlo simulations to analyze supersymmetric field theories defined on a finite set of points (the sites) arranged in a regular d-dimensional hypercubic structure with periodic boundary conditions. These systems have been extensively examined over the past few years using various mathematical techniques, including approximate diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently, we introduced an innovative Monte Carlo simulation method based on the worm algorithm, which allows us to simulate large systems at significantly high altitudes, overcoming the limitations of traditional Monte Carlo methods that struggle with critical slowing down. We applied our new technique to estimate the free energies of several supersymmetric lattice models, including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory related to matter groups in various representations.",
        "ori-fast-z-score": 0.6255432421712244,
        "water-fast-z-score": 6.118014998909509,
        "rewrite-fast-z-score": 1.3199500146737049
    },
    {
        "original_text": "We present the results of numerical simulations of magnetohydrostatic equilibrium for solar prominences embedded into non-potential, sheared and twisted coronal fields. The model is based on solving numerically the Grad-Shafranov equation with boundary conditions at both ends of the computational domain that are derived using an approximate analytical solution to this problem. We find that the resulting equilibria have shapes similar to those observed in white-light coronagraph images. In particular, we show how the shape of the prominence changes as its position relative to the underlying photospheric magnetic flux distribution varies. Our results also demonstrate that the presence of shear and/or twist leads to significant deviations from potential-field models commonly used in theoretical studies of prominences. These findings may be useful for interpreting observations of prominences made by space-based instruments such as SDO/AIA. \n \n Keywords: Solar prominence, Magnetohydrostatics",
        "watermark_text": "We present the conclusion of computational simulations of magnetohydrostatic equilibrium for solar prominences localized into non - potential , sheared and spun coronal fields . The model is based on solving numerically the Grad - Shafranov equation with boundary conditions at both sides of the theoretical domain that are derived using an approximate mathematical solution to this question .We see that the resulting equilibria have shapes parallel to those observed in white - light coronagraph images . In particular , we find how the form of the prominence changes as its position relative to the underlying photospheric magnetic flux flow varies .Our results also demonstrate that the presence of shear and / or twist contributes to significant deviations from potential - field methods commonly used in theoretical analyses of prominences . These conclusions could be suitable for interpreting observations of prominences made by space - based instruments such as SDO / AIA .Keywords : Solar prominence , Magnetohydrostatics",
        "rewrite_text": "We present the results of computational simulations that investigate magnetohydrostatic equilibrium in solar prominences situated within non-potential, sheared, and twisted coronal magnetic fields. Our model employs a numerical solution to the Grad-Shafranov equation, with boundary conditions established on both sides of the theoretical domain based on an approximate mathematical solution. The equilibria we obtained exhibit shapes that align closely with those seen in white-light coronagraph images. Notably, we observe that the prominence's form changes depending on its position relative to the underlying photospheric magnetic flux flow. Our findings further reveal that the presence of shear and/or twist leads to significant deviations from the potential-field methods typically employed in theoretical analyses of prominences. These insights may prove valuable for interpreting observations of prominences captured by space-based instruments such as SDO/AIA. \n\nKeywords: Solar prominence, Magnetohydrostatics",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.541868715470696,
        "rewrite-fast-z-score": -0.35603449745815596
    },
    {
        "original_text": "We study the problem of finding an optimal set of points on the unit circle that are equidistant to each other and have minimum angular separation between them, known as the Costas array or Costas configuration.  We show how this problem can be formulated as a convex optimization problem with linear constraints over the space of probability measures supported by the unit circle. This formulation allows us to use tools from convex analysis for solving it efficiently. In particular we provide a polynomial time algorithm which computes an approximate solution within any desired accuracy. The proposed method is based on the Frank-Wolfe (FW) algorithm combined with the Sinkhorn-Knopp scaling technique. Finally, numerical experiments demonstrate the efficiency of our approach compared to existing methods. Keywords: Convex Optimization; Probability Measures; Frank Wolfe Algorithm; Scaling Technique; Unit Circle; Costas Array; Costas Configuration.",
        "watermark_text": "We explore the question of finding an appropriate collection of points on the unit circle that are equidistant to each other and have minimum spatial separation between them , known as the Costas array or Costas configuration . We see how this question can be formulated as a convex optimization problem with linear constraints over the space of likelihood measures supported by the unit circle .This formulation enables us to use tools from convex optimization for solving it easily . In particular we provide a polynomial period approximation which computes an approximate solve within any specified precision .The proposed approach is based on the Frank - Wolfe ( FW ) algorithm coupled with the Sinkhorn - Knopp scaling methodology . Finally , numerical studies demonstrate the performance of our approach relative to existing techniques .Keywords: Convex Optimization; Probability Measures; Frank Wolfe Algorithm; Scaling Technique; Unit Circle; Costas Array; Costas Configuration.",
        "rewrite_text": "We investigate the challenge of identifying a suitable arrangement of points on the unit circle that are equally spaced and exhibit minimal spatial separation between them, referred to as a Costas array or Costas configuration. Our approach formulates this problem as a convex optimization task with linear constraints situated in the space of likelihood measures that are supported by the unit circle. This formulation allows us to leverage convex optimization tools to facilitate a straightforward solution process. Specifically, we provide a polynomial period approximation that yields an approximate solution within any desired level of precision. Our method utilizes the Frank-Wolfe (FW) algorithm in conjunction with the Sinkhorn-Knopp scaling technique. Finally, we present numerical studies that illustrate the effectiveness of our method compared to existing techniques.  \n\nKeywords: Convex Optimization; Probability Measures; Frank-Wolfe Algorithm; Scaling Technique; Unit Circle; Costas Array; Costas Configuration.",
        "ori-fast-z-score": -1.2909944487358056,
        "water-fast-z-score": 4.389381125701739,
        "rewrite-fast-z-score": 0.12216944435630522
    },
    {
        "original_text": "The traveling salesman problem (TSP) is one of the most famous problems in combinatorial optimization, which asks for finding an optimal tour through all vertices of a given graph such that each edge is visited exactly once. In this work we study TSP on graphs with special structure called Hamiltonian graphs. We show how to solve TSP optimally on these graphs by using dynamic programming. Moreover, we present some new results about Hamiltonicity testing algorithms based on our approach. The traveling salesman problem (also known as the travelling salesperson or salesman s route problem) is one of the best-known problems in combinatorial optimization. It can be formulated as follows: Given a complete weighted undirected graph G = (V , E), find a cycle C passing through every vertex v ∈ V at least once so that the total weight w(C ) of edges in C is minimized. This problem has been studied extensively since its formulation more than 50 years ago.",
        "watermark_text": "The touring salesman problem ( TSP ) is one of the most famous difficulties in combinatorial algorithms , which asks for finding an efficient walk through all edges of a given graph such that each edge is attended exactly once . In this research we study TSP on graphs with special structure named Hamiltonian graphs .We see how to solve TSP optimally on these graphs by using dynamic programming . Moreover , we present some different results about Hamiltonicity testing algorithms based on our approach .The touring salesman problem ( also known as the travelling salesperson or salesman s route puzzle ) is one of the best - famous difficulties in combinatorial algorithms . It can be formulated as follows : Given a complete weighted undirected graph G = ( V , E ) , find a cycle C passing through every vertex v ∈ V at least once so that the total weight w ( C ) of vertices in C is minimized .This problem has been studied frequently since its formulation more than 50 years previously .",
        "rewrite_text": "The traveling salesman problem (TSP) is one of the most well-known challenges in combinatorial algorithms. It seeks to determine an efficient path through all the edges of a given graph, ensuring that each edge is visited exactly once. In this research, we focus on TSP in a specific type of graph known as Hamiltonian graphs. We demonstrate how to optimally solve TSP on these graphs using dynamic programming techniques. Additionally, we present various findings regarding Hamiltonicity testing algorithms that are based on our method. The touring salesman problem, also referred to as the traveling salesperson or salesman’s route puzzle, can be defined as follows: Given a complete weighted undirected graph G = (V, E), the task is to find a cycle C that visits every vertex v ∈ V at least once while minimizing the total weight w(C) of the vertices in C. This problem has been a subject of extensive study since its inception over 50 years ago.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 0.46499055497527714
    },
    {
        "original_text": "In this work, we study the throughput capacity region for multi-hop wireless networks with multiple transmitters and one receiver (MISO-MHWN). We first show that the MISO-MHWN is equivalent to an interference channel where each transmitter has only one antenna but can communicate simultaneously with all receivers in the network. Then, by using the concept of degrees-of-freedom (DoF), which characterizes how many parallel data streams are supported at high signal-to-noise ratio (SNR) regime, we derive outer bounds on the DoF region of the MISO-MHWNS. Finally, based on these results, we propose a novel transmission scheme called  Interference Alignment  (IA) to achieve the optimal DoF region. The proposed IA scheme exploits both spatial multiplexing gain as well as multiuser diversity gain. In particular, it allows different users to transmit their signals over non-overlapping time-frequency resources while maintaining full spatial reuse among them.",
        "watermark_text": "In this research , we study the throughput capacity area for single - hop wireless networks with many transmitters and one antenna ( MISO - MHWN ) . We first see that the MISO - MHWN is analogous to an interference channel where each antenna has only one antenna but can communicate simultaneously with all receivers in the channel .Then , by using the notion of degrees - of - independence ( DoF ) , which characterizes how many concurrent data feeds are implemented at high signal - to - noise ratio ( SNR ) regime , we derive upper limits on the DoF area of the MISO - MHWNS . Finally , based on these results , we undertake a new transmission strategy titled Interference Alignment ( IA ) to achieve the ideal DoF area .The proposed IA plan involves both temporal multiplexing gain as well as multiuser diversity gain . In particular , it allows different users to transmit their messages over non - overlapping period - frequency resources while maintaining full temporal reuse among them .",
        "rewrite_text": "In this study, we investigate the throughput capacity region for single-hop wireless networks featuring multiple transmitters with one antenna each (MISO-MHWN). We begin by recognizing that the MISO-MHWN behaves similarly to an interference channel wherein each transmitter is equipped with a single antenna but can simultaneously communicate with all receivers. Utilizing the concept of degrees-of-freedom (DoF), which quantifies the number of concurrent data streams that can be accommodated in a high signal-to-noise ratio (SNR) environment, we establish upper bounds on the DoF region for MISO-MHWNs. Building on these findings, we propose a novel transmission strategy called Interference Alignment (IA) to optimize the achievable DoF region. This IA approach leverages both temporal multiplexing and multiuser diversity gains, allowing various users to send their messages across non-overlapping time-frequency resources while enabling complete temporal reuse among them.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 5.421151989096865,
        "rewrite-fast-z-score": -1.5
    },
    {
        "original_text": "The present work is devoted to the investigation of new possible nuclear properties using nonlinear methods, namely fractal dimension (FD) and recurrence quantification analysis (RQA). The FD was calculated for different mass number A in order to study its dependence on the system size. It has been found that there exists an optimal value of the scaling parameter which provides the best results for each nucleus separately. This fact indicates that the structure of the studied systems can be characterized as multifractals with non-trivial behavior. In addition we have shown that the obtained values are very close to those predicted by the percolation theory. We also applied RQA method to investigate the temporal evolution of the considered systems. Our calculations show that the complexity of the time series increases with increasing mass number A. Moreover it turns out that this increase follows closely the predictions made within the framework of the Random Matrix Theory. Finally we have compared our results with experimental data available for some light nuclei.",
        "watermark_text": "The present work is devoted to the exploration of new possible nuclear properties using nonlinear methods , notably fractal dimension ( FD ) and recurrence quantification analysis ( RQA ) . The FD was calculated for different mass quantity A in order to study its dependence on the system size .It has been determined that there exists an appropriate value of the scaling parameter which offers the best results for each particle separately . This fact suggests that the composition of the studied structures can be described as multifractals with non - simple properties .In addition we have shown that the achieved values are very close to those predicted by the percolation theory . We also used RQA approach to examine the temporal evolution of the considered systems .Our calculations show that the complexity of the time series increases with expanding mass quantity A . Moreover it turns out that this increase follows carefully the assumptions done within the framework of the Random Matrix Theory .Finally we have linked our findings with experimental evidence available for some light nuclei .",
        "rewrite_text": "This study focuses on investigating new potential nuclear properties through nonlinear methods, specifically fractal dimension (FD) and recurrence quantification analysis (RQA). We calculated the FD for various mass quantities (A) to assess its dependence on system size. Our findings indicate that there is an optimal scaling parameter that yields the best results for each individual particle. This suggests that the structures we examined can be characterized as multifractals with complex properties. Additionally, we demonstrated that the values obtained are in close agreement with those predicted by percolation theory. The RQA approach was also utilized to analyze the temporal evolution of the systems in question. Our calculations reveal that the complexity of the time series increases with mass quantity (A), and this increase aligns with the predictions made within the framework of Random Matrix Theory. Finally, we have correlated our results with available experimental evidence for certain light nuclei.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 5.142956348249516,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "We study how galactic discs warp in response to tidal forces exerted by dark matter haloes and intergalactic gas filaments, using high-resolution cosmological simulations with radiative cooling and star formation. We find that the majority (>80%) of simulated galaxies have significant warping at z = 0. The amplitude of the warp increases with decreasing galaxy mass, but is independent of redshift for massive galaxies. Warp amplitudes are typically less than 10 kpc, which agrees well with observations. Our results suggest that most observed warps can be explained as being caused by external gravitational torques on galactic discs. This implies that there may not exist any intrinsic mechanism within galactic discs themselves that causes warps. In addition, we show that the presence of an AGN does not significantly affect the shape or strength of the warp. Finally, we demonstrate that our model predicts a correlation between the direction of the warp and the angular momentum vector of the host halo.",
        "watermark_text": "We research how galactic discs warp in reaction to tidal forces exerted by black material haloes and intergalactic gas filaments , using high - resolution cosmological simulations with radiative cooling and galaxy formation . We see that the majority ( > 80 % ) of simulated galaxies have considerable warping at z = 0 .The amplitude of the warp increases with varying universe weight , but is independent of redshift for huge objects . Warp amplitudes are typically lower than 10 kpc , which agrees well with observations .Our results propose that most observed warps can be described as being affected by external gravitational torques on galactic discs . This implies that there may not exist any inherent mechanism within galactic discs themselves that creates warps .In addition , we prove that the presence of an AGN does not dramatically impact the form or strength of the warp . Finally , we prove that our model predicts a correlation between the direction of the warp and the angular velocity tensor of the host halo .",
        "rewrite_text": "We investigate how galactic discs deform in response to tidal forces from dark matter haloes and intergalactic gas filaments, employing high-resolution cosmological simulations that incorporate radiative cooling and galaxy formation processes. Our findings indicate that over 80% of the simulated galaxies exhibit significant warping at redshift z = 0. The degree of warping is positively correlated with the mass of the universe but shows no dependence on redshift for large objects. Typically, the warp amplitudes are under 10 kpc, aligning well with observational data. Our results suggest that the majority of observed warps are predominantly influenced by external gravitational torques acting on the galactic discs, indicating that there may not be an intrinsic mechanism within the discs themselves responsible for creating these warps. Furthermore, we demonstrate that the presence of an active galactic nucleus (AGN) does not substantially alter the shape or strength of the warp. Lastly, our model reveals a correlation between the orientation of the warp and the angular velocity tensor of the host halo.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 4.572004572006858,
        "rewrite-fast-z-score": 1.4552137502179978
    },
    {
        "original_text": "We propose a model that describes the spontaneous formation of neuronal assemblies in terms of an unsupervised Hebbian learning rule with synaptic depression. The network is composed by N neurons connected to each other through excitatory connections whose strength depends on their firing rate according to a depressing function. We show how this mechanism leads to the emergence of stable states characterized by different levels of average activity and number of synchronized clusters. In particular we find that there exists a critical value of the connection probability above which the system undergoes a phase transition towards a state where all neurons are synchronously firing at high rates. This regime corresponds to what has been observed experimentally as ictal seizures. \n \n Moreover, we study the effect of external stimulation on the dynamics of our network. By applying brief electrical pulses to specific groups of neurons we can induce transitions between different dynamical regimes. Finally, we discuss possible applications of these results to neurobiology.",
        "watermark_text": "We suggest a theory that describes the spontaneous development of neuronal assemblies in terms of an unsupervised Hebbian learning principle with synaptic depression . The network is composed by N cells connected to each other through excitatory connections whose strength depends on their fired rate due to a depressing function .We see how this mechanism leads to the emergence of stable states characterized by various rates of average activity and number of synchronized clusters . In particular we find that there exists a critical factor of the connection probability above which the system undergoes a phase shift towards a state where all neurons are synchronously firing at high levels .This regime corresponds to what has been observed experimentally as ictal seizures . Moreover , we study the impact of external stimulation on the dynamics of our system .By applying short electrical pulses to individual groups of neurons we can induce changes between various dynamical regimes . Finally , we explain possible use of these results to neurobiology .",
        "rewrite_text": "We propose a theory that explains the spontaneous formation of neuronal assemblies through an unsupervised Hebbian learning mechanism alongside synaptic depression. This network consists of N interconnected cells linked by excitatory connections, with synaptic strength being influenced by their firing rates, governed by a depressing function. Our analysis reveals that this mechanism facilitates the emergence of stable states, which are characterized by varying levels of average activity and different numbers of synchronized clusters. Notably, we discover that there is a critical threshold for connection probability, beyond which the system experiences a phase transition, resulting in all neurons firing synchronously at elevated levels. This state aligns with what is experimentally observed during ictal seizures. Additionally, we investigate how external stimuli affect the dynamics of our model. By delivering brief electrical pulses to specific groups of neurons, we can trigger transitions between different dynamical regimes. Lastly, we discuss the potential implications of our findings for neurobiology.",
        "ori-fast-z-score": 1.162476387438193,
        "water-fast-z-score": 5.969098507002659,
        "rewrite-fast-z-score": -0.7071067811865476
    },
    {
        "original_text": "We present the results of an analysis to determine whether physical vetoes can be used as part of a pipeline to reduce false alarms in searches for gravitational waves (GWs) from binary black hole mergers and other astrophysical sources.  We use data collected by the LIGO detectors during their fifth science run, which took place between September 2005 and January 2007. The search pipeline is based on matched filtering with template waveforms that are generated using post-Newtonian expansions up to 3PN order. In addition to standard cuts applied to the signal-to-noise ratio (SNR), we also apply two different types of physical vetoes:  1) Vetoing events whose SNRs exceed some threshold value when they occur simultaneously at multiple detector sites; 2) Vetoing events where there is evidence of excess power above background noise levels in the frequency bands below 100 Hz or above 1000 Hz. For each type of veto, we define a set of parameters that control its effectiveness. Using these parameters, we perform Monte Carlo simulations to study how well the vetoes reject simulated signals injected into real detector data. Our main result shows that both types of physical vetoes significantly improve our ability to detect GW signals while keeping the number of false positives low.",
        "watermark_text": "We report the results of an assessment to find whether physical vetoes can be used as part of a pipeline to reduce false alarms in searches for gravitational waves ( GWs ) from binary white hole mergers and other astrophysical sources . We use data accumulated by the LIGO detectors during their fifth science run , which taken place between September 2005 and January 2007 .The scan pipeline is based on matched sampling with template waveforms that are produced using post - Newtonian expansions up to 3PN order . In addition to standard cuts applied to the signal - to - noise proportion ( SNR ) , we also apply two different kinds of physical vetoes : 1 ) Vetoing events whose SNRs reach some threshold value when they occur simultaneously at multiple detector sites ; 2 ) Vetoing events where there is evidence of excess energy above background noise heights in the frequency bands below 100 Hz or above 1000 Hz .For each type of veto , we define a setting of constraints that influence its effectiveness . Using these parameters , we perform Monte Carlo simulations to study how best the vetoes reject simulated messages imported into real detector data .Our main result suggests that both types of physical vetoes significantly boost our power to identify GW signals while staying the number of false positives small .",
        "rewrite_text": "We present the findings of an evaluation aimed at determining whether physical vetoes can be incorporated into a process to minimize false alarms in the search for gravitational waves (GWs) originating from binary white hole mergers and other astronomical sources. Our analysis utilizes data collected by the LIGO detectors during their fifth science run, which occurred from September 2005 to January 2007. The scanning pipeline is based on matched filtering with template waveforms generated using post-Newtonian expansions up to third post-Newtonian (3PN) order. In addition to conventional thresholds applied to the signal-to-noise ratio (SNR), we implement two distinct types of physical vetoes: 1) Vetoing events that reach a certain SNR threshold when detected simultaneously across multiple detector sites; 2) Vetoing events that demonstrate excess energy above background noise levels in frequency ranges below 100 Hz or above 1000 Hz. For each veto type, we establish a set of constraints that affect its efficiency. Employing these parameters, we conduct Monte Carlo simulations to investigate the optimal way to reject simulated signals that have been inserted into actual detector data. Our primary finding indicates that both types of physical vetoes substantially enhance our ability to identify GW signals while maintaining a low rate of false positives.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.252793231671496,
        "rewrite-fast-z-score": 1.6
    },
    {
        "original_text": "The human genome is organized into chromatin, which consists of DNA wrapped around histone proteins and other associated factors. The folding of the chromatin fiber plays an important role in regulating gene expression by controlling access of transcription machinery to genes. In this review we discuss recent advances on how chromatin folds at different levels (from single nucleosome to chromosome) and its relation with human genome function. We also introduce some new concepts such as  topological domains  and  chromosome territories  that are emerging from high-throughput technologies. Finally, we provide examples for future research directions. Chromatin is the basic unit of eukaryotic chromosomes consisting of DNA wrapped around histones and other associated factors. It has been shown that chromatin structure plays an important role in regulation of gene expression through control of accessibility of transcription machinery to genes  1  . Recent studies have revealed that chromatin fibers fold into higher-order structures ranging from 30 nm fibers  2  , loops  3  , topologically associating domains  4  , and chromosome territories  5  .\nIn this review, we will first briefly describe the organization of chromatin fibers at various scales. Then we will focus on the relationship between chromatin folding and human genome functions including gene regulation, replication, repair, and recombination. Finally, we will give several examples for possible future research directions.",
        "watermark_text": "The human genome is organized into chromatin , which consists of DNA wrapped around histone proteins and other associated factors . The folding of the chromatin fiber plays an important role in controlling gene transcription by regulation entry of transcription machinery to proteins .In this review we explain latest advances on how chromatin layers at different levels ( from single nucleosome to genome ) and its connection with human genome function . We especially introduce some fresh concepts such as topological domains and genome territories that are emerging from high - throughput methods .Finally , we provide examples for future research approaches . Chromatin is the fundamental unit of eukaryotic chromosomes consisting of DNA wrapped around histones and other associated factors .It has been shown that chromatin structure serves an important role in regulation of gene transcription through regulating of accessibility of transcription apparatus to factors 1 . Recent research have revealed that chromatin fibers fold into greater - order regions ranging from 30 nm fibers 2 , loops 3 , topologically associating domains 4 , and chromosome areas 5 .In this review , we will first briefly investigate the organization of chromatin fibers at several scales . Then we will focus on the relationship between chromatin folding and human genetic functions namely gene control , replication , maintenance , and recombination .Finally , we will provide several examples for future future research paths .",
        "rewrite_text": "The human genome is structured into chromatin, which consists of DNA wrapped around histone proteins and other associated components. The way chromatin fibers fold is crucial for regulating gene transcription, as it influences the accessibility of the transcription machinery to the necessary proteins. In this review, we will discuss recent advancements in our understanding of chromatin organization across various levels, from individual nucleosomes to the entire genome, and its implications for human genomic function. We will introduce innovative concepts such as topological domains and genome territories that have emerged from high-throughput technologies. Additionally, we will present examples of potential future research directions. Chromatin is the essential unit of eukaryotic chromosomes, consisting of DNA coiled around histones and other related factors. Research has demonstrated that chromatin structure is vital for gene transcription regulation by controlling the accessibility of transcription factors. Recent studies have shown that chromatin fibers can fold into higher-order structures, including 30 nm fibers, loops, topologically associating domains, and specific chromosome regions. In this review, we will first provide a brief overview of chromatin fiber organization at multiple scales. We will then explore the connection between chromatin folding and key human genetic functions, such as gene regulation, DNA replication, genomic maintenance, and recombination. Lastly, we will outline several examples of future research avenues.",
        "ori-fast-z-score": -1.3471506281091268,
        "water-fast-z-score": 6.609001368025944,
        "rewrite-fast-z-score": 0.6897304947150052
    },
    {
        "original_text": "We consider the problem of designing distributed consensus algorithms in sensor networks where links are randomly generated and can be lost or added over time. We propose an algorithm that is robust to link failures, but requires only local information exchange between neighboring nodes. The proposed algorithm achieves global convergence under mild conditions on network topology. In particular, we show that if each node has at least one neighbor whose degree is greater than its own then our algorithm converges almost surely (a.s.) to the correct value. Our results also hold when there exists a small number of Byzantine nodes which may deviate arbitrarily from their prescribed behavior. Finally, numerical simulations demonstrate the effectiveness of our approach. Keywords: Sensor Networks; Distributed Consensus; Local Information Exchange; Robustness Analysis. 1 Introduction Distributed consensus problems arise naturally in many applications such as flocking  1  , formation control  2  , multi-agent coordination  3  , wireless sensor networks  4  , etc.. A typical example is the average-consensus problem: given a set of n agents connected by communication links, each agent holds some initial data xi(0) ∈ Rm, i = 1, ..., n; it aims to compute the average x̄=1/n∑in=1xi(0). This problem was first studied by Tsitsiklis et al.  5  . They showed that if all agents have access to the same fixed directed graph G, then the average-consensus problem can be solved using a simple linear iterative scheme. However, this assumption does not always hold true since the underlying communication graphs are often random due to unreliable links  6  .\nIn recent years, several researchers have investigated the design of distributed consensus algorithms in dynamic networks  7-10 . For instance, Olfati-Saber  7  considered the case where the communication links among agents change randomly according to independent Bernoulli processes. Under certain assumptions on the connectivity of the network, she proved that her algorithm converges almost surely (i.e., with probability one) to the desired average. Subsequently, Jadbabaie et al.  8  extended these results to undirected networks. More recently,",
        "watermark_text": "We consider the issue of constructing dispersed consensus algorithms in sensor networks where links are randomly generated and can be lost or added over time . We suggest an algorithm that is robust to link errors , but requires only local information transfer between neighboring connections .The proposed algorithm achieves global convergence under mild assumptions on system topology . In particular , we prove that if each node has at least one neighbor whose degree is greater than its own then our algorithm converges almost always ( a . s . ) to the appropriate value .Our results also hold when there exists a small number of Byzantine nodes which may deviate arbitrarily from their appropriate behavior . Finally , numerical simulations prove the ability of our approach .Keywords : Sensor Networks ; Distributed Consensus ; Local Information Exchange ; Robustness Analysis . 1 Introduction Distributed consensus difficulties arise naturally in different applications such as flocking 1 , structure control 2 , multi - agent coordination 3 , wireless sensor networks 4 , etc . . A typical example is the average - consensus problem : considering a setting of n agents connected by communication connections , each agent possesses some initial data xi ( 0 ) ∈ Rm , i = 1 , . . . , n ; it aims to compute the average [UNK] = 1 / [UNK] = 1xi ( 0 ) .This problem was first investigated by Tsitsiklis et al . 5 .They showed that if all agents have access to the same fixed directed graph G , then the average - consensus problem can be answered using a simple linear iterative plan . However , this claim does not always hold false since the underlying communication graphs are often random due to unreliable links 6 .In recent years , various papers have researched the development of distributed agreement algorithms in dynamic networks 7 - 10 . For instance , Olfati - Saber 7 considered the case where the information links among participants change automatically according to individual Bernoulli processes .Under certain assumptions on the connectivity of the network , she proved that her algorithm converges almost always ( i . e . , with probability one ) to the desired average . Subsequently , Jadbabaie et al .8 extended these results to undirected networks . More recently ,",
        "rewrite_text": "We address the challenge of creating dispersed consensus algorithms within sensor networks characterized by randomly generated and fluctuating links. Our proposed algorithm is resilient to link failures and requires only local information exchange between neighboring nodes. It guarantees global convergence under lenient conditions regarding the system's topology. Specifically, we demonstrate that if each node has at least one neighbor with a higher degree, our algorithm almost always converges to the correct value. Our findings are also valid even in the presence of a limited number of Byzantine nodes that may behave inappropriately. Additionally, numerical simulations validate the effectiveness of our approach. \n\n**Keywords:** Sensor Networks; Distributed Consensus; Local Information Exchange; Robustness Analysis.\n\n**1 Introduction** Distributed consensus challenges are inherent in various applications, including flocking, structural control, multi-agent coordination, and wireless sensor networks. A classic example is the average-consensus problem, where a group of n agents connected by communication links each holds initial data \\(x_i(0) \\in \\mathbb{R}^m\\) (for \\(i = 1, \\ldots, n\\)) and seeks to compute the average \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i(0)\\). This problem was initially explored by Tsitsiklis et al., who established that if all agents rely on a consistent fixed directed graph \\(G\\), the average-consensus problem can be resolved using a straightforward linear iterative process. However, this assertion sometimes fails because communication graphs are frequently random, stemming from unreliable links. In recent years, several studies have investigated distributed agreement algorithms in dynamic networks. For example, Olfati-Saber examined situations where the information links among participants fluctuate according to individual Bernoulli processes and proved that her algorithm converges almost always (i.e., with probability one) to the desired average, given specific connectivity conditions. Jadbabaie et al. later extended these results to undirected networks. More recently,",
        "ori-fast-z-score": 1.4791479939068937,
        "water-fast-z-score": 7.597626364883187,
        "rewrite-fast-z-score": 1.7614096918559585
    },
    {
        "original_text": "We study the orbital magnetization (OM) induced by spin-orbit coupling on a kagome lattice with chiral magnetic order, which is realized as an emergent property of the system at low temperatures. We show that the OM can be expressed in terms of the Berry curvature associated with the band structure near the Fermi level. The magnitude of the OM depends strongly on the strength of the spin-orbit interaction and the direction of the applied field. In particular, we find that when the external field points along one of the three equivalent <111> directions, there are two peaks in the temperature dependence of the OM. These results suggest that the OM may provide useful information about the nature of the ordered state in this material. \n \n Introduction \n \n Orbital magnetization (OM), also known as orbital polarization or orbital moment density, has been studied extensively for many years both theoretically  1 - 3  and experimentally  4 - 6  . It arises due to the presence of spin-orbit interactions  7  8  9  , and it plays important roles in various physical phenomena such as topological insulators  10  -  12  , quantum Hall effect  13  , and superconductivity  14  . Recently, the OM was observed in several materials including SrRuO3  15  , La0.7Sr0.3MnO3  16  , YbMgGaO4  17  , and FeSe  18  .\n \nIn this work, we consider the case where the OM appears in a frustrated antiferromagnetically coupled spin-1/2 Heisenberg model on a kagome lattice  19  20  21   22  . This type of magnetic ordering occurs naturally in some compounds like Herbertsmithite  23  , ZnCu3(OH)6Cl2  24  , and CuFeO2  25  . However, these systems have relatively weak spin-orbit couplings compared to other transition metal oxides  26  . Therefore, they do not exhibit large values of the OM  27  . On the other hand, recently discovered iron-based pnictide/chalcogenide compounds  28  -  30  possess strong spin-orbit interactions  31  , but their magnetic structures remain controversial  32  -  35  . Thus, our theoretical investigation provides valuable insight into possible experimental realiz",
        "watermark_text": "We explore the orbital magnetization ( OM ) induced by spin - orbit interaction on a kagome lattice with chiral magnetic order , which is realized as an emergent property of the system at low temperatures . We see that the OM can be described in terms of the Berry curvature associated with the band structure near the Fermi level .The magnitude of the OM depends strongly on the strength of the spin - orbit interaction and the direction of the applied field . In particular , we find that when the external field points along one of the three analogous < 111 > directions , there are two peaks in the temperature dependence of the OM .These data suggest that the OM may provide useful info about the nature of the ordered state in this material . Introduction Orbital magnetization ( OM ) , sometimes called as orbital polarization or orbital moment density , has been studied thoroughly for numerous years both theoretically 1 - 3 and experimentally 4 - 6 .It arises due to the presence of spin - orbit interactions 7 8 9 , and it serves crucial roles in different physical phenomena such as topological insulators 10 - 12 , quantum Hall impact 13 , and superconductivity 14 . Recently , the OM was seen in multiple materials namely SrRuO3 15 , La0 . 7Sr0 . 3MnO3 16 , YbMgGaO4 17 , and FeSe 18 .In this research , we define the case where the OM appears in a frustrated antiferromagnetically linked spin - 1 / 2 Heisenberg model on a kagome lattice 19 20 21 22 . This kind of magnetic ordering occurs commonly in some molecules like Herbertsmithite 23 , ZnCu3 ( OH ) 6Cl2 24 , and CuFeO2 25 .However , these systems have fairly weak spin - orbit couplings compared to other transition metal oxides 26 . Therefore , they do not show many values of the OM 27 .On the other hand , recently discovered iron - based pnictide / chalcogenide compounds 28 - 30 contain powerful spin - orbit bonding 31 , but their magnetic structures remain disputed 32 - 35 . Thus , our theoretical investigation gives valuable inquiry into possible experimental realiz",
        "rewrite_text": "We investigate the orbital magnetization (OM) resulting from spin-orbit interactions in a kagome lattice exhibiting chiral magnetic order, which emerges as a property of the system at low temperatures. Our findings indicate that the OM can be characterized by the Berry curvature associated with the band structure near the Fermi level. The strength of the OM is significantly influenced by the intensity of the spin-orbit interaction and the orientation of the applied magnetic field. Notably, when the external field is aligned with one of the three equivalent <111> directions, we observe two peaks in the temperature dependence of the OM. This behavior suggests that the OM might offer insightful information regarding the nature of the ordered state in this material. \n\nIntroduction: Orbital magnetization (OM), also referred to as orbital polarization or orbital moment density, has been the subject of extensive study for many years, both theoretically and experimentally. It arises from spin-orbit interactions and plays essential roles in various physical phenomena, including topological insulators, quantum Hall effects, and superconductivity. Recent observations of OM have been made in several materials, such as SrRuO3, La0.7Sr0.3MnO3, YbMgGaO4, and FeSe. In this study, we focus on the scenario where OM appears in a frustrated antiferromagnetically coupled spin-1/2 Heisenberg model on a kagome lattice. This type of magnetic ordering is often found in certain compounds like Herbertsmithite, ZnCu3(OH)6Cl2, and CuFeO2. However, these systems typically exhibit relatively weak spin-orbit coupling compared to other transition metal oxides, resulting in low OM values. Conversely, the newly discovered iron-based pnictide/chalcogenide compounds possess strong spin-orbit coupling, yet their magnetic structures remain a topic of debate. Thus, our theoretical analysis provides valuable insights that could guide future experimental investigations.",
        "ori-fast-z-score": 0.0873704056661038,
        "water-fast-z-score": 5.722930891116555,
        "rewrite-fast-z-score": -0.17025130615174972
    },
    {
        "original_text": "The standard model (SM) is the most successful theory in particle physics, but it has some problems such as hierarchy problem and CP violation. In this talk I will discuss how we can solve these problems by using string theories. First let us consider the SM with three generations of quarks and leptons. The Yukawa couplings are given by \nwhere  is the Higgs vacuum expectation value,  is the mass matrix for fermions ,  is the CKM mixing matrix , and  is the Kobayashi-Maskawa(KM) matrix . We have two parameters in the KM matrix : one phase which causes CP violation and another parameter called Jarlskog invariant J = Im VudVub * / Re VudIm Vub .\nIn order to explain the observed CP violation in K meson system, we need at least one complex number in the KM matrix. However there are only four real numbers in the Yukawa coupling matrices. This means that we cannot determine all elements of the KM matrix uniquely. Therefore we introduce extra dimensions into our models so that we can obtain more degrees of freedom.",
        "watermark_text": "The conventional model ( SM ) is the most efficient model in particle theory , but it has some problems such as hierarchy problem and CP violation . In this talk I will explore how we can answer these problems by using string theories .First let us consider the SM with three generations of quarks and leptons . The Yukawa couplings are given by where is the Higgs vacuum expectation parameter , is the mass vector for fermions , is the CKM mixing function , and is the Kobayashi - Maskawa ( KM ) matrix .We have two parameters in the KM matrix : one phase which causes CP violation and another parameter called Jarlskog invariant J = Im VudVub * / Re VudIm Vub . In order to explain the observed CP violation in K meson scheme , we require at least one complex number in the KM matrix .However there are only four real numbers in the Yukawa interaction matrices . This implies that we never determine all members of the KM matrix uniquely .Therefore we incorporate additional dimensions into our models so that we can obtain more degrees of liberty .",
        "rewrite_text": "The Standard Model (SM) is currently the most efficient framework in particle physics, yet it faces challenges such as the hierarchy problem and CP violation. In this presentation, I will discuss how string theory could provide solutions to these issues. To begin, let's examine the SM, which includes three generations of quarks and leptons. The Yukawa couplings can be expressed as functions of the Higgs vacuum expectation value, the fermion mass vector, the CKM mixing matrix, and the Kobayashi-Maskawa (KM) matrix itself. The KM matrix contains two parameters: one phase that induces CP violation and the Jarlskog invariant, J = Im(VudVub*) / Re(Vud)Im(Vub). To account for the observed CP violation in the context of K mesons, we need at least one complex entry in the KM matrix; however, the Yukawa interaction matrices consist of only four real values, meaning we cannot uniquely determine all components of the KM matrix. Therefore, we introduce additional dimensions in our models to increase the number of degrees of freedom available.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 4.093146241443879,
        "rewrite-fast-z-score": -0.601929265428846
    },
    {
        "original_text": "The VSOP (Variable Star One-shot Project) is an ambitious project to obtain high-precision radial velocities for thousands of stars with the goal of detecting extrasolar planets by means of gravitational microlensing, transit photometry or direct imaging techniques.  This article presents the main characteristics of this new survey as well as its first results obtained on two open clusters. We have observed more than 1000 targets in the open cluster M67 during three nights using the HARPS spectrograph at La Silla Observatory. Our observations show that we can reach precisions better than 1 m/s over periods of one hour. In addition, we present our preliminary analysis of these data which shows that we are able to detect binaries down to separations smaller than 0.1 AU. Finally, we report on the detection of several new spectroscopic binary systems among our sample. Keywords: Open Cluster, Radial Velocity Survey",
        "watermark_text": "The VSOP ( Variable Star One - shot Project ) is an ambitious project to obtain high - precision radial velocities for thousands of stars with the objective of detecting extrasolar stars by means of gravitational microlensing , transit photometry or direct scanning techniques . This page presents the main characteristics of this new study as well as its initial results derived on two open complexes .We have discovered more than 1000 targets in the open cluster M67 during three nights using the HARPS spectrograph at La Silla Observatory . Our observations show that we can reach precisions good than 1 m / s over periods of one minute .In addition , we present our preliminary analysis of these information which demonstrates that we are able to identify binaries down to separations smaller than 0 . 1 AU . Finally , we publish on the discovery of several novel spectroscopic binary complexes among our sample .Keywords: Open Cluster, Radial Velocity Survey",
        "rewrite_text": "The Variable Star One-shot Project (VSOP) is an ambitious initiative aimed at obtaining high-precision radial velocities for thousands of stars to detect extrasolar planets through methods like gravitational microlensing, transit photometry, and direct scanning techniques. This webpage outlines the key features of this new study, along with initial results obtained from two open clusters. During three nights of observation with the HARPS spectrograph at La Silla Observatory, we identified over 1,000 targets in the open cluster M67. Our findings reveal that we can achieve precision better than 1 m/s over one-minute intervals. Additionally, we present preliminary analyses that indicate our capability to detect binary systems with separations as small as 0.1 AU. Lastly, we report the discovery of several new spectroscopic binary complexes within our sample.  \nKeywords: Open Cluster, Radial Velocity Survey",
        "ori-fast-z-score": 0.39056673294247163,
        "water-fast-z-score": 5.777483045827792,
        "rewrite-fast-z-score": 0.9271726499455306
    },
    {
        "original_text": "The future evolution of the local large scale structure is investigated by using N-body simulations with different cosmological models, including dark energy (DE) and cold dark matter (CDM). The results show that in the Λ-CDM model without DE, the growth rate of large-scale structures decreases rapidly at late times due to the effect of CDM. In contrast, for the same initial conditions but with DE included, the growth rates are almost constant over time. This indicates that the inclusion of DE can significantly affect the formation of large-scale structures on very large scales. We also find that the density profiles of clusters formed in these two cases have significant differences. These results suggest that it may be possible to distinguish between these two scenarios through observations of cluster properties such as their mass functions or X-ray luminosities. Keywords: Cold dark matter; Dark energy; Growth factor; Clustering statistics; Density profile; Cosmology",
        "watermark_text": "The future development of the local big scale system is investigated by using N - bodies simulations with various cosmological models , notably dark energy ( DE ) and cold bright matter ( CDM ) . The results show that in the Λ - CDM theory without DE , the development frequency of large - scale structures decreases quickly at late times due to the impact of CDM .In contrast , for the same original conditions but with DE included , the development rates are almost steady over time . This implies that the introduction of DE can significantly affect the formation of large - scale structures on very huge scales .We additionally find that the density characteristics of clusters formed in these two situations have considerable variations . These data suggest that it could be possible to distinguish between these two scenarios through observations of cluster structures such as their mass distributions or X - ray luminosities .Keywords : Cold black material ; Dark energy ; Growth factor ; Clustering statistics ; Density profile ; Cosmology",
        "rewrite_text": "The future evolution of the local large-scale structure is explored through N-body simulations utilizing various cosmological models, particularly focusing on dark energy (DE) and cold dark matter (CDM). The findings indicate that in the Λ-CDM framework without DE, the growth rate of large-scale structures declines sharply at later times due to the influence of CDM. In contrast, when DE is included under the same initial conditions, the growth rates remain nearly constant over time. This suggests that the presence of DE has a profound impact on the development of large-scale structures at significant scales. Additionally, we observe significant differences in the density profiles of clusters formed under these two scenarios. These results imply that it may be possible to differentiate between the two models by examining the structural properties of clusters, such as their mass distributions and X-ray luminosities. Keywords: Cold dark matter; Dark energy; Growth factor; Clustering statistics; Density profile; Cosmology",
        "ori-fast-z-score": -0.11396057645963795,
        "water-fast-z-score": 6.267831705280087,
        "rewrite-fast-z-score": 0.6793662204867574
    },
    {
        "original_text": "We present the results of our study on binary models for gamma-ray bursts (GRBs) with progenitors in the mass range 8-40 M⊙, which are expected to produce GRB jets that can be observed at cosmological distances. We find that these systems evolve into double-degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae. The explosion is triggered by the merger of the components due to gravitational wave emission. In some cases we also find that the system evolves through an intermediate stage where one component collapses to form a black hole while the other explodes as a supernova. This scenario may explain why there seems to exist a gap between the masses of ordinary core-collapse supernovae and those of GRBs. Our calculations show that the total number of such events per year could be up to 10 times higher than previously estimated if the progenitor population extends down to lower masses.",
        "watermark_text": "We present the conclusion of our study on binary models for gamma - ray bursts ( GRBs ) with progenitors in the mass range 8 - 40 [UNK] , which are expected to produce GRB jets that can be observed at cosmological distances . We say that these systems evolve into double - degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae .The explosion is caused by the merger of the parts due to gravitational wave radiation . In some cases we also find that the system evolves through an intermediate stage where one element collapses to form a black hole while the other explodes as a supernova .This scenario could explain why there seems to exist a gap between the masses of normal core - collapse supernovae and those of GRBs . Our calculations show that the total number of such events per year could be up to 10 twice higher than previously predicted if the progenitor number extends down to smaller masses .",
        "rewrite_text": "We summarize the findings of our study on binary models for gamma-ray bursts (GRBs) originating from progenitors with masses between 8 and 40 solar masses, which are anticipated to generate GRB jets detectable at cosmological distances. Our analysis indicates that these systems evolve into double-degenerate binaries comprising two white dwarfs or helium stars prior to their supernova explosion. This explosion occurs as a result of the merger of the components driven by gravitational wave radiation. In certain instances, we also observe that the system passes through an intermediate phase where one component collapses into a black hole while the other undergoes a supernova explosion. This scenario may clarify the observed mass gap between typical core-collapse supernovae and GRBs. Additionally, our calculations suggest that the total number of such events annually could be as much as twice the previously estimated number if we consider progenitors with smaller masses.",
        "ori-fast-z-score": -0.12216944435630522,
        "water-fast-z-score": 3.298574997620241,
        "rewrite-fast-z-score": 1.1952286093343936
    },
    {
        "original_text": "We present new observations and analysis of the infrared emission lines in the spectra of two yellow hypergiants, IRC+10420 and AFGL2136. We find that these objects have very high mass-loss rates (10^-6 to 10^-5 Msun/yr) with outflow velocities ranging between 100-200 km/sec. The observed line profiles are consistent with an expanding shell model for the wind. In addition we detect several forbidden transitions which indicate the presence of highly ionized species such as Fe + , Si ++ , S ++ . These ions may be formed by photoionization or collisional ionization processes within the stellar winds. \n \n Keywords: Yellow Hypergiants; Circumstellar Envelopes; Mass loss rate; Outflows; Emission Lines; IRAS 08544-4431. Astronomy & Astrophysics manuscript no. aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope",
        "watermark_text": "We report new experiments and investigation of the infrared absorption tracks in the spectra of two yellow hypergiants , IRC + 10420 and AFGL2136 . We see that these objects have very high mass - loss rates ( 10 ^ - 6 to 10 ^ - 5 Msun / yr ) with outflow velocities ranging between 100 - 200 kilometers / sec .The observed line profiles are compatible with an moving shell model for the wind . In addition we find various forbidden transitions which demonstrate the presence of highly ionized compounds such as Fe + , Si + + , S + + .These ions may be formed by photoionization or collisional ionization processes within the stellar winds . Keywords : Yellow Hypergiants ; Circumstellar Envelopes ; Mass loss rate ; Outflows ; Emission Lines ; IRAS 08544 - 4431 .Astronomy & Astrophysics manuscript no.aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope",
        "rewrite_text": "We present new experiments and investigations into the infrared absorption features in the spectra of two yellow hypergiants, IRC + 10420 and AFGL 2136. Our findings indicate that these stars exhibit extremely high mass-loss rates, ranging from 10^-6 to 10^-5 M⊙ per year, with outflow velocities between 100 and 200 kilometers per second. The observed line profiles support a moving shell model for the stellar wind. Additionally, we identify several forbidden transitions that indicate the presence of highly ionized species such as Fe+, Si++, and S++. These ions may arise from photoionization or collisional ionization processes occurring within the stellar winds. Keywords: Yellow Hypergiants; Circumstellar Envelopes; Mass Loss Rate; Outflows; Emission Lines; IRAS 08544-4431. Astronomy & Astrophysics manuscript no. aa20031118, May 31, 2003. The chemical composition of the circumstellar envelope.",
        "ori-fast-z-score": 0.7453559924999299,
        "water-fast-z-score": 4.323064756499593,
        "rewrite-fast-z-score": 1.6641005886756874
    },
    {
        "original_text": "We present an analytical solution to the problem of bending thin elastic shells with variable thickness under surface loads, which is applicable to the case where the shell s thickness varies by several orders of magnitude over its radius and where the load distribution is not necessarily axisymmetric.  We show that in this case it is possible to obtain accurate results using only two parameters instead of three as was previously thought necessary (the third parameter being the ratio between the maximum and minimum values of the shell s thickness). The new formulation allows us to calculate the deflection of the shell at any point on its surface without having to solve additional equations or perform numerical integration. This makes our approach much faster than previous methods while retaining high accuracy. Our method can be used to model the response of the Earth s crust to tectonic stresses and other processes such as volcanic loading and sedimentary deposition. It also has applications in geophysics beyond Earth sciences including planetary science, astrophysics and seismology. \nTheory\n\nIn order to study the deformation of the Earth s crust we need to know how the stress field changes across different regions of the planet. In particular, we are interested in understanding how the stress field evolves during plate boundary interactions like subduction zones and transform faults. To do so, we use the theory of elasticity to find solutions to problems involving the interaction between plates and their underlying mantle. However, solving these problems analytically requires simplifying assumptions about the geometry of the system and the mechanical properties of the materials involved. \n\nOne important simplification made when studying the mechanics of plate boundaries is to assume that they behave as if they were composed of thin elastic shells. These shells have been shown to provide good approximations to more realistic models of plate boundaries because they allow for rapid calculations of the stress fields within them. For example, Figure 1 shows a comparison between the predictions obtained using a simple spherical shell model and those produced by a finite element model of the San Andreas Fault System.\n\nFigure 1: Comparison between the predicted displacements along the San Andreas fault calculated using a spherical shell model (blue line) and a finite element model (red dots).",
        "watermark_text": "We present an analytical solution to the issue of stretching thin elastic pieces with variable size under surface loads , which is applicable to the case where the shell s thickness differs by many orders of magnitude over its radius and where the load distribution is not necessarily axisymmetric . We see that in this situation it is possible to obtain precise conclusions using only two parameters instead of three as was formerly thought required ( the third parameter being the proportion between the maximum and minimum values of the shell s thickness ) .The revised formulation enables us to estimate the deflection of the shell at any point on its surface without having to correct additional equations or undergo numerical integration . This gives our approach much quick than prior methods while retaining high sensitivity .Our model can be used to model the response of the Earth s crust to tectonic stresses and other processes such as geological loading and sedimentary deposition . It additionally has uses in geophysics beyond Earth studies including planetary scientists , astrophysics and seismology .Theory In order to study the deformation of the Earth s crust we require to see how the strain field shifts across different regions of the planet . In particular , we are concerned in understanding how the strain field evolves during plate boundary interactions like subduction zones and transform faults .To do so , we utilize the principle of elasticity to find solutions to problems concerning the interaction between plates and their underlying mantle . However , solving these problems analytically takes simplifying theories about the topology of the process and the structural properties of the materials involved .One important simplification taken when researching the mechanics of sheet borders is to assume that they react as if they were consisting of short elastic shells . These shells have been shown to provide better approximations to more realistic descriptions of plate boundaries because they allow for rapid determination of the strain fields within them .For instance , Figure 1 shows a comparison between the estimates obtained using a simple spherical shell model and those generated by a finite element model of the San Andreas Fault System . Figure 1 : Comparison between the expected displacements along the San Andreas fault calculated using a circular shell model ( blue line ) and a finite element model ( red stripes ) .",
        "rewrite_text": "We offer an analytical solution to the challenge of stretching thin elastic materials of varying sizes under surface loads. This solution applies to scenarios in which the thickness of the shell varies significantly across its radius and the load distribution may not be axisymmetric. Notably, we demonstrate that precise conclusions can be drawn using only two parameters, contrary to the previous belief that three were necessary (the third being the ratio of maximum to minimum shell thickness). Our refined formulation allows for the calculation of deflection at any point on the shell's surface without the need for additional corrections or numerical integration, making our approach considerably faster than earlier methods while maintaining high sensitivity. This model is applicable in modeling the Earth's crust's response to tectonic stresses and other phenomena such as geological loading and sediment deposition. It also extends its utility to geophysical studies beyond Earth, benefiting disciplines such as planetary science, astrophysics, and seismology.\n\nTo explore the deformation of the Earth's crust, we need to analyze how the strain field varies across different regions of the planet. Our focus is on understanding the evolution of the strain field during interactions at plate boundaries, such as subduction zones and transform faults. Using the principle of elasticity, we seek solutions to problems related to the interactions between tectonic plates and their underlying mantle. However, deriving analytical solutions requires us to simplify our theories concerning the topology of these processes and the structural characteristics of the involved materials. A key simplification in studying the mechanics of plate boundaries is to treat them as if composed of short elastic shells. This approach has proven to yield more accurate approximations of plate boundaries, facilitating quick assessments of the strain fields within. For example, Figure 1 illustrates a comparison between displacement estimates along the San Andreas Fault obtained using a simple spherical shell model (blue line) and a finite element model (red stripes).",
        "ori-fast-z-score": -1.270639657678291,
        "water-fast-z-score": 8.198915917499228,
        "rewrite-fast-z-score": 2.401922307076307
    },
    {
        "original_text": "We present the theory for anomalous diffusion in terms of fractional Fokker-Planck equations and apply it to study the time evolution of chemical reactions occurring on realistic self-affined fractals, such as porous media or biological tissues. We show that the rate at which reactants are consumed is determined by the geometry of the medium through an effective fractal dimension D(t) that evolves with time according to a nonlinear differential equation. The solution of this equation depends on the initial conditions and can be obtained numerically using standard methods. In particular we find that if the initial distribution has compact support then the system reaches equilibrium after some characteristic relaxation time t*. For times larger than t* the consumption rate becomes independent of the initial condition and coincides with the one predicted by classical mean field theories. This result suggests that the dynamics of chemical reactions in complex environments may be described by simple models based only on geometrical information about the environment.",
        "watermark_text": "We introduce the principle for anomalous diffusion in terms of fractional Fokker - Planck equations and application it to study the period evolution of biological compounds resulting on ideal self - affined fractals , such as porous material or biological tissues . We see that the speed at which reactants are consumed is chosen by the topology of the medium through an efficient fractal dimension D ( t ) that evolves with time according to a nonlinear integral equation .The solving of this equation depends on the first conditions and can be obtained numerically using conventional methods . In particular we find that if the first distribution has compact support then the system reaches stability after some characteristic relaxation time t * .For times bigger than t * the consumption level appears independent of the first situation and coincides with the one expected by classical mean field theories . This result suggests that the dynamics of chemical processes in complex environments could be described by simple models relying only on geometrical information about the surroundings .",
        "rewrite_text": "We present the principle of anomalous diffusion through fractional Fokker-Planck equations and apply it to analyze the temporal evolution of biological compounds, which exhibit ideal self-affine fractal characteristics, such as in porous materials and biological tissues. Our findings indicate that the rate at which reactants are consumed is determined by the medium's topology, represented by an efficient fractal dimension D(t) that evolves over time according to a nonlinear integral equation. The solution to this equation depends on the initial conditions and can be numerically approximated using standard methods. Notably, we observe that if the initial distribution has compact support, the system attains stability after a specific relaxation time, denoted as t*. For times exceeding t*, the consumption level appears to be independent of the initial conditions and aligns with predictions from classical mean field theories. This outcome implies that the dynamics of chemical processes in complex environments can be effectively described using simplified models that depend solely on the geometric characteristics of the surroundings.",
        "ori-fast-z-score": -0.9058216273156765,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": 0.562543950463012
    },
    {
        "original_text": "In this work, we propose an encounter based worm interaction model to evaluate the performance of different node characteristics in terms of their ability to detect and prevent worms spreading over mobile ad hoc networks (MANETs). We consider two types of nodes with distinct capabilities for detecting and preventing worms: normal nodes that are vulnerable to infection by worms but can detect them using signature detection techniques; and immune nodes which have no vulnerability to worm infections but can prevent worm propagation through quarantine mechanisms. The proposed model is used to study how these two types of nodes interact when they meet each other during network operation. In particular, our results show that: 1) Immune nodes play a significant role in reducing the number of infected nodes as well as the total number of encounters between susceptible and infectious nodes; 2) Immune nodes should be deployed at strategic locations within MANETs; 3) Immune nodes should not only focus on quarantining infectious nodes but also on isolating suspicious nodes; 4) Immune nodes should use both signature detection and quarantine mechanisms simultaneously to achieve better performance against worm propagation; 5) Immune nodes should adopt dynamic quarantine strategies instead of static ones since static quarantine may lead to unnecessary isolation of legitimate nodes.",
        "watermark_text": "In this project , we propose an encounter centered worm engagement theory to analyze the performance of different node characteristics in terms of their capabilities to identify and avoid worms distribution over mobile ad hoc networks ( MANETs ) . We consider two forms of nodes with distinct capabilities for detecting and preventing worms : normal networks that are susceptible to disease by viruses but can identify them utilizing pattern screening strategies ; and immune nodes which have no sensitivity to virus diseases but can prevent worm transmission through quarantine mechanisms .The proposed theory is utilized to study how these two kind of nodes interact when they meet each other during network activity . In particular , our findings show that : 1 ) Immune networks serve a substantial importance in reducing the quantity of infected connections as well as the total quantity of visits between vulnerable and infectious networks ; 2 ) Immune networks should be deployed at strategic locations within MANETs ; 3 ) Immune networks should not only focus on quarantining infectious networks but also on isolating suspect nodes ; 4 ) Immune networks should use both signature detection and quarantine mechanisms separately to achieve good efficiency against worm transmission ; 5 ) Immune networks should implement dynamic quarantine techniques instead of static ones since static quarantine may contribute to inappropriate isolation of genuine nodes .",
        "rewrite_text": "In this project, we introduce a theory centered on encounters to analyze the effectiveness of various node characteristics in detecting and preventing the spread of worms across mobile ad hoc networks (MANETs). We focus on two types of nodes with differing capabilities: normal nodes, which are vulnerable to virus attacks but can detect them using pattern recognition techniques, and immune nodes, which are resistant to virus infections and can stop the spread of worms through quarantine methods. Our proposed theory examines the interactions between these two types of nodes when they come into contact during network operations. Specifically, our research reveals several key findings: 1) Immune nodes play a crucial role in reducing the number of infected connections and the frequency of interactions between vulnerable and infected networks; 2) Immune nodes should be strategically deployed within MANETs; 3) Immune nodes should prioritize not only the quarantine of infected nodes but also the isolation of suspicious nodes; 4) Immune nodes should employ both signature detection and quarantine methods separately to enhance their effectiveness against worm propagation; 5) Immune nodes should adopt dynamic quarantine strategies over static ones, as static quarantines may lead to the wrongful isolation of legitimate nodes.",
        "ori-fast-z-score": -1.4237369936287485,
        "water-fast-z-score": 8.067842963896242,
        "rewrite-fast-z-score": 0.4975185951049946
    },
    {
        "original_text": "The discovery space for astronomical research is vast, with many different types of surveys being conducted at all wavelengths across the electromagnetic spectrum. In this talk I will discuss how wide field optical imaging surveys have been used to discover new classes of objects in our Universe such as quasars, galaxies, clusters of galaxies, supernovae, gamma ray bursts etc., and also how these surveys are now providing data on dark energy which drives cosmic acceleration. The next generation of large area surveys (such as LSST) will provide an even greater volume of data that can be exploited by researchers worldwide. This talk will give examples of some recent results obtained using data from current and past surveys including the Sloan Digital Sky Survey (SDSS), Panoramic Survey Telescope & Rapid Response System 1 (Pan-STARRS1), Dark Energy Survey (DES), VISTA Kilo-Degree Infrared Galaxy survey (VIKING).",
        "watermark_text": "The discovery area for astronomical investigations is vast , with many various types of surveys being performed at all wavelengths across the electromagnetic spectrum . In this talk I will explore how wide field visual imaging observations have been used to find new classes of bodies in our Universe such as quasars , galaxies , clusters of stars , supernovae , alpha ray bursts etc . , and also how these observations are now offering data on dark energy which explains cosmic acceleration .The future generation of large area surveys ( such as LSST ) will provide an much larger volume of statistics that can be exploited by researchers worldwide . This discussion will provide descriptions of some latest findings obtained using data from recent and previous surveys including the Sloan Digital Sky Survey ( SDSS ) , Panoramic Survey Telescope & Rapid Response System 1 ( Pan - STARRS1 ) , Dark Energy Survey ( DES ) , VISTA Kilo - Degree Infrared Galaxy survey ( VIKING ) .",
        "rewrite_text": "The realm of astronomical research is immensely expansive, encompassing a wide variety of surveys conducted across all wavelengths of the electromagnetic spectrum. In this presentation, I will discuss how wide-field visual imaging has been instrumental in discovering new classes of celestial bodies, including quasars, galaxies, star clusters, supernovae, and gamma-ray bursts. Additionally, I will highlight how these observations are providing valuable data on dark energy, which accounts for the cosmic acceleration we observe. The next generation of extensive surveys, such as the Large Synoptic Survey Telescope (LSST), is set to generate a significantly larger dataset for researchers around the globe to analyze. This talk will also showcase some of the latest discoveries made using data from recent and previous surveys, including the Sloan Digital Sky Survey (SDSS), Pan-STARRS1, the Dark Energy Survey (DES), and the VISTA Kilo-Degree Infrared Galaxy Survey (VIKING).",
        "ori-fast-z-score": -1.8325416653445783,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": -0.5
    },
    {
        "original_text": "We relate the notion of information-theoretic security to that of computational indistinguishability, and show how this relationship can be used in practice for proving security properties of cryptographic protocols.  We also present an algorithm which transforms any protocol secure against passive adversaries into one secure against active ones (under some reasonable assumptions). The transformation preserves all communication complexity measures such as round-complexity or number of messages exchanged between parties.   Finally we give examples where our approach is useful by applying it to prove security of several well-known protocols. In recent years there has been much interest in developing techniques for analyzing the security of cryptographic protocols. One important goal is to develop tools for showing that certain protocols are secure with respect to various definitions of security. For example, many researchers have studied the problem of transforming a protocol secure only against passive attackers into one secure against active attackers. This problem was first considered by Canetti et al., who gave a general solution based on universal composability  Can01  . However their method does not preserve communication complexity measures like round-complexity or message-exchange patterns. Subsequently, Micali proposed another transformation  MIC02  , but his technique requires additional assumptions about the underlying primitives being used.",
        "watermark_text": "We relate the notion of communication - theoretic integrity to that of computational indistinguishability , and suggest how this relationship can be used in practice for determining security characteristics of cryptographic protocols . We also demonstrate an algorithm which transforms any protocol secure against passive adversaries into one secure against active ones ( under some reasonable assumptions ) .The transformation maintains all transmission complexity measures such as round - complexity or number of communication received between parties . Finally we give instance where our approach is utilized by using it to prove security of several well - famous protocols .In past decades there has been much interest in improving procedures for evaluating the safety of cryptographic protocols . One important aim is to develop tools for showing that particular protocols are secure with regard to several definitions of security .For instance , many scholars have researched the question of converting a protocol secure only against passive attackers into one stable against active attackers . This problem was first investigated by Canetti et al . , who offered a general solution based on universal composability Can01 .However their method does not retain transmission diversity values like round - complexity or message - transfer patterns . Subsequently , Micali proposed another transformation MIC02 , but his method needs additional observations about the underlying primitives being used .",
        "rewrite_text": "We connect the concept of communication-theoretic integrity to that of computational indistinguishability, and we propose ways to leverage this connection in practice to assess the security characteristics of cryptographic protocols. Additionally, we present an algorithm that can convert any protocol, which is secure against passive adversaries, into one that is secure against active adversaries, given certain reasonable assumptions. This transformation preserves all metrics related to transmission complexity, such as round complexity and the number of messages exchanged between parties. Moreover, we provide examples demonstrating the practical application of our approach by proving the security of several well-known protocols. Over the past few decades, there has been a growing interest in enhancing the evaluation procedures for the safety of cryptographic protocols. A key objective is to create tools that can demonstrate the security of specific protocols in relation to various security definitions. For example, researchers have explored ways to transform protocols that are secure only against passive attackers into those that can withstand active adversaries. This challenge was initially addressed by Canetti et al., who proposed a general solution founded on universal composability. However, their approach does not preserve the diversity of transmission metrics, such as round complexity and message patterns. Following this, Micali introduced another transformation, but his method requires additional insights regarding the underlying primitives being utilized.",
        "ori-fast-z-score": -0.2873478855663454,
        "water-fast-z-score": 7.120653320005384,
        "rewrite-fast-z-score": 0.38138503569823695
    },
    {
        "original_text": "We present the results of our analysis of U Gem, which is one of the brightest and most studied dwarf novae in outbursts. We have used all available photometric data to determine its ephemeris with an accuracy better than 0.1 s. The light curve shows that there are two maxima per orbit during quiescence as well as in outburst. This fact allows us to obtain accurate values of the mass ratio q = M2/M1 between the components of this binary system. Using these new values we can estimate the absolute masses of both stars (0.7 ± 0.05M⊙) and their radii (3.5 ± 0.2R⊙). These values agree very well with those obtained by other authors using different methods. \n \n Keywords: Dwarf Novae - Orbital Period Changes - Mass Ratio - Photometry - Ephemerides",
        "watermark_text": "We present the conclusion of our analysis of U Gem , which is one of the brightest and most studied dwarf novae in outbursts . We have utilized all available photometric data to estimate its ephemeris with an precision better than 0 . 1 s . The light curve shows that there are two maxima per orbit during quiescence as well as in outburst .This fact allows us to obtain precise measures of the mass ratio q = M2 / M1 between the parts of this binary system . Using these new values we can calculate the absolute masses of both stars ( 0 . 7 ± 0 . [UNK] ) and their radii ( 3 . 5 ± 0 . [UNK] ) .These measurements agree very best with those achieved by other researchers using separate methods . Keywords : Dwarf Novae - Orbital Period Changes - Mass Ratio - Photometry - Ephemerides",
        "rewrite_text": "We present the findings of our analysis of U Gem, one of the brightest and most extensively studied dwarf novae during outbursts. Utilizing all available photometric data, we have determined its ephemeris with an accuracy of better than 0.1 seconds. The light curve indicates that there are two maxima per orbit, both during quiescence and in outburst. This characteristic enables us to obtain precise measurements of the mass ratio \\( q = \\frac{M_2}{M_1} \\) for the components of this binary system. With these new values, we can also calculate the absolute masses of both stars as \\( 0.7 \\pm 0. \\,[UNK] \\) and their radii as \\( 3.5 \\pm 0. \\,[UNK] \\). These results are highly consistent with measurements obtained by other researchers using different methods. \n\nKeywords: Dwarf Novae - Orbital Period Changes - Mass Ratio - Photometry - Ephemerides",
        "ori-fast-z-score": -0.7293249574894728,
        "water-fast-z-score": 3.015113445777636,
        "rewrite-fast-z-score": -0.29488391230979427
    },
    {
        "original_text": "We study the ground-state properties of two-flavor color superconducting (2SC) quark matter at finite density and temperature by using an effective chiral model with vector interaction, which is derived from QCD under the mean-field approximation. We find that there exists a new type of 2SC phase where quarks are paired into diquark condensates with different colors but same flavor. This novel phase has been named as Larkin-Ovchinnikova-Fulde-Ferreell (LOFF) state because it was first proposed to describe superfluidity in nuclear systems. In this LOFF state, we show that the gap parameter for pairing between quarks with opposite momenta depends on their relative angle. The magnitude of the gap decreases rapidly when they move away from each other along the Fermi surface. As a result, the energy gap vanishes completely near the boundary of the Brillouin zone.",
        "watermark_text": "We research the ground - state properties of two - flavor color superconducting ( 2SC ) quark matter at finite density and heat by using an efficient chiral description with vector coupling , which is generated from QCD under the mean - field approximation . We see that there exists a new kind of 2SC phase where quarks are paired into diquark condensates with various shades but same flavor .This novel mode has been called as Larkin - Ovchinnikova - Fulde - Ferreell ( LOFF ) state because it was first suggested to explain superfluidity in nuclear systems . In this LOFF state , we find that the gap parameter for pairing between quarks with opposite momenta depends on their relative angle .The magnitude of the gap falls strongly when they go away from each other along the Fermi surface . As a result , the power gap vanishes totally near the boundary of the Brillouin zone .",
        "rewrite_text": "We investigate the ground-state properties of two-flavor color superconducting (2SC) quark matter at finite density and temperature using an effective chiral model with vector coupling, derived from quantum chromodynamics (QCD) under the mean-field approximation. Our findings reveal a novel type of 2SC phase in which quarks form diquark condensates that possess different angular configurations but share the same flavor. This unique state is referred to as the Larkin-Ovchinnikov-Fulde-Ferrell (LOFF) phase, originally proposed to explain superfluidity in nuclear matter. In the LOFF state, we observe that the pairing gap for quarks with opposite momenta is influenced by their relative angles. Notably, the gap magnitude decreases significantly as the quarks move apart along the Fermi surface, leading to a complete vanishing of the gap near the edges of the Brillouin zone.",
        "ori-fast-z-score": -1.6924558427507104,
        "water-fast-z-score": 3.5151005964822444,
        "rewrite-fast-z-score": 1.4084056792618558
    },
    {
        "original_text": "We show that the exchange-only optimized potentials (OEPs) are not equivalent to the Kohn-Sham (KS) method in general, even if one uses an exact density functional for the exchangecorrelation energy. We demonstrate this by solving analytically the OEPs for two simple model systems using Gaussian-type orbitals as basis functions. The results obtained within both approaches differ significantly. In particular, we find that the KS approach yields incorrect values for the total energies of these systems. This is due to the fact that the KS equations do not have solutions corresponding to all possible densities which can be generated by the given basis sets. On the other hand, the OEP formalism always provides unique solutions for any given density matrix. Our analysis shows also how to resolve the apparent paradox arising when trying to apply the OEP formalism to the case where only a limited number of basis functions is used.",
        "watermark_text": "We see that the transfer - only optimized potentials ( OEPs ) are not equivalent to the Kohn - Sham ( KS ) method in general , even if one uses an precise density functional for the exchangecorrelation energy . We showed this by solving analytically the OEPs for two simple model models using Gaussian - class orbitals as basis maps .The results derived within both approaches differ significantly . In particular , we find that the KS approach produces unreliable estimates for the total energies of these systems .This is due to the fact that the KS coefficients do not have solutions equivalent to all possible densities which can be generated by the particular basis sets . On the other hand , the OEP formalism certainly presents specific solutions for any certain density matrix .Our study shows also how to overcome the apparent paradox emerging when trying to apply the OEP formalism to the case where only a small number of basis functions is utilized .",
        "rewrite_text": "We find that transfer-only optimized potentials (OEPs) are generally not the same as the Kohn-Sham (KS) method, even when a highly accurate density functional for the exchange-correlation energy is employed. This conclusion is based on our analytical solutions of the OEPs for two simple model systems using Gaussian-type orbitals as the basis set. The outcomes from both methods reveal significant differences. Notably, the KS approach yields unreliable total energy estimates for these systems because the KS coefficients do not correspond to all possible densities that can be generated by the given basis sets. In contrast, the OEP framework provides specific solutions for any chosen density matrix. Our study also addresses how to resolve the apparent contradiction that arises when applying the OEP formalism with a limited number of basis functions.",
        "ori-fast-z-score": -1.2309149097933272,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": -1.778001778002667
    },
    {
        "original_text": "The STEREO experiment is designed to search for sterile neutrino oscillations in the CERN Neutrinos to Gran Sasso (CNGS) beamline using two identical detectors located near the source and far away, respectively. \n \n The detector consists of an active target made up by three concentric cylinders filled with liquid scintillator surrounded by a passive shielding composed of lead bricks and iron plates. A total mass of about 1 tonne has been used for each detector. The innermost cylinder contains 0.5 tonnes of pseudocumene doped with PPO as solvent and POPOP as wavelength shifter. The second and third layers contain 0.3 tonnes of mineral oil per layer. Each detector is instrumented with 192 photomultiplier tubes arranged on eight rings around its central axis. In addition, there are four planes of plastic scintillators surrounding the outer part of the first cylinder which provide additional information on charged particles entering or leaving the active volume.",
        "watermark_text": "The STEREO study is designed to search for sterile neutrino oscillations in the CERN Neutrinos to Gran Sasso ( CNGS ) beamline use two similar detectors located near the origin and far away , respectively . The detector consists of an active target made up by three concentric cylinders loaded with liquid scintillator surrounded by a passive shielding composed of lead cement and iron plates .A total mass of about 1 tonne has been used for each sensor . The innermost cylinder contains 0 . 5 tonnes of pseudocumene doped with PPO as solvent and POPOP as frequency shifter .The second and third layers contain 0 . 3 tonnes of mineral oil per layer . Each sensor is instrumented with 192 photomultiplier frames arranged on eight loops around its central axis .In addition , there are four zones of plastic scintillators surrounding the exterior part of the first cylinder which provide additional information on charged particles entering or left the active volume .",
        "rewrite_text": "The STEREO study aims to investigate sterile neutrino oscillations using the CERN Neutrinos to Gran Sasso (CNGS) beamline. It employs two similar detectors positioned near the source and at a greater distance, respectively. Each detector features an active target composed of three concentric cylinders filled with liquid scintillator, encased in a passive shielding made of lead cement and iron plates, with a total mass of approximately 1 tonne per sensor. The innermost cylinder contains 0.5 tonnes of pseudocumene, which is doped with PPO as a solvent and POPOP as a frequency shifter. The second and third layers each contain 0.3 tonnes of mineral oil. Each sensor is equipped with 192 photomultiplier tubes arranged in eight loops around the central axis. Additionally, four zones of plastic scintillators surround the outer part of the first cylinder, providing extra information on charged particles entering or exiting the active volume.",
        "ori-fast-z-score": -0.12403473458920847,
        "water-fast-z-score": 3.8450767722654624,
        "rewrite-fast-z-score": 1.25
    },
    {
        "original_text": "Globular clusters are dense stellar systems that contain thousands to millions of stars, and may be the oldest gravitationally bound objects known.  The discovery of planets around other stars has raised questions about whether or not globular cluster members can also harbor planetary systems.   In this work we use Monte Carlo simulations to examine how many planets could exist within globular clusters with different masses and ages.  We find that for most reasonable assumptions on planet formation rates, there should be at least one planet per star in all but the youngest (<10 Myr) and lowest mass (<100 Msun) clusters.  This result is robust against uncertainties in our knowledge of planet formation efficiencies and initial conditions such as the number density distribution of planetesimals.  Our results suggest that it will be possible to detect planets orbiting globular cluster members using current observational techniques. Keywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "watermark_text": "Globular complexes are dense stellar structures that host hundreds to millions of stars , and may be the earliest gravitationally locked objects known . The observation of planets around other stars has raised questions about whether or not globular cluster groups can also harbor planetary structures .In this research we utilize Monte Carlo simulations to examine how many worlds could occur within globular complexes with various masses and periods . We see that for most reasonable assumptions on planet development rates , there should be at least one planet per star in all but the youngest ( < 10 Myr ) and lowest mass ( < 100 Msun ) clusters .This result is robust against uncertainties in our know of planet development efficiencies and first situations such as the number density density of planetesimals . Our results propose that it will be possible to identify planets orbiting globular cluster elements using current observational techniques .Keywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "rewrite_text": "Globular clusters are compact stellar formations that contain hundreds to millions of stars, and they may represent some of the earliest gravitationally bound objects known. The discovery of planets orbiting other stars has prompted inquiries into the possibility of planets existing within globular clusters as well. In this study, we employ Monte Carlo simulations to investigate the potential number of planets that could form in globular clusters with varying masses and periods. Our findings indicate that, under most reasonable assumptions regarding planet formation rates, there should be at least one planet for every star in all but the youngest (less than 10 million years) and lowest mass (less than 100 solar masses) clusters. This conclusion remains strong despite uncertainties regarding our understanding of planetary formation efficiencies and initial conditions, such as the density of planetesimals. Our results suggest that it may be feasible to detect planets orbiting stars in globular clusters using contemporary observational methods. \nKeywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 6.527299120066193,
        "rewrite-fast-z-score": 0.1125087900926024
    },
    {
        "original_text": "We discuss the gauge invariance properties of effective actions obtained by integrating out heavy degrees of freedom, such as quarks and gluons, within supersymmetric Yang-Mills theories. We show that these actions are not generally invariant under local gauge transformations unless certain conditions on their form are satisfied. These results have important consequences for the construction of gauge-invariant observables in supersymmetric gauge theories. They also provide an explanation why it is possible to construct nontrivial superpotentials even though supersymmetry does not allow any explicit breaking terms at tree level. Finally we argue that our findings can be used to resolve some puzzling features observed recently in lattice simulations of N = 1 supersymmetric QCD with four flavors. Supersymmetric Yang-Mills theories play an important role both in particle physics and string theory. Their low-energy dynamics is described by an effective action which contains all quantum corrections due to the integration over heavy fields like quarks or gluons. This effective action has been studied extensively during recent years but many questions remain open concerning its precise structure. One particular issue concerns the question whether this action is gauge invariant. It was shown already more than twenty years ago  1  that if one integrates out only massive fermions then the resulting effective action is indeed gauge invariant. However, when including also massive bosonic degrees of freedom there exist counterexamples where the effective action fails to be gauge invariant  2  . Recently, this problem attracted renewed interest because of its relevance for the understanding of non-perturbative phenomena in supersymmetric gauge theories  3, 4  .\nIn this work we study the gauge invariance properties systematically using functional methods. Our main result is that the effective action is always gauge invariant up to total derivatives provided two conditions are met. First, the effective action must contain no higher-order time-derivatives acting on the gauge field. Second, the coefficients appearing in front of the various operators in the effective action should satisfy certain relations. For example, they cannot depend explicitly on the gauge coupling constant g. If either condition is violated then the effective action will fail to be gauge",
        "watermark_text": "We discuss the gauge invariance characteristics of effective actions obtained by combining out heavy degrees of liberty , such as quarks and gluons , within supersymmetric Yang - Mills theories . We see that these actions are not generally invariant under local gauge processes unless particular conditions on their form are fulfilled .These results have important implications for the creation of gauge - invariant observables in supersymmetric gauge fields . They especially offer an reason why it is easy to build nontrivial superpotentials even though supersymmetry does not enable any explicit breaking terms at tree level .Finally we claim that our findings can be used to explain some puzzling properties observed lately in crystal simulations of N = 1 supersymmetric QCD with four flavors . Supersymmetric Yang - Mills theories play an important role both in particle theory and string theory .Their low - energy dynamics is characterized by an efficient action which contains all quantum corrections due to the integration over heavy fields like quarks or gluons . This effective operation has been studied thoroughly during recent months but numerous concerns remain open concerning its precise shape .One particular issue concerns the question whether this action is gauge invariant . It was shown still more than twenty years previously 1 that if one integrates out only massive fermions then the resulting effective operation is indeed gauge invariant .However , when including also massive bosonic degrees of liberty there remain counterexamples where the effective operation fails to be gauge invariant 2 . Recently , this question attracted heightened interest because of its significance for the knowledge of non - perturbative processes in supersymmetric gauge physics 3 , 4 .In this study we study the gauge invariance effects systematically using functional technique . Our main consequence is that the effective act is usually gauge invariant up to maximum derivatives assuming two conditions are fulfilled .First , the effective act must include no higher - order time - derivatives acting on the gauge field . Second , the coefficients appearing in front of the various operators in the effective act should satisfy certain relations .For instance , they cannot depend explicitly on the gauge interaction function g . If either situation is violated then the effective act will fail to be gauge",
        "rewrite_text": "We examine the characteristics of gauge invariance in effective actions derived from integrating out heavy degrees of freedom, such as quarks and gluons, in supersymmetric Yang-Mills theories. Our analysis shows that these actions generally do not maintain local gauge invariance unless specific conditions regarding their formulation are met. These findings have significant implications for the construction of gauge-invariant observables within supersymmetric gauge fields. They particularly provide an explanation for the relative ease in developing nontrivial superpotentials, despite the absence of explicit breaking terms at tree level due to supersymmetry. Additionally, we propose that our results may clarify some puzzling behaviors recently observed in crystal simulations of N = 1 supersymmetric QCD with four flavors. Supersymmetric Yang-Mills theories are crucial in both particle physics and string theory. The dynamics at low energy are described by an effective action that encompasses all quantum corrections resulting from integrating out heavy fields like quarks and gluons. While this effective action has been extensively studied in recent months, several questions remain regarding its exact form. One key issue is whether this action is gauge invariant. Over twenty years ago, it was established that integrating out only massive fermions results in a gauge-invariant effective action. However, when massive bosonic degrees of freedom are included, counterexamples exist where the resulting action fails to maintain gauge invariance. Recently, this topic has garnered increased attention due to its importance in understanding non-perturbative phenomena in supersymmetric gauge theories. In this study, we systematically investigate the effects of gauge invariance using functional techniques. Our primary conclusion is that the effective action is generally gauge invariant up to maximum derivatives, provided two conditions are satisfied: firstly, it must not incorporate higher-order time derivatives acting on the gauge field; and secondly, the coefficients of the various operators in the effective action should adhere to specific relationships, such as avoiding explicit dependence on the gauge coupling constant \\( g \\). If either of these conditions is not met, the effective action will lose its gauge invariance.",
        "ori-fast-z-score": -1.9487823913892397,
        "water-fast-z-score": 7.698581455344939,
        "rewrite-fast-z-score": -1.0737509843863184
    },
    {
        "original_text": "We present an effective method to conserve the total energy and linear momentum in molecular dynamics (MD) simulations by introducing two types of potentials: one is used during the MD run, while another is only used when calculating the forces on particles at each time step. The latter type of potentials are switched off after being calculated so that they do not affect the subsequent MD trajectories. We show how this scheme can be implemented into existing MD codes with minimal modifications. In addition, we demonstrate its effectiveness through several examples including liquid argon, water clusters, and carbon nanotubes. Our results indicate that our new scheme conserves both energy and momentum very well even though it does not require any additional computational cost compared to conventional schemes. This work was supported by the National Natural Science Foundation of China under Grants No. 10874145 and No. 10934011 . Keywords: Energy-momentum conservation; Switching potentials; Molecular dynamics",
        "watermark_text": "We present an efficient method to conserve the total energy and linear momentum in polymer mechanics ( MD ) simulations by using two forms of potentials : one is utilized during the MD run , while another is only employed when calculating the forces on particles at each time step . The latter type of potentials are switched off after being calculated so that they do not alter the subsequent MD trajectories .We see how this scheme can be applied into older MD codes with minimal modifications . In addition , we test its effectiveness through several examples representing liquid argon , water complexes , and carbon nanotubes .Our results show that our new project conserves both heat and momentum very best even though it does not require any additional mathematical price compared to conventional proposals . This research was supported by the National Natural Science Foundation of China under Grants No .10874145 and No.10934011 .Keywords : Energy - momentum conservation ; Switching potentials ; Molecular dynamics",
        "rewrite_text": "We introduce an innovative approach for preserving total energy and linear momentum in molecular dynamics (MD) simulations involving polymers. This method employs two types of potentials: one is active during the MD simulation, while the other is utilized solely for computing the forces on particles at each time step. The second type of potential is deactivated after computation to ensure it does not influence the subsequent MD trajectories. Our implementation demonstrates that this approach can be integrated into existing MD codes with minimal adjustments. Furthermore, we evaluate its efficacy through various examples, including liquid argon, water complexes, and carbon nanotubes. The results indicate that our method achieves excellent conservation of energy and momentum without additional computational cost compared to traditional methods. This research was funded by the National Natural Science Foundation of China under Grants No. 10874145 and No. 10934011. Keywords: Energy-momentum conservation; Switching potentials; Molecular dynamics.",
        "ori-fast-z-score": -1.1523319193960637,
        "water-fast-z-score": 4.993438317382943,
        "rewrite-fast-z-score": -2.25
    },
    {
        "original_text": "We present the first supersymmetric generalization of the isometry group of string theory, which we call the hidden dimensions (HD) group.  The HD group contains all known symmetries of string theory as subgroups, including the Poincare symmetry in ten spacetime dimensions, the SO(32) gauge symmetry of heterotic strings, and the E8xE8 or SO(16)xSO(16) gauge symmetry of type IIA or IIB superstrings respectively.   We show that the HD group can be realized on any closed Riemann surface with genus g > 1 by constructing an explicit action for it on the space of conformal field theories associated to this surface.  This construction generalizes previous results obtained using orbifold techniques, but has several advantages over them.  In particular, our approach allows us to construct new models of string compactification without introducing extra massless states into the spectrum at tree level.   ...",
        "watermark_text": "We introduce the first supersymmetric generalization of the isometry group of string theory , which we call the concealed dimensions ( HD ) group . The HD group contains all known symmetries of string theory as subgroups , notably the Poincare symmetry in twelve spacetime dimensions , the SO ( 32 ) gauge symmetry of heterotic strings , and the E8xE8 or SO ( 16 ) xSO ( 16 ) gauge symmetry of type IIA or IIB superstrings respectively .We see that the HD group can be realized on any closed Riemann surface with genus g > 1 by constructing an explicit act for it on the space of conformal field theories associated to this surface . This construction generalizes earlier findings obtained using orbifold techniques , but has various advantages over them .In particular , our approach allows us to build modern representations of string compactification without putting extra massless states into the spectrum at tree level . . . .",
        "rewrite_text": "We present the first supersymmetric extension of the isometry group in string theory, which we refer to as the concealed dimensions (HD) group. This HD group encompasses all established symmetries of string theory as subgroups, including the Poincaré symmetry in twelve-dimensional spacetime, the SO(32) gauge symmetry of heterotic strings, and the E8×E8 or SO(16)×SO(16) gauge symmetry of type IIA and IIB superstrings, respectively. Notably, we demonstrate that the HD group can be realized on any closed Riemann surface with genus \\( g > 1 \\) by explicitly constructing an action for it on the space of conformal field theories linked to this surface. This construction builds on previous results achieved through orbifold techniques, yet it offers several advantages. Notably, our approach facilitates the development of modern representations of string compactification without introducing additional massless states into the spectrum at tree level.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 2.6887744785908154,
        "rewrite-fast-z-score": 0.12803687993289598
    },
    {
        "original_text": "We study the semiclassical dynamics of electrons in magnetic fields, which are described by the Dirac equation with spin-orbit coupling and Zeeman splitting. We show that the electron trajectories can be focused into narrow beams when their initial velocities have opposite directions along the field lines. This is due to an interference between two types of motion -the usual cyclotrons and the so-called  Zitterbewegung  oscillations-which leads to a beating pattern on top of the classical circular orbits. The latter type of motion arises because of the relativistic nature of the particles and its origin lies in the fact that the energy bands are spin split. Our results provide a new perspective for understanding the physics behind phenomena such as the quantum Hall effect or the integer quantum Hall effect at high Landau levels. \nI. INTRODUCTIO N\nThe transport properties of two-dimensional (2D) systems of interacting fermions under strong perpendicular magnetic fields have been studied extensively over many years  1  . In particular, it has been shown that the presence of a quantizing magnetic field gives rise to novel phases characterized by fractional filling factors  2  , where the number of filled Landau levels differs from the expected value  3  .\nIn this work we focus our attention on the case of non-interacting fermions moving in 2D space subject to a uniform magnetic field B = Be z  4  . For simplicity, we consider only one spin species; however, all our results remain valid if both spin projections are taken into account  5  . In addition, we assume that the Fermi level lies within the conduction band  6  . Under these conditions, the low-energy excitations around the Fermi surface are well-described by the massless Dirac Hamiltonian  7, 8  \nwhere v F denotes the Fermi velocity, σ i=x,y,z denote Pauli matrices acting on the spinor wave function Ψ(r), p x = −i∂/∂x and p y = −i∂/(−i∂y). Hereafter, we seth = 1 and e = 1. It should be noted that Eq. (1) \nII. ELECT",
        "watermark_text": "We research the semiclassical dynamics of electrons in magnetic fields , which are explained by the Dirac formula with spin - orbit coupling and Zeeman splitting . We see that the electron trajectories can be focused into narrow beams when their initial velocities have different directions along the field lines .This is due to an interference between two forms of movement - the usual cyclotrons and the so - called Zitterbewegung oscillations - which results to a beating sequence on top of the classical circular orbits . The latter type of movement occurs because of the relativistic behavior of the atoms and its origin lies in the fact that the power groups are momentum separated .Our results bring a new insight for studying the physics behind processes such as the quantum Hall impact or the integer quantum Hall impact at high Landau concentrations . I . INTRODUCTIO N The transport properties of two - dimensional ( 2D ) complexes of interacting fermions under strong diagonal magnetic waves have been studied thoroughly over numerous years 1 .In particular , it has been shown that the presence of a quantizing magnetic force gives rise to novel phases characterized by fractional filling variables 2 , where the proportion of filled Landau concentrations differs from the expected value 3 . In this research we focus our focus on the case of non - interacting fermions moving in 2D space subject to a uniform magnetic force B = Be z 4 .For simplicity , we treat only one spin species ; however , all our findings remain correct if both spinning projections are took into consideration 5 . In addition , we suppose that the Fermi level sits within the conduction band 6 .Under these conditions , the small - energy excitations around the Fermi surface are better - described by the massless Dirac Hamiltonian 7 , 8 where v F denotes the Fermi velocity , σ i = x , y , z define Pauli matrices acting on the spinor wave function Ψ ( r ) , r x = −i∂ / ∂x and p y = −i∂ / ( −i∂y ) . Hereafter , we seth = 1 and e = 1 .It should be mentioned that Eq . ( 1 ) II .ELECT",
        "rewrite_text": "We investigate the semiclassical dynamics of electrons in magnetic fields, using the Dirac equation that incorporates spin-orbit coupling and Zeeman splitting. Our findings indicate that electron trajectories can converge into narrow beams when their initial velocities vary in direction along the magnetic field lines. This phenomenon arises from the interference between two movement types: traditional cyclotron motion and the oscillations known as Zitterbewegung, which superimpose a beating pattern on the classical circular orbits. The latter type of motion stems from the relativistic behavior of electrons, originating from the separation of momentum power groups. Our results provide fresh insights into the underlying physics of phenomena such as the quantum Hall effect and the integer quantum Hall effect at high Landau level fillings.\n\n**I. INTRODUCTION** The transport properties of two-dimensional (2D) systems of interacting fermions under strong perpendicular magnetic fields have been studied extensively over the years. Notably, the application of a quantizing magnetic field leads to the emergence of novel phases characterized by fractional filling factors, where the occupancy of Landau levels deviates from the expected values. In this work, we focus on non-interacting fermions moving in 2D space subjected to a uniform magnetic field \\( B = Be_z \\). For simplicity, we consider only one spin species; however, our conclusions hold true even when both spin projections are taken into account. Additionally, we assume that the Fermi level lies within the conduction band. Under these assumptions, the low-energy excitations near the Fermi surface are best described by the massless Dirac Hamiltonian, where \\( v_F \\) represents the Fermi velocity, \\( \\sigma_i \\) (for \\( i = x, y, z \\)) are Pauli matrices acting on the spinor wave function \\( \\Psi(r) \\), and \\( r_x = -i \\partial / \\partial x \\) and \\( p_y = -i \\partial / (-i \\partial y) \\). We set \\( \\hbar = 1 \\) and \\( e = 1 \\). It is worth noting that Equation (1) holds. \n\n**II. ELECT**",
        "ori-fast-z-score": -1.007017629956027,
        "water-fast-z-score": 7.216959681351526,
        "rewrite-fast-z-score": 1.191759143062248
    },
    {
        "original_text": "We have performed ab initio molecular dynamics simulations to study the in-plane structure, order parameters, and surface tension of liquid Na(l) in contact with vacuum or solid NaCl (001). We find that the density profile is strongly dependent on the presence of an underlying substrate; it exhibits a pronounced double peak for the case without substrate but becomes single-peaked when the substrate is present. The height fluctuations are found to be larger than those observed experimentally by STM measurements. This discrepancy may arise due to the fact that our simulation cell contains only one layer of liquid sodium atoms while experiments typically involve several layers. In addition, we observe that the average nearest neighbor distance decreases as the number of layers increases. Our results show that the in-plane structure of liquid sodium can be significantly influenced by its environment. Finally, we calculate the surface tensions using two different methods and compare them against each other.",
        "watermark_text": "We have done ab initio polymer mechanics simulations to study the in - plane structure , order variables , and surface tension of liquid Na ( l ) in contact with vacuum or solid NaCl ( 001 ) . We see that the density profile is strongly dependent on the presence of an underlying substrate ; it displays a noticeable double peak for the case without substrate but appears single - peaked when the substrate is present .The altitude fluctuations are found to be larger than those observed experimentally by STM observations . This discrepancy may arise due to the fact that our modeling room contains only one layer of liquid sodium atoms while tests usually require many layers .In addition , we find that the average closest neighbor distance tends as the number of thickness increases . Our results show that the in - plane structure of liquid sodium can be greatly altered by its surroundings .Finally , we estimate the surface tensions use two different methods and contrast them against each other .",
        "rewrite_text": "We conducted ab initio polymer mechanics simulations to investigate the in-plane structure, order parameters, and surface tension of liquid sodium (Na) when in contact with either a vacuum or a solid NaCl (001) substrate. Our findings indicate that the density profile is highly influenced by the presence of the underlying substrate; specifically, it exhibits a marked double peak in the absence of a substrate but displays a single peak when the substrate is present. Additionally, we noted that the altitude fluctuations in our simulations are greater than those recorded experimentally via STM observations. This difference may be attributed to our model including only one layer of liquid sodium atoms, whereas experimental tests typically involve multiple layers. Furthermore, we observed that the average distance to the nearest neighbor increases with the thickness of the sodium layer. Our results suggest that the surrounding environment significantly alters the in-plane structure of liquid sodium. Finally, we assessed surface tensions using two different methods and compared the outcomes.",
        "ori-fast-z-score": 0.3333333333333333,
        "water-fast-z-score": 6.405028512341099,
        "rewrite-fast-z-score": 0.8728715609439696
    },
    {
        "original_text": "We present an overview of our recent work on Bayesian methods for reconstructing the large scale structures in the universe using galaxy redshift surveys. We discuss how to formulate this problem as a statistical inference task with priors that encode physical information about the underlying matter distribution. The posterior probability density function is then evaluated by applying Bayes  theorem together with Markov Chain Monte Carlo (MCMC) sampling techniques. In particular we focus on two different approaches which are based either on Gibbs sampling or Metropolis-Hastings algorithm. Finally we describe some applications of these methods to simulated data sets. This research was supported by NSF grant AST-0707763. Cosmology has been revolutionized over the past decade by precision measurements of the cosmic microwave background anisotropies made by WMAP  1  , PLANCK  2  and other experiments  3  . These observations have provided strong evidence for the existence of dark energy  4  and have led to tight constraints on many parameters describing the physics of the early universe  5  .\nHowever, despite their successes there remain several open questions regarding fundamental aspects of the standard model of cosmology  6  . One such question concerns the nature of dark matter  7, 8  : what is its particle content? What is its mass? How does it interact with ordinary matter?\nAnswering these questions requires detailed knowledge of the spatial distribution of dark matter throughout space and time  9  . Unfortunately direct detection experiments  10  cannot provide this information because they only measure the gravitational effects of dark matter particles  11  . Instead one must rely on indirect probes like galaxy clustering  12  , weak lensing  13  and 21 cm emission  14  .",
        "watermark_text": "We present an overview of our latest work on Bayesian methods for reconstructing the huge scale structures in the universe using galaxy redshift surveys . We discuss how to formulate this question as a statistical inference job with priors that encode physical information about the underlying matter distribution .The posterior likelihood density function is then evaluated by using Bayes relation together with Markov Chain Monte Carlo ( MCMC ) filtering approaches . In particular we focus on two different methods which are based either on Gibbs filtering or Metropolis - Hastings algorithm .Finally we explain some applications of these systems to modeled information sets . This research was supported by NSF grant AST - 0707763 .Cosmology has been revolutionized over the previous decade by precision observations of the cosmic microwave background anisotropies made by WMAP 1 , PLANCK 2 and other experiments 3 . These measurements have provided strong evidence for the existence of deep energy 4 and have led to strict constraints on numerous variables describing the physics of the early world 5 .However , despite their successes there remain many open questions regarding essential aspects of the standard theory of cosmology 6 . One such problem concerns the nature of deep material 7 , 8 : what is its particle content ?What is its weight ? How does it behave with normal matter ?Answering these problems demands clear information of the spatial distribution of dark matter throughout space and time 9 . Unfortunately direct detection experiments 10 cannot offer this data because they only measure the gravitational impacts of dark matter grains 11 .Instead one must rely on indirect probes like galaxy clustering 12 , soft lensing 13 and 21 cm emission 14 .",
        "rewrite_text": "We provide an overview of our recent research on Bayesian methods for reconstructing large-scale structures in the universe through galaxy redshift surveys. Our approach formulates this problem as a statistical inference task, utilizing priors that encapsulate physical information about the underlying matter distribution. We evaluate the posterior likelihood density function using Bayes' theorem in conjunction with Markov Chain Monte Carlo (MCMC) filtering techniques. Specifically, we concentrate on two distinct methods: one based on Gibbs filtering and the other on the Metropolis-Hastings algorithm. We also discuss applications of these methods to simulated datasets. This research is supported by NSF grant AST-0707763. \n\nIn the past decade, cosmology has been transformed by precise observations of cosmic microwave background anisotropies conducted by WMAP, PLANCK, and other experiments. These findings have provided compelling evidence for the existence of dark energy and have imposed stringent constraints on various parameters describing the physics of the early universe. However, despite these advancements, many fundamental questions about the standard cosmological model remain unresolved. One such question pertains to the nature of dark matter: what constitutes its particle content, what is its mass, and how does it interact with normal matter? Addressing these issues necessitates a clear understanding of the spatial distribution of dark matter across space and time. Unfortunately, direct detection experiments can only measure the gravitational effects of dark matter and cannot provide this critical information. Therefore, we must depend on indirect methods such as galaxy clustering, weak lensing, and 21 cm emission studies.",
        "ori-fast-z-score": 0.9838699100999074,
        "water-fast-z-score": 8.131727983645296,
        "rewrite-fast-z-score": 1.2780193008453875
    },
    {
        "original_text": "The in vivo behavior of looped DNA-protein complexes is investigated by using multiscale modeling and simulation techniques. The results show that the looping process can be divided into three stages, namely initiation, elongation, and termination. During the initiation stage, the protein binds to its recognition site on one end of the double helix; during the elongation stage, it slides along the double helix until reaching another recognition site at which point the complex terminates. These findings are consistent with experimental observations. Furthermore, we find that the sliding motion of the protein is driven by thermal fluctuations rather than Brownian diffusion. Finally, our simulations suggest that the formation of loops may play an important role in regulating gene expression. This article is part of a Special Issue entitled  Advances in Computational Science  guest edited by Professors S. J. Liou and C. Y. Wu. It has been accepted for publication in Journal of Physics A: Mathematical and Theoretical (JPhysA)",
        "watermark_text": "The in vivo behavior of looped DNA - protein complexes is investigated by using multiscale simulation and modeling techniques . The results show that the looping cycle can be categorized into three stages , namely initiation , elongation , and termination .During the initiation stage , the protein binds to its recognition location on one end of the double helix ; during the elongation phase , it slides along the double helix until reaching another recognition location at which point the complex terminates . These conclusions are compatible with experimental studies .Furthermore , we find that the sliding motion of the protein is caused by temperature fluctuations rather than Brownian absorption . Finally , our simulations confirm that the formation of loops might play an important role in controlling gene activity .This page is part of a Special Issue entitled Advances in Computational Science guest edited by Professors S . J . Liou and C . Y . Wu . It has been accepted for published in Journal of Physics A : Mathematical and Theoretical ( JPhysA )",
        "rewrite_text": "The in vivo behavior of looped DNA-protein complexes is examined through multiscale simulation and modeling techniques. The findings reveal that the looping cycle can be divided into three distinct stages: initiation, elongation, and termination. In the initiation stage, the protein binds to its target site on one end of the double helix. During the elongation phase, the protein slides along the double helix until it reaches another recognition site, at which point the complex terminates. These findings are consistent with experimental studies. Additionally, we discover that the protein's sliding motion results from temperature fluctuations rather than Brownian motion. Ultimately, our simulations suggest that loop formation may significantly influence gene activity. This work is included in a Special Issue titled Advances in Computational Science, guest edited by Professors S. J. Liou and C. Y. Wu, and has been accepted for publication in the Journal of Physics A: Mathematical and Theoretical (JPhysA).",
        "ori-fast-z-score": -0.25,
        "water-fast-z-score": 4.589285179800713,
        "rewrite-fast-z-score": -0.254000254000381
    },
    {
        "original_text": "We report on observations made with the Submillimeter Array and the Atacama Large Millimeter/submillimeter Array in order to study the kinematics of an outflow driven by the high-mass protostellar object, IRAS 18566+0408; this source is associated with a cluster of young stellar objects located at a distance of 3 kpc. The data reveal that there are two components along the line-of-sight; one component has a systemic velocity of ~10 km s-1 , while another component shows blueshifted emission up to -60 km s-1 . We find evidence for a collimated jet-like structure extending over ~0.5 pc. This suggests that the driving source may be deeply embedded within its natal cloud core. In addition, we detect several compact knots distributed along the flow axis which show blue-shifted velocities ranging between 10-60 km s-1 .\nThe mass-loss rate estimated from our observations ranges between 1×10-3 -1×10-2 M⊙ yr-1 .",
        "watermark_text": "We report on observations made with the Submillimeter Array and the Atacama Large Millimeter / submillimeter Array in order to study the kinematics of an outflow generated by the high - weight protostellar element , IRAS 18566 + 0408 ; this source is associated with a cluster of young stellar bodies located at a distance of 3 kpc . The data reveal that there are two systems along the line - of - view ; one component has a systemic speed of ~ 10 km s - 1 , while another component displays blueshifted emission up to - 60 km s - 1 .We see evidence for a collimated jet - like structure extending over ~ 0 . 5 pc . This implies that the driving source may be deeply lodged within its natal cloud core .In addition , we find various compact knots scattered along the flow axis which show blue - shifted velocities ranging between 10 - 60 km s - 1 . The mass - loss rate calculated from our observations runs between 1×10 - 3 - 1×10 - 2 [UNK] yr - 1 .",
        "rewrite_text": "We present observations from the Submillimeter Array and the Atacama Large Millimeter/submillimeter Array to investigate the kinematics of an outflow associated with the high-mass protostar IRAS 18566 + 0408, located within a cluster of young stellar objects at a distance of 3 kpc. The data show two components along the line of sight: one exhibits a systemic velocity of approximately 10 km/s, while the other features blueshifted emission reaching -60 km/s. We observe evidence of a collimated, jet-like structure that extends roughly 0.5 pc, suggesting that the driving source is situated deep within its natal cloud core. Additionally, we identify several compact knots distributed along the flow axis, displaying blue-shifted velocities between 10 and 60 km/s. The mass-loss rate derived from our observations ranges from 1×10^-3 to 1×10^-2 solar masses per year.",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 0.13018891098082389
    },
    {
        "original_text": "We propose an efficient numerical scheme to solve the nonlinear dynamics of semiconductor microcavity lasers with arbitrary pumping profiles and cavity losses, which is based on the combination of two different truncation schemes. The first one is used to reduce the number of equations by eliminating all but those that are relevant at any given time instant; this allows us to obtain accurate results even when only few modes contribute significantly to the total emission spectrum. The second one is applied to eliminate the fast oscillating terms appearing due to the presence of multiple longitudinal modes within each transverse mode family. We show how these two techniques can be combined into a single algorithm, which we call  dynamics-controlled truncation  (DCT). Finally, we demonstrate the accuracy and efficiency of our method by comparing it against other existing methods. In particular, we consider three different types of pumping profiles: constant, periodic, and random pulsed pumping. \nI. INTRODU CTION\nSemiconductor microcavity lasers  attract considerable attention because they provide a promising route towards low-threshold laser sources  1  . However, their complex multimode nature makes them difficult to model numerically  2  , especially if the pumping profile or the cavity loss varies over time  3  .\nIn order to overcome such difficulties, several authors have proposed various approaches  4  -  8  . For example, in Ref.  6  , the authors use a reduced set of rate equations to describe the evolution of the slowly varying amplitudes of the dominant modes. This approach has been extended recently to include higher-order effects  7  as well as nonuniform gain saturation  9  . Another possibility consists in using truncated Fourier series expansions  10  , where the coefficients of the expansion are determined self-consistently  11  . Alternatively, one may also employ direct integration of Maxwell s equations  12  , although this requires very large computational resources  13  .",
        "watermark_text": "We suggest an efficient numerical system to solve the nonlinear dynamics of semiconductor microcavity lasers with arbitrary flow profiles and cavity costs , which is based on the combination of two different truncation schemes . The first first is utilized to reduce the number of equations by removing all but those that are applicable at any certain time instant ; this enables us to obtain precise conclusions even when only few modes contribute greatly to the total emission spectrum .The second one is applied to eliminate the fast oscillating terms appearing caused to the presence of multiple longitudinal frequencies within each transverse mode family . We see how these two procedures can be merged into a single method , which we call dynamics - controlled truncation ( DCT ) .Finally , we prove the accuracy and efficiency of our technique by comparing it against other existing techniques . In particular , we study three different kinds of flow profiles : constant , continuous , and random pulsed pumping .I . INTRODU CTION Semiconductor microcavity lasers draw great popularity because they give a viable path towards non - threshold beam sources 1 . However , their complex multimode nature making them harder to model numerically 2 , particularly if the pumping profile or the cavity gain varies over time 3 .In try to overcome such problems , various published have proposed several methods 4 - 8 . For instance , in Ref .6 , the papers use a reduced series of rate coefficients to explain the evolution of the slowly varying amplitudes of the dominant modes . This method has been extended recently to consider higher - order effects 7 as well as nonuniform gain saturation 9 .Another possibility consists in utilizing truncated Fourier series expansions 10 , where the coefficients of the integration are chosen self - consistently 11 . Alternatively , one may even employ direct combining of Maxwell s coefficients 12 , although this demands very huge computational resources 13 .",
        "rewrite_text": "We propose an effective numerical approach for addressing the nonlinear dynamics of semiconductor microcavity lasers with arbitrary flow profiles and cavity losses. This approach combines two distinct truncation schemes. The first scheme is employed to streamline the number of equations by retaining only those relevant to specific time instants, allowing us to draw accurate conclusions even when only a few modes significantly influence the total emission spectrum. The second scheme targets the removal of rapidly oscillating terms resulting from multiple longitudinal frequencies within each transverse mode family. We demonstrate how these two strategies can be integrated into a single method, which we refer to as dynamics-controlled truncation (DCT). Finally, we validate the accuracy and efficiency of our technique by comparing it to other existing methodologies. Specifically, we analyze three types of flow profiles: constant, continuous, and randomly pulsed pumping.\n\nI. INTRODUCTION\n\nSemiconductor microcavity lasers have gained considerable attention due to their potential as non-threshold beam sources. However, their complex multimode behavior poses significant challenges for numerical modeling, especially when the pumping profile or cavity gain fluctuates over time. To address these challenges, various approaches have been proposed in the literature. For example, reference 6 introduces a reduced series of rate coefficients to capture the evolution of the slowly varying amplitudes of dominant modes, a method that has recently been expanded to incorporate higher-order effects as well as nonuniform gain saturation. Another approach involves using truncated Fourier series expansions, where the integration coefficients are chosen in a self-consistent manner. Alternatively, one could directly combine Maxwell's equations, although this requires substantial computational resources.",
        "ori-fast-z-score": -1.632993161855452,
        "water-fast-z-score": 8.110396013138502,
        "rewrite-fast-z-score": 0.42107596053325946
    },
    {
        "original_text": "We report the observation of very weak electron-phonon coupling (EPC) in La0.7Sr0.3MnO3 thin films grown on SrTiO3 substrates by pulsed laser deposition, which is consistent with previous reports for bulk samples.  We also observe that EPC decreases as temperature increases up to 300 K. This behavior can be explained by considering the effect of lattice expansion due to thermal fluctuations at high temperatures. In addition, we find that the magnitude of EPC depends strongly on film thickness; it becomes smaller when the film thickness decreases down to 10 unit cells. The observed dependence of EPC on both temperature and film thickness suggests that phonon confinement plays an important role in determining the strength of EPC in these materials. Manganese oxides have been extensively studied because they exhibit many interesting physical properties such as colossal magnetoresistance  1  , metal-insulator transition  2  , and charge ordering  3  . Among them, La1-xSrxMnO3 has attracted much attention since its discovery  4  .\nIn this compound, Mn ions are located at two different sites, i.e., Mn3+(tetrahedral site) and Mn4+(octahedral site). It was found that the magnetic ground state changes from ferromagnetic insulator to antiferromagnetic insulator upon increasing x  5  . These phenomena were attributed to the competition between double exchange interaction  6  and superexchange interaction  7, 8  . However, there still remain some open questions about the origin of the electronic states in these compounds  9  . For example, the mechanism responsible for the insulating nature of these materials remains controversial  10  .",
        "watermark_text": "We report the observation of very weakened electron - phonon coupling ( EPC ) in La0 . 7Sr0 . 3MnO3 narrow bands grown on SrTiO3 substrates by pulsed infrared deposition , which is compatible with previous findings for bulk samples . We additionally observe that EPC changes as temperature increases up to 300 K . This activity can be described by using the impact of lattice increase due to heat fluctuations at high temperatures .In addition , we find that the severity of EPC depends strongly on film thickness ; it becomes weaker when the film thickness decreases down to 10 unit cells . The observed influence of EPC on both heat and film thickness implies that phonon confinement serves an important role in measuring the strength of EPC in these materials .Manganese oxides have been heavily explored because they demonstrate many interesting physical properties such as colossal magnetoresistance 1 , metal - insulator transition 2 , and charge ordering 3 . Among them , La1 - xSrxMnO3 has garnered considerable scrutiny since its observation 4 .In this compound , Mn ions are situated at two different places , i . e . , Mn3 + ( tetrahedral site ) and Mn4 + ( octahedral site ) . It was shown that the magnetic ground state changes from ferromagnetic insulator to antiferromagnetic insulator upon increasing x 5 .These phenomena were attributed to the competition between multiple transfer exchange 6 and superexchange interaction 7 , 8 . However , there still continue some open questions about the origin of the electronic states in these interactions 9 .For instance , the process responsible for the insulating behavior of these structures remains controversial 10 .",
        "rewrite_text": "We report a significant reduction in electron-phonon coupling (EPC) within La0.7Sr0.3MnO3 narrow bands fabricated on SrTiO3 substrates via pulsed infrared deposition, which aligns with earlier findings from bulk samples. Additionally, our observations indicate that EPC varies with temperature, increasing up to 300 K. This variation can be explained by the effects of lattice expansion caused by thermal fluctuations at elevated temperatures. Furthermore, we discover that the intensity of EPC is highly dependent on the film thickness; it diminishes notably as the thickness decreases to 10 unit cells. The demonstrated impact of EPC on both temperature and film thickness suggests that phonon confinement plays a crucial role in determining the strength of EPC in these materials. Manganese oxides have been widely studied due to their intriguing physical properties, such as colossal magnetoresistance, metal-insulator transitions, and charge ordering. Among these, La1-xSrxMnO3 has received significant attention since its discovery. In this compound, manganese ions occupy two distinct sites: Mn3+ at tetrahedral sites and Mn4+ at octahedral sites. It has been established that the magnetic ground state transitions from a ferromagnetic insulator to an antiferromagnetic insulator with increasing x. These behaviors have been linked to the interplay between multiple transfer exchange and superexchange interactions. Nevertheless, fundamental questions regarding the origins of the electronic states involved in these interactions remain unresolved, particularly regarding the mechanisms behind the insulating characteristics of these structures.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.375262396202865,
        "rewrite-fast-z-score": 0.09853292781642932
    },
    {
        "original_text": "We prove that the set of all polynomials in one variable with integer coefficients is uniquely representable as a sum of squares of rational functions, and we give an explicit algorithm for computing such representation.  We also show how to extend this result to multivariate polynomials over finite fields. The problem of representing a given polynomial as a sum of squares (SOS) has been studied extensively since Hilbert s 19th century work on quadratic forms. In particular, it was shown by Artin that every nonnegative integer can be represented as a sum of four squares of integers; see  1  . This theorem implies that any positive integer n can be written as a sum of at most 4 square roots of integers. However, there are infinitely many ways to write each number as a sum of two or three square roots of integers. For example, 1 = 2 + √ 3 , but also 1 = 2 − √ 3 .\nThe uniqueness question becomes more interesting when restricted to certain classes of polynomials. It turns out that if f ∈ Z x  is a monic polynomial whose Galois group G(f ) is solvable then f is uniquely representable as SOS  2  . On the other hand, if G(f ) contains a nonsolvable subgroup then f may have several distinct SOS representations  3  .",
        "watermark_text": "We establish that the group of all polynomials in one variable with integer coefficients is uniquely representable as a sum of squares of rational functions , and we give an explicit algorithm for generating such representation . We also demonstrate how to apply this result to multivariate polynomials over arbitrary fields .The question of representing a given polynomial as a sum of squares ( SOS ) has been studied frequently since Hilbert s 19th century study on quadratic forms . In particular , it was shown by Artin that every nonnegative integer can be represented as a sum of four squares of integers ; seeing 1 .This formula assumes that any positive integer n can be written as a sum of at most 4 square roots of integers . However , there are infinitely many ways to write each number as a sum of two or three square roots of numbers .For instance , 1 = 2 + √ 3 , but also 1 = 2 − √ 3 . The uniqueness problem arises more exciting when confined to different categories of polynomials .It turns out that if f ∈ Z x is a monic polynomial whose Galois group G ( f ) is solvable then f is uniquely representable as SOS 2 . On the other hand , if G ( f ) contains a nonsolvable subgroup then g may have several different SOS representations 3 .",
        "rewrite_text": "We demonstrate that the set of all polynomials in one variable with integer coefficients can be uniquely represented as a sum of squares of rational functions, and we provide a detailed algorithm to generate such representations. Furthermore, we illustrate how to extend this finding to multivariate polynomials over arbitrary fields. The question of expressing a given polynomial as a sum of squares (SOS) has been frequently examined since Hilbert's studies on quadratic forms in the 19th century. Notably, Artin established that every nonnegative integer can be represented as the sum of four squares of integers; for instance, the number 1. This implies that any positive integer \\( n \\) can be expressed as the sum of at most four square roots of integers. However, there are infinitely many representations for each number using two or three square roots. For example, we can write \\( 1 = 2 + \\sqrt{3} \\) and also \\( 1 = 2 - \\sqrt{3} \\). The uniqueness of such representations becomes particularly interesting when considering different classes of polynomials. It turns out that if \\( f \\in \\mathbb{Z}[x] \\) is a monic polynomial and its Galois group \\( G(f) \\) is solvable, then \\( f \\) has a unique SOS representation. Conversely, if \\( G(f) \\) includes a nonsolvable subgroup, the polynomial \\( f \\) may have multiple distinct SOS representations.",
        "ori-fast-z-score": 0.32539568672798425,
        "water-fast-z-score": 4.364357804719848,
        "rewrite-fast-z-score": 1.078327732034384
    },
    {
        "original_text": "A slide-o-cam transmission is an alternative to conventional geared transmissions in which the input and output shafts are connected by means of cams that rotate on their own axes, with no gears or other mechanical elements between them.  The main advantages of this type of transmission are:  • No backlash due to tooth meshing; • High efficiency (up to 98%); • Low noise levels; • Reduced weight and volume compared to traditional gearboxes; • Easy assembly and disassembly; • Possibility of using different types of motors as inputs. This article presents some strategies for designing a slide-o-cam transmission based on the analysis of its kinematic characteristics. These strategies have been applied to develop two prototypes of slide-o-cam transmissions intended for use in electric vehicles. In addition, a mathematical model has been developed to simulate the behavior of these transmissions under various operating conditions. Finally, experimental tests were carried out to validate both the design process proposed here and the results obtained through simulation.",
        "watermark_text": "A slide - o - cam transmission is an alternative to conventional geared transmissions in which the input and input shafts are connected by means of cams that rotate on their own axes , with no gears or other hydraulic factors between them . The main benefits of this form of transmission are : • No backlash due to tooth meshing ; • Low reliability ( up to 98 % ) ; • Low sound levels ; • Reduced weight and mass compared to conventional gearboxes ; • Easy assembly and disassembly ; • Possibility of using multiple types of motors as output .This page presents some techniques for constructing a slide - o - cam transmission depending on the evaluation of its kinematic qualities . These strategies have been used to develop two models of slide - o - cam transmissions intended for use in electric vehicles .In addition , a mathematical description has been constructed to simulate the performance of these transmissions under various operating circumstances . Finally , experimental tests were carried out to validate both the design process described here and the results derived through simulation .",
        "rewrite_text": "A slide-o-cam transmission offers an alternative to traditional geared systems by connecting the input and output shafts through cams that rotate around their own axes, eliminating the need for gears or hydraulic components. This type of transmission boasts several advantages, including: • Absence of backlash from gear meshing; • High reliability (up to 98%); • Low noise levels; • Lighter weight compared to standard gearboxes; • Simplified assembly and disassembly; • Compatibility with various motor types for output. This document outlines techniques for constructing slide-o-cam transmissions based on an assessment of their kinematic properties. These methods were employed to develop two models of slide-o-cam transmissions designed for use in electric vehicles. Additionally, a mathematical framework was created to simulate the performance of these transmissions under different operating conditions. Lastly, experimental tests were conducted to validate both the outlined design process and the simulation results.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 5.812381937190964,
        "rewrite-fast-z-score": -0.23570226039551587
    },
    {
        "original_text": "The Web is an important source of information that can be used to support decision making processes and business intelligence applications. However, the Web contains heterogeneous sources with different structures and formats which makes it difficult to integrate them into existing databases or data warehouse systems. In this work we propose a new approach based on ontologies to model web data as well as their relationships. The proposed method uses semantic technologies such as RDF (Resource Description Framework) and OWL (Ontology Web Language). We also present how our approach can be integrated within a data warehouse system using ETL tools. Finally, we show some experimental results obtained by applying our approach to real world datasets. Keywords: Data Warehouse, Semantic Technologies, Ontology, Integration, Web Data Modeling. 1 Introduction With the rapid development of Internet technology, more and more organizations are collecting large amounts of data from various online resources. These data may include customer profiles, product catalogs, financial records, etc., all of which need to be stored and analyzed efficiently. This has led to the emergence of several research areas including data mining  1  , knowledge discovery  2  , and data warehousing  3  . A data warehouse  4  is a subject-oriented, integrated, time-variant collection of data in support of management s decisions. It provides users with fast access to historical data along with current operational data. As shown in Figure 1 , a typical data warehouse consists of three main components: OLTP (OnLine Transaction Processing), Extract-Transform-Load (ETL), and On-Line Analytical Processing (OLAP).\nFigure 1: Architecture of a data warehouse",
        "watermark_text": "The Web is an important source of information that can be used to support decision making systems and business intelligence applications . However , the Web includes heterogeneous sources with various structures and formats which makes it difficult to integrate them into older databases or data warehouse applications .In this project we attempt a new approach using on ontologies to model internet data as well as their connections . The proposed approach utilizes semantic methods such as RDF ( Resource Description Framework ) and OWL ( Ontology Web Language ) .We also demonstrate how our approach can be unified within a data warehouse system using ETL techniques . Finally , we give some experimental results derived by using our approach to real life datasets .Keywords : Data Warehouse , Semantic Technologies , Ontology , Integration , Web Data Modeling . 1 Introduction With the fast development of Internet technology , more and more organizations are collecting huge amounts of statistics from numerous web resources .These data may include customer lists , product catalogs , financial records , etc . , all of which require to be processed and studied accurately . This has led to the emergence of several study fields including data extraction 1 , knowledge creation 2 , and information warehousing 3 .A data warehouse 4 is a subject - oriented , integrated , time - variant collection of statistics in support of management s decisions . It provides users with fast access to historical data along with current operational statistics .As seen in Figure 1 , a typical database warehouse consists of three principal portions : OLTP ( OnLine Transaction Processing ) , Extract - Transform - Load ( ETL ) , and On - Line Analytical Processing ( OLAP ) . Figure 1 : Architecture of a data warehouse",
        "rewrite_text": "The internet serves as a crucial reservoir of information that can enhance decision-making systems and business intelligence applications. Nonetheless, the presence of diverse sources with varying structures and formats on the web complicates the integration of this data into traditional databases or data warehouse solutions. In this project, we propose a novel approach that employs ontologies to model both web data and their interconnections. Our method leverages semantic technologies such as RDF (Resource Description Framework) and OWL (Web Ontology Language). We also illustrate how our strategy can be effectively integrated into a data warehouse system through ETL (Extract-Transform-Load) techniques. Lastly, we present experimental results obtained by applying our approach to real-world datasets. \n\nKeywords: Data Warehouse, Semantic Technologies, Ontology, Integration, Web Data Modeling. \n\n1 Introduction \nWith the rapid advancement of internet technology, organizations are increasingly gathering vast amounts of data from numerous online resources. This information may encompass customer lists, product catalogs, financial records, and more, all of which must be processed and analyzed with precision. Consequently, this has given rise to various fields of study, including data extraction, knowledge generation, and information warehousing. A data warehouse is an organized, integrated, and time-variant collection of data designed to support managerial decision-making. It grants users swift access to historical data as well as current operational information. As illustrated in Figure 1, a typical data warehouse architecture comprises three core components: OLTP (Online Transaction Processing), ETL (Extract-Transform-Load), and OLAP (Online Analytical Processing). \n\nFigure 1: Architecture of a Data Warehouse.",
        "ori-fast-z-score": 0.09016696346674323,
        "water-fast-z-score": 7.483857967739688,
        "rewrite-fast-z-score": 0.5432144762551112
    },
    {
        "original_text": "The purpose of this study is to investigate how privacy issues are perceived by users in Europe and what their attitude towards these issues is, when using online learning environments (OLE). The research questions are: \nWhat do students think about privacy?\nHow does student s perception change over time?\nWhich factors influence students  attitudes on privacy? How can we measure it? What are the implications for OLE developers? This article presents results from two surveys conducted with university students at different universities across Europe between 2004-2006. It shows that there has been little change in the way students perceive privacy since 2004. Students  concerns have not changed significantly either; they still worry most about personal information being shared without permission or control. However, some differences were found among countries. In addition, the findings show that gender plays a role as well as age. Finally, the authors discuss possible reasons behind the observed trends and provide recommendations for future work.",
        "watermark_text": "The purpose of this study is to examine how privacy issues are seen by users in Europe and what their attitude towards these problems is , when using internet learning habitats ( OLE ) . The survey problems are : What do students think about privacy ?How does student s perception shift over time ? Which variables influence pupils beliefs on privacy ?How can we measure it?What are the implications for OLE developers?This page presents scores from two polls conducted with university students at different schools across Europe between 2004 - 2006 . It demonstrates that there has been less shift in the way pupils understand privacy since 2004 .Students issues have not altered significantly either ; they still concern most about personal data being communicated without authorization or supervision . However , some differences were found among countries .In addition , the discoveries reveal that gender plays a role as well as aging . Finally , the articles discuss possible reasons behind the reported developments and include recommendations for future projects .",
        "rewrite_text": "This study aims to explore European users' perceptions of privacy issues related to online learning environments (OLE) and their attitudes towards these concerns. Key research questions include: What are students' views on privacy? How do their perceptions evolve over time? Which factors influence their beliefs about privacy? How can these views be effectively measured? What implications do these findings have for OLE developers? This report presents the results from two surveys conducted with university students across various European institutions between 2004 and 2006. It indicates that there has been little change in students' understanding of privacy since 2004. Concerns remain largely consistent, with students primarily worried about the unauthorized or unsupervised sharing of personal data. However, some differences were noted between countries. The findings also suggest that factors such as gender and age influence privacy perceptions. Finally, the article discusses potential reasons for the observed trends and offers recommendations for future initiatives.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 7.9881240965747695,
        "rewrite-fast-z-score": 0.6708203932499369
    },
    {
        "original_text": "We study the existence, uniqueness and ergodicity of physical measures in some classes of infinite measure preserving transformations on probability spaces. We prove that if T is an infinite measure preserving transformation on (X, B(X), m) such that there exists a set A ∈ B(X) satisfying m(A) > 0 and m(T −1 A) = 0 then T has at least one physical measure which is unique up to scalar multiples. Moreover we show that any two physical measures are equivalent under suitable conditions. Finally we give examples showing how our results can be applied. The main motivation comes from the theory of infinite ergodic group actions where it was shown by Glasner and Weiss  GW  ,  GS1  that every infinite ergodic action of a countable amenable group G admits a unique invariant Radon measure µG . In this case the space X is taken as the dual group G equipped with its left Haar measure m.",
        "watermark_text": "We research the existence , uniqueness and ergodicity of physical measures in some categories of infinite measure retaining transformations on probability spaces . We prove that if T is an infinite measure preserving transformation on ( X , B ( X ) , m ) such that there exists a setting A ∈ B ( X ) satisfying m ( A ) > 0 and m ( T −1 A ) = 0 then T has at least one physical measure which is unique up to scalar multiples .Moreover we prove that any two physical measures are comparable under suitable conditions . Finally we give instance demonstrating how our findings can be applied .The main motivation arises from the principle of infinite ergodic group events where it was shown by Glasner and Weiss GW , GS1 that every infinite ergodic act of a countable amenable group G accepts a unique invariant Radon number µG . In this instance the space X is taken as the dual group G equipped with its leaving Haar measure m .",
        "rewrite_text": "We investigate the existence, uniqueness, and ergodicity of physical measures associated with certain categories of infinite measure-preserving transformations on probability spaces. Our results establish that if \\( T \\) is an infinite measure-preserving transformation on \\( (X, B(X), m) \\) and there exists a set \\( A \\in B(X) \\) such that \\( m(A) > 0 \\) and \\( m(T^{-1}A) = 0 \\), then \\( T \\) possesses at least one physical measure that is unique up to scalar multiples. Furthermore, we demonstrate that any two physical measures can be compared under appropriate conditions. We also provide examples that illustrate the applicability of our findings. This research is motivated by the principle of infinite ergodic group actions; Glasner and Weiss showed that every infinite ergodic action of a countable amenable group \\( G \\) has a unique invariant Radon measure \\( \\mu_G \\). In this context, the space \\( X \\) is chosen to be the dual group \\( G \\) equipped with its associated Haar measure \\( m \\).",
        "ori-fast-z-score": -0.9113223768657671,
        "water-fast-z-score": 3.7754784184438925,
        "rewrite-fast-z-score": 0.2626128657194451
    },
    {
        "original_text": "The article presents new data on the paleogeography and geodynamics of the Russian Arctic in the late Quaternary period, based on the results of field studies conducted by the authors over the past decade. The study area is located between the Lena River to the west and the Kolyma River to the east (Fig.  1) . It includes the northern part of Yakutia, the southern part of Chukotka Autonomous Okrug, and the western part of Magadan Oblast. In this region, the authors studied more than 100 sites with deposits of loess-like sediments that accumulated during the last glacial cycle. These are mainly sandy silts with an admixture of gravel particles up to 5 mm in diameter; they contain numerous mollusk shells, bones of terrestrial mammals, and other remains of biota. Based on these materials, we reconstructed the history of climatic fluctuations in the study area since the Last Glacial Maximum (LGM) until today.",
        "watermark_text": "The section offers latest data on the paleogeography and geodynamics of the Russian Arctic in the last Quaternary period , based on the results of field analyses conducted by the writers over the previous decade . The survey area is situated between the Lena River to the west and the Kolyma River to the west ( Fig .1 ) . It includes the northern part of Yakutia , the southern portion of Chukotka Autonomous Okrug , and the western part of Magadan Oblast .In this area , the authors studied more than 100 sites with formations of loess - like sediments that eroded during the last ice cycle . These are chiefly sandy silts with an admixture of sandy fragments up to 5 mm in width ; they contain many mollusk shells , fossils of terrestrial birds , and other remains of biota .Based on these resources , we analyzed the history of climatic fluctuations in the program field since the Last Glacial Maximum ( LGM ) until today .",
        "rewrite_text": "This section presents the most recent data on the paleogeography and geodynamics of the Russian Arctic during the last Quaternary period, derived from field studies conducted by the authors over the past decade. The surveyed region is located between the Lena River to the west and the Kolyma River to the east (see Fig. 1). It encompasses the northern part of Yakutia, the southern region of Chukotka Autonomous Okrug, and the western area of Magadan Oblast. Within this region, the authors investigated over 100 sites featuring loess-like sediments that have eroded during the last ice cycle. These sediments mainly consist of sandy silts with sandy fragments measuring up to 5 mm in diameter and are rich in mollusk shells, fossils of terrestrial birds, and various other biological remains. Utilizing these findings, we analyzed the climatic fluctuations in this region from the Last Glacial Maximum (LGM) to the present day.",
        "ori-fast-z-score": -2.516611478423583,
        "water-fast-z-score": 4.464418717230567,
        "rewrite-fast-z-score": -0.3841106397986879
    },
    {
        "original_text": "We present new photometric and spectroscopic observations for two stars, HD 122563 (=HR 5171A) and BD+17°3248, which are suspected to be members of the proposed intermediate age population of helium-rich giants in the globular cluster Omega Cen.  We find that both stars have very similar atmospheric parameters as those found by previous studies for other candidate helium-rich giant candidates in Omega Cen: T eff = 8200 K; log g = 3.8;  Fe/H  = -1.0 dex. The observed spectra show no evidence for He II lines at 4686 Å or 5412 Å, but do exhibit strong Balmer line emission with equivalent widths ranging between -40 and -50 mÅ. These results suggest that these stars may not actually belong to this proposed class of objects. However, we cannot rule out the possibility that they are indeed helium-rich giants on the basis of our current data set alone.",
        "watermark_text": "We present new photometric and spectroscopic observations for two stars , HD 122563 ( = HR 5171A ) and BD + 17°3248 , which are known to be members of the suggested intermediate age population of helium - rich giants in the globular cluster Omega Cen . We see that both stars have very similar atmospheric parameters as those shown by earlier surveys for other candidate helium - rich giant finalists in Omega Cen : T eff = 8200 K ; log f = 3 . 8 ; Fe / H = - 1 . 0 dex .The observed spectra show no evidence for He II systems at 4686 Å or 5412 Å , but do exhibit strong Balmer line emission with corresponding widths ranging between - 40 and - 50 mÅ . These conclusions show that these stars must not actually belong to this possible category of bodies .However , we cannot judge out the suggestion that they are indeed helium - rich giants on the grounds of our latest data set alone .",
        "rewrite_text": "We present new photometric and spectroscopic observations of two stars, HD 122563 (HR 5171A) and BD + 17°3248, which are believed to be part of the proposed intermediate-age population of helium-rich giants in the globular cluster Omega Centauri. Our findings indicate that both stars share similar atmospheric parameters with those reported in previous surveys of other candidate helium-rich giants in Omega Cen: effective temperature (T_eff) of 8200 K, log g of 3.8, and iron abundance (Fe/H) of -1.0 dex. The spectra we observed show no signs of He II absorption features at 4686 Å or 5412 Å, but they do reveal strong Balmer line emission with widths ranging from -40 to -50 mÅ. These observations suggest that these stars might not belong to this proposed category of objects. However, we cannot entirely dismiss the possibility that they are helium-rich giants based solely on our latest data.",
        "ori-fast-z-score": -2.1009029257555607,
        "water-fast-z-score": 3.2547227745205967,
        "rewrite-fast-z-score": -2.54000254000381
    },
    {
        "original_text": "We present the first results on clustering measurements for luminous red galaxies (LRGs) in the redshift range 0.5 <z<0.8, obtained with the Anglo-Australian Observatory s multi-object spectrograph AAOmega. We use data from the 2dF-SDSS LRG and QSO survey to measure the projected correlation function wp(rp). The observed clustering amplitude is consistent with that expected from linear theory predictions based on current cosmological models. This result provides an important test of these models over this redshift range where there are few other constraints available. In addition we find evidence for evolution in the galaxy bias parameter between our two samples separated by ~0.2 Gyrs. These results will be presented in detail elsewhere. \n \n Keywords: Luminous Red Galaxies; Clustering; Bias Evolution; Cosmology. 1 Introduction \n \n A number of recent studies have shown that luminous red galaxies (hereafter LRGs), selected via their optical colours or near-infrared photometry, provide powerful probes of large-scale structure out to high redshifts (e.g., Eisenstein et al. 2001; Wake et al. 2006; Padmanabhan et al. 2007; Blake et al. 2008; Ross et al. 2008) . Their large luminosities mean they can be detected efficiently even at relatively low redshifts, while their red colours make them easy to identify spectroscopically. They also tend to reside in massive dark matter haloes which evolve slowly through cosmic time, making them useful tracers of the underlying mass distribution. As such, they offer unique opportunities to study both the growth of structures as well as the nature of dark energy driving its accelerated expansion (see e.g., Percival & White 2009 , for a review). \n \n Here we report the first measurement of the spatial clustering properties of LRGs in the redshift range 0<z<0.8 made possible by combining data from the Sloan Digital Sky Survey (SDSS) (York et al. 2000) , the Two Degree Field Galaxy Redshift Survey (2dFGRS) (Colless et al.",
        "watermark_text": "We publish the first findings on clustering observations for luminous red objects ( LRGs ) in the redshift region 0 . 5 < z < 0 . 8 , obtained with the Anglo - Australian Observatory s multi - object spectrograph AAOmega . We use data from the 2dF - SDSS LRG and QSO studies to measure the projected relationship value wp ( rp ) .The observed clustering amplitude is compatible with that expected from linear theoretical estimates based on current cosmological models . This result provides an important test of these models over this redshift region where there are few other constraints offered .In addition we find proof for evolution in the galaxy bias variable between our two specimens divided by ~ 0 . 2 Gyrs . These conclusions will be shown in detail elsewhere .Keywords : Luminous Red Galaxies ; Clustering ; Bias Evolution ; Cosmology . 1 Introduction A variety of recent studies have shown that luminous red clusters ( hereafter LRGs ) , selected via their optical colours or near - infrared photometry , provide potent probes of large - scale organization out to large redshifts ( e . g . , Eisenstein et al .2001 ; Wake et al . 2006 ; Padmanabhan et al .2007 ; Blake et al . 2008 ; Ross et al .2008 ) . Their large luminosities guarantee they can be identified efficiently even at fairly little redshifts , while their red colours help them easy to identify spectroscopically .They also seem to live in massive dark matter haloes which evolution gradually through cosmic time , making them useful tracers of the ongoing mass distribution . As such , they give unique possibilities to study both the development of structures as also as the nature of bright energy causing its rapid increase ( saw e . g . , Percival & White 2009 , for a review ) .Here we publish the first measurement of the spatial clustering behavior of LRGs in the redshift region 0 < z < 0 . 8 made possible by combining information from the Sloan Digital Sky Survey ( SDSS ) ( York et al . 2000 ) , the Two Degree Field Galaxy Redshift Survey ( 2dFGRS ) ( Colless et al .",
        "rewrite_text": "We present our initial findings on the clustering characteristics of luminous red objects (LRGs) within the redshift range of 0.5 < z < 0.8, using data from the AAOmega multi-object spectrograph at the Anglo-Australian Observatory. This analysis incorporates data from the 2dF-SDSS LRG and QSO studies to determine the projected correlation function, wp(rp). Our observations indicate that the clustering amplitude aligns well with predictions from linear theoretical models based on current cosmological frameworks. This finding serves as a crucial test for these models in a redshift range that has limited other data. Additionally, we detect evidence of evolution in the galaxy bias parameter between our two analyzed samples, separated by approximately 0.2 Gyr. Further details of these conclusions will be presented in subsequent publications.\n\n**Keywords:** Luminous Red Galaxies; Clustering; Bias Evolution; Cosmology.\n\n**1 Introduction**  \nRecent studies have demonstrated that luminous red galaxies (LRGs), selected through their optical colors or near-infrared photometry, are powerful probes of large-scale structure at significant redshifts (e.g., Eisenstein et al. 2001; Wake et al. 2006; Padmanabhan et al. 2007; Blake et al. 2008; Ross et al. 2008). Their high luminosities enable efficient identification even at relatively low redshifts, while their distinctive red colors facilitate spectroscopic classification. These galaxies are also observed to reside in massive dark matter halos, which evolve over cosmic time, making them valuable tracers of the underlying mass distribution. Consequently, they offer unique opportunities to investigate both the evolution of cosmic structures and the nature of dark energy driving this expansion (see Percival & White 2009 for a review). In this study, we report the first measurement of the spatial clustering of LRGs in the redshift range of 0 < z < 0.8, made possible through the integration of data from the Sloan Digital Sky Survey (SDSS) (York et al. 2000) and the Two Degree Field Galaxy Redshift Survey (2dFGRS) (Colless et al. 2001).",
        "ori-fast-z-score": -0.9610744623271417,
        "water-fast-z-score": 7.0164641544562345,
        "rewrite-fast-z-score": -1.3206763594884356
    },
    {
        "original_text": "The aim of this study was to evaluate whether virtual ultrasound (US) puncture tracts can improve percutaneous renal access in patients with complex anatomy and difficult-to-visualize calices on fluoroscopy.  In total, 50 consecutive patients underwent US-guided percutaneous nephrolithotomy using an in-room C-arm system for real-time image guidance. The procedure was performed under general anesthesia or conscious sedation. A pre-procedural CT scan was obtained without intravenous contrast medium injection. Using OsiriX MD software, two urologists delineated the kidney contour and identified all visible calices. Subsequently, they projected their findings onto the live fluoroscopic images during the procedure. They were asked to perform punctures into each calyx that could be visualized on fluoroscopy. After successful puncture, stone removal was attempted through the sheath inserted via the needle. Successful puncture was defined as reaching at least one calix. Overall success rate was 88%. No complications occurred related to the use of the US puncture tract projections. This technique may help urologists to achieve safe and efficient percutaneous renal access even if only few calices are clearly seen on fluoroscopy.",
        "watermark_text": "The goal of this study was to examine whether digital ultrasound ( US ) puncture tracts can boost percutaneous renal entry in patients with difficult anatomy and difficult - to - visualize calices on fluoroscopy . In total , 50 consecutive patients received US - guided percutaneous nephrolithotomy employing an in - room C - arm network for real - time vision tracking .The technique was done under general anesthesia or conscious sedation . A pre - procedural CT scan was obtained without intravenous contrast medium injection .Using OsiriX MD software , two urologists delineated the kidney contour and identified all evident calices . Subsequently , they projected their findings onto the live fluoroscopic images during the surgery .They were asked to conduct punctures into each calyx that might be visualized on fluoroscopy . After successful puncture , stone extraction was attempted through the sheath inserted via the needle .Successful puncture was calculated as reaching at least one calix . Overall success percentage was 88 % .No complications caused associated to the using of the US puncture tract projections . This method may assist urologists to achieve safe and easy percutaneous renal entry even if only few calices are obvious saw on fluoroscopy .",
        "rewrite_text": "The objective of this study was to explore whether digital ultrasound (US) puncture tracts can facilitate percutaneous renal access in patients with challenging anatomical conditions and poorly visualized calices during fluoroscopy. A total of 50 consecutive patients underwent US-guided percutaneous nephrolithotomy, utilizing an in-room C-arm system for real-time imaging. The procedure was performed under general anesthesia or conscious sedation, and a pre-procedural CT scan was conducted without the use of intravenous contrast medium. Using OsiriX MD software, two urologists outlined the kidney shape and identified all visible calices. They then overlaid this information onto the live fluoroscopic images during the surgery, allowing them to target punctures into each calyx visible on the fluoroscope. Following a successful puncture, stone extraction was attempted through the sheath inserted via the needle, with successful puncture defined as accessing at least one calyx. The overall success rate was found to be 88%. There were no complications related to the use of US puncture tract projections. This technique may help urologists achieve safe and efficient percutaneous renal access, even when only a few calices are visible on fluoroscopy.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.495752858199363,
        "rewrite-fast-z-score": -0.4588314677411235
    },
    {
        "original_text": "The Next-to-Minimal Supersymmetric Standard Model (nMSSM) is an extension of the Minimal Supersymmetric Standard Model that includes additional Higgs doublets and singlet fields, as well as new vector-like quarks and leptons. The phenomenological consequences of this model are investigated in detail using state-of-the-art tools for precision calculations at both low energies and high energy scales. In particular we study the impact on dark matter searches, electroweak observables, flavour physics constraints, LHC signatures and direct detection experiments. We find that the nMSSM can accommodate all current experimental data with minimal fine-tuning while simultaneously providing viable candidates for cold dark matter particles. This talk will present our results. It was presented by Jens Hjorth-Jensen at EPS-HEP 2013 conference held in Vienna Austria between July 24-29th 2013. The slides used during the presentation are available here .",
        "watermark_text": "The Next - to - Minimal Supersymmetric Standard Model ( nMSSM ) is an extension of the Minimal Supersymmetric Standard Model that contains additional Higgs doublets and singlet fields , as well as additional vector - like quarks and leptons . The phenomenological consequences of this model are examined in detail using state - of - the - art tools for precision analysis at both high energies and low power scales .In particular we study the impact on dark matter surveys , electroweak observables , flavour physics requirements , LHC signatures and direct detection experiments . We see that the nMSSM can handle all recent experimental evidence with minimal fine - tuned while simultaneously offering viable candidates for cold bright matter ions .This discussion will present our findings . It was presented by Jens Hjorth - Jensen at EPS - HEP 2013 meeting held in Vienna Austria between July 24 - 29th 2013 .The slides seen during the lecture are available here .",
        "rewrite_text": "The Next-to-Minimal Supersymmetric Standard Model (nMSSM) extends the Minimal Supersymmetric Standard Model by incorporating additional Higgs doublets and singlet fields, along with extra vector-like quarks and leptons. We thoroughly investigate the phenomenological implications of this model using advanced tools for precision analysis across both high-energy and low-energy scales. Our study specifically focuses on the model's effects on dark matter surveys, electroweak observables, flavor physics constraints, signatures at the LHC, and direct detection experiments. Our results indicate that the nMSSM accommodates recent experimental findings with minimal fine-tuning while also providing promising candidates for cold dark matter particles. This discussion will present our findings, which were delivered by Jens Hjorth-Jensen at the EPS-HEP 2013 meeting in Vienna, Austria, from July 24 to 29, 2013. The presentation slides used during the lecture are available here.",
        "ori-fast-z-score": -1.270001270001905,
        "water-fast-z-score": 4.064004064006096,
        "rewrite-fast-z-score": 0.629940788348712
    },
    {
        "original_text": "We consider the linearization problem for the Painleve  equations PIII, PV and PVI in terms of their Lax pairs. We show that these problems are equivalent to certain reductions of the three-wave resonant system. The results obtained here can be regarded as an extension of those by Zakharov et al., who studied the reduction of the two-wave resonant system corresponding to the Painleve  II equation. In particular we prove that there is no non-trivial solution of the linearized PIII or PV equation which decays exponentially at infinity if one imposes some additional conditions on its asymptotic behavior near the origin. \nIntroduction\n\nThe Painleve  equations play important roles both in mathematics and physics (see e.g.   1  ,  4  ). They have been extensively investigated during last decades mainly due to their rich structures such as soliton solutions and Bäcklund transformations. Recently it has been shown that they also appear naturally in various physical models including nonlinear optics  2  . For example, the so-called NLS equation with cubic-quintic nonlinearity arises from the propagation of intense laser beams through Kerr media  3  .\nIn this article we study the linearization problem for several types of the Painleve s equations. More precisely let us consider the following systems of partial differential equations: \nwhere u = u(t, x) ∈ C n+1 , v = v(t, x) and w = w(t, x) are complex-valued functions of t > 0 and x ∈ R 1 . Hereafter subscripts denote differentiation with respect to variables indicated by them. It should be noted that all the above systems possess infinitely many conservation laws given by",
        "watermark_text": "We consider the linearization problem for the Painleve coefficients PIII , PV and PVI in terms of their Lax pairs . We see that these problems are comparable to certain reductions of the three - wave resonant system .The results derived here can be regarded as an extension of those by Zakharov et al . , who studied the reduction of the two - wave resonant system analogous to the Painleve II equation . In particular we prove that there is no non - trivial solution of the linearized PIII or PV function which decays exponentially at infinity if one imposes some additional conditions on its asymptotic evolution near the origin .Introduction The Painleve coefficients play essential roles both in math and physics ( saw e . g . 1 , 4 ) .They have been heavily explored during last decades mostly owing to their deep structures such as soliton solutions and Bäcklund interactions . Recently it has been shown that they also appear naturally in different physical theories including nonlinear optics 2 .For instance , the so - called NLS equation with cubic - quintic nonlinearity emerges from the propagation of aggressive laser beams through Kerr material 3 . In this page we study the linearization problem for various types of the Painleve s equations .More specifically let us consider the following systems of partial differential equations : where u = u ( t , x ) ∈ C n + 1 , v = v ( t , x ) and w = w ( t , z ) are complex - valued functions of t > 0 and x ∈ R 1 . Hereafter subscripts describe separation with regard to factors indicated by them .It should be mentioned that all the above schemes contain infinitely many conservation forces given by",
        "rewrite_text": "We examine the linearization challenge associated with the Painleve coefficients PIII, PV, and PVI through their Lax pairs. These issues present similarities to certain reductions of the three-wave resonant system. The findings presented in this work can be viewed as an expansion of those by Zakharov et al., who investigated the reduction of the two-wave resonant system analogous to the Painleve II equation. Specifically, we demonstrate that under certain conditions regarding its asymptotic behavior near the origin, there are no non-trivial solutions to the linearized PIII or PV functions that exhibit exponential decay at infinity. \n\n**Introduction**  \nThe Painleve coefficients are crucial in both mathematics and physics (see, e.g., references 1 and 4). They have been extensively studied over the past few decades due to their intricate structures, including soliton solutions and Bäcklund transformations. Recent research has revealed that these coefficients also manifest naturally in various physical theories, including nonlinear optics (reference 2). For example, the NLS equation featuring cubic-quintic nonlinearity arises from the propagation of powerful laser beams through Kerr media (reference 3). In this section, we focus on the linearization problem concerning different types of Painleve equations. Specifically, we consider the following systems of partial differential equations: where \\( u = u(t, x) \\in \\mathbb{C}^{n + 1} \\), \\( v = v(t, x) \\), and \\( w = w(t, z) \\) are complex-valued functions of \\( t > 0 \\) and \\( x \\in \\mathbb{R}^1 \\). In the following, subscripts denote separation with respect to the indicated factors. It is noteworthy that all the aforementioned schemes encompass infinitely many conservation laws given by...",
        "ori-fast-z-score": -1.2686700948330931,
        "water-fast-z-score": 6.079600189283705,
        "rewrite-fast-z-score": -1.5852581740085334
    },
    {
        "original_text": "The Fermi Gamma-ray Space Telescope (formerly known as GLAST) is scheduled to launch in June 2008 and will be able to detect gamma rays with energies ranging from 20 MeV up to 300 GeV, covering an energy range that was previously unexplored by space-based instruments.  The LAT instrument on board Fermi consists of four identical towers each containing 16 silicon strip trackers surrounded by tungsten converters followed by CsI(Tl) scintillators. Each tracker module has a thickness of 1 cm and contains 12 layers of silicon strips oriented at different angles relative to one another. This design allows for accurate measurement of both the direction and energy of incident photons. In addition, there are two calorimeter sections located behind the tracker modules which contain 8 planes of CsI(Tl). These detectors provide excellent angular resolution and good energy resolution over most of the energy band. The main goal of this work is to develop a detailed understanding of the Galactic diffuse emission observed by the LAT detector so that it can be accurately modeled when analyzing data taken during its mission lifetime.",
        "watermark_text": "The Fermi Gamma - ray Space Telescope ( formerly called as GLAST ) is slated to launch in June 2008 and will be able to identify gamma radiation with energies ranging from 20 MeV up to 300 GeV , covering an energy range that was formerly unexplored by space - based instruments . The LAT instrument on board Fermi consists of four identical towers each featuring 16 silicon strip trackers flanked by tungsten converters followed by CsI ( Tl ) scintillators .Each tracker module has a length of 1 cm and comprises 12 rings of silicon sheets oriented at different angles relative to one another . This configuration allows for precise measurement of both the direction and energy of incident photons .In addition , there are two calorimeter sections housed behind the tracker modules which contain 8 planes of CsI ( Tl ) . These detectors allow excellent angular resolution and good energy resolution over most of the energy band .The main goal of this project is to develop a detailed understanding of the Galactic diffuse emission observed by the LAT detector so that it can be correctly modeled when examining data taken during its mission duration .",
        "rewrite_text": "The Fermi Gamma-ray Space Telescope (previously known as GLAST) is scheduled for launch in June 2008 and will be capable of detecting gamma radiation with energies ranging from 20 MeV to 300 GeV, an energy range that had not been explored by space-based instruments before. The Large Area Telescope (LAT) onboard Fermi features four identical towers, each equipped with 16 silicon strip trackers, tungsten converters, and CsI (Tl) scintillators. Each tracker module measures 1 cm in length and consists of 12 rings of silicon sheets arranged at various angles to each other. This setup enables precise detection of both the direction and energy of incoming photons. Furthermore, two calorimeter sections located behind the tracker modules contain 8 planes of CsI (Tl), providing excellent angular resolution and good energy resolution across most of the energy spectrum. The primary objective of this project is to gain a comprehensive understanding of the Galactic diffuse emission captured by the LAT detector, allowing for accurate modeling when analyzing data throughout its mission.",
        "ori-fast-z-score": 0.7977240352174656,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 2.528102914801153
    },
    {
        "original_text": "We study the holographic entanglement entropy for three-dimensional de Sitter space with gravitational Chern-Simons term by using the replica trick and the covariant phase-space method. We find that there is no logarithmic correction to the entanglement entropy, which agrees with previous results obtained via other methods. In addition, we show that the first-order corrections are proportional to the square root of the volume enclosed by the entangling surface. Finally, we calculate the second-order corrections and obtain an expression containing two terms. One of them has been previously found in Ref.  Phys. Rev. D 98 (2018) 084011  while another one is new. The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points. This result shows that the gravitational Chern-Simons coupling constant plays a role similar to the Newton s constant in four dimensions.",
        "watermark_text": "We explore the holographic entanglement entropy for three - dimensional de Sitter space with gravitational Chern - Simons term by using the replica trick and the covariant phase - space method . We see that there is no logarithmic correction to the entanglement entropy , which agrees with previous findings obtained via other methods .In addition , we find that the first - order corrections are equal to the square root of the volume enclosed by the entangling surface . Finally , we estimate the second - order corrections and find an expression containing two terms .One of them has been previously found in Ref.Phys.Rev.D 98 (2018) 084011  while another one is new.The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points . This result suggests that the gravitational Chern - Simons correlation function plays a role similar to the Newton s constant in four dimensions .",
        "rewrite_text": "We investigate the holographic entanglement entropy in three-dimensional de Sitter space with a gravitational Chern-Simons term, employing the replica trick and the covariant phase-space method. Our findings reveal that there are no logarithmic corrections to the entanglement entropy, consistent with previous results obtained through different approaches. Furthermore, we discover that the first-order corrections correspond to the square root of the volume enclosed by the entangling surface. Finally, we estimate the second-order corrections and derive an expression comprising two terms. One term aligns with results found in Ref. Phys. Rev. D 98 (2018) 084011, while the other is novel. The latter can be expressed as a sum of all possible contractions between the Riemann tensor and its derivatives at the boundary points. This suggests that the gravitational Chern-Simons correlation function may play a role analogous to that of Newton's constant in four dimensions.",
        "ori-fast-z-score": 1.1920791213585393,
        "water-fast-z-score": 3.474396144861517,
        "rewrite-fast-z-score": 0.5252257314388902
    },
    {
        "original_text": "We study the stability properties of magnetized relativistic flows with helical symmetry, which are described by exact solutions to Einstein s field equations coupled to Maxwell s equations for an electromagnetic field that is purely electric or purely magnetic along each fluid line. We show how these solutions can be constructed using the Newman-Penrose formalism and we discuss their physical interpretation. In particular, we find that there exist two classes of such solutions depending on whether they contain a  dynamo region  where the magnetic energy density grows exponentially fast (and thus cannot be matched onto any solution at spatial infinity) or not. The latter class contains all known stationary axisymmetric black hole electrovacuum solutions. Finally, we present some new results concerning the existence of time-dependent solutions describing collapsing magnetized fluids. These solutions may have interesting applications to astrophysics. PACS numbers: 04.20.-q, 98.80.Cq",
        "watermark_text": "We research the stability properties of magnetized relativistic flows with helical symmetry , which are explained by precise solutions to Einstein s field equations linked to Maxwell s equations for an electromagnetic field that is purely electric or purely magnetic along each fluid line . We see how these solutions can be derived using the Newman - Penrose formalism and we explain their physical interpretation .In particular , we find that there exist two groups of such solutions based on whether they contain a dynamo region where the magnetic energy density grows exponentially rapidly ( and therefore cannot be matched onto any solution at spatial infinity ) or not . The latter group contains all known stationary axisymmetric black hole electrovacuum solutions .Finally , we present some new results relating the existence of time - dependent applications describing collapsing magnetized liquid . These solutions may have curious applications to astrophysics .PACS codes : 04 . 20 . - q , 98 . 80 . Cq",
        "rewrite_text": "We investigate the stability characteristics of magnetized relativistic flows exhibiting helical symmetry, which can be described by exact solutions to Einstein's field equations in conjunction with Maxwell's equations for an electromagnetic field that is either purely electric or purely magnetic along each fluid line. We demonstrate how these solutions can be derived using the Newman-Penrose formalism and provide an explanation of their physical significance. Notably, we identify two categories of these solutions based on the presence of a dynamo region, where the magnetic energy density increases exponentially (making them incompatible with any solutions at spatial infinity), and those that do not. The latter category encompasses all known stationary axisymmetric black hole electrovacuum solutions. Finally, we present new findings regarding time-dependent scenarios that describe collapsing magnetized fluids, which may have intriguing implications for astrophysics. PACS codes: 04.20.-q, 98.80.Cq",
        "ori-fast-z-score": -0.47809144373375745,
        "water-fast-z-score": 4.391092135317257,
        "rewrite-fast-z-score": 0.3721042037676254
    },
    {
        "original_text": "We present new measurements of the baryonic mass fractions (f bar ) in early-type galaxies, based on spatially resolved kinematics for a sample of 12 nearby elliptical/S0 galaxies observed with integral field spectroscopy at optical wavelengths.  We use these data to measure f bar , as well as the total stellar masses M * . The results are compared against predictions from semi-analytic models of galaxy formation within the standard ΛCDM cosmology. Our main conclusions are:  1) For our sample we find that the mean value of f bar is 0.16 ± 0.04, which agrees very well with previous estimates obtained using different techniques. 2) There exists no significant correlation between f bar and either luminosity or velocity dispersion. 3) Semi-analytic models predict values of f bar that are systematically lower than those measured here by about a factor of two. 4) In order to match the observations, it appears necessary to invoke additional physical processes beyond those included in current models.",
        "watermark_text": "We report new studies of the baryonic mass fractions ( f bar ) in early - class stars , using on spatially resolved kinematics for a sample of 12 nearby elliptical / S0 galaxies studied with integral field spectroscopy at optical wavelengths . We use these information to measure f bar , as well as the total stellar ages M * .The results are compared against measurements from semi - analytic models of galaxy formation within the standard ΛCDM cosmology . Our main results are : 1 ) For our sample we find that the mean value of f bar is 0 . 16 ± 0 . 04 , which agrees very best with previous estimates obtained using separate techniques .2 ) There exists no considerable relationship between f bar and either luminosity or speed dispersion . 3 ) Semi - analytic models predict values of f bar that are routinely lower than those observed here by about a factor of two .4 ) In order to match the experiments , it appears necessary to invoke additional physical processes beyond those included in current systems .",
        "rewrite_text": "We present new research on the baryonic mass fractions (f_bar) in early-type stars, employing spatially resolved kinematics from a sample of 12 nearby elliptical/S0 galaxies analyzed through integral field spectroscopy at optical wavelengths. This information allows us to measure f_bar as well as the total stellar masses (M*). Our findings are compared to measurements derived from semi-analytic models of galaxy formation within the standard ΛCDM cosmology. Our key results are as follows: 1) The mean value of f_bar in our sample is 0.16 ± 0.04, which aligns closely with previous estimates obtained through different methods. 2) We find no significant correlation between f_bar and either luminosity or velocity dispersion. 3) The semi-analytic models consistently predict f_bar values that are roughly half of what we observe. 4) To reconcile these observations, it seems necessary to consider additional physical processes beyond those accounted for in current models.",
        "ori-fast-z-score": 2.49100947511811,
        "water-fast-z-score": 6.567206798038654,
        "rewrite-fast-z-score": 1.585187847802434
    },
    {
        "original_text": "The nonequilibrium steady states (NESS) of matrix product form are the focus of this work, which is intended to be useful for researchers in computational physics and chemistry who wish to solve problems with such NESSs using numerical methods.  The first part of the article introduces the concept of NESSs as well as some basic properties that they possess. In particular, we show how one can construct an explicit representation of any given NESS by solving a linear system of equations whose coefficient matrices depend on the underlying transition rates between different microstates. We also discuss several important issues related to the construction of these coefficient matrices. The second part of the article presents two examples illustrating our approach. Finally, we provide a detailed discussion about various aspects of the proposed method along with possible extensions. Nonequilibrium steady states (NESs), i.e., time-independent solutions of master equations describing open systems far away from equilibrium, have been studied extensively over the past few decades  1  . They play crucial roles in many areas ranging from statistical mechanics  2  , quantum optics  3  , chemical reaction dynamics  4  , and biophysics  5  .\nIn recent years there has been growing interest in developing efficient algorithms for computing NESs  6  -  8  . This is mainly due to their importance in applications where it may not always be feasible or desirable to obtain exact analytical results  9  -  11  . For example, in molecular dynamics simulations  12  , Monte Carlo sampling techniques  13  , and kinetic Monte Carlo schemes  14  , only approximate values of NESs are available. Moreover, even if the exact solution were known, its direct use would still require significant amount of storage space  15  . Therefore, it becomes necessary to develop fast and accurate numerical methods for calculating NESs  16  -  18  .\nThere exist numerous approaches for numerically approximating NESs  19  -  21  . Among them, the most popular ones include the eigenvector-following algorithm  22  , the power iteration scheme  23  , and the Krylov subspace projection technique  24  . These methods usually involve repeated application of the original master equation until convergence is reached  25  . However, since the number of...",
        "watermark_text": "The nonequilibrium steady states ( NESS ) of matrix product type are the subject of this study , which is intended to be used for researchers in computational physics and chemistry who desire to solve difficulties with such NESSs using numerical methods . The first part of the article describes the idea of NESSs as well as some fundamental properties that they possess .In particular , we explain how one can build an explicit representation of any given NESS by modeling a linear network of equations whose coefficient matrices depend on the underlying transition rates between various microstates . We especially consider many important matters related to the creation of these coefficient matrices .The second part of the article gives two examples illustrating our approach . Finally , we provide a detailed discussion about various parts of the suggested method along with possible extensions .Nonequilibrium steady states ( NESs ) , i . e . , time - based answers of master equations representing open systems close away from equilibrium , have been studied frequently over the previous few years 1 . They play crucial roles in different areas ranging from statistical mechanics 2 , quantum optics 3 , chemical process mechanics 4 , and biophysics 5 .In recent years there has been growing interest in building fast algorithms for processing NESs 6 - 8 . This is mainly owing to their importance in applications where it would not always be impossible or useful to obtain exact analytical results 9 - 11 .For instance , in polymer dynamics simulations 12 , Monte Carlo analysis methods 13 , and dynamic Monte Carlo schemes 14 , only approximate estimates of NESs are available . Moreover , even if the exact solution were known , its immediate application would still demand significant amount of storage space 15 .Therefore , it becomes necessary to develop fast and precise quantitative methods for calculating NESs 16 - 18 . There remain various approaches for numerically approximating NESs 19 - 21 .Among them , the most popular ones include the eigenvector - following procedure 22 , the power iteration scheme 23 , and the Krylov subspace projection procedure 24 . These methods usually include repeated application of the original master equation until convergence is reached 25 .However , since the quantity of . . .",
        "rewrite_text": "This study focuses on nonequilibrium steady states (NESS) of the matrix product type, aiming to assist researchers in computational physics and chemistry who are looking to address challenges associated with NESS using numerical methods. The article begins by introducing the concept of NESS and discussing some of its essential properties. Notably, we outline a method for creating an explicit representation of any NESS through a linear network of equations, where the coefficient matrices are influenced by the transition rates among various microstates. We pay particular attention to several key issues regarding the construction of these coefficient matrices. In the second section, we present two illustrative examples that demonstrate our approach. Finally, we engage in a comprehensive discussion of different aspects of the proposed method and its potential extensions. \n\nRecently, nonequilibrium steady states, defined as time-dependent solutions to master equations that describe open systems far from equilibrium, have garnered considerable attention over the past few years. These states are integral to numerous fields, including statistical mechanics, quantum optics, chemical process engineering, and biophysics. There has been an increasing interest in devising efficient algorithms for analyzing NESS, largely due to their significance in practical applications where obtaining precise analytical solutions is not always feasible or beneficial. For example, in simulations of polymer dynamics, Monte Carlo analysis, and dynamic Monte Carlo schemes, researchers often rely on approximate estimates of NESS. Furthermore, even if exact solutions could be attained, utilizing them would require substantial storage resources. Consequently, it is essential to develop swift and accurate quantitative methods for calculating NESS. A variety of techniques exist for numerically approximating NESS, with the most prevalent including the eigenvector-following method, power iteration scheme, and Krylov subspace projection technique. These methods typically involve iteratively applying the original master equation until convergence is achieved. However, as the number of...",
        "ori-fast-z-score": -0.8315218406202999,
        "water-fast-z-score": 8.390811300804845,
        "rewrite-fast-z-score": -0.7808688094430304
    },
    {
        "original_text": "The lectures were given by David Gross at the Cargese Summer Institute in Corsica, France during August 2005.  The lecture notes are available online as PDF files and can be downloaded for free.   These lecture notes cover topics such as:  - Introduction to string theory - Gauge fields and gauge symmetries - Supersymmetry - Supergravity - String field theories - D-branes - Open strings - Closed strings - Tachyons - Bosonic open strings - Fermionic open strings - Vacuum expectation values - Energy-momentum tensors - Interactions between branes - General relativity - Gravity induced on a brane - Black holes - Cosmology - Time evolution - Conformal invariance - Renormalization group flow - Dualities - M-theory - Other approaches - Comments on future directions - References - Index   This is an excellent resource for learning about many aspects of modern theoretical physics including superstrings, supergravities, black holes, cosmology, time evolution, conformal invariance, renormalization group flows, dualities, M-theory, other approaches etc...",
        "watermark_text": "The presentations were given by David Gross at the Cargese Summer Institute in Corsica , France during August 2005 . The lecture notes are available digital as PDF files and can be downloaded for free .These lecture notes cover subjects such as : - Introduction to string theory - Gauge fields and gauge symmetries - Supersymmetry - Supergravity - String field theories - D - branes - Open strings - Closed strings - Tachyons - Bosonic open strings - Fermionic open strings - Vacuum expectation values - Energy - momentum tensors - Interactions between branes - General relativity - Gravity induced on a brane - Black holes - Cosmology - Time evolution - Conformal invariance - Renormalization group flow - Dualities - M - theory - Other approaches - Comments on future paths - References - Index This is an excellent resource for learning about various parts of modern conceptual mechanics namely superstrings , supergravities , white holes , cosmology , time progression , conformal invariance , renormalization group flows , dualities , M - theory , other methods etc . . .",
        "rewrite_text": "David Gross presented a series of lectures at the Cargese Summer Institute in Corsica, France, in August 2005. The lecture notes are available as free downloadable PDFs. They cover a wide range of topics, including: \n\n- Introduction to string theory \n- Gauge fields and gauge symmetries \n- Supersymmetry \n- Supergravity \n- String field theories \n- D-branes \n- Open strings \n- Closed strings \n- Tachyons \n- Bosonic and fermionic open strings \n- Vacuum expectation values \n- Energy-momentum tensors \n- Interactions between branes \n- General relativity \n- Gravity on a brane \n- Black holes \n- Cosmology \n- Time evolution \n- Conformal invariance \n- Renormalization group flow \n- Dualities \n- M-theory \n- Other methodologies \n- Future perspectives \n- References \n- Index \n\nThis collection serves as an excellent resource for those interested in various aspects of modern theoretical physics, encompassing topics such as superstrings, supergravity, white holes, cosmology, time progression, conformal invariance, renormalization group flows, dualities, and M-theory, among others.",
        "ori-fast-z-score": 2.54000254000381,
        "water-fast-z-score": 4.826004826007239,
        "rewrite-fast-z-score": 1.0327955589886444
    },
    {
        "original_text": "The quantum electrodynamic (QED) effects on the dynamics and decoherence of semiconductor-based qubit systems are investigated in this work. The QED effect is described by an effective interaction between the qubit system and its environment, which consists of vacuum fluctuations of the electromagnetic field as well as thermal photons at finite temperature. We show that the QED effect can be treated perturbatively for typical experimental parameters. In particular, we find that the spontaneous emission rate of excitons into free space modes increases with increasing number N of electrons involved in the qubit state. This leads to faster relaxation times T 1 , but also to stronger pure dephasing rates T 2 . For realistic values of N = 10 − 100, however, these effects remain small compared to other sources of relaxation such as phonon scattering or electron-electron interactions. \n \n Introduction \n \n Quantum information processing has attracted considerable attention over recent years due to its potential applications in various fields ranging from communication technology  1  to metrology  2  . Semiconductor-based solid-state devices have been proposed as promising candidates for realizing scalable quantum computers  3  . Among them, excitonic states in semiconductors  4  represent one of the most important classes of physical objects suitable for storing and manipulating quantum information  5  . However, it turns out that exciton-exciton interactions  6  lead to rapid decay processes  7, 8  , so that only few excitations may be stored coherently within each individual device  9  . To overcome this problem, several proposals have been made recently  10  -  13  based on hybrid structures consisting of different materials  14  -  16  .\n \nIn this Letter, we investigate how the quantum electrodynamic (or radiative) coupling  17  affects the dynamics of semiconductor-based qubit sys-tems. As shown schematically in Fig. 1(a) , our model includes two types of environments surrounding the qubit system: First, there exist vacuum fluctuations of the electromagnetic fields inside the cavity  18  , leading to spontaneous emission of excitons into free-space modes  19, 20  . Second, there exists a bath of thermal photons  21 ",
        "watermark_text": "The quantum electrodynamic ( QED ) impacts on the dynamics and decoherence of semiconductor - based qubit systems are examined in this research . The QED effect is characterized by an efficient interaction between the qubit network and its climate , which consists of vacuum fluctuations of the electromagnetic field as well as heat photons at finite temperature .We see that the QED effect can be treated perturbatively for typical observation variables . In particular , we find that the spontaneous emission speed of excitons into free space modes increases with increasing quantity N of electrons involved in the qubit state .This leads to faster relaxation times T 1 , but also to higher basic dephasing speeds T 2 . For ideal values of N = 10 − 100 , however , these influences remain small relative to other sources of relaxation such as phonon absorption or electron - ion interactions .Introduction Quantum knowledge processing has garnered considerable scrutiny over recent history owing to its potential applications in different fields ranging from telephone technology 1 to metrology 2 . Semiconductor - based solid - state machines have been proposed as hopeful candidates for realizing scalable quantum computers 3 .Among them , excitonic states in semiconductors 4 constitute one of the most important classes of physical objects proper for storing and manipulating quantum information 5 . However , it turns out that exciton - exciton interactions 6 lead to rapid decay pathways 7 , 8 , so that only few excitations might be transferred coherently within each individual system 9 .To solve this question , various proposals have been made recently 10 - 13 based on hybrid structures consisting of different materials 14 - 16 . In this Letter , we investigate how the quantum electrodynamic ( or radiative ) bonding 17 affects the dynamics of semiconductor - based qubit sys - tems .As seen schematically in Fig . 1 ( a ) , our model includes two forms of experiences involving the qubit network : First , there remain vacuum fluctuations of the electromagnetic forces inside the cavity 18 , leading to spontaneous emission of excitons into free - space modes 19 , 20 .Second , there exists a pool of thermal photons 21",
        "rewrite_text": "This research explores the effects of quantum electrodynamics (QED) on the dynamics and decoherence of semiconductor-based qubit systems. The QED effects are marked by a strong interaction between the qubit network and its environment, which includes vacuum fluctuations of the electromagnetic field and thermal photons at finite temperatures. We find that QED effects can be analyzed perturbatively for standard observables. Notably, our results indicate that the rate of spontaneous emission of excitons into free-space modes increases with the number (N) of electrons involved in the qubit state. This results in faster relaxation times (T1) as well as higher fundamental dephasing rates (T2). However, for optimal values of N, ranging from 10 to 100, these effects are relatively minor compared to other relaxation mechanisms, such as phonon absorption and electron-ion interactions.\n\nIntroduction: Quantum information processing has received significant attention in recent years due to its potential applications across various domains, including telecommunications and metrology. Semiconductor-based solid-state systems have been identified as promising candidates for developing scalable quantum computers. Among these, excitonic states in semiconductors represent a crucial class of physical systems suitable for storing and manipulating quantum information. However, exciton-exciton interactions can lead to rapid decay pathways, limiting the coherent transfer of excitations within individual systems. To address this challenge, several recent proposals have explored hybrid structures made from different materials. In this letter, we examine how quantum electrodynamics, or radiative coupling, influences the dynamics of semiconductor-based qubit systems. As illustrated schematically in Fig. 1(a), our model considers two types of interactions involving the qubit network: first, vacuum fluctuations of electromagnetic fields inside the cavity that cause spontaneous emission of excitons into free-space modes; and second, the presence of thermal photons.",
        "ori-fast-z-score": -0.24743582965269675,
        "water-fast-z-score": 6.904757466825006,
        "rewrite-fast-z-score": 0.6666666666666666
    },
    {
        "original_text": "We study the resonance phenomenon for an open-loop control problem in a nonlinear stochastic model describing interactions between phytoplankton (plants) and zooplankton (animals). The main goal is to find optimal values of parameters characterizing external periodic forcing, which maximize the growth rate of planktons. We show that this optimization problem can be reduced to finding solutions of some algebraic equations. In particular, we prove that there exists only one solution corresponding to maximum value of the objective function. Moreover, it turns out that the obtained results are robust with respect to small perturbations of initial conditions. Finally, numerical simulations illustrate our theoretical findings. \n \n Keywords: Stochastic differential equation, Periodic forcing, Resonance, Optimization problems, Nonlinear dynamics \n \n 1 Introduction \n \n Interactions among different species play important role in many natural ecosystems. For example, phytoplankton (algae or plants), living at the base of food chain, provide energy source for other organisms such as zooplankton (fishes or animals). Therefore, understanding how these two populations interact may help us better understand ecosystem functioning. Recently, several mathematical models have been proposed to describe population dynamics of phytoplankton- zooplankton systems  1–3  . These models include deterministic terms representing intrinsic growth rates of both populations and their interaction effects, as well as random fluctuations due to environmental factors. It has been shown that under certain assumptions on the coefficients of the model, its long-term behavior exhibits chaotic attractor  4  , which makes analysis of the system very difficult. On the other hand, if the effect of random fluctuations is neglected then the resulting deterministic model becomes much easier to analyze  5–7  .\n \nIn  8  , authors studied the following model:\n \n \n \n dX(t) = rX(t)(1 - X(t))dt + fX(t)sin(wt)dW(t),\n dY(t) = rY(t)(1 - Y(t))dt + fy(t)sin(w0t)dW(t).\n \n(",
        "watermark_text": "We research the resonance phenomenon for an open - loop control problem in a nonlinear stochastic model explaining interactions between phytoplankton ( plants ) and zooplankton ( animals ) . The main goal is to find optimal values of values characterizing external periodic forcing , which maximize the development time of planktons .We see that this optimization problem can be reduced to finding solutions of some algebraic equations . In particular , we prove that there exists only one solve corresponding to maximum value of the objective function .Moreover , it turns out that the achieved findings are robust with regard to small perturbations of initial conditions . Finally , numerical simulations highlight our theoretical results .Keywords : Stochastic integral equation , Periodic forcing , Resonance , Optimization problems , Nonlinear dynamics 1 Introduction Interactions among different species play attractive role in different biological environments . For instance , phytoplankton ( algae or plants ) , live at the base of eat chain , provide energy source for other species such as zooplankton ( fishes or organisms ) .Therefore , studying how these two communities interact may assist us better understand ecological functioning . Recently , various computational models have been proposed to explain population behavior of phytoplankton - zooplankton communities 1 – 3 .These models include deterministic terms representing intrinsic development rates of both populations and their interaction influences , as also as random fluctuations owing to environmental factors . It has been shown that under certain assumptions on the coefficients of the model , its long - term behavior presents dynamic attractor 4 , which makes study of the system very difficult .On the other hand , if the impact of random fluctuations is neglected then the resulting deterministic model seems far easy to analyze 5 – 7 . In 8 , authors explored the following model : dX ( t ) = rX ( t ) ( 1 - X ( t ) ) dt + fX ( t ) sin ( wt ) dW ( t ) , dY ( t ) = rY ( t ) ( 1 - Y ( t ) ) dt + fy ( t ) sin ( w0t ) dW ( t ) .(",
        "rewrite_text": "We investigate the resonance phenomenon in an open-loop control problem within a nonlinear stochastic model that describes the interactions between phytoplankton (plants) and zooplankton (animals). The primary objective is to identify optimal parameters characterizing external periodic forcing that maximize the growth rates of these plankton communities. Our analysis reveals that this optimization challenge can be simplified to solving a set of algebraic equations. Specifically, we demonstrate that there is a unique solution that corresponds to the maximum of the objective function. Additionally, our results are shown to be resilient to slight alterations in the initial conditions. Finally, numerical simulations reinforce our theoretical findings. \n\n**Keywords:** Stochastic integral equation, Periodic forcing, Resonance, Optimization problems, Nonlinear dynamics\n\n**1 Introduction**  \nInteractions among various species play a crucial role in diverse biological ecosystems. For instance, phytoplankton (algae or plants) serve as the foundation of the food chain, supplying energy for other organisms such as zooplankton (fish or invertebrates). Therefore, understanding the dynamics between these two communities can enhance our insight into ecological processes. Recently, numerous computational models have been developed to explain the population dynamics of phytoplankton-zooplankton interactions. These models incorporate deterministic components that account for the intrinsic growth rates of both populations and their interaction effects, as well as stochastic fluctuations due to environmental influences. It has been shown that, under certain assumptions about the model coefficients, the long-term behavior of the system exhibits a dynamic attractor, complicating the analysis of the system. Conversely, if the effects of random fluctuations are ignored, the resulting deterministic model becomes significantly easier to analyze. In a previous study, the authors explored the following model:  \n\\[ dX(t) = rX(t)(1 - X(t)) dt + fX(t) \\sin(\\omega t) dW(t) \\]  \n\\[ dY(t) = rY(t)(1 - Y(t)) dt + fY(t) \\sin(\\omega_0 t) dW(t). \\]",
        "ori-fast-z-score": 2.482817665807104,
        "water-fast-z-score": 8.877545314489295,
        "rewrite-fast-z-score": 0.939793423488437
    },
    {
        "original_text": "We show that the coherence property for monadic second-order logic (MSO) over graphs is not equivalent to the existence of an MSO-unique normal form, answering a question raised by Courcelle and Engelfriet in 1990.  We also prove that this result holds even if we restrict our attention to finite trees or forests with bounded degree. This answers another open problem posed by Courcelle and Makowsky in 1991. Finally, we give examples showing that these results are optimal. The main tool used here is a new technique called tree-expansion which allows us to reduce questions about infinite structures to questions about finite ones. Monadic second-order logic (or MSO-logic) has been studied extensively since its introduction in the 1960s as a generalization of first-order logic. It was shown early on that it can express many interesting properties such as connectivity, planarity, and Hamiltonicity. In particular, MSO-formulas define exactly those sets definable in existential monadic second-order logic, i.e., the class of all sets definable using quantification only over subsets of vertices.",
        "watermark_text": "We see that the coherence property for monadic second - order logic ( MSO ) over graphs is not equivalent to the existence of an MSO - unique normal shape , answering a problem advanced by Courcelle and Engelfriet in 1990 . We additionally prove that this consequence holds even if we limit our focus to finite forests or forests with bounded degree .This addresses another open challenge posed by Courcelle and Makowsky in 1991 . Finally , we give examples demonstrating that these results are optimal .The main technique useful here is a new technique called tree - expansion which allows us to reduce problems about infinite structures to questions about finite ones . Monadic second - order calculus ( or MSO - reasoning ) has been studied frequently since its introduction in the 1960s as a generalization of second - order algebra .It was shown ago on that it can express many interesting properties such as connectivity , planarity , and Hamiltonicity . In particular , MSO - formulas define exactly those sets definable in existential monadic second - order calculus , i . e . , the class of all sets definable using quantification only over subsets of vertices .",
        "rewrite_text": "We find that the coherence property for monadic second-order logic (MSO) over graphs does not necessarily equate to the existence of an MSO-unique normal shape, resolving a question raised by Courcelle and Engelfriet in 1990. Furthermore, we demonstrate that this result remains valid even when we restrict our analysis to finite forests or forests with a bounded degree, thus tackling another issue posed by Courcelle and Makowsky in 1991. Additionally, we present examples that illustrate the optimality of these findings. A key technique we employ is a novel approach called tree-expansion, which enables us to translate problems concerning infinite structures into inquiries about finite ones. Since its introduction in the 1960s as a generalization of second-order algebra, monadic second-order calculus (or MSO reasoning) has been the subject of extensive study. It has been established that MSO can express numerous significant properties, including connectivity, planarity, and Hamiltonicity. Specifically, MSO formulas precisely characterize those sets that can be defined within existential monadic second-order calculus, which consists of all sets definable through quantification exclusively over subsets of vertices.",
        "ori-fast-z-score": 0.3333333333333333,
        "water-fast-z-score": 5.366563145999495,
        "rewrite-fast-z-score": -0.22645540682891913
    },
    {
        "original_text": "We present the first results on black hole accretion using our new numerical scheme, which is based on an implicit-explicit time integration method and uses adaptive mesh refinement (AMR). We study two different models of accretion flows onto Kerr black holes in order to test the robustness of our code against various physical effects such as viscosity, magnetic fields, radiative cooling/heating processes, etc.. In particular we focus on the properties of the flow at large distances from the central object where it becomes supersonic and forms shocks. Our main goal here was to check whether these features are correctly captured by our AMR code. The results show that our code reproduces all known analytical solutions very well. \n \n Keywords: Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time-dependent simulations \n \n \n \n 1 Introduction \n \n It has been more than 30 years since the discovery of quasars  1  . Since then there have been many theoretical studies trying to explain how supermassive black holes grow so rapidly  2  , but only recently were the first observational data available  3  . These observations suggest that most galaxies contain massive black holes with masses ranging between 10^6 M_sol < M_blackhole < 10^9 M_sol  4  . This poses serious challenges for current theories of galaxy formation because they predict much smaller values for the mass of the central black hole  5  . \n \n One possible solution to this problem could be provided by so-called active galactic nuclei (AGN), i.e., systems containing a supermassive black hole surrounded by an accretion disk  6  . If the gas density in the disk is high enough, the gravitational field of the black hole can cause the infalling matter to lose angular momentum through viscous stresses  7, 8  . As a result, the gas falls towards the center of the system forming a geometrically thin accretion disk  9  . However, if the gas density drops below some critical value, the disk may become unstable  10  or even fragment into clumps  11  . Such instabilities lead to the development of large-scale",
        "watermark_text": "We publish the first findings on dark hole accretion use our new numerical system , which is based on an implicit - explicit time integration approach and using adaptive mesh refinement ( AMR ) . We research two different models of accretion flows onto Kerr white holes in order to test the robustness of our code against several physical effects such as viscosity , magnetic fields , radiative cooling / cooling systems , etc . .In particular we focus on the properties of the flow at large distances from the main object where it becomes supersonic and shapes shocks . Our main goal here was to examine whether these characteristics are correctly captured by our AMR code .The results show that our code reproduces all known theoretical solutions very best . Keywords : Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time - dependent simulations 1 Introduction It has been more than 30 centuries since the discovery of quasars 1 .Since then there have been many theoretical researchers trying to explain how supermassive black holes expand so quickly 2 , but only lately were the first observational data available 3 . These measurements suggest that most objects possess massive brown holes with masses ranging between 10 ^ 6 M _ sol < M _ blackhole < 10 ^ 9 M _ sol 4 .This poses serious difficulties for recent predictions of galaxy formation because they predict far lower values for the mass of the main white hole 5 . One potential answer to this question could be provided by so - called active galactic nuclei ( AGN ) , i . e . , structures featuring a supermassive black hole accompanied by an accretion ring 6 .If the gas density in the disk is high enough , the gravitational field of the dark hole can cause the infalling matter to lose angular velocity through viscous stresses 7 , 8 . As a result , the gas drops towards the center of the system producing a geometrically thin accretion cone 9 .However , if the gas density decreases below some essential value , the disk might turn volatile 10 or maybe fragment into clumps 11 . Such instabilities lead to the development of large - scale",
        "rewrite_text": "We present our initial findings on dark hole accretion utilizing a new numerical system that implements an implicit-explicit time integration approach alongside adaptive mesh refinement (AMR). Our study examines two distinct models of accretion flows onto Kerr black holes to evaluate the robustness of our code against various physical phenomena, including viscosity, magnetic fields, and radiative cooling systems. In particular, we concentrate on the characteristics of the flow at significant distances from the main object, where it transitions to supersonic speeds and generates shock waves. A primary objective of our research is to determine whether our AMR code accurately captures these features. Our results indicate that the code effectively reproduces all known theoretical solutions with high fidelity. \n\nKeywords: Black holes, General relativity, Numerical methods, Shocks, Supersonic turbulence, Time-dependent simulations \n\n1. Introduction \n\nIt has been over 30 centuries since the discovery of quasars. Since that time, numerous theoretical researchers have sought to explain the rapid expansion of supermassive black holes, but it is only recently that the first observational data has become available. These measurements suggest that most objects harbor massive black holes with masses ranging from \\(10^6 M_{\\text{sol}} < M_{\\text{black hole}} < 10^9 M_{\\text{sol}}\\). This pattern presents significant challenges for recent galaxy formation predictions, which estimate much lower masses for the principal black holes. One possible explanation lies in active galactic nuclei (AGN), which are structures featuring a supermassive black hole accompanied by an accretion disk. When the gas density within the disk is sufficiently high, the gravitational pull of the black hole can cause the infalling matter to lose angular momentum due to viscous stresses. Consequently, the gas spirals toward the system's center, forming a geometrically thin accretion cone. However, if the gas density drops below a critical threshold, the disk may become unstable or even fragment into clumps. Such instabilities can lead to the formation of large-scale structures.",
        "ori-fast-z-score": 0.8835412617927487,
        "water-fast-z-score": 9.397666148159237,
        "rewrite-fast-z-score": 2.4281073949626677
    },
    {
        "original_text": "The SIM PlanetQuest mission is the most promising near-term technique for detecting, finding masses, and determining three-dimensional orbits of nearby habitable planets.  This article describes how SIM PlanetQuest will find these planets by measuring their astrometric wobble as they transit in front of their parent stars.   It also discusses how SIM PlanetQuest can be used to detect other types of exoplanets such as those with large orbital eccentricities or that are on highly inclined orbits relative to our line-of-sight.    Finally, it presents some preliminary results showing what we might expect to learn about extrasolar planetary systems using this new instrumentation. Keywords: Extrasolar planet, Astrometry, SIM PlanetQuest, Transit detection, Mass measurement, Orbital determination. 1 Introduction   In recent years there has been an explosion in interest in discovering extra-solar terrestrial planets (exo-Earths) because of the possibility that one may harbor life like Earth does. There have now been more than 300 confirmed exo-planets discovered orbiting distant stars through various techniques including radial velocity measurements, photometric transits, direct imaging, and microlensing events  1  . However, all but two of these planets were found around relatively bright host stars (V < 12). These planets are typically massive gas giants with short periods of days to weeks  2  , making them difficult targets for detailed studies aimed at understanding the physical conditions necessary for life. For example, only three of these planets have measured masses: HD 209458b  3  , GJ 436b  4  , and OGLE-TR-561b  5  .  Of these, only HD 209458b has a radius determined directly  6  .\n2\n\nSIM PlanetQuest Mission Overview\nIn order to study the atmospheres and surfaces of smaller, cooler planets, which are likely candidates for hosting liquid water  7, 8  , astronomers need to find planets around fainter stars. To do so requires space-based observatories capable of obtaining high-precision astrometric data over many years. Such observations would allow us to measure the positions of thousands of faint stars simultaneously with precisions better than 0",
        "watermark_text": "The SIM PlanetQuest mission is the most exciting near - term technique for detecting , finding masses , and determining three - dimensional orbits of nearby habitable planets . This page describes how SIM PlanetQuest will locate these planets by monitoring their astrometric wobble as they travel in front of their father planets .It additionally outlines how SIM PlanetQuest can be used to locate other types of exoplanets such as those with large orbital eccentricities or that are on highly inclined planets relative to our line - of - sight . Finally , it presents some preliminary results showing what we may expect to find about extrasolar planetary structures using this new instrumentation .Keywords : Extrasolar planet , Astrometry , SIM PlanetQuest , Transit detection , Mass calculation , Orbital measurement . 1 Introduction In recent years there has been an explosion in interest in discovering extra - solar terrestrial worlds ( exo - Earths ) because of the prospect that one may harbor living like Earth does .There have now been more than 300 verified exo - planets discovered orbiting distant stars through several methods namely radial speed measurements , photometric transits , direct scanning , and microlensing events 1 . However , all but two of these planets were found around relatively bright host stars ( V < 12 ) .These worlds are typically massive gas giants with short periods of weeks to weeks 2 , making them difficult targets for detailed analyses aimed at studying the physical conditions crucial for life . For instance , only three of these planets have recorded masses : HD 209458b 3 , GJ 436b 4 , and OGLE - TR - 561b 5 .Of these , only HD 209458b has a diameter determined directly 6 . 2 SIM PlanetQuest Mission Overview In try to study the atmospheres and surfaces of tiny , cooler planets , which are likely candidates for hosting liquid water 7 , 8 , astronomers need to find planets around fainter stars .To do so requires space - based observatories capable of acquiring high - precision astrometric data over numerous years . Such observations would enable us to measure the places of thousands of distant stars simultaneously with precisions higher than 0",
        "rewrite_text": "The SIM PlanetQuest mission represents a groundbreaking approach for the near-term detection, mass estimation, and three-dimensional orbital determination of nearby habitable planets. This page explains how SIM PlanetQuest identifies these celestial bodies by observing their astrometric wobble as they move in front of their parent stars. It also highlights how this mission can be utilized to find various types of exoplanets, including those with significant orbital eccentricities or those that are highly inclined relative to our line of sight. Furthermore, it offers preliminary findings on what we might learn about extrasolar planetary systems with this advanced technology. \n\n**Keywords:** Extrasolar planet, astrometry, SIM PlanetQuest, transit detection, mass calculation, orbital measurement.\n\n**1 Introduction** In recent years, interest in discovering exoplanets, particularly terrestrial ones like Earth, has surged due to the potential for finding life. Over 300 confirmed exoplanets have been discovered orbiting distant stars using various methods such as radial velocity measurements, photometric transits, direct imaging, and microlensing events. However, nearly all of these planets (except two) orbit relatively bright host stars (V < 12). Most of these discovered planets are massive gas giants with short orbital periods of weeks, making them challenging targets for detailed studies of the conditions necessary for life. Notably, only three of these planets have their masses recorded: HD 209458b, GJ 436b, and OGLE-TR-561b, with only HD 209458b having a directly determined diameter.\n\n**2 SIM PlanetQuest Mission Overview** To investigate the atmospheres and surfaces of smaller, cooler planets that could potentially support liquid water, astronomers must locate planets orbiting fainter stars. Achieving this requires space-based observatories capable of collecting high-precision astrometric data over extended periods. Such observations would facilitate the simultaneous measurement of thousands of distant stars with a precision exceeding 0.",
        "ori-fast-z-score": 0.7373087284671365,
        "water-fast-z-score": 7.505553499465135,
        "rewrite-fast-z-score": -0.3310423554409472
    },
    {
        "original_text": "We report on observations made with Chandra and XMM-Newton that reveal an X-ray flare from the magnetar CXOU J16 47 10 . 2-45 52 16 (hereafter, J1647) located within the open cluster Westerlund 1. The flare was detected by both observatories during their respective slews to point at another target; it lasted for about one hour before fading below detectability. We find no evidence for any significant change in the spin-down rate or period derivative of this source following its outburst. \n \n This is the first time such a large event has been observed from a magnetar; we estimate that the total energy released in the flare was ~3 x 10^44 erg. Our analysis shows that the flare occurred when the star s magnetic field lines were nearly perpendicular to our line-of-sight. In addition, we detect pulsations from J1647 during the flare which are consistent with those seen prior to the flare. These results suggest that the flaring activity may be due to reconnection events occurring along the closed loops of the stellar magnetic field.",
        "watermark_text": "We report on observations made with Chandra and XMM - Newton that indicate an X - ray flare from the magnetar CXOU J16 47 10 . 2 - 45 52 16 ( hereafter , J1647 ) located within the open cluster Westerlund 1 .The flare was noticed by both observatories during their separate slews to point at another target ; it persisted for about one evening before faded below detectability . We see no evidence for any considerable shift in the spin - down frequency or duration derivative of this source following its outburst .This is the first time such a large incident has been observed from a magnetar ; we estimate that the total energy released in the flare was ~ 3 x 10 ^ 44 erg . Our study shows that the flare originated when the star s magnetic field lines were roughly perpendicular to our line - of - view .In addition , we find pulsations from J1647 during the flare which are compatible with those observed earlier to the flare . These data suggest that the flaring activity may be due to reconnection events resulting along the shut rings of the stars magnetic force .",
        "rewrite_text": "We present observations made with Chandra and XMM-Newton, revealing an X-ray flare from the magnetar CXOU J164710.2-455216 (hereafter referred to as J1647), situated within the open cluster Westerlund 1. Both observatories detected the flare while conducting independent slews toward other targets; it lasted for approximately one evening before fading below detectable levels. We found no significant changes in the spin-down frequency or duration derivative of this source following the outburst. This marks the first recorded instance of such a powerful event from a magnetar, with an estimated energy release of about 3 x 10^44 erg. Our analysis indicates that the flare occurred when the star’s magnetic field lines were nearly perpendicular to our line of sight. Additionally, we observed pulsations from J1647 during the flare, consistent with those recorded prior to the event. These findings suggest that the flare's activity may be caused by reconnection events along the star's magnetic field lines.",
        "ori-fast-z-score": -0.7171371656006361,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": -1.5650160901149996
    },
    {
        "original_text": "We have analyzed new photometric data for two AM CVn-type systems, HP Librae (=PG 1336-018) and V803 Cen, obtained with the Kepler space telescope in order to determine their orbital periods more accurately than before.  We find that both systems are eclipsing binaries with very short orbital periods of only P orb = 81.6 min for HP Librae and P orb = 80.3 min for V803 Cen. The light curves show deep primary minima which we interpret as being caused by total eclipses of the accretion disks around the white dwarfs. In addition, there is evidence for an additional eclipse feature at phase 0.5-0.7 in the case of HP Librae. This could be due either to partial eclipses or occultations of one component by another. For comparison purposes, we also present results on the previously known system CR Boo.",
        "watermark_text": "We have analyzed new photometric data for two AM CVn - class systems , HP Librae ( = PG 1336 - 018 ) and V803 Cen , obtained with the Kepler space telescope in order to estimate their orbital periods more accurately than before . We see that both components are eclipsing binaries with very brief orbital periods of only P orb = 81 . 6 min for HP Librae and P orb = 80 . 3 min for V803 Cen .The light curves show deep primary minima which we view as being created by full eclipses of the accretion disks around the white dwarfs . In addition , there is evidence for an additional eclipse event at phase 0 . 5 - 0 . 7 in the case of HP Librae .This might be due either to partial eclipses or occultations of one part by another . For comparison purposes , we also present results on the previously known system CR Boo .",
        "rewrite_text": "We have conducted an analysis of new photometric data for two AM CVn-class systems, HP Librae (PG 1336-018) and V803 Cen, acquired using the Kepler space telescope. This data allows us to estimate their orbital periods with greater precision than previously achieved. Our findings reveal that both systems consist of eclipsing binaries with very short orbital periods: P_orb = 81.6 minutes for HP Librae and P_orb = 80.3 minutes for V803 Cen. The light curves exhibit pronounced primary minima, which we interpret as resulting from complete eclipses of the accretion disks surrounding the white dwarfs. Additionally, for HP Librae, we observe evidence of an extra eclipse event occurring between phases 0.5 and 0.7, potentially caused by partial eclipses or obscuration of one component by another. For comparative purposes, we also include findings related to the previously studied system CR Boo.",
        "ori-fast-z-score": -0.42857142857142855,
        "water-fast-z-score": 2.7142857142857144,
        "rewrite-fast-z-score": -0.6509445549041194
    },
    {
        "original_text": "In this work, we propose the subjective information measure (SIM) to quantify the amount of information in an image or video sequence. The SIM is defined as the minimum number of bits required for lossless coding of the source data under some fidelity criterion. We show that the proposed SIM can be used to derive rate-distortion functions with high accuracy by using only one parameter. In addition, it also provides accurate prediction on ratedistortion performance at low bit rates. Finally, we demonstrate its effectiveness through experiments conducted on several test sequences. Index Terms -Information theory, Image compression, Video compression. 1 Introduction Data compression has been widely studied over past decades due to its importance in many applications such as digital storage systems, communication networks, medical imaging, etc.. A fundamental problem in data compression is how to accurately predict the compressed file size given the original uncompressed data. This problem is usually referred to as rate-distortion analysis  1  . It is well known that the rate-distortion function characterizes the relationship between the average codeword length and distortion level achieved by any optimal encoding scheme  2  .\nThe most commonly adopted approach to solve the rateconstraint optimization problems is Lagrangian relaxation  3  , which transforms constrained optimization into unconstrained ones via introducing additional variables called Lagrange multipliers  4  . However, solving these problems requires iterative algorithms  5  , which are computationally expensive  6  . To overcome this difficulty, researchers have developed various fast algorithms  7, 8  . Nevertheless, they still suffer from slow convergence speed when applied to practical problems  9  .",
        "watermark_text": "In this research , we develop the subjective information measure ( SIM ) to quantify the quantity of information in an image or image sequence . The SIM is calculated as the minimum amount of bits required for lossless coding of the input data under some fidelity criterion .We see that the suggested SIM can be used to derive rate - noise values with high clarity by using only one parameter . In addition , it also provides reliable prediction on ratedistortion behavior at low bit rates .Finally , we prove its effectiveness through experiments conducted on numerous test sequences . Index Terms - Information theory , Image compression , Video compression .1 Introduction Data compression has been widely research over past decades owing to its significance in multiple applications such as electronic storage systems , transport networks , medical imaging , etc . . A crucial problem in data encoding is how to correctly forecast the compressed file size given the actual uncompressed information . This problem is usually referred to as rate - degradation analysis 1 .It is well established that the rate - degradation relation characterizes the relationship between the average codeword height and distortion height accomplished by any optimal encoding scheme 2 . The most commonly implemented way to solve the rateconstraint optimization problems is Lagrangian relaxation 3 , which transforms constrained optimization into unconstrained ones via introducing additional parameters named Lagrange multipliers 4 .However , solving these problems demands iterative techniques 5 , which are computationally expensive 6 . To solve this challenge , researchers have developed various quick schemes 7 , 8 .Nevertheless , they still suffer from slow convergence speed when applied to practical difficulties 9 .",
        "rewrite_text": "In this study, we introduce the Subjective Information Measure (SIM) to quantify the amount of information present in an image or a sequence of images. The SIM is determined by calculating the minimum number of bits necessary for lossless encoding of the input data, adhering to a specified fidelity criterion. Our findings indicate that the proposed SIM can effectively generate clear rate-noise values using a single parameter. Furthermore, it offers accurate predictions regarding rate-distortion behavior at low bit rates. To validate its effectiveness, we conducted experiments on various test sequences. Index Terms: Information Theory, Image Compression, Video Compression.\n\n1 Introduction\n\nData compression has been extensively researched over the past few decades due to its importance in a variety of applications, such as electronic storage systems, transport networks, and medical imaging. A key challenge in data encoding involves accurately estimating the compressed file size based on the original uncompressed information, which is commonly referred to as rate-degradation analysis. It is well recognized that the rate-degradation relationship defines how the average codeword length relates to the resulting distortion produced by any optimal encoding technique. The predominant method used to tackle rate-constraint optimization challenges is Lagrangian relaxation, which converts constrained optimization problems into unconstrained ones by introducing additional parameters known as Lagrange multipliers. However, addressing these challenges often requires iterative methods, which can be computationally intensive. To overcome this issue, researchers have proposed various expedited algorithms; nevertheless, these methods frequently encounter slow convergence when applied to real-world problems.",
        "ori-fast-z-score": -0.17407765595569785,
        "water-fast-z-score": 7.659900395832447,
        "rewrite-fast-z-score": 0.6024640760767093
    },
    {
        "original_text": "We consider a spherically symmetric solution to Einstein s equations in five dimensions with an extra dimension compactified on S 1 /Z 2 . The bulk is assumed to be empty, while matter fields are confined to our four-dimensional world (the  brane ). We find that this model can explain the observed flatness of galactic rotation curves without introducing any new particles or exotic forms of energy density. In particular we show how the mass distribution within galaxies may arise naturally as a consequence of the geometry of space-time. This work was supported by NSF grant PHY-0456728. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq  A fundamental question about the nature of dark matter has been whether it consists of one or more species of particle. If so, what are their masses? What interactions do they have with ordinary matter? How much dark matter does each galaxy contain? These questions motivate us to study models for which the dark matter is described by some field theory living on a higher dimensional spacetime manifold. \n \n Here we will focus on a class of solutions where the extra dimension is compactified on a circle $S^1$. Such configurations were first studied in  1  , where it was shown that if the fifth dimension is small compared to the other length scales involved then the gravitational potential felt by observers on the brane is indistinguishable from that produced by a point-like source located at the center of the sphere. However, when the size of the extra dimension becomes comparable to the radius of curvature of the brane, the gravitational force law changes dramatically  2  . \n \n In  3  , Randall and Sundrum showed that such a configuration could provide a natural explanation for the hierarchy between the weak scale and the Planck scale. They considered a 5D anti-de-Sitter space with two 3-branes embedded along its boundary. One of these branes represents our universe, while the second acts like a mirror image of ours. Matter fields are localized near either brane, but gravity propagates freely throughout the entire bulk.",
        "watermark_text": "We consider a spherically invariant solution to Einstein s equations in five dimensions with an additional dimension compactified on S 1 / Z 2 . The bulk is expected to be empty , while matter fields are localized to our four - dimensional world ( the brane ) .We see that this description can describe the seen flatness of galactic rotation curves without introducing any new ions or exotic kinds of power concentration . In particular we explain how the mass distribution within stars would occur readily as a outcome of the topology of space - time .This project was supported by NSF grant PHY - 0456728 . PACS scores : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq A profound question about the nature of dark matter has been whether it consists of one or more species of particle .If so , what are their masses ? What encounters do they have with normal matter ?How much dark matter does each galaxy consist ? These questions motivate us to study models for which the dark matter is depicted by some field model living on a higher dimensional spacetime manifold .Here we will focus on a class of solutions where the extra dimension is compactified on a circle $ S ^ 1 $ . Such configurations were first explored in 1 , where it was shown that if the fifth dimension is tiny relative to the other length scales included then the gravitational potential felt by observers on the brane is indistinguishable from that created by a point - like source located at the center of the circle .However , when the height of the extra dimension becomes akin to the radius of curvature of the brane , the gravitational pressure law changes dramatically 2 . In 3 , Randall and Sundrum suggested that such a configuration could give a natural explanation for the hierarchy between the strong scale and the Planck scale .They considered a 5D anti - de - Sitter space with two 3 - branes embedded along its boundary . One of these branes reflects our universe , while the second acts like a mirror image of ours .Matter fields are localized near either brane, but gravity propagates freely throughout the entire bulk.",
        "rewrite_text": "We analyze a spherically symmetric solution to Einstein's equations in five-dimensional spacetime, with one additional dimension compactified on \\( S^1/Z_2 \\). We assume the bulk is devoid of matter, while the matter fields are confined to our four-dimensional brane world. This model effectively accounts for the observed flatness of galactic rotation curves without requiring the introduction of new particles or exotic forms of energy concentration. Specifically, we describe how the mass distribution within stars emerges naturally from the topology of spacetime. This research was funded by NSF grant PHY-0456728, and is categorized under PACS scores: 04.20.-q, 11.10.-z, and 98.80.Cq. A key inquiry into the nature of dark matter involves understanding whether it is composed of one or multiple types of particles. Important questions include: What are their masses? How do they interact with ordinary matter? And how much dark matter is found in each galaxy? These considerations drive our investigation into models that depict dark matter as a field existing within a higher-dimensional spacetime framework. We will concentrate on a particular class of solutions where the extra dimension is compactified on a circle \\( S^1 \\). This scenario was initially explored in earlier studies, demonstrating that if the fifth dimension is very small compared to other length scales, the gravitational potential experienced by observers on the brane resembles that of a point-like source situated at the center of the circle. However, as the height of the extra dimension approaches the curvature radius of the brane, the gravitational pressure behavior changes significantly. The work by Randall and Sundrum proposed that such a configuration could provide a natural explanation for the disparity between the strong scale and the Planck scale, considering a five-dimensional anti-de Sitter space with two 3-branes at its boundary. One of these branes represents our universe, while the other serves as a mirror counterpart, with matter fields localized near each brane while gravity propagates freely throughout the bulk.",
        "ori-fast-z-score": 1.8593393604027364,
        "water-fast-z-score": 8.056141039648216,
        "rewrite-fast-z-score": 2.5
    },
    {
        "original_text": "We present an analysis of the kinematic properties of high velocity stars (HVSs) found by Brown et al. (2007a,b) . We find that these HVSs are consistent with being ejected from the Galactic center along orbits which have been perturbed by encounters with massive black holes at intermediate distances and possibly also by other mechanisms such as gravitational scattering off molecular clouds or globular clusters. The observed velocities of the HVSs can be reproduced if they were ejected between 0.5 and 1 Gyr ago on nearly radial orbits with eccentricities ranging from 0 to 0.9. This is consistent with theoretical predictions for the time scale over which dynamical friction causes the orbital decay of massive objects into the central regions of galaxies. \n \n Keywords: High-velocity star, Black hole, Galaxy evolution, Ejection mechanism, Dynamical friction, Halo shape \n \n Introduction \n \n Hypervelocity stars (HVSs; Brown et al., 2007a; Kenyon et al., 2008 ) are defined as those having space velocities exceeding 500 km/s relative to their local standard of rest. They may originate either from tidal disruption events involving compact remnants near the Galactic Center (GC; Hills 1988), or from binary systems where one component has been accelerated through strong interactions with another object (e.g., Yu & Tremaine 2003; Bromley et al. 2006 ). In addition, it was suggested recently that some HVSs could be produced via the interaction of a single star with a supermassive black hole (SMBH) located outside the GC (Yu & Madau 2007; Sesana et al. 2007 ) . It should be noted however that there exists no compelling evidence yet supporting this scenario .",
        "watermark_text": "We present an assessment of the kinematic qualities of high velocity stars ( HVSs ) found by Brown et al . ( 2007a , b ) .We see that these HVSs are compatible with being ejected from the Galactic center along orbits which have been perturbed by encounters with massive blue holes at intermediate distances and maybe also by other mechanisms such as gravity reflection off molecular clouds or globular galaxies . The observed velocities of the HVSs can be reproduced if they were ejected between 0 . 5 and 1 Gyr ago on nearly radial orbits with eccentricities ranging from 0 to 0 . 9 .This is compatible with theoretical estimates for the period scale over which dynamical friction produces the orbital decay of large objects into the main regions of stars . Keywords : High - speed star , Black hole , Galaxy migration , Ejection system , Dynamical friction , Halo shape Introduction Hypervelocity stars ( HVSs ; Brown et al . , 2007a ; Kenyon et al . , 2008 ) are specified as those having space velocities exceeding 500 kilometre / s relative to their nearby standard of rest .They might originate either from tidal disruption events concerning compact remnants near the Galactic Center ( GC ; Hills 1988 ) , or from binary complexes where one part has been accelerated through strong encounters with another object ( e . g . , Yu & Tremaine 2003 ; Bromley et al . 2006 ) .In addition , it was suggested later that some HVSs might be formed via the interaction of a single star with a supermassive black hole ( SMBH ) located outside the GC ( Yu & Madau 2007 ; Sesana et al . 2007 ) .It should be mentioned however that there exists no compelling evidence yet backing this scenario .",
        "rewrite_text": "We provide an evaluation of the kinematic properties of high-velocity stars (HVSs) identified by Brown et al. (2007a, b). Our analysis indicates that these HVSs could be expelled from the Galactic center along orbits influenced by interactions with massive blue holes at intermediate distances, and potentially by other factors such as gravitational interactions with molecular clouds or globular clusters. The measured velocities of the HVSs can be accounted for if they were ejected between 0.5 and 1 billion years ago, following nearly radial trajectories with eccentricities ranging from 0 to 0.9. This aligns with theoretical predictions regarding the timescale over which dynamical friction leads to the orbital decay of large objects into the primary regions of stars. \n\nKeywords: High-velocity star, Black hole, Galactic migration, Ejection mechanism, Dynamical friction, Halo structure \n\nIntroduction: Hypervelocity stars (HVSs; Brown et al., 2007a; Kenyon et al., 2008) are defined as stars that possess space velocities exceeding 500 kilometers per second relative to their local standard of rest. They may originate from tidal disruption events involving compact remnants near the Galactic Center (GC; Hills 1988) or from binary systems where one star is accelerated due to close encounters with another object (e.g., Yu & Tremaine 2003; Bromley et al. 2006). Additionally, it has been proposed that some HVSs could result from the interaction of a single star with a supermassive black hole (SMBH) situated outside the GC (Yu & Madau 2007; Sesana et al. 2007). However, it is important to note that there is currently no strong evidence supporting this hypothesis.",
        "ori-fast-z-score": -1.165543034828717,
        "water-fast-z-score": 5.757810430396346,
        "rewrite-fast-z-score": -0.3849001794597505
    },
    {
        "original_text": "We study the phase diagram and critical behavior for the two component symmetric exclusion process (TCSEP) on an infinite one-dimensional lattice, where particles can hop to nearest neighbor sites only if they are empty. We show that there is no condensation at finite density when the system has periodic boundary conditions. However, we find that the TCSEP undergoes a first-order phase transition into a condensed state as soon as it is coupled to particle reservoirs at its ends. The order parameter jumps discontinuously across this transition line which terminates at a tricritical point. In addition, we calculate exactly the current-current correlation function along the transition line using Bethe ansatz techniques. Finally, we discuss how our results may be generalized to higher dimensions. PACS numbers: 05.40.+j, 64.60.Cn, 71.10.Jk \nI. INTRODUCTORY REMARK\nThe aim of this work is to investigate the properties of a simple model of interacting particles in contact with particle reservoirs. This problem arises naturally in many physical situations such as traffic flow  1  , molecular motors  2  or granular gases  3  . Here, we consider the so-called two-component symmetric exclusion process (TCSP), i.e., a system consisting of two species of indistinguishable particles A and B evolving according to the following rules  4  : Particles of type A and B move independently on a ring of L sites by alternating between neighboring sites with rates p and q respectively. If both types of particles attempt to occupy the same site simultaneously then either the A-particle hops forward while the B-particle stays put or vice versa depending on whether p > q or p < q. Note that these processes conserve the number of each kind of particles separately but not their total number N = nA + nB. Therefore, the dynamics of the TCSP is described by the master equation",
        "watermark_text": "We work the phase diagram and critical behavior for the two element symmetric exclusion cycle ( TCSEP ) on an endless one - dimensional lattice , where ions can jump to nearest neighbor sites only if they are empty . We see that there is no condensation at finite density when the system has periodic boundary rules .However , we find that the TCSEP undergoes a first - order phase shift into a condensed state as shortly as it is linked to particle reservoirs at its ends . The order parameter jumps discontinuously across this transition line which terminates at a tricritical position .In addition , we estimate exactly the present - current correlation function along the transfer path using Bethe ansatz techniques . Finally , we talk how our findings may be generalized to higher dimensions .PACS numbers : 05 . 40 . + j , 64 . 60 . Cn , 71 . 10 . Jk I . INTRODUCTORY REMARK The goal of this research is to examine the properties of a simple representation of interacting molecules in contact with particle tanks .This problem arises readily in different mechanical circumstances such as traffic flow 1 , molecular motors 2 or granular materials 3 . Here , we define the so - called two - component symmetric exclusion system ( TCSP ) , i . e . , a system consisting of two species of indistinguishable particles A and B evolving according to the following laws 4 : Particles of type A and B go independently on a ring of L locations by alternating between neighboring sites with levels p and q respectively .If both types of particles attempt to fill the same site jointly then either the A - particle hops forward while the B - particle stays put or vice versa varying on whether p > q or p < p . Note that these mechanisms conserve the number of each types of molecules individually but not their total value N = nA + nB .Therefore , the dynamics of the TCSP is modeled by the master equation",
        "rewrite_text": "We investigate the phase diagram and critical behavior of the two-component symmetric exclusion process (TCSEP) on an infinite one-dimensional lattice, where ions can only jump to nearest neighbor sites if those sites are unoccupied. Our analysis reveals that, under periodic boundary conditions, there is no condensation at finite density. However, we observe that the TCSEP undergoes a first-order phase transition to a condensed state when it is connected to particle reservoirs at both ends. The order parameter exhibits a discontinuous jump across this transition line, which concludes at a tricritical point. Additionally, we precisely calculate the present-current correlation function along the transfer pathway using techniques from the Bethe ansatz. Finally, we discuss how our results may be extended to higher-dimensional systems. \n\nPACS numbers: 05.40.+j, 64.60.Cn, 71.10.Jk \n\nI. INTRODUCTORY REMARK\n\nThe aim of this research is to explore the characteristics of a simplified model representing interacting molecules in contact with particle reservoirs. This scenario is relevant in various mechanical contexts, such as traffic flow, molecular motors, and granular materials. We define the two-component symmetric exclusion process (TCSP), consisting of two species of indistinguishable particles, A and B, that move according to specific rules. Particles A and B traverse a ring of L sites, alternately hopping between neighboring locations with rates p and q, respectively. When both types of particles attempt to occupy the same site, either the A-particle advances while the B-particle remains stationary or vice versa, depending on whether p > q or p < q. Importantly, these dynamics preserve the count of each type of molecule independently, while not conserving their total number, \\( N = n_A + n_B \\). Thus, the dynamics of the TCSP can be described by a master equation.",
        "ori-fast-z-score": -1.76851903423969,
        "water-fast-z-score": 5.747048932153913,
        "rewrite-fast-z-score": 0.0854357657716761
    },
    {
        "original_text": "We have performed molecular dynamics simulations to investigate the dynamic crack propagation in an icosahedral AlPdMn quasicrystal and its periodic approximant, i-AlCuFe. The results show that both materials exhibit similar features for the crack growth process at low temperatures (T = 300 K). However, there are significant differences between them when T is increased up to 600 K. In particular, we find that the quasicrystal shows a higher resistance against crack propagation than the approximant under tensile loading conditions. This behavior can be explained by considering the different atomic structures of these two systems. \n \n We also studied how the temperature affects the mechanical properties of the quasicrystal. Our results indicate that increasing the temperature leads to a decrease in the elastic constants C11 and C44 as well as in the bulk modulus B. Moreover, our calculations reveal that the Young s moduli E decreases with increasing temperature.",
        "watermark_text": "We have done chemical dynamics simulations to examine the dynamic crack propagation in an icosahedral AlPdMn quasicrystal and its periodic approximant , i - AlCuFe . The results show that both materials exhibit similar features for the crack growth process at low temperatures ( T = 300 K ) .However , there are significant variations between them when T is expanded up to 600 K . In particular , we find that the quasicrystal demonstrates a higher resistance against fracture propagation than the approximant under tensile loading circumstances . This phenomenon can be described by analyzing the different atomic relationships of these two systems .We additionally investigated how the temperature changes the mechanical behavior of the quasicrystal . Our results show that raising the temperature leads to a reduction in the elastic constants C11 and C44 as well as in the bulk modulus B .Moreover , our calculations reveal that the Young s moduli E decreases with expanding temperature .",
        "rewrite_text": "We conducted chemical dynamics simulations to study the dynamic crack propagation in an icosahedral AlPdMn quasicrystal and its periodic approximant, i - AlCuFe. The findings indicate that both materials display similar behavior in crack growth at low temperatures (T = 300 K). However, significant differences arise as the temperature increases to 600 K. Notably, the quasicrystal exhibits greater resistance to fracture propagation compared to the approximant when subjected to tensile loading. This difference can be explained by examining the distinct atomic interactions in these two systems. Additionally, we explored the impact of temperature on the mechanical properties of the quasicrystal, revealing that higher temperatures lead to a decrease in the elastic constants C11 and C44, as well as the bulk modulus B. Furthermore, our calculations show that the Young's modulus E decreases with increasing temperature.",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 5.938574464184706,
        "rewrite-fast-z-score": 1.9867985355975657
    },
    {
        "original_text": "We study the phenomenological consequences of supersymmetric models with gauge-mediated breaking, in which the Standard Model is extended by adding new vector-like matter fields and extra dimensions. We show that these models can be constructed such that they are free of any unnatural fine-tuning problems associated with the Higgs mass or flavor-changing neutral currents. In particular we find that:  1) The lightest scalar superpartner (the  Higgs  boson) has a mass at most around 300 GeV.  2) Flavor changing neutral current effects are suppressed to an acceptable level for generic values of parameters.  3) Gauge coupling unification occurs naturally within experimental uncertainties. 4) There exists a large parameter space where all sparticles have masses above 1 TeV while still satisfying constraints on electroweak symmetry breaking. 5) These models provide a natural explanation for why there may not yet exist evidence for supersymmetry at accelerator experiments.",
        "watermark_text": "We research the phenomenological consequences of supersymmetric theories with gauge - mediated breaking , in which the Standard Model is extended by added new vector - like matter fields and extra dimensions . We see that these models can be built such that they are free of any strange fine - tuned flaws associated with the Higgs mass or flavor - changing neutral currents .In particular we find that : 1 ) The lightest scalar superpartner ( the Higgs boson ) has a mass at most approximately 300 GeV . 2 ) Flavor shifting neutral current effects are suppressed to an acceptable level for generic values of values .3 ) Gauge coupling unification happens easily within experimental uncertainties . 4 ) There exists a large parameter room where all sparticles have masses above 1 TeV while already satisfying constraints on electroweak symmetry breaking .5 ) These models represent a natural explanation for why there may not already exist evidence for supersymmetry at accelerator studies .",
        "rewrite_text": "We investigate the phenomenological implications of supersymmetric theories with gauge-mediated breaking, which extend the Standard Model by incorporating new vector-like matter fields and additional dimensions. Our findings indicate that these models can be constructed to avoid any problematic fine-tuning issues related to the Higgs mass or flavor-changing neutral currents. Specifically, we discover that: 1) The lightest scalar superpartner, i.e., the Higgs boson, has a mass that does not exceed approximately 300 GeV. 2) Flavor-changing neutral current effects are sufficiently suppressed for typical parameter values. 3) Gauge coupling unification occurs smoothly within the range of experimental uncertainties. 4) There is ample parameter space in which all sparticles possess masses greater than 1 TeV while still adhering to the constraints of electroweak symmetry breaking. 5) These models offer a natural explanation for the absence of direct evidence for supersymmetry in current accelerator experiments.",
        "ori-fast-z-score": 0.7385489458759964,
        "water-fast-z-score": 5.908391567007971,
        "rewrite-fast-z-score": 0.7276068751089989
    },
    {
        "original_text": "We present numerical simulations to study the formation, evolution, and collapse of quiescent cloud cores induced by dynamic compressions in turbulent molecular clouds. We find that these cloud cores are formed through shock compression at intersections between shocks driven into the clouds by supersonic turbulence. The cloud core masses range from 0.1 M⊙ to 1 M⊙ with typical sizes of about 1000 AU. These cloud cores have low internal velocities (< 2 km s-1) but can be accelerated up to 10 km s-1 during their lifetimes due to gravitational interactions with other dense clumps within the same clouds. Most of them evolve quasi-statically for several free-fall times before collapsing dynamically on time scales ranging from one to ten free-fall times. Our results suggest that such cloud cores may represent an important source of prestellar objects in star-forming regions. Keywords: Turbulence, Star Formation",
        "watermark_text": "We create numerical simulations to study the formation , emergence , and collapse of quiescent cloud cores induced by dynamic compressions in volatile molecular clouds . We see that these cloud cores are created through shock compression at intersections between shocks driven into the clouds by supersonic turbulence .The cloud core masses range from 0 . 1 [UNK] to 1 [UNK] with typical sizes of about 1000 AU . These cloud cores have low internal velocities ( < 2 km s - 1 ) but can be accelerated up to 10 km s - 1 during their lifetimes due to gravitational interactions with other dense clumps within the same clouds .Most of them evolve quasi - statically for multiple free - fall times before crashing dynamically on time ranges ranging from one to ten free - fall times . Our results propose that such cloud cores might represent an important source of prestellar objects in star - creating areas .Keywords: Turbulence, Star Formation",
        "rewrite_text": "We conduct numerical simulations to investigate the formation, emergence, and collapse of quiescent cloud cores caused by dynamic compressions in volatile molecular clouds. Our findings indicate that these cloud cores form through shock compression occurring at the intersections of shocks generated by supersonic turbulence within the clouds. The masses of these cloud cores vary from 0.1 [UNK] to 1 [UNK], with typical sizes around 1000 AU. Although they possess low internal velocities (less than 2 km/s), they can experience accelerations up to 10 km/s over their lifetimes due to gravitational interactions with other dense clumps within the clouds. Most of these cores evolve quasi-statically for several free-fall times before undergoing dynamic collapse, which occurs over timescales ranging from one to ten free-fall times. Our results suggest that these cloud cores could play a significant role in the formation of prestellar objects in star-forming regions. Keywords: Turbulence, Star Formation.",
        "ori-fast-z-score": 0.629940788348712,
        "water-fast-z-score": 3.75,
        "rewrite-fast-z-score": 0.9847319278346618
    },
    {
        "original_text": "The quantum auction is an extension to the classical sealed-bid auction, where bidders can submit bids in superposition and the auctioneer announces the winning bid by measuring the state of the system.  The main advantage over classical auctions is that it allows for more efficient use of resources when there are many items being sold or many potential buyers.   In this work we study the problem of finding optimal strategies for both the seller and buyer in such an auction setting with multiple items and multiple buyers.   We show how to find the optimal strategy for the seller using linear programming techniques and provide bounds on the number of measurements required to determine the winner s identity.   For the case of two buyers competing for three items, our results imply that the seller should always announce one item as the winner at each round until only one buyer remains.   ... Quantum mechanics has been used to develop new types of auctions which allow sellers to make better decisions about what goods they want to sell and buyers to make better decisions about which goods they wish to purchase. These auctions have been shown to be useful even if all parties involved do not trust each other completely.",
        "watermark_text": "The quantum auction is an extension to the classical sealed - bid auction , where bidders can submit entries in superposition and the auctioneer announces the wins bidding by observing the state of the scheme . The main advantage over traditional auctions is that it allows for more efficient application of assets when there are many items being sold or many potential buyers .In this research we study the question of finding ideal options for both the seller and seller in such an auction setting with many items and multiple buyers . We see how to find the ideal scenario for the seller using linear programming tools and include bounds on the quantity of measurements required to obtain the winner s identity .For the case of two buyers competing for three goods , our findings mean that the seller should always announce one element as the winner at each round until only one owner stays . . . . Quantum theory has been used to develop new types of auctions which allow sellers to make better decisions about what goods they wish to sell and buyers to make better decisions about which goods they wish to buy .These auctions have been shown to be valuable even if all parties involved do not trust each other completely .",
        "rewrite_text": "The quantum auction builds upon the classical sealed-bid auction format, enabling bidders to present their offers in a state of superposition, while the auctioneer determines the winning bids by observing the system's condition. One significant advantage of this approach over traditional auctions is its potential for more efficient allocation of resources, particularly when dealing with numerous items and multiple prospective buyers. In this study, we explore optimal strategies for both the seller and the buyers in a multi-item auction context. Utilizing linear programming techniques, we identify the best possible outcomes for the seller and establish limits on the number of measurements needed to reveal the identity of the winner. Specifically, when two buyers compete for three items, our results suggest that the seller should consistently declare one item as the winner in each round until only one owner remains. Quantum theory has facilitated the creation of innovative auction formats that empower sellers to make more informed decisions about their goods and buyers to choose more strategically among the available items. Moreover, these auction types have proven to be effective even in scenarios where mutual trust among participants is lacking.",
        "ori-fast-z-score": 2.1105794120443453,
        "water-fast-z-score": 8.140806303599618,
        "rewrite-fast-z-score": 0.9138115486202573
    },
    {
        "original_text": "We report on the optical investigation of single self-assembled InAs/GaAs quantum dots (QDs) in an external magnetic field applied along their growth direction. The QD emission line splits into two components with opposite circular polarization when the magnetic field is increased to about 1 T, which corresponds to the Zeeman splitting energy of 0.5 meV at 4 K. We observe that this splitting increases linearly as temperature decreases down to 20 mK and then saturates below 10 mK. This behavior can be explained by taking into account both electron-hole exchange interaction and phonon-assisted relaxation processes between different excitonic states within QDs. Our results show that the spin-flip time for electrons confined inside QDs is longer than 100 ns even under high magnetic fields up to 5 T. Quantum dot (QD), also known as semiconductor nanocrystal or artificial atom, has attracted much attention due to its unique physical properties such as size-tunable band gap  1  , strong confinement effect  2  , and large oscillator strength  3  . These features make it possible to use QDs as building blocks for various optoelectronic devices including light-emitting diodes  4  , lasers  5  , solar cells  6  , photodetectors  7  , and so forth  8  .\nIn recent years, there have been many efforts devoted to investigating the spin dynamics of carriers confined in QDs  9  -  11  . It was found that the carrier spins are very stable against decoherence caused by environmental noise  12  -  14  . However, the spin flip times were reported to vary widely depending on experimental conditions  15  -  17  . For example, the spin lifetimes of holes  18  and electrons  19  confined in QDs were measured to be several nanoseconds using pulsed excitation techniques. On the other hand, the spin lifetime of electrons  20  and holes  21  confined in QDs could reach microsecond level if continuous wave laser was used instead.",
        "watermark_text": "We report on the optical study of single self - assembled InAs / GaAs quantum dots ( QDs ) in an external magnetic current applied along their development path . The QD absorption system separates into two parts with opposite spherical polarization when the magnetic field is expanded to about 1 T , which corresponds to the Zeeman splitting energy of 0 . 5 meV at 4 K . We determine that this splitting changes linearly as temperature grows down to 20 mK and then saturates below 10 mK .This phenomenon can be described by take into consideration both electron - hole exchange behavior and phonon - aided vibration mechanisms between various excitonic states within QDs . Our results show that the spin - flip time for electrons trapped inside QDs is longer than 100 ns even under high magnetic waves up to 5 T . Quantum dot ( QD ) , sometimes called as semiconductor nanocrystal or artificial electron , has garnered considerable scrutiny due to its unique physical properties such as size - tunable band gap 1 , large confinement phenomenon 2 , and large oscillator strength 3 .These features make it able to use QDs as building blocks for various optoelectronic applications notably light - emitting diodes 4 , lasers 5 , solar cells 6 , photodetectors 7 , and so forth 8 . In recent seasons , there have been many efforts devoted to investigating the spin behavior of carriers restricted in QDs 9 - 11 .It was shown that the carrier spins are very stable against decoherence caused by environmental noise 12 - 14 . However , the spin flip times were reported to vary widely depending on experimental environments 15 - 17 .For instance , the spin lifetimes of holes 18 and electrons 19 restricted in QDs were calculated to be several nanoseconds using pulsed excitation techniques . On the other hand , the spin lifetime of electrons 20 and holes 21 confined in QDs might reach microsecond level if continuous wave beam was used instead .",
        "rewrite_text": "We present an optical study of individual self-assembled InAs/GaAs quantum dots (QDs) under an external magnetic current aligned with their growth direction. When the magnetic field is increased to approximately 1 T, the quantum dot absorption spectrum splits into two components with opposite circular polarization, corresponding to a Zeeman energy splitting of 0.5 meV at 4 K. We observe that this energy splitting increases linearly with temperature up to 20 mK before saturating below 10 mK. This behavior can be explained by incorporating both the electron-hole exchange interaction and phonon-mediated vibration processes among various excitonic states within the QDs. Our findings indicate that the spin-flip time for electrons confined in QDs exceeds 100 ns, even in the presence of high magnetic fields up to 5 T. Quantum dots, often referred to as semiconductor nanocrystals or artificial atoms, have attracted significant attention due to their distinctive physical characteristics such as size-tunable band gaps, strong confinement effects, and high oscillator strengths. These attributes make QDs suitable for diverse optoelectronic applications, including light-emitting diodes, lasers, solar cells, photodetectors, and more. Recent research has focused on exploring the spin dynamics of carriers confined in QDs. It has been demonstrated that carrier spins exhibit remarkable stability against decoherence from environmental noise. However, reported spin flip times can vary significantly based on experimental conditions. For example, the spin lifetimes of holes and electrons in QDs have been estimated at several nanoseconds when analyzed using pulsed excitation techniques. In contrast, the spin lifetimes of these carriers can reach the microsecond range when continuous wave excitation is employed.",
        "ori-fast-z-score": 0.9330078226479681,
        "water-fast-z-score": 7.209605902279753,
        "rewrite-fast-z-score": -0.5222329678670935
    },
    {
        "original_text": "We study cascades on large blog graphs, where each node is an individual blogger and edges represent links between blogs. We propose a novel cascade model that captures the fact that people are more likely to read posts by their friends than random posts. Our main contributions are: (1) we develop efficient algorithms for computing the cascade size distribution under our model; (2) we show how to use these results to estimate the number of active users at any given time during a cascade; (3) we demonstrate the effectiveness of our approach using data collected from LiveJournal.com. The Web has become one of the most important communication channels today. In particular, social networks such as Facebook or Twitter have attracted millions of users who share information with others through online messages known as tweets or status updates. These messages can be seen by all followers of the user posting them, which may cause further propagation of the message within the network. This phenomenon is called viral marketing  1  , and it has been studied extensively over recent years  2  . However, despite its importance, there still remain many open questions about the dynamics of this process  3  .\nIn this work, we focus on studying cascades on large blogging communities, where each node represents an individual blogger and edges connect pairs of blogs written by the same person  4  . A cascade starts when some blogger writes a post containing a URL pointing to another blog s page. Then, if her readers click on the link, they will visit the other blog and possibly continue reading additional posts. As shown in Figure 1 , the resulting graph contains several connected components representing different topics discussed by the community members.",
        "watermark_text": "We research cascades on huge website graphs , where each node is an individual blogger and edges represent connections between blogs . We suggest a novel cascade model that captures the fact that individuals are more likely to see posts by their colleagues than random posts .Our main contributions are : ( 1 ) we develop fast algorithms for modeling the cascade size distribution under our model ; ( 2 ) we study how to use these results to estimate the total of active participants at any given time during a cascade ; ( 3 ) we prove the ability of our approach using data taken from LiveJournal . com . The Web has become one of the most important communication platforms today .In particular , social sites such as Facebook or Twitter have garnered millions of people who share data with others through online emails known as tweets or status updates . These messages can be saw by all supporters of the user sending them , which would cause further transmission of the message within the channel .This phenomenon is dubbed viral marketing 1 , and it has been studied thoroughly over recent years 2 . However , despite its significance , there still continue several open questions about the dynamics of this process 3 .In this research , we focus on studying cascades on huge blogging environments , where each node symbol an individual blogger and edges connect sets of blogs written by the same people 4 . A cascade starts when some blogger writes a post containing a URL pointing to another blog s page .Then , if her viewers flip on the link , they will access the other blog and maybe continue reading additional posts . As seen in Figure 1 , the resulting graph contains multiple connected elements representing different issues discussed by the community members .",
        "rewrite_text": "We investigate cascades within extensive website graphs, where each node represents an individual blogger and the edges indicate connections between blogs. We propose a novel cascade model that reflects the propensity of individuals to encounter posts by their peers rather than random content. Our primary contributions are: (1) the development of efficient algorithms to model the cascade size distribution under our framework; (2) an analysis of how to leverage these results to estimate the total number of active participants at any given moment during a cascade; and (3) validation of our approach using data from LiveJournal.com. The web has emerged as a critical communication platform, with social media sites like Facebook and Twitter attracting millions of users who share information through online messages known as tweets or status updates. These messages can be viewed by all of a user’s followers, potentially leading to further dissemination of the content within the network. This phenomenon is referred to as viral marketing and has been extensively researched in recent years. Nonetheless, several unanswered questions about the dynamics of this process persist. In this study, we concentrate on examining cascades within large blogging ecosystems, where each node denotes an individual blogger and the edges link collections of blogs authored by the same individuals. A cascade is initiated when a blogger posts a message containing a URL that directs readers to another blog's page. If their followers click on the link, they gain access to the other blog and may continue exploring additional posts. As illustrated in Figure 1, the resulting graph showcases various interconnected elements representing different topics discussed by community members.",
        "ori-fast-z-score": 0.42717882885838043,
        "water-fast-z-score": 8.799883874482637,
        "rewrite-fast-z-score": 2.1213203435596424
    },
    {
        "original_text": "We present an analysis of X-ray data obtained with the Chandra X-Ray Observatory for a sample of galaxies selected to be bright at infrared wavelengths (Ks < 20) and located within the Chandra Deep Field-South survey region. We find that most of these objects are active galactic nuclei, but we also identify several cases where the observed X-ray emission is likely due to star formation activity. The majority of our sources have luminosities below 1043 erg s-1, which suggests they may represent a population of low-luminosity AGNs missed by previous surveys. In addition, we detect two heavily absorbed quasars whose optical counterparts were previously classified as reddened stars or galaxies. These results demonstrate how deep X-ray observations can provide new insights into galaxy evolution studies based on multiwavelength datasets. This work was supported under NASA Contract NAS8-38252. We thank the referee for useful comments. \n \n Keywords: Active Galactic Nuclei",
        "watermark_text": "We present an assessment of X - ray data received with the Chandra X - Ray Observatory for a sample of stars selected to be bright at infrared wavelengths ( Ks < 20 ) and located within the Chandra Deep Field - South search region . We see that most of these objects are active galactic nuclei , but we also identify several examples where the seen X - ray radiation is probably due to star formation activity .The majority of our sources have luminosities below 1043 erg s - 1 , which implies they may indicate a population of low - luminosity AGNs missed by earlier surveys . In addition , we find two heavily emitted quasars whose optical cousins were formerly designated as reddened stars or galaxies .These data demonstrate how deep X - ray observations can provide fresh insights into universe evolution investigations based on multiwavelength datasets . This effort was supported under NASA Contract NAS8 - 38252 .We praise the referee for useful comments . Keywords : Active Galactic Nuclei",
        "rewrite_text": "We present an evaluation of X-ray data acquired using the Chandra X-Ray Observatory for a selection of stars that are bright in infrared wavelengths (Ks < 20) and situated within the Chandra Deep Field-South search area. Our analysis reveals that the majority of these objects are active galactic nuclei (AGNs), while we also discover several cases where the detected X-ray emissions are likely the result of star formation activity. Most of our sources exhibit luminosities below 10^43 erg s^-1, suggesting they may represent a population of low-luminosity AGNs that previous surveys have overlooked. Additionally, we identify two heavily emitting quasars whose optical counterparts were previously misclassified as reddened stars or galaxies. These findings illustrate how deep X-ray observations can enhance our understanding of cosmic evolution when combined with multiwavelength datasets. This research was conducted under NASA Contract NAS8-38252, and we appreciate the valuable feedback from the referee. Keywords: Active Galactic Nuclei.",
        "ori-fast-z-score": 0.24253562503633297,
        "water-fast-z-score": 6.5484618759809905,
        "rewrite-fast-z-score": 1.4770978917519928
    },
    {
        "original_text": "We report on X-ray observations made with ASCA and Chandra of PSR B1257+12, which is in orbit around its companion star. The observed fluxes are consistent with those expected for an isolated neutron star heated by accretion from the stellar wind.  We find that the spectrum can be fit equally well using either blackbody or power-law models; however, we cannot rule out other spectral shapes such as thermal bremsstrahlung. In addition to the point source at the position of the pulsar, there appears to be diffuse emission surrounding it. This may arise from hot plasma trapped between the pulsar s magnetosphere and the stellar surface. If this interpretation is correct then our results suggest that the magnetic field lines connecting the two stars have been disrupted by tidal forces during their close passage through periastron. Finally, we discuss possible origins for the unusually high spin-down rate inferred for PSR B1257 + 12 based on radio timing measurements.",
        "watermark_text": "We report on X - ray observations made with ASCA and Chandra of PSR B1257 + 12 , which is in orbit around its daughter star . The observed fluxes are compatible with those expected for an small neutron galaxy heated by accretion from the stellar wind .We see that the spectrum can be fit similarly well using either blackbody or power - law models ; however , we cannot leave out other spectral patterns such as heat bremsstrahlung . In addition to the point origin at the orientation of the pulsar , there seems to be diffuse emission surrounding it .This might arise from hot plasma trapped between the pulsar s magnetosphere and the stellar surface . If this interpretation is accurate then our findings show that the magnetic force connections connecting the two stars have been disrupted by tidal forces during their close passage through periastron .Finally , we investigate possible origins for the unusually high spinning - down probability inferred for PSR B1257 + 12 based on radio timing measurements .",
        "rewrite_text": "We present X-ray observations of PSR B1257+12, conducted with ASCA and Chandra, which orbits its companion star. The detected fluxes align with what is expected for a small neutron star heated by accretion from the stellar wind. The spectrum can be adequately modeled using either blackbody or power-law fits; however, we cannot dismiss other spectral features, such as thermal bremsstrahlung. Alongside the point source associated with the pulsar, there appears to be diffuse emission surrounding it, possibly originating from hot plasma trapped between the pulsar's magnetosphere and the stellar surface. If this interpretation holds true, our results suggest that tidal forces during the close approach at periastron have disrupted the magnetic connections between the two stars. Lastly, we explore potential explanations for the unusually high spin-down rate inferred for PSR B1257+12 from radio timing observations.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.5778737935111105,
        "rewrite-fast-z-score": -0.3721042037676254
    },
    {
        "original_text": "We present an algorithm for the statistical flow inversion problem that is based on variational Bayesian inference and uses Gaussian processes to model the forward operator. The proposed method allows us to obtain posterior distributions over the unknown source parameters, which can be used as prior information in subsequent inverse problems or uncertainty quantification studies. We demonstrate our approach by applying it to two different test cases with synthetic data generated using the finite element method (FEM). Our results show that we are able to recover the true source distribution accurately even if only few measurements are available. Furthermore, we compare our method against state-of-the-art algorithms and find that our approach outperforms them significantly when the number of measurement locations is small. Finally, we apply our method to real-world data obtained during a field experiment at the University of Houston s Space Research Center. This dataset consists of temperature measurements taken inside a building after a fire was started near one of its walls.",
        "watermark_text": "We introduce an algorithm for the statistical flow inversion problem that is based on variational Bayesian inference and using Gaussian functions to model the forward operator . The proposed approach allows us to obtain posterior distributions over the unseen source parameters , which can be used as prior information in later inverse problems or risk quantification experiments .We suggest our approach by using it to two different test situations with artificial results derived using the finite element method ( FEM ) . Our results show that we are able to extract the true source distribution accurately even if only few measurements are available .Furthermore , we compare our technique against state - of - the - art algorithms and find that our approach outperforms them significantly when the proportion of measurement locations is tiny . Finally , we apply our technique to real - time measurements obtained during a field test at the University of Houston s Space Research Center .This dataset contains of temperature measurements taken inside a building after a fire was opened near one of its walls .",
        "rewrite_text": "We present an algorithm designed for the statistical flow inversion problem, utilizing variational Bayesian inference along with Gaussian functions to model the forward operator. This novel approach enables the acquisition of posterior distributions for the unobserved source parameters, which can subsequently serve as prior information in future inverse problems or risk assessment studies. We demonstrate our method through two distinct test scenarios using synthetic data generated by the finite element method (FEM). Our findings indicate that we can accurately reconstruct the true source distribution, even with a limited number of measurements. Additionally, a comparison with leading algorithms reveals that our technique significantly outperforms them, particularly when the number of measurement locations is minimal. Finally, we apply our method to real-time data collected during a field test at the University of Houston’s Space Research Center, which includes temperature measurements recorded inside a building following the ignition of a fire near one of its walls.",
        "ori-fast-z-score": 1.3251783128981585,
        "water-fast-z-score": 7.134676899329873,
        "rewrite-fast-z-score": 1.7888543819998317
    },
    {
        "original_text": "We consider the scalar field theory on a noncommutative (NC) background with spherical symmetry and derive the NC generalization of the Klein-Gordon equation for such fields. We show that this equation can be written as an infinite set of coupled ordinary differential equations, which are equivalent to each other under certain conditions. The solutions of these equations describe different types of waves propagating along geodesics in the NC geometry. In particular we find exact solutions describing massless particles moving at the speed of light. These results may have important implications for quantum gravity phenomenology. Introduction -The idea that spacetime coordinates do not commute is one of the most intriguing concepts emerging from recent developments in string/M-theory  1  . It has been shown  2  , however, that if one considers only classical physics then it is impossible to construct a consistent model based on this concept because of the presence of ultraviolet divergences. This problem was solved by Snyder  3  who introduced a new type of coordinate transformations leading to a deformed Poincaré algebra  4  .\nIn order to study physical effects associated with the existence of a minimal length scale in nature, several authors considered various models where the commutator between two spatial coordinates  x i , x j   = ıθ ij depends on some parameters θ ij  5  . Such deformations lead to modifications of the standard dispersion relations  6  and also affect the propagation properties of matter fields  7, 8  . For example, it has recently been suggested  9  that the introduction of a minimal length scale into the description of gravitational interactions could resolve the black hole information paradox  10  . Another interesting possibility is related to the fact that the deformation parameter θ ij can be chosen so that its magnitude decreases rapidly when the distance r increases  11  . As a result, the effect of noncommutativity becomes negligible outside a small region around the origin  12  . Thus, it seems reasonable to assume that the noncommutativity of space affects only local phenomena while leaving global ones unchanged  13  .",
        "watermark_text": "We consider the scalar field theory on a noncommutative ( NC ) background with spherical symmetry and derive the NC generalization of the Klein - Gordon equation for such fields . We see that this equation can be written as an endless group of coupled ordinary differential coefficients , which are analogous to each other under certain conditions .The solutions of these equations describe varying kinds of waves propagating along geodesics in the NC geometry . In particular we find detailed solutions expressing massless molecules moving at the speed of light .These data may have important implications for quantum gravitational phenomenology . Introduction - The idea that spacetime directions do not commute is one of the most exciting concepts emerging from recent developments in string / M - theory 1 .It has been shown 2 , however , that if one uses only classical physics then it is unable to build a consistent model based on this concept because of the presence of ultraviolet divergences . This problem was solved by Snyder 3 who proposed a new kind of coordinate transformations giving to a deformed Poincaré algebra 4 .In order to study physical effects involved with the existence of a reduced long scale in nature , various authors considered many theories where the commutator between two spatial coordinates x i , x j = ıθ ij depends on some parameters θ ij 5 . Such deformations result to modifications of the standard dispersion relations 6 and also affect the propagation properties of matter spheres 7 , 8 .For instance , it has recently been proposed 9 that the introduction of a reduced distance scale into the description of gravitational interactions might resolve the dark hole information paradox 10 . Another important suggestion is related to the fact that the deformation vector θ ij can be chosen so that its magnitude decreases quickly when the distance r rises 11 .As a result , the impact of noncommutativity appears negligible outside a small area around the origin 12 . Thus , it appears justified to assume that the noncommutativity of space impacts only local phenomena while leaving international ones untouched 13 .",
        "rewrite_text": "We examine scalar field theory within a noncommutative (NC) framework characterized by spherical symmetry, deriving the NC version of the Klein-Gordon equation for these fields. This equation can be expressed as an infinite series of coupled ordinary differential equations, which are interrelated under certain conditions. The solutions to these equations depict various types of waves traveling along geodesics in the NC geometry. Notably, we identify specific solutions that represent massless particles moving at the speed of light. These findings may have significant implications for quantum gravitational phenomenology. \n\nIntroduction - The notion that spatial dimensions do not commute is one of the most intriguing concepts to emerge from recent advancements in string and M-theory. However, it has been demonstrated that relying solely on classical physics fails to create a coherent model based on this principle due to ultraviolet divergences. Snyder addressed this issue by proposing a new type of coordinate transformation that leads to a modified Poincaré algebra. To investigate the physical effects of a reduced length scale in nature, various researchers have explored models where the commutator between two spatial coordinates, \\( x_i \\) and \\( x_j \\), is given by \\( ı\\theta_{ij} \\), involving certain parameters \\( \\theta_{ij} \\). Such deformations lead to alterations in standard dispersion relations and influence the propagation characteristics of matter fields. For example, recent proposals suggest that incorporating a reduced distance scale in gravitational interactions may provide a resolution to the black hole information paradox. Another key idea is that the deformation vector \\( \\theta_{ij} \\) can be chosen to diminish rapidly as distance \\( r \\) increases. Consequently, the effects of noncommutativity seem negligible outside a small region near the origin. This leads to the reasonable assumption that noncommutative effects primarily influence local phenomena while leaving larger-scale interactions relatively unaffected.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 8.327056459580765,
        "rewrite-fast-z-score": 1.087114613009218
    },
    {
        "original_text": "We report on observations made in March and April 2002 at 345 GHz using the Atacama Submillimiter Telescope Experiment (ASTE). We detected no significant emission associated with the host galaxy of the gamma-ray burst GRB 980425, which was discovered by BeppoSAX on Apr. 25, 1998 . The 3 sigma upper limit to the flux density is < 0.5 Jy beam-1 , corresponding to an integrated line intensity ICO< 1 x 10^9 K km s^-1 pc^2 or Mgas < 2 x 10^8 Msun .\nThe non-detection suggests that either the molecular gas mass is much lower than previously estimated or that it has been destroyed by the intense UV radiation field produced during the burst.  These results are consistent with those obtained recently by other groups who have searched for CO emission from this source. If our result can be confirmed then it will provide important constraints on models of the formation of massive stars.",
        "watermark_text": "We report on observations made in March and April 2002 at 345 GHz use the Atacama Submillimiter Telescope Experiment ( ASTE ) . We observed no major radiation associated with the target galaxy of the gamma - ray burst GRB 980425 , which was discovered by BeppoSAX on Apr .25 , 1998 . The 3 sigma higher limit to the flux concentration is < 0 . 5 Jy beam - 1 , equivalent to an integrated line intensity ICO < 1 x 10 ^ 9 K km s ^ - 1 pc ^ 2 or Mgas < 2 x 10 ^ 8 Msun .The non - discovery suggests that either the molecular gas mass is much lower than previously predicted or that it has been destroyed by the strong UV rays field produced during the explosion . These data are compatible with those acquired previously by other groups who have searched for CO radiation from this source .If our outcome can be verified then it will provide important restrictions on estimates of the formation of large stars .",
        "rewrite_text": "In March and April 2002, we conducted observations at 345 GHz using the Atacama Submillimeter Telescope Experiment (ASTE). Our findings indicated no significant radiation from the target galaxy associated with the gamma-ray burst GRB 980425, which was detected by BeppoSAX on April 25, 1998. We determined a 3 sigma upper limit for the flux density of < 0.5 Jy beam^-1, corresponding to an integrated line intensity of ICO < 1 x 10^9 K km s^-1 pc^2 or a molecular gas mass (Mgas) of < 2 x 10^8 M_sun. This lack of detection implies that the molecular gas mass may be considerably lower than earlier estimates or that it has been depleted due to the intense UV radiation generated by the explosion. Our results align with previous observations by other teams seeking CO radiation from this source. If our findings can be confirmed, they will significantly constrain models of massive star formation.",
        "ori-fast-z-score": -1.9867985355975657,
        "water-fast-z-score": 4.106050306901635,
        "rewrite-fast-z-score": -2.065591117977289
    },
    {
        "original_text": "The MiniBooNE experiment has recently reported the observation of an excess in electron-neutrino-like events at low energies, which could be explained by sterile neutrinos with mass around 1 eV and mixing angle sin2(2θ) ~ 0.1. \n \n In this work we study how these results can be accommodated within the framework of three-flavor leptonic mixing using the latest global fits to experimental data on neutrino oscillation parameters as well as cosmological constraints on the sum of active neutrino masses. We find that the allowed parameter space is strongly constrained if one assumes that the observed excess corresponds to true neutrino oscillations into sterile states rather than being due to background systematics or statistical fluctuations. The best-fit values for the sterile neutrino mass splitting are found to be Δm32 = (0.5 - 2.3) meV and Δm2 = (0.4 - 3.6) meV, while the corresponding ranges for the mixing angles are θ23 = 42° - 50° , θ13 < 5° and θ12 > 40° .",
        "watermark_text": "The MiniBooNE experiment has recently noted the observation of an increase in electron - neutrino - like phenomena at low energies , which could be described by sterile neutrinos with mass around 1 eV and mixing angle sin2 ( 2θ ) ~ 0 . 1 . In this research we study how these results can be accommodated within the framework of three - flavor leptonic mixing using the latest global fits to experimental evidence on neutrino oscillation components as well as cosmological limitations on the sum of active neutrino masses .We see that the allowed parameter space is strongly constrained if one assumes that the reported amount corresponds to genuine neutrino oscillations into sterile states instead than being owing to background systematics or statistical fluctuations . The best - fitting values for the sterile neutrino mass separation are found to be Δm32 = ( 0 . 5 - 2 . 3 ) meV and Δm2 = ( 0 . 4 - 3 . 6 ) meV , while the equivalent ranges for the blending angles are θ23 = 42° - 50° , θ13 < 5° and θ12 > 40° .",
        "rewrite_text": "The MiniBooNE experiment has recently reported an increase in electron-neutrino-like interactions at low energies, which may be attributed to sterile neutrinos with a mass around 1 eV and a mixing angle of sin²(2θ) ~ 0.1. In this study, we explore how these findings fit within the framework of three-flavor leptonic mixing, utilizing the most recent global fits of experimental data on neutrino oscillation parameters as well as cosmological constraints on the total mass of active neutrinos. We find that the permitted parameter space is significantly limited if we assume that the observed results stem from genuine neutrino oscillations into sterile states rather than being due to background systematics or statistical fluctuations. The best-fitting values for the mass differences of the sterile neutrinos are Δm³₂ = (0.5 - 2.3) meV and Δm² = (0.4 - 3.6) meV, while the corresponding ranges for the mixing angles are θ₂₃ = 42° - 50°, θ₁₃ < 5°, and θ₁₂ > 40°.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 4.106050306901635,
        "rewrite-fast-z-score": -0.13245323570650439
    },
    {
        "original_text": "The electronic structure, the equilibrium geometry and the finite-temperature properties are investigated for sodium clusters with 39 to 55 atoms by using density functional theory within generalized gradient approximation. The results show that the ground state is always in C_sv symmetry except for Na_55 which has D_5h symmetry as its lowest energy configuration. For all these clusters, we find that there exists an odd-even effect on their structural parameters such as bond length, bond angle and dihedral angle. We also investigate the vibrational frequencies and infrared intensities of these clusters at room temperature. It turns out that the calculated values agree well with those obtained experimentally. Finally, we study the thermodynamic stability of these clusters. Our calculations indicate that most of them have high melting temperatures but some of them may be unstable against dissociation into smaller fragments. \n \n Keywords: Sodium cluster, Electronic structure, Vibrational frequency, Infrared intensity",
        "watermark_text": "The electronic structure , the equilibrium geometry and the finite - temperature properties are investigated for sodium clusters with 39 to 55 atoms by using density functional theory within generalized gradient approximation . The results show that the ground state is usually in C _ sv symmetry except for Na _ 55 which has D _ 5h symmetry as its lowest energy configuration .For all these complexes , we find that there exists an odd - even effect on their structural values such as bond length , bond angle and dihedral angle . We additionally probe the vibrational speeds and infrared intensities of these complexes at room temperature .It turns out that the derived values comply perfectly with those achieved experimentally . Finally , we study the thermodynamic stability of these complexes .Our calculations suggest that most of them have high cooling conditions but some of them may be unstable against dissociation into tiny pieces . Keywords : Sodium cluster , Electronic structure , Vibrational rate , Infrared intensity",
        "rewrite_text": "The electronic structure, equilibrium geometry, and finite-temperature properties of sodium clusters containing 39 to 55 atoms are examined using density functional theory within the generalized gradient approximation. The findings indicate that the ground state typically exhibits C_s symmetry, with the exception of Na_55, which has a lowest energy configuration characterized by D_5h symmetry. We also observe an odd-even effect on structural parameters such as bond length, bond angles, and dihedral angles across these complexes. Furthermore, we investigate the vibrational frequencies and infrared intensities of these complexes at room temperature, and our calculated values align closely with experimental data. Lastly, we assess the thermodynamic stability of these clusters, revealing that while many favor high cooling conditions, a few may be prone to dissociation into smaller fragments. \nKeywords: Sodium cluster, Electronic structure, Vibrational frequency, Infrared intensity.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.3466401061363023,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "We study the effects of selection biases in cluster samples, as well as covariance between observables, on scaling relations derived from X-ray data using mock galaxy clusters generated with the semi-analytic model GALFORM. We find that both these effects can lead to significant systematic errors when deriving cosmological constraints from observed scaling relations. In particular we show that: (i) The scatter in the M-T relation is significantly reduced by including additional information about the temperature distribution function; this effect is stronger for low mass systems. (ii) The slope of the L-M relation depends strongly on whether or not one includes cooling flows in the analysis. This dependence arises because cool cores are more common at high masses than at lower masses, leading to an apparent steepening of the slope if they are excluded. (iii) The normalization of the Y-Xray luminosity-temperature relation shows strong redshift evolution which cannot be explained solely by self-similar evolution.",
        "watermark_text": "We research the effects of selection biases in cluster specimens , as well as covariance between observables , on scaling relations derived from X - ray data utilizing simulated galaxy galaxies produced with the semi - analytic model GALFORM . We see that both these influences can lead to significant systematic errors when deriving cosmological limitations from observed scaling relations .In particular we prove that : ( i ) The scatter in the M - T relation is significantly reduced by including extra data about the temperature distribution function ; this effect is strengthened for low mass systems . ( ii ) The slope of the L - M relation depends strongly on whether or not one includes heating flows in the evaluation .This dependence occurs because cool cores are more common at high masses than at lower masses , leading to an apparent steepening of the slope if they are ignored . ( iii ) The normalization of the Y - Xray luminosity - temperature relation shows deep redshift development which cannot be described solely by self - similar development .",
        "rewrite_text": "We investigate the impact of selection biases in cluster samples and the covariance among observables on scaling relations derived from X-ray data, using simulated galaxy clusters generated by the semi-analytic model GALFORM. Our findings indicate that both factors can introduce considerable systematic errors when extracting cosmological constraints from observed scaling relations. Specifically, we demonstrate that: (i) incorporating additional data regarding the temperature distribution function significantly reduces the scatter in the M-T relation, with a more pronounced effect observed in low-mass systems; (ii) the slope of the L-M relation is highly sensitive to the inclusion of heating flows during analysis, as cool cores are more prevalent in high-mass clusters compared to their lower-mass counterparts, resulting in an apparent steepening of the slope if these cores are overlooked; and (iii) the normalization of the Y-Xray luminosity-temperature relation exhibits substantial redshift evolution that cannot be accounted for by self-similar evolution alone.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.564604740649092,
        "rewrite-fast-z-score": -0.22941573387056174
    },
    {
        "original_text": "We present new measurements of the mean and scatter in the velocity dispersion-optical richness relation (VRR) for galaxy clusters selected by their red-sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use two different methods to measure cluster velocities, one based on spectroscopic observations of member galaxies and another that uses photometric redshift estimates of all galaxies within a fixed aperture centered on each cluster s brightest cluster galaxy (BGG).\nThe results are consistent with previous studies at low-redshift but show significant differences when compared to recent work done at higher redshifts. The discrepancies between our results and those presented in earlier works may be due to systematic effects associated with the measurement techniques used or could indicate evolution in the VRR over time. In either case, these results highlight the need for further investigation into this relationship as well as other scaling relations involving galaxy clusters. \n \n Keywords: galaxy cluster, optical richness",
        "watermark_text": "We create additional measurements of the mean and scatter in the velocity dispersion - optical richness relation ( VRR ) for galaxy galaxies designated by their red - sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) . We use two different methods to measure cluster velocities , one based on spectroscopic observations of member galaxies and another that using photometric redshift estimates of all galaxies within a fixed aperture concentrated on each cluster s brightest cluster galaxy ( BGG ) .The results are compatible with previous research at low - redshift but demonstrate considerable variations when compared to recent work done at higher redshifts . The discrepancies between our findings and those offered in earlier works might be due to systematic effects involved with the measurement methods used or could indicate evolution in the VRR over time .In either case , these results emphasize the necessity for further investigation into this relationship as well as other scaling connections involving galaxy regions . Keywords : galaxy region , optical richness",
        "rewrite_text": "We enhance the analysis of the mean and scatter in the velocity dispersion-optical richness relation (VRR) for red-sequence galaxies, utilizing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). To measure cluster velocities, we employ two distinct approaches: one that relies on spectroscopic observations of member galaxies and another that utilizes photometric redshift estimates for all galaxies within a designated aperture around each cluster's brightest cluster galaxy (BCG). Our findings align with previous studies at low redshift but exhibit significant variations when contrasted with more recent research conducted at higher redshifts. The differences between our results and earlier findings could stem from systematic biases inherent in the measurement techniques used or might reflect an evolution in the VRR over time. Regardless, these results highlight the need for further exploration of this relationship, as well as other scaling relations within galaxy clusters. Keywords: galaxy clusters, optical richness.",
        "ori-fast-z-score": 1.835325870964494,
        "water-fast-z-score": 7.111887749987414,
        "rewrite-fast-z-score": 1.4552137502179978
    },
    {
        "original_text": "The conformational space available to the side chains in proteins can be explored using molecular dynamics (MD) simulations, but this approach requires extensive computational resources for large systems. We have developed an alternative method that uses normal mode analysis (NMA), which has been shown previously to provide accurate predictions of protein motions at low computational cost.  In our new method we use NMA to calculate the free energy landscape associated with each side chain rotation angle. The resulting landscapes are then used as input into a statistical mechanical model that predicts the equilibrium distribution of rotamers observed experimentally. This method was tested on two small proteins containing multiple proline residues, where it correctly predicted the relative population of different rotameric states. It also accurately reproduced experimental results obtained by nuclear magnetic resonance spectroscopy (NMR). Finally, we applied this method to study the effect of mutations on the rotamer distributions of several amino acids. Our results show how changes in the local environment affect the rotamer preferences of these residues.",
        "watermark_text": "The conformational area available to the side groups in proteins can be investigated using molecular mechanics ( MD ) simulations , but this methodology demands vast computational resources for large systems . We have developed an additional method that using normal mode assessment ( NMA ) , which has been shown previously to provide accurate calculations of gene movements at low numerical cost .In our new method we utilize NMA to estimate the free energy landscape attributed with each side chain rotation angle . The resulting ecosystems are then used as input into a statistical mechanical model that predicts the equilibrium distribution of rotamers observed experimentally .This method was tested on two small proteins containing several proline proteins , where it correctly forecast the relative population of different rotameric states . It also accurately reproduced experimental results derived by nuclear magnetic resonance spectroscopy ( NMR ) .Finally , we applied this technology to study the impact of mutations on the rotamer distributions of several amino acids . Our results show how variations in the local ecosystem affect the rotamer preferences of these acids .",
        "rewrite_text": "The conformational space accessible to side groups in proteins can be explored through molecular dynamics (MD) simulations; however, this approach requires substantial computational resources for larger systems. To address this challenge, we developed a new method that leverages normal mode analysis (NMA). Previous research has demonstrated that NMA can deliver accurate calculations of molecular movements at a lower computational cost. In our approach, we use NMA to estimate the free energy landscape associated with each rotation angle of side chains. The resulting configurations serve as input for a statistical mechanical model that predicts the equilibrium distribution of rotamers, as observed in experimental settings. We tested this method on two small proteins containing several proline residues, and it successfully predicted the relative populations of different rotameric states. Additionally, it accurately reflected experimental findings from nuclear magnetic resonance spectroscopy (NMR). Finally, we utilized this technique to examine the influence of mutations on the rotamer distributions of several amino acids, revealing how changes in the local environment affect the rotamer preferences of these residues.",
        "ori-fast-z-score": 0.41256849850351734,
        "water-fast-z-score": 6.25846794771944,
        "rewrite-fast-z-score": 0.41256849850351734
    },
    {
        "original_text": "We present an approach to the assimilation of magnetic field observations into numerical models using variational methods. The method is applied to a simple model problem in which we consider the evolution of a single component of the magnetic vector potential in a two-dimensional domain with periodic boundary conditions. We use this example as a testbed to explore how different choices of observation operator affect the quality of the resulting analysis fields. In particular, we compare results obtained by applying our algorithm directly on the state variable (the magnetic vector potential) against those obtained when it is first projected onto a set of basis functions that are chosen to be optimal for representing the solution at each time step. Our experiments show that both approaches yield similar results but that the latter can lead to significant computational savings. Finally, we demonstrate the utility of our approach by performing a series of twin experiments in which synthetic magnetometer measurements are used to update the initial condition of the simulation.",
        "watermark_text": "We present an approach to the assimilation of magnetic field observations into numerical models using variational techniques . The method is applied to a simple simulation problem in which we study the evolution of a single part of the magnetic vector potential in a two - dimensional domain with periodic boundary constraints .We use this example as a testbed to examine how various choosing of observation operator impact the performance of the resulting assessment fields . In particular , we compare findings obtained by using our technique directly on the state variable ( the magnetic vector potential ) against those achieved when it is initially projected onto a setting of basis functions that are chosen to be appropriate for describing the solution at each time step .Our experiments show that both approaches yield similar results but that the latter can lead to significant computational savings . Finally , we prove the utility of our approach by performing a sequence of twin tests in which synthetic magnetometer measurements are applied to modify the early condition of the model .",
        "rewrite_text": "We introduce a method for integrating magnetic field observations into numerical models through variational techniques. This approach is tested on a straightforward simulation problem that examines the evolution of a specific segment of the magnetic vector potential within a two-dimensional domain featuring periodic boundary conditions. This example serves as a testbed to investigate how different choices of observation operators affect the performance of the resulting evaluation fields. Specifically, we compare the results obtained by applying our method directly to the state variable (the magnetic vector potential) with those derived from initially projecting the variable onto a set of basis functions deemed suitable for adequately describing the solution at each time step. Our experiments indicate that while both methods yield comparable results, the latter offers significant computational advantages. Lastly, we demonstrate the effectiveness of our approach by conducting a series of twin tests in which synthetic magnetometer measurements are utilized to adjust the initial conditions of the model.",
        "ori-fast-z-score": -0.8528028654224417,
        "water-fast-z-score": 5.330017908890261,
        "rewrite-fast-z-score": -0.21081851067789195
    },
    {
        "original_text": "We present new optical spectroscopic observations and archival X-ray data for the Seyfert 1 galaxy NGC 3783, which show that its nuclear activity is dominated by an obscured AGN with a luminosity of ~10^43 erg/s (2-10 keV). The observed spectral energy distribution suggests that this source has been undergoing rapid changes between two distinct states over the past decade or so; one where it appears to be radiating at close to Eddington limit and another where it is much fainter but still highly luminous. We argue that these variations are likely driven by fluctuations in the mass inflow rate onto the central black hole. This interpretation is supported by our analysis of the time delays between different emission lines as well as their line widths. In particular we find that the Hβ emission line lags behind other lines such as He ii λ4686Å and C iv λ1549Å by upto several hundred days during periods when the source was brightest.",
        "watermark_text": "We present new optical spectroscopic observations and archival X - ray data for the Seyfert 1 galaxy NGC 3783 , which show that its radioactive action is dominated by an obscured AGN with a luminosity of ~ 10 ^ 43 erg / s ( 2 - 10 keV ) . The observed spectral power distribution indicates that this source has been experiencing rapid variations between two separate states over the previous decade or so ; one where it appears to be pulsing at close to Eddington limit and another where it is much fainter but still strongly luminous .We argue that these changes are likely due by fluctuations in the mass inflow rate onto the central black hole . This interpretation is backed by our analysis of the period delays between various emission lines as also as their path widths .In particular we find that the Hβ emission band lags behind other lines such as He ii λ4686Å and C iv λ1549Å by upto many hundred days during periods when the source was brightest .",
        "rewrite_text": "We present new optical spectroscopic observations along with archival X-ray data for the Seyfert 1 galaxy NGC 3783. Our findings indicate that its radiative output is primarily driven by an obscured AGN with a luminosity of approximately \\(10^{43}\\) erg/s in the 2-10 keV range. The spectral power distribution suggests that the source has undergone rapid fluctuations between two distinct states over the past decade; one in which it appears to be pulsing near the Eddington limit, and another where it is considerably dimmer yet still highly luminous. We propose that these variations are likely caused by changes in the mass inflow rate onto the central black hole. This interpretation is supported by our analysis of the time delays between different emission lines as well as their broadening. Notably, we observe that the Hβ emission line lags behind others, such as He ii \\(λ4686\\,Å\\) and C iv \\(λ1549\\,Å\\), by several hundred days during the periods of peak brightness.",
        "ori-fast-z-score": -1.1523319193960637,
        "water-fast-z-score": 4.318004318006477,
        "rewrite-fast-z-score": -0.508000508000762
    },
    {
        "original_text": "We report the observation of measurement-induced dephasing in a Bose-Einstein condensate (BEC) using Ramsey interferometry and atom counting.  The experiment is performed by splitting a single trapped BEC into two spatially separated clouds, which are allowed to evolve for different times before being recombined on a beam splitter. We observe that the visibility of interference fringes decreases as we increase the number of atoms counted at one output port of the beam splitter. This effect can be explained by considering how repeated measurements affect the phase evolution of the system. Our results demonstrate that it is possible to use cold-atom experiments to study fundamental questions about quantum mechanics. Quantum mechanics predicts that any attempt to measure a physical quantity will disturb its value. In this work, we experimentally investigate such effects in a Bose-Einsteint Condensate (BEC). To do so, we perform Ramsey interferometry between two spatially separated regions of our sample. By varying the time spent evolving freely after splitting off part of the initial cloud, we control the relative phase accumulated during free evolution. After recombination, we count the number of atoms arriving at each output port of the beam-splitter and record their arrival-time distribution. As expected, we find that the visibility of the resulting interference pattern decreases when increasing the number of detected particles.",
        "watermark_text": "We report the observation of measurement - caused dephasing in a Bose - Einstein condensate ( BEC ) using Ramsey interferometry and electron searching . The observation is conducted by breaking a single trapped BEC into two spatially separated clouds , which are allowed to evolve for different times before being recombined on a laser splitter .We see that the visibility of interference fringes decreases as we increase the number of atoms counted at one output port of the light splitter . This phenomenon can be described by examining how repetitive measurements alter the phase evolution of the system .Our results show that it is easy to use cold - atom experiments to study essential problems about quantum mechanics . Quantum theory predicts that any attempt to measure a physical quantity will interrupt its value .In this research , we experimentally examine such interactions in a Bose - Einsteint Condensate ( BEC ) . To do so , we perform Ramsey interferometry between two spatially separated regions of our sample .By varying the period spending expanding freely after splitting off part of the first cloud , we control the relative phase accumulated during free evolution . After recombination , we list the quantity of atoms arriving at each output port of the laser - splitter and record their arrival - time distribution .As expected , we find that the visibility of the resulting interference pattern decreases when increasing the proportion of identified particles .",
        "rewrite_text": "We present our findings on measurement-induced dephasing in a Bose-Einstein condensate (BEC) through the use of Ramsey interferometry and electron detection. This experiment involves splitting a single trapped BEC into two spatially distinct clouds that evolve for varying durations before being recombined via a laser splitter. Our observations reveal that the visibility of the interference fringes diminishes as the number of atoms detected at one output port of the splitter increases. This behavior can be understood by analyzing how consecutive measurements impact the phase evolution of the system. Our results demonstrate the feasibility of using cold-atom experiments to investigate fundamental questions in quantum mechanics. According to quantum theory, any attempt to measure a physical quantity inherently disrupts its value. In this study, we explore such interactions within a BEC by performing Ramsey interferometry between two separate regions of our sample. By manipulating the duration of free expansion following the initial cloud's division, we control the accumulated relative phase during this evolution. After recombination, we tally the number of atoms detected at each output port of the laser splitter and record their arrival-time distribution. As anticipated, we observe that the visibility of the resulting interference pattern decreases with an increasing fraction of detected particles.",
        "ori-fast-z-score": 0.6897304947150052,
        "water-fast-z-score": 6.404640308067906,
        "rewrite-fast-z-score": 1.6329931618554523
    },
    {
        "original_text": "We prove that the transitive powers of the so-called ``Young-Jucys-Murphy  (YJM) elements in any finite group G form an abelian subgroup of G, which is central if and only if G has no non-trivial normal subgroups. The YJM elements were introduced by Jucys and Murphy as generators for the center of the symmetric group S_n. We use this result to give a new proof of the fact that the center of the wreath product H ≀ S_n equals the direct sum of the centers of H and S_n. This also yields a simple proof of the fact that every element in the center of a finite group can be written as a linear combination of commutators with coefficients ±1. \nIntroduction\n\nLet G be a finite group. In their seminal work on the representation theory of the symmetric groups  Juc77  , Jucys and Murphy defined certain elements in the center Z(S_n) of the symmetric group S_n$ over the field F_q . These elements have been studied extensively since then; see e.g.   BW03,  BW04,   BZ08,   ?   .\nIn particular, it was shown in  BW03   that these elements generate all of the center of S_n when n > 2 or q = p^r where r > 1.   It follows immediately from this theorem that the center of S_n contains at least one noncentral element whenever n < 4 or q = p^1 .  However, it remained open whether there exist other examples of finite groups whose center does not contain any noncentral elements besides those already known such as cyclic groups, dihedral groups, alternating groups etc. . \nThe main goal of our present article is to answer this question affirmatively by proving the following result:",
        "watermark_text": "We prove that the transitive powers of the so - called ` ` Young - Jucys - Murphy ( YJM ) elements in any finite group G constitute an abelian subgroup of G , which is central if and only if G has no non - trivial normal subgroups . The YJM elements were introduced by Jucys and Murphy as generators for the center of the symmetric group S _ n .We use this result to give a new proof of the fact that the center of the wreath product H [UNK] S _ n represents the direct sum of the centers of H and S _ n . This also yields a simple proof of the fact that every element in the center of a finite group can be written as a linear combination of commutators with coefficients ±1 .Introduction Let G be a finite group . In their seminal study on the representation theory of the symmetric groups Juc77 , Jucys and Murphy defined certain elements in the center Z ( S _ n ) of the symmetric group S _ n $ over the field F _ q .These components have been studied frequently since then ; seeing e . g . BW03 , BW04 , BZ08 , ?. In particular , it was shown in BW03 that these elements generate all of the center of S _ n when n > 2 or p = p ^ q where p > 1 .It follows instantly from this theorem that the center of S _ n contains at least one noncentral element whenever n < 4 or p = p ^ 1 . However , it remained open whether there remain other instances of finite groups whose center does not include any noncentral groups besides those already established such as cyclic groups , dihedral groups , alternating groups etc .. The main goal of our present paragraph is to approach this question affirmatively by proving the following result :",
        "rewrite_text": "We demonstrate that the transitive powers of the so-called \"Young-Jucys-Murphy (YJM) elements\" in any finite group \\( G \\) form an abelian subgroup of \\( G \\), which is central if and only if \\( G \\) has no non-trivial normal subgroups. The YJM elements were originally introduced by Jucys and Murphy as generators for the center of the symmetric group \\( S_n \\). Utilizing this finding, we provide a new proof that the center of the wreath product \\( H \\wr S_n \\) is the direct sum of the centers of \\( H \\) and \\( S_n \\). This also leads to a straightforward proof that any element in the center of a finite group can be expressed as a linear combination of commutators with coefficients of \\( \\pm 1 \\).\n\n**Introduction**: Let \\( G \\) be a finite group. In their foundational work on the representation theory of symmetric groups, Jucys and Murphy defined specific elements within the center \\( Z(S_n) \\) of the symmetric group \\( S_n \\) over the field \\( F_q \\). These elements have been the focus of various studies in subsequent literature, such as in BW03, BW04, and BZ08. Notably, BW03 established that these elements generate the entire center of \\( S_n \\) when \\( n > 2 \\) or when \\( p = p^q \\) with \\( p > 1 \\). This theorem immediately implies that the center of \\( S_n \\) contains at least one non-central element whenever \\( n < 4 \\) or \\( p = p^1 \\). However, it is still an open question whether other finite groups exist whose centers consist solely of non-central groups previously identified, such as cyclic groups, dihedral groups, and alternating groups. The primary aim of this section is to positively address this question by proving the following result:",
        "ori-fast-z-score": 0.8320502943378436,
        "water-fast-z-score": 4.345151537097628,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "We present new near-infrared observations and analysis of the super star cluster (SSC) in the interacting galaxy pair NGC 1705, which is located at a distance of ~10 Mpc. The SSC has an age of ~30 Myr and contains several thousand massive stars with masses >20M☉ . We find that it exhibits many properties similar to those observed for young stellar clusters associated with gamma-ray bursts (GRBs). In particular, we detect a bright Wolf-Rayet population as well as evidence for ongoing mass loss via winds driven by evolved red supergiants. These results suggest that this system may be a local analogue to GRB progenitors. This work was supported by NASA grant NNX11AI18G issued through the Astrophysics Data Analysis Program. The authors wish to recognize and acknowledge the very significant cultural role and reverence that the summit of Mauna Kea has always had within the indigenous Hawaiian community. We are most fortunate to have the opportunity to conduct observations from this mountain.",
        "watermark_text": "We present new near - infrared observations and investigation of the super galaxy cluster ( SSC ) in the interacting galaxy pair NGC 1705 , which is situated at a distance of ~ 10 Mpc . The SSC has an age of ~ 30 Myr and hosts several thousand massive galaxies with masses > 20M☉ .We see that it displays many properties similar to those observed for young stellar clusters identified with gamma - ray clusters ( GRBs ) . In particular , we find a bright Wolf - Rayet population as well as proof for ongoing mass loss via winds driven by evolved red supergiants .These data suggest that this scheme may be a local precursor to GRB progenitors . This research was supported by NASA grant NNX11AI18G submitted through the Astrophysics Data Analysis Program .The authors wish to acknowledge and appreciate the very significant cultural importance and reverence that the summit of Mauna Kea has always had within the native Hawaiian population . We are most lucky to have the ability to conduct measurements from this mountain .",
        "rewrite_text": "We report on new near-infrared observations and analysis of the super galaxy cluster (SSC) associated with the interacting galaxy pair NGC 1705, located approximately 10 Mpc away. The SSC is about 30 million years old and contains several thousand massive galaxies, each with masses exceeding 20 solar masses. Our findings reveal that it shares many characteristics with young stellar clusters linked to gamma-ray bursts (GRBs). Notably, we have identified a prominent Wolf-Rayet population and evidence of ongoing mass loss through winds produced by evolved red supergiants. These observations indicate that this environment may represent a local precursor to GRB progenitors. This research was funded by NASA grant NNX11AI18G via the Astrophysics Data Analysis Program. The authors would like to express their deep respect for the significant cultural importance that the summit of Mauna Kea holds for the native Hawaiian community. We feel incredibly fortunate to have the opportunity to conduct our measurements from this remarkable mountain.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 2.1376670117594454
    },
    {
        "original_text": "We study the critical behaviour of irreversible reaction systems with mass-action kinetics in one dimension, using Monte Carlo simulations and mean-field theory. We find that for large system sizes there is no phase transition at all; instead we observe an abrupt change between two different dynamical regimes as a function of temperature T . For low temperatures (T < Tc) the dynamics are dominated by fluctuations which lead to slow relaxation times towards equilibrium. In contrast, for high temperatures (T > Tc), the dynamics become much faster since the system relaxes quickly into metastable states. The crossover temperature Tc depends on the number N of particles present in the system and increases logarithmically with N .\nThe results presented here can be understood within the framework of the recently developed concept of  active matter . Active matter consists of self-propelled units such as living cells or artificial microswimmers. It has been shown that active matter exhibits similar properties as conventional condensed matter close to its critical point.",
        "watermark_text": "We explore the important dynamics of irreversible synthesis systems with mass - action kinetics in one dimension , using Monte Carlo simulations and mean - field theory . We see that for large network types there is no phase shift at all ; instead we study an sudden difference between two different dynamical regimes as a function of temperature T .For lowest temperatures ( T < Tc ) the dynamics are dominated by fluctuations which cause to slow relax rates towards equilibrium . In comparison , for high temperatures ( T > Tc ) , the dynamics become considerably faster since the system relaxes quickly into metastable levels .The crossover temperature Tc depends on the number N of atoms existing in the system and varies logarithmically with N . The results presented here can be understood within the framework of the recently established concept of active material .Active material consists of self - propelled units such as living cells or artificial microswimmers . It has been shown that active material exhibits similar characteristics as conventional condensed matter close to its critical position .",
        "rewrite_text": "We investigate the critical dynamics of irreversible synthesis systems characterized by mass-action kinetics in one dimension through Monte Carlo simulations and mean-field theory. Our findings reveal that, for larger network configurations, there is no phase transition; rather, we observe a sharp transition between two distinct dynamical regimes as a function of temperature (T). At lower temperatures (T < Tc), the dynamics are heavily influenced by fluctuations, resulting in slow relaxation rates toward equilibrium. In contrast, at higher temperatures (T > Tc), the dynamics accelerate significantly, allowing the system to quickly reach metastable states. The crossover temperature, Tc, is dependent on the number of atoms (N) in the system and varies logarithmically with N. The results can be interpreted within the framework of the recently developed concept of active matter, which encompasses self-propelled units like living cells or artificial microswimmers. It has been demonstrated that active matter exhibits behaviors akin to conventional condensed matter near its critical point.",
        "ori-fast-z-score": -1.5652475842498528,
        "water-fast-z-score": 4.555555555555555,
        "rewrite-fast-z-score": -1.1322770341445956
    },
    {
        "original_text": "The rapid development in the field of intelligent transportation systems (ITS) has led to an increasing demand on wireless communications, which is expected to be fulfilled by using Code Division Multiple Access (CDMA). In this paper we present a novel CDMA-based ITS system that can provide high data rate and low latency services with improved security features compared to existing technologies such as Global Positioning Systems (GPS), Wireless Local Area Networks (WLANs), Bluetooth or ZigBee. The proposed system consists of three main components; namely, roadside units (RSUs), mobile vehicles equipped with on-board units (OBUs) and central servers located at traffic control centers (TCCs).\nWe first introduce the basic concepts behind the proposed system followed by describing its key characteristics. Then, we discuss some important issues related to the design of the proposed system including channel modeling, resource allocation algorithms, power consumption analysis and performance evaluation. Finally, simulation results are presented to demonstrate the effectiveness of our proposed system.",
        "watermark_text": "The rapid progress in the field of intelligent transportation devices ( ITS ) has led to an increasing demand on wireless communications , which is expected to be satisfied by using Code Division Multiple Access ( CDMA ) . In this paper we present a new CDMA - based ITS technology that can provide high data rate and low latency services with improved security features relative to existing technologies such as Global Positioning Systems ( GPS ) , Wireless Local Area Networks ( WLANs ) , Bluetooth or ZigBee .The proposed system contains of three principal portions ; namely , roadside centers ( RSUs ) , wireless trucks equipped with on - board units ( OBUs ) and central computers located at road control centers ( TCCs ) . We first introduce the fundamental concepts behind the suggested system followed by describing its essential aspects .Then , we explain some important problems related to the development of the suggested system namely channel modeling , resource sharing algorithms , power consumption evaluation and performance audit . Finally , simulation data are presented to indicate the performance of our proposed system .",
        "rewrite_text": "The swift advancements in intelligent transportation systems (ITS) have created a growing demand for wireless communication, which is anticipated to be addressed through the implementation of Code Division Multiple Access (CDMA). This paper introduces a novel CDMA-based ITS technology capable of delivering high data rates and low latency services, while also enhancing security compared to existing technologies like Global Positioning Systems (GPS), Wireless Local Area Networks (WLANs), Bluetooth, and ZigBee. The proposed system comprises three main components: roadside units (RSUs), wireless trucks fitted with on-board units (OBUs), and central computers located at traffic control centers (TCCs). We begin by outlining the core principles of the proposed system, followed by a discussion of its key features. Next, we address several critical challenges associated with the system's development, which include channel modeling, resource allocation algorithms, power consumption assessment, and performance evaluation. Lastly, we present simulation data to demonstrate the effectiveness of our proposed system.",
        "ori-fast-z-score": 1.8439088914585775,
        "water-fast-z-score": 7.3326285778338125,
        "rewrite-fast-z-score": 1.6666666666666667
    },
    {
        "original_text": "The aim of this article is to present the basic concepts in Riemannian geometry that are needed for understanding the main results presented here.  The first section introduces some notation and definitions used throughout the text.   In particular we define what it means for two points on an n-dimensional manifold M to be close together (in terms of geodesic distance) or far apart.    We also introduce the concept of a local coordinate system at each point p ∈ M which allows us to describe any other point q near p by giving its coordinates with respect to these local charts.   Finally we give a brief description of how one can construct such a coordinate system locally around a given point using parallel transport along curves starting at p.    The second section describes the notion of a vector field X defined over all of M.   This is done by defining a map F : T M → R where T M denotes the tangent bundle of M.   Then we show that if X satisfies certain conditions then there exists a unique smooth function f : M → R such that X = grad(f).   Here grad(f) denotes the gradient of f.   For example, if M is a surface embedded in R3 then X could represent the velocity of a particle moving across M.   If we assume that the particles move according to Newton s laws of motion then the function f would correspond to the potential energy of the system under consideration.   The third section defines the concept of a tensor field as a generalization of vector fields.   Tensor fields allow us to associate several vectors...",
        "watermark_text": "The goal of this page is to provide the fundamental concepts in Riemannian topology that are needed for studying the main results presented here . The first section introduces some terminology and definitions found throughout the text .In particular we define what it means for two points on an n - dimensional manifold M to be close together ( in terms of geodesic length ) or far separated . We additionally introduce the notion of a local coordinate system at each point p ∈ M which allows us to define any other point q near r by giving its coordinates with regard to these local charts .Finally we give a brief description of how one can build such a coordinate system locally around a given point using parallel transport along curves beginning at p . The second chapter explains the notion of a vector field X defined over all of M . This is accomplished by constructing a mapping F : T M → R where T M denotes the tangent bundle of M . Then we prove that if X satisfies certain conditions then there exists a unique smooth function f : M → R such that X = grad ( f ) . Here grad ( f ) denotes the gradient of f . For instance , if M is a surface embedded in R3 then X could represent the velocity of a particle moving across M . If we suppose that the molecules moving according to Newton s rules of movement then the function r would correspond to the potential electricity of the system under consideration .The third chapter explains the idea of a tensor field as a generalization of vector spaces . Tensor varieties allow us to relate several vectors . . .",
        "rewrite_text": "The purpose of this page is to present the essential concepts of Riemannian topology that are necessary for understanding the key results discussed here. The first section introduces important terminology and definitions utilized throughout the text. Specifically, it clarifies what it means for two points on an n-dimensional manifold \\( M \\) to be considered close (in terms of geodesic length) or distant from each other. We also define a local coordinate system at each point \\( p \\in M \\), which enables us to describe any nearby point \\( q \\) using its coordinates in relation to these local charts. Furthermore, we provide a brief overview of how to construct such a local coordinate system around a point by employing parallel transport along curves originating from \\( p \\). \n\nThe second chapter discusses the concept of a vector field \\( X \\) defined across the entire manifold \\( M \\). This is achieved by creating a mapping \\( F : TM \\to \\mathbb{R} \\), where \\( TM \\) designates the tangent bundle of \\( M \\). We then demonstrate that if \\( X \\) meets specific criteria, a unique smooth function \\( f : M \\to \\mathbb{R} \\) exists such that \\( X = \\text{grad}(f) \\), with \\( \\text{grad}(f) \\) representing the gradient of \\( f \\). For example, in the case of a surface \\( M \\) embedded in \\( \\mathbb{R}^3 \\), \\( X \\) could represent the velocity of a particle moving across the surface. Assuming the molecules follow Newtonian motion, the function \\( r \\) would correspond to the potential energy of the system in question. \n\nThe third chapter introduces the concept of a tensor field, which serves as a generalization of vector spaces, allowing us to relate multiple vectors in a unified framework.",
        "ori-fast-z-score": -0.1781741612749496,
        "water-fast-z-score": 7.365059028153745,
        "rewrite-fast-z-score": 2.057182539299806
    },
    {
        "original_text": "We consider the problem of approximating an unknown function f in L^2(0,1) by a linear combination of orthonormal polynomials with respect to some weight function w on (0,1). We show that if we choose the weights so as to minimize the error between the approximation and the true solution then this leads to a system of equations which can be solved using spectral methods. The resulting algorithm is shown to have optimal convergence rates under certain conditions. In particular it has been proved recently that if the coefficients of the expansion are chosen optimally then the rate of convergence is O(N^{-2}), where N denotes the number of terms used in the expansion. \nThe main purpose of this thesis was to implement these algorithms numerically and compare their performance against other existing techniques such as Chebyshev expansions or Legendre expansions. This work will also allow us to investigate whether there exist any advantages associated with choosing different types of basis functions when solving differential equations.",
        "watermark_text": "We consider the question of approximating an unknown function f in L ^ 2 ( 0 , 1 ) by a linear mixture of orthonormal polynomials with regard to some weight distribution w on ( 0 , 1 ) . We see that if we choose the weights so as to minimize the error between the approximation and the true answer then this results to a system of equations which can be answered using spectral algorithms .The resulting algorithm is demonstrated to have optimal convergence rates under certain conditions . In particular it has been determined recently that if the coefficients of the expansion are chosen optimally then the frequency of convergence is O ( N ^ { - 2 } ) , where N describes the number of words using in the expansion .The main aim of this dissertation was to execute these algorithms numerically and compare their performance against other existing techniques such as Chebyshev expansions or Legendre expansions . This research will also enable us to examine whether there exist any advantages associated with choosing particular kinds of basis variables when solving differential equations .",
        "rewrite_text": "We investigate the problem of approximating an unknown function \\( f \\) in \\( L^2(0,1) \\) using a linear combination of orthonormal polynomials, weighted by a distribution \\( w \\) over the interval \\( (0,1) \\). Our analysis shows that selecting the weights to minimize the approximation error leads to a system of equations that can be solved using spectral algorithms. This newly derived algorithm exhibits optimal convergence rates under specific conditions. Notably, recent findings indicate that if the expansion coefficients are chosen optimally, the convergence rate is \\( O(N^{-2}) \\), where \\( N \\) is the number of terms in the expansion. The primary objective of this dissertation was to implement these algorithms numerically and evaluate their performance in comparison to established methods like Chebyshev and Legendre expansions. Furthermore, this research aims to explore whether certain types of basis functions offer advantages when used to solve differential equations.",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": -1.1952286093343936
    },
    {
        "original_text": "We study the complexity of computing permanent polynomials over graphs with bounded treewidth, focusing on two special cases that are relevant to combinatorial optimization problems: (1) Perfect matchings in bipartite graphs, (2) Matricies whose entries can be expressed as linear combinations of perfect matchings in bipartite subgraphs. We show that both these classes have polynomial-time algorithms for evaluating their respective permanent polynomials when restricted to matrices of bounded treewidth. Our results imply new efficient algorithms for several important combinatorial optimization problems such as maximum weight independent set, minimum vertex cover, and weighted matroid intersection. In addition, we give an algorithm for approximating the permanent of any matrix within a factor of O(n^{1/3}), where n is the number of rows or columns of the input matrix. This improves upon the best known approximation ratio of O(n^{2/3}). Finally, we present some open questions related to our work.",
        "watermark_text": "We research the complexity of computing permanent polynomials over graphs with bounded treewidth , concentrating on two particular instances that are applicable to combinatorial algorithms problems : ( 1 ) Perfect matchings in bipartite graphs , ( 2 ) Matricies whose entries can be described as linear pairs of perfect matchings in bipartite subgraphs . We see that both these classes have polynomial - time algorithms for evaluating their respective permanent polynomials when confined to matrices of bounded treewidth .Our results yield modern effective methods for numerous crucial combinatorial algorithms problems such as maximum weight independent setting , minimum vertex cover , and weighted matroid intersection . In addition , we give an algorithm for approximating the permanent of any matrix within a factor of O ( n ^ { 1 / 3 } ) , where n is the number of columns or rows of the input matrix .This improves upon the best known approximation ratio of O ( n ^ { 2 / 3 } ) . Finally , we present some open questions related to our work .",
        "rewrite_text": "We investigate the complexity of computing permanent polynomials for graphs with bounded treewidth, focusing on two specific scenarios relevant to combinatorial algorithmic problems: (1) Perfect matchings in bipartite graphs and (2) Matrices where their entries correspond to linear combinations of perfect matchings in bipartite subgraphs. We demonstrate that both of these cases have polynomial-time algorithms for evaluating their respective permanent polynomials when restricted to matrices of bounded treewidth. Our findings provide modern and effective approaches for various important combinatorial algorithm problems, including maximum weight independent sets, minimum vertex cover, and weighted matroid intersection. Additionally, we propose an algorithm that approximates the permanent of any matrix within a factor of O(n^{1/3}), where n represents the number of rows or columns of the input matrix. This marks an improvement over the previously established best approximation ratio of O(n^{2/3}). Lastly, we outline several open questions related to our research.",
        "ori-fast-z-score": -0.11867816581938533,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": -0.11547005383792514
    },
    {
        "original_text": "We study the non-equilibrium evolution of an interacting onedimensional Bose gas initially prepared in a coherent state with finite particle number fluctuations and show that it exhibits universal features, which are independent of microscopic details such as interactions or initial conditions. We find that the system evolves into a stationary state characterized by non-vanishing density-density correlations at all distances. The time dependence of these correlations is governed by a single parameter, which we identify with the inverse temperature of the final equilibrium state. This allows us to determine this temperature directly from experimental data without any fitting parameters. Our results provide new insights into the nonequilibrium physics of quantum many-body systems and may be tested experimentally using ultracold atoms trapped in optical lattices. \nI. INTRODUCTORY REMARK\nThe recent development of techniques for trapping and manipulating cold atomic gases has opened up exciting possibilities for studying strongly correlated quantum matter far from thermal equilibrium  1  . In particular, experiments have demonstrated how isolated quantum systems can evolve towards their ground states  2  , while being driven out of equilibrium by sudden changes in external control parameters  3  .\nIn this work, we consider the case where the system is suddenly quenched across a phase transition  4  . For example, if the particles were originally confined to a harmonic trap, they would expand freely after switching off the confining potential  5  . Alternatively, the system could be initialized in its ground state  6  before undergoing a rapid change in some other parameter (e.g., magnetic field)  7, 8  . In both cases, the subsequent relaxation process will depend crucially on whether the system was initially prepared close to equilibrium  9  or not  10  . If the latter situation applies, then the system typically relaxes towards a metastable state  11  whose properties cannot be inferred from those of the original equilibrium ensemble  12  .",
        "watermark_text": "We explore the non - equilibrium evolution of an interacting onedimensional Bose gas initially produced in a coherent state with discrete particle size fluctuations and find that it displays universal features , which are independent of microscopic information such as interactions or initial conditions . We see that the system evolves into a stationary state characterized by non - vanishing density - density correlations at all distances .The period dependence of these correlations is governed by a single parameter , which we identify with the inverse temperature of the finished equilibrium state . This enables us to predict this heat directly from experimental evidence without any fitting factors .Our results bring fresh insights into the nonequilibrium dynamics of quantum several - bodies systems and may be investigated experimentally utilizing ultracold atoms trapped in optical lattices . I .INTRODUCTORY REMARK The rapid progress of techniques for trapping and manipulating cool nuclear gases has opened up interesting possibilities for studying strongly coupled quantum matter far from temperature equilibrium 1 . In particular , researchers have demonstrated how isolated quantum systems can evolve towards their ground positions 2 , while being driven out of equilibrium by sudden variations in external control factors 3 .In this research , we imagine the case where the system is suddenly quenched across a phase transition 4 . For instance , if the molecules were first confined to a harmonic trap , they may expand freely after switching off the confining potential 5 .Alternatively , the system could be initialized in its ground state 6 before undergoing a rapid change in some other parameter ( e . g . , magnetic field ) 7 , 8 . In both cases , the subsequent relaxed process will depend crucially on whether the system was initially developed close to equilibrium 9 or not 10 .If the latter situation applies , then the system typically relaxes towards a metastable state 11 whose characteristics never be inferred from those of the previous equilibrium ensemble 12 .",
        "rewrite_text": "We investigate the non-equilibrium dynamics of an interacting one-dimensional Bose gas that starts in a coherent state with discrete fluctuations in particle size. Our findings reveal universal characteristics that remain unaffected by microscopic details such as interactions or initial conditions. The system evolves into a stationary state defined by persistent density-density correlations at all distances. The periodicity of these correlations is determined by a single parameter that we identify as the inverse temperature of the final equilibrium state. This allows us to directly predict this temperature from experimental data without the need for fitting parameters. Our results provide new insights into the non-equilibrium behavior of quantum many-body systems and can potentially be explored experimentally using ultracold atoms in optical lattices.\n\n**I. INTRODUCTION** The rapid advancements in techniques for trapping and manipulating ultracold atomic gases have opened exciting avenues for investigating strongly coupled quantum matter far from thermal equilibrium. Specifically, researchers have shown that isolated quantum systems can evolve toward their ground states while being perturbed out of equilibrium due to abrupt changes in external control parameters. In our study, we consider scenarios where the system is suddenly quenched across a phase transition. For example, if initially confined within a harmonic trap, the molecules would be allowed to expand freely after the confining potential is removed. Alternatively, the system might start in its ground state before experiencing a swift alteration in another parameter, such as a magnetic field. In both scenarios, the subsequent relaxation process is highly dependent on whether the system began in a state close to equilibrium or not. If the latter is true, the system typically relaxes into a metastable state, whose properties cannot be deduced from those of the prior equilibrium ensemble.",
        "ori-fast-z-score": -0.8111071056538127,
        "water-fast-z-score": 6.651078266361265,
        "rewrite-fast-z-score": 0.6666666666666666
    },
    {
        "original_text": "The development of large space telescopes requires the use of lightweight materials to reduce launch costs and increase telescope performance in orbit. Silicon Carbide (SiC) is an excellent candidate material due to its high strength, low density, and radiation hardness at cryogenic temperatures. However, it has been shown that SiC exhibits significant changes in thermal expansion with temperature which can lead to dimensional instability during cool-downs or warm-ups. This work presents results on the measurement of the coefficient of thermal expansion (CTE) of SiC-100 over a wide range of temperatures using a novel technique based on laser interferometry. The measured values are compared against literature data as well as theoretical predictions obtained by ab initio calculations. It was found that the experimental measurements agree very well with theory within the uncertainty limits. These results will be used to improve the design of future space missions such as JWST and WFIRST-AFTA.",
        "watermark_text": "The construction of large space telescopes demands the using of lightweight materials to reduce rocket expenses and increase telescope performance in orbit . Silicon Carbide ( SiC ) is an excellent candidate material owing to its high strength , low density , and radiation hardness at cryogenic temperatures .However , it has been shown that SiC exhibits significant variations in heat expansion with temperature which can lead to dimensional instability during cool - downs or warm - ups . This research provides findings on the observation of the coefficient of thermal expansion ( CTE ) of SiC - 100 over a broad variety of conditions utilizing a innovative method based on laser interferometry .The measured measures are compared against literature statistics as well as conceptual predictions acquired by ab initio estimates . It was shown that the empirical observations coincide very best with theoretical within the uncertainty constraints .These data will be used to improve the development of potential space missions such as JWST and WFIRST - AFTA .",
        "rewrite_text": "The construction of large space telescopes necessitates the use of lightweight materials to decrease launch costs and enhance telescope performance in orbit. Silicon Carbide (SiC) is a highly suitable candidate due to its exceptional strength, low density, and resistance to radiation at cryogenic temperatures. However, it has been observed that SiC experiences significant variations in thermal expansion with temperature changes, which can result in dimensional instability during heating and cooling cycles. This research presents findings on the coefficient of thermal expansion (CTE) of SiC-100 across a wide range of conditions, utilizing a novel laser interferometry method. The measured values are compared to existing literature and theoretical predictions obtained through ab initio calculations. The empirical data aligns closely with theoretical expectations within the bounds of uncertainty. These findings will contribute to the advancement of future space missions, such as the James Webb Space Telescope (JWST) and Wide-Field Infrared Survey Telescope - Astrophysics Focused Telescope Assets (WFIRST-AFTA).",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 5.737948294722722,
        "rewrite-fast-z-score": 0.9299811099505543
    },
    {
        "original_text": "We study the controllability properties of quantum subsystems that are described by a master equation with Lindblad form. We show how to construct invariant sets for such systems in terms of their generators. These results allow us to prove that certain classes of open quantum systems cannot be controlled using only unitary operations on the system s Hilbert space. Finally we present an algorithm which allows one to determine whether or not a given set is invariant under the dynamics generated by a particular generator. This work was supported by NSF grant PHY-0456628. Quantum mechanics has been successfully applied to many physical phenomena ranging from atomic physics to condensed matter theory. However, it remains unclear what exactly constitutes a quantum mechanical description of reality. One approach towards answering this question involves studying the behavior of closed quantum systems whose states evolve according to Schrödinger equations. Another approach focuses on describing the evolution of open quantum systems where the state of the system interacts continuously with its environment. The latter class of problems can often be formulated as questions about the controllability of quantum dynamical systems. For example, consider the problem of steering the state of a two-level atom between different energy levels using laser pulses.",
        "watermark_text": "We research the controllability properties of quantum subsystems that are explained by a master equation with Lindblad form . We see how to build invariant pairs for such systems in terms of their generators .These results allow us to prove that particular categories of open quantum systems cannot be governed using only unitary operations on the scheme s Hilbert space . Finally we present an algorithm which allows one to find whether or not a given set is invariant under the dynamics generated by a certain generator .This research was supported by NSF grant PHY - 0456628 . Quantum theory has been successfully application to many natural objects including from atomic physics to condensed matter theory .However , it remains unsure what actually constitutes a quantum mechanical explanation of reality . One approach towards answering this question involves studying the dynamics of closed quantum systems whose states evolve according to Schrödinger equations .Another approach focuses on explaining the evolution of open quantum systems where the state of the state interacts continuously with its surroundings . The latter group of problems can often be understood as challenges about the controllability of quantum dynamical systems .For instance , consider the question of steering the state of a two - level atom between various energy levels use laser pulses .",
        "rewrite_text": "We investigate the controllability characteristics of quantum subsystems described by a master equation in Lindblad form. Our approach demonstrates how to construct invariant pairs for these systems based on their generators. These findings enable us to establish that certain categories of open quantum systems cannot be controlled solely through unitary operations on the system's Hilbert space. Additionally, we introduce an algorithm that determines whether a particular set is invariant under the dynamics defined by a specified generator. This research was funded by NSF grant PHY - 0456628. Quantum theory has been effectively applied to a wide range of natural phenomena, spanning from atomic physics to condensed matter theory. Nonetheless, there remains uncertainty about what precisely constitutes a quantum mechanical explanation of reality. One method to address this question is to explore the dynamics of closed quantum systems, where states evolve in accordance with Schrödinger's equation. Another method focuses on the evolution of open quantum systems, which continuously interact with their environment. The latter presents challenges related to the controllability of quantum dynamical systems. For example, we may ask how to manipulate the state of a two-level atom between various energy levels using laser pulses.",
        "ori-fast-z-score": 1.1547005383792515,
        "water-fast-z-score": 6.543303050815759,
        "rewrite-fast-z-score": 0.9805806756909202
    },
    {
        "original_text": "We present the first dual field theory in emergent spacetime, which is derived from a unifying field theory in higher dimensional spacetime. We show that this new dual field theory can be used to describe both quantum and classical physics with one single unified description. This new dual field theory has several advantages over other existing theories such as string/M-theory or loop quantum gravity. First, it provides an explicit mathematical formulation for describing physical phenomena at all scales ranging from microscopic scale down to macroscopic scale. Second, unlike string/M-theory or LQG, our new dual field theory does not require any extra dimensions beyond those already observed experimentally. Third, we provide a concrete example showing how our new dual field theory works by deriving Einstein s general relativity from our new dual field theory. Finally, we also derive Maxwell s equations from our new dual field... \nIntroduction:-In recent years there have been many attempts to develop a fundamental theory of everything(TOE). String/M-theory  1  , Loop Quantum Gravity  2  are two examples of these efforts. However, despite their successes they still suffer from some problems. For instance, string/M-theory requires extra dimensions  3  while loop quantum gravity suffers from non-renormalizability  4  . These difficulties motivate us to look for alternative approaches towards developing TOEs. Recently, a novel approach called  emergent spacetime  was proposed  5, 6  . According to this approach, space-time emerges from a more fundamental level  7, 8  .\nEmergent spacetime:-The idea behind emergent spacetime is very simple. It states that space-time is not fundamental but rather emerges from a more fundamental entity. To see why this might happen consider the following argument. Imagine you are sitting on your couch watching TV. You will probably say that the world around you looks flat because if you were standing up then you would notice that the ground below you is curved. Now imagine yourself floating above Earth. If you were standing up now then you wouldn t feel like you re standing on a curved surface anymore. Instead you d feel like you re standing on top of a",
        "watermark_text": "We introduce the first dual field model in emergent spacetime , which is developed from a unifying field model in larger dimensional spacetime . We see that this new dual field model can be used to explain both quantum and classical physics with one single unified description .This new dual field model has numerous benefits over other existing models such as string / M - theory or loop quantum gravitational . First , it gives an explicit mathematical formulation for describing physical phenomena at all scales ranging from microscopic range down to macroscopic scale .Second , unlike string / M - theory or LQG , our new dual field theory does not require any additional dimensions beyond those already detected experimentally . Third , we provide a clear example showing how our new dual field theory works by deriving Einstein s general relativity from our new dual field theory .Finally , we also generate Maxwell s coefficients from our new dual field . . . Introduction : - In recent history there have been many efforts to develop a basic theory of things ( TOE ) . String / M - theory 1 , Loop Quantum Gravity 2 are two examples of these attempts .However , despite their successes they still suffer from some problems . For instance , string / M - theory requires added dimensions 3 while loop quantum gravitational suffers from non - renormalizability 4 .These difficulties motivate us to search for alternative approaches towards developing TOEs . Recently , a new approach called emergent spacetime was suggested 5 , 6 .According to this methodology , space - time arises from a more fundamental level 7 , 8 . Emergent spacetime : - The idea behind emergent spacetime is very simple .It says that space - time is not essential but rather emerges from a more fundamental entity . To see why this might happen think the following argument .Imagine you are sat on your couch watching TV . You will probably say that the world around you looks flat because if you were standing up then you might see that the earth below you is curved .Now imagine yourself rising above Earth . If you were standing up now then you wouldn t felt like you re standing on a curved surface anymore .Instead you d feel like you re standing on top of a",
        "rewrite_text": "We present the first dual field model within the framework of emergent spacetime, which evolves from a unifying field model situated in higher-dimensional spacetime. This innovative dual field model offers a comprehensive explanation for both quantum and classical physics through a single, unified framework. It boasts several advantages over existing theories such as string/M-theory and loop quantum gravity. Firstly, it provides a precise mathematical formulation that accounts for physical phenomena across all scales, from the microscopic to the macroscopic. Secondly, in contrast to string/M-theory and loop quantum gravity, our dual field theory does not necessitate additional dimensions beyond those that have already been confirmed through experimentation. Thirdly, we illustrate the functionality of our dual field theory by deriving Einstein's general relativity within its framework. Additionally, we are able to derive Maxwell's coefficients from our model as well.\n\nIntroduction: In recent years, a variety of attempts have been made to establish a theory of everything (TOE). Notable examples include string/M-theory and loop quantum gravity. However, despite their achievements, they encounter significant challenges. For example, string/M-theory depends on the existence of extra dimensions, while loop quantum gravity faces issues with non-renormalizability. These limitations inspire the pursuit of alternative methodologies for developing TOEs. A promising new approach, known as emergent spacetime, has recently been proposed. This concept posits that spacetime emerges from a more fundamental level of reality.\n\nEmergent Spacetime: The essence of emergent spacetime is straightforward. It argues that spacetime is not fundamental but rather arises from deeper underlying entities. To illustrate this idea, consider this scenario: when you are sitting on your couch watching TV, you may perceive the surroundings as flat. However, if you were to stand up, you might recognize the Earth's curvature beneath you. Now, envision elevating above the Earth; at that height, you would no longer feel as though you were standing on a curved surface. Instead, you would experience a sense of standing atop a...",
        "ori-fast-z-score": 0.3892494720807615,
        "water-fast-z-score": 6.538461538461538,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The authors present an experimental method for determining the probability that two photons simultaneously hit pixels in a detector, as well as its spatial resolution. The method is based on measuring correlations between pairs of photons emitted by a source with known angular distribution. It can be used to characterize any type of photon-counting detector (CCD cameras, photomultipliers etc.) without requiring knowledge about their internal structure or electronics. This information may then be used to improve the performance of imaging systems such as telescopes. The results are presented for a silicon-strip detector. They show good agreement with Monte Carlo simulations. DOI: 10.1088/1742-6596/aa5e20\nSpatial resolution and coincidence resolving time measurement of Si strip detectors using single-photon counting technique \nI. INTRODUCTIO N\nIn many applications it is important to know how accurately one can determine the position where a photon hits a detector. For example this information is needed when designing optical instruments like telescopes  1  . In order to measure the spatial resolution of a detector we need to have some reference point against which we compare our measured data  2  .\nOne way to obtain this reference point is to use a light source emitting photons at a well-defined angle relative to the normal direction  3  , see Fig.  1(a) . If the detector has no intrinsic spatial resolution, all detected photons will come from a small area around the center of the detector surface. By scanning the detector over different angles θ, we can find out what fraction of the total number of counts comes from each part of the detector  4  . We call these fractions the response function R(θ) of the detector  5  . Knowing the shape of the response function allows us to calculate the spatial resolution of the detector  6  . However, if there is more than one pixel per unit solid angle, the situation becomes complicated because now several pixels could detect a given photon  7, 8  . To solve this problem we introduce here a new concept -the joint probability P ij that i-th and j-th pixels detect a photon simultaneously  9  . Using this concept together with the response function we",
        "watermark_text": "The authors present an research technique for determining the probability that two photons simultaneously struck pixels in a detector , as well as its spatial resolution . The method is based on measuring correlations between pairs of photons generated by a source with known angular distribution .It can be used to characterize any type of photon - tracking detector ( CCD cameras , photomultipliers etc . ) without using knowledge about their internal structure or electronics .This knowledge might then be used to upgrade the performance of optical applications such as telescopes . The results are presented for a silicon - strip detector .They show good agreement with Monte Carlo simulations . DOI : 10 . 1088 / 1742 - 6596 / aa5e20 Spatial resolution and coincidence resolving time measurement of Si strip detectors using single - photon counting technique I . INTRODUCTIO N In many applications it is important to consider how accurately one can determine the position where a photon hits a detector .For instance this data is required when designing optical equipment like telescopes 1 . In order to measure the spatial resolution of a detector we require to have some reference location against which we compare our measured data 2 .One method to obtain this reference location is to use a light source emitting photons at a better - defined angle relative to the normal direction 3 , see Fig . 1 ( a ) .If the sensor has no intrinsic temporal resolution , all detected photons will coming from a small area around the center of the sensor surface . By scanning the sensor over different angles θ , we can find out what fraction of the total number of counts coming from each portion of the detector 4 .We call these fractions the response function R ( θ ) of the sensor 5 . Understanding the shape of the response function allows us to estimate the spatial resolution of the sensor 6 .However , if there is more than one pixel per unit solid angle , the situation grows difficult because now multiple pixels might detect a given photon 7 , 8 . To solve this situation we approach here a new notion - the joint probability P ij that i - th and j - th pixels detect a photon simultaneously 9 .Using this concept together with the response function we",
        "rewrite_text": "The authors introduce a research technique for assessing the probability that two photons hit detector pixels at the same time, as well as evaluating the detector's spatial resolution. This method relies on measuring correlations between photon pairs produced by a source with a known angular distribution. It can characterize various types of photon-tracking detectors, such as CCD cameras and photomultipliers, without needing detailed knowledge of their internal structure or electronic components. Such insights could enhance the performance of optical devices like telescopes. Results for a silicon-strip detector demonstrate good alignment with Monte Carlo simulations (DOI: 10.1088/1742-6596/aa5e20). \n\nIn many applications, accurately determining where a photon impacts a detector is crucial. For example, this information is essential for designing optical instruments such as telescopes. To evaluate a detector's spatial resolution, we need a reference point for comparison with our measurements. One way to establish this reference is by utilizing a light source that emits photons at a precisely defined angle concerning the normal direction. If the sensor lacks intrinsic temporal resolution, all detected photons will originate from a small area near the center of the sensor's surface. By scanning the sensor through various angles, we can determine the fraction of total counts originating from different sections of the detector, referred to as the sensor's response function R(θ). Understanding the shape of this response function enables us to assess the sensor's spatial resolution. However, if there is more than one pixel per unit solid angle, the complexity increases, as multiple pixels may register the same photon. To address this challenge, we introduce the joint probability P_ij, which represents the likelihood that the i-th and j-th pixels detect a photon simultaneously. By combining this concept with the response function, we can further analyze the detection capabilities of the sensor.",
        "ori-fast-z-score": 2.2998495985826177,
        "water-fast-z-score": 8.380035799446205,
        "rewrite-fast-z-score": 1.4896905994842622
    },
    {
        "original_text": "We have analyzed the shapes of interstellar extinction curves in the infrared through ultraviolet wavelength range using data for more than 100 sight lines with known distances and reddenings, including those obtained by us at Kitt Peak National Observatory (KPNO) and Cerro Tololo Inter-American Observatory (CTIO). We find that all observed extinction curves can be fitted well by a single power law function A(lambda) = lambda -alpha , where alpha is an index ranging between 1.5 to 2.0. This result suggests that there are no significant differences among various types of interstellar dust grains as far as their optical properties are concerned. In addition, we show that the value of alpha correlates strongly with the total-to-selective extinction ratio Rv . These results suggest that the shape of interstellar extinction curve may provide important information on the physical conditions of interstellar matter along individual sight lines. Keywords: Interstellar extinction; Power-law index; Reddening; Dust grain",
        "watermark_text": "We have analyzed the shapes of interstellar extinction lines in the infrared through ultraviolet frequency range utilizing information for more than 100 sight lines with reported distances and reddenings , notably those acquired by us at Kitt Peak National Observatory ( KPNO ) and Cerro Tololo Inter - American Observatory ( CTIO ) . We see that all observed extinction lines can be fit well by a single power law relation A ( lambda ) = lambda - alpha , where alpha is an index ranging between 1 . 5 to 2 . 0 .This result suggests that there are no major changes among various types of interstellar dust grains as long as their optical properties are concerned . In addition , we find that the value of α correlates heavily with the total - to - selective extinction ratio Rv .These data suggest that the form of interstellar extinction curve might give important information on the physical conditions of interstellar matter along individual sight lines . Keywords : Interstellar extinction ; Power - law index ; Reddening ; Dust grain",
        "rewrite_text": "We have examined the characteristics of interstellar extinction lines across the infrared to ultraviolet frequency spectrum, utilizing data from over 100 sight lines with documented distances and reddenings, particularly those collected at Kitt Peak National Observatory (KPNO) and Cerro Tololo Inter-American Observatory (CTIO). Our analysis reveals that all observed extinction lines can be accurately described by a single power law relationship, A(λ) = λ^(-α), where the index α ranges between 1.5 and 2.0. This finding indicates that there are no significant variations in the optical properties of different types of interstellar dust grains. Furthermore, we observe a strong correlation between the value of α and the total-to-selective extinction ratio Rv. These results imply that the shape of the interstellar extinction curve may provide crucial insights into the physical conditions of interstellar matter along specific sight lines. \n\nKeywords: Interstellar extinction; Power-law index; Reddening; Dust grain.",
        "ori-fast-z-score": 1.0834726777719228,
        "water-fast-z-score": 5.820855000871991,
        "rewrite-fast-z-score": 1.6733200530681511
    },
    {
        "original_text": "The book is intended for graduate students in physics and mathematics, as well as researchers working on the foundations of quantum mechanics.  The first part deals with conservation laws (energy-momentum) and generation of physical fields by sources. In particular, it contains an introduction to gauge invariance and its role in modern physics. The second part presents basic principles of field theory. It includes chapters devoted to Lagrangian formalism, quantization rules, Feynman diagrams, renormalization group methods etc. This textbook covers all topics that are usually included into courses on Quantum Field Theory at universities. The text is written in a clear style without using too many mathematical formulas. All necessary definitions and theorems are presented in detail. A large number of examples illustrate various concepts discussed throughout the book. An extensive set of exercises helps readers to understand main ideas behind each chapter. The solutions to most problems can be found in the end of the book.",
        "watermark_text": "The text is intended for graduate studying in science and mathematics , as well as scientists studying on the foundations of quantum mechanics . The first part deals with conservation laws ( power - momentum ) and generation of physical fields by sources .In particular , it contains an outline to gauge invariance and its role in modern physics . The second part provides fundamental principles of field principle .It includes sections devoted to Lagrangian formalism , quantization laws , Feynman diagrams , renormalization group models etc . This book covers all issues that are typically included into courses on Quantum Field Theory at institutions .The text is authored in a clear manner without using too big mathematical formulas . All necessary statements and theorems are presented in detail .A wide score of instances highlight numerous concepts discussed throughout the book . An elaborate series of tests allows users to comprehend main ideas behind each volume .The answer to most problems can be found in the end of the book .",
        "rewrite_text": "This text is designed for graduate students in science and mathematics, as well as researchers exploring the foundations of quantum mechanics. The first section addresses conservation laws (such as power and momentum) and the generation of physical fields by sources, with a particular focus on gauge invariance and its significance in modern physics. The second section outlines essential principles of field theory, including topics such as Lagrangian formalism, quantization, Feynman diagrams, and renormalization group models. This book comprehensively covers all the topics typically found in Quantum Field Theory courses at academic institutions. It is written clearly, avoiding overly complex mathematical formulas, and presents all necessary statements and theorems in detail. Numerous examples illustrate the concepts discussed throughout the text, while a thorough set of exercises helps readers grasp the fundamental ideas of each chapter. Solutions to most problems are provided at the end of the book.",
        "ori-fast-z-score": -0.4364357804719848,
        "water-fast-z-score": 6.764754597315764,
        "rewrite-fast-z-score": 0.7977240352174656
    },
    {
        "original_text": "We present new observations of the central region of the nearby Seyfert galaxy NGC 4258, which show that its nuclear disk is warped by an angle of ~20 degrees with respect to the plane of the host galaxy s stellar bulge (see Figure 1 ). The warp has been detected using near-infrared integral field spectroscopy obtained at Gemini Observatory on Mauna Kea, Hawaii. \n \n We also report the detection of significant rotation about the minor axis of this warped structure, as well as evidence for counter-rotation within the innermost few hundred parsecs of the nucleus. These results are consistent with previous studies based on optical data alone. \n \n In addition, we find that the kinematics of the gas in the outer regions of the nuclear disk can be explained if it orbits around the supermassive black hole located at the center of the galaxy under the influence of both gravitational forces and magnetic fields. This result suggests that the observed warps may have their origin in the magneto-rotational instability (MRI) operating in accretion disks surrounding massive black holes. \n \n Finally, we discuss how these findings could help us understand the physics behind the so-called  Bardeen-Petterson effect : i.e., the alignment between the spin axes of the stars and the angular momentum vector of the accreting material onto the central supermassive black hole.",
        "watermark_text": "We present new images of the central region of the nearby Seyfert galaxy NGC 4258 , which show that its nuclear core is warped by an angle of ~ 20 degrees with regard to the plane of the host universe s stellar bulge ( see Figure 1 ) . The warp has been detected using near - infrared inverse field spectroscopy acquired at Gemini Observatory on Mauna Kea , Hawaii .We additionally report the observation of large rotation about the minor axis of this warped structure , as also as data for counter - movement within the innermost few hundred parsecs of the nucleus . These conclusions are compatible with previous research based on optical data alone .In addition , we find that the kinematics of the gas in the exterior areas of the atomic disk can be understood if it orbits around the supermassive black hole located at the center of the galaxy under the effects of both gravity forces and magnetic fields . This result suggests that the seen warps may have their source in the magneto - rotational instability ( MRI ) working in accretion disks surrounding massive blue holes .Finally , we talk how these results could assist us explain the physics behind the so - called Bardeen - Petterson effect : i . e . , the alignment between the spin axes of the stars and the angular velocity tensor of the accreting matter onto the main supermassive black hole .",
        "rewrite_text": "We present new images of the central region of the nearby Seyfert galaxy NGC 4258, revealing that its nuclear core is warped by approximately 20 degrees relative to the plane of the host galaxy's stellar bulge (see Figure 1). This warp was detected through near-infrared inverse field spectroscopy conducted at the Gemini Observatory on Mauna Kea, Hawaii. Additionally, we report observations of significant rotation around the minor axis of this warped structure, along with evidence of counter-movement within the innermost few hundred parsecs of the nucleus. These findings align with previous studies based solely on optical data. Furthermore, we discover that the kinematics of the gas in the outer regions of the atomic disk can be explained by its orbit around the supermassive black hole at the galaxy's center, influenced by both gravitational forces and magnetic fields. This indicates that the observed warps may originate from magneto-rotational instability (MRI) occurring in the accretion disks surrounding massive black holes. Lastly, we discuss how these findings could help elucidate the physics behind the Bardeen-Petterson effect, specifically the alignment between the spin axes of stars and the angular velocity tensor of the accreting matter onto the central supermassive black hole.",
        "ori-fast-z-score": -2.4494897427831783,
        "water-fast-z-score": 4.4907311951024935,
        "rewrite-fast-z-score": -0.7504787743864564
    },
    {
        "original_text": "We present new near-infrared (NIR) spectroscopy and photometry for HD 97048, an evolved star with a dusty circumstellar environment that is surrounded by a large debris disk. The NIR spectrum shows strong emission lines in H I Paschen series as well as Brackett γ line at 2.166 µm. We also detect CO bandheads around 2.3 µm which are characteristic features of late-type stars. In addition, we find evidence of water vapor absorption bands near 1.4-1.8 µm indicating the presence of warm water vapor in the inner part of the system. \n \n Using our newly obtained data together with archival optical spectra, we have derived physical parameters such as effective temperature T eff = 8200 K, surface gravity log g = 3.9 dex, luminosity L = 4 × 10^6 Lsun, mass M = 5M⊙, radius R = 6R⊙, and age t = 7×10^7 years. These values indicate that this object belongs to the red giant branch phase on its way towards becoming a white dwarf.",
        "watermark_text": "We present new near - infrared ( NIR ) spectroscopy and photometry for HD 97048 , an evolved star with a dusty circumstellar climate that is surrounded by a large debris ring . The NIR spectrum displays strong absorption patterns in H I Paschen series as well as Brackett γ line at 2 . 166 µm .We additionally observe CO bandheads around 2 . 3 µm which are peculiar characteristics of late - class stars . In addition , we find proof of water vapor absorption groups near 1 . 4 - 1 . 8 µm indicating the presence of warm water vapor in the inner part of the system .Using our newly derived measurements coupled with archival optical spectra , we have derived mechanical parameters such as effective heat T eff = 8200 K , surface gravity log f = 3 . 9 dex , luminosity L = 4 × 10 ^ 6 Lsun , mass M = [UNK] , diameter R = [UNK] , and age t = 7×10 ^ 7 years . These values indicate that this body belongs to the red dwarf branch stage on its route towards becoming a white dwarf .",
        "rewrite_text": "We provide new near-infrared (NIR) spectroscopy and photometry for HD 97048, an evolved star surrounded by a dusty circumstellar environment and a large debris ring. The NIR spectrum exhibits prominent absorption features in the H I Paschen series as well as a Brackett γ line at 2.166 µm. We also detect CO bandheads around 2.3 µm, which are characteristic of later-type stars. Additionally, we find evidence of water vapor absorption bands between 1.4 and 1.8 µm, suggesting the presence of warm water vapor in the inner region of the system. By combining our new measurements with archival optical spectra, we have determined key parameters: an effective temperature (T_eff) of 8200 K, surface gravity (log g) of 3.9 dex, luminosity (L) of 4 × 10^6 L_sun, mass (M) of [UNK], radius (R) of [UNK], and an age (t) of 7 × 10^7 years. These values indicate that this star is currently in the red dwarf branch stage, on its path toward becoming a white dwarf.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 3.4874291623145783,
        "rewrite-fast-z-score": -0.9299811099505543
    },
    {
        "original_text": "The standard model is the most successful theory in particle physics, but it fails to explain gravity. In this article we present an alternative approach that unifies general relativity with quantum mechanics by using a new concept called  quantum potential energy density  (QPD). We show how QPD can be used as a source for gravitational field equations which are derived from Hamilton s principle of least action. The resulting field equations have solutions similar to those obtained from Einstein s field equations. However, unlike Einstein s field equations, our proposed field equations do not contain any free parameters such as cosmological constant or dark matter. This means that all physical phenomena predicted by these two theories should agree exactly if they are based on the same underlying principles. Finally, we discuss some possible experimental tests of our proposal. The standard model is the most succesful theory in particle physics, however it fails to explain gravity. \n \n In this article we present another approach that unifies general relavity with quantum mechanics by introducing a new concept called “quantum potental energy density”(QPD). \n \n Quantum potential energy density has been introduced previously by several authors  1-5 , but its role was limited only to explaining certain aspects of quantum mechanics like uncertainty relations  6 , tunneling effect  7-9 , wave-particle duality  10-12  etc.. \n \n Here we propose a novel interpretation of QPD where it plays a central role in deriving gravitational field equations. These field equations are then derived from Hamilton’s principle of least action. \n \n Our results suggest that QPD may play a fundamental role in understanding both gravity and quantum mechanics at their deepest level.",
        "watermark_text": "The classic description is the most popular concept in particle theory , but it fails to explain gravity . In this page we present an additional method that unifies general relativity with classical mechanics by using a new definition called quantum potential energy density ( QPD ) .We see how QPD can be used as a source for gravitational field equations which are derived from Hamilton s principle of least action . The resulting field equations have solutions comparable to those generated from Einstein s field equations .However , unlike Einstein s field equations , our proposed field equations do not include any free parameters such as cosmological factor or black material . This implies that all physical phenomena predicted by these two explanations should agree precisely if they are based on the same underlying principles .Finally , we talk some possible experimental tests of our proposal . The conventional hypothesis is the most succesful model in particle theory , however it fails to explain gravity .In this article we present another methodology that unifies general relavity with quantum mechanics by offering a new notion called “ quantum potental energy density ” ( QPD ) . Quantum potential electricity density has been proposed earlier by various literature 1 - 5 , but its significance was confined only to describing particular aspects of quantum mechanics like uncertainty relations 6 , tunneling effect 7 - 9 , wave - particle duality 10 - 12 etc . .Here we develop a new definition of QPD where it takes a central role in deriving gravitational field equations . These field equations are then constructed from Hamilton ’ s principle of least action .Our results propose that QPD would play a basic part in understanding both gravity and quantum mechanics at their deepest level .",
        "rewrite_text": "The traditional model is the most widely recognized concept in particle theory, but it does not adequately account for gravity. In this article, we introduce an alternative approach that successfully integrates general relativity with classical mechanics through a novel definition known as quantum potential energy density (QPD). We demonstrate how QPD can serve as a foundation for gravitational field equations derived from Hamilton's principle of least action. The field equations we present yield solutions that are comparable to those generated by Einstein's field equations. However, unlike Einstein's equations, our formulation does not include any free parameters, such as a cosmological constant or dark matter. This suggests that any physical phenomena predicted by both approaches should be consistent if they are based on the same fundamental principles. Additionally, we discuss potential experimental tests to validate our proposal. While the conventional hypothesis has been highly successful in particle theory, it still falls short in explaining gravity. Here, we offer a new perspective on QPD, which has been referenced in previous literature but primarily addressed specific aspects of quantum mechanics, such as uncertainty relations, tunneling effects, and wave-particle duality. We propose a fresh definition of QPD that positions it as a crucial element in deriving gravitational field equations, ultimately positing that QPD is fundamental to comprehending both gravity and quantum mechanics at their core.",
        "ori-fast-z-score": 2.2132669799727465,
        "water-fast-z-score": 8.842595171198449,
        "rewrite-fast-z-score": 1.7162326606420661
    },
    {
        "original_text": "We study the possibility that electroweak symmetry breaking is triggered by strong dynamics at an ultraviolet (UV) fixed point, as suggested by gauge-Higgs unification models with warped extra dimensions. We show how this scenario can be realized within the framework of composite Higgs models based on strongly-coupled gauge theories. In particular we consider two different realizations of such scenarios: one where the Higgs arises as a pseudo-Nambu-Goldstone boson associated to spontaneous breaking of approximate global symmetries; another where it emerges as a bound state of new fermions charged under the Standard Model gauge group. The latter case leads to novel signatures for Higgs production through gluon fusion which are potentially observable at future colliders. Finally, we discuss possible implications of these results for cosmology. Gauge-Higgs unification provides a compelling explanation for why the weak scale is so much smaller than any other mass scale in nature  1  . It also offers a natural solution to the hierarchy problem between the Planck and TeV scales  2  , since quantum corrections to the Higgs potential are cut off at the UV scale  3  .\nIn order to realize this idea in practice, however, several challenges must be overcome  4  : i) the Higgs should arise naturally out of some strongly coupled sector; ii) the Higgs couplings to SM particles should agree with experiment; iii) there should exist a mechanism to generate masses for all SM fields without introducing large hierarchies among them. These issues have been addressed recently using the Randall-Sundrum model  5  , where the Higgs field lives on the IR brane while gravity propagates into the bulk  6  -  8  . This setup allows for a calculable description of the Higgs physics  9  , but introduces additional complications due to the presence of Kaluza-Klein gravitons  10  .",
        "watermark_text": "We research the idea that electroweak symmetry breaking is caused by intense dynamics at an ultraviolet ( UV ) fixed point , as suggested by gauge - Higgs unification theory with warped extra dimensions . We see how this situation can be realized within the framework of composite Higgs theories based on highly - coupled gauge physics .In particular we suppose two different realizations of such scenarios : one where the Higgs arises as a quasi - Nambu - Goldstone boson associated to spontaneous breaking of approximate global symmetries ; another where it appears as a bound state of new fermions charged under the Standard Model gauge group . The latter situation leads to novel signatures for Higgs production through gluon fusion which are possibly observable at current colliders .Finally , we explain possible possibilities of these results for cosmology . Gauge - Higgs unification presents a powerful explanation for why the weak scale is so greatly smaller than any other mass scale in nature 1 .It additionally offers a natural solution to the ranking problem between the Planck and TeV scales 2 , since quantum corrections to the Higgs potential are cut off at the UV scale 3 . In order to realize this idea in practice , however , various challenges must be overcome 4 : i ) the Higgs should occur readily out of some strongly coupled sector ; ii ) the Higgs couplings to SM objects should comply with observation ; iii ) there should exist a process to produce masses for all SM fields without eliminating huge hierarchies among them .These issues have been addressed recently utilizing the Randall - Sundrum model 5 , where the Higgs field lives on the IR brane while gravity propagates into the bulk 6 - 8 . This configuration allows for a calculable description of the Higgs physics 9 , but introduces additional difficulties owing to the presence of Kaluza - Klein gravitons 10 .",
        "rewrite_text": "We investigate the hypothesis that electroweak symmetry breaking results from strong dynamics at a specific ultraviolet (UV) fixed point, as proposed by gauge-Higgs unification theories involving warped extra dimensions. We explore how this scenario can be implemented within the framework of composite Higgs models derived from highly-coupled gauge interactions. In particular, we consider two distinct realizations of such frameworks: one in which the Higgs emerges as a quasi-Nambu-Goldstone boson linked to the spontaneous breaking of approximate global symmetries, and another where it represents a bound state of new fermions that interact with the Standard Model gauge group. The latter case could yield unique signatures for Higgs production via gluon fusion, which may be detectable at present collider experiments. Additionally, we discuss the implications of these findings for cosmology. Gauge-Higgs unification offers a compelling explanation for the significant disparity between the weak scale and other mass scales in nature. It also provides a natural resolution to the hierarchy problem between the Planck scale and the TeV scale, as quantum corrections to the Higgs potential are limited by the UV scale. However, realizing this concept in practice involves overcoming several challenges: i) the Higgs must emerge naturally from a strongly coupled sector; ii) its couplings to Standard Model particles must match experimental observations; iii) there must be a mechanism to generate masses for all Standard Model fields without creating large hierarchies among them. Recent work has addressed these challenges using the Randall-Sundrum model, in which the Higgs field is localized on the infrared (IR) brane while gravity propagates in the bulk. This setup provides a calculable framework for Higgs physics but introduces complexities due to the presence of Kaluza-Klein gravitons.",
        "ori-fast-z-score": -0.2727272727272727,
        "water-fast-z-score": 6.6996452071463715,
        "rewrite-fast-z-score": -0.6311687442672026
    },
    {
        "original_text": "We consider the problem of quantizing an arbitrary vector in a Grassmannian manifold to minimize its distortion under a given rate constraint, which is relevant for multi-input-multi-output (MIMO) communications systems employing feedback. We derive upper bounds on the minimum achievable distortion as well as lower bounds on the corresponding optimal rates by using information-theoretic tools such as entropy power inequalities and data processing inequalities. Our results show that the performance gap between these two bounds increases when the dimension of the underlying Grassmannian manifold grows large. Finally, we provide numerical examples illustrating our theoretical findings. The work presented here was supported by NSF Grant CCF-0635035. Multi-input-multi-output communication systems are widely used in wireless networks due to their high spectral efficiency  1  . In this context, it has been shown recently  2  , that the use of limited-rate feedback can significantly improve system performance at low signal-to-noise ratios (SNRs). However, the amount of available feedback resources may be severely constrained in practice  3  .\nIn order to reduce the required feedback overhead while maintaining good performance, one approach consists of exploiting channel state information (CSI), i.e., knowledge about the current fading coefficients, to perform joint encoding across multiple transmit antennas  4  -  6  . This technique, known as spatial multiplexing or beamforming, requires CSI at both transmitter and receiver sides. Since acquiring perfect CSI at the transmitter side through training-based schemes typically involves significant signaling overhead  7  , practical implementations often resort to quantized versions of the true CSI  8 -  10  . Therefore, there exists a trade-off between the accuracy of the transmitted signals and the amount of feedback needed to convey them  11  .\nThe design of efficient transmission strategies over MIMO channels with limited feedback remains an open research area  12  . A number of recent works have focused on characterizing fundamental limits associated with different aspects of MIMO systems operating under various assumptions regarding the availability of CSI  13  -  16  . For example,  17  considers the case where only statistical information about the channel...",
        "watermark_text": "We consider the question of quantizing an arbitrary vector in a Grassmannian manifold to minimize its distortion under a given rate constraint , which is relevant for multi - input - multi - output ( MIMO ) communications systems employing feedback . We derive upper limits on the minimum achievable distortion as well as lower bounds on the associated optimal prices by using data - theoretic techniques such as entropy energy inequalities and information processing inequalities .Our results show that the performance difference between these two limits increases when the dimension of the embedded Grassmannian manifold rises huge . Finally , we provide numerical examples illustrating our theoretical results .The work presented here was supported by NSF Grant CCF - 0635035 . Multi - input - multi - output communication systems are widely used in wireless networks due to their high spectral efficiency 1 .In this sense , it has been shown recently 2 , that the using of restricted - frequency feedback can significantly boost design efficiency at low sound - to - noise ratios ( SNRs ) . However , the quantity of available feedback opportunities would be severely constrained in practice 3 .In order to reduce the necessary feedback overhead while maintaining good efficiency , one method relies of exploiting channel state information ( CSI ) , i . e . , knowledge about the present fading coefficients , to conduct joint encoding across multiple send antennas 4 - 6 . This method , known as spatial multiplexing or beamforming , requires CSI at both antenna and receiver sides .Since obtaining perfect CSI at the broadcasting side through education - based methods typically requires large signaling overhead 7 , practical implementations usually resort to quantized versions of the true CSI 8 - 10 . Therefore , there exists a trade - off between the accuracy of the transmitted signals and the quantity of feedback needed to transport them 11 .The model of effective transmission strategies over MIMO sources with restricted input remains an open research field 12 . A several of recent works have concentrated on characterizing fundamental limits related with various parts of MIMO systems operating under various assumptions regarding the availability of CSI 13 - 16 .For instance , 17 considers the case where only statistical information about the channel . . .",
        "rewrite_text": "We explore the challenge of quantizing an arbitrary vector in a Grassmannian manifold to minimize distortion while adhering to a specific rate constraint, which is pertinent for multi-input multi-output (MIMO) communication systems that utilize feedback. By employing data-theoretic methods, such as entropy energy inequalities and information processing inequalities, we derive upper limits on the minimum achievable distortion and lower bounds on the corresponding optimal costs. Our findings indicate that the performance gap between these two limits widens significantly as the dimension of the embedded Grassmannian manifold increases. Additionally, we present numerical examples that demonstrate our theoretical results. This research was supported by NSF Grant CCF-0635035. \n\nMIMO communication systems are prevalent in wireless networks, primarily due to their high spectral efficiency. Recent studies have shown that employing restricted-frequency feedback can greatly enhance design efficiency, particularly at low signal-to-noise ratios (SNRs). However, practical implementations often face significant constraints on the available feedback opportunities. To mitigate the required feedback overhead while preserving efficiency, one approach hinges on utilizing channel state information (CSI)—knowledge of current fading coefficients—to facilitate joint encoding across multiple transmitting antennas. This technique, referred to as spatial multiplexing or beamforming, necessitates CSI at both the transmitter and receiver ends. Since acquiring perfect CSI via education-based methods typically incurs substantial signaling overhead, practical implementations generally resort to quantized versions of the true CSI. Consequently, a trade-off arises between the accuracy of the transmitted signals and the amount of feedback required to convey them. The study of effective transmission strategies for MIMO systems with limited input remains an open area of research. Several recent papers have focused on identifying fundamental limits associated with various aspects of MIMO systems under different assumptions about CSI availability. For example, one study examines scenarios where only statistical information about the channel is accessible.",
        "ori-fast-z-score": 0.8944271909999159,
        "water-fast-z-score": 8.944587733233664,
        "rewrite-fast-z-score": 2.076923076923077
    },
    {
        "original_text": "The observed smallness of the vacuum energy density is one of the most puzzling problems in physics today, and it has been suggested that this problem may be solved by considering quantum gravity effects on the vacuum fluctuations.  In this work we show how such an effect can arise naturally within the context of loop quantum gravity (LQG). We consider a model where the gravitational field is quantized using LQG techniques while matter fields are treated classically. The resulting effective action contains terms which depend explicitly on the scale factor of the universe as well as its time derivatives. These terms lead to corrections to the standard Friedmann equations at high energies. Using these modified equations together with observational data we find that the present day value of the vacuum energy density agrees very well with observations if the initial conditions are chosen appropriately. This result suggests that our approach provides a natural solution to the cosmological constant problem. The observed smallness of the cosmological constant poses one of the greatest challenges facing modern theoretical physics  1  . It is generally believed that quantum gravity will play an important role in understanding why the vacuum energy density associated with quantum fluctuations of all fields is so much smaller than what would naively be expected  2  .\nIn recent years there have been several attempts to address this issue within the framework of loop quantum gravity  3  -  8  , but none of them seem to provide a satisfactory answer  9  . In particular, the results obtained in Refs.  6  -  8  do not agree with each other or with current experimental bounds  10  . Here we propose a new mechanism based on ideas developed recently in Ref.  11  . Our starting point is the observation that the Wheeler-DeWitt equation derived from the canonical formulation of general relativity leads to modifications of the usual Schrödinger equation when applied to states describing macroscopic systems  12  . As shown in Ref.  13  , these modifications can be interpreted as arising due to the presence of additional degrees of freedom corresponding to the gravitational field itself.",
        "watermark_text": "The observed smallness of the vacuum energy density is one of the most puzzling difficulties in science today , and it has been proposed that this question could be answered by investigating quantum gravitational influences on the vacuum fluctuations . In this research we explain how such an influence can arise naturally within the context of loop quantum gravitational ( LQG ) .We consider a description where the gravitational field is quantized utilizing LQG techniques while matter forces are treated classically . The resulting effective action contains terms which depend explicitly on the scale factor of the universe as well as its time functions .These terms lead to corrections to the standard Friedmann equations at high energies . Using these modified equations together with observational data we find that the present day value of the vacuum energy density agrees very best with observations if the first terms are chosen properly .This result suggests that our approach offers a natural solution to the cosmological constant problem . The observed smallness of the cosmological constant presents one of the greatest challenges facing current theoretical physics 1 .It is usually thought that quantum gravitational will take an important role in understanding why the vacuum energy density associated with quantum fluctuations of all fields is so even smaller than what would naively be anticipated 2 . In recent history there have been numerous attempts to tackle this question within the framework of loop quantum gravitational 3 - 8 , but none of them seem to provide a adequate answer 9 .In particular , the results derived in Refs . 6 - 8 do not comply with each other or with current experimental bounds 10 .Here we propose a new method using on ideas developed lately in Ref . 11 .Our starting point is the observation that the Wheeler - DeWitt equation derived from the canonical formulation of general relativity leads to modifications of the usual Schrödinger equation when applied to states describing macroscopic systems 12 . As seen in Ref .13 , these alterations can be interpreted as occurring due to the presence of added degrees of freedom corresponding to the gravitational field itself .",
        "rewrite_text": "The unexpectedly low value of vacuum energy density stands as one of the most perplexing challenges in contemporary science. One proposed avenue for addressing this issue involves exploring the quantum gravitational effects on vacuum fluctuations. In our research, we demonstrate how these effects can emerge naturally within the framework of loop quantum gravity (LQG). We adopt a model in which the gravitational field is quantized using LQG techniques, while the matter forces are treated classically. This approach results in an effective action that incorporates terms explicitly dependent on the universe's scale factor and its time evolution. These terms introduce modifications to the standard Friedmann equations at high energy levels. By utilizing these revised equations alongside observational data, we discover that the current value of the vacuum energy density aligns best with observations when the initial terms are selected judiciously. This finding indicates that our method may provide a natural resolution to the cosmological constant problem. The small value of the cosmological constant poses a significant hurdle for modern theoretical physics. Many believe that quantum gravitational effects will play a crucial role in understanding why the vacuum energy density associated with quantum fluctuations of various fields is markedly lower than naïve expectations. Recent efforts to address this issue within the LQG framework have emerged, yet none have offered a sufficient solution. Specifically, the results from various studies do not align with one another or with current experimental limits. In this work, we introduce a novel approach inspired by recent developments. Our basis lies in the understanding that the Wheeler-DeWitt equation, derived from the canonical formulation of general relativity, leads to alterations of the standard Schrödinger equation when applied to states representing macroscopic systems. As noted in earlier work, these modifications can be understood as arising from additional degrees of freedom associated with the gravitational field itself.",
        "ori-fast-z-score": 1.323448205074589,
        "water-fast-z-score": 8.17423891369599,
        "rewrite-fast-z-score": -0.31426968052735443
    },
    {
        "original_text": "The polarimetry of Comet Hale-Bopp was studied by the University of Hawaii 2 m telescope (UH2T) and the 1.5 m telescope at Mt. Wilson Observatory in 1997-1998, using broadband filters centered on 0.55 μm, 0.65 μm, 0.85 μm, and 1.25 μm.  The results show that the polarization degree is generally low for all wavelengths except near perihelion when it increases to about 10%-15% with an angle of ~40°-50° counterclockwise relative to solar north pole. This indicates that the dust particles are aligned perpendicularly to the Sun-comet-observer line. We also find that there exists a wavelength dependence of the polarization degree which decreases as the wavelength becomes longer. These observations suggest that the size distribution of the dust grains has a power law index between -3 and -4. In addition, we found that the polarization degree shows no significant variation during the period of our observation.",
        "watermark_text": "The polarimetry of Comet Hale - Bopp was studied by the University of Hawaii 2 m observatory ( UH2T ) and the 1 . 5 m observatory at Mt . Wilson Observatory in 1997 - 1998 , using broadband filters centered on 0 . 55 μm , 0 . 65 μm , 0 . 85 μm , and 1 . 25 μm .The results show that the polarization degree is typically poor for all wavelengths except near perihelion when it rises to about 10 % - 15 % with an angle of ~ 40° - 50° counterclockwise relative to solar west pole . This implies that the dust particles are aligned perpendicularly to the Sun - comet - observer path .We additionally find that there exists a frequency dependence of the polarization degree which drops as the frequency becomes longer . These measurements suggest that the size distribution of the dust grains has a power law index between - 3 and - 4 .In addition , we concluded that the polarization degree exhibits no considerable variation during the period of our experiment .",
        "rewrite_text": "The polarimetric analysis of Comet Hale-Bopp was conducted by the University of Hawaii's 2-meter telescope (UH2T) and the 1.5-meter telescope at Mt. Wilson Observatory during 1997-1998. This study utilized broadband filters centered at 0.55 μm, 0.65 μm, 0.85 μm, and 1.25 μm. The findings indicate that the degree of polarization is generally low across all wavelengths, except near perihelion, where it increases to approximately 10% to 15% at an angle of about 40° to 50° counterclockwise from the solar west pole. This suggests that the dust particles are aligned perpendicular to the line connecting the Sun, comet, and observer. Additionally, we observed a frequency dependence in the polarization degree, which decreases with longer wavelengths, implying that the size distribution of the dust grains follows a power-law index between -3 and -4. Furthermore, we concluded that there was no significant variation in the polarization degree during the duration of our observations.",
        "ori-fast-z-score": 1.1094003924504583,
        "water-fast-z-score": 5.547001962252292,
        "rewrite-fast-z-score": 0.13736056394868904
    },
    {
        "original_text": "We consider the capacity region of a multiple-input-multiple-output (MIMO) fading channel in which each transmitter has perfect knowledge of its own instantaneous channel state information and uses a quantized precoding matrix to maximize mutual information between itself and the receiver. We show that, for any given number of transmit antennas at all transmitters, there exists an optimal number of receive antennas such that the sum-capacity is maximized when all receivers have this same number of antennas. This result holds even if the channels are correlated across time or frequency. The optimal number of receive anten-nas increases as more transmit antennas are used by the transmitters. For example, we find that using four transmit antennas results in the highest sum-rate when three receive antennas are employed per user; however, five receive antennas should be used instead if eight transmit antennas are available.",
        "watermark_text": "We consider the capacity area of a multiple - input - multiple - output ( MIMO ) fading signal in which each antenna has good knowledge of its own instantaneous channel state information and using a quantized precoding vector to maximize mutual information between itself and the receiver . We see that , for any certain number of transmit antennas at all transmitters , there exists an appropriate number of receive antennas such that the sum - capacity is maximized when all transmissions have this same amount of antennas .This result holds even if the transmissions are correlated across time or bandwidth . The appropriate number of receive anten - nas increases as more transmit antennas are using by the transmitters .For instance , we find that using four transmit antennas results in the highest sum - efficiency when three receive antennas are employed per user ; however , five send antennas should be used instead if eight send antennas are available .",
        "rewrite_text": "We examine the capacity region of a multiple-input multiple-output (MIMO) fading signal in which each antenna possesses accurate knowledge of its own instantaneous channel state information. By employing a quantized precoding vector, the goal is to maximize the mutual information between the transmitter and receiver. Our findings indicate that for a given number of transmit antennas across all transmitters, there exists an optimal number of receive antennas that maximizes the sum capacity when all transmissions utilize the same number of antennas. This conclusion remains valid even in scenarios where the transmissions exhibit temporal or bandwidth correlations. Additionally, the optimal number of receive antennas increases as the transmitters utilize more transmit antennas. For example, when using four transmit antennas, the highest sum efficiency is achieved with three receive antennas per user; however, if eight transmit antennas are available, five transmit antennas should be employed instead.",
        "ori-fast-z-score": -0.762000762001143,
        "water-fast-z-score": 4.318004318006477,
        "rewrite-fast-z-score": 1.3438638879193574
    },
    {
        "original_text": "We present the results of an investigation into the spatial distribution and temporal evolution of magnetic field strength in active region plage using high-resolution spectropolarimetric data obtained with Hinode/SOT-SP. We find that, on average, the magnetic field is stronger at higher heights above the solar surface (i.e., closer to the limb) than it is near disk center. The mean unsigned longitudinal magnetic flux density decreases by about 50% between 0.3′′ and 1′′ above the solar surface. This decrease occurs over a range of heliocentric angles where the photospheric plasma β increases significantly. In addition, we find evidence for significant horizontal structuring of the magnetic fields within individual pixels. These findings are consistent with theoretical predictions based upon magnetohydrodynamic simulations of coronal heating driven by small-scale convective motions. Finally, we show how these observations can be used as input parameters for models of chromospheric heating via Alfvén waves generated by resonant absorption.",
        "watermark_text": "We present the conclusion of an research into the spatial distribution and spatial evolution of magnetic force power in active region plage using high - resolution spectropolarimetric evidence derived with Hinode / SOT - SP . We see that , on average , the magnetic force is greater at higher altitudes above the solar surface ( i . e . , farther to the limb ) than it is near disk area .The mean unsigned lateral magnetic flux concentration drops by about 50 % between 0 . 3 ′ ′ and 1 ′ ′ above the solar surface . This decrease occurs over a range of heliocentric angles where the photospheric plasma β changes significantly .In addition , we find proof for significant horizontal structuring of the magnetic waves within individual pixels . These studies are compatible with theoretical estimates based upon magnetohydrodynamic simulations of coronal heating driven by small - scale convective movements .Finally , we give how these observations can be used as input parameters for models of chromospheric heating via Alfvén currents produced by resonant absorption .",
        "rewrite_text": "We present the findings of a study on the spatial distribution and evolution of magnetic force in active region plage, utilizing high-resolution spectropolarimetric data obtained from Hinode/SOT-SP. Our analysis reveals that, on average, the magnetic force is stronger at greater altitudes above the solar surface (i.e., closer to the limb) compared to areas near the solar disk. Specifically, the mean unsigned lateral magnetic flux concentration decreases by approximately 50% between 0.3″ and 1.0″ above the solar surface. This decline occurs over a range of heliocentric angles where the plasma beta in the photosphere experiences significant changes. Additionally, we observe substantial horizontal structuring of the magnetic fields within individual pixels. These results are consistent with theoretical predictions derived from magnetohydrodynamic simulations that explore coronal heating driven by small-scale convective motions. Finally, we discuss how these observations can serve as input parameters for models regarding chromospheric heating through Alfvén currents generated by resonant absorption.",
        "ori-fast-z-score": -0.1111111111111111,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": -0.7777777777777778
    },
    {
        "original_text": "We present photoionization models for the heliosheath, which is the region between the termination shock (TS) at ~100 AU and the solar wind termination shock (SWTS). The SWTS is located beyond 1 AU in the solar system s frame but within 0.3 AU in the rest frame of the Sun. We use these models to constrain the boundary conditions of the heliosphere using interstellar neutral hydrogen data obtained with the Lyman-alpha instrument on board the Solar Wind Anisotropy Probe (SWAP), as well as in situ measurements made near Earth during the Voyager 2 mission. Our results show that the TS distance decreases with increasing solar activity; this effect can be explained by an increase in the density of the solar wind plasma. For low solar activity levels we find that the TS distance agrees very well with previous estimates based on observations of energetic particles.",
        "watermark_text": "We use photoionization estimates for the heliosheath , which is the region between the termination shock ( TS ) at ~ 100 AU and the solar wind termination shock ( SWTS ) . The SWTS is situated beyond 1 AU in the solar system s frame but within 0 . 3 AU in the remainder frame of the Sun .We use these models to constrain the boundary parameters of the heliosphere utilizing interstellar neutral hydrogen evidence derived with the Lyman - alpha instrument on board the Solar Wind Anisotropy Probe ( SWAP ) , as well as in situ measurements made near Earth during the Voyager 2 mission . Our results show that the TS distance reduces with rising sun activity ; this effect can be described by an increase in the density of the solar wind plasma .For low solar activity rates we find that the TS distance agrees very best with previous estimates based on observations of energetic particles .",
        "rewrite_text": "We analyze photoionization estimates for the heliosheath, the region situated between the termination shock (TS) at approximately 100 AU and the solar wind termination shock (SWTS). While the SWTS is located beyond 1 AU in the solar system's frame of reference, it is found within 0.3 AU when viewed from the Sun's rest frame. Using these models, we constrain the boundary parameters of the heliosphere by leveraging interstellar neutral hydrogen data collected by the Lyman-alpha instrument on the Solar Wind Anisotropy Probe (SWAP), along with in situ measurements taken near Earth during the Voyager 2 mission. Our findings indicate that the distance to the TS decreases with increasing solar activity, a phenomenon that can be attributed to a rise in the density of solar wind plasma. During periods of low solar activity, we observe that the TS distance aligns closely with earlier estimates derived from observations of energetic particles.",
        "ori-fast-z-score": -0.13018891098082389,
        "water-fast-z-score": 2.994344952558949,
        "rewrite-fast-z-score": 0.13018891098082389
    },
    {
        "original_text": "We present an approach to extract the underlying physics from large sets of experimental data by using machine learning techniques and statistical analysis. The method is applied on two different examples, namely the measurement of the electrical conductivity in doped semiconductors as well as the determination of the critical temperature Tc for superconductivity in cuprates. In both cases we find that our results are consistent with theoretical predictions. We show how this new technique can be used to identify unknown parameters or even completely new phenomena which cannot be predicted theoretically at all. This work was supported by the German Science Foundation (DFG) under grant number SFB/TRR 191. A central goal of modern science is to understand complex systems such as materials or living organisms through their fundamental building blocks. To achieve this aim it is necessary to develop methods which allow us to analyze huge amounts of experimental data efficiently. Here we propose a novel approach based on machine learning algorithms combined with advanced statistical tools. Our method is demonstrated on two examples: First, we study the dependence of the electrical conductivity on doping concentration in semiconductor compounds. Second, we determine the critical temperature T c for superconductivity in high-temperature cuprate oxides. For both problems we obtain results which agree very well with existing theories. Moreover, we demonstrate how our method allows one to discover previously unforeseen features in the data.",
        "watermark_text": "We present an way to extract the fundamental theory from huge sets of experimental evidence by using computer learning techniques and statistical analysis . The method is applied on two different examples , notably the determination of the electrical conductivity in doped semiconductors as well as the determination of the significant heat Tc for superconductivity in cuprates .In both cases we find that our findings are compatible with theoretical expectations . We see how this new technique can be used to identify unseen conditions or even completely new events which cannot be described theoretically at all .This project was supported by the German Science Foundation ( DFG ) under grant number SFB / TRR 191 . A central goal of modern science is to explain complex systems such as materials or living organisms through their major building frames .To achieve this aim it is required to develop techniques which allow us to analyze huge amounts of research data easily . Here we undertake a innovative method built on machine computing methods coupled with sophisticated statistical tools .Our method is demonstrated on two examples : First , we study the relationship of the electrical conductivity on doping concentration in semiconductor compounds . Second , we determine the significant heat T c for superconductivity in high - temperature cuprate oxides .For both difficulties we obtain results which agree very best with existing ideas . Moreover , we prove how our technique permits one to identify formerly unforeseen features in the information .",
        "rewrite_text": "We introduce a novel approach for extracting fundamental theories from extensive experimental data by employing machine learning techniques and statistical analysis. This method is applied to two distinct cases: the investigation of electrical conductivity in doped semiconductors and the determination of the critical temperature (Tc) for superconductivity in cuprate materials. In both instances, our results align well with theoretical predictions. Our findings illustrate how this innovative technique can uncover hidden conditions and even entirely new phenomena that existing theories fail to describe. This project received support from the German Science Foundation (DFG) under grant number SFB/TRR 191. A primary objective of contemporary science is to elucidate complex systems, such as materials and living organisms, by understanding their fundamental components. Achieving this goal necessitates the development of methods that facilitate the analysis of vast amounts of research data. In this work, we present a pioneering technique that integrates machine learning with advanced statistical tools. We demonstrate our method through two examples: first, we explore the relationship between electrical conductivity and doping concentration in semiconductor compounds; second, we analyze the critical temperature for superconductivity in high-temperature cuprate oxides. In both cases, our results are in strong agreement with established theoretical concepts. Additionally, we showcase how our approach can reveal previously unrecognized features in the data.",
        "ori-fast-z-score": -0.3682298471593294,
        "water-fast-z-score": 7.98489954161323,
        "rewrite-fast-z-score": -0.09667364890456635
    },
    {
        "original_text": "The projectile fragmentation of 86Kr at 64MeV/nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a beam intensity of 1nAe. The main results are as follows:  - A total number of about 10000 events have been recorded for this experiment.  - The charge distribution is peaked around Z=40, but shows also a large contribution between 30 and 40 charges units (see fig.1 ). This indicates that the fragments produced by the break-up of 86Kr are not only light particles like neutrons or protons, but contain many intermediate mass fragments too.   - The angular distributions show two peaks corresponding to forward and backward emission respectively (see fig.2 ).  - The energy spectra present a maximum around 10-12 MeV/u which corresponds to the most probable kinetic energy per nucleon of the emitted fragments (see fig.3 ).\n- The isotopic composition of the fragments is shown on figure 4 . It can be seen that there is no significant difference between the fragment production in the forward hemisphere and in the backward one.",
        "watermark_text": "The projectile fragmentation of 86Kr at 64MeV / nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a laser intensity of 1nAe . The main results are as follows : - A total quantity of about 10000 events have been observed for this study .- The charge distribution is peaked around Z = 40 , but shows also a large contribution between 30 and 40 charges units ( view fig . 1 ) . This implies that the fragments produced by the broken - up of 86Kr are not only light particles like neutrons or protons , but contain many intermediate mass pieces too .- The angular distributions show two peaks related to forward and back emission respectively ( see fig . 2 ) . - The energy spectra produce a maximum around 10 - 12 MeV / u which equals to the most likely kinetic power per nucleon of the emitted objects ( see fig . 3 ) .- The isotopic structure of the fragments is displayed on figure 4 . It can be shown that there is no major variation between the fragment production in the front hemisphere and in the backward one .",
        "rewrite_text": "The projectile fragmentation of 86Kr at 64 MeV/nucleon was investigated using the INDRA multidetector system in inverse kinematics with an 8 cm thick natK target and a laser intensity of 1 nA. The key findings are as follows: - Approximately 10,000 events were recorded during this study. - The charge distribution is concentrated around Z = 40 but also displays significant contributions from charge units ranging between 30 and 40 (see Fig. 1). This indicates that the fragments resulting from the breakup of 86Kr include not only light particles like neutrons and protons but also numerous intermediate mass fragments. - The angular distributions reveal two peaks corresponding to forward and backward emissions, respectively (refer to Fig. 2). - The energy spectra exhibit a peak around 10-12 MeV/u, which corresponds to the most probable kinetic energy per nucleon of the emitted fragments (see Fig. 3). - The isotopic composition of the fragments is illustrated in Figure 4, which shows no significant difference in fragment production between the forward and backward hemispheres.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -0.47809144373375745
    },
    {
        "original_text": "In this work, we propose an energy-efficient cooperative transmission scheme for wireless sensor networks (WSNs). The proposed scheme is based on the combination of collaborative beamforming at the source node with cooperative transmission to multiple relay nodes. In particular, by exploiting channel state information (CSI) feedbacks from all the relays, the source can adjust its transmit power level according to the instantaneous CSI so as to maximize the total network lifetime while satisfying certain quality-of-service requirements. We first derive closed-form expressions for the optimal power allocation between the source and each relay under different system settings. Then, using these results, we formulate the problem of maximizing the WSN s lifetime subject to minimum data rate constraints into a convex optimization framework which can be efficiently solved numerically. Finally, simulation results are presented to verify our theoretical analysis and demonstrate that significant performance gains over conventional schemes can be achieved through the use of the proposed approach.",
        "watermark_text": "In this research , we propose an energy - efficient joint propagation scheme for wireless sensor networks ( WSNs ) . The proposed system is based on the combination of collaborative beamforming at the source node with coordinated transmission to multiple relay nodes .In particular , by exploiting channel state information ( CSI ) feedbacks from all the relays , the source can adjust its broadcast capacity level according to the instantaneous CSI so as to maximize the total channel lifetime while fulfilling various quality - of - service standards . We first derive closed - form expressions for the ideal power distribution between the source and each relay under various system settings .Then , using these results , we formulate the question of maximizing the WSN s life due to minimum data rate constraints into a convex optimization framework which can be easily solution numerically . Finally , simulation data are presented to confirm our theoretical analysis and suggest that significant efficiency increases over traditional schemes can be obtained through the using of the suggested approach .",
        "rewrite_text": "In this study, we introduce an energy-efficient joint propagation scheme designed for wireless sensor networks (WSNs). Our proposed system integrates collaborative beamforming at the source node with coordinated transmission to multiple relay nodes. By leveraging channel state information (CSI) feedback from all relays, the source can dynamically adjust its broadcast capacity based on real-time CSI, aiming to maximize the overall channel lifetime while meeting various quality-of-service requirements. We begin by deriving closed-form expressions for the optimal power distribution between the source and each relay across different system configurations. Building on these findings, we frame the challenge of maximizing the WSN's lifespan under minimum data rate constraints as a convex optimization problem that can be solved efficiently through numerical methods. Finally, we present simulation results that validate our theoretical analysis and demonstrate that our proposed approach can achieve substantial efficiency improvements compared to traditional methods.",
        "ori-fast-z-score": -0.4364357804719848,
        "water-fast-z-score": 5.7486571321943885,
        "rewrite-fast-z-score": 0.5488212999484517
    },
    {
        "original_text": "We propose to use the concept of Maxwell s demon in order to explain how nanodevices can be used for information processing, storage or transmission. We show that this approach is useful because it allows us to understand why some devices are more efficient than others at performing these tasks. In particular we consider two types of nanodevices which have been proposed recently as candidates for quantum computers - spin chains and arrays of coupled cavities. The first type consists of an array of spins arranged on a line with nearest neighbour interactions between them while the second one has a similar structure but instead of spins it contains atoms trapped inside optical cavities. For both cases we calculate their efficiency using the Landauer principle. Finally we discuss possible experimental implementations of our ideas. Introduction:-The idea of Maxwell s demon was introduced by James Clerk Maxwell (1831-1879)  1  . It describes a hypothetical intelligent being who could control microscopic particles individually so that they would always move into separate containers depending on whether each particle had a higher energy level or lower energy level  2  .\nMaxwell s demon is usually described as a thought experiment whose purpose is to demonstrate that entropy cannot decrease spontaneously  3  , i.e., that heat does not flow spontaneously from hot bodies to cold ones  4  . However, there exists another interpretation of Maxwell s demon according to which he imagined a device capable of sorting individual molecules based on their velocities  5  . This interpretation leads naturally to the question about what sort of physical system might behave like such a device  6  .",
        "watermark_text": "We suggest to use the idea of Maxwell s demon in order to explain how nanodevices can be used for information processing , processing or transmission . We suggest that this methodology is beneficial because it allows us to realize why some machines are more efficient than others at performing these tasks .In particular we investigate two forms of nanodevices which have been proposed lately as candidates for quantum computers - spinning chains and arrays of coupled cavities . The first sort consists of an array of spinning grouped on a line with nearest neighbour interactions between them while the second one has a similar composition but instead of spinning it contains atoms trapped inside optical cavities .For both cases we estimate their productivity using the Landauer law . Finally we talk proposed experimental implementations of our concepts .Introduction : - The idea of Maxwell s demon was introduced by James Clerk Maxwell ( 1831 - 1879 ) 1 . It describes a hypothetical intellectual being who might control microscopic particles individually so that they would everyone travel into independent tanks depending on whether each particle had a higher energy level or lesser energy level 2 .Maxwell s creature is usually characterized as a thought experiment whose purpose is to demonstrate that entropy cannot fall spontaneously 3 , i . e . , that heat does not flow spontaneously from hot bodies to hot ones 4 . However , there exists another explanation of Maxwell s demon according to which he imagined a device capable of sorting individual molecules based on their velocities 5 .This interpretation turns naturally to the question about what sort of natural system might perform like such a device 6 .",
        "rewrite_text": "We propose utilizing the concept of Maxwell's demon to elucidate how nanodevices can facilitate information processing, transfer, or computation. This approach is advantageous as it helps explain why certain machines outperform others in these functions. Specifically, we examine two types of recently proposed nanodevices that are candidates for quantum computing: spinning chains and arrays of coupled cavities. The spinning chains consist of an array of spinning elements aligned in a line with nearest-neighbor interactions, while the second type replaces spinning with atoms trapped in optical cavities. We assess the productivity of both configurations using the Landauer principle. Lastly, we discuss potential experimental implementations of our ideas.\n\nIntroduction: The concept of Maxwell's demon was first introduced by James Clerk Maxwell (1831 - 1879). It depicts a hypothetical intelligent being capable of individually controlling microscopic particles, directing them into separate containers based on whether they possess higher or lower energy levels. Maxwell's demon is often considered a thought experiment aimed at demonstrating that entropy cannot spontaneously decrease, meaning that heat does not flow on its own from hotter to cooler bodies. However, there is an alternative interpretation of Maxwell's demon, which envisions a device that can sort individual molecules according to their velocities. This leads us to ponder what kind of natural system could function like such a device.",
        "ori-fast-z-score": 0.09016696346674323,
        "water-fast-z-score": 6.825001477017446,
        "rewrite-fast-z-score": 1.1881770515720091
    }
]